<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 23 Jul 2025 04:01:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Continuous Test-time Domain Adaptation for Efficient Fault Detection under Evolving Operating Conditions</title>
      <link>https://arxiv.org/abs/2507.16354</link>
      <description>arXiv:2507.16354v1 Announce Type: new 
Abstract: Fault detection is essential in complex industrial systems to prevent failures and optimize performance by distinguishing abnormal from normal operating conditions. With the growing availability of condition monitoring data, data-driven approaches have seen increased adoption in detecting system faults. However, these methods typically require large, diverse, and representative training datasets that capture the full range of operating scenarios, an assumption rarely met in practice, particularly in the early stages of deployment.
  Industrial systems often operate under highly variable and evolving conditions, making it difficult to collect comprehensive training data. This variability results in a distribution shift between training and testing data, as future operating conditions may diverge from previously observed ones. Such domain shifts hinder the generalization of traditional models, limiting their ability to transfer knowledge across time and system instances, ultimately leading to performance degradation in practical deployments.
  To address these challenges, we propose a novel method for continuous test-time domain adaptation, designed to support robust early-stage fault detection in the presence of domain shifts and limited representativeness of training data. Our proposed framework --Test-time domain Adaptation for Robust fault Detection (TARD) -- explicitly separates input features into system parameters and sensor measurements. It employs a dedicated domain adaptation module to adapt to each input type using different strategies, enabling more targeted and effective adaptation to evolving operating conditions. We validate our approach on two real-world case studies from multi-phase flow facilities, delivering substantial improvements over existing domain adaptation methods in both fault detection accuracy and model robustness under real-world variability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16354v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Han Sun, Olga Fink</dc:creator>
    </item>
    <item>
      <title>Accommodating the Analysis Model in Multiple Imputation for the Weibull Mixture Cure Model:Performance under Penalized Likelihood</title>
      <link>https://arxiv.org/abs/2507.16690</link>
      <description>arXiv:2507.16690v1 Announce Type: new 
Abstract: Introduction In analysis of time-to-event outcomes, a mixture cure (MC) model is preferred over a standard survival model when the sample includes individuals who will never experience the event of interest. Motivated by a cohort study of breast cancer patients with incomplete biomarkers, we develop multiple imputation (MI) methods assuming a Weibull proportional hazards (PH-MC) analysis model with multiple prognostic factors. However, for MI with fully conditional specification, an incorrectly-specified imputation model can impair accuracy of point and interval estimates.
  Objectives and Methods Our goal is to propose imputation models that are compatible with the Weibull PH-MC analysis models. We derive an exact conditional distribution (ECD) imputation model which involves the analysis model likelihood. Using simulation studies, we compare effect estimate bias and confidence interval (CI) coverage under alternative imputation models including the ECD model, an approximation that includes a cure indicator (cECD), and a comprehensive simple (CS) model. For robust parameter estimation in finite and/or sparse samples, we incorporate the Firth-type penalized likelihood (FT-PL) and combined likelihood profile (CLIP) methods into the MI.
  Results Compared to complete case analysis, MI with penalization reduces estimation bias and improves coverage. Although ECD and cECD perform similarly at higher event rates, ECD generates smaller bias and higher coverage at lower rates. CS has larger bias and lower coverage than ECD and cECD, but CIs are narrower than for cECD.
  Conclusions In analyses of biomarkers and composite subtypes for prognosis studies such as in breast cancer, use of compatible imputation models and penalization methods are recommended for MC modelling in samples with low event numbers and/or with covariate imbalance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16690v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Changchang Xu, Laurent Briollais, Irene L Andrulis, Shelley B Bull</dc:creator>
    </item>
    <item>
      <title>Ballot Design and Electoral Outcomes: The Role of Candidate Order and Party Affiliation</title>
      <link>https://arxiv.org/abs/2507.16722</link>
      <description>arXiv:2507.16722v1 Announce Type: new 
Abstract: We use causal inference to study how designing ballots with and without party designations impacts electoral outcomes when partisan voters rely on party-order cues to infer candidate affiliation in races without designations. If the party orders of candidates in races with and without party designations differ, these voters might cast their votes incorrectly. We identify a quasi-randomized natural experiment with contest-level treatment assignment pertaining to North Carolina judicial elections and use double machine learning to accurately capture the magnitude of such incorrectly cast votes. Using precinct-level election and demographic data, we estimate that 11.8% (95% confidence interval: [4.0%, 19.6%]) of democratic partisan voters and 15.4% (95% confidence interval: [7.8%, 23.1%]) of republican partisan voters cast their votes incorrectly due to the difference in party orders. Our results indicate that ballots mixing contests with and without party designations mislead many voters, leading to outcomes that do not reflect true voter preferences. To accurately capture voter intent, such ballot designs should be avoided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16722v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessandro Arlotto, Alexandre Belloni, Fei Fang, Sa\v{s}a Peke\v{c}</dc:creator>
    </item>
    <item>
      <title>Evaluating virtual-control-augmented trials for reproducing treatment effect from original RCTs</title>
      <link>https://arxiv.org/abs/2507.16048</link>
      <description>arXiv:2507.16048v1 Announce Type: cross 
Abstract: This study investigates the use of virtual patient data to augment control arms in randomised controlled trials (RCTs). Using data from the IST and IST3 trials, we simulated RCTs in which the recruitment in the control arms would stop after a fraction of the initially planned sample size, and would be completed by virtual patients generated by CTGAN and TVAE, two AI algorithms trained on the recruited control patients. In IST, the absolute risk difference(ARD) on death or dependency at 14 days was -0.012 (SE 0.014). Completing the control arm by CTGAN-generated virtual patients after the recruitment of 10% and 50% of participants, yielded an ARD of 0.004 (SE 0.014) (relative difference 133%) and -0.021 (SE 0.014) (relative difference 76%), respectively. Results were comparable with IST3 or TVAE. This is the first empirical demonstration of the risk of errors and misleading conclusions associated with generating virtual controls solely from trial data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16048v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alex Fernandes, Rapha\"el Porcher, Viet-Thi Tran, Fran\c{c}ois Petit</dc:creator>
    </item>
    <item>
      <title>Density Prediction of Income Distribution Based on Mixed Frequency Data</title>
      <link>https://arxiv.org/abs/2507.16150</link>
      <description>arXiv:2507.16150v1 Announce Type: cross 
Abstract: Modeling large dependent datasets in modern time series analysis is a crucial research area. One effective approach to handle such datasets is to transform the observations into density functions and apply statistical methods for further analysis. Income distribution forecasting, a common application scenario, benefits from predicting density functions as it accounts for uncertainty around point estimates, leading to more informed policy formulation. However, predictive modeling becomes challenging when dealing with mixed-frequency data. To address this challenge, this paper introduces a mixed data sampling regression model for probability density functions (PDF-MIDAS). To mitigate variance inflation caused by high-frequency prediction variables, we utilize exponential Almon polynomials with fewer parameters to regularize the coefficient structure. Additionally, we propose an iterative estimation method based on quadratic programming and the BFGS algorithm. Simulation analyses demonstrate that as the sample size for estimating density functions and observation length increase, the estimator approaches the true value. Real data analysis reveals that compared to single-sequence prediction models, PDF-MIDAS incorporating high-frequency exogenous variables offers a wider range of application scenarios with superior fitting and prediction performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16150v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yinzhi Wang, Yingqiu Zhu, Ben-Chang Shia, Lei Qin</dc:creator>
    </item>
    <item>
      <title>A Bayesian block maxima over threshold approach applied to corrosion assessment in heat exchanger tubes</title>
      <link>https://arxiv.org/abs/2507.16416</link>
      <description>arXiv:2507.16416v1 Announce Type: cross 
Abstract: Corrosion poses a hurdle for numerous industrial processes, and though corrosion can be measured directly, statistical approaches are often required to either correct for measurement error or extrapolate estimates of corrosion severity where measurements are unavailable. This article considers corrosion in heat exchangers tubes, where corrosion is typically reported in terms of maximum pit depth per inspected tube, and only a small proportion of tubes are inspected, suggesting extreme value theory (EVT) as suitable methodology. However, in data analysis of heat exchanger data, shallow tube-maxima pits often cannot be considered as extreme; although previous EVT approaches assume all the data are extreme. We overcome this by introducing a threshold - suggesting a block maxima over threshold approach, which leads to more robust inference around model parameters and predicted maximum pit depth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16416v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jess Spearing, Jarno Hartog</dc:creator>
    </item>
    <item>
      <title>Canonical Correlation Patterns for Validating Clustering of Multivariate Time Series</title>
      <link>https://arxiv.org/abs/2507.16497</link>
      <description>arXiv:2507.16497v1 Announce Type: cross 
Abstract: Clustering of multivariate time series using correlation-based methods reveals regime changes in relationships between variables across health, finance, and industrial applications. However, validating whether discovered clusters represent distinct relationships rather than arbitrary groupings remains a fundamental challenge. Existing clustering validity indices were developed for Euclidean data, and their effectiveness for correlation patterns has not been systematically evaluated. Unlike Euclidean clustering, where geometric shapes provide discrete reference targets, correlations exist in continuous space without equivalent reference patterns. We address this validation gap by introducing canonical correlation patterns as mathematically defined validation targets that discretise the infinite correlation space into finite, interpretable reference patterns. Using synthetic datasets with perfect ground truth across controlled conditions, we demonstrate that canonical patterns provide reliable validation targets, with L1 norm for mapping and L5 norm for silhouette width criterion and Davies-Bouldin index showing superior performance. These methods are robust to distribution shifts and appropriately detect correlation structure degradation, enabling practical implementation guidelines. This work establishes a methodological foundation for rigorous correlation-based clustering validation in high-stakes domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16497v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Isabella Degen, Zahraa S Abdallah, Kate Robson Brown, Henry W J Reeve</dc:creator>
    </item>
    <item>
      <title>A Partitioned Sparse Variational Gaussian Process for Fast, Distributed Spatial Modeling</title>
      <link>https://arxiv.org/abs/2507.16771</link>
      <description>arXiv:2507.16771v1 Announce Type: cross 
Abstract: The next generation of Department of Energy supercomputers will be capable of exascale computation. For these machines, far more computation will be possible than that which can be saved to disk. As a result, users will be unable to rely on post-hoc access to data for uncertainty quantification and other statistical analyses and there will be an urgent need for sophisticated machine learning algorithms which can be trained in situ. Algorithms deployed in this setting must be highly scalable, memory efficient and capable of handling data which is distributed across nodes as spatially contiguous partitions. One suitable approach involves fitting a sparse variational Gaussian process (SVGP) model independently and in parallel to each spatial partition. The resulting model is scalable, efficient and generally accurate, but produces the undesirable effect of constructing discontinuous response surfaces due to the disagreement between neighboring models at their shared boundary. In this paper, we extend this idea by allowing for a small amount of communication between neighboring spatial partitions which encourages better alignment of the local models, leading to smoother spatial predictions and a better fit in general. Due to our decentralized communication scheme, the proposed extension remains highly scalable and adds very little overhead in terms of computation (and none, in terms of memory). We demonstrate this Partitioned SVGP (PSVGP) approach for the Energy Exascale Earth System Model (E3SM) and compare the results to the independent SVGP case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16771v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Grosskopf, Kellin Rumsey, Ayan Biswas, Earl Lawrence</dc:creator>
    </item>
    <item>
      <title>A Topological Gaussian Mixture Model for Bone Marrow Morphology in Leukaemia</title>
      <link>https://arxiv.org/abs/2408.13685</link>
      <description>arXiv:2408.13685v2 Announce Type: replace 
Abstract: Acute myeloid leukaemia (AML) is a type of blood and bone marrow cancer characterized by the proliferation of abnormal clonal haematopoietic cells in the bone marrow leading to bone marrow failure. Over the course of the disease, angiogenic factors released by leukaemic cells drastically alter the bone marrow vascular niches resulting in observable structural abnormalities. We use a technique from topological data analysis - persistent homology - to quantify the images and infer on the disease through the imaged morphological features. We find that persistent homology uncovers succinct dissimilarities between the control, early, and late stages of AML development. We then integrate persistent homology into stage-dependent Gaussian mixture models for the first time, proposing a new class of models which are applicable to persistent homology summaries and able to both infer patterns in morphological changes between different stages of progression as well as provide a basis for prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13685v2</guid>
      <category>stat.AP</category>
      <category>math.AT</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiquan Wang, Anna Song, Antoniana Batsivari, Dominique Bonnet, Anthea Monod</dc:creator>
    </item>
    <item>
      <title>Multivariate spatial models for small area estimation of species-specific forest inventory parameters</title>
      <link>https://arxiv.org/abs/2503.07118</link>
      <description>arXiv:2503.07118v2 Announce Type: replace 
Abstract: National Forest Inventories (NFIs) provide statistically reliable information on forest resources at national and other large spatial scales. As forest management and conservation needs become increasingly complex, NFIs are being called upon to provide forest parameter estimates at spatial scales smaller than current design-based estimation procedures can provide. This is particularly true when estimates are desired by species or species groups. Here we propose a multivariate spatial model for small area estimation of species-specific forest inventory parameters. The hierarchical Bayesian modeling framework accounts for key complexities in species-specific forest inventory data, such as zero-inflation, correlations among species, and residual spatial autocorrelation. Importantly, by fitting the model directly to the individual plot-level data, the framework enables estimates of species-level forest parameters, with associated uncertainty, across any user-defined small area of interest. A simulation study revealed minimal bias and higher accuracy of the proposed model-based approach compared to design-based estimator. We applied the model to estimate species-specific county-level aboveground biomass for the 20 most abundant tree species in the southern United States using Forest Inventory and Analysis (FIA) data. Model-based biomass estimates had high correlations with design-based estimates, yet the model-based estimates tended to have a slight positive bias relative to design-based estimates. Importantly, the proposed model provided large gains in precision across all 20 species. On average across species, 91.5% of county-level biomass estimates had higher precision compared to the design-based estimates. The proposed framework improves the ability of NFI data users to generate species-level forest parameter estimates with reasonable precision at management-relevant spatial scales.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07118v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeffrey W. Doser, Malcolm S. Itter, Grant M. Domke, Andrew O. Finley</dc:creator>
    </item>
    <item>
      <title>Modeling Maritime Transportation Behavior Using AIS Trajectories and Markovian Processes in the Gulf of St. Lawrence</title>
      <link>https://arxiv.org/abs/2506.00025</link>
      <description>arXiv:2506.00025v2 Announce Type: replace 
Abstract: Maritime transportation is central to the global economy, and analyzing its large-scale behavioral data is critical for operational planning, environmental stewardship, and governance. This work presents a spatio-temporal analytical framework based on discrete-time Markov chains to model vessel movement patterns in the Gulf of St. Lawrence, with particular emphasis on disruptions induced by the COVID-19 pandemic. We discretize the maritime domain into hexagonal cells and construct mobility signatures for distinct vessel types using cell transition frequencies and dwell times. These features are used to build origin-destination matrices and spatial transition probability models that characterize maritime dynamics across multiple temporal resolutions. Focusing on commercial, fishing, and passenger vessels, we analyze the temporal evolution of mobility behaviors during the pandemic, highlighting significant yet transient disruptions to recurring transport patterns. The methodology we contribute to this paper allows for an extensive behavioral analytics key for transportation planning. Accordingly, our findings reveal vessel-specific mobility signatures that persist across spatially disjoint regions, suggesting behaviors invariant to time. In contrast, we observe temporal deviations among passenger and fishing vessels during the pandemic, reflecting the influence of social isolation measures and operational constraints on non-essential maritime transport in this region.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00025v2</guid>
      <category>stat.AP</category>
      <category>cs.CE</category>
      <category>math.PR</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriel Spadon, Vaishnav Vaidheeswaran, Md Mahbub Alam, Ruixin Song, Floris Goerlandt, Ronald Pelot</dc:creator>
    </item>
    <item>
      <title>Unveiling the Impact of Social and Environmental Determinants of Health on Lung Function Decline in Cystic Fibrosis through Data Integration using the US Registry</title>
      <link>https://arxiv.org/abs/2506.08731</link>
      <description>arXiv:2506.08731v2 Announce Type: replace 
Abstract: Integrating diverse data sources offers a comprehensive view of patient health and holds potential for improving clinical decision-making. In Cystic Fibrosis (CF), which is a genetic disorder primarily affecting the lungs, biomarkers that track lung function decline such as FEV1 serve as important predictors for assessing disease progression. Prior research has shown that incorporating social and environmental determinants of health improves prognostic accuracy. To investigate the lung function decline among individuals with CF, we integrate data from the U.S. Cystic Fibrosis Foundation Patient Registry with social and environmental health information. Our analysis focuses on the relationship between lung function and the deprivation index, a composite measure of socioeconomic status.
  We used advanced multivariate mixed-effects models, which allow for the joint modelling of multiple longitudinal outcomes with flexible functional forms. This methodology provides an understanding of interrelationships among outcomes, addressing the complexities of dynamic health data. We examine whether this relationship varies with patients' exposure duration to high-deprivation areas, analyzing data across time and within individual US states. Results show a strong relation between lung function and the area under the deprivation index curve across all states. These results underscore the importance of integrating social and environmental determinants of health into clinical models of disease progression. By accounting for broader contextual factors, healthcare providers can gain deeper insights into disease trajectories and design more targeted intervention strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08731v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eleni-Rosalina Andrinopoulou, Emrah Gecili, Rhonda D Szczesniak</dc:creator>
    </item>
    <item>
      <title>Explainable Linear and Generalized Linear Models by the Predictions Plot</title>
      <link>https://arxiv.org/abs/2412.16980</link>
      <description>arXiv:2412.16980v3 Announce Type: replace-cross 
Abstract: Multiple linear regression is a basic statistical tool, yielding a prediction formula with the input variables, slopes, and an intercept. But is it really easy to see which terms have the largest effect, or to explain why the prediction of a specific case is unusually high or low? To assist with this the so-called predictions plot is proposed. Its simplicity makes it easy to interpret, and it combines much information. Its main benefit is that it helps explainability of the prediction formula as it is, without depending on how the formula was derived. The input variables can be numerical or categorical. Interaction terms are also handled, and the model can be linear or generalized linear. Another display is proposed to visualize correlations and covariances between prediction terms, in a way that is tailored for this setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16980v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Peter J. Rousseeuw</dc:creator>
    </item>
    <item>
      <title>Antibiotic Resistance Microbiology Dataset (ARMD): A Resource for Antimicrobial Resistance from EHRs</title>
      <link>https://arxiv.org/abs/2503.07664</link>
      <description>arXiv:2503.07664v2 Announce Type: replace-cross 
Abstract: The Antibiotic Resistance Microbiology Dataset (ARMD) is a de-identified resource derived from electronic health records (EHR) that facilitates research in antimicrobial resistance (AMR). ARMD encompasses big data from adult patients collected from over 15 years at two academic-affiliated hospitals, focusing on microbiological cultures, antibiotic susceptibilities, and associated clinical and demographic features. Key attributes include organism identification, susceptibility patterns for 55 antibiotics, implied susceptibility rules, and de-identified patient information. This dataset supports studies on antimicrobial stewardship, causal inference, and clinical decision-making. ARMD is designed to be reusable and interoperable, promoting collaboration and innovation in combating AMR. This paper describes the dataset's acquisition, structure, and utility while detailing its de-identification process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07664v2</guid>
      <category>q-bio.QM</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fateme Nateghi Haredasht, Fatemeh Amrollahi, Manoj Maddali, Nicholas Marshall, Stephen P. Ma, Lauren N. Cooper, Andrew O. Johnson, Ziming Wei, Richard J. Medford, Sanjat Kanjilal, Niaz Banaei, Stanley Deresinski, Mary K. Goldstein, Steven M. Asch, Amy Chang, Jonathan H. Chen</dc:creator>
    </item>
    <item>
      <title>Financial resilience of agricultural and food production companies in Spain: A compositional cluster analysis of the impact of the Ukraine-Russia war (2021-2023)</title>
      <link>https://arxiv.org/abs/2504.05912</link>
      <description>arXiv:2504.05912v2 Announce Type: replace-cross 
Abstract: This study analyzes the financial resilience of agricultural and food production companies in Spain amid the Ukraine-Russia war using cluster analysis based on financial ratios. This research utilizes centered log-ratios to transform financial ratios for compositional data analysis. The dataset comprises financial information from 1197 firms in Spain's agricultural and food sectors over the period 2021-2023. The analysis reveals distinct clusters of firms with varying financial performance, characterized by metrics of solvency and profitability. The results highlight an increase in resilient firms by 2023, underscoring sectoral adaptation to the conflict's economic challenges. These findings together provide insights for stakeholders and policymakers to improve sectorial stability and strategic planning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05912v2</guid>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Mike Hernandez-Romero, Germ\`a Coenders</dc:creator>
    </item>
    <item>
      <title>Bayesian Bootstrap based Gaussian Copula Model for Mixed Data with High Missing Rates</title>
      <link>https://arxiv.org/abs/2507.06785</link>
      <description>arXiv:2507.06785v2 Announce Type: replace-cross 
Abstract: Missing data is a common issue in various fields such as medicine, social sciences, and natural sciences, and it poses significant challenges for accurate statistical analysis. Although numerous imputation methods have been proposed to address this issue, many of them fail to adequately capture the complex dependency structure among variables. To overcome this limitation, models based on the Gaussian copula framework have been introduced. However, most existing copula-based approaches do not account for the uncertainty in the marginal distributions, which can lead to biased marginal estimates and degraded performance, especially under high missingness rates.
  In this study, we propose a Bayesian bootstrap-based Gaussian Copula model (BBGC) that explicitly incorporates uncertainty in the marginal distributions of each variable. The proposed BBGC combines the flexible dependency modeling capability of the Gaussian copula with the Bayesian uncertainty quantification of marginal cumulative distribution functions (CDFs) via the Bayesian bootstrap. Furthermore, it is extended to handle mixed data types by incorporating methods for ordinal variable modeling.
  Through simulation studies and experiments on real-world datasets from the UCI repository, we demonstrate that the proposed BBGC outperforms existing imputation methods across various missing rates and mechanisms (MCAR, MAR). Additionally, the proposed model shows superior performance on real semiconductor manufacturing process data compared to conventional imputation approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06785v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seongmin Kim, Jeunghun Oh, Hungkuk Ko, Jeongmin Park, Jaeyong Lee</dc:creator>
    </item>
  </channel>
</rss>
