<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 05 Jun 2024 01:58:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 04 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Far beyond day-ahead with econometric models for electricity price forecasting</title>
      <link>https://arxiv.org/abs/2406.00326</link>
      <description>arXiv:2406.00326v1 Announce Type: new 
Abstract: The surge in global energy prices during the recent energy crisis, which peaked in 2022, has intensified the need for mid-term to long-term forecasting for hedging and valuation purposes. This study analyzes the statistical predictability of power prices before, during, and after the energy crisis, using econometric models with an hourly resolution. To stabilize the model estimates, we define fundamentally derived coefficient bounds. We provide an in-depth analysis of the unit root behavior of the power price series, showing that the long-term stochastic trend is explained by the prices of commodities used as fuels for power generation: gas, coal, oil, and emission allowances (EUA). However, as the forecasting horizon increases, spurious effects become extremely relevant, leading to highly significant but economically meaningless results. To mitigate these spurious effects, we propose the "current" model: estimating the current same-day relationship between power prices and their regressors and projecting this relationship into the future. This flexible and interpretable method is applied to hourly German day-ahead power prices for forecasting horizons up to one year ahead, utilizing a combination of regularized regression methods and generalized additive models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00326v1</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul Ghelasi, Florian Ziel</dc:creator>
    </item>
    <item>
      <title>Estimating the Number of Street Vendors in New York City</title>
      <link>https://arxiv.org/abs/2406.00527</link>
      <description>arXiv:2406.00527v1 Announce Type: new 
Abstract: We estimate the number of street vendors in New York City. We first summarize the process by which vendors receive licenses and permits to legally operate in New York City. We then describe a survey that was administered by the Street Vendor Project while distributing Coronavirus relief aid to vendors operating in New York City both with and without a license or permit. Finally, we calculate the total number of vendors using ratio estimation. We find that approximately 23,000 street vendors operate in New York City: 20,500 mobile food vendors and 2,300 general merchandise vendors. One third are located in just six ZIP Codes: 11368 (16%), 11372 (3%), and 11354 (3%) in North and West Queens and 10036 (5%), 10019 (4%), and 10001 (3%) in the Chelsea and Clinton neighborhoods of Manhattan. We also provide a theoretical justification of our estimates based on the theory of point processes and a discussion of their accuracy and implications. In particular, our estimates suggest the American Community Survey fails to cover the majority of New York City street vendors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00527v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan Auerbach</dc:creator>
    </item>
    <item>
      <title>Analyzing trends for agricultural decision support system using twitter data</title>
      <link>https://arxiv.org/abs/2406.00577</link>
      <description>arXiv:2406.00577v2 Announce Type: new 
Abstract: The trends and reactions of the general public towards global events can be analyzed using data from social platforms, including Twitter. The number of tweets has been reported to help detect variations in communication traffic within subsets like countries, age groups and industries. Similarly, publicly accessible data and (in particular) data from social media about agricultural issues provide a great opportunity for obtaining instantaneous snapshots of farmer opinions and a method to track changes in opinion through temporal analysis. In this paper we hypothesize that the presence of keywords like precision agriculture, digital agriculture, Internet of Things (IoT), BigData, remote sensing, GPS, etc., in tweets could serve as an indicator of discussions centered around interest in modern farming practices. We extracted relevant tweets using keywords such as IoT, BigData and Geographical Information System (GIS), and then analyzed their geographical origin and frequency of their mention. We analyzed the Twitter data for the period of 1st -11th January 2018 to understand these trends and the factors affecting them. These factors, such as special events, projects, biogeography, etc., were further analyzed using tweet sources and trending hashtags from the database. The regions with the highest interest in the keywords were United States, Egypt, Brazil, Japan and China. A comparison of frequency of keywords revealed IoT as the most tweeted word (77.6%) in the downloaded data. The most used language was English followed by Spanish, Japanese and French. Periodical tweets on IoT from an account handled by IoT project on Twitter and Seminars on IoT in January in Santa Catarina (Brazil) were found to be the underlying factors for the observed trends.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00577v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sneha Jha, Dharmendra Saraswat, Mark D. Ward</dc:creator>
    </item>
    <item>
      <title>Aging modeling and lifetime prediction of a proton exchange membrane fuel cell using an extended Kalman filter</title>
      <link>https://arxiv.org/abs/2406.01259</link>
      <description>arXiv:2406.01259v1 Announce Type: new 
Abstract: This article presents a methodology that aims to model and to provide predictive capabilities for the lifetime of Proton Exchange Membrane Fuel Cell (PEMFC). The approach integrates parametric identification, dynamic modeling, and Extended Kalman Filtering (EKF). The foundation is laid with the creation of a representative aging database, emphasizing specific operating conditions. Electrochemical behavior is characterized through the identification of critical parameters. The methodology extends to capture the temporal evolution of the identified parameters. We also address challenges posed by the limiting current density through a differential analysis-based modeling technique and the detection of breakpoints. This approach, involving Monte Carlo simulations, is coupled with an EKF for predicting voltage degradation. The Remaining Useful Life (RUL) is also estimated. The results show that our approach accurately predicts future voltage and RUL with very low relative errors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01259v1</guid>
      <category>stat.AP</category>
      <category>cs.NA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.NA</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Serigne Daouda Pene, Antoine Picot, Fabrice Gamboa, Nicolas Savy, Christophe Turpin, Amine Jaafar</dc:creator>
    </item>
    <item>
      <title>Structural Health Monitoring with Functional Data: Two Case Studies</title>
      <link>https://arxiv.org/abs/2406.01262</link>
      <description>arXiv:2406.01262v1 Announce Type: new 
Abstract: Structural Health Monitoring (SHM) is increasingly used in civil engineering. One of its main purposes is to detect and assess changes in infrastructure conditions to reduce possible maintenance downtime and increase safety. Ideally, this process should be automated and implemented in real-time. Recent advances in sensor technology facilitate data collection and process automation, resulting in massive data streams. Functional data analysis (FDA) can be used to model and aggregate the data obtained transparently and interpretably. In two real-world case studies of bridges in Germany and Belgium, this paper demonstrates how a function-on-function regression approach, combined with profile monitoring, can be applied to SHM data to adjust sensor/system outputs for environmental-induced variation and detect changes in construction. Specifically, we consider the R package \texttt{funcharts} and discuss some challenges when using this software on real-world SHM data. For instance, we show that pre-smoothing of the data can improve and extend its usability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01262v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philipp Wittenberg, Sven Knoth, Jan Gertheiss</dc:creator>
    </item>
    <item>
      <title>Arbitrary Length Generalization for Addition</title>
      <link>https://arxiv.org/abs/2406.00075</link>
      <description>arXiv:2406.00075v1 Announce Type: cross 
Abstract: This paper introduces a novel training methodology that enables a small Transformer model to generalize the addition of two-digit numbers to numbers with unseen lengths of digits. The proposed approach employs an autoregressive generation technique, processing from right to left, which mimics a common manual method for adding large numbers. To the best of my knowledge, this methodology has not been previously explored in the literature. All results are reproducible, and the corresponding R code is available at: \url{https://github.com/AGPatriota/ALGA-R/}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00075v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexandre Galvao Patriota</dc:creator>
    </item>
    <item>
      <title>A Seamless Phase II/III Design with Dose Optimization for Oncology Drug Development</title>
      <link>https://arxiv.org/abs/2406.00196</link>
      <description>arXiv:2406.00196v1 Announce Type: cross 
Abstract: The US FDA's Project Optimus initiative that emphasizes dose optimization prior to marketing approval represents a pivotal shift in oncology drug development. It has a ripple effect for rethinking what changes may be made to conventional pivotal trial designs to incorporate a dose optimization component. Aligned with this initiative, we propose a novel Seamless Phase II/III Design with Dose Optimization (SDDO framework). The proposed design starts with dose optimization in a randomized setting, leading to an interim analysis focused on optimal dose selection, trial continuation decisions, and sample size re-estimation (SSR). Based on the decision at interim analysis, patient enrollment continues for both the selected dose arm and control arm, and the significance of treatment effects will be determined at final analysis. The SDDO framework offers increased flexibility and cost-efficiency through sample size adjustment, while stringently controlling the Type I error. This proposed design also facilitates both Accelerated Approval (AA) and regular approval in a "one-trial" approach. Extensive simulation studies confirm that our design reliably identifies the optimal dosage and makes preferable decisions with a reduced sample size while retaining statistical power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00196v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuhan Li, Yiding Zhang, Gu Mi, Ji Lin</dc:creator>
    </item>
    <item>
      <title>Model-based Clustering of Zero-Inflated Single-Cell RNA Sequencing Data via the EM Algorithm</title>
      <link>https://arxiv.org/abs/2406.00245</link>
      <description>arXiv:2406.00245v1 Announce Type: cross 
Abstract: Biological cells can be distinguished by their phenotype or at the molecular level, based on their genome, epigenome, and transcriptome. This paper focuses on the transcriptome, which encompasses all the RNA transcripts in a given cell population, indicating the genes being expressed at a given time. We consider single-cell RNA sequencing data and develop a novel model-based clustering method to group cells based on their transcriptome profiles. Our clustering approach takes into account the presence of zero inflation in the data, which can occur due to genuine biological zeros or technological noise. The proposed model for clustering involves a mixture of zero-inflated Poisson or zero-inflated negative binomial distributions, and parameter estimation is carried out using the EM algorithm. We evaluate the performance of our proposed methodology through simulation studies and analyses of publicly available datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00245v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zahra AghahosseinaliShirazi, Pedro A. Rangel, Camila P. E. de Souza</dc:creator>
    </item>
    <item>
      <title>Adaptive Penalized Likelihood method for Markov Chains</title>
      <link>https://arxiv.org/abs/2406.00322</link>
      <description>arXiv:2406.00322v1 Announce Type: cross 
Abstract: Maximum Likelihood Estimation (MLE) and Likelihood Ratio Test (LRT) are widely used methods for estimating the transition probability matrix in Markov chains and identifying significant relationships between transitions, such as equality. However, the estimated transition probability matrix derived from MLE lacks accuracy compared to the real one, and LRT is inefficient in high-dimensional Markov chains. In this study, we extended the adaptive Lasso technique from linear models to Markov chains and proposed a novel model by applying penalized maximum likelihood estimation to optimize the estimation of the transition probability matrix. Meanwhile, we demonstrated that the new model enjoys oracle properties, which means the estimated transition probability matrix has the same performance as the real one when given. Simulations show that our new method behave very well overall in comparison with various competitors. Real data analysis further convince the value of our proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00322v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yining Zhou, Ming Gao, Yiting Chen, Xiaoping Shi</dc:creator>
    </item>
    <item>
      <title>Localized FDG loss in lung cancer lesions</title>
      <link>https://arxiv.org/abs/2406.00382</link>
      <description>arXiv:2406.00382v1 Announce Type: cross 
Abstract: Purpose: Analysis of [18F]-Fluorodeoxyglucose (FDG) kinetics in cancer has been most often limited to the evaluation of the average uptake over relatively large volumes. Nevertheless, tumor lesion almost contains inflammatory infiltrates whose cells are characterized by a significant radioactivity washout due to the hydrolysis of FDG-6P catalyzed by glucose-6P phosphatase. The present study aimed to verify whether voxel-wise compartmental analysis of dynamic imaging can identify tumor regions characterized by tracer washout. Materials &amp; Methods: The study included 11 patients with lung cancer submitted to PET/CT imaging for staging purposes. Tumor was defined by drawing a volume of interest loosely surrounding the lesion and considering all inside voxels with standardized uptake value (SUV) &gt;40% of the maximum. After 20 minutes dynamic imaging centered on the heart, eight whole body scans were repeated. Six parametric maps were progressively generated by computing six regression lines that considered all eight frames, the last seven ones, and so on, up to the last three. Results: Progressively delaying the starting point of regression line computation identified a progressive increase in the prevalence of voxels with a negative slope. Conclusions: The voxel-wise parametric maps provided by compartmental analysis permits to identify a measurable volume characterized by radioactivity washout. The spatial localization of this pattern is compatible with the recognized preferential site of inflammatory infiltrates populating the tumor stroma and might improve the power of FDG imaging in monitoring the effectiveness of treatments aimed to empower the host immune response against the cancer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00382v1</guid>
      <category>q-bio.TO</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Davide Parodi, Edoardo Dighero, Giorgia Biddau, Francesca D'Amico, Matteo Bauckneht, Cecilia Marini, Sara Garbarino, Cristina Campi, Michele Piana, Gianmario Sambuceti</dc:creator>
    </item>
    <item>
      <title>The Firefighter Algorithm: A Hybrid Metaheuristic for Optimization Problems</title>
      <link>https://arxiv.org/abs/2406.00528</link>
      <description>arXiv:2406.00528v1 Announce Type: cross 
Abstract: This paper presents the Firefighter Optimization (FFO) algorithm as a new hybrid metaheuristic for optimization problems. This algorithm stems inspiration from the collaborative strategies often deployed by firefighters in firefighting activities. To evaluate the performance of FFO, extensive experiments were conducted, wherein the FFO was examined against 13 commonly used optimization algorithms, namely, the Ant Colony Optimization (ACO), Bat Algorithm (BA), Biogeography-Based Optimization (BBO), Flower Pollination Algorithm (FPA), Genetic Algorithm (GA), Grey Wolf Optimizer (GWO), Harmony Search (HS), Particle Swarm Optimization (PSO), Simulated Annealing (SA), Tabu Search (TS), and Whale Optimization Algorithm (WOA), and across 24 benchmark functions of various dimensions and complexities. The results demonstrate that FFO achieves comparative performance and, in some scenarios, outperforms commonly adopted optimization algorithms in terms of the obtained fitness, time taken for exaction, and research space covered per unit of time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00528v1</guid>
      <category>cs.NE</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>M. Z. Naser, A. Z. Naser</dc:creator>
    </item>
    <item>
      <title>Discovering an interpretable mathematical expression for a full wind-turbine wake with artificial intelligence enhanced symbolic regression</title>
      <link>https://arxiv.org/abs/2406.00695</link>
      <description>arXiv:2406.00695v1 Announce Type: cross 
Abstract: The rapid expansion of wind power worldwide underscores the critical significance of engineering-focused analytical wake models in both the design and operation of wind farms. These theoretically-derived ana lytical wake models have limited predictive capabilities, particularly in the near-wake region close to the turbine rotor, due to assumptions that do not hold. Knowledge discovery methods can bridge these gaps by extracting insights, adjusting for theoretical assumptions, and developing accurate models for physical processes. In this study, we introduce a genetic symbolic regression (SR) algorithm to discover an interpretable mathematical expression for the mean velocity deficit throughout the wake, a previously unavailable insight. By incorporating a double Gaussian distribution into the SR algorithm as domain knowledge and designing a hierarchical equation structure, the search space is reduced, thus efficiently finding a concise, physically informed, and robust wake model. The proposed mathematical expression (equation) can predict the wake velocity deficit at any location in the full-wake region with high precision and stability. The model's effectiveness and practicality are validated through experimental data and high-fidelity numerical simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00695v1</guid>
      <category>physics.flu-dyn</category>
      <category>cs.LG</category>
      <category>cs.SC</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ding Wang, Yuntian Chen, Shiyi Chen</dc:creator>
    </item>
    <item>
      <title>High-energy Neutrino Source Cross-correlations with Nearest Neighbor Distributions</title>
      <link>https://arxiv.org/abs/2406.00796</link>
      <description>arXiv:2406.00796v1 Announce Type: cross 
Abstract: The astrophysical origins of the majority of the IceCube neutrinos remain unknown. Effectively characterizing the spatial distribution of the neutrino samples and associating the events with astrophysical source catalogs can be challenging given the large atmospheric neutrino background and underlying non-Gaussian spatial features in the neutrino and source samples. In this paper, we investigate a framework for identifying and statistically evaluating the cross-correlations between IceCube data and an astrophysical source catalog based on the $k$-Nearest Neighbor Cumulative Distribution Functions ($k$NN-CDFs). We propose a maximum likelihood estimation procedure for inferring the true proportions of astrophysical neutrinos in the point-source data. We conduct a statistical power analysis of an associated likelihood ratio test with estimations of its sensitivity and discovery potential with synthetic neutrino data samples and a WISE-2MASS galaxy sample. We apply the method to IceCube's public ten-year point-source data and find no statistically significant evidence for spatial cross-correlations with the selected galaxy sample. We discuss possible extensions to the current method and explore the method's potential to identify the cross-correlation signals in data sets with different sample sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00796v1</guid>
      <category>astro-ph.HE</category>
      <category>astro-ph.CO</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhuoyang Zhou, Jessi Cisewski-Kehe, Ke Fang, Arka Banerjee</dc:creator>
    </item>
    <item>
      <title>animal2vec and MeerKAT: A self-supervised transformer for rare-event raw audio input and a large-scale reference dataset for bioacoustics</title>
      <link>https://arxiv.org/abs/2406.01253</link>
      <description>arXiv:2406.01253v1 Announce Type: cross 
Abstract: Bioacoustic research provides invaluable insights into the behavior, ecology, and conservation of animals. Most bioacoustic datasets consist of long recordings where events of interest, such as vocalizations, are exceedingly rare. Analyzing these datasets poses a monumental challenge to researchers, where deep learning techniques have emerged as a standard method. Their adaptation remains challenging, focusing on models conceived for computer vision, where the audio waveforms are engineered into spectrographic representations for training and inference. We improve the current state of deep learning in bioacoustics in two ways: First, we present the animal2vec framework: a fully interpretable transformer model and self-supervised training scheme tailored for sparse and unbalanced bioacoustic data. Second, we openly publish MeerKAT: Meerkat Kalahari Audio Transcripts, a large-scale dataset containing audio collected via biologgers deployed on free-ranging meerkats with a length of over 1068h, of which 184h have twelve time-resolved vocalization-type classes, each with ms-resolution, making it the largest publicly-available labeled dataset on terrestrial mammals. Further, we benchmark animal2vec against the NIPS4Bplus birdsong dataset. We report new state-of-the-art results on both datasets and evaluate the few-shot capabilities of animal2vec of labeled training data. Finally, we perform ablation studies to highlight the differences between our architecture and a vanilla transformer baseline for human-produced sounds. animal2vec allows researchers to classify massive amounts of sparse bioacoustic data even with little ground truth information available. In addition, the MeerKAT dataset is the first large-scale, millisecond-resolution corpus for benchmarking bioacoustic models in the pretrain/finetune paradigm. We believe this sets the stage for a new reference point for bioacoustics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01253v1</guid>
      <category>cs.SD</category>
      <category>cs.AI</category>
      <category>eess.AS</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julian C. Sch\"afer-Zimmermann, Vlad Demartsev, Baptiste Averly, Kiran Dhanjal-Adams, Mathieu Duteil, Gabriella Gall, Marius Fai{\ss}, Lily Johnson-Ulrich, Dan Stowell, Marta B. Manser, Marie A. Roch, Ariana Strandburg-Peshkin</dc:creator>
    </item>
    <item>
      <title>Convolutional Unscented Kalman Filter for Multi-Object Tracking with Outliers</title>
      <link>https://arxiv.org/abs/2406.01380</link>
      <description>arXiv:2406.01380v1 Announce Type: cross 
Abstract: Multi-object tracking (MOT) is an essential technique for navigation in autonomous driving. In tracking-by-detection systems, biases, false positives, and misses, which are referred to as outliers, are inevitable due to complex traffic scenarios. Recent tracking methods are based on filtering algorithms that overlook these outliers, leading to reduced tracking accuracy or even loss of the objects trajectory. To handle this challenge, we adopt a probabilistic perspective, regarding the generation of outliers as misspecification between the actual distribution of measurement data and the nominal measurement model used for filtering. We further demonstrate that, by designing a convolutional operation, we can mitigate this misspecification. Incorporating this operation into the widely used unscented Kalman filter (UKF) in commonly adopted tracking algorithms, we derive a variant of the UKF that is robust to outliers, called the convolutional UKF (ConvUKF). We show that ConvUKF maintains the Gaussian conjugate property, thus allowing for real-time tracking. We also prove that ConvUKF has a bounded tracking error in the presence of outliers, which implies robust stability. The experimental results on the KITTI and nuScenes datasets show improved accuracy compared to representative baseline algorithms for MOT tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01380v1</guid>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shiqi Liu, Wenhan Cao, Chang Liu, Tianyi Zhang, Shengbo Eben Li</dc:creator>
    </item>
    <item>
      <title>Bayesian compositional regression with flexible microbiome feature aggregation and selection</title>
      <link>https://arxiv.org/abs/2406.01557</link>
      <description>arXiv:2406.01557v1 Announce Type: cross 
Abstract: Ongoing advances in microbiome profiling have allowed unprecedented insights into the molecular activities of microbial communities. This has fueled a strong scientific interest in understanding the critical role the microbiome plays in governing human health, by identifying microbial features associated with clinical outcomes of interest. Several aspects of microbiome data limit the applicability of existing variable selection approaches. In particular, microbiome data are high-dimensional, extremely sparse, and compositional. Importantly, many of the observed features, although categorized as different taxa, may play related functional roles. To address these challenges, we propose a novel compositional regression approach that leverages the data-adaptive clustering and variable selection properties of the spiked Dirichlet process to identify taxa that exhibit similar functional roles. Our proposed method, Bayesian Regression with Agglomerated Compositional Effects using a dirichLET process (BRACElet), enables the identification of a sparse set of features with shared impacts on the outcome, facilitating dimension reduction and model interpretation. We demonstrate that BRACElet outperforms existing approaches for microbiome variable selection through simulation studies and an application elucidating the impact of oral microbiome composition on insulin resistance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01557v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Satabdi Saha, Liangliang Zhang, Kim-Anh Do, Christine B. Peterson</dc:creator>
    </item>
    <item>
      <title>Risk Set Matched Difference-in-Differences for the Analysis of Effect Modification in an Observational Study on the Impact of Gun Violence on Health Outcomes</title>
      <link>https://arxiv.org/abs/2305.04143</link>
      <description>arXiv:2305.04143v5 Announce Type: replace 
Abstract: Gun violence is a major source of injury and death in the United States. However, relatively little is known about the effects of firearm injuries on survivors and their family members and how these effects vary across subpopulations. To study these questions and, more generally, to address a gap in the causal inference literature, we present a framework for the study of effect modification or heterogeneous treatment effects in difference-in-differences designs. We implement a new matching technique, which combines profile matching and risk set matching, to (i) preserve the time alignment of covariates, exposure, and outcomes, avoiding pitfalls of other common approaches for difference-in-differences, and (ii) explicitly control biases due to imbalances in observed covariates in subgroups discovered from the data. Our case study shows significant and persistent effects of nonfatal firearm injuries on several health outcomes for those injured and on the mental health of their family members. Sensitivity analyses reveal that these results are moderately robust to unmeasured confounding bias. Finally, while the effects for those injured vary largely by the severity of the injury and its documented intent, for families, effects are strongest for those whose relative's injury is documented as resulting from an assault, self-harm, or law enforcement intervention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.04143v5</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eric R. Cohn, Zirui Song, Jose R. Zubizarreta</dc:creator>
    </item>
    <item>
      <title>Not feeling the buzz: Correction study of mispricing and inefficiency in online sportsbooks</title>
      <link>https://arxiv.org/abs/2306.01740</link>
      <description>arXiv:2306.01740v3 Announce Type: replace 
Abstract: We present a replication and correction of a recent article (Ramirez, P., Reade, J.J., Singleton, C., Betting on a buzz: Mispricing and inefficiency in online sportsbooks, International Journal of Forecasting, 39:3, 2023, pp. 1413-1423, doi: 10.1016/j.ijforecast.2022.07.011). RRS measure profile page views on Wikipedia to generate a "buzz factor" metric for tennis players and show that it can be used to form a profitable gambling strategy by predicting bookmaker mispricing. Here, we use the same dataset as RRS to reproduce their results exactly, thus confirming the robustness of their mispricing claim. However, we discover that the published betting results are significantly affected by a single bet (the "Hercog" bet), which returns substantial outlier profits based on erroneously long odds. When this data quality issue is resolved, the majority of reported profits disappear and only one strategy, which bets on "competitive" matches, remains significantly profitable in the original out-of-sample period. While one profitable strategy offers weaker support than the original study, it still provides an indication that market inefficiencies may exist, as originally claimed by RRS. As an extension, we continue backtesting after 2020 on a cleaned dataset. Results show that (a) the "competitive" strategy generates no further profits, potentially suggesting markets have become more efficient, and (b) model coefficients estimated over this more recent period are no longer reliable predictors of bookmaker mispricing. We present this work as a case study demonstrating the importance of replication studies in sports forecasting, and the necessity to clean data. We open-source release comprehensive datasets and code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.01740v3</guid>
      <category>stat.AP</category>
      <category>cs.CE</category>
      <category>q-fin.GN</category>
      <category>q-fin.ST</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lawrence Clegg, John Cartlidge</dc:creator>
    </item>
    <item>
      <title>Individual claims reserving using the Aalen--Johansen estimator</title>
      <link>https://arxiv.org/abs/2311.07384</link>
      <description>arXiv:2311.07384v3 Announce Type: replace 
Abstract: We propose an individual claims reserving model based on the conditional Aalen-Johansen estimator, as developed in Bladt and Furrer (2023b). In our approach, we formulate a multi-state problem, where the underlying variable is the individual claim size, rather than time. The states in this model represent development periods, and we estimate the cumulative density function of individual claim sizes using the conditional Aalen-Johansen method as transition probabilities to an absorbing state. Our methodology reinterprets the concept of multi-state models and offers a strategy for modeling the complete curve of individual claim sizes. To illustrate our approach, we apply our model to both simulated and real datasets. Having access to the entire dataset enables us to support the use of our approach by comparing the predicted total final cost with the actual amount, as well as evaluating it in terms of the continuously ranked probability score.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.07384v3</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Bladt, Gabriele Pittarello</dc:creator>
    </item>
    <item>
      <title>Ensemble distributional forecasting for insurance loss reserving</title>
      <link>https://arxiv.org/abs/2206.08541</link>
      <description>arXiv:2206.08541v5 Announce Type: replace-cross 
Abstract: Loss reserving generally focuses on identifying a single model that can generate superior predictive performance. However, different loss reserving models specialise in capturing different aspects of loss data. This is recognised in practice in the sense that results from different models are often considered, and sometimes combined. For instance, actuaries may take a weighted average of the prediction outcomes from various loss reserving models, often based on subjective assessments.
  In this paper, we propose a systematic framework to objectively combine (i.e. ensemble) multiple _stochastic_ loss reserving models such that the strengths offered by different models can be utilised effectively. Our framework contains two main innovations compared to existing literature and practice. Firstly, our criteria model combination considers the full distributional properties of the ensemble and not just the central estimate - which is of particular importance in the reserving context. Secondly, our framework is that it is tailored for the features inherent to reserving data. These include, for instance, accident, development, calendar, and claim maturity effects. Crucially, the relative importance and scarcity of data across accident periods renders the problem distinct from the traditional ensembling techniques in statistical learning.
  Our framework is illustrated with a complex synthetic dataset. In the results, the optimised ensemble outperforms both (i) traditional model selection strategies, and (ii) an equally weighted ensemble. In particular, the improvement occurs not only with central estimates but also relevant quantiles, such as the 75th percentile of reserves (typically of interest to both insurers and regulators). The framework developed in this paper can be implemented thanks to an R package, `ADLP`, which is available from CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.08541v5</guid>
      <category>stat.ME</category>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Avanzi, Yanfeng Li, Bernard Wong, Alan Xian</dc:creator>
    </item>
    <item>
      <title>Examining properness in the external validation of survival models with squared and logarithmic losses</title>
      <link>https://arxiv.org/abs/2212.05260</link>
      <description>arXiv:2212.05260v2 Announce Type: replace-cross 
Abstract: Scoring rules promote rational and honest decision-making, which is becoming increasingly important for automated procedures in `auto-ML'. In this paper we survey common squared and logarithmic scoring rules for survival analysis and determine which losses are proper and improper. We prove that commonly utilised squared and logarithmic scoring rules that are claimed to be proper are in fact improper, such as the Integrated Survival Brier Score (ISBS). We further prove that under a strict set of assumptions a class of scoring rules is strictly proper for, what we term, `approximate' survival losses. Despite the difference in properness, experiments in simulated and real-world datasets show there is no major difference between improper and proper versions of the widely-used ISBS, ensuring that we can reasonably trust previous experiments utilizing the original score for evaluation purposes. We still advocate for the use of proper scoring rules, as even minor differences between losses can have important implications in automated processes such as model tuning. We hope our findings encourage further research into the properties of survival measures so that robust and honest evaluation of survival models can be achieved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.05260v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Raphael Sonabend, John Zobolas, Philipp Kopper, Lukas Burk, Andreas Bender</dc:creator>
    </item>
    <item>
      <title>Composite Dyadic Models for Spatio-Temporal Data</title>
      <link>https://arxiv.org/abs/2311.01341</link>
      <description>arXiv:2311.01341v3 Announce Type: replace-cross 
Abstract: Mechanistic statistical models are commonly used to study the flow of biological processes. For example, in landscape genetics, the aim is to infer spatial mechanisms that govern gene flow in populations. Existing statistical approaches in landscape genetics do not account for temporal dependence in the data and may be computationally prohibitive. We infer mechanisms with a Bayesian hierarchical dyadic model that scales well with large data sets and that accounts for spatial and temporal dependence. We construct a fully-connected network comprising spatio-temporal data for the dyadic model and use normalized composite likelihoods to account for the dependence structure in space and time. We develop a dyadic model to account for physical mechanisms commonly found in physical-statistical models and apply our methods to ancient human DNA data to infer the mechanisms that affected human movement in Bronze Age Europe.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.01341v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael R Schwob, Mevin B Hooten, Vagheesh Narasimhan</dc:creator>
    </item>
    <item>
      <title>A Stochastic-Geometrical Framework for Object Pose Estimation based on Mixture Models Avoiding the Correspondence Problem</title>
      <link>https://arxiv.org/abs/2311.18107</link>
      <description>arXiv:2311.18107v5 Announce Type: replace-cross 
Abstract: Background: Pose estimation of rigid objects is a practical challenge in optical metrology and computer vision. This paper presents a novel stochastic-geometrical modeling framework for object pose estimation based on observing multiple feature points.
  Methods: This framework utilizes mixture models for feature point densities in object space and for interpreting real measurements. Advantages are the avoidance to resolve individual feature correspondences and to incorporate correct stochastic dependencies in multi-view applications. First, the general modeling framework is presented, second, a general algorithm for pose estimation is derived, and third, two example models (camera and lateration setup) are presented.
  Results: Numerical experiments show the effectiveness of this modeling and general algorithm by presenting four simulation scenarios for three observation systems, including the dependence on measurement resolution, object deformations and measurement noise. Probabilistic modeling utilizing mixture models shows the potential for accurate and robust pose estimations while avoiding the correspondence problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.18107v5</guid>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wolfgang Hoegele</dc:creator>
    </item>
    <item>
      <title>Approaches to biological species delimitation based on genetic and spatial dissimilarity</title>
      <link>https://arxiv.org/abs/2401.12126</link>
      <description>arXiv:2401.12126v4 Announce Type: replace-cross 
Abstract: The delimitation of biological species, i.e., deciding which individuals belong to the same species and whether and how many different species are represented in a data set, is key to the conservation of biodiversity. Much existing work uses only genetic data for species delimitation, often employing some kind of cluster analysis. This can be misleading, because geographically distant groups of individuals can be genetically quite different even if they belong to the same species. We investigate the problem of testing whether two potentially separated groups of individuals can belong to a single species or not based on genetic and spatial data. Existing methods such as the partial Mantel test and jackknife-based distance-distance regression are considered. New approaches, i.e., an adaptation of a mixed effects model, a bootstrap approach, and a jackknife version of partial Mantel, are proposed. All these methods address the issue that distance data violate the independence assumption for standard inference regarding correlation and regression; a standard linear regression is also considered. The approaches are compared on simulated meta-populations generated with SLiM and GSpace - two software packages that can simulate spatially-explicit genetic data at an individual level. Simulations show that the new jackknife version of the partial Mantel test provides a good compromise between power and respecting the nominal type I error rate. Mixed-effects models have larger power than jackknife-based methods, but tend to display type I error rates slightly above the significance level. An application on brassy ringlets concludes the paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.12126v4</guid>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriele d'Angella, Christian Hennig</dc:creator>
    </item>
    <item>
      <title>The Multi-Range Theory of Translation Quality Measurement: MQM scoring models and Statistical Quality Control</title>
      <link>https://arxiv.org/abs/2405.16969</link>
      <description>arXiv:2405.16969v3 Announce Type: replace-cross 
Abstract: The year 2024 marks the 10th anniversary of the Multidimensional Quality Metrics (MQM) framework for analytic translation quality evaluation. The MQM error typology has been widely used by practitioners in the translation and localization industry and has served as the basis for many derivative projects. The annual Conference on Machine Translation (WMT) shared tasks on both human and automatic translation quality evaluations used the MQM error typology.
  The metric stands on two pillars: error typology and the scoring model. The scoring model calculates the quality score from annotation data, detailing how to convert error type and severity counts into numeric scores to determine if the content meets specifications. Previously, only the raw scoring model had been published. This April, the MQM Council published the Linear Calibrated Scoring Model, officially presented herein, along with the Non-Linear Scoring Model, which had not been published before.
  This paper details the latest MQM developments and presents a universal approach to translation quality measurement across three sample size ranges. It also explains why Statistical Quality Control should be used for very small sample sizes, starting from a single sentence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16969v3</guid>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Arle Lommel, Serge Gladkoff, Alan Melby, Sue Ellen Wright, Ingemar Strandvik, Katerina Gasova, Angelika Vaasa, Andy Benzo, Romina Marazzato Sparano, Monica Foresi, Johani Innis, Lifeng Han, Goran Nenadic</dc:creator>
    </item>
    <item>
      <title>MC-GTA: Metric-Constrained Model-Based Clustering using Goodness-of-fit Tests with Autocorrelations</title>
      <link>https://arxiv.org/abs/2405.18395</link>
      <description>arXiv:2405.18395v2 Announce Type: replace-cross 
Abstract: A wide range of (multivariate) temporal (1D) and spatial (2D) data analysis tasks, such as grouping vehicle sensor trajectories, can be formulated as clustering with given metric constraints. Existing metric-constrained clustering algorithms overlook the rich correlation between feature similarity and metric distance, i.e., metric autocorrelation. The model-based variations of these clustering algorithms (e.g. TICC and STICC) achieve SOTA performance, yet suffer from computational instability and complexity by using a metric-constrained Expectation-Maximization procedure. In order to address these two problems, we propose a novel clustering algorithm, MC-GTA (Model-based Clustering via Goodness-of-fit Tests with Autocorrelations). Its objective is only composed of pairwise weighted sums of feature similarity terms (square Wasserstein-2 distance) and metric autocorrelation terms (a novel multivariate generalization of classic semivariogram). We show that MC-GTA is effectively minimizing the total hinge loss for intra-cluster observation pairs not passing goodness-of-fit tests, i.e., statistically not originating from the same distribution. Experiments on 1D/2D synthetic and real-world datasets demonstrate that MC-GTA successfully incorporates metric autocorrelation. It outperforms strong baselines by large margins (up to 14.3% in ARI and 32.1% in NMI) with faster and stabler optimization (&gt;10x speedup).</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18395v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhangyu Wang, Gengchen Mai, Krzysztof Janowicz, Ni Lao</dc:creator>
    </item>
  </channel>
</rss>
