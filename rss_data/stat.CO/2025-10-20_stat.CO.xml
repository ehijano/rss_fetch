<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 21 Oct 2025 02:42:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Standardization for improved Spatio-Temporal Image Fusion</title>
      <link>https://arxiv.org/abs/2510.15589</link>
      <description>arXiv:2510.15589v1 Announce Type: cross 
Abstract: Spatio-Temporal Image Fusion (STIF) methods usually require sets of images with matching spatial and spectral resolutions captured by different sensors. To facilitate the application of STIF methods, we propose and compare two different standardization approaches. The first method is based on traditional upscaling of the fine-resolution images. The second method is a sharpening approach called Anomaly Based Satellite Image Standardization (ABSIS) that blends the overall features found in the fine-resolution image series with the distinctive attributes of a specific coarse-resolution image to produce images that more closely resemble the outcome of aggregating the fine-resolution images. Both methods produce a significant increase in accuracy of the Unpaired Spatio Temporal Fusion of Image Patches (USTFIP) STIF method, with the sharpening approach increasing the spectral and spatial accuracies of the fused images by up to 49.46\% and 78.40\%, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15589v1</guid>
      <category>cs.CV</category>
      <category>stat.CO</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harkaitz Goyena, Peter M. Atkinson, Unai P\'erez-Goya, M. Dolores Ugarte</dc:creator>
    </item>
    <item>
      <title>Adaptive Influence Diagnostics in High-Dimensional Regression</title>
      <link>https://arxiv.org/abs/2510.15618</link>
      <description>arXiv:2510.15618v1 Announce Type: cross 
Abstract: An adaptive Cook's distance (ACD) for diagnosing influential observations in high-dimensional single-index models with multicollinearity and outlier contamination is proposed. ACD is a model-free technique built on sparse local linear gradients to temper leverage effects. In simulations spanning low- and high-dimensional design settings with strong correlation, ACD based on LASSO (ACD-LASSO) and SCAD (ACD-SCAD) penalties reduced masking and swamping relative to classical Cook's distance and local influence as well as the DF-Model and Case-Weight adjusted solution for LASSO. Trimming points flagged by ACD stabilizes variable selection while preserving core signals. Applications to two datasets--the 1960 US cities pollution study and a high-dimensional riboflavin genomics experiment show consistent gains in selection stability and interpretability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15618v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdul-Nasah Soale, Adewale Lukman</dc:creator>
    </item>
    <item>
      <title>lqmix: an R package for longitudinal data analysis via linear quantile mixtures</title>
      <link>https://arxiv.org/abs/2302.11363</link>
      <description>arXiv:2302.11363v4 Announce Type: replace 
Abstract: The analysis of longitudinal data gives the chance to observe how unit behaviors change over time, but it also poses a series of issues. These have been the focus of an extensive literature in the context of linear and generalized linear regression moving also, in the last ten years or so, to the context of linear quantile regression for continuous responses. In this paper, we present \texttt{lqmix}, a novel \texttt{R} package that assists in estimating a class of linear quantile regression models for longitudinal data, in the presence of time-constant and/or time-varying, unit-specific, random coefficients, with unspecified distribution. Model parameters are estimated in a maximum likelihood framework via an extended EM algorithm, while parameters' standard errors are derived via a block-bootstrap procedure. The analysis of a benchmark dataset is used to give details on the package functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.11363v4</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Alf\'o, Maria Francesca Marino, Maria Giovanna Ranalli, Nicola Salvati</dc:creator>
    </item>
    <item>
      <title>Linear-Cost Vecchia Approximation of Multivariate Normal Probabilities</title>
      <link>https://arxiv.org/abs/2311.09426</link>
      <description>arXiv:2311.09426v4 Announce Type: replace 
Abstract: Multivariate normal (MVN) probabilities arise in myriad applications, but they are analytically intractable and need to be evaluated via Monte-Carlo-based numerical integration. For the state-of-the-art minimax exponential tilting (MET) method, we show that the complexity of each of its components can be greatly reduced through an integrand parameterization that utilizes the sparse inverse Cholesky factor produced by the Vecchia approximation, whose approximation error is often negligible relative to the Monte-Carlo error. Based on this idea, we derive algorithms that can estimate MVN probabilities and sample from truncated MVN distributions in linear time (and that are easily parallelizable) at the same convergence or acceptance rate as MET, whose complexity is cubic in the dimension of the MVN probability. We showcase the advantages of our methods relative to existing approaches using several simulated examples. We also analyze a groundwater-contamination dataset with over twenty thousand censored measurements to demonstrate the scalability of our method for partially censored Gaussian-process models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.09426v4</guid>
      <category>stat.CO</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jian Cao, Matthias Katzfuss</dc:creator>
    </item>
    <item>
      <title>A Computable Measure of Suboptimality for Entropy-Regularised Variational Objectives</title>
      <link>https://arxiv.org/abs/2509.10393</link>
      <description>arXiv:2509.10393v2 Announce Type: replace 
Abstract: Several emerging post-Bayesian methods target a probability distribution for which an entropy-regularised variational objective is minimised. This increased flexibility introduces a computational challenge, as one loses access to an explicit unnormalised density for the target. To mitigate this difficulty, we introduce a novel measure of suboptimality called 'gradient discrepancy', and in particular a 'kernel' gradient discrepancy (KGD) that can be explicitly computed. In the standard Bayesian context, KGD coincides with the kernel Stein discrepancy (KSD), and we obtain a novel characterisation of KSD as measuring the size of a variational gradient. Outside this familiar setting, KGD enables novel sampling algorithms to be developed and compared, even when unnormalised densities cannot be obtained. To illustrate this point several novel algorithms are proposed and studied, including a natural generalisation of Stein variational gradient descent, with applications to mean-field neural networks and predictively oriented posteriors presented. On the theoretical side, our principal contribution is to establish sufficient conditions for desirable properties of KGD, such as continuity and convergence control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10393v2</guid>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cl\'ementine Chazal, Heishiro Kanagawa, Zheyang Shen, Anna Korba, Chris. J. Oates</dc:creator>
    </item>
    <item>
      <title>Bayesian Signal Matching for Transfer Learning in ERP-Based Brain Computer Interface</title>
      <link>https://arxiv.org/abs/2401.07111</link>
      <description>arXiv:2401.07111v3 Announce Type: replace-cross 
Abstract: An Event-Related Potential (ERP)-based Brain-Computer Interface (BCI) Speller System assists people with disabilities to communicate by decoding electroencephalogram (EEG) signals. A P300-ERP embedded in EEG signals arises in response to a rare, but relevant event (target) among a series of irrelevant events (non-target). Different machine learning methods have constructed binary classifiers to detect target events, known as calibration. The existing calibration strategy uses data from participants themselves with lengthy training time. Participants feel bored and distracted, which causes biased P300 estimation and decreased prediction accuracy. To resolve this issue, we propose a Bayesian signal matching (BSM) framework to calibrate EEG signals from a new participant using data from source participants. BSM specifies the joint distribution of stimulus-specific EEG signals among source participants via a Bayesian hierarchical mixture model. We apply the inference strategy. If source and new participants are similar, they share the same set of model parameters; otherwise, they keep their own sets of model parameters; we predict on the testing data using parameters of the baseline cluster directly. Our hierarchical framework can be generalized to other base classifiers with parametric forms. We demonstrate the advantages of BSM using simulations and focus on the real data analysis among participants with neuro-degenerative diseases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.07111v3</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tianwen Ma, Jane E. Huggins, Jian Kang</dc:creator>
    </item>
    <item>
      <title>Uncertainty Quantification for Prior-Data Fitted Networks using Martingale Posteriors</title>
      <link>https://arxiv.org/abs/2505.11325</link>
      <description>arXiv:2505.11325v2 Announce Type: replace-cross 
Abstract: Prior-data fitted networks (PFNs) have emerged as promising foundation models for prediction from tabular data sets, achieving state-of-the-art performance on small to moderate data sizes without tuning. While PFNs are motivated by Bayesian ideas, they do not provide any uncertainty quantification for predictive means, quantiles, or similar quantities. We propose a principled and efficient sampling procedure to construct Bayesian posteriors for such estimates based on Martingale posteriors, and prove its convergence. Several simulated and real-world data examples showcase the uncertainty quantification of our method in inference applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11325v2</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Nagler, David R\"ugamer</dc:creator>
    </item>
  </channel>
</rss>
