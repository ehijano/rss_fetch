<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PL</link>
    <description>cs.PL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 02 Dec 2025 05:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Word Sampler for Well-Typed Functions</title>
      <link>https://arxiv.org/abs/2512.01036</link>
      <description>arXiv:2512.01036v1 Announce Type: new 
Abstract: We describe an exact sampler for a simply-typed, first-order functional programming language. Given an acyclic finite automaton, $\alpha_{\varnothing}$, it samples a random function uniformly without replacement from well-typed functions in $\mathcal{L}(\alpha_{\varnothing})$. This is achieved via a fixed-parameter tractable reduction from a syntax-directed type system to a context-free grammar, preserving type soundness and completeness w.r.t. $\mathcal{L}(\alpha_{\varnothing})$, while retaining the robust metatheory of formal languages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01036v1</guid>
      <category>cs.PL</category>
      <category>cs.FL</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Breandan Considine</dc:creator>
    </item>
    <item>
      <title>Generating Verifiable CoT from Execution-Traces</title>
      <link>https://arxiv.org/abs/2512.00127</link>
      <description>arXiv:2512.00127v1 Announce Type: cross 
Abstract: Teaching language models to reason about code execution remains a fundamental challenge. While Chain-of-Thought (CoT) prompting has shown promise, current synthetic training data suffers from a critical weakness: the reasoning steps are often plausible-sounding explanations generated by teacher models, not verifiable accounts of what the code actually does. This creates a troubling failure mode where models learn to mimic superficially convincing but logically flawed reasoning patterns.
  We address this by grounding CoT generation directly in program execution traces. Our pipeline instruments code to capture its dynamic behavior, then narrates these verified execution traces into natural language rationales that are correct by construction. This execution-grounded approach ensures every reasoning step reflects what the program genuinely computes, eliminating logical hallucinations at the source. We evaluate our method on code reasoning tasks (forward reasoning on CruxEval and LiveCodeBench-Exec, backward reasoning on CruxEval-Input), as well as code generation and explanation tasks from HumanEval. Models trained on our bi-directional trace-grounded data achieve substantial improvements, with gains of up to 30 points on output prediction and 28 points on input prediction over base models, alongside improved explanation and code generation, demonstrating that verifiable reasoning fundamentally enhances model capabilities. https://github.ibm.com/IBM-Research-AI/Verified-Code-CoT</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00127v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.PL</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shailja Thakur, Vaibhav Saxena, Rohan Kulkarni, Shivdeep Singh, Parameswaran Selvam, Hima Patel, Hiroshi Kanayama</dc:creator>
    </item>
    <item>
      <title>Faster Verified Explanations for Neural Networks</title>
      <link>https://arxiv.org/abs/2512.00164</link>
      <description>arXiv:2512.00164v1 Announce Type: cross 
Abstract: Verified explanations are a theoretically-principled way to explain the decisions taken by neural networks, which are otherwise black-box in nature. However, these techniques face significant scalability challenges, as they require multiple calls to neural network verifiers, each of them with an exponential worst-case complexity. We present FaVeX, a novel algorithm to compute verified explanations. FaVeX accelerates the computation by dynamically combining batch and sequential processing of input features, and by reusing information from previous queries, both when proving invariances with respect to certain input features, and when searching for feature assignments altering the prediction. Furthermore, we present a novel and hierarchical definition of verified explanations, termed verifier-optimal robust explanations, that explicitly factors the incompleteness of network verifiers within the explanation. Our comprehensive experimental evaluation demonstrates the superior scalability of both FaVeX, and of verifier-optimal robust explanations, which together can produce meaningful formal explanation on networks with hundreds of thousands of non-linear activations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00164v1</guid>
      <category>cs.LG</category>
      <category>cs.PL</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alessandro De Palma, Greta Dolcetti, Caterina Urban</dc:creator>
    </item>
    <item>
      <title>Counting and Sampling Traces in Regular Languages</title>
      <link>https://arxiv.org/abs/2512.00314</link>
      <description>arXiv:2512.00314v1 Announce Type: cross 
Abstract: In this work, we study the problems of counting and sampling Mazurkiewicz traces that a regular language touches. Fix an alphabet $\Sigma$ and an independence relation $\mathbb{I} \subseteq \Sigma \times \Sigma$. The input consists of a regular language $L \subseteq \Sigma^*$, given by a finite automaton with $m$ states, and a natural number $n$ (in unary). For the counting problem, the goal is to compute the number of Mazurkiewicz traces (induced by $\mathbb{I}$) that intersect the $n^\text{th}$ slice $L_n = L \cap \Sigma^n$, i.e., traces that admit at least one linearization in $L_n$. For the sampling problem, the goal is to output a trace drawn from a distribution that is approximately uniform over all such traces. These tasks are motivated by bounded model checking with partial-order reduction, where an \emph{a priori} estimate of the reduced state space is valuable, and by testing methods for concurrent programs that use partial-order-aware random exploration.
  We first show that the counting problem is #P-hard even when $L$ is accepted by a deterministic automaton, in sharp contrast to counting words of a DFA, which is polynomial-time solvable. We then prove that the problem lies in #P for both NFAs and DFAs, irrespective of whether $L$ is trace-closed. Our main algorithmic contributions are a \emph{fully polynomial-time randomized approximation scheme} (FPRAS) that, with high probability, approximates the desired count within a prescribed accuracy, and a \emph{fully polynomial-time almost uniform sampler} (FPAUS) that generates traces whose distribution is provably close to uniform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00314v1</guid>
      <category>cs.FL</category>
      <category>cs.CC</category>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3776723</arxiv:DOI>
      <dc:creator>Alexis de Colnet, Kuldeep S. Meel, Umang Mathur</dc:creator>
    </item>
    <item>
      <title>Partial Cross-Compilation and Mixed Execution for Accelerating Dynamic Binary Translation</title>
      <link>https://arxiv.org/abs/2512.00487</link>
      <description>arXiv:2512.00487v1 Announce Type: cross 
Abstract: With the growing diversity of instruction set architectures (ISAs), cross-ISA program execution has become common. Dynamic binary translation (DBT) is the main solution but suffers from poor performance. Cross-compilation avoids emulation costs but is constrained by an "all-or-nothing" model-programs are either fully cross-compiled or entirely emulated. Complete cross-compilation is often unfeasible due to ISA-specific code or missing dependencies, leaving programs with high emulation overhead.
  We propose a hybrid execution system that combines compilation and emulation, featuring a selective function offloading mechanism. This mechanism establishes cross-environment calling channels, offloading eligible functions to the host for native execution to reduce DBT overhead. Key optimizations address offloading costs, enabling efficient hybrid operation. Built on LLVM and QEMU, the system works automatically for both applications and libraries. Evaluations show it achieves up to 13x speedups over existing DBT, with strong practical value.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00487v1</guid>
      <category>cs.AR</category>
      <category>cs.PL</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuhao Gu, Zhongchun Zheng, Nong Xiao, Yutong Lu, Xianwei Zhang</dc:creator>
    </item>
    <item>
      <title>Morphling: Fast, Fused, and Flexible GNN Training at Scale</title>
      <link>https://arxiv.org/abs/2512.01678</link>
      <description>arXiv:2512.01678v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) present a fundamental hardware challenge by fusing irregular, memory-bound graph traversals with regular, compute-intensive dense matrix operations. While frameworks such as PyTorch Geometric (PyG) and Deep Graph Library (DGL) prioritize high-level usability, they fail to address these divergent execution characteristics. As a result, they rely on generic kernels that suffer from poor cache locality, excessive memory movement, and substantial intermediate allocations. To address these limitations, we present Morphling, a domain-specific code synthesizer designed to bridge this gap. Morphling compiles high-level GNN specifications into portable, backend-specialized implementations targeting OpenMP, CUDA, and MPI. It achieves this by instantiating a library of optimized, architecture-aware primitives tailored to each execution environment. Morphling also incorporates a runtime sparsity-aware execution engine that dynamically selects dense or sparse execution paths using input feature statistics, reducing unnecessary computation on zero-valued entries. We evaluate Morphling on eleven real-world datasets spanning diverse graph structures, feature dimensionalities, and sparsity regimes. The results show that Morphling improves per-epoch training throughput by an average of 20X on CPUs and 19X on GPUs over PyG and DGL, with peak speedups reaching 66X. Morphling's memory-efficient layouts further reduce peak memory consumption by up to 15X, enabling large-scale GNN training on commodity hardware. These findings demonstrate that specialized, architecture-aware code synthesis provides an effective and scalable path toward high-performance GNN execution across diverse parallel and distributed platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01678v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.PL</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator> Anubhab, Rupesh Nasre</dc:creator>
    </item>
    <item>
      <title>The Complexity of Testing Message-Passing Concurrency</title>
      <link>https://arxiv.org/abs/2505.05162</link>
      <description>arXiv:2505.05162v2 Announce Type: replace 
Abstract: A key computational question underpinning the automated testing and verification of concurrent programs is the consistency question - given a partial execution history, can it be completed in a consistent manner? Due to its importance, consistency testing has been studied extensively for memory models, as well as for database isolation levels. A common theme in all these settings is the use of shared-memory as the primal mode of interthread communication. On the other hand, modern programming languages, such as Go, Rust and Kotlin, advocate a paradigm shift towards channel-based (i.e., message-passing) communication. However, the consistency question for channel-based concurrency is currently poorly understood.
  In this paper we lift the study of fundamental consistency problems to channels, taking into account various input parameters, such as the number of threads executing, the number of channels, and the channel capacities. We draw a rich complexity landscape, including upper bounds that become polynomial when certain input parameters are fixed, as well as hardness lower bounds. Our upper bounds are based on algorithms that can drive the verification of channel consistency in automated verification tools. Our lower bounds characterize minimal input parameters that are sufficient for hardness to arise, and thus shed light on the intricacies of testing channel-based concurrency. In combination, our upper and lower bounds characterize the boundary of tractability/intractability of verifying channel consistency, and imply that our algorithms are often (nearly) optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05162v2</guid>
      <category>cs.PL</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zheng Shi, Lasse M{\o}ldrup, Umang Mathur, Andreas Pavlogiannis</dc:creator>
    </item>
    <item>
      <title>Quantum Circuits Are Just a Phase</title>
      <link>https://arxiv.org/abs/2507.11676</link>
      <description>arXiv:2507.11676v2 Announce Type: replace 
Abstract: Quantum programs today are written at a low level of abstraction - quantum circuits akin to assembly languages - and the unitary parts of even advanced quantum programming languages essentially function as circuit description languages. This state of affairs impedes scalability, clarity, and support for higher-level reasoning. More abstract and expressive quantum programming constructs are needed. To this end, we introduce a simple syntax for generating unitaries from "just a phase"; we combine a (global) phase operation that captures phase shifts with a quantum analogue of the "if let" construct that captures subspace selection via pattern matching. This minimal language lifts the focus from gates to eigendecomposition, conjugation, and controlled unitaries; common building blocks in quantum algorithm design. We demonstrate several aspects of the expressive power of our language in several ways. Firstly, we establish that our representation is universal by deriving a universal quantum gate set. Secondly, we show that important quantum algorithms can be expressed naturally and concisely, including Grover's search algorithm, Hamiltonian simulation, Quantum Fourier Transform, Quantum Signal Processing, and the Quantum Eigenvalue Transformation. Furthermore, we give clean denotational semantics grounded in categorical quantum mechanics. Finally, we implement a prototype compiler that efficiently translates terms of our language to quantum circuits, and prove that it is sound with respect to these semantics. Collectively, these contributions show that this construct offers a principled and practical step toward more abstract and structured quantum programming.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11676v2</guid>
      <category>cs.PL</category>
      <category>cs.LO</category>
      <category>quant-ph</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3776731</arxiv:DOI>
      <dc:creator>Chris Heunen, Louis Lemonnier, Christopher McNally, Alex Rice</dc:creator>
    </item>
    <item>
      <title>The Ghosts of Empires: Extracting Modularity from Interleaving-Based Proofs (Extended Version)</title>
      <link>https://arxiv.org/abs/2511.20369</link>
      <description>arXiv:2511.20369v2 Announce Type: replace 
Abstract: Implementation bugs threaten the soundness of algorithmic software verifiers. Generating correctness certificates for correct programs allows for efficient independent validation of verification results, and thus helps to reveal such bugs. Automatic generation of small, compact correctness proofs for concurrent programs is challenging, as the correctness arguments may depend on the particular interleaving, which can lead to exponential explosion. We present an approach that converts an interleaving-based correctness proof, as generated by many algorithmic verifiers, into a thread-modular correctness proof in the style of Owicki and Gries. We automatically synthesize ghost variables that capture the relevant interleaving information, and abstract away irrelevant details. Our evaluation shows that the approach is efficient in practice and generates compact proofs, compared to a baseline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20369v2</guid>
      <category>cs.PL</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3776684</arxiv:DOI>
      <dc:creator>Frank Sch\"ussele, Matthias Zumkeller, Miriam Lagunes-Rochin, Dominik Klumpp</dc:creator>
    </item>
    <item>
      <title>REASONING COMPILER: LLM-Guided Optimizations for Efficient Model Serving</title>
      <link>https://arxiv.org/abs/2506.01374</link>
      <description>arXiv:2506.01374v3 Announce Type: replace-cross 
Abstract: While model serving has unlocked unprecedented capabilities, the high cost of serving large-scale models continues to be a significant barrier to widespread accessibility and rapid innovation. Compiler optimizations have long driven substantial performance improvements, but existing compilers struggle with neural workloads due to the exponentially large and highly interdependent space of possible transformations. Although existing stochastic search techniques can be effective, they are often sample-inefficient and fail to leverage the structural context underlying compilation decisions. We set out to investigate the research question of whether reasoning with large language models (LLMs), without any retraining, can leverage the context-aware decision space of compiler optimizations to significantly improve sample efficiency. To that end, we introduce a novel compilation framework (dubbed Reasoning Compiler) that formulates optimization as a sequential, context-aware decision process guided by a large language model and structured Monte Carlo tree search (MCTS). The LLM acts as a proposal mechanism, suggesting hardware-informed transformations that reflect the current program state and accumulated performance feedback. MCTS incorporates the LLM-generated proposals to balance exploration and exploitation, facilitating structured, context-sensitive traversal of the expansive compiler optimization space. By achieving substantial speedups with markedly fewer samples than leading neural compilers, our approach demonstrates the potential of LLM-guided reasoning to transform the landscape of compiler optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01374v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.PL</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Annabelle Sujun Tang, Christopher Priebe, Rohan Mahapatra, Lianhui Qin, Hadi Esmaeilzadeh</dc:creator>
    </item>
  </channel>
</rss>
