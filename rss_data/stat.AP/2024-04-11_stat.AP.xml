<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 12 Apr 2024 04:00:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 12 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Certification of MPC-based zonal controller security properties using accuracy-aware machine learning proxies</title>
      <link>https://arxiv.org/abs/2404.07275</link>
      <description>arXiv:2404.07275v1 Announce Type: new 
Abstract: The fast growth of renewable energies increases the power congestion risk. To address this issue, the French Transmission System Operator (RTE) has developed closed-loop controllers to handle congestion. RTE wishes to estimate the probability that the controllers ensure the equipment's safety to guarantee their proper functioning. The naive approach to estimating this probability relies on simulating many randomly drawn scenarios and then using all the outcomes to build a confidence interval around the probability. Although theory ensures convergence, the computational cost of power system simulations makes such a process intractable.
  The present paper aims to propose a faster process using machine-learning-based proxies. The amount of required simulations is significantly reduced thanks to an accuracy-aware proxy built with Multivariate Gaussian Processes. However, using a proxy instead of the simulator adds uncertainty to the outcomes. An adaptation of the Central Limit Theorem is thus proposed to include the uncertainty of the outcomes predicted with the proxy into the confidence interval. As a case study, we designed a simple simulator that was tested on a small network. Results show that the proxy learns to approximate the simulator's answer accurately, allowing a significant time gain for the machine-learning-based process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07275v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Pierre Houdouin, Manuel Ruiz, Patrick Panciatici</dc:creator>
    </item>
    <item>
      <title>Statistics in Phonetics</title>
      <link>https://arxiv.org/abs/2404.07567</link>
      <description>arXiv:2404.07567v1 Announce Type: new 
Abstract: Phonetics is the scientific field concerned with the study and modeling of the articulation and perception of language, in particular how speech is produced, heard and perceived. The goal of this paper is to provide the reader with an overview of statistical methods used in phonetics. We introduce some of the major areas of phonetics, their main research questions, the types of phonetic data that are available, and the statistical methods that are used to model them. We conclude by identifying the opportunities for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07567v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shahin Tavakoli, Beatrice Matteo, Davide Pigoli, Eleanor Chodroff, John Coleman, Michele Gubian, Margaret E. L. Renwick, Morgan Sonderegger</dc:creator>
    </item>
    <item>
      <title>State-Space Modeling of Shape-constrained Functional Time Series</title>
      <link>https://arxiv.org/abs/2404.07586</link>
      <description>arXiv:2404.07586v1 Announce Type: new 
Abstract: Functional time series data frequently appears in economic applications, where the functions of interest are subject to some shape constraints, including monotonicity and convexity, as typical of the estimation of the Lorenz curve. This paper proposes a state-space model for time-varying functions to extract trends and serial dependence from functional time series while imposing the shape constraints on the estimated functions. The function of interest is modeled by a convex combination of selected basis functions to satisfy the shape constraints, where the time-varying convex weights on simplex follow the dynamic multi-logit models. For the complicated likelihood of this model, a novel data augmentation technique is devised to enable posterior computation by an efficient Markov chain Monte Carlo method. The proposed method is applied to the estimation of time-varying Lorenz curves, and its utility is illustrated through numerical experiments and analysis of panel data of household incomes in Japan.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07586v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daichi Hiraki, Yasuyuki Hamura, Kaoru Irie, Shonosuke Sugasawa</dc:creator>
    </item>
    <item>
      <title>Quality check of a sample partition using multinomial distribution</title>
      <link>https://arxiv.org/abs/2404.07778</link>
      <description>arXiv:2404.07778v1 Announce Type: new 
Abstract: In this paper, we advocate a novel measure for the purpose of checking the quality of a cluster partition for a sample into several distinct classes, and thus, determine the unknown value for the true number of clusters prevailing the provided set of data. Our objective leads us to the development of an approach through applying the multinomial distribution to the distances of data members, clustered in a group, from their respective cluster representatives. This procedure is carried out independently for each of the clusters, and the concerned statistics are combined together to design our targeted measure. Individual clusters separately possess the category-wise probabilities which correspond to different positions of its members in the cluster with respect to a typical member, in the form of cluster-centroid, medoid or mode, referred to as the corresponding cluster representative. Our method is robust in the sense that it is distribution-free, since this is devised irrespective of the parent distribution of the underlying sample. It fulfills one of the rare coveted qualities, present in the existing cluster accuracy measures, of having the capability to investigate whether the assigned sample owns any inherent clusters other than a single group of all members or not. Our measure's simple concept, easy algorithm, fast runtime, good performance, and wide usefulness, demonstrated through extensive simulation and diverse case-studies, make it appealing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07778v1</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soumita Modak</dc:creator>
    </item>
    <item>
      <title>ACRONYM: Augmented degree corrected, Community Reticulated Organized Network Yielding Model</title>
      <link>https://arxiv.org/abs/2404.07462</link>
      <description>arXiv:2404.07462v1 Announce Type: cross 
Abstract: Modeling networks can serve as a means of summarizing high-dimensional complex systems. Adapting an approach devised for dense, weighted networks, we propose a new method for generating and estimating unweighted networks. This approach can describe a broader class of potential networks than existing models, including those where nodes in different subnetworks connect to one another via various attachment mechanisms, inducing flexible and varied community structures. While unweighted edges provide less resolution than continuous weights, restricting to the binary case permits the use of likelihood-based estimation techniques, which can improve estimation of nodal features. The extra flexibility may contribute a different understanding of network generating structures, particularly for networks with heterogeneous densities in different regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07462v1</guid>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin Leinwand, Vince Lyzinski</dc:creator>
    </item>
    <item>
      <title>Anomaly Detection in Power Grids via Context-Agnostic Learning</title>
      <link>https://arxiv.org/abs/2404.07898</link>
      <description>arXiv:2404.07898v1 Announce Type: cross 
Abstract: An important tool grid operators use to safeguard against failures, whether naturally occurring or malicious, involves detecting anomalies in the power system SCADA data. In this paper, we aim to solve a real-time anomaly detection problem. Given time-series measurement values coming from a fixed set of sensors on the grid, can we identify anomalies in the network topology or measurement data? Existing methods, primarily optimization-based, mostly use only a single snapshot of the measurement values and do not scale well with the network size. Recent data-driven ML techniques have shown promise by using a combination of current and historical data for anomaly detection but generally do not consider physical attributes like the impact of topology or load/generation changes on sensor measurements and thus cannot accommodate regular context-variability in the historical data. To address this gap, we propose a novel context-aware anomaly detection algorithm, GridCAL, that considers the effect of regular topology and load/generation changes. This algorithm converts the real-time power flow measurements to context-agnostic values, which allows us to analyze measurement coming from different grid contexts in an aggregate fashion, enabling us to derive a unified statistical model that becomes the basis of anomaly detection. Through numerical simulations on networks up to 2383 nodes, we show that our approach is accurate, outperforming state-of-the-art approaches, and is computationally efficient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07898v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>SangWoo Park, Amritanshu Pandey</dc:creator>
    </item>
    <item>
      <title>Statistical Process Monitoring of Isolated and Persistent Defects in Complex Geometrical Shapes</title>
      <link>https://arxiv.org/abs/2310.12876</link>
      <description>arXiv:2310.12876v2 Announce Type: replace 
Abstract: Traditional Statistical Process Control methodologies face several challenges when monitoring defects in complex geometries, such as those of products obtained via Additive Manufacturing techniques. Many approaches cannot be applied in these settings due to the high dimensionality of the data and the lack of parametric and distributional assumptions on the object shapes. Motivated by a case study involving the monitoring of egg-shaped trabecular structures, we investigate two recently-proposed methodologies to detect deviations from the nominal IC model caused by excess or lack of material. Our study focuses on the detection of both isolated large changes in the geometric structure, as well as persistent small deviations. We compare the approach of Scimone et al. (2022) with Zhao and del Castillo (2021) for monitoring defects in a small Phase I sample of 3D-printed objects. While the former control chart is able to detect large defects, the latter allows the detection of nonconforming objects with persistent small defects. Furthermore, we address the fundamental issue of selecting the number of eigenvalues to be monitored in Zhao and del Castillo's method by proposing a dimensionality reduction technique based on kernel principal components. This approach is shown to provide a good detection capability even when considering a large number of eigenvalues. By leveraging the sensitivity of the two monitoring schemes to different magnitudes of nonconformities, we also propose a novel joint monitoring scheme that is capable of identifying both types of defects in the considered case study. Computer code in R and Matlab that implements these methods and replicates the results is available as part of the supplementary material.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.12876v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sara Bonacina, Daniele Zago, Giovanna Capizzi, Bianca Maria Colosimo</dc:creator>
    </item>
    <item>
      <title>On non-negative auto-correlated integer demand processes</title>
      <link>https://arxiv.org/abs/2312.11273</link>
      <description>arXiv:2312.11273v2 Announce Type: replace 
Abstract: Methods to generate realistic non-stationary demand scenarios are a key component for analyzing and optimizing decision policies in supply chains. Typical forecasting techniques recommended in standard inventory control textbooks consist of some form of simple exponential smoothing (SES) for both the estimates for the mean and standard deviation. We study demand generating processes (DGPs) that yield non-stationary demand scenarios, and that are consistent with SES, meaning that SES yields unbiased estimates when applied to the generated demand scenarios. As demand in typical practical settings is discrete and non-negative, we study consistent DGPs on the non-negative integers. We derive conditions under which the existence of such DGPs can be guaranteed, and propose a specific DGP that yields autocorrelated, discrete demands when these conditions are satisfied.
  Our subsequent simulation study gains further insights into the proposed DGP. It demonstrates that from a given initial forecast, our DGPs yields a diverse set of demand scenarios with a wide range of properties. To show the applicability of the DGP, we apply it to generate demand in a standard inventory problem with full backlogging and a positive lead time. We find that appropriate dynamic base-stock levels can be obtained using a new and relatively simple algorithm, and we demonstrate that this algorithm outperforms relevant benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.11273v2</guid>
      <category>stat.AP</category>
      <category>math.PR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lotte van Hezewijk, Nico Dellaert, Willem van Jaarsveld</dc:creator>
    </item>
    <item>
      <title>Performance is not enough: the story told by a Rashomon quartet</title>
      <link>https://arxiv.org/abs/2302.13356</link>
      <description>arXiv:2302.13356v4 Announce Type: replace-cross 
Abstract: The usual goal of supervised learning is to find the best model, the one that optimizes a particular performance measure. However, what if the explanation provided by this model is completely different from another model and different again from another model despite all having similarly good fit statistics? Is it possible that the equally effective models put the spotlight on different relationships in the data? Inspired by Anscombe's quartet, this paper introduces a Rashomon Quartet, i.e. a set of four models built on a synthetic dataset which have practically identical predictive performance. However, the visual exploration reveals distinct explanations of the relations in the data. This illustrative example aims to encourage the use of methods for model visualization to compare predictive models beyond their performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.13356v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Przemyslaw Biecek, Hubert Baniecki, Mateusz Krzyzinski, Dianne Cook</dc:creator>
    </item>
    <item>
      <title>Enhancing Data Efficiency and Feature Identification for Lithium-Ion Battery Lifespan Prediction by Deciphering Interpretation of Temporal Patterns and Cyclic Variability Using Attention-Based Models</title>
      <link>https://arxiv.org/abs/2311.10792</link>
      <description>arXiv:2311.10792v3 Announce Type: replace-cross 
Abstract: Accurately predicting the lifespan of lithium-ion batteries is crucial for optimizing operational strategies and mitigating risks. While numerous studies have aimed at predicting battery lifespan, few have examined the interpretability of their models or how such insights could improve predictions. Addressing this gap, we introduce three innovative models that integrate shallow attention layers into a foundational model from our previous work, which combined elements of recurrent and convolutional neural networks. Utilizing a well-known public dataset, we showcase our methodology's effectiveness. Temporal attention is applied to identify critical timesteps and highlight differences among test cell batches, particularly underscoring the significance of the "rest" phase. Furthermore, by applying cyclic attention via self-attention to context vectors, our approach effectively identifies key cycles, enabling us to strategically decrease the input size for quicker predictions. Employing both single- and multi-head attention mechanisms, we have systematically minimized the required input from 100 to 50 and then to 30 cycles, refining this process based on cyclic attention scores. Our refined model exhibits strong regression capabilities, accurately forecasting the initiation of rapid capacity fade with an average deviation of only 58 cycles by analyzing just the initial 30 cycles of easily accessible input data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.10792v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jaewook Lee, Seongmin Heo, Jay H. Lee</dc:creator>
    </item>
    <item>
      <title>Inferring flavor mixtures in multijet events</title>
      <link>https://arxiv.org/abs/2404.01387</link>
      <description>arXiv:2404.01387v2 Announce Type: replace-cross 
Abstract: Multijet events with heavy-flavors are of central importance at the LHC since many relevant processes -- such as $t\bar t$, $hh$, $t\bar t h$ and others -- have a preferred branching ratio for this final state. Current techniques for tackling these processes use hard-assignment selections through $b$-tagging working points, and suffer from systematic uncertainties because of the difficulties in Monte Carlo simulations. We develop a flexible Bayesian mixture model approach to simultaneously infer $b$-tagging score distributions and the flavor mixture composition in the dataset. We model multidimensional jet events, and to enhance estimation efficiency, we design structured priors that leverages the continuity and unimodality of the $b$-tagging score distributions. Remarkably, our method eliminates the need for a parametric assumption and is robust against model misspecification -- It works for arbitrarily flexible continuous curves and is better if they are unimodal. We have run a toy inferential process with signal $bbbb$ and backgrounds $bbcc$ and $cccc$, and we find that with a few hundred events we can recover the true mixture fractions of the signal and backgrounds, as well as the true $b$-tagging score distribution curves, despite their arbitrariness and nonparametric shapes. We discuss prospects for taking these findings into a realistic scenario in a physics analysis. The presented results could be a starting point for a different and novel kind of analysis in multijet events, with a scope competitive with current state-of-the-art analyses. We also discuss the possibility of using these results in general cases of signals and backgrounds with approximately known continuous distributions and/or expected unimodality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01387v2</guid>
      <category>hep-ph</category>
      <category>hep-ex</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ezequiel Alvarez, Yuling Yao</dc:creator>
    </item>
  </channel>
</rss>
