<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 06 Dec 2024 05:03:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Effect of Simulated Space Conditions on functional Connectivity</title>
      <link>https://arxiv.org/abs/2412.03628</link>
      <description>arXiv:2412.03628v1 Announce Type: new 
Abstract: Long duration spaceflight missions can affect the cognitive and behavioral activities of astronauts due to changes in gravity. The microgravity significantly impacts the central nervous system physiology which causes the degradation in the performance and lead to potential risk in the space exploration. The aim of this study was to evaluate functional connectivity at simulated space conditions using an unloading harness system to mimic the body-weight distribution related to Earth, Mars, and International Space Station. A unity model with six directional arrows to imagine six different motor imagery tasks associated with arms and legs were designed for the Oculus Rift S virtual reality headset for testing. An Electroencephalogram (EEG) and functional near infrared spectroscopy (fNIRS) signals were recorded from 10 participants in the distributed weight conditions related to Earth, Mars, and International Space station using the g.Nautilus fNIRS system at sampling rate of 500 Hz. The magnitude squared coherence were estimated from left vs right hemisphere of the brain that represents functional connectivity. The EEG coherence was the higher which shows the strong functional connectivity and fNIRS coherence was lower shows weak functional connectivity between left vs right hemisphere of the brain, during all the tasks and trials irrespective of the simulated space conditions. Further analysis of functional connectivity needed between the intra-regions of the brain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03628v1</guid>
      <category>q-bio.NC</category>
      <category>cs.HC</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Parshuram N Aarotale, Jaydip Desai</dc:creator>
    </item>
    <item>
      <title>Neuromodulation and homeostasis: complementary mechanisms for robust neural function</title>
      <link>https://arxiv.org/abs/2412.04172</link>
      <description>arXiv:2412.04172v1 Announce Type: new 
Abstract: Neurons depend on two interdependent mechanisms-homeostasis and neuromodulation-to maintain robust and adaptable functionality. Homeostasis stabilizes neuronal activity by adjusting ionic conductances, whereas neuromodulation dynamically modifies ionic properties in response to external signals. Combining these mechanisms in conductance-based models often produces unreliable outcomes, particularly when sharp neuromodulation interferes with homeostatic tuning. This study explores how a biologically inspired neuromodulation controller can harmonize with homeostasis to ensure reliable neuronal function. Using computational models of stomatogastric ganglion and dopaminergic neurons, we demonstrate that controlled neuromodulation preserves neuronal firing patterns while maintaining intracellular calcium levels. Unlike sharp neuromodulation, the neuromodulation controller integrates activity-dependent feedback through mechanisms mimicking G-protein-coupled receptor cascades. The interaction between these controllers critically depends on the existence of an intersection in conductance space, representing a balance between target calcium levels and neuromodulated firing patterns. Maximizing neuronal degeneracy enhances the likelihood of such intersections, enabling robust modulation and compensation for channel blockades. We further show that this controller pairing extends to network-level activity, reliably modulating central pattern generators in crustaceans. These findings suggest that targeting neuromodulation pathways-rather than ion channels directly-may offer safer pharmacological strategies to manage neuronal dysfunctions. This study highlights the complementary roles of homeostasis and neuromodulation, proposing a unified control framework for maintaining robust and adaptive neural activity under physiological and pathological conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04172v1</guid>
      <category>q-bio.NC</category>
      <category>math.DS</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arthur Fyon, Guillaume Drion</dc:creator>
    </item>
    <item>
      <title>Predictive Strategies for the Control of Complex Motor Skills: Recent Insights into Individual and Joint Actions</title>
      <link>https://arxiv.org/abs/2412.04191</link>
      <description>arXiv:2412.04191v1 Announce Type: new 
Abstract: Humans can perform exquisite sensorimotor skills, both individually and in teams, from athletes performing rhythmic gymnastics to everyday tasks like carrying a cup of coffee. The "predictive brain" framework suggests that mastering these tasks relies on predictive mechanisms, raising the question of how we deploy such predictions for real-time control and coordination. This review highlights two lines of research: one showing that during the control of complex objects people make the interaction with 'tools' predictable; the second one examines dyadic coordination showing that people make their behavior predictable for their partners. These studies demonstrate that to achieve sophisticated motor skills, we play "prediction tricks": we select subspaces of predictable solutions and make sensorimotor interactions more predictable and legible by and for others. This synthesis underscores the critical role of predictability in optimizing control strategies across various contexts and establishes a link between predictive processing and closed-loop control theories of behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04191v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marta Russo, Antonella Maselli, Dagmar Sternad, Giovanni Pezzulo</dc:creator>
    </item>
    <item>
      <title>Theoretical / numerical study of modulated traveling waves in inhibition stabilized networks</title>
      <link>https://arxiv.org/abs/2412.03613</link>
      <description>arXiv:2412.03613v1 Announce Type: cross 
Abstract: We prove a principle of linearized stability for traveling wave solutions to neural field equations posed on the real line. Additionally, we provide the existence of a finite dimensional invariant center manifold close to a traveling wave, this allows to study bifurcations of traveling waves. Finally, the spectral properties of the modulated traveling waves are investigated. Numerical schemes for the computation of modulated traveling waves are provided. We then apply these results and methods to study a neural field model in a inhibitory stabilized regime. We showcase Fold, Hopf and Bodgdanov-Takens bifurcations of traveling pulses. Additionally, we continue the modulated traveling pulses as function of the time scale ratio of the two neural populations and show numerical evidences for snaking of modulated traveling pulses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03613v1</guid>
      <category>math.DS</category>
      <category>math.FA</category>
      <category>math.SP</category>
      <category>nlin.PS</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Safaa Habib, Romain Veltz</dc:creator>
    </item>
    <item>
      <title>Artificial intelligence and the internal processes of creativity</title>
      <link>https://arxiv.org/abs/2412.04366</link>
      <description>arXiv:2412.04366v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) systems capable of generating creative outputs are reshaping our understanding of creativity. This shift presents an opportunity for creativity researchers to reevaluate the key components of the creative process. In particular, the advanced capabilities of AI underscore the importance of studying the internal processes of creativity. This paper explores the neurobiological machinery that underlies these internal processes and describes the experiential component of creativity. It is concluded that although the products of artificial and human creativity can be similar, the internal processes are different. The paper also discusses how AI may negatively affect the internal processes of human creativity, such as the development of skills, the integration of knowledge, and the diversity of ideas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04366v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jaan Aru</dc:creator>
    </item>
    <item>
      <title>Probabilistic models, compressible interactions, and neural coding</title>
      <link>https://arxiv.org/abs/2112.14334</link>
      <description>arXiv:2112.14334v2 Announce Type: replace 
Abstract: In physics we often use very simple models to describe systems with many degrees of freedom, but it is not clear why or how this success can be transferred to the more complex biological context. We consider models for the joint distribution of many variables, as with the combinations of spiking and silence in large networks of neurons. In this probabilistic framework, we argue that simple models are possible if the mutual information between two halves of the system is consistently sub--extensive, and if this shared information is compressible. These conditions are not met generically, but they are met by real world data such as natural images and the activity in a population of retinal output neurons. We introduce compression strategies that combine the information bottleneck with an iteration scheme inspired by the renormalization group, and find that the number of parameters needed to describe the distribution of joint activity scales with the square of the number of neurons, even though the interactions are not well approximated as pairwise. Our results also show that this shared information is essentially equal to the information that individual neurons carry about natural visual inputs, which has surprising implications for the neural code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.14334v2</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.stat-mech</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luisa Ramirez, William Bialek, Stephanie E. Palmer, David J. Schwab</dc:creator>
    </item>
    <item>
      <title>Brain Morphology Normative modelling platform for abnormality and Centile estimation: Brain MoNoCle</title>
      <link>https://arxiv.org/abs/2406.01107</link>
      <description>arXiv:2406.01107v3 Announce Type: replace 
Abstract: Normative models of brain structure estimate the effects of covariates such as age and sex using large samples of healthy controls. These models can then be applied to e.g. smaller clinical cohorts to distinguish disease effects from other covariates. However, these advanced statistical modelling approaches can be difficult to access, and processing large healthy cohorts is computationally demanding. Thus, accessible platforms with pre-trained normative models are needed.
  We present such a platform for brain morphology analysis as an open-source web application (https://cnnplab.shinyapps.io/BrainMoNoCle/), with six key features: (i) user-friendly web interface, (ii) individual and group outputs, (iii) multi-site analysis, (iv) regional and whole-brain analysis, (v) integration with existing tools, and (vi) featuring multiple morphology metrics.
  Using a diverse sample of 3,276 healthy controls across 21 sites, we pre-trained normative models on various metrics. We validated the models with a small sample of individuals with bipolar disorder, showing outputs that aligned closely with existing literature only after applying our normative modelling. Using a cohort of people with temporal lobe epilepsy, we showed that individual-level abnormalities were in line with seizure lateralisation. Finally, with the ability to investigate multiple morphology measures in the same framework, we found that biological covariates are better explained in specific morphology measures, and for applications, only some measures are sensitive to the disease process.
  Our platform offers a comprehensive framework to analyse brain morphology in clinical and research settings. Validations confirm the superiority of normative models and the advantage of investigating a range of brain morphology metrics together.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01107v3</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bethany Little, Nida Alyas, Alexander Surtees, Gavin P Winston, John S Duncan, David A Cousins, John-Paul Taylor, Peter Taylor, Karoline Leiberg, Yujiang Wang</dc:creator>
    </item>
    <item>
      <title>Learning in Wilson-Cowan model for metapopulation</title>
      <link>https://arxiv.org/abs/2406.16453</link>
      <description>arXiv:2406.16453v2 Announce Type: replace 
Abstract: The Wilson-Cowan model for metapopulation, a Neural Mass Network Model, treats different subcortical regions of the brain as connected nodes, with connections representing various types of structural, functional, or effective neuronal connectivity between these regions. Each region comprises interacting populations of excitatory and inhibitory cells, consistent with the standard Wilson-Cowan model. By incorporating stable attractors into such a metapopulation model's dynamics, we transform it into a learning algorithm capable of achieving high image and text classification accuracy. We test it on MNIST and Fashion MNIST, in combination with convolutional neural networks, on CIFAR-10 and TF-FLOWERS, and, in combination with a transformer architecture (BERT), on IMDB, always showing high classification accuracy. These numerical evaluations illustrate that minimal modifications to the Wilson-Cowan model for metapopulation can reveal unique and previously unobserved dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16453v2</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Neural Computation 2024</arxiv:journal_reference>
      <dc:creator>Raffaele Marino, Lorenzo Buffoni, Lorenzo Chicchi, Francesca Di Patti, Diego Febbe, Lorenzo Giambagli, Duccio Fanelli</dc:creator>
    </item>
    <item>
      <title>Spiking representation learning for associative memories</title>
      <link>https://arxiv.org/abs/2406.03054</link>
      <description>arXiv:2406.03054v2 Announce Type: replace-cross 
Abstract: Networks of interconnected neurons communicating through spiking signals offer the bedrock of neural computations. Our brains spiking neural networks have the computational capacity to achieve complex pattern recognition and cognitive functions effortlessly. However, solving real-world problems with artificial spiking neural networks (SNNs) has proved to be difficult for a variety of reasons. Crucially, scaling SNNs to large networks and processing large-scale real-world datasets have been challenging, especially when compared to their non-spiking deep learning counterparts. The critical operation that is needed of SNNs is the ability to learn distributed representations from data and use these representations for perceptual, cognitive and memory operations. In this work, we introduce a novel SNN that performs unsupervised representation learning and associative memory operations leveraging Hebbian synaptic and activity-dependent structural plasticity coupled with neuron-units modelled as Poisson spike generators with sparse firing (~1 Hz mean and ~100 Hz maximum firing rate). Crucially, the architecture of our model derives from the neocortical columnar organization and combines feedforward projections for learning hidden representations and recurrent projections for forming associative memories. We evaluated the model on properties relevant for attractor-based associative memories such as pattern completion, perceptual rivalry, distortion resistance, and prototype extraction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03054v2</guid>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.3389/fnins.2024.1439414</arxiv:DOI>
      <dc:creator>Naresh Ravichandran, Anders Lansner, Pawel Herman</dc:creator>
    </item>
    <item>
      <title>Scaling Laws for Task-Optimized Models of the Primate Visual Ventral Stream</title>
      <link>https://arxiv.org/abs/2411.05712</link>
      <description>arXiv:2411.05712v2 Announce Type: replace-cross 
Abstract: When trained on large-scale object classification datasets, certain artificial neural network models begin to approximate core object recognition (COR) behaviors and neural response patterns in the primate visual ventral stream (VVS). While recent machine learning advances suggest that scaling model size, dataset size, and compute resources improve task performance, the impact of scaling on brain alignment remains unclear. In this study, we explore scaling laws for modeling the primate VVS by systematically evaluating over 600 models trained under controlled conditions on benchmarks spanning V1, V2, V4, IT and COR behaviors. We observe that while behavioral alignment continues to scale with larger models, neural alignment saturates. This observation remains true across model architectures and training datasets, even though models with stronger inductive bias and datasets with higher-quality images are more compute-efficient. Increased scaling is especially beneficial for higher-level visual areas, where small models trained on few samples exhibit only poor alignment. Finally, we develop a scaling recipe, indicating that a greater proportion of compute should be allocated to data samples over model size. Our results suggest that while scaling alone might suffice for alignment with human core object recognition behavior, it will not yield improved models of the brain's visual ventral stream with current architectures and datasets, highlighting the need for novel strategies in building brain-like models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05712v2</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 06 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Abdulkadir Gokce, Martin Schrimpf</dc:creator>
    </item>
  </channel>
</rss>
