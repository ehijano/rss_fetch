<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 10 Mar 2025 04:00:40 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Bhirkuti's Relative Efficiency (BRE): Examining in Psychometric Simulations</title>
      <link>https://arxiv.org/abs/2503.04775</link>
      <description>arXiv:2503.04775v1 Announce Type: new 
Abstract: Traditional Relative Efficiency (RE), based solely on variance, has limitations in estimator performance evaluation, especially in planned missing data designs. We introduce Bhirkuti's Relative Efficiency (BRE), a novel metric that integrates precision and accuracy for a more robust assessment. BRE is computed using interquartile range (IQR) overlap for precision and a bias adjustment factor based on the absolute median relative bias (AMRB). Monte Carlo simulations using a Latent Growth Model (LGM) with planned missing data (SWMD-6) illustrate that BRE remains theoretically consistent and interpretable, avoiding paradoxes such as RE exceeding 100%. Visualizations via boxplots and ridgeline plots confirm that BRE provides a stable and meaningful estimator efficiency evaluation, making it a valuable advancement in psychometric and statistical modeling. By addressing fundamental weaknesses in traditional RE, BRE provides a superior, theoretically justified alternative for relative efficiency assessment in psychometric modeling, structural equation modeling, and missing data research. This advancement enhances data-driven decision-making and offers a methodologically rigorous tool for researchers analyzing incomplete datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04775v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aneel Bhusal, Todd D. Little</dc:creator>
    </item>
    <item>
      <title>Estimation of relative risk, odds ratio and their logarithms with guaranteed accuracy and controlled sample size ratio</title>
      <link>https://arxiv.org/abs/2503.04876</link>
      <description>arXiv:2503.04876v1 Announce Type: new 
Abstract: Given two populations from which independent binary observations are taken with parameters $p_1$ and $p_2$ respectively, estimators are proposed for the relative risk $p_1/p_2$, the odds ratio $p_1(1-p_2)/(p_2(1-p_1))$ and their logarithms. The estimators guarantee that the relative mean-square error, or the mean-square error for the logarithmic versions, is less than a target value for any $p_1, p_2 \in (0,1)$, and the ratio of average sample sizes from the two populations is close to a prescribed value. The estimators can also be used with group sampling, whereby samples are taken in batches of fixed size from the two populations. The efficiency of the estimators with respect to the Cram\'er-Rao bound is good, and in particular it is close to $1$ for small values of the target error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04876v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luis Mendo</dc:creator>
    </item>
    <item>
      <title>A Partial Linear Estimator for Small Study Regression Discontinuity Designs</title>
      <link>https://arxiv.org/abs/2503.04904</link>
      <description>arXiv:2503.04904v1 Announce Type: new 
Abstract: Regression discontinuity (RD) designs are a popular approach to estimating a treatment effect of cutoff-based interventions. Two current estimation approaches dominate the literature. One fits separate regressions on either side of the cutoff, and the other performs finite sample inference based on a local randomization assumption. Recent developments of these approaches have often focused on asymptotic properties and large sample sizes. Educational applications often contain relatively small samples or sparsity near the cutoff, making estimation more difficult. As an alternative to the aforementioned approaches, we develop a partial linear estimator for RD designs. We show in simulations that our estimator outperforms certain leading estimators in several realistic, small-sample scenarios. We apply our estimator to school accountability scores in Indiana.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04904v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daryl Swartzentruber, Eloise Kaizar</dc:creator>
    </item>
    <item>
      <title>Kernel-based estimators for functional causal effects</title>
      <link>https://arxiv.org/abs/2503.05024</link>
      <description>arXiv:2503.05024v1 Announce Type: new 
Abstract: We propose causal effect estimators based on empirical Fr\'{e}chet means and operator-valued kernels, tailored to functional data spaces. These methods address the challenges of high-dimensionality, sequential ordering, and model complexity while preserving robustness to treatment misspecification. Using structural assumptions, we obtain compact representations of potential outcomes, enabling scalable estimation of causal effects over time and across covariates. We provide both theoretical, regarding the consistency of functional causal effects, as well as empirical comparison of a range of proposed causal effect estimators.
  Applications to binary treatment settings with functional outcomes illustrate the framework's utility in biomedical monitoring, where outcomes exhibit complex temporal dynamics. Our estimators accommodate scenarios with registered covariates and outcomes, aligning them to the Fr\'{e}chet means, as well as cases requiring higher-order representations to capture intricate covariate-outcome interactions. These advancements extend causal inference to dynamic and non-linear domains, offering new tools for understanding complex treatment effects in functional data settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05024v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yordan P. Raykov, Hengrui Luo, Justin D. Strait, Wasiur R. KhudaBukhsh</dc:creator>
    </item>
    <item>
      <title>A New Representation of Ewens-Pitman's Partition Structure and Its Characterization via Riordan Array Sums</title>
      <link>https://arxiv.org/abs/2503.05034</link>
      <description>arXiv:2503.05034v1 Announce Type: new 
Abstract: Ewens-Pitman's partition structure arises as a system of sampling consistent probability distributions on set partitions induced by the Pitman-Yor process. It is widely used in statistical applications, particularly in species sampling models in Bayesian nonparametrics. Drawing references from the area of representation theory of the infinite symmetric group, we view Ewens-Pitman's partition structure as an example of a non-extreme harmonic function on a branching graph, specifically, the Kingman graph. Taking this perspective enables us to obtain combinatorial and algebraic constructions of this distribution using the interpolation polynomial approach proposed by Borodin and Olshanski (The Electronic Journal of Combinatorics, 7, 2000). We provide a new explicit representation of Ewens-Pitman's partition structure using modern umbral interpolation based on Sheffer polynomial sequences. In addition, we show that a certain type of marginals of this distribution can be computed using weighted row sums of a Riordan array. In this way, we show that some summary statistics and estimators derived from Ewens-Pitman's partition structure can be obtained using methods of generating functions. This approach simplifies otherwise cumbersome calculations of these quantities often involving various special combinatorial functions. In addition, it has the added benefit of being amenable to symbolic computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05034v1</guid>
      <category>stat.ME</category>
      <category>math.CO</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Greve</dc:creator>
    </item>
    <item>
      <title>Inverse sampling intensity weighting for preferential sampling adjustment</title>
      <link>https://arxiv.org/abs/2503.05067</link>
      <description>arXiv:2503.05067v1 Announce Type: new 
Abstract: Traditional geostatistical methods assume independence between observation locations and the spatial process of interest. Violations of this independence assumption are referred to as preferential sampling (PS). Standard methods to address PS rely on estimating complex shared latent variable models and can be difficult to apply in practice. We study the use of inverse sampling intensity weighting (ISIW) for PS adjustment in model-based geostatistics. ISIW is a two-stage approach wherein we estimate the sampling intensity of the observation locations then define intensity-based weights within a weighted likelihood adjustment. Prediction follows by substituting the adjusted parameter estimates within a kriging framework. A primary contribution was to implement ISIW by means of the Vecchia approximation, which provides large computational gains and improvements in predictive accuracy. Interestingly, we found that accurate parameter estimation had little correlation with predictive performance, raising questions about the conditions and parameter choices driving optimal implementation of kriging-based predictors under PS. Our work highlights the potential of ISIW to adjust for PS in an intuitive, fast, and effective manner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05067v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas W. Hsiao, Lance A. Waller</dc:creator>
    </item>
    <item>
      <title>Cluster weighted models for functional data</title>
      <link>https://arxiv.org/abs/2503.05159</link>
      <description>arXiv:2503.05159v1 Announce Type: new 
Abstract: We propose a method, funWeightClust, based on a family of parsimonious models for clustering heterogeneous functional linear regression data. These models extend cluster weighted models to functional data, and they allow for multivariate functional responses and predictors. The proposed methodology follows the approach used by the the functional high dimensional data clustering (funHDDC) method. We construct an expectation maximization (EM) algorithm for parameter estimation. Using simulated and benchmark data we show that funWeightClust outperforms funHDDC and several two-steps clustering methods. We also use funWeightClust to analyze traffic patterns in Edmonton, Canada.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05159v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Cristina Anton, Iain Smith</dc:creator>
    </item>
    <item>
      <title>Subbagging Variable Selection for Big Data</title>
      <link>https://arxiv.org/abs/2503.05300</link>
      <description>arXiv:2503.05300v1 Announce Type: new 
Abstract: This article introduces a subbagging (subsample aggregating) approach for variable selection in regression within the context of big data. The proposed subbagging approach not only ensures that variable selection is scalable given the constraints of available computational resources, but also preserves the statistical efficiency of the resulting estimator. In particular, we propose a subbagging loss function that aggregates the least-squares approximations of the loss function for each subsample. Subsequently, we penalize the subbagging loss function via an adaptive LASSO-type regularizer, and obtain a regularized estimator to achieve variable selection. We then demonstrate that the regularized estimator exhibits $\sqrt{N}$-consistency and possesses the oracle properties, where $N$ represents the size of the full sample in the big data. In addition, we propose a subbagging Bayesian information criterion to select the regularization parameter, ensuring that the regularized estimator achieves selection consistency. Simulation experiments are conducted to demonstrate the numerical performance. A U.S. census dataset is analyzed to illustrate the usefulness and computational scalability of the subbagging variable selection method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05300v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xian Li, Xuan Liang, Tao Zou</dc:creator>
    </item>
    <item>
      <title>Matrix Time Series Modeling: A Hybrid Framework Combining Autoregression and Common Factors</title>
      <link>https://arxiv.org/abs/2503.05340</link>
      <description>arXiv:2503.05340v1 Announce Type: new 
Abstract: Matrix-valued time series analysis has gained prominence in econometrics and finance due to the increasing availability of high-dimensional data with inherent matrix structures. Traditional approaches, such as Matrix Autoregressive (MAR) models and Dynamic Matrix Factor (DMF) models, often impose restrictive assumptions that may not align with real-world data complexities. To address this gap, we propose a novel Matrix Autoregressive with Common Factors (MARCF) model, which bridges the gap between MAR and DMF frameworks by introducing common bases between predictor and response subspaces. The MARCF model achieves significant dimension reduction and enables a more flexible and interpretable factor representation of dynamic relationships. We develop a computationally efficient estimator and a gradient descent algorithm. Theoretical guarantees for computational and statistical convergence are provided, and extensive simulations demonstrate the robustness and accuracy of the model. Applied to a multinational macroeconomic dataset, the MARCF model outperforms existing methods in forecasting and provides meaningful insights into the interplay between countries and economic factors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05340v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiyun Fan, Xiaoyu Zhang, Mingyang Chen, Di Wang</dc:creator>
    </item>
    <item>
      <title>Joint graphical model estimation using Stein-type shrinkage for fast large scale network inference in scRNAseq data</title>
      <link>https://arxiv.org/abs/2503.05448</link>
      <description>arXiv:2503.05448v1 Announce Type: new 
Abstract: Graphical modeling is a widely used tool for analyzing conditional dependencies between variables and traditional methods may struggle to capture shared and distinct structures in multi-group or multi-condition settings. Joint graphical modeling (JGM) extends this framework by simultaneously estimating network structures across multiple related datasets, allowing for a deeper understanding of commonalities and differences. This capability is particularly valuable in fields such as genomics and neuroscience, where identifying variations in network topology can provide critical biological insights. Existing JGM methodologies largely fall into two categories: regularization-based approaches, which introduce additional penalties to enforce structured sparsity, and Bayesian frameworks, which incorporate prior knowledge to improve network inference. In this study, we explore an alternative method based on two-target linear covariance matrix shrinkage. Formula for optimal shrinkage intensities is proposed which leads to the development of JointStein framework. Performance of JointStein framework is proposed through simulation benchmarking which demonstrates its effectiveness for large-scale single-cell RNA sequencing (scRNA-seq) data analysis. Finally, we apply our approach to glioblastoma scRNA-seq data, uncovering dynamic shifts in T cell network structures across disease progression stages. The result highlights potential of JointStein framework in extracting biologically meaningful insights from high-dimensional data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05448v1</guid>
      <category>stat.ME</category>
      <category>q-bio.MN</category>
      <category>q-bio.QM</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Duong H. T. Vo, Nelofer Syed, Thomas Thorne</dc:creator>
    </item>
    <item>
      <title>Estimation of the generalized Laplace distribution and its projection onto the circle</title>
      <link>https://arxiv.org/abs/2503.05485</link>
      <description>arXiv:2503.05485v1 Announce Type: new 
Abstract: The generalized Laplace (GL) distribution, which falls in the larger family of generalized hyperbolic distributions, provides a versatile model to deal with a variety of applications thanks to its shape parameters. The elliptically symmetric GL admits a polar representation that can be used to yield a circular distribution, which we call \emph{projected} GL distribution. The latter does not appear to have been considered yet in practical applications. In this article, we explore an easy-to-implement maximum likelihood estimation strategy based on Gaussian quadrature for the scale-mixture representation of the GL and its projection onto the circle. A simulation study is carried out to benchmark the fitting routine against alternative estimation methods to assess its feasibility, while the projected GL model is contrasted with other popular circular distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05485v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marco Geraci</dc:creator>
    </item>
    <item>
      <title>A functional approach for curve alignment and shape analysis</title>
      <link>https://arxiv.org/abs/2503.05632</link>
      <description>arXiv:2503.05632v1 Announce Type: new 
Abstract: The shape $\tilde{\mathbf{X}}$ of a random planar curve $\mathbf{X}$ is what remains after removing deformation effects such as scaling, rotation, translation, and parametrization. Previous studies in statistical shape analysis have focused on analyzing $\tilde{\bf X}$ through discrete observations of the curve ${\bf X}$. While this approach has some computational advantages, it overlooks the continuous nature of both ${\bf X}$ and its shape $\tilde{\bf X}$. It also ignores potential dependencies among the deformation variables and their effect on $\tilde{ \bf X}$, which may result in information loss and reduced interpretability. In this paper, we introduce a novel framework for analyzing $\bf X$ in the context of Functional Data Analysis (FDA). Basis expansion techniques are employed to derive analytic solutions for estimating the deformation variables such as rotation and reparametrization, thereby achieving shape alignment. The generative model of $\bf X$ is then investigated using a joint-principal component analysis approach. Numerical experiments on simulated data and the \textit{MPEG-7} database demonstrate that our new approach successfully identifies the deformation parameters and captures the underlying distribution of planar curves in situations where traditional FDA methods fail to do so.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05632v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Issam-Ali Moindji\'e, C\'edric Beaulac, Marie-H\'el\`ene Descary</dc:creator>
    </item>
    <item>
      <title>Integration of aggregated data in causally interpretable meta-analysis by inverse weighting</title>
      <link>https://arxiv.org/abs/2503.05634</link>
      <description>arXiv:2503.05634v1 Announce Type: new 
Abstract: Obtaining causally interpretable meta-analysis results is challenging when there are differences in the distribution of effect modifiers between eligible trials. To overcome this, recent work on transportability methods has considered standardizing results of individual studies over the case-mix of a target population, prior to pooling them as in a classical random-effect meta-analysis. One practical challenge, however, is that case-mix standardization often requires individual participant data (IPD) on outcome, treatments and case-mix characteristics to be fully accessible in every eligible study, along with IPD case-mix characteristics for a random sample from the target population. In this paper, we aim to develop novel strategies to integrate aggregated-level data from eligible trials with non-accessible IPD into a causal meta-analysis, by extending moment-based methods frequently used for population-adjusted indirect comparison in health technology assessment. Since valid inference for these moment-based methods by M-estimation theory requires additional aggregated data that are often unavailable in practice, computational methods to address this concern are also developed. We assess the finite-sample performance of the proposed approaches by simulated data, and then apply these on real-world clinical data to investigate the effectiveness of risankizumab versus ustekinumab among patients with moderate to severe psoriasis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05634v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tat-Thang Vo, Tran Trong Khoi Le, Sivem Afach, Stijn Vansteelandt</dc:creator>
    </item>
    <item>
      <title>A comparison of the Alkire-Foster method and a Markov random field approach in the analysis of multidimensional poverty</title>
      <link>https://arxiv.org/abs/2503.05676</link>
      <description>arXiv:2503.05676v1 Announce Type: new 
Abstract: Multidimensional poverty measurement is crucial for capturing deprivation beyond income-based metrics. This study compares the Alkire-Foster (AF) method and a Markov Random Field (MRF) approach for classifying multidimensional poverty using a simulation-based analysis. The AF method applies a deterministic threshold-based classification, while the MRF approach leverages probabilistic graphical modelling to account for correlations between deprivation indicators. Using a synthetic dataset of 50,000 individuals with ten binary deprivation indicators, we assess classification accuracy, false positive/negative trade-offs, and agreement between the methods. Results show that AF achieves higher classification accuracy (89.5%) compared to MRF (75.4%), with AF minimizing false negatives and MRF reducing false positives. The overall agreement between the two methods is 65%, with discrepancies primarily occurring when AF classifies individuals as poor while MRF does not. While AF is transparent and easy to implement, it does not capture interdependencies among indicators, potentially leading to misclassification. MRF, though computationally intensive, offers a more nuanced understanding of deprivation clusters. These findings highlight the trade-offs in multidimensional poverty measurement and provide insights for policymakers on method selection based on data availability and policy objectives. Future research should extend these approaches to non-binary indicators and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05676v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Joseph Lam</dc:creator>
    </item>
    <item>
      <title>Maximizing Signal in Human-Model Preference Alignment</title>
      <link>https://arxiv.org/abs/2503.04910</link>
      <description>arXiv:2503.04910v1 Announce Type: cross 
Abstract: The emergence of powerful LLMs has led to a paradigm shift in Natural Language Understanding and Natural Language Generation. The properties that make LLMs so valuable for these tasks -- creativity, ability to produce fluent speech, and ability to quickly and effectively abstract information from large corpora -- also present new challenges to evaluating their outputs. The rush to market has led teams to fall back on quick, cost-effective automatic evaluations which offer value, but do not obviate the need for human judgments in model training and evaluation. This paper argues that in cases in which end users need to agree with the decisions made by ML models -- e.g. in toxicity detection or extraction of main points for summarization -- models should be trained and evaluated on data that represent the preferences of those users. We support this argument by explicating the role of human feedback in labeling and judgment tasks for model training and evaluation. First, we propose methods for disentangling noise from signal in labeling tasks. Then we show that noise in labeling disagreement can be minimized by adhering to proven methodological best practices, while signal can be maximized to play an integral role in model training and evaluation tasks. Finally, we illustrate best practices by providing a case study in which two guardrails classifiers are evaluated using human judgments to align final model behavior to user preferences. We aim for this paper to provide researchers and professionals with guidelines to integrating human judgments into their ML and generative AI evaluation toolkit, particularly when working toward achieving accurate and unbiased features that align with users' needs and expectations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04910v1</guid>
      <category>cs.CL</category>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Kelsey Kraus, Margaret Kroll</dc:creator>
    </item>
    <item>
      <title>A Novel Framework for Modeling Quarantinable Disease Transmission</title>
      <link>https://arxiv.org/abs/2503.04951</link>
      <description>arXiv:2503.04951v1 Announce Type: cross 
Abstract: The COVID-19 pandemic has significantly challenged traditional epidemiological models due to factors such as delayed diagnosis, asymptomatic transmission, isolation-induced contact changes, and underreported mortality. In response to these complexities, this paper introduces a novel CURNDS model prioritizing compartments and transmissions based on contact levels, rather than merely on symptomatic severity or hospitalization status. The framework surpasses conventional uniform mixing and static rate assumptions by incorporating adaptive power laws, dynamic transmission rates, and spline-based smoothing techniques. The CURNDS model provides accurate estimates of undetected infections and undocumented deaths from COVID-19 data, uncovering the pandemic's true impact. Our analysis challenges the assumption of homogeneous mixing between infected and non-infected individuals in traditional epidemiological models. By capturing the nuanced transmission dynamics of infection and confirmation, our model offers new insights into the spread of different COVID-19 strains. Overall, CURNDS provides a robust framework for understanding the complex transmission patterns of highly contagious, quarantinable diseases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04951v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenchen Liu, Chang Liu, Dehui Wang, Yiyuan She</dc:creator>
    </item>
    <item>
      <title>Bayesian analysis of restricted mean survival time adjusted for covariates using pseudo-observations</title>
      <link>https://arxiv.org/abs/2503.05225</link>
      <description>arXiv:2503.05225v1 Announce Type: cross 
Abstract: The difference in restricted mean survival time (RMST) is a clinically meaningful measure to quantify treatment effect in randomized controlled trials, especially when the proportional hazards assumption does not hold. Several frequentist methods exist to estimate RMST adjusted for covariates based on modeling and integrating the survival function. A more natural approach may be a regression model on RMST using pseudo-observations, which allows for a direct estimation without modeling the survival function. Only a few Bayesian methods exist, and each requires a model of the survival function. We developed a new Bayesian method that combines the use of pseudo-observations with the generalized method of moments. This offers RMST estimation adjusted for covariates without the need to model the survival function, making it more attractive than existing Bayesian methods. A simulation study was conducted with different time-dependent treatment effects (early, delayed, and crossing survival) and covariate effects, showing that our approach provides valid results, aligns with existing methods, and shows improved precision after covariate adjustment. For illustration, we applied our approach to a phase III trial in prostate cancer, providing estimates of the treatment effect on RMST, comparable to existing methods. In addition, our approach provided the effect of other covariates on RMST and determined the posterior probability of the difference in RMST exceeds any given time threshold for any covariate, allowing for nuanced and interpretable results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05225v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'ea Orsini (CESP, U1018), Emmanuel Lesaffre (KU Leuven), Guosheng Yin (DSAS), Caroline Brard (U1018), David Dejardin (U1018), Gw\'ena\"el Le Teuff (U1018)</dc:creator>
    </item>
    <item>
      <title>regMMD: a package for parametric estimation and regression with maximum mean discrepancy</title>
      <link>https://arxiv.org/abs/2503.05297</link>
      <description>arXiv:2503.05297v1 Announce Type: cross 
Abstract: The Maximum Mean Discrepancy (MMD) is a kernel-based metric widely used for nonparametric tests and estimation. Recently, it has also been studied as an objective function for parametric estimation, as it has been shown to yield robust estimators. We have implemented MMD minimization for parameter inference in a wide range of statistical models, including various regression models, within an R package called regMMD. This paper provides an introduction to the regMMD package. We describe the available kernels and optimization procedures, as well as the default settings. Detailed applications to simulated and real data are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05297v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pierre Alquier, Mathieu Gerber</dc:creator>
    </item>
    <item>
      <title>Sparse Bayesian Multidimensional Item Response Theory</title>
      <link>https://arxiv.org/abs/2310.17820</link>
      <description>arXiv:2310.17820v4 Announce Type: replace 
Abstract: Multivariate Item Response Theory (MIRT) is sought-after widely by applied researchers looking for interpretable (sparse) explanations underlying response patterns in questionnaire data. There is, however, an unmet demand for such sparsity discovery tools in practice. Our paper develops a Bayesian platform for binary and ordinal item MIRT which requires minimal tuning and scales well on large datasets due to its parallelizable features. Bayesian methodology for MIRT models has traditionally relied on MCMC simulation, which cannot only be slow in practice, but also often renders exact sparsity recovery impossible without additional thresholding. In this work, we develop a scalable Bayesian EM algorithm to estimate sparse factor loadings from mixed continuous, binary, and ordinal item responses. We address the seemingly insurmountable problem of unknown latent factor dimensionality with tools from Bayesian nonparametrics which enable estimating the number of factors. Rotations to sparsity through parameter expansion further enhance convergence and interpretability without identifiability constraints. In our simulation study, we show that our method reliably recovers both the factor dimensionality as well as the latent structure on high-dimensional synthetic data even for small samples. We demonstrate the practical usefulness of our approach on three datasets: an educational assessment dataset, a quality-of-life measurement dataset, and a bio-behavioral dataset. All demonstrations show that our tool yields interpretable estimates, facilitating interesting discoveries that might otherwise go unnoticed under a pure confirmatory factor analysis setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.17820v4</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/01621459.2025.2476786</arxiv:DOI>
      <arxiv:journal_reference>Journal of the American Statistical Association, 2025</arxiv:journal_reference>
      <dc:creator>Jiguang Li, Robert Gibbons, Veronika Rockova</dc:creator>
    </item>
    <item>
      <title>Robust Functional Principal Component Analysis for Non-Euclidean Random Objects</title>
      <link>https://arxiv.org/abs/2312.07741</link>
      <description>arXiv:2312.07741v2 Announce Type: replace 
Abstract: Functional data analysis offers a diverse toolkit of statistical methods tailored for analyzing samples of real-valued random functions. Recently, samples of time-varying random objects, such as time-varying networks, have been increasingly encountered in modern data analysis. These data structures represent elements within general metric spaces that lack local or global linear structures, rendering traditional functional data analysis methods inapplicable. Moreover, the existing methodology for time-varying random objects does not work well in the presence of outlying objects. In this paper, we propose a robust method for analysing time-varying random objects. Our method employs pointwise Fr\'{e}chet medians and then constructs pointwise distance trajectories between the individual time courses and the sample Fr\'{e}chet medians. This representation effectively transforms time-varying objects into functional data. A novel robust approach to functional principal component analysis based on a Winsorized U-statistic estimator of the covariance structure is introduced. The proposed robust analysis of these distance trajectories is able to identify key features of time-varying objects and is useful for downstream analysis. To illustrate the efficacy of our approach, numerical studies focusing on dynamic networks are conducted. The results indicate that the proposed method exhibits good all-round performance and surpasses the existing approach in terms of robustness, showcasing its superior performance in handling time-varying objects data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.07741v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jiazhen Xu, Andrew T. A. Wood, Tao Zou</dc:creator>
    </item>
    <item>
      <title>Bootstrap inference for linear regression between variables that are never jointly observed: application in in vivo experiments</title>
      <link>https://arxiv.org/abs/2403.00140</link>
      <description>arXiv:2403.00140v4 Announce Type: replace 
Abstract: In modern experimental science, there is a common problem of estimating the coefficients of a linear regression in a context where the variables of interest cannot be observed simultaneously. When there is a categorical variable that is observed on all statistical units, we consider two estimators of linear regression that take this additional information into account: an estimator based on moments and an estimator based on optimal transport theory. These estimators are shown to be consistent and asymptotically Gaussian under weak hypotheses. The asymptotic variance has no explicit expression, except in some special cases, for which reason a stratified bootstrap approach is developed to construct confidence intervals for the estimated parameters, whose consistency is also shown. A simulation study evaluating and comparing the finite sample performance of these estimators demonstrates the advantages of the bootstrap approach in several realistic scenarios. An application to in vivo experiments, conducted in the context of studying radio-induced adverse effects in mice, revealed important relationships between the biomarkers of interest that could not be identified with the considered naive approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00140v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Polina Arsenteva, Mohamed Amine Benadjaoud, Herv\'e Cardot</dc:creator>
    </item>
    <item>
      <title>Evaluating and Utilizing Surrogate Outcomes in Covariate-Adjusted Response-Adaptive Designs</title>
      <link>https://arxiv.org/abs/2408.02667</link>
      <description>arXiv:2408.02667v3 Announce Type: replace 
Abstract: Surrogate outcomes have long been studied as substitutes for long-term primary outcomes. However, current surrogate evaluation methods do not directly account for their benefits in updating treatment randomization probabilities in adaptive experiments that aim to learn and respond to treatment effect heterogeneity. In this context, surrogate outcomes can expedite updates to randomization probabilities and thus improve expected outcomes of newly-enrolled participants by enabling earlier detection of heterogeneous treatment effects. We introduce a novel approach for evaluating candidate surrogate outcomes that quantifies both of these benefits in sequential adaptive experiments. We also propose a new Covariate-Adjusted Response-Adaptive design that uses an Online-Superlearner to evaluate and adaptively select surrogate outcomes for updating treatment randomization probabilities during the trial. We further introduce a Targeted Maximum Likelihood Estimation method that addresses dependence in adaptively collected data and achieves asymptotic normality without parametric assumptions. Our design and estimation methods show robust performance in simulations, including those using real trial data. Overall, this framework not only provides a comprehensive way to quantify benefits and select among candidate surrogate outcomes, but also offers a general tool for evaluating various adaptive designs with inferences, providing insights into opportunities and costs of alternative designs that could have been implemented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02667v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenxin Zhang, Aaron Hudson, Maya Petersen, Mark van der Laan</dc:creator>
    </item>
    <item>
      <title>Just Ramp-up: Unleash the Potential of Regression-based Estimator for A/B Tests under Network Interference</title>
      <link>https://arxiv.org/abs/2410.12740</link>
      <description>arXiv:2410.12740v3 Announce Type: replace 
Abstract: Recent research in causal inference under network interference has explored various experimental designs and estimation techniques to address this issue. However, existing methods, which typically rely on single experiments, often reach a performance bottleneck and face limitations in handling diverse interference structures. In contrast, we propose leveraging multiple experiments to overcome these limitations. In industry, the use of sequential experiments, often known as the ramp-up process, where traffic to the treatment gradually increases, is common due to operational needs like risk management and cost control. Our approach shifts the focus from operational aspects to the statistical advantages of merging data from multiple experiments. By combining data from sequentially conducted experiments, we aim to estimate the global average treatment effect more effectively. In this paper, we begin by analyzing the bias and variance of the linear regression estimator for GATE under general linear network interference. We demonstrate that bias plays a dominant role in the bias-variance tradeoff and highlight the intrinsic bias reduction achieved by merging data from experiments with strictly different treatment proportions. Herein the improvement introduced by merging two steps of experimental data is essential. In addition, we show that merging more steps of experimental data is unnecessary under general linear interference, while it can become beneficial when nonlinear interference occurs. Furthermore, we look into a more advanced estimator based on graph neural networks. Through extensive simulation studies, we show that the regression-based estimator benefits remarkably from training on merged experiment data, achieving outstanding statistical performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12740v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qianyi Chen, Bo Li</dc:creator>
    </item>
    <item>
      <title>Correcting Annotator Bias in Training Data: Population-Aligned Instance Replication (PAIR)</title>
      <link>https://arxiv.org/abs/2501.06826</link>
      <description>arXiv:2501.06826v2 Announce Type: replace 
Abstract: Models trained on crowdsourced labels may not reflect broader population views, because those who work as annotators do not represent the population. We propose Population-Aligned Instance Replication (PAIR), a method to address bias caused by non-representative annotator pools. Using a simulation study of offensive language and hate speech, we create two types of annotators with different labeling tendencies and generate datasets with varying proportions of the types. We observe that models trained on unbalanced annotator pools show poor calibration compared to those trained on representative data. By duplicating labels from underrepresented annotator groups to match population proportions, PAIR reduces bias without collecting additional annotations. These results suggest that statistical techniques from survey research can improve model performance. We conclude with practical recommendations for improving the representativity of training data and model performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06826v2</guid>
      <category>stat.ME</category>
      <category>cs.CL</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stephanie Eckman, Bolei Ma, Christoph Kern, Rob Chew, Barbara Plank, Frauke Kreuter</dc:creator>
    </item>
    <item>
      <title>Exact Simulation of Longitudinal Data from Marginal Structural Models</title>
      <link>https://arxiv.org/abs/2502.07991</link>
      <description>arXiv:2502.07991v2 Announce Type: replace 
Abstract: Simulating longitudinal data from specified marginal structural models is a crucial but challenging task for evaluating causal inference methods and clinical trial design. While data generation typically proceeds in a fully conditional manner using structural equations according to a temporal ordering, it is difficult to ensure alignment between conditional distributions and the target marginal causal effects. This misalignment presents a fundamental challenge in simulating data that adheres to marginal structural model specifications. To address this, we propose a flexible and efficient algorithm for simulating longitudinal data that adheres exactly to a specified marginal structural model. Recognizing the importance of time-to-event outcomes in clinical research, we extend our approach to accommodate survival models. Compared to existing approaches, our method offers several advantages: it enables exact simulation from a known causal model rather than relying on approximations; avoids restrictive assumptions about the data-generating process; and remains computationally efficient by requiring only the evaluation of analytic functions. This last benefit contrasts with methods that use computationally intensive techniques such as Monte Carlo approximations or numerical integration. Through simulation studies replicating realistic scenarios, we validate the method's accuracy and utility. Our method will facilitate researchers in effectively simulating data with target causal structures for their specific scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07991v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xi Lin, Daniel de Vassimon Manela, Chase Mathis, Jens Magelund Tarp, Robin J. Evans</dc:creator>
    </item>
    <item>
      <title>Deep Computerized Adaptive Testing</title>
      <link>https://arxiv.org/abs/2502.19275</link>
      <description>arXiv:2502.19275v2 Announce Type: replace 
Abstract: Computerized adaptive tests (CATs) play a crucial role in educational assessment and diagnostic screening in behavioral health. Unlike traditional linear tests that administer a fixed set of pre-assembled items, CATs adaptively tailor the test to an examinee's latent trait level by selecting a smaller subset of items based on their previous responses. Existing CAT frameworks predominantly rely on item response theory (IRT) models with a single latent variable, a choice driven by both conceptual simplicity and computational feasibility. However, many real-world item response datasets exhibit complex, multi-factor structures, limiting the applicability of CATs in broader settings. In this work, we develop a novel CAT system that incorporates multivariate latent traits, building on recent advances in Bayesian sparse multivariate IRT. Our approach leverages direct sampling from the latent factor posterior distributions, significantly accelerating existing information-theoretic item selection criteria by eliminating the need for computationally intensive Markov Chain Monte Carlo (MCMC) simulations. Recognizing the potential sub-optimality of existing item selection rules, which are often based on myopic one-step-lookahead optimization of some information-theoretic criterion, we propose a double deep Q-learning algorithm to learn an optimal item selection policy. Through simulation and real-data studies, we demonstrate that our approach not only accelerates existing item selection methods but also highlights the potential of reinforcement learning in CATs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19275v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiguang Li, Robert Gibbons, Veronika Rockova</dc:creator>
    </item>
    <item>
      <title>Tensor PCA for Factor Models</title>
      <link>https://arxiv.org/abs/2212.12981</link>
      <description>arXiv:2212.12981v3 Announce Type: replace-cross 
Abstract: Modern empirical analysis often relies on high-dimensional panel datasets with non-negligible cross-sectional and time-series correlations. Factor models are natural for capturing such dependencies. A tensor factor model describes the $d$-dimensional panel as a sum of a reduced rank component and an idiosyncratic noise, generalizing traditional factor models for two-dimensional panels. We consider a tensor factor model corresponding to the notion of a reduced multilinear rank of a tensor. We show that for a strong factor model, a simple tensor principal component analysis algorithm is optimal for estimating factors and loadings. When the factors are weak, the convergence rate of simple TPCA can be improved with alternating least-squares iterations. We also provide inferential results for factors and loadings and propose the first test to select the number of factors. The new tools are applied to the problem of imputing missing values in a multidimensional panel of firm characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.12981v3</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrii Babii, Eric Ghysels, Junsu Pan</dc:creator>
    </item>
    <item>
      <title>BayesFLo: Bayesian fault localization of complex software systems</title>
      <link>https://arxiv.org/abs/2403.08079</link>
      <description>arXiv:2403.08079v2 Announce Type: replace-cross 
Abstract: Software testing is essential for the reliable development of complex software systems. A key step in software testing is fault localization, which uses test data to pinpoint failure-inducing combinations for further diagnosis. Existing fault localization methods have two key limitations: they (i) do not incorporate domain and/or structural knowledge from test engineers, and (ii) do not provide a probabilistic assessment of risk for potential root causes. Such methods can thus fail to confidently whittle down the combinatorial number of potential root causes in complex systems, resulting in prohibitively high testing costs. To address this, we propose a novel Bayesian fault localization framework called BayesFLo, which leverages a flexible Bayesian model for identifying potential root causes with probabilistic uncertainty. Using a carefully-specified prior on root cause probabilities, BayesFLo permits the integration of domain and structural knowledge via the principles of combination hierarchy and heredity, which capture the expected structure of failure-inducing combinations. We then develop new algorithms for efficient computation of posterior root cause probabilities, leveraging recent tools from integer programming and graph representations. Finally, we demonstrate the effectiveness of BayesFLo over existing methods in two fault localization case studies on the Traffic Alert and Collision Avoidance System and the JMP Easy DOE platform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08079v2</guid>
      <category>cs.SE</category>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yi Ji, Simon Mak, Ryan Lekivetz, Joseph Morgan</dc:creator>
    </item>
    <item>
      <title>Curb Your Attention: Causal Attention Gating for Robust Trajectory Prediction in Autonomous Driving</title>
      <link>https://arxiv.org/abs/2410.07191</link>
      <description>arXiv:2410.07191v2 Announce Type: replace-cross 
Abstract: Trajectory prediction models in autonomous driving are vulnerable to perturbations from non-causal agents whose actions should not affect the ego-agent's behavior. Such perturbations can lead to incorrect predictions of other agents' trajectories, potentially compromising the safety and efficiency of the ego-vehicle's decision-making process. Motivated by this challenge, we propose $\textit{Causal tRajecTory predICtion}$ $\textbf{(CRiTIC)}$, a novel model that utilizes a $\textit{Causal Discovery Network}$ to identify inter-agent causal relations over a window of past time steps. To incorporate discovered causal relationships, we propose a novel $\textit{Causal Attention Gating}$ mechanism to selectively filter information in the proposed Transformer-based architecture. We conduct extensive experiments on two autonomous driving benchmark datasets to evaluate the robustness of our model against non-causal perturbations and its generalization capacity. Our results indicate that the robustness of predictions can be improved by up to $\textbf{54%}$ without a significant detriment to prediction accuracy. Lastly, we demonstrate the superior domain generalizability of the proposed model, which achieves up to $\textbf{29%}$ improvement in cross-domain performance. These results underscore the potential of our model to enhance both robustness and generalization capacity for trajectory prediction in diverse autonomous driving domains. Further details can be found on our project page: https://ehsan-ami.github.io/critic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07191v2</guid>
      <category>cs.RO</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ehsan Ahmadi, Ray Mercurius, Soheil Alizadeh, Kasra Rezaee, Amir Rasouli</dc:creator>
    </item>
    <item>
      <title>Monitoring time to event in registry data using CUSUMs based on relative survival models</title>
      <link>https://arxiv.org/abs/2411.09353</link>
      <description>arXiv:2411.09353v3 Announce Type: replace-cross 
Abstract: An aspect of interest in surveillance of diseases is whether the survival time distribution changes over time. By following data in health registries over time, this can be monitored, either in real time or retrospectively. With relevant risk factors registered, these can be taken into account in the monitoring as well. A challenge in monitoring survival times based on registry data is that the information related to cause of death might either be missing or uncertain. To quantify the burden of disease in such cases, relative survival methods can be used, where the total hazard is modelled as the population hazard plus the excess hazard due to the disease.
  We propose a CUSUM procedure for monitoring for changes in the survival time distribution in cases where use of excess hazard models is relevant. The CUSUM chart is based on a survival log-likelihood ratio and extends previously suggested methods for monitoring of time to event data to the excess hazard setting. The procedure takes into account changes in the population risk over time, as well as changes in the excess hazard which is explained by observed covariates. Properties, challenges and an application to cancer registry data will be presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09353v3</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jimmy Huy Tran, Jan Terje Kval{\o}y, Hartwig K{\o}rner</dc:creator>
    </item>
    <item>
      <title>A Novel Hybrid Approach to Contraceptive Demand Forecasting: Integrating Point Predictions with Probabilistic Distributions</title>
      <link>https://arxiv.org/abs/2502.09685</link>
      <description>arXiv:2502.09685v2 Announce Type: replace-cross 
Abstract: Accurate demand forecasting is vital for ensuring reliable access to contraceptive products, supporting key processes like procurement, inventory, and distribution. However, forecasting contraceptive demand in developing countries presents challenges, including incomplete data, poor data quality, and the need to account for multiple geographical and product factors. Current methods often rely on simple forecasting techniques, which fail to capture demand uncertainties arising from these factors, warranting expert involvement. Our study aims to improve contraceptive demand forecasting by combining probabilistic forecasting methods with expert knowledge. We developed a hybrid model that combines point forecasts from domain-specific model with probabilistic distributions from statistical and machine learning approaches, enabling human input to fine-tune and enhance the system-generated forecasts. This approach helps address the uncertainties in demand and is particularly useful in resource-limited settings. We evaluate different forecasting methods, including time series, Bayesian, machine learning, and foundational time series methods alongside our new hybrid approach. By comparing these methods, we provide insights into their strengths, weaknesses, and computational requirements. Our research fills a gap in forecasting contraceptive demand and offers a practical framework that combines algorithmic and human expertise. Our proposed model can also be generalized to other humanitarian contexts with similar data patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09685v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Harsha Chamara Hewage, Bahman Rostami-Tabar, Aris Syntetos, Federico Liberatore, Glenn Milano</dc:creator>
    </item>
    <item>
      <title>Method for recovering data on unreported low-severity crashes</title>
      <link>https://arxiv.org/abs/2503.04529</link>
      <description>arXiv:2503.04529v2 Announce Type: replace-cross 
Abstract: Objective: Many low-severity crashes are not reported due to sampling criteria, introducing missing not at random (MNAR) bias. If not addressed, MNAR bias can lead to inaccurate safety analyses. This paper illustrates a statistical method to address such bias. Methods: We defined a custom probability distribution for the observed data as a product of an exponential population distribution and a logistic reporting function. We used modern Bayesian probabilistic programming techniques. Results: Using simulated data, we verified the correctness of the procedure. Applying it to real crash data, we estimated the {\Delta}v distribution for passenger vehicles involved in personal damage-only (PDO) rear-end crashes. We found that about 77% of cases are unreported. Conclusions: The method preserves the original data and it accounts well for uncertainty from both modeling assumptions and input data. It can improve safety assessments and it applies broadly to other MNAR cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04529v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alberto Morando</dc:creator>
    </item>
  </channel>
</rss>
