<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 18 Sep 2025 04:00:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Selective and marginal selective inference for exceptional groups</title>
      <link>https://arxiv.org/abs/2509.13538</link>
      <description>arXiv:2509.13538v1 Announce Type: new 
Abstract: Statistical analyses of multipopulation studies often use the data to select a particular population as the target of inference. For example, a confidence interval may be constructed for a population only in the event that its sample mean is larger than that of the other populations. We show that for the normal means model, confidence interval procedures that maintain strict coverage control conditional on such a selection event will have infinite expected width. For applications where such selective coverage control is of interest, this result motivates the development of procedures with finite expected width and approximate selective coverage control over a range of plausible parameter values. To this end, we develop selection-adjusted empirical Bayes confidence procedures that use information from the data to approximate an oracle confidence procedure that has exact selective coverage control and finite expected width. In numerical comparisons of the oracle and empirical Bayes procedures to procedures that only guarantee selective coverage control marginally over selection events, we find that improved selective coverage control comes at the cost of increased expected interval width.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13538v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peter Hoff, Surya Tokdar</dc:creator>
    </item>
    <item>
      <title>Robust, sub-Gaussian mean estimators in metric spaces</title>
      <link>https://arxiv.org/abs/2509.13606</link>
      <description>arXiv:2509.13606v1 Announce Type: new 
Abstract: Estimating the mean of a random vector from i.i.d. data has received considerable attention, and the optimal accuracy one may achieve with a given confidence is fairly well understood by now. When the data take values in more general metric spaces, an appropriate extension of the notion of the mean is the Fr\'echet mean. While asymptotic properties of the most natural Fr\'echet mean estimator (the empirical Fr\'echet mean) have been thoroughly researched, non-asymptotic performance bounds have only been studied recently.
  The aim of this paper is to study the performance of estimators of the Fr\'echet mean in general metric spaces under possibly heavy-tailed and contaminated data. In such cases, the empirical Fr\'echet mean is a poor estimator. We propose a general estimator based on high-dimensional extensions of trimmed means and prove general performance bounds. Unlike all previously established bounds, ours generalize the optimal bounds known for Euclidean data. The main message of the bounds is that, much like in the Euclidean case, the optimal accuracy is governed by two "variance" terms: a "global variance" term that is independent of the prescribed confidence, and a potentially much smaller, confidence-dependent "local variance" term.
  We apply our results for metric spaces with curvature bounded from below, such as Wasserstein spaces, and for uniformly convex Banach spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13606v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Bartl, Gabor Lugosi, Roberto Imbuzeiro Oliveira, Zoraida F. Rico</dc:creator>
    </item>
    <item>
      <title>Variable Selection for Additive Global Fr\'echet Regression</title>
      <link>https://arxiv.org/abs/2509.13685</link>
      <description>arXiv:2509.13685v1 Announce Type: new 
Abstract: We present a novel framework for variable selection in Fr\'echet regression with responses in general metric spaces, a setting increasingly relevant for analyzing non-Euclidean data such as probability distributions and covariance matrices. Building on the concept of (weak) Fr\'echet conditional means, we develop an additive regression model that represents the metric-based discrepancy of the response as a sum of covariate-specific nonlinear functions in reproducing kernel Hilbert spaces (RKHS). To address the absence of linear structure in the response space, we transform the response via squared distances, enabling an interpretable and tractable additive decomposition. Variable selection is performed using Elastic Net regularization, extended to the RKHS setting, and further refined through a local linear approximation scheme that incorporates folded concave penalties such as the SCAD. We establish theoretical guarantees, including variable selection consistency and the strong oracle property, under minimal assumptions tailored to metric-space-valued responses. Simulations and applications to distributional and matrix-valued data demonstrate the scalability, interpretability, and practical effectiveness of the proposed approach. This work provides a principled foundation for statistical learning with random object data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13685v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoyi Yang, Satarupa Bhattacharjee, Lingzhou Xue, Bing Li</dc:creator>
    </item>
    <item>
      <title>Optimal Transport Based Testing in Factorial Design</title>
      <link>https://arxiv.org/abs/2509.13970</link>
      <description>arXiv:2509.13970v1 Announce Type: new 
Abstract: We introduce a general framework for testing statistical hypotheses for probability measures supported on finite spaces, which is based on optimal transport (OT). These tests are inspired by the analysis of variance (ANOVA) and its nonparametric counterparts. They allow for testing linear relationships in factorial designs between discrete probability measures and are based on pairwise comparisons of the OT distance and corresponding barycenters. To this end, we derive under the null hypotheses and (local) alternatives the asymptotic distribution of empirical OT costs and the empirical OT barycenter cost functional as the optimal value of linear programs with random objective function. In particular, we extend existing techniques for probability to signed measures and show directional Hadamard differentiability and the validity of the functional delta method. We discuss computational issues, permutation and bootstrap tests, and back up our findings with simulations. We illustrate our methodology on two datasets from cellular biophysics and biometric identification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13970v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michel Groppe, Linus Niem\"oller, Shayan Hundrieser, David Ventzke, Anna Blob, Sarah K\"oster, Axel Munk</dc:creator>
    </item>
    <item>
      <title>Spacing Test for Fused Lasso</title>
      <link>https://arxiv.org/abs/2509.14229</link>
      <description>arXiv:2509.14229v1 Announce Type: new 
Abstract: This study addresses the unresolved problem of selecting the regularization parameter in the fused lasso. In particular, we extend the framework of the Spacing Test proposed by Tibshirani et al. to the fused lasso, providing a theoretical foundation for post-selection inference by characterizing the selection event as a polyhedral constraint. Based on the analysis of the solution path of the fused lasso using a LARS-type algorithm, we derive exact conditional $p$-values for the selected change-points. Our method broadens the applicability of the Spacing Test from the standard lasso to fused penalty structures. Furthermore, through numerical experiments comparing the proposed method with sequential versions of AIC and BIC as well as cross-validation, we demonstrate that the proposed approach properly controls the type I error while achieving high detection power. This work offers a theoretically sound and computationally practical solution for parameter selection and post-selection inference in structured signal estimation problems. Keywords: Fused Lasso, Regularization parameter selection, Spacing Test for Lasso, Selective inference, Change-point detection</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14229v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.TH</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rieko Tasaka, Tatsuya Kimura, Joe Suzuki</dc:creator>
    </item>
    <item>
      <title>Holdout cross-validation for large non-Gaussian covariance matrix estimation using Weingarten calculus</title>
      <link>https://arxiv.org/abs/2509.13923</link>
      <description>arXiv:2509.13923v1 Announce Type: cross 
Abstract: Cross-validation is one of the most widely used methods for model selection and evaluation; its efficiency for large covariance matrix estimation appears robust in practice, but little is known about the theoretical behavior of its error. In this paper, we derive the expected Frobenius error of the holdout method, a particular cross-validation procedure that involves a single train and test split, for a generic rotationally invariant multiplicative noise model, therefore extending previous results to non-Gaussian data distributions. Our approach involves using the Weingarten calculus and the Ledoit-P\'ech\'e formula to derive the oracle eigenvalues in the high-dimensional limit. When the population covariance matrix follows an inverse Wishart distribution, we approximate the expected holdout error, first with a linear shrinkage, then with a quadratic shrinkage to approximate the oracle eigenvalues. Under the linear approximation, we find that the optimal train-test split ratio is proportional to the square root of the matrix dimension. Then we compute Monte Carlo simulations of the holdout error for different distributions of the norm of the noise, such as the Gaussian, Student, and Laplace distributions and observe that the quadratic approximation yields a substantial improvement, especially around the optimal train-test split ratio. We also observe that a higher fourth-order moment of the Euclidean norm of the noise vector sharpens the holdout error curve near the optimal split and lowers the ideal train-test ratio, making the choice of the train-test ratio more important when performing the holdout method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13923v1</guid>
      <category>q-fin.ST</category>
      <category>math.ST</category>
      <category>q-fin.RM</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lamia Lamrani, Beno\^it Collins, Jean-Philippe Bouchaud</dc:creator>
    </item>
    <item>
      <title>Quickest Change Detection with Cost-Constrained Experiment Design</title>
      <link>https://arxiv.org/abs/2509.14186</link>
      <description>arXiv:2509.14186v1 Announce Type: cross 
Abstract: In the classical quickest change detection problem, an observer performs only one experiment to monitor a stochastic process. This paper considers the case where, at each observation time, the decision-maker needs to choose between multiple experiments with different information qualities and costs. The goal is to minimize the worst-case average detection delay subject to false alarm and cost constraints. An algorithm called the 2E-CUSUM Algorithm has been developed to achieve this goal for the two-experiment case. Extensions to multiple-experiment designs are also studied, and 2E-CUSUM is extended accordingly. Data efficiency, where the observer has the choice not to perform an experiment, is explored as well. The proposed algorithms are analyzed and shown to be asymptotically optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14186v1</guid>
      <category>eess.SP</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patrick Vincent N. Lubenia, Taposh Banerjee</dc:creator>
    </item>
    <item>
      <title>Adaptive Off-Policy Inference for M-Estimators Under Model Misspecification</title>
      <link>https://arxiv.org/abs/2509.14218</link>
      <description>arXiv:2509.14218v1 Announce Type: cross 
Abstract: When data are collected adaptively, such as in bandit algorithms, classical statistical approaches such as ordinary least squares and $M$-estimation will often fail to achieve asymptotic normality. Although recent lines of work have modified the classical approaches to ensure valid inference on adaptively collected data, most of these works assume that the model is correctly specified. We propose a method that provides valid inference for M-estimators that use adaptively collected bandit data with a (possibly) misspecified working model. A key ingredient in our approach is the use of flexible machine learning approaches to stabilize the variance induced by adaptive data collection. A major novelty is that our procedure enables the construction of valid confidence sets even in settings where treatment policies are unstable and non-converging, such as when there is no unique optimal arm and standard bandit algorithms are used. Empirical results on semi-synthetic datasets constructed from the Osteoarthritis Initiative demonstrate that the method maintains type I error control, while existing methods for inference in adaptive settings do not cover in the misspecified case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14218v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Leiner, Robin Dunn, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Maximum Likelihood Estimation of the Parameters of Matrix Variate Symmetric Laplace Distribution</title>
      <link>https://arxiv.org/abs/2502.04118</link>
      <description>arXiv:2502.04118v2 Announce Type: replace 
Abstract: This paper considers an extension of the multivariate symmetric Laplace distribution to matrix variate case. The symmetric Laplace distribution is a scale mixture of normal distribution. The maximum likelihood estimators (MLE) of the parameters of multivariate and matrix variate symmetric Laplace distribution are proposed, which are not explicitly obtainable, as the density function involves the modified Bessel function of the third kind. Thus, the EM algorithm is applied to find the maximum likelihood estimators. The parameters and their maximum likelihood estimators of matrix variate symmetric Laplace distribution are defined up to a positive multiplicative constant with their Kronecker product uniquely defined. The condition for the existence of the MLE is given, and the stability of the estimators is discussed. The empirical bias and the dispersion of the Kronecker product of the estimators for different sample sizes are discussed using simulated data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04118v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pooja Yadav, Tanuja Srivastava</dc:creator>
    </item>
    <item>
      <title>Optimal Data Splitting for Holdout Cross-Validation in Large Covariance Matrix Estimation</title>
      <link>https://arxiv.org/abs/2503.15186</link>
      <description>arXiv:2503.15186v2 Announce Type: replace 
Abstract: Cross-validation is a statistical tool that can be used to improve large covariance matrix estimation. Although its efficiency is observed in practical applications and a convergence result towards the error of the non linear shrinkage is available in the high-dimensional regime, formal proofs that take into account the finite sample size effects are currently lacking. To carry on analytical analysis, we focus on the holdout method, a single iteration of cross-validation, rather than the traditional $k$-fold approach. We derive a closed-form expression for the expected estimation error when the population matrix follows a white inverse Wishart distribution, and we observe the optimal train-test split scales as the square root of the matrix dimension. For general population matrices, we connected the error to the variance of eigenvalues distribution, but approximations are necessary. In this framework and in the high-dimensional asymptotic regime, both the holdout and $k$-fold cross-validation methods converge to the optimal estimator when the train-test ratio scales with the square root of the matrix dimension which is coherent with the existing theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15186v2</guid>
      <category>math.ST</category>
      <category>q-fin.PM</category>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lamia Lamrani, Christian Bongiorno, Marc Potters</dc:creator>
    </item>
    <item>
      <title>Bayesian Model Averaging in Causal Instrumental Variable Models</title>
      <link>https://arxiv.org/abs/2504.13520</link>
      <description>arXiv:2504.13520v4 Announce Type: replace-cross 
Abstract: Instrumental variables are a popular tool to infer causal effects under unobserved confounding, but choosing suitable instruments is challenging in practice. We propose gIVBMA, a Bayesian model averaging procedure that addresses this challenge by averaging across different sets of instrumental variables and covariates in a structural equation model. Our approach extends previous work through a scale-invariant prior structure and accommodates non-Gaussian outcomes and treatments, offering greater flexibility than existing methods. The computational strategy uses conditional Bayes factors to update models separately for the outcome and treatments. We prove that this model selection procedure is consistent. By explicitly accounting for model uncertainty, gIVBMA allows instruments and covariates to switch roles and provides robustness against invalid instruments. In simulation experiments, gIVBMA outperforms current state-of-the-art methods. We demonstrate its usefulness in two empirical applications: the effects of malaria and institutions on income per capita and the returns to schooling. A software implementation of gIVBMA is available in Julia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13520v4</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gregor Steiner, Mark Steel</dc:creator>
    </item>
    <item>
      <title>A transport approach to the cutoff phenomenon</title>
      <link>https://arxiv.org/abs/2509.08560</link>
      <description>arXiv:2509.08560v2 Announce Type: replace-cross 
Abstract: Substantial progress has recently been made in the understanding of the cutoff phenomenon for Markov processes, using an information-theoretic statistics known as varentropy [Sal23; Sal24; Sal25a; PS25]. In the present paper, we propose an alternative approach which bypasses the use of varentropy and exploits instead a new W-TV transport inequality, combined with a classical parabolic regularization estimate [BGL01; OV01]. While currently restricted to non-negatively curved processes on smooth spaces, our argument no longer requires the chain rule, nor any approximate version thereof. As applications, we recover the main result of [Sal25a] establishing cutoff for the log-concave Langevin dynamics, and extend the conclusion to a widely-used discrete-time sampling algorithm known as the Proximal Sampler.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08560v2</guid>
      <category>math.PR</category>
      <category>math.AP</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francesco Pedrotti, Justin Salez</dc:creator>
    </item>
    <item>
      <title>A Kernel-based Stochastic Approximation Framework for Nonlinear Operator Learning</title>
      <link>https://arxiv.org/abs/2509.11070</link>
      <description>arXiv:2509.11070v2 Announce Type: replace-cross 
Abstract: We develop a stochastic approximation framework for learning nonlinear operators between infinite-dimensional spaces utilizing general Mercer operator-valued kernels. Our framework encompasses two key classes: (i) compact kernels, which admit discrete spectral decompositions, and (ii) diagonal kernels of the form $K(x,x')=k(x,x')T$, where $k$ is a scalar-valued kernel and $T$ is a positive operator on the output space. This broad setting induces expressive vector-valued reproducing kernel Hilbert spaces (RKHSs) that generalize the classical $K=kI$ paradigm, thereby enabling rich structural modeling with rigorous theoretical guarantees. To address target operators lying outside the RKHS, we introduce vector-valued interpolation spaces to precisely quantify misspecification error. Within this framework, we establish dimension-free polynomial convergence rates, demonstrating that nonlinear operator learning can overcome the curse of dimensionality. The use of general operator-valued kernels further allows us to derive rates for intrinsically nonlinear operator learning, going beyond the linear-type behavior inherent in diagonal constructions of $K=kI$. Importantly, this framework accommodates a wide range of operator learning tasks, ranging from integral operators such as Fredholm operators to architectures based on encoder-decoder representations. Moreover, we validate its effectiveness through numerical experiments on the two-dimensional Navier-Stokes equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11070v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.FA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jia-Qi Yang, Lei Shi</dc:creator>
    </item>
  </channel>
</rss>
