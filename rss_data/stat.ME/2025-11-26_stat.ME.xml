<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 27 Nov 2025 02:40:49 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Clarifying identification and estimation of treatment effects in the Sequential Parallel Comparison Design</title>
      <link>https://arxiv.org/abs/2511.19677</link>
      <description>arXiv:2511.19677v1 Announce Type: new 
Abstract: Sequential parallel comparison design (SPCD) clinical trials aim to adjust active treatment effect estimates for placebo response to minimize the impact of placebo responders on the estimates. This is potentially accomplished using a two stage design by measuring treatment effects among all participants during the first stage, then classifying some placebo arm participants as placebo non-responders who will be re-randomized in the second stage. In this paper, we use causal inference tools to clarify under what assumptions treatment effects can be identified in SPCD trials and what effects the conventional estimators target at each stage of the SPCD trial. We further illustrate the highly influential impact of placebo response misclassification on the second stage estimate. We conclude that the conventional SPCD estimators do not target meaningful treatment effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19677v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Stockton, Michele Santacatterina, Soutrik Mandal, Charles M. Cleland, Erinn M. Hade, Nicholas Illenberger, Sharon Meropol, Andrea B. Troxel, Eva Petkova, Chang Yu, Thaddeus Tarpey</dc:creator>
    </item>
    <item>
      <title>Integrating RCTs, RWD, AI/ML and Statistics: Next-Generation Evidence Synthesis</title>
      <link>https://arxiv.org/abs/2511.19735</link>
      <description>arXiv:2511.19735v1 Announce Type: new 
Abstract: Randomized controlled trials (RCTs) have been the cornerstone of clinical evidence; however, their cost, duration, and restrictive eligibility criteria limit power and external validity. Studies using real-world data (RWD), historically considered less reliable for establishing causality, are now recognized to be important for generating real-world evidence (RWE). In parallel, artificial intelligence and machine learning (AI/ML) are being increasingly used throughout the drug development process, providing scalability and flexibility but also presenting challenges in interpretability and rigor that traditional statistics do not face. This Perspective argues that the future of evidence generation will not depend on RCTs versus RWD, or statistics versus AI/ML, but on their principled integration. To this end, a causal roadmap is needed to clarify inferential goals, make assumptions explicit, and ensure transparency about tradeoffs. We highlight key objectives of integrative evidence synthesis, including transporting RCT results to broader populations, embedding AI-assisted analyses within RCTs, designing hybrid controlled trials, and extending short-term RCTs with long-term RWD. We also outline future directions in privacy-preserving analytics, uncertainty quantification, and small-sample methods. By uniting statistical rigor with AI/ML innovation, integrative approaches can produce robust, transparent, and policy-relevant evidence, making them a key component of modern regulatory science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19735v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shu Yang, Margaret Gamalo, Haoda Fu</dc:creator>
    </item>
    <item>
      <title>Order Selection in Vector Autoregression by Mean Square Information Criterion</title>
      <link>https://arxiv.org/abs/2511.19761</link>
      <description>arXiv:2511.19761v1 Announce Type: new 
Abstract: Vector autoregressive (VAR) processes are ubiquitously used in economics, finance, and biology. Order selection is an essential step in fitting VAR models. While many order selection methods exist, all come with weaknesses. Order selection by minimizing AIC is a popular approach but is known to consistently overestimate the true order for processes of small dimension. On the other hand, methods based on BIC or the Hannan-Quinn (HQ) criteria are shown to require large sample sizes in order to accurately estimate the order for larger-dimensional processes. We propose the mean square information criterion (MIC) based on the observation that the expected squared error loss is flat once the fitted order reaches or exceeds the true order. MIC is shown to consistently estimate the order of the process under relatively mild conditions. Our simulation results show that MIC offers better performance relative to AIC, BIC, and HQ under misspecification. This advantage is corroborated when forecasting COVID-19 outcomes in New York City. Order selection by MIC is implemented in the micvar R package available on CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19761v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Hellstern, Ali Shojaie</dc:creator>
    </item>
    <item>
      <title>Differentially Private Computation of the Gini Index for Income Inequality</title>
      <link>https://arxiv.org/abs/2511.19771</link>
      <description>arXiv:2511.19771v1 Announce Type: new 
Abstract: The Gini index is a widely reported measure of income inequality. In some settings, the underlying data used to compute the Gini index are confidential. The organization charged with reporting the Gini index may be concerned that its release could leak information about the underlying data. We present an approach for bounding this information leakage by releasing a differentially private version of the Gini index. In doing so, we analyze how adding, deleting, or altering a single observation in any specific dataset can affect the computation of the Gini index; this is known as the local sensitivity. We then derive a smooth upper bound on the local sensitivity. Using this bound, we define a mechanism that adds noise to the Gini index, thereby satisfying differential privacy. Using simulated and genuine income data, we show that the mechanism can reduce the errors from noise injection substantially relative to differentially private algorithms that rely on the global sensitivity, that is, the maximum of the local sensitivities over all possible datasets. We characterize settings where using smooth sensitivity can provide highly accurate estimates, as well as settings where the noise variance is simply too large to provide reliably useful results. We also present a Bayesian post-processing step that provides interval estimates about the value of the Gini index computed with the confidential data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19771v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenjie Lan, Jerome P. Reiter</dc:creator>
    </item>
    <item>
      <title>Threshold Tensor Factor Model in CP Form</title>
      <link>https://arxiv.org/abs/2511.19796</link>
      <description>arXiv:2511.19796v1 Announce Type: new 
Abstract: This paper proposes a new Threshold Tensor Factor Model in Canonical Polyadic (CP) form for tensor time series. By integrating a thresholding autoregressive structure for the latent factor process into the tensor factor model in CP form, the model captures regime-switching dynamics in the latent factor processes while retaining the parsimony and interpretability of low-rank tensor representations. We develop estimation procedures for the model and establish the theoretical properties of the resulting estimators. Numerical experiments and a real-data application illustrate the practical performance and usefulness of the proposed framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19796v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Stevenson Bolivar, Rong Chen, Yuefeng Han</dc:creator>
    </item>
    <item>
      <title>Dependence-Aware False Discovery Rate Control in Two-Sided Gaussian Mean Testing</title>
      <link>https://arxiv.org/abs/2511.19960</link>
      <description>arXiv:2511.19960v1 Announce Type: new 
Abstract: This paper develops a general framework for controlling the false discovery rate (FDR) in multiple testing of Gaussian means against two-sided alternatives. The widely used Benjamini-Hochberg (BH) procedure provides exact FDR control under independence or conservative control under specific one-sided dependence structures, but its validity for correlated two-sided tests has remained an open question. We introduce the notion of positive left-tail dependence under the null (PLTDN), extending classical dependence assumptions to two-sided settings, and show that it ensures valid FDR control for BH-type procedures. Building on this framework, we propose a family of generalized shifted BH (GSBH) methods that incorporate correlation information through simple p-value adjustments. Simulation results demonstrate reliable FDR control and improved power across a range of dependence structures, while an application to an HIV gene expression dataset illustrates the practical effectiveness of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19960v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Deepra Ghosh, Sanat K. Sarkar</dc:creator>
    </item>
    <item>
      <title>Hierarchical Causal Structure Learning</title>
      <link>https://arxiv.org/abs/2511.20021</link>
      <description>arXiv:2511.20021v1 Announce Type: new 
Abstract: Traditional statistical approaches primarily aim to model associations between variables, but many scientific and practical questions require causal methods instead. These approaches rely on assumptions about an underlying structure, often represented by a directed acyclic graph (DAG). When all variables are measured at the same level, causal structures can be learned using existing techniques. However, no suitable methods exist when data are organized hierarchically or across multiple levels. This paper addresses such cases, where both unit-level and group-level variables are present. These multi-level structures frequently arise in fields such as agriculture, where plants (units) grow within different environments (groups). Building on nonlinear structural causal models, or additive noise models, we propose a method that accommodates unobserved confounders as well as group-specific causal functions. The approach is implemented in the R package HSCM, available at https://CRAN.R-project.org/package=HSCM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20021v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sjoerd Hermes, Joost van Heerwaarden, Fred van Eeuwijk, Pariya Behrouzi</dc:creator>
    </item>
    <item>
      <title>Rectangular augmented row-column designs generated from contractions</title>
      <link>https://arxiv.org/abs/2511.20052</link>
      <description>arXiv:2511.20052v1 Announce Type: new 
Abstract: Row-column designs play an important role in applications where two orthogonal sources of error need to be controlled for by blocking. Field or greenhouse experiments, in which experimental units are arranged as a rectangular array of experimental units are a prominent example. In plant breeding, the amount of seed available for the treatments to be tested may be so limited that only one experimental unit per treatment can be accommodated. In such settings, augmented designs become an interesting option, where a small set of treatments, for which sufficient seed is available, are replicated across the rectangular layout so that row and column effects, as well as the error variance can be estimated. Here, we consider the use of an auxiliary design, also known as a contraction, to generate an augmented row-column design. We make use of the fact that the efficiency factors of the contraction and the associated augmented design are closely interlinked. A major advantage of this approach is that an efficient contraction can be found by computer search at much higher computational speed than is required for direct search for an efficient augmented design. Two examples are used to illustrate the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20052v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hans-Peter Piepho, Emlyn Williams</dc:creator>
    </item>
    <item>
      <title>A Generalized Additive Partial-Mastery Cognitive Diagnosis Model</title>
      <link>https://arxiv.org/abs/2511.20191</link>
      <description>arXiv:2511.20191v1 Announce Type: new 
Abstract: Cognitive diagnosis models (CDMs) are restricted latent class models widely used for measuring attributes of interest in diagnostic assessments in education, psychology, biomedical sciences, and related fields. Partial-mastery CDMs (PM-CDMs) are an important extension of CDMs. They model individuals' status for each attribute to be continuous for measuring the partial mastery level, which relaxes the restrictive discrete-attribute assumption of classical CDMs. As a result, PM-CDMs often yield better fits for real-world data and refined measurement of the substantive attributes of interest. However, these models inherit some strong parametric assumptions from the traditional CDMs about the item response functions and, thus, still suffer from a significant risk of model misspecification. This paper proposes a generalized additive PM-CDM (GaPM-CDM) that substantially relaxes the parametric assumptions of PM-CDMs. This proposal leverages model parsimony and interpretability by modeling each item response function as a mixture of nonparametric monotone functions of attributes. A method for the estimation of GaPM-CDM is developed, which combines the marginal maximum likelihood estimator with a sieve approximation of the nonparametric functions. The new model is applicable under both confirmatory and exploratory settings, depending on whether prior knowledge is available about the relationship between observed variables and attributes. The proposed method is applied to two measurement problems from educational testing and healthcare research, respectively, and further evaluated and compared with PM-CDMs through extensive simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20191v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Camilo C\'ardenas-Hurtado, Yunxiao Chen, Irini Moustaki</dc:creator>
    </item>
    <item>
      <title>Wilcoxon-Mann-Whitney Test of No Group Discrimination</title>
      <link>https://arxiv.org/abs/2511.20308</link>
      <description>arXiv:2511.20308v1 Announce Type: new 
Abstract: The traditional WMW null hypothesis $H_0: F = G$ is erroneously too broad. WMW actually tests narrower $H_0: AUC = 0.5$. Asymptotic distribution of the standardized $U$ statistic (i.e., the empirical AUC) under the correct $H_0$ is derived along with finite sample bias corrections. The traditional alternative hypothesis of stochastic dominance is too narrow. WMW is consistent against $H_1: AUC \neq 0.5$, as established by Van Dantzig in 1951.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20308v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marian Grendar</dc:creator>
    </item>
    <item>
      <title>Pseudo-strata learning via maximizing misclassification reward</title>
      <link>https://arxiv.org/abs/2511.20318</link>
      <description>arXiv:2511.20318v1 Announce Type: new 
Abstract: Online advertising aims to increase user engagement and maximize revenue, but users respond heterogeneously to ad exposure. Some users purchase only when exposed to ads, while others purchase regardless of exposure, and still others never purchase. This heterogeneity can be characterized by latent response types, commonly referred to as principal strata, defined by users' joint potential outcomes under exposure and non-exposure. However, users' true strata are unobserved, making direct analysis infeasible. In this article, instead of learning the true strata, we propose a novel approach that learns users' pseudo-strata by leveraging information from an outcome (revenue) observed after the response (purchase). We construct pseudo-strata to classify users and introduce misclassification rewards to quantify the expected revenue gain of pseudo-strata-based policies relative to true strata. Within a Bayesian classification framework, we learn the pseudo-strata by optimizing the expected revenue. To implement these procedures, we introduce identification assumptions and estimation methods, and establish their large-sample properties. Simulation studies show that the proposed method achieves more accurate strata classification and substantially higher revenue than baselines. We further illustrate the method using a large-scale industrial dataset from the Criteo Predictive Search Platform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20318v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shanshan Luo, Peng Wu, Zhi Geng</dc:creator>
    </item>
    <item>
      <title>A novel multi-exposure-to-multi-mediator mediation model for imaging genetic study of brain disorders</title>
      <link>https://arxiv.org/abs/2511.20412</link>
      <description>arXiv:2511.20412v1 Announce Type: new 
Abstract: Common psychiatric and brain disorders are highly heritable and affected by a number of genetic risk factors, yet the mechanism by which these genetic factors contribute to the disorders through alterations in brain structure and function remain poorly understood. Contemporary imaging genetic studies integrate genetic and neuroimaging data to investigate how genetic variation contributes to brain disorders via intermediate neuroimaging endophenotypes. However, the large number of potential exposures (genes) and mediators (neuroimaging features) pose new challenges to the traditional mediation analysis. In this paper, we propose a novel multi-exposure-to-multi-mediator mediation model that integrates genetic, neuroimaging and phenotypic data to investigate the "geneneuroimaging-brain disorder" mediation pathway. Our method jointly reduces the dimensions of exposures and mediators into low-dimensional aggregators where the mediation effect is maximized. We further introduce sparsity into the loadings to improve the interpretability. To target the bi-convex optimization problem, we implement an efficient alternating direction method of multipliers algorithm with block coordinate updates. We provide theoretical guarantees for the convergence of our algorithm and establish the asymptotic properties of the resulting estimators. Through extensive simulations, we demonstrate that our method outperforms other competing methods in recovering true loadings and true mediation proportions across a wide range of signal strengths, noise levels, and correlation structures. We further illustrate the utility of the method through a mediation analysis that integrates genetic, brain functional connectivity and smoking behavior data from UK Biobank, and identifies critical genes that impact nicotine dependence via changing the functional connectivity in specific brain regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20412v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Neng Wang, Eric V. Slud, Tianzhou Ma</dc:creator>
    </item>
    <item>
      <title>Extrapolating into the Extremes with Minimum Distance Estimation</title>
      <link>https://arxiv.org/abs/2511.20466</link>
      <description>arXiv:2511.20466v1 Announce Type: new 
Abstract: Understanding complex dependencies and extrapolating beyond observations are key challenges in modeling environmental space-time extremes. To address this, we introduce a simplifying approach that projects a wide range of multivariate exceedance problems onto a univariate peaks-over-threshold problem. In this framework, an estimator is computed by minimizing the $L_2$-distance between the empirical distribution function of the data and the theoretical distribution of the model. Asymptotic properties of this estimator are derived and validated in a simulation study. We evaluated our estimator in the EVA (2025) conference Data Challenge as part of Team Bochum's submission. The challenge provided precipitation data from four runs of LENS2, an ensemble of long-term weather simulations, on a $5 \times 5$ grid of locations centered at the grid point closest to Asheville, NC. Our estimator achieved a top-three rank in two of six competitive categories and won the overall preliminary challenge against ten competing teams.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20466v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexis Boulin, Erik Haufs</dc:creator>
    </item>
    <item>
      <title>Big Wins, Small Net Gains: Direct and Spillover Effects of First Industry Entries in Puerto Rico</title>
      <link>https://arxiv.org/abs/2511.19469</link>
      <description>arXiv:2511.19469v1 Announce Type: cross 
Abstract: I study how first sizable industry entries reshape local and neighboring labor markets in Puerto Rico. Using over a decade of quarterly municipality--industry data (2014Q1--2025Q1), I identify ``first sizable entries'' as large, persistent jumps in establishments, covered employment, and wage bill, and treat these as shocks to local industry presence at the municipio--industry level. Methodologically, I combine staggered-adoption difference-in-differences estimators that are robust to heterogeneous treatment timing with an imputation-based event-study approach, and I use a doubly robust difference-in-differences framework that explicitly allows for interference through pre-specified exposure mappings on a contiguity graph. The estimates show large and persistent direct gains in covered employment and wage bill in the treated municipality--industry cells over 0--16 quarters. Same-industry neighbors experience sizable short-run gains that reverse over the medium run, while within-municipality cross-industry and neighbor all-industries spillovers are small and imprecisely estimated. Once these spillovers are taken into account and spatially robust inference and sensitivity checks are applied, the net regional 0--16 quarter effect on covered employment is positive but modest in magnitude and estimated with considerable uncertainty. The results imply that first sizable entries generate substantial local gains where they occur, but much smaller and less precisely measured net employment gains for the broader regional economy, highlighting the importance of accounting for spatial spillovers when evaluating place-based policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19469v1</guid>
      <category>econ.GN</category>
      <category>econ.EM</category>
      <category>q-fin.EC</category>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jorge A. Arroyo</dc:creator>
    </item>
    <item>
      <title>RFX: High-Performance Random Forests with GPU Acceleration and QLORA Compression</title>
      <link>https://arxiv.org/abs/2511.19493</link>
      <description>arXiv:2511.19493v1 Announce Type: cross 
Abstract: RFX (Random Forests X), where X stands for compression or quantization, presents a production-ready implementation of Breiman and Cutler's Random Forest classification methodology in Python. RFX v1.0 provides complete classification: out-of-bag error estimation, overall and local importance measures, proximity matrices with QLORA compression, case-wise analysis, and interactive visualization (rfviz)--all with CPU and GPU acceleration. Regression, unsupervised learning, CLIQUE importance, and RF-GAP proximity are planned for v2.0.
  This work introduces four solutions addressing the proximity matrix memory bottleneck limiting Random Forest analysis to ~60,000 samples: (1) QLORA (Quantized Low-Rank Adaptation) compression for GPU proximity matrices, reducing memory from 80GB to 6.4MB for 100k samples (12,500x compression with INT8 quantization) while maintaining 99% geometric structure preservation, (2) CPU TriBlock proximity--combining upper-triangle storage with block-sparse thresholding--achieving 2.7x memory reduction with lossless quality, (3) SM-aware GPU batch sizing achieving 95% GPU utilization, and (4) GPU-accelerated 3D MDS visualization computing embeddings directly from low-rank factors using power iteration.
  Validation across four implementation modes (GPU/CPU x case-wise/non-case-wise) demonstrates correct implementation. GPU achieves 1.4x speedup over CPU for overall importance with 500+ trees. Proximity computation scales from 1,000 to 200,000+ samples (requiring GPU QLORA), with CPU TriBlock filling the gap for medium-scale datasets (10K-50K samples). RFX v1.0 eliminates the proximity memory bottleneck, enabling proximity-based Random Forest analysis on datasets orders of magnitude larger than previously feasible. Open-source production-ready classification following Breiman and Cutler's original methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19493v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chris Kuchar</dc:creator>
    </item>
    <item>
      <title>Beyond the ACE Score: Replicable Combinations of Adverse Childhood Experiences That Worsen Depression Risk</title>
      <link>https://arxiv.org/abs/2511.19574</link>
      <description>arXiv:2511.19574v1 Announce Type: cross 
Abstract: Adverse childhood experiences (ACEs) are categories of childhood abuse, neglect, and household dysfunction. Screening by a single additive ACE score (e.g., a $\ge 4$ cutoff) has poor individual-level discrimination. We instead identify replicable combinations of ACEs that elevate adult depression risk. Our data turnover framework enables a single research team to explore, confirm, and replicate within one observational dataset while controlling the family-wise error rate. We integrate isotonic subgroup selection (ISS) to estimate a higher-risk subgroup under a monotonicity assumption -- additional ACE exposure or higher intensity cannot reduce depression risk. We pre-specify a risk threshold $\tau$ corresponding to roughly a two-fold increase in the odds of depression relative to the no-ACE baseline. Within data turnover, the prespecified component improves power while maintaining FWER control, as demonstrated in simulations. Guided by EDA, we adopt frequency coding for ACE items, retaining intensity information that reduces false positives relative to binary or score codings. The result is a replicable, pattern-based higher-risk subgroup. On held-out BRFSS 2022, we show that, at the same level of specificity (0.95), using our replicable subgroup as the screening rule increases sensitivity by 26\% compared with an ACE-score cutoff, yielding concrete triggers that are straightforward to implement and help target scarce clinical screening resources toward truly higher-risk profiles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19574v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruizhe Zhang, Jooyoung Kong, Dylan S. Small, William Bekerman</dc:creator>
    </item>
    <item>
      <title>Anchoring Convenience Survey Samples to a Baseline Census for Vaccine Coverage Monitoring in Global Health</title>
      <link>https://arxiv.org/abs/2511.19742</link>
      <description>arXiv:2511.19742v1 Announce Type: cross 
Abstract: While conducting probabilistic surveys is the gold standard for assessing vaccine coverage, implementing these surveys poses challenges for global health. There is a need for more convenient option that is more affordable and practical. Motivated by childhood vaccine monitoring programs in rural areas of Chad and Niger, we conducted a simulation study to evaluate calibration-weighted design-based and logistic regression-based imputation estimators of the finite-population proportion of MCV1 coverage. These estimators use a hybrid approach that anchors non-probabilistic follow-up survey to probabilistic baseline census to account for selection bias. We explored varying degrees of non-ignorable selection bias (odds ratios from 1.0-1.5), percentage of villages sampled (25-75%), and village-level survey response rate to the follow-up survey (50-80%). Our performance metrics included bias, coverage, and proportion of simulated 95% confidence intervals falling within equivalence margins of 5% and 7.5% (equivalence tolerance). For both adjustment methods, the performance worsened with higher selection bias and lower response rate and generally improved as a larger proportion of villages was sampled. Under the worst scenario with 1.5 OR, 25% village sampled, and 50% survey response rate, both methods showed empirical biases of 2.1% or less, below 95% coverage, and low equivalence tolerances. In more realistic scenarios, the performance of our estimators showed lower biases and close to 95% coverage. For example, at OR$\leq$1.2, both methods showed high performance, except at the lowest village sampling and participation rates. Our simulations show that a hybrid anchoring survey approach is a feasible survey option for vaccine monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19742v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathaniel Dyrkton, Shomoita Alam, Susan Shepherd, Ibrahim Sana, Kevin Phelan, Jay JH Park</dc:creator>
    </item>
    <item>
      <title>Clustering Approaches for Mixed-Type Data: A Comparative Study</title>
      <link>https://arxiv.org/abs/2511.19755</link>
      <description>arXiv:2511.19755v1 Announce Type: cross 
Abstract: Clustering is widely used in unsupervised learning to find homogeneous groups of observations within a dataset. However, clustering mixed-type data remains a challenge, as few existing approaches are suited for this task. This study presents the state-of-the-art of these approaches and compares them using various simulation models. The compared methods include the distance-based approaches k-prototypes, PDQ, and convex k-means, and the probabilistic methods KAy-means for MIxed LArge data (KAMILA), the mixture of Bayesian networks (MBNs), and latent class model (LCM). The aim is to provide insights into the behavior of different methods across a wide range of scenarios by varying some experimental factors such as the number of clusters, cluster overlap, sample size, dimension, proportion of continuous variables in the dataset, and clusters' distribution. The degree of cluster overlap and the proportion of continuous variables in the dataset and the sample size have a significant impact on the observed performances. When strong interactions exist between variables alongside an explicit dependence on cluster membership, none of the evaluated methods demonstrated satisfactory performance. In our experiments KAMILA, LCM, and k-prototypes exhibited the best performance, with respect to the adjusted rand index (ARI). All the methods are available in R.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19755v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1155/jpas/2242100</arxiv:DOI>
      <arxiv:journal_reference>Journal of Probability and Statistics, 2025</arxiv:journal_reference>
      <dc:creator>Badih Ghattas, Alvaro Sanchez San-Benito</dc:creator>
    </item>
    <item>
      <title>Sigmoid-FTRL: Design-Based Adaptive Neyman Allocation for AIPW Estimators</title>
      <link>https://arxiv.org/abs/2511.19905</link>
      <description>arXiv:2511.19905v1 Announce Type: cross 
Abstract: We consider the problem of Adaptive Neyman Allocation for the class of AIPW estimators in a design-based setting, where potential outcomes and covariates are deterministic. As each subject arrives, an adaptive procedure must select both a treatment assignment probability and a linear predictor to be used in the AIPW estimator. Our goal is to construct an adaptive procedure that minimizes the Neyman Regret, which is the difference between the variance of the adaptive procedure and an oracle variance which uses the optimal non-adaptive choice of assignment probability and linear predictors. While previous work has drawn insightful connections between Neyman Regret and online convex optimization for the Horvitz--Thompson estimator, one of the central challenges for AIPW estimator is that the underlying optimization is non-convex. In this paper, we propose Sigmoid-FTRL, an adaptive experimental design which addresses the non-convexity via simultaneous minimization of two convex regrets. We prove that under standard regularity conditions, the Neyman Regret of Sigmoid-FTRL converges at a $T^{-1/2} R^2$ rate, where $T$ is the number of subjects in the experiment and $R$ is the maximum norm of covariate vectors. Moreover, we show that no adaptive design can improve upon the $T^{-1/2}$ rate under our regularity conditions. Finally, we establish a central limit theorem and a consistently conservative variance estimator which facilitate the construction of asymptotically valid Wald-type confidence intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19905v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fangyi Chen, Shu Ge, Jian Qian, Christopher Harshaw</dc:creator>
    </item>
    <item>
      <title>An Efficient Adaptive Sequential Procedure for Simple Hypotheses with Expression for Finite Number of Applications of Less Effective Treatment</title>
      <link>https://arxiv.org/abs/2511.20061</link>
      <description>arXiv:2511.20061v1 Announce Type: cross 
Abstract: We propose an adaptive sequential framework for testing two simple hypotheses that analytically ensures finite exposure to the less effective treatment. Our proposed procedure employs a likelihood ratio-driven adaptive allocation rule, dynamically concentrating sampling effort on the superior population while preserving asymptotic efficiency (in terms of average sample number) comparable to the Sequential Probability Ratio Test (SPRT). The foremost contribution of this work is the derivation of an explicit closed-form expression for the expected number of applications to the inferior treatment. This approach achieves a balanced method between statistical precision and ethical responsibility, aligning inferential reliability with patient safety. Extensive simulation studies substantiate the theoretical results, confirming stability in allocation and consistently high probability of correct selection (PCS) across different settings. In addition, we demonstrate how the adaptive procedure markedly reduces inferior allocations compared with the classical SPRT, highlighting its practical advantage in ethically sensitive sequential testing scenarios. The proposed design thus offers an ethically efficient and computationally tractable framework for adaptive sequential decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20061v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sampurna Kundu, Jayant Jha, Subir Kumar Bhandari</dc:creator>
    </item>
    <item>
      <title>An Infinite BART model</title>
      <link>https://arxiv.org/abs/2511.20087</link>
      <description>arXiv:2511.20087v1 Announce Type: cross 
Abstract: Bayesian additive regression trees (BART) are popular Bayesian ensemble models used in regression and classification analysis. Under this modeling framework, the regression function is approximated by an ensemble of decision trees, interpreted as weak learners that capture different features of the data. In this work, we propose a generalization of the BART model that has two main features: first, it automatically selects the number of decision trees using the given data; second, the model allows clusters of observations to have different regression functions since each data point can only use a selection of weak learners, instead of all of them. This model generalization is accomplished by including a binary weight matrix in the conditional distribution of the response variable, which activates only a specific subset of decision trees for each observation. Such a matrix is endowed with an Indian Buffet process prior, and sampled within the MCMC sampler, together with the other BART parameters. We then compare the Infinite BART model with the classic one on simulated and real datasets. Specifically, we provide examples illustrating variable importance, partial dependence and causal estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20087v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marco Battiston, Yu Luo</dc:creator>
    </item>
    <item>
      <title>Multiple Randomization Designs: Estimation and Inference with Interference</title>
      <link>https://arxiv.org/abs/2112.13495</link>
      <description>arXiv:2112.13495v3 Announce Type: replace 
Abstract: In this study we introduce a new class of experimental designs. In a classical randomized controlled trial (RCT), or A/B test, a randomly selected subset of a population of units (e.g., individuals, plots of land, or experiences) is assigned to a treatment (treatment A), and the remainder of the population is assigned to the control treatment (treatment B). The difference in average outcome by treatment group is an estimate of the average effect of the treatment. However, motivating our study, the setting for modern experiments is often different, with the outcomes and treatment assignments indexed by multiple populations. For example, outcomes may be indexed by buyers and sellers, by content creators and subscribers, by drivers and riders, or by travelers and airlines and travel agents, with treatments potentially varying across these indices. Spillovers or interference can arise from interactions between units across populations. For example, sellers' behavior may depend on buyers' treatment assignment, or vice versa. This can invalidate the simple comparison of means as an estimator for the average effect of the treatment in classical RCTs. We propose new experiment designs for settings in which multiple populations interact. We show how these designs allow us to study questions about interference that cannot be answered by classical randomized experiments. Finally, we develop new statistical methods for analyzing these Multiple Randomization Designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.13495v3</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lorenzo Masoero, Suhas Vijaykumar, Thomas Richardson, James McQueen, Ido Rosen, Brian Burdick, Pat Bajari, Guido Imbens</dc:creator>
    </item>
    <item>
      <title>Quantile Fourier Transform, Quantile Series, and Nonparametric Estimation of Quantile Spectra</title>
      <link>https://arxiv.org/abs/2211.05844</link>
      <description>arXiv:2211.05844v4 Announce Type: replace 
Abstract: A nonparametric method is proposed for estimating the quantile spectra and cross-spectra introduced in Li (2012; 2014) as bivariate functions of frequency and quantile level. The method is based on the quantile discrete Fourier transform (QDFT) defined by trigonometric quantile regression and the quantile series (QSER) defined by the inverse Fourier transform of the QDFT. A nonparametric spectral estimator is constructed from the autocovariance function of the QSER using the lag-window (LW) approach. Smoothing techniques are also employed to reduce the statistical variability of the LW estimator across quantiles when the underlying spectrum varies smoothly with respect to the quantile level. The performance of the proposed estimation method is evaluated through a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.05844v4</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1080/03610918.2025.2509820</arxiv:DOI>
      <dc:creator>Ta-Hsin Li</dc:creator>
    </item>
    <item>
      <title>Credible Distributions of Overall Ranking of Entities</title>
      <link>https://arxiv.org/abs/2401.01833</link>
      <description>arXiv:2401.01833v3 Announce Type: replace 
Abstract: Ranking, and inferences based on ranking of a set of entities, are important problems in numerous contexts. This is especially true in small area statistics where there may be only a limited amount of directly observed data from each entity or small area, while precise and accurate estimates of best or worst performing entities are needed for fund allocation, planning and policymaking, stakeholder advocacy, evaluation of welfare programs, and so on. However, ranks estimates constructed exclusively on point estimates of parameters lack uncertainty quantification, and may lead to imbalances and inequities when these are based on small sample sizes. We propose novel Bayesian approaches to address this problem. Our proposals result in partitions of the parameter space with posterior distribution driven partial ordering of the sets in a partition. This in turn translates to a coherent probability mass function over ranks for every entity, and a coherent probability mass function over entities for every rank. Our Bayesian algorithms significantly outperform the state-of-the-art non-Bayesian alternatives, and are amenable to inclusion of covariates in the model as well as borrowing strengths across small areas. We evaluate our proposed Bayesian algorithms in terms of accuracy and stability using a number of applications and a simulation study. Additionally, we develop a novel theoretical framework for inference and ranking problems involving a triangular array of Fay-Herriot models and data, and provide probabilistic guarantees of performances of the proposed Bayesian ranking algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.01833v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Snigdhansu Chatterjee, Gauri Sankar Datta, Yiren Hou, Abhyuday Mandal</dc:creator>
    </item>
    <item>
      <title>PriME: Privacy-aware Membership profile Estimation in networks</title>
      <link>https://arxiv.org/abs/2406.02794</link>
      <description>arXiv:2406.02794v2 Announce Type: replace 
Abstract: This paper presents a novel approach to estimating community membership probabilities for network vertices generated by the Degree Corrected Mixed Membership Stochastic Block Model while preserving individual edge privacy. Operating within the $\varepsilon$-edge local differential privacy framework, we introduce an optimal private algorithm based on a symmetric edge flip mechanism and spectral clustering for accurate estimation of vertex community memberships. We conduct a comprehensive analysis of the estimation risk and establish the optimality of our procedure by providing matching lower bounds to the minimax risk under privacy constraints. To validate our approach, we demonstrate its performance through numerical simulations and its practical application to real-world data. This work represents a significant step forward in balancing accurate community membership estimation with stringent privacy preservation in network data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02794v2</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhinav Chakraborty, Sayak Chatterjee, Sagnik Nandy</dc:creator>
    </item>
    <item>
      <title>Predictive Performance Test based on the Exhaustive Nested Cross-Validation for High-dimensional data</title>
      <link>https://arxiv.org/abs/2408.03138</link>
      <description>arXiv:2408.03138v2 Announce Type: replace 
Abstract: It is crucial to assess the predictive performance of a model to establish its practicality and relevance in real-world scenarios, particularly for high-dimensional data analysis. Among data splitting or resampling methods, cross-validation (CV) is extensively used for several tasks such as estimating the prediction error, tuning the regularization parameter, and selecting the most suitable predictive model among competing alternatives. The $K$-fold cross-validation is a popular CV method but its limitation is that the risk estimates are highly dependent on the partitioning of the data (for training and testing). Here, the issues regarding the reproducibility of the $K$-fold CV estimator are demonstrated in hypothesis testing wherein different partitions lead to notably disparate conclusions. This study presents a novel predictive performance test and valid confidence intervals based on exhaustive nested cross-validation for determining the difference in prediction error between two model-fitting algorithms. A naive implementation of the exhaustive nested cross-validation is computationally costly. Here, we address concerns regarding computational complexity by devising a computationally tractable closed-form expression for the proposed cross-validation estimator. Our study also investigates strategies aimed at enhancing statistical power within high-dimensional scenarios while controlling the Type I error rate. \tr{Through comprehensive numerical experiments, we demonstrate that Ridge-based methods using bias to measure uncertainty of CV estimates and adaptive hyperparameter selection provide the most reliable approach for high-dimensional predictive performance testing.} To illustrate the practical utility of our method, we apply it to an RNA sequencing study and demonstrate its effectiveness in the context of biological data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03138v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iris Ivy Gauran, Hernando Ombao, Zhaoxia Yu</dc:creator>
    </item>
    <item>
      <title>Multi-Attribute Preferences: A Transfer Learning Approach</title>
      <link>https://arxiv.org/abs/2408.10558</link>
      <description>arXiv:2408.10558v2 Announce Type: replace 
Abstract: This contribution introduces a novel statistical learning methodology based on the Bradley-Terry method for pairwise comparisons, where the novelty arises from the method's capacity to estimate the worth of objects for a primary attribute by incorporating data of secondary attributes. These attributes are properties on which objects are evaluated in a pairwise fashion by individuals. By assuming that the main interest of practitioners lies in the primary attribute, and the secondary attributes only serve to improve estimation of the parameters underlying the primary attribute, this paper utilises the well-known transfer learning framework. To wit, the proposed method first estimates a biased worth vector using data pertaining to both the primary attribute and the set of informative secondary attributes, which is followed by a debiasing step based on a penalised likelihood of the primary attribute. When the set of informative secondary attributes is unknown, we allow for their estimation by a data-driven algorithm. Theoretically, we show that, under mild conditions, the $\ell_\infty$ and $\ell_2$ rates are improved compared to fitting a Bradley-Terry model on just the data pertaining to the primary attribute. The favourable (comparative) performance under more general settings is shown by means of a simulation study. To illustrate the usage and interpretation of the method, an application of the proposed method is provided on consumer preference data pertaining to a cassava derived food product: eba. An R package containing the proposed methodology can be found on https://CRAN.R-project.org/package=BTTL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10558v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sjoerd Hermes, Joost van Heerwaarden, Pariya Behrouzi</dc:creator>
    </item>
    <item>
      <title>Spatial Proportional Hazards Model with Differential Regularization</title>
      <link>https://arxiv.org/abs/2410.13420</link>
      <description>arXiv:2410.13420v4 Announce Type: replace 
Abstract: The Proportional Hazards (PH) model is one of the most common model used in survival analysis, which typically assumes a log-linear relationship between covariates and the hazard function. However, this assumption may not hold in practice. This paper introduces a nonparametric extension of the PH model, which generalizes the log-linear assumption by allowing for an unspecified, smooth function of covariates, enabling more flexible modeling. We focus on applications with spatial survival data, where the location of an event affects the risk. The proposed model captures this spatial variation using a nonparametric spatial effect. We estimate the spatial effect using finite element methods on a mesh constructed from a triangulation of the domain, which allows us to handle irregular shapes. The model remains within the classical partial likelihood framework, ensuring computational feasibility. To enforce the smoothness in the nonparametric spatial effect, we consider a differential penalization. We establish the asymptotic properties of the proposed estimator using sieve methods, demonstrating its consistency and the asymptotic normality of the parametric component. A simulation study is conducted to evaluate the model's performance, followed by two empirical applications that demonstrate its practical advantages over standard PH models, especially in settings with spatial dependence in survival data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13420v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Tedesco, Francesco Finazzi</dc:creator>
    </item>
    <item>
      <title>Transfer Learning for High-dimensional Quantile Regression with Distribution Shift</title>
      <link>https://arxiv.org/abs/2411.19933</link>
      <description>arXiv:2411.19933v2 Announce Type: replace 
Abstract: Information from related source studies can often enhance the findings of a target study. However, the distribution shift between target and source studies can severely impact the efficiency of knowledge transfer. In the high-dimensional regression setting, existing transfer approaches mainly focus on the parameter shift. In this paper, we focus on the high-dimensional quantile regression with knowledge transfer under three types of distribution shift: parameter shift, covariate shift, and residual shift. We propose a novel transferable set and a new transfer framework to address the above three discrepancies. Non-asymptotic estimation error bounds and source detection consistency are established to validate the availability and superiority of our method in the presence of distribution shift. Additionally, an orthogonal debiased approach is proposed for statistical inference with knowledge transfer, leading to sharper asymptotic results. Extensive simulation results as well as real data applications further demonstrate the effectiveness of our proposed procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19933v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruiqi Bai, Yijiao Zhang, Hanbo Yang, Zhongyi Zhu</dc:creator>
    </item>
    <item>
      <title>Quantile-Crossing Spectrum and Spline Autoregression Estimation</title>
      <link>https://arxiv.org/abs/2412.02513</link>
      <description>arXiv:2412.02513v2 Announce Type: replace 
Abstract: The quantile-crossing spectrum is the spectrum of quantile-crossing processes created from a time series by the indicator function that shows whether or not the time series lies above or below a given quantile at a given time. This bivariate function of frequency and quantile level provides a richer view of serial dependence than that offered by the ordinary spectrum. We propose a new method for estimating the quantile-crossing spectrum as a bivariate function of frequency and quantile level. The proposed method, called spline autoregression (SAR), jointly fits an AR model to the quantile-crossing series across multiple quantiles; the AR coefficients are represented as spline functions of the quantile level and penalized for their roughness. Numerical experiments show that when the underlying spectrum is smooth in quantile level the proposed method is able to produce more accurate estimates in comparison with the alternative that ignores the smoothness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02513v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s11203-025-09336-7</arxiv:DOI>
      <dc:creator>Ta-Hsin Li</dc:creator>
    </item>
    <item>
      <title>Spline Autoregression Method for Estimation of Quantile Spectrum</title>
      <link>https://arxiv.org/abs/2412.17163</link>
      <description>arXiv:2412.17163v3 Announce Type: replace 
Abstract: The quantile spectrum was introduced in Li (2012; 2014) as an alternative tool for spectral analysis of time series. It has the capability of providing a richer view of time series data than that offered by the ordinary spectrum especially for nonlinear dynamics such as stochastic volatility. A novel method, called spline autoregression (SAR), is proposed in this paper for estimating the quantile spectrum as a bivaraite function of frequency and quantile level, under the assumption that the quantile spectrum varies smoothly with the quantile level. The SAR method is facilitated by the quantile discrete Fourier transform (QDFT) based on trigonometric quantile regression. It is enabled by the resulting time-domain quantile series (QSER) which represents properly scaled oscillatory characteristics of the original time series around a quantile. A functional autoregressive (AR) model is fitted to the QSER on a grid of quantile levels by penalized least-squares with the AR coefficients represented as smoothing splines of the quantile level. While the ordinary AR model is widely used for conventional spectral estimation, the proposed SAR method provides an effective way of estimating the quantile spectrum as a bivariate function in comparison with the alternatives. This is confirmed by a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17163v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1080/10618600.2025.2549452</arxiv:DOI>
      <dc:creator>Ta-Hsin Li</dc:creator>
    </item>
    <item>
      <title>Finding Distributions that Differ, with False Discovery Rate Control</title>
      <link>https://arxiv.org/abs/2505.13769</link>
      <description>arXiv:2505.13769v3 Announce Type: replace 
Abstract: We consider the problem of comparing a reference distribution with several other distributions. Given a sample from both the reference and the comparison groups, we aim to identify the comparison groups whose distributions differ from that of the reference group. Viewing this as a multiple testing problem, we introduce a methodology that provides exact, distribution-free control of the false discovery rate. To do so, we introduce the concept of batch conformal p-values and demonstrate that they satisfy positive regression dependence across the groups [Benjamini and Yekutieli, 2001], thereby enabling control of the false discovery rate through the Benjamini-Hochberg procedure. The proof of positive regression dependence introduces a novel technique for the inductive construction of rank vectors with almost sure dominance under exchangeability. We evaluate the performance of the proposed procedure through simulations, where, despite being distribution-free, in some cases they show performance comparable to methods with knowledge of the data-generating normal distribution; and further have more power than direct approaches based on conformal out-of-distribution detection. Further, we illustrate our methods on a Hepatitis C treatment dataset, where they can identify patient groups with large treatment effects; and on the Current Population Survey dataset, where they can identify sub-population with long work hours.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13769v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yonghoon Lee, Edgar Dobriban, Eric Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Wavelet Canonical Coherence for Nonstationary Signals</title>
      <link>https://arxiv.org/abs/2505.14253</link>
      <description>arXiv:2505.14253v2 Announce Type: replace 
Abstract: Understanding the evolving dependence between two clusters of multivariate signals is fundamental in neuroscience and other domains where sub-networks in a system interact dynamically over time. Despite the growing interest in multivariate time series analysis, existing methods for between-clusters dependence typically rely on the assumption of stationarity and lack the temporal resolution to capture transient, frequency-specific interactions. To overcome this limitation, we propose scale-specific wavelet canonical coherence (WaveCanCoh), a novel framework that extends canonical coherence analysis to the nonstationary setting by leveraging the multivariate locally stationary wavelet model. The proposed WaveCanCoh enables the estimation of time-varying canonical coherence between clusters, providing interpretable insight into scale-specific time-varying interactions between clusters. Through extensive simulation studies, we demonstrate that WaveCanCoh accurately recovers true coherence structures under both locally stationary and general nonstationary conditions. Application to local field potential (LFP) activity data recorded from the hippocampus reveals distinct dynamic coherence patterns between correct and incorrect memory-guided decisions, illustrating the capacity of the method to detect behaviorally relevant neural coordination. These results highlight WaveCanCoh as a flexible and principled tool for modeling complex cross-group dependencies in nonstationary multivariate systems. The code for WaveCanCoh is available at: https://github.com/mhaibo/WaveCanCoh.git.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14253v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haibo Wu, Marina I. Knight, Keiland W. Cooper, Norbert J. Fortin, Hernando Ombao</dc:creator>
    </item>
    <item>
      <title>Integrating Expert Knowledge and Recursive Bayesian Inference: A Framework for Spatial and Spatio-Temporal Data Challenges</title>
      <link>https://arxiv.org/abs/2506.00221</link>
      <description>arXiv:2506.00221v2 Announce Type: replace 
Abstract: Integrating heterogeneous data sources and expert knowledge is essential for overcoming data scarcity and enhancing estimation accuracy. Two main frameworks naturally arise to perform the integration of these multiple sources: sequential Bayesian inference and integrated models. The first one consists of updating posterior information in a sequential data analysis procedure, without the need to reanalyze previous data when new data become available. The second one consists of bringing together diverse sources of information in a joint inferential analysis through hierarchical Bayesian models. Within the context of the first framework, we propose a recursive inference method grounded in the methodological principles of INLA, designed to handle spatial and spatio-temporal problems, although its applicability is not limited to these cases, as the procedure is general in nature. Within the integrated models framework, we also present a comprehensive approach to address change of support issues that arise when combining heterogeneous information sources, developing a typology that classifies such changes as spatial, temporal, spatio-temporal, or categorical. Both frameworks can be combined, as there is neither a theoretical nor a practical incompatibility preventing their joint use. Finally, detailed examples are provided to illustrate clear and replicable procedures for combining heterogeneous data sources with change of support and recursive inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00221v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mario Figueira, David Conesa, Antonio L\'opez-Qu\'ilez, H{\aa}vard Rue</dc:creator>
    </item>
    <item>
      <title>Temporal Exposure Dependence Bias in Vaccine Efficacy Trials</title>
      <link>https://arxiv.org/abs/2509.03476</link>
      <description>arXiv:2509.03476v2 Announce Type: replace 
Abstract: This work investigates the estimation of the per-contact vaccine efficacy (VE), using Cox proportional hazards models, in infectious disease clinical trials. We identify a previously unrecognized source of bias that arises from temporally correlated exposure to the pathogen. A key finding is that, under our proposed framework, this bias can emerge even when treatment groups are perfectly randomly assigned. Moreover, since the bias stems from an unobservable variable-the individual's exposure status to the pathogen-it is inherently challenging to adjust for. We develop a mathematical framework to characterize this mechanism and derive a closed-form approximation that quantifies the magnitude of the resulting bias. Building on this, we propose a practical, conservative bias-correction method that does not require direct measurement of exposure, which is typically unavailable in real-world trials. Our findings show that, under realistic parameter settings, the resulting bias can be substantial. These results suggest that temporally correlated exposure should be recognized as a potentially important factor in the design and analysis of infectious disease vaccine trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03476v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hiroyasu Ando, A. James O'Malley, Akihiro Nishi</dc:creator>
    </item>
    <item>
      <title>Subspace Ordering for Maximum Response Preservation in Sufficient Dimension Reduction</title>
      <link>https://arxiv.org/abs/2510.27593</link>
      <description>arXiv:2510.27593v2 Announce Type: replace 
Abstract: Sufficient dimension reduction (SDR) methods aim to identify a dimension reduction subspace (DRS) that preserves all the information about the conditional distribution of a response given its predictor. Traditional SDR methods determine the DRS by solving a method-specific generalized eigenvalue problem and selecting the eigenvectors corresponding to the largest eigenvalues. In this article, we argue against the long-standing convention of using eigenvalues as the measure of subspace importance and propose alternative ordering criteria that directly assess the predictive relevance of each subspace. For a binary response, we introduce a subspace ordering criterion based on the absolute value of the independent Student's T-statistic. Theoretically, our criterion identifies subspaces that achieve the local minimum Bayes' error rate and yields consistent ordering of directions under mild regularity conditions. Additionally, we employ an F-statistic to provide a framework that unifies categorical and continuous responses under a single subspace criterion. We evaluate our proposed criteria within multiple SDR methods through extensive simulation studies and applications to real data. Our empirical results demonstrate the efficacy of reordering subspaces using our proposed criteria, which generally improves classification accuracy and subspace estimation compared to ordering by eigenvalues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27593v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Derik T. Boonstra, Rakheon Kim, Dean M. Young</dc:creator>
    </item>
    <item>
      <title>Faster estimation of the transformation-free linear simplicial-simplicial regression via constrained iterative reweighted least squares</title>
      <link>https://arxiv.org/abs/2511.13296</link>
      <description>arXiv:2511.13296v2 Announce Type: replace 
Abstract: Simplicial-simplicial regression refers to the regression setting where both the responses and predictor variables lie within the simplex space, i.e. they are compositional. \cite{fiksel2022} proposed a transformation-free linear regression model, that minimizes the Kullback-Leibler divergence from the observed to the fitted compositions, where the EM algorithm is used to estimate the regression coefficients. We formulate the model as a constrained logistic regression, in the spirit of \cite{tsagris2025}, and we estimate the regression coefficients using constrained iteratively reweighted least squares. The simulation studies depict that this algorithm makes the estimation procedure significantly faster, uses less memory, and in some cases gives a better solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13296v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michail Tsagris</dc:creator>
    </item>
    <item>
      <title>Flexible unimodal density estimation in hidden Markov models</title>
      <link>https://arxiv.org/abs/2511.17071</link>
      <description>arXiv:2511.17071v2 Announce Type: replace 
Abstract: 1. Hidden Markov models (HMMs) are powerful tools for modelling time-series data with underlying state structure. However, selecting appropriate parametric forms for the state-dependent distributions is often challenging and can lead to model misspecification. To address this, P-spline-based nonparametric estimation of state-dependent densities has been proposed. While offering great flexibility, these approaches can result in overly complex densities (e.g. bimodal) that hinder interpretability. 2. We propose a straightforward method that builds on shape-constrained spline theory to enforce unimodality in the estimated state-dependent densities through enforcing unimodality of the spline coefficients. This constraint strikes a practical balance between model flexibility, interpretability, and parsimony. 3. Through two simulation studies and a real-world case study using narwhal (Monodon monoceros) dive data, we demonstrate the proposed approach yields more stable estimates compared to fully flexible, unconstrained models improving model performance and interpretability. 4. Our method bridges a key methodological gap, by providing a parsimonious HMM framework that balances the interpretability of parametric models with the flexibility of nonparametric estimation. This provides ecologists with a powerful tool to derive ecologically meaningful inference from telemetry data while avoiding the pitfalls of overly complex models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17071v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan-Ole Koslik, Fanny Dupont, Marie Auger-M\'eth\'e, Marianne Marcoux, Nigel Hussey, Nancy Heckman</dc:creator>
    </item>
    <item>
      <title>A Unified Spatiotemporal Framework for Modeling Censored and Missing Areal Responses</title>
      <link>https://arxiv.org/abs/2511.17725</link>
      <description>arXiv:2511.17725v2 Announce Type: replace 
Abstract: We propose a new Bayesian approach for spatiotemporal areal data with censored and missing observations. The method introduces a flexible random effect that combines the spatial dependence structures of the Simultaneous Autoregressive (SAR) and Directed Acyclic Graph Autoregressive (DAGAR) models with a temporal autoregressive component. We demonstrate that this formulation extends both spatial models into a unified spatiotemporal framework, expressing them as Gaussian Markov random fields in their innovation form. The resulting model captures spatial, temporal, and joint spatiotemporal correlations in an interpretable way. Simulation studies show that the proposed model outperforms common ad hoc imputation strategies, such as replacing censored values with the limit of detection (LOD) or imputing missing data by the sample mean. We further apply the method to carbon monoxide (CO) concentration data from Beijing's air quality network, comparing the proposed DAGAR-AR model with the traditional Conditional Autoregressive (CAR) approach. The results indicate that while the CAR model achieves slightly better predictive performance, the DAGAR-AR specification offers clearer interpretability and a more coherent representation of the spatiotemporal dependence structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17725v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jose A. Ordo\~nez, Tsung-I Lin, Victor H. Lachos, Luis M. Castro</dc:creator>
    </item>
    <item>
      <title>The Asymptotic Distribution for a Single Joinpoint Changepoint Model</title>
      <link>https://arxiv.org/abs/2511.17942</link>
      <description>arXiv:2511.17942v2 Announce Type: replace 
Abstract: A single joinpoint changepoint model partitions a time series into two segments, joined at the changepoint time by constraining the estimated piecewise linear regression responses to be continuous. This manuscript derives the exact asymptotic distribution of the changepoint existence test statistic gauging whether or not a second segment is necessary. The identified asymptotic distribution, a supremum of a Gaussian process over the unit interval, is rather unwieldy. The work presented here provides the result and its derivation; quantiles of the asymptotic distribution are presented for the user. This addresses a subtle gap in the changepoint literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17942v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xueheng Shi, Robert Lund</dc:creator>
    </item>
    <item>
      <title>Sparse-Smooth Spatially Varying Coefficient Quantile Regression</title>
      <link>https://arxiv.org/abs/2511.18106</link>
      <description>arXiv:2511.18106v2 Announce Type: replace 
Abstract: We develop a convex framework for spatially varying coefficient quantile regression that, for each predictor, separates a location-invariant \emph{global} effect from a \emph{spatial deviation}. An adaptive group penalty selects whether a predictor varies over space, while a graph\textendash Laplacian quadratic promotes spatial continuity of the deviations on irregular networks. The formulation is identifiable via degree-weighted centering and scales with sparse linear algebra. We provide two practical solvers\textemdash an ADMM algorithm with closed-form proximal maps for the check loss and a smoothed proximal-gradient scheme based on the Moreau envelope\textemdash together with implementation guidance (projection for identifiability, stopping diagnostics, and preconditioning). Under mild conditions on the sampling design, covariates, error density, and graph geometry, we establish selection consistency for the deviation groups, mean-squared error bounds that balance Laplacian bias and stochastic variability, and root-\(n\) asymptotic normality for the global coefficients with an oracle property. Simulations mimicking air-pollution applications demonstrate accurate recovery of global vs.\ local effects and competitive predictive performance under heteroskedastic, heavy-tailed noise. We discuss graph construction, spatially blocked cross-validation (to prevent leakage), and options for robust standard errors under spatial dependence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18106v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Hou Jian, Meng Tan, Tian Maozai</dc:creator>
    </item>
    <item>
      <title>Quantile-Frequency Analysis and Spectral Measures for Diagnostic Checks of Time Series With Nonlinear Dynamics</title>
      <link>https://arxiv.org/abs/1908.02545</link>
      <description>arXiv:1908.02545v2 Announce Type: replace-cross 
Abstract: Nonlinear dynamic volatility has been observed in many financial time series. The recently proposed quantile periodogram offers an alternative way to examine this phenomena in the frequency domain. The quantile periodogram is constructed from trigonometric quantile regression of time series data at different frequencies and quantile levels, enabling the quantile-frequency analysis (QFA) of nonlinear serial dependence. This paper introduces some spectral measures based on the quantile periodogram for diagnostic checks of financial time series models and for model-based discriminant analysis. A simulation-based parametric bootstrapping technique is employed to compute the $p$-values of the spectral measures. The usefulness of the proposed method is demonstrated by a simulation study and a motivating application using the daily log returns of the S\&amp;P 500 index together with GARCH-type models. The results show that the QFA method is able to provide additional insights into the goodness of fit of these financial time series models that may have been missed by conventional tests. The results also show that the QFA method offers a more informative way of discriminant analysis for detecting regime changes in financial time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:1908.02545v2</guid>
      <category>q-fin.ST</category>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1111/rssc.12458</arxiv:DOI>
      <dc:creator>Ta-Hsin Li</dc:creator>
    </item>
  </channel>
</rss>
