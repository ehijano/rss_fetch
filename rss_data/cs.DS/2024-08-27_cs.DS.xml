<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 28 Aug 2024 01:41:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Parallel Set Cover and Hypergraph Matching via Uniform Random Sampling</title>
      <link>https://arxiv.org/abs/2408.13362</link>
      <description>arXiv:2408.13362v1 Announce Type: new 
Abstract: The SetCover problem has been extensively studied in many different models of computation, including parallel and distributed settings. From an approximation point of view, there are two standard guarantees: an $O(\log \Delta)$-approximation (where $\Delta$ is the maximum set size) and an $O(f)$-approximation (where $f$ is the maximum number of sets containing any given element).
  In this paper, we introduce a new, surprisingly simple, model-independent approach to solving SetCover in unweighted graphs. We obtain multiple improved algorithms in the MPC and CRCW PRAM models. First, in the MPC model with sublinear space per machine, our algorithms can compute an $O(f)$ approximation to SetCover in $\hat{O}(\sqrt{\log \Delta} + \log f)$ rounds, where we use the $\hat{O}(x)$ notation to suppress $\mathrm{poly} \log x$ and $\mathrm{poly} \log \log n$ terms, and a $O(\log \Delta)$ approximation in $O(\log^{3/2} n)$ rounds. Moreover, in the PRAM model, we give a $O(f)$ approximate algorithm using linear work and $O(\log n)$ depth. All these bounds improve the existing round complexity/depth bounds by a $\log^{\Omega(1)} n$ factor.
  Moreover, our approach leads to many other new algorithms, including improved algorithms for the HypergraphMatching problem in the MPC model, as well as simpler SetCover algorithms that match the existing bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13362v1</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Laxman Dhulipala, Michael Dinitz, Jakub {\L}\k{a}cki, Slobodan Mitrovi\'c</dc:creator>
    </item>
    <item>
      <title>The Parameterized Complexity Landscape of Two-Sets Cut-Uncut</title>
      <link>https://arxiv.org/abs/2408.13543</link>
      <description>arXiv:2408.13543v1 Announce Type: new 
Abstract: In Two-Sets Cut-Uncut, we are given an undirected graph $G=(V,E)$ and two terminal sets $S$ and $T$. The task is to find a minimum cut $C$ in $G$ (if there is any) separating $S$ from $T$ under the following ``uncut'' condition. In the graph $(V,E \setminus C)$, the terminals in each terminal set remain in the same connected component. In spite of the superficial similarity to the classic problem Minimum $s$-$t$-Cut, Two-Sets Cut-Uncut is computationally challenging. In particular, even deciding whether such a cut of any size exists, is already NP-complete. We initiate a systematic study of Two-Sets Cut-Uncut within the context of parameterized complexity. By leveraging known relations between many well-studied graph parameters, we characterize the structural properties of input graphs that allow for polynomial kernels, fixed-parameter tractability (FPT), and slicewise polynomial algorithms (XP). Our main contribution is the near-complete establishment of the complexity of these algorithmic properties within the described hierarchy of graph parameters. On a technical level, our main results are fixed-parameter tractability for the (vertex-deletion) distance to cographs and an OR-cross composition excluding polynomial kernels for the vertex cover number of the input graph (under the standard complexity assumption NP is not contained in coNP/poly).</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13543v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthias Bentert, Fedor V. Fomin, Fanny Hauser, Saket Saurabh</dc:creator>
    </item>
    <item>
      <title>Revisit the Partial Coloring Method: Prefix Spencer and Sampling</title>
      <link>https://arxiv.org/abs/2408.13756</link>
      <description>arXiv:2408.13756v1 Announce Type: new 
Abstract: As the most powerful tool in discrepancy theory, the partial coloring method has wide applications in many problems including the Beck-Fiala problem and Spencer's celebrated result. Currently, there are two major algorithmic methods for the partial coloring method: the first approach uses linear algebraic tools; and the second is called Gaussian measure algorithm. We explore the advantages of these two methods and show the following results for them separately.
  1. Spencer conjectured that the prefix discrepancy of any $\mathbf{A} \in \{0,1\}^{m \times n}$ is $O(\sqrt{m})$. We show how to find a partial coloring with prefix discrepancy $O(\sqrt{m})$ and $\Omega(n)$ entries in $\{ \pm 1\}$ efficiently. To the best of our knowledge, this provides the first partial coloring whose prefix discrepancy is almost optimal. However, unlike the classical discrepancy problem, there is no reduction on the number of variables $n$ for the prefix problem. By recursively applying partial coloring, we obtain a full coloring with prefix discrepancy $O(\sqrt{m} \cdot \log \frac{O(n)}{m})$. Prior to this work, the best bounds of the prefix Spencer conjecture for arbitrarily large $n$ were $2m$ and $O(\sqrt{m \log n})$.
  2. Our second result extends the first linear algebraic approach to a sampling algorithm in Spencer's classical setting. On the first hand, Spencer proved that there are $1.99^m$ good colorings with discrepancy $O(\sqrt{m})$. Hence a natural question is to design efficient random sampling algorithms in Spencer's setting. On the other hand, some applications of discrepancy theory, prefer a random solution instead of a fixed one. Our second result is an efficient sampling algorithm whose random output has min-entropy $\Omega(n)$ and discrepancy $O(\sqrt{m})$. Moreover, our technique extends the linear algebraic framework by incorporating leverage scores of randomized matrix algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13756v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dongrun Cai, Xue Chen, Wenxuan Shu, Haoyu Wang, Guangyi Zou</dc:creator>
    </item>
    <item>
      <title>On the Parameterized Complexity of Eulerian Strong Component Arc Deletion</title>
      <link>https://arxiv.org/abs/2408.13819</link>
      <description>arXiv:2408.13819v1 Announce Type: new 
Abstract: In this paper, we study the Eulerian Strong Component Arc Deletion problem, where the input is a directed multigraph and the goal is to delete the minimum number of arcs to ensure every strongly connected component of the resulting digraph is Eulerian. This problem is a natural extension of the Directed Feedback Arc Set problem and is also known to be motivated by certain scenarios arising in the study of housing markets. The complexity of the problem, when parameterized by solution size (i.e., size of the deletion set), has remained unresolved and has been highlighted in several papers. In this work, we answer this question by ruling out (subject to the usual complexity assumptions) a fixed-parameter tractable (FPT) algorithm for this parameter and conduct a broad analysis of the problem with respect to other natural parameterizations. We prove both positive and negative results. Among these, we demonstrate that the problem is also hard (W[1]-hard or even para-NP-hard) when parameterized by either treewidth or maximum degree alone. Complementing our lower bounds, we establish that the problem is in XP when parameterized by treewidth and FPT when parameterized either by both treewidth and maximum degree or by both treewidth and solution size. We show that these algorithms have near-optimal asymptotic dependence on the treewidth assuming the Exponential Time Hypothesis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13819v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>V\'aclav Bla\v{z}ej, Satyabrata Jana, M. S. Ramanujan, Peter Strulo</dc:creator>
    </item>
    <item>
      <title>Quantum Speedups for Approximating the John Ellipsoid</title>
      <link>https://arxiv.org/abs/2408.14018</link>
      <description>arXiv:2408.14018v1 Announce Type: new 
Abstract: In 1948, Fritz John proposed a theorem stating that every convex body has a unique maximal volume inscribed ellipsoid, known as the John ellipsoid. The John ellipsoid has become fundamental in mathematics, with extensive applications in high-dimensional sampling, linear programming, and machine learning. Designing faster algorithms to compute the John ellipsoid is therefore an important and emerging problem. In [Cohen, Cousins, Lee, Yang COLT 2019], they established an algorithm for approximating the John ellipsoid for a symmetric convex polytope defined by a matrix $A \in \mathbb{R}^{n \times d}$ with a time complexity of $O(nd^2)$. This was later improved to $O(\text{nnz}(A) + d^\omega)$ by [Song, Yang, Yang, Zhou 2022], where $\text{nnz}(A)$ is the number of nonzero entries of $A$ and $\omega$ is the matrix multiplication exponent. Currently $\omega \approx 2.371$ [Alman, Duan, Williams, Xu, Xu, Zhou 2024]. In this work, we present the first quantum algorithm that computes the John ellipsoid utilizing recent advances in quantum algorithms for spectral approximation and leverage score approximation, running in $O(\sqrt{n}d^{1.5} + d^\omega)$ time. In the tall matrix regime, our algorithm achieves quadratic speedup, resulting in a sublinear running time and significantly outperforming the current best classical algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14018v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoyu Li, Zhao Song, Junwei Yu</dc:creator>
    </item>
    <item>
      <title>An Efficient and Exact Algorithm for Locally h-Clique Densest Subgraph Discovery</title>
      <link>https://arxiv.org/abs/2408.14022</link>
      <description>arXiv:2408.14022v1 Announce Type: new 
Abstract: Detecting locally, non-overlapping, near-clique densest subgraphs is a crucial problem for community search in social networks. As a vertex may be involved in multiple overlapped local cliques, detecting locally densest sub-structures considering h-clique density, i.e., locally h-clique densest subgraph (LhCDS) attracts great interests. This paper investigates the LhCDS detection problem and proposes an efficient and exact algorithm to list the top-k non-overlapping, locally h-clique dense, and compact subgraphs. We in particular jointly consider h-clique compact number and LhCDS and design a new "Iterative Propose-Prune-and-Verify" pipeline (IPPV) for top-k LhCDS detection. (1) In the proposal part, we derive initial bounds for h-clique compact numbers; prove the validity, and extend a convex programming method to tighten the bounds for proposing LhCDS candidates without missing any. (2) Then a tentative graph decomposition method is proposed to solve the challenging case where a clique spans multiple subgraphs in graph decomposition. (3) To deal with the verification difficulty, both a basic and a fast verification method are proposed, where the fast method constructs a smaller-scale flow network to improve efficiency while preserving the verification correctness. The verified LhCDSes are returned, while the candidates that remained unsure reenter the IPPV pipeline. (4) We further extend the proposed methods to locally more general pattern densest subgraph detection problems. We prove the exactness and low complexity of the proposed algorithm. Extensive experiments on real datasets show the effectiveness and high efficiency of IPPV.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14022v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaojia Xu, Haoyu Liu, Xiaowei Lv, Yongcai Wang, Deying Li</dc:creator>
    </item>
    <item>
      <title>Multi-variable Quantification of BDDs in External Memory using Nested Sweeping (Extended Paper)</title>
      <link>https://arxiv.org/abs/2408.14216</link>
      <description>arXiv:2408.14216v1 Announce Type: new 
Abstract: Previous research on the Adiar BDD package has been successful at designing algorithms capable of handling large Binary Decision Diagrams (BDDs) stored in external memory. To do so, it uses consecutive sweeps through the BDDs to resolve computations. Yet, this approach has kept algorithms for multi-variable quantification, the relational product, and variable reordering out of its scope.
  In this work, we address this by introducing the nested sweeping framework. Here, multiple concurrent sweeps pass information between eachother to compute the result. We have implemented the framework in Adiar and used it to create a new external memory multi-variable quantification algorithm. Compared to conventional depth-first implementations, Adiar with nested sweeping is able to solve more instances of our benchmarks and/or solve them faster.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14216v1</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Steffan Christ S{\o}lvsten, Jaco van de Pol</dc:creator>
    </item>
    <item>
      <title>The Power of Proportional Fairness for Non-Clairvoyant Scheduling under Polyhedral Constraints</title>
      <link>https://arxiv.org/abs/2408.14310</link>
      <description>arXiv:2408.14310v1 Announce Type: new 
Abstract: The Polytope Scheduling Problem (PSP) was introduced by Im, Kulkarni, and Munagala (JACM 2018) as a very general abstraction of resource allocation over time and captures many well-studied problems including classical unrelated machine scheduling, multidimensional scheduling, and broadcast scheduling. In PSP, jobs with different arrival times receive processing rates that are subject to arbitrary packing constraints. An elegant and well-known algorithm for instantaneous rate allocation with good fairness and efficiency properties is the Proportional Fairness algorithm (PF), which was analyzed for PSP by Im et al.
  We drastically improve the analysis of the PF algorithm for both the general PSP and several of its important special cases subject to the objective of minimizing the sum of weighted completion times. We reduce the upper bound on the competitive ratio from 128 to 27 for general PSP and to 4 for the prominent class of monotone PSP. For certain heterogeneous machine environments we even close the substantial gap to the lower bound of 2 for non-clairvoyant scheduling. Our analysis also gives the first polynomial-time improvements over the nearly 30-year-old bounds on the competitive ratio of the doubling framework by Hall, Shmoys, and Wein (SODA 1996) for clairvoyant online preemptive scheduling on unrelated machines. Somewhat surprisingly, we achieve this improvement by a non-clairvoyant algorithm, thereby demonstrating that non-clairvoyance is not a (significant) hurdle.
  Our improvements are based on exploiting monotonicity properties of PSP, providing tight dual fitting arguments on structured instances, and showing new additivity properties on the optimal objective value for scheduling on unrelated machines. Finally, we establish new connections of PF to matching markets, and thereby provide new insights on equilibria and their computational complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14310v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sven J\"ager, Alexander Lindermayr, Nicole Megow</dc:creator>
    </item>
    <item>
      <title>Fully Dynamic Shortest Paths in Sparse Digraphs</title>
      <link>https://arxiv.org/abs/2408.14406</link>
      <description>arXiv:2408.14406v1 Announce Type: new 
Abstract: We study the exact fully dynamic shortest paths problem. For real-weighted directed graphs, we show a deterministic fully dynamic data structure with $\tilde{O}(mn^{4/5})$ worst-case update time processing arbitrary $s,t$-distance queries in $\tilde{O}(n^{4/5})$ time. This constitutes the first non-trivial update/query tradeoff for this problem in the regime of sparse weighted directed graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14406v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4230/LIPICS.ICALP.2023.84</arxiv:DOI>
      <dc:creator>Adam Karczmarz, Piotr Sankowski</dc:creator>
    </item>
    <item>
      <title>Targeted Least Cardinality Candidate Key for Relational Databases</title>
      <link>https://arxiv.org/abs/2408.13540</link>
      <description>arXiv:2408.13540v1 Announce Type: cross 
Abstract: Functional dependencies (FDs) are a central theme in databases, playing a major role in the design of database schemas and the optimization of queries. In this work, we introduce the {\it targeted least cardinality candidate key problem} (TCAND). This problem is defined over a set of functional dependencies $F$ and a target variable set $T \subseteq V$, and it aims to find the smallest set $X \subseteq V$ such that the FD $X \to T$ can be derived from $F$. The TCAND problem generalizes the well-known NP-hard problem of finding the least cardinality candidate key~\cite{lucchesi1978candidate}, which has been previously demonstrated to be at least as difficult as the set cover problem.
  We present an integer programming (IP) formulation for the TCAND problem, analogous to a layered set cover problem. We analyze its linear programming (LP) relaxation from two perspectives: we propose two approximation algorithms and investigate the integrality gap. Our findings indicate that the approximation upper bounds for our algorithms are not significantly improvable through LP rounding, a notable distinction from the standard set cover problem. Additionally, we discover that a generalization of the TCAND problem is equivalent to a variant of the set cover problem, named red-blue set cover~\cite{carr1999red}, which cannot be approximated within a sub-polynomial factor in polynomial time under plausible conjectures~\cite{chlamtavc2023approximating}. Despite the extensive history surrounding the issue of identifying the least cardinality candidate key, our research contributes new theoretical insights, novel algorithms, and demonstrates that the general TCAND problem poses complexities beyond those encountered in the set cover problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13540v1</guid>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vasileios Nakos, Hung Q. Ngo, Charalampos E. Tsourakakis</dc:creator>
    </item>
    <item>
      <title>Finding the Center and Centroid of a Graph with Multiple Sources</title>
      <link>https://arxiv.org/abs/2408.13688</link>
      <description>arXiv:2408.13688v1 Announce Type: cross 
Abstract: We consider the problem of finding a "fair" meeting place when S people want to get together. Specifically, we will consider the cases where a "fair" meeting place is defined to be either 1) a node on a graph that minimizes the maximum time/distance to each person or 2) a node on a graph that minimizes the sum of times/distances to each of the sources. In graph theory, these nodes are denoted as the center and centroid of a graph respectively. In this paper, we propose a novel solution for finding the center and centroid of a graph by using a multiple source alternating Dijkstra's Algorithm. Additionally, we introduce a stopping condition that significantly saves on time complexity without compromising the accuracy of the solution. The results of this paper are a low complexity algorithm that is optimal in computing the center of S sources among N nodes and a low complexity algorithm that is close to optimal for computing the centroid of S sources among N nodes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13688v1</guid>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>cs.SI</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Chou</dc:creator>
    </item>
    <item>
      <title>Learning Tree-Structured Composition of Data Augmentation</title>
      <link>https://arxiv.org/abs/2408.14381</link>
      <description>arXiv:2408.14381v1 Announce Type: cross 
Abstract: Data augmentation is widely used for training a neural network given little labeled data. A common practice of augmentation training is applying a composition of multiple transformations sequentially to the data. Existing augmentation methods such as RandAugment randomly sample from a list of pre-selected transformations, while methods such as AutoAugment apply advanced search to optimize over an augmentation set of size $k^d$, which is the number of transformation sequences of length $d$, given a list of $k$ transformations.
  In this paper, we design efficient algorithms whose running time complexity is much faster than the worst-case complexity of $O(k^d)$, provably. We propose a new algorithm to search for a binary tree-structured composition of $k$ transformations, where each tree node corresponds to one transformation. The binary tree generalizes sequential augmentations, such as the SimCLR augmentation scheme for contrastive learning. Using a top-down, recursive search procedure, our algorithm achieves a runtime complexity of $O(2^d k)$, which is much faster than $O(k^d)$ as $k$ increases above $2$. We apply our algorithm to tackle data distributions with heterogeneous subpopulations by searching for one tree in each subpopulation and then learning a weighted combination, resulting in a forest of trees.
  We validate our proposed algorithms on numerous graph and image datasets, including a multi-label graph classification dataset we collected. The dataset exhibits significant variations in the sizes of graphs and their average degrees, making it ideal for studying data augmentation. We show that our approach can reduce the computation cost by 43% over existing search methods while improving performance by 4.3%. The tree structures can be used to interpret the relative importance of each transformation, such as identifying the important transformations on small vs. large graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14381v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>cs.DS</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dongyue Li, Kailai Chen, Predrag Radivojac, Hongyang R. Zhang</dc:creator>
    </item>
    <item>
      <title>Exponentially Reduced Circuit Depths Using Trotter Error Mitigation</title>
      <link>https://arxiv.org/abs/2408.14385</link>
      <description>arXiv:2408.14385v1 Announce Type: cross 
Abstract: Product formulae are a popular class of digital quantum simulation algorithms due to their conceptual simplicity, low overhead, and performance which often exceeds theoretical expectations. Recently, Richardson extrapolation and polynomial interpolation have been proposed to mitigate the Trotter error incurred by use of these formulae. This work provides an improved, rigorous analysis of these techniques for the task of calculating time-evolved expectation values. We demonstrate that, to achieve error $\epsilon$ in a simulation of time $T$ using a $p^\text{th}$-order product formula with extrapolation, circuits depths of $O\left(T^{1+1/p} \textrm{polylog}(1/\epsilon)\right)$ are sufficient -- an exponential improvement in the precision over product formulae alone. Furthermore, we achieve commutator scaling, improve the complexity with $T$, and do not require fractional implementations of Trotter steps. Our results provide a more accurate characterisation of the algorithmic error mitigation techniques currently proposed to reduce Trotter error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14385v1</guid>
      <category>quant-ph</category>
      <category>cond-mat.str-el</category>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>James D. Watson, Jacob Watkins</dc:creator>
    </item>
    <item>
      <title>Path-Reporting Distance Oracles with Logarithmic Stretch and Size O(n loglog n)</title>
      <link>https://arxiv.org/abs/2304.04445</link>
      <description>arXiv:2304.04445v4 Announce Type: replace 
Abstract: Given an $n$-vertex undirected graph $G=(V,E,w)$, and a parameter $k\geq1$, a path-reporting distance oracle (or PRDO) is a data structure of size $S(n,k)$, that given a query $(u,v)\in V^2$, returns an $f(k)$-approximate shortest $u-v$ path $P$ in $G$ within time $q(k)+O(|P|)$. Here $S(n,k)$, $f(k)$ and $q(k)$ are arbitrary functions.
  A landmark PRDO due to Thorup and Zwick, with an improvement of Wulff-Nilsen, has $S(n,k)=O(k\cdot n^{1+\frac{1}{k}})$, $f(k)=2k-1$ and $q(k)=O(\log k)$. The size of this oracle is $\Omega(n\log n)$ for all $k$. Elkin and Pettie and Neiman and Shabat devised much sparser PRDOs, but their stretch was polynomially larger than the optimal $2k-1$. On the other hand, for non-path-reporting distance oracles, Chechik devised a result with $S(n,k)=O(n^{1+\frac{1}{k}})$, $f(k)=2k-1$ and $q(k)=O(1)$.
  In this paper we make a dramatic progress in bridging the gap between path-reporting and non-path-reporting distance oracles. We devise a PRDO with size $S(n,k)=O(\lceil\frac{k\log\log n}{\log n}\rceil\cdot n^{1+\frac{1}{k}})$, stretch $f(k)=O(k)$ and query time $q(k)=O(\log\lceil\frac{k\log\log n}{\log n}\rceil)$. We can also have size $O(n^{1+\frac{1}{k}})$, stretch $O(k\cdot\lceil\frac{k\log\log n}{\log n}\rceil)$ and query time $q(k)=O(\log\lceil\frac{k\log\log n}{\log n}\rceil)$.
  Our results on PRDOs are based on novel constructions of approximate distance preservers, that we devise in this paper. Specifically, we show that for any $\epsilon&gt;0$, any $k=1,2,...$, and any graph $G$ and a collection $\mathcal{P}$ of $p$ vertex pairs, there exists a $(1+\epsilon)$-approximate preserver with $O(\gamma(\epsilon,k)\cdot p+n\log k+n^{1+\frac{1}{k}})$ edges, where $\gamma(\epsilon,k)=(\frac{\log k}{\epsilon})^{O(\log k)}$. These new preservers are significantly sparser than the previous state-of-the-art approximate preservers due to Kogan and Parter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.04445v4</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Elkin, Idan Shabat</dc:creator>
    </item>
    <item>
      <title>Selection Improvements on the Parallel Iterative Algorithm for Stable Matching</title>
      <link>https://arxiv.org/abs/2401.07467</link>
      <description>arXiv:2401.07467v3 Announce Type: replace 
Abstract: Sequential algorithms for the Stable Matching Problem are often too slow in the context of some large scale applications like switch scheduling. Parallel architectures can offer a notable decrease in runtime complexity. We propose a stable matching algorithm using $n^2$ processors that converges in $O(n log(n))$ average runtime. The algorithm is structurally based on the Parallel Iterative Improvement (PII) algorithm, where we improve the convergence rate from $90\%$ to $100\%$ over a large number of trials. We suggest alternative selection methods for pairs in the PII algorithm, called Right-Minimum and Dynamic Selection, as well as a faster preprocessing step, called Quick Initialization, resulting in full convergence over $3.6$ million trials and significantly improved runtime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.07467v3</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Scott Wynn, Alec Kyritsis, Stephora Alberi, Enyue Lu</dc:creator>
    </item>
    <item>
      <title>Sensitivity, Proximity and FPT Algorithms for Exact Matroid Problems</title>
      <link>https://arxiv.org/abs/2404.03747</link>
      <description>arXiv:2404.03747v2 Announce Type: replace 
Abstract: We consider the problem of finding a basis of a matroid with weight exactly equal to a given target. Here weights can be discrete values from $\{-\Delta,\ldots,\Delta\}$ or more generally $m$-dimensional vectors of such discrete values. We resolve the parameterized complexity completely, by presenting an FPT algorithm parameterized by $\Delta$ and $m$ for arbitrary matroids. Prior to our work, no such algorithms were known even when weights are in $\{0,1\}$, or arbitrary $\Delta$ and $m=1$. Our main technical contributions are new proximity and sensitivity bounds for matroid problems, independent of the number of elements. These bounds imply FPT algorithms via matroid intersection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03747v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Friedrich Eisenbrand, Lars Rohwedder, Karol W\k{e}grzycki</dc:creator>
    </item>
    <item>
      <title>Enumerating models of DNF faster: breaking the dependency on the formula size</title>
      <link>https://arxiv.org/abs/1810.04006</link>
      <description>arXiv:1810.04006v3 Announce Type: replace-cross 
Abstract: In this article, we study the problem of enumerating the models of DNF formulas. The aim is to provide enumeration algorithms with a delay that depends polynomially on the size of each model and not on the size of the formula, which can be exponentially larger. We succeed for two subclasses of DNF formulas: we provide a constant delay algorithm for $k$-DNF with fixed $k$ by an appropriate amortization method and we give a quadratic delay algorithm for monotone formulas. We then focus on the \emph{average delay} of enumeration algorithms and show how to obtain a sublinear delay in the formula size.</description>
      <guid isPermaLink="false">oai:arXiv.org:1810.04006v3</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.dam.2020.02.014</arxiv:DOI>
      <arxiv:journal_reference>Discrete Applied Mathematics, Volume 303, 2021, Pages 203-215</arxiv:journal_reference>
      <dc:creator>Florent Capelli, Yann Strozecki</dc:creator>
    </item>
    <item>
      <title>From Tutte to Floater and Gotsman: On the Resolution of Planar Straight-line Drawings and Morphs</title>
      <link>https://arxiv.org/abs/2108.09483</link>
      <description>arXiv:2108.09483v5 Announce Type: replace-cross 
Abstract: The algorithm of Tutte for constructing convex planar straight-line drawings and the algorithm of Floater and Gotsman for constructing planar straight-line morphs are among the most popular graph drawing algorithms. In this paper, focusing on maximal plane graphs, we prove tight bounds on the resolution of the planar straight-line drawings produced by Floater's algorithm, which is a broad generalization of Tutte's algorithm. Further, we use such a result in order to prove a lower bound on the resolution of the drawings of maximal plane graphs produced by Floater and Gotsman's morphing algorithm. Finally, we show that such a morphing algorithm might produce drawings with exponentially-small resolution, even when transforming drawings with polynomial resolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2108.09483v5</guid>
      <category>cs.CG</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giuseppe Di Battista, Fabrizio Frati</dc:creator>
    </item>
    <item>
      <title>Keeping the Harmony Between Neighbors: Local Fairness in Graph Fair Division</title>
      <link>https://arxiv.org/abs/2401.14825</link>
      <description>arXiv:2401.14825v2 Announce Type: replace-cross 
Abstract: We study the problem of allocating indivisible resources under the connectivity constraints of a graph $G$. This model, initially introduced by Bouveret et al. (published in IJCAI, 2017), effectively encompasses a diverse array of scenarios characterized by spatial or temporal limitations, including the division of land plots and the allocation of time plots. In this paper, we introduce a novel fairness concept that integrates local comparisons within the social network formed by a connected allocation of the item graph. Our particular focus is to achieve pairwise-maximin fair share (PMMS) among the "neighbors" within this network. For any underlying graph structure, we show that a connected allocation that maximizes Nash welfare guarantees a $(1/2)$-PMMS fairness. Moreover, for two agents, we establish that a $(3/4)$-PMMS allocation can be efficiently computed. Additionally, we demonstrate that for three agents and the items aligned on a path, a PMMS allocation is always attainable and can be computed in polynomial time. Lastly, when agents have identical additive utilities, we present a pseudo-polynomial-time algorithm for a $(3/4)$-PMMS allocation, irrespective of the underlying graph $G$. Furthermore, we provide a polynomial-time algorithm for obtaining a PMMS allocation when $G$ is a tree.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.14825v2</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <category>cs.MA</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Halvard Hummel, Ayumi Igarashi</dc:creator>
    </item>
    <item>
      <title>Convergence of a L2 regularized Policy Gradient Algorithm for the Multi Armed Bandit</title>
      <link>https://arxiv.org/abs/2402.06388</link>
      <description>arXiv:2402.06388v2 Announce Type: replace-cross 
Abstract: Although Multi Armed Bandit (MAB) on one hand and the policy gradient approach on the other hand are among the most used frameworks of Reinforcement Learning, the theoretical properties of the policy gradient algorithm used for MAB have not been given enough attention. We investigate in this work the convergence of such a procedure for the situation when a $L2$ regularization term is present jointly with the 'softmax' parametrization. We prove convergence under appropriate technical hypotheses and test numerically the procedure including situations beyond the theoretical setting. The tests show that a time dependent regularized procedure can improve over the canonical approach especially when the initial guess is far from the solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.06388v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Stefana Anita, Gabriel Turinici</dc:creator>
    </item>
  </channel>
</rss>
