<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.med-ph updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.med-ph</link>
    <description>physics.med-ph updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.med-ph" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 01 Jul 2025 04:06:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>What if each voxel were measured with a different diffusion protocol?</title>
      <link>https://arxiv.org/abs/2506.22650</link>
      <description>arXiv:2506.22650v1 Announce Type: new 
Abstract: Expansion of diffusion MRI (dMRI) both into the realm of strong gradients, and into accessible imaging with portable low-field devices, brings about the challenge of gradient nonlinearities. Spatial variations of the diffusion gradients make diffusion weightings and directions non-uniform across the field of view, and deform perfect shells in the q-space designed for isotropic directional coverage. Such imperfections hinder parameter estimation: Anisotropic shells hamper the deconvolution of fiber orientation distribution function (fODF), while brute-force retraining of a nonlinear regressor for each unique set of directions and diffusion weightings is computationally inefficient. Here we propose a protocol-independent parameter estimation (PIPE) method that enables fast parameter estimation for the most general case where the scan in each voxel is acquired with a different protocol in q-space. PIPE applies for any spherical convolution-based dMRI model, irrespective of its complexity, which makes it suitable both for white and gray matter in the brain or spinal cord, and for other tissues where fiber bundles have the same properties within a voxel (fiber response), but are distributed with an arbitrary fODF. In vivo human MRI experiments on a high-performance system show that PIPE can map fiber response and fODF parameters for the whole brain in the presence of significant gradient nonlinearities in under 3 minutes. PIPE enables fast parameter estimation in the presence of arbitrary gradient nonlinearities, eliminating the need to arrange dMRI in shells or to retrain the estimator for different protocols in each voxel. PIPE applies for any model based on a convolution of a voxel-wise fiber response and fODF, and data from varying b-tensor shapes, diffusion/echo times, and other scan parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22650v1</guid>
      <category>physics.med-ph</category>
      <category>physics.bio-ph</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Santiago Coelho, Gregory Lemberskiy, Ante Zhu, Hong-Hsi Lee, Nastaren Abad, Thomas K. F. Foo, Els Fieremans, Dmitry S. Novikov</dc:creator>
    </item>
    <item>
      <title>Supervised Diffusion-Model-Based PET Image Reconstruction</title>
      <link>https://arxiv.org/abs/2506.24034</link>
      <description>arXiv:2506.24034v1 Announce Type: new 
Abstract: Diffusion models (DMs) have recently been introduced as a regularizing prior for PET image reconstruction, integrating DMs trained on high-quality PET images with unsupervised schemes that condition on measured data. While these approaches have potential generalization advantages due to their independence from the scanner geometry and the injected activity level, they forgo the opportunity to explicitly model the interaction between the DM prior and noisy measurement data, potentially limiting reconstruction accuracy. To address this, we propose a supervised DM-based algorithm for PET reconstruction. Our method enforces the non-negativity of PET's Poisson likelihood model and accommodates the wide intensity range of PET images. Through experiments on realistic brain PET phantoms, we demonstrate that our approach outperforms or matches state-of-the-art deep learning-based methods quantitatively across a range of dose levels. We further conduct ablation studies to demonstrate the benefits of the proposed components in our model, as well as its dependence on training data, parameter count, and number of diffusion steps. Additionally, we show that our approach enables more accurate posterior sampling than unsupervised DM-based methods, suggesting improved uncertainty estimation. Finally, we extend our methodology to a practical approach for fully 3D PET and present example results from real [$^{18}$F]FDG brain PET data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.24034v1</guid>
      <category>physics.med-ph</category>
      <category>cs.CV</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>George Webber, Alexander Hammers, Andrew P King, Andrew J Reader</dc:creator>
    </item>
    <item>
      <title>Scout-Dose-TCM: Direct and Prospective Scout-Based Estimation of Personalized Organ Doses from Tube Current Modulated CT Exams</title>
      <link>https://arxiv.org/abs/2506.24062</link>
      <description>arXiv:2506.24062v1 Announce Type: new 
Abstract: This study proposes Scout-Dose-TCM for direct, prospective estimation of organ-level doses under tube current modulation (TCM) and compares its performance to two established methods. We analyzed contrast-enhanced chest-abdomen-pelvis CT scans from 130 adults (120 kVp, TCM). Reference doses for six organs (lungs, kidneys, liver, pancreas, bladder, spleen) were calculated using MC-GPU and TotalSegmentator. Based on these, we trained Scout-Dose-TCM, a deep learning model that predicts organ doses corresponding to discrete cosine transform (DCT) basis functions, enabling real-time estimates for any TCM profile. The model combines a feature learning module that extracts contextual information from lateral and frontal scouts and scan range with a dose learning module that output DCT-based dose estimates. A customized loss function incorporated the DCT formulation during training. For comparison, we implemented size-specific dose estimation per AAPM TG 204 (Global CTDIvol) and its organ-level TCM-adapted version (Organ CTDIvol). A 5-fold cross-validation assessed generalizability by comparing mean absolute percentage dose errors and r-squared correlations with benchmark doses. Average absolute percentage errors were 13% (Global CTDIvol), 9% (Organ CTDIvol), and 7% (Scout-Dose-TCM), with bladder showing the largest discrepancies (15%, 13%, and 9%). Statistical tests confirmed Scout-Dose-TCM significantly reduced errors vs. Global CTDIvol across most organs and improved over Organ CTDIvol for the liver, bladder, and pancreas. It also achieved higher r-squared values, indicating stronger agreement with Monte Carlo benchmarks. Scout-Dose-TCM outperformed Global CTDIvol and was comparable to or better than Organ CTDIvol, without requiring organ segmentations at inference, demonstrating its promise as a tool for prospective organ-level dose estimation in CT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.24062v1</guid>
      <category>physics.med-ph</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maria Jose Medrano, Sen Wang, Liyan Sun, Abdullah-Al-Zubaer Imran, Jennie Cao, Grant Stevens, Justin Ruey Tse, Adam S. Wang</dc:creator>
    </item>
    <item>
      <title>Physics informed guided diffusion for accelerated multi-parametric MRI reconstruction</title>
      <link>https://arxiv.org/abs/2506.23311</link>
      <description>arXiv:2506.23311v1 Announce Type: cross 
Abstract: We introduce MRF-DiPh, a novel physics informed denoising diffusion approach for multiparametric tissue mapping from highly accelerated, transient-state quantitative MRI acquisitions like Magnetic Resonance Fingerprinting (MRF). Our method is derived from a proximal splitting formulation, incorporating a pretrained denoising diffusion model as an effective image prior to regularize the MRF inverse problem. Further, during reconstruction it simultaneously enforces two key physical constraints: (1) k-space measurement consistency and (2) adherence to the Bloch response model. Numerical experiments on in-vivo brain scans data show that MRF-DiPh outperforms deep learning and compressed sensing MRF baselines, providing more accurate parameter maps while better preserving measurement fidelity and physical model consistency-critical for solving reliably inverse problems in medical imaging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23311v1</guid>
      <category>eess.IV</category>
      <category>cs.LG</category>
      <category>physics.med-ph</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Perla Mayo, Carolin M. Pirkl, Alin Achim, Bjoern Menze, Mohammad Golbabaee</dc:creator>
    </item>
    <item>
      <title>FD-DiT: Frequency Domain-Directed Diffusion Transformer for Low-Dose CT Reconstruction</title>
      <link>https://arxiv.org/abs/2506.23466</link>
      <description>arXiv:2506.23466v1 Announce Type: cross 
Abstract: Low-dose computed tomography (LDCT) reduces radiation exposure but suffers from image artifacts and loss of detail due to quantum and electronic noise, potentially impacting diagnostic accuracy. Transformer combined with diffusion models has been a promising approach for image generation. Nevertheless, existing methods exhibit limitations in preserving finegrained image details. To address this issue, frequency domain-directed diffusion transformer (FD-DiT) is proposed for LDCT reconstruction. FD-DiT centers on a diffusion strategy that progressively introduces noise until the distribution statistically aligns with that of LDCT data, followed by denoising processing. Furthermore, we employ a frequency decoupling technique to concentrate noise primarily in high-frequency domain, thereby facilitating effective capture of essential anatomical structures and fine details. A hybrid denoising network is then utilized to optimize the overall data reconstruction process. To enhance the capability in recognizing high-frequency noise, we incorporate sliding sparse local attention to leverage the sparsity and locality of shallow-layer information, propagating them via skip connections for improving feature representation. Finally, we propose a learnable dynamic fusion strategy for optimal component integration. Experimental results demonstrate that at identical dose levels, LDCT images reconstructed by FD-DiT exhibit superior noise and artifact suppression compared to state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23466v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>physics.med-ph</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiqing Liu, Guoquan Wei, Zekun Zhou, Yiyang Wen, Liu Shi, Qiegen Liu</dc:creator>
    </item>
    <item>
      <title>Artificial Intelligence-assisted Pixel-level Lung (APL) Scoring for Fast and Accurate Quantification in Ultra-short Echo-time MRI</title>
      <link>https://arxiv.org/abs/2506.23506</link>
      <description>arXiv:2506.23506v1 Announce Type: cross 
Abstract: Lung magnetic resonance imaging (MRI) with ultrashort echo-time (UTE) represents a recent breakthrough in lung structure imaging, providing image resolution and quality comparable to computed tomography (CT). Due to the absence of ionising radiation, MRI is often preferred over CT in paediatric diseases such as cystic fibrosis (CF), one of the most common genetic disorders in Caucasians. To assess structural lung damage in CF imaging, CT scoring systems provide valuable quantitative insights for disease diagnosis and progression. However, few quantitative scoring systems are available in structural lung MRI (e.g., UTE-MRI). To provide fast and accurate quantification in lung MRI, we investigated the feasibility of novel Artificial intelligence-assisted Pixel-level Lung (APL) scoring for CF. APL scoring consists of 5 stages, including 1) image loading, 2) AI lung segmentation, 3) lung-bounded slice sampling, 4) pixel-level annotation, and 5) quantification and reporting. The results shows that our APL scoring took 8.2 minutes per subject, which was more than twice as fast as the previous grid-level scoring. Additionally, our pixel-level scoring was statistically more accurate (p=0.021), while strongly correlating with grid-level scoring (R=0.973, p=5.85e-9). This tool has great potential to streamline the workflow of UTE lung MRI in clinical settings, and be extended to other structural lung MRI sequences (e.g., BLADE MRI), and for other lung diseases (e.g., bronchopulmonary dysplasia).</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23506v1</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>physics.med-ph</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bowen Xin, Rohan Hickey, Tamara Blake, Jin Jin, Claire E Wainwright, Thomas Benkert, Alto Stemmer, Peter Sly, David Coman, Jason Dowling</dc:creator>
    </item>
    <item>
      <title>Fourier-based Inversion of Partial X-ray Transforms in n Dimensions</title>
      <link>https://arxiv.org/abs/2505.05372</link>
      <description>arXiv:2505.05372v3 Announce Type: replace 
Abstract: We present two theorems describing analytic left-inverses of partial X-ray transforms. The first theorem concerns X-ray data collected with an arbitrary distribution of parallel projections; it contains a convolution-backprojection formula and a backprojection-convolution formula for recovering the transformed volume, provided the data is sufficient. The second theorem concerns X-ray data collected with a cone-beam; it contains a backprojection-convolution formula for recovering the transformed volume, provided the data is amenable to this method (for example: (n-1)-dimensional source loci that `surround' the reconstruction support; detectors of finite size are supported). These theorems are the outcome of a modestly general and rigorous investigation undertaken into the existence of backprojection-convolution methods in n-dimensional space. Necessary and sufficient conditions on the experiment geometry are established for the existence of such methods, as are the particular error metrics minimised by backprojection-convolution methods and the uniqueness of those minimum-error solutions. A major practical outcome of this work is the production of the first known exact inversion methods for cone-beam geometries where the X-ray source point loci are multidimensional, such as (in 3D) a cylinder or a sphere of X-ray source positions. A separate article describes a practical computer implementation for the case of a cylinder in 3D.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05372v3</guid>
      <category>physics.med-ph</category>
      <category>math.FA</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Murdock G. Grewar</dc:creator>
    </item>
    <item>
      <title>Pixel super-resolved virtual staining of label-free tissue using diffusion models</title>
      <link>https://arxiv.org/abs/2410.20073</link>
      <description>arXiv:2410.20073v2 Announce Type: replace-cross 
Abstract: Virtual staining of tissue offers a powerful tool for transforming label-free microscopy images of unstained tissue into equivalents of histochemically stained samples. This study presents a diffusion model-based super-resolution virtual staining approach utilizing a Brownian bridge process to enhance both the spatial resolution and fidelity of label-free virtual tissue staining, addressing the limitations of traditional deep learning-based methods. Our approach integrates novel sampling techniques into a diffusion model-based image inference process to significantly reduce the variance in the generated virtually stained images, resulting in more stable and accurate outputs. Blindly applied to lower-resolution auto-fluorescence images of label-free human lung tissue samples, the diffusion-based super-resolution virtual staining model consistently outperformed conventional approaches in resolution, structural similarity and perceptual accuracy, successfully achieving a super-resolution factor of 4-5x, increasing the output space-bandwidth product by 16-25-fold compared to the input label-free microscopy images. Diffusion-based super-resolved virtual tissue staining not only improves resolution and image quality but also enhances the reliability of virtual staining without traditional chemical staining, offering significant potential for clinical diagnostics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20073v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>physics.med-ph</category>
      <category>physics.optics</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1038/s41467-025-60387-z</arxiv:DOI>
      <arxiv:journal_reference>Nature Communications (2025)</arxiv:journal_reference>
      <dc:creator>Yijie Zhang, Luzhe Huang, Nir Pillar, Yuzhu Li, Hanlong Chen, Aydogan Ozcan</dc:creator>
    </item>
  </channel>
</rss>
