<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 18 Jun 2025 01:33:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>On Hierarchies of Fairness Notions in Cake Cutting: From Proportionality to Super Envy-Freeness</title>
      <link>https://arxiv.org/abs/2506.12950</link>
      <description>arXiv:2506.12950v1 Announce Type: new 
Abstract: We consider the classic cake-cutting problem of producing fair allocations for $n$ agents, in the Robertson-Webb query model. In this model, it is known that: (i) proportional allocations can be computed using $O(n \log n)$ queries, and this is optimal for deterministic protocols; (ii) envy-free allocations (a subset of proportional allocations) can be computed using $O\left( n^{n^{n^{n^{n^{n}}}}} \right)$ queries, and the best known lower bound is $\Omega(n^2)$; (iii) perfect allocations (a subset of envy-free allocations) cannot be computed using a bounded (in $n$) number of queries.
  In this work, we introduce two hierarchies of new fairness notions: Complement Harmonically Bounded (CHB) and Complement Linearly Bounded (CLB). Intuitively, these notions of fairness ask that, for every agent $i$, the collective value that a group of agents has (from the perspective of agent $i$) is limited. CHB-$k$ and CLB-$k$ coincide with proportionality for $k=1$. For all $k \leq n$, CHB-$k$ allocations are a superset of envy-free allocations (i.e., easier to find). On the other hand, for $k \in [2, \lceil n/2 \rceil - 1]$, CLB-$k$ allocations are incomparable to envy-free allocations. For $k \geq \lceil n/2 \rceil$, CLB-$k$ allocations are a subset of envy-free allocations (i.e., harder to find).
  We prove that CHB-$n$ allocations can be computed using $O(n^4)$ queries in the Robertson-Webb model. On the flip side, finding CHB-$2$ (and therefore all CHB-$k$ for $k \geq 2$) allocations requires $\Omega(n^2)$ queries, while CLB-$2$ (and therefore all CLB-$k$ for $k \geq 2$) allocations cannot be computed using a bounded (in $n$) number of queries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12950v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Arnav Mehra, Alexandros Psomas</dc:creator>
    </item>
    <item>
      <title>Quantitative Relaxations of Arrow's Axioms</title>
      <link>https://arxiv.org/abs/2506.12961</link>
      <description>arXiv:2506.12961v1 Announce Type: new 
Abstract: In this paper we develop a novel approach to relaxing Arrow's axioms for voting rules, addressing a long-standing critique in social choice theory. Classical axioms (often styled as fairness axioms or fairness criteria) are assessed in a binary manner, so that a voting rule fails the axiom if it fails in even one corner case. Many authors have proposed a probabilistic framework to soften the axiomatic approach. Instead of immediately passing to random preference profiles, we begin by measuring the degree to which an axiom is upheld or violated on a given profile. We focus on two foundational axioms-Independence of Irrelevant Alternatives (IIA) and Unanimity (U)-and extend them to take values in $[0,1]$. Our $\sigma_{IIA}$ measures the stability of a voting rule when candidates are removed from consideration, while $\sigma_{U}$ captures the degree to which the outcome respects majority preferences. Together, these metrics quantify how a voting rule navigates the fundamental trade-off highlighted by Arrow's Theorem. We show that $\sigma_{IIA}\equiv 1$ recovers classical IIA, and $\sigma_{U}&gt;0$ recovers classical Unanimity, allowing a quantitative restatement of Arrow's Theorem. In the empirical part of the paper, we test these metrics on two kinds of data: a set of over 1000 ranked choice preference profiles from Scottish local elections, and a batch of synthetic preference profiles generated with a Bradley-Terry-type model. We use those to investigate four positional voting rules-Plurality, 2-Approval, 3-Approval, and the Borda rule-as well as the iterative rule known as Single Transferable Vote (STV). The Borda rule consistently receives the highest $\sigma_{IIA}$ and $\sigma_{U}$ scores across observed and synthetic elections. This compares interestingly with a recent result of Maskin showing that weakening IIA to include voter preference intensity uniquely selects Borda.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12961v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Suvadip Sana, Daniel Brous, Martin T. Wells, Moon Duchin</dc:creator>
    </item>
    <item>
      <title>One-dimensional vs. Multi-dimensional Pricing in Blockchain Protocols</title>
      <link>https://arxiv.org/abs/2506.13271</link>
      <description>arXiv:2506.13271v1 Announce Type: new 
Abstract: Blockchain transactions consume diverse resources, foremost among them storage, but also computation, communication, and others. Efficiently charging for these resources is crucial for effective system resource allocation and long-term economic viability. The prevailing approach, one-dimensional pricing, sets a single price for a linear combination of resources. However, this often leads to under-utilization when resource capacities are limited. Multi-dimensional pricing, which independently prices each resource, offers an alternative but presents challenges in price discovery.
  This work focuses on the welfare achieved by these two schemes. We prove that multi-dimensional pricing is superior under stable blockchain conditions. Conversely, we show that one-dimensional pricing outperforms its multi-dimensional counterpart in transient states, exhibiting faster convergence and greater computational tractability. These results highlight a critical trade-off: while multi-dimensional pricing offers efficiency gains at equilibrium, its implementation incurs costs associated with system transitions. Our findings underscore the necessity for a deeper understanding of these transient effects before widespread adoption. Finally, we propose mechanisms that aim to mitigate some of these issues, paving the way for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13271v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aggelos Kiayias, Elias Koutsoupias, Giorgos Panagiotakos, Kyriaki Zioga</dc:creator>
    </item>
    <item>
      <title>The impact of uncertainty on regularized learning in games</title>
      <link>https://arxiv.org/abs/2506.13286</link>
      <description>arXiv:2506.13286v1 Announce Type: new 
Abstract: In this paper, we investigate how randomness and uncertainty influence learning in games. Specifically, we examine a perturbed variant of the dynamics of "follow-the-regularized-leader" (FTRL), where the players' payoff observations and strategy updates are continually impacted by random shocks. Our findings reveal that, in a fairly precise sense, "uncertainty favors extremes": in any game, regardless of the noise level, every player's trajectory of play reaches an arbitrarily small neighborhood of a pure strategy in finite time (which we estimate). Moreover, even if the player does not ultimately settle at this strategy, they return arbitrarily close to some (possibly different) pure strategy infinitely often. This prompts the question of which sets of pure strategies emerge as robust predictions of learning under uncertainty. We show that (a) the only possible limits of the FTRL dynamics under uncertainty are pure Nash equilibria; and (b) a span of pure strategies is stable and attracting if and only if it is closed under better replies. Finally, we turn to games where the deterministic dynamics are recurrent - such as zero-sum games with interior equilibria - and we show that randomness disrupts this behavior, causing the stochastic dynamics to drift toward the boundary on average.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13286v1</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pierre-Louis Cauvin, Davide Legacci, Panayotis Mertikopoulos</dc:creator>
    </item>
    <item>
      <title>EMERGENT: Efficient and Manipulation-resistant Matching using GFlowNets</title>
      <link>https://arxiv.org/abs/2506.12033</link>
      <description>arXiv:2506.12033v1 Announce Type: cross 
Abstract: The design of fair and efficient algorithms for allocating public resources, such as school admissions, housing, or medical residency, has a profound social impact. In one-sided matching problems, where individuals are assigned to items based on ranked preferences, a fundamental trade-off exists between efficiency and strategyproofness. Existing algorithms like Random Serial Dictatorship (RSD), Probabilistic Serial (PS), and Rank Minimization (RM) capture only one side of this trade-off: RSD is strategyproof but inefficient, while PS and RM are efficient but incentivize manipulation. We propose EMERGENT, a novel application of Generative Flow Networks (GFlowNets) to one-sided matching, leveraging its ability to sample diverse, high-reward solutions. In our approach, efficient and manipulation-resistant matches emerge naturally: high-reward solutions yield efficient matches, while the stochasticity of GFlowNets-based outputs reduces incentives for manipulation. Experiments show that EMERGENT outperforms RSD in rank efficiency while significantly reducing strategic vulnerability compared to matches produced by RM and PS. Our work highlights the potential of GFlowNets for applications involving social choice mechanisms, where it is crucial to balance efficiency and manipulability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12033v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mayesha Tasnim, Erman Acar, Sennay Ghebreab</dc:creator>
    </item>
    <item>
      <title>Perfect Secrecy in the Wild: A Characterization</title>
      <link>https://arxiv.org/abs/2506.12416</link>
      <description>arXiv:2506.12416v1 Announce Type: cross 
Abstract: Alice wishes to reveal the state $X$ to Bob, if he knows some other information $Y$ also known to her. If Bob does not, she wishes to reveal nothing about $X$ at all. When can Alice accomplish this? We provide a simple necessary and sufficient condition on the joint distribution of $X$ and $Y$. Shannon's result on the perfect secrecy of the one-time pad follows as a special case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12416v1</guid>
      <category>econ.TH</category>
      <category>cs.GT</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Costas Cavounidis, Massimiliano Furlan, Alkis Georgiadis-Harris</dc:creator>
    </item>
    <item>
      <title>Trust-MARL: Trust-Based Multi-Agent Reinforcement Learning Framework for Cooperative On-Ramp Merging Control in Heterogeneous Traffic Flow</title>
      <link>https://arxiv.org/abs/2506.12600</link>
      <description>arXiv:2506.12600v1 Announce Type: cross 
Abstract: Intelligent transportation systems require connected and automated vehicles (CAVs) to conduct safe and efficient cooperation with human-driven vehicles (HVs) in complex real-world traffic environments. However, the inherent unpredictability of human behaviour, especially at bottlenecks such as highway on-ramp merging areas, often disrupts traffic flow and compromises system performance. To address the challenge of cooperative on-ramp merging in heterogeneous traffic environments, this study proposes a trust-based multi-agent reinforcement learning (Trust-MARL) framework. At the macro level, Trust-MARL enhances global traffic efficiency by leveraging inter-agent trust to improve bottleneck throughput and mitigate traffic shockwave through emergent group-level coordination. At the micro level, a dynamic trust mechanism is designed to enable CAVs to adjust their cooperative strategies in response to real-time behaviors and historical interactions with both HVs and other CAVs. Furthermore, a trust-triggered game-theoretic decision-making module is integrated to guide each CAV in adapting its cooperation factor and executing context-aware lane-changing decisions under safety, comfort, and efficiency constraints. An extensive set of ablation studies and comparative experiments validates the effectiveness of the proposed Trust-MARL approach, demonstrating significant improvements in safety, efficiency, comfort, and adaptability across varying CAV penetration rates and traffic densities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12600v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.GT</category>
      <category>cs.RO</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jie Pan, Tianyi Wang, Christian Claudel, Jing Shi</dc:creator>
    </item>
    <item>
      <title>Semivalue-based data valuation is arbitrary and gameable</title>
      <link>https://arxiv.org/abs/2506.12619</link>
      <description>arXiv:2506.12619v1 Announce Type: cross 
Abstract: The game-theoretic notion of the semivalue offers a popular framework for credit attribution and data valuation in machine learning. Semivalues have been proposed for a variety of high-stakes decisions involving data, such as determining contributor compensation, acquiring data from external sources, or filtering out low-value datapoints. In these applications, semivalues depend on the specification of a utility function that maps subsets of data to a scalar score. While it is broadly agreed that this utility function arises from a composition of a learning algorithm and a performance metric, its actual instantiation involves numerous subtle modeling choices. We argue that this underspecification leads to varying degrees of arbitrariness in semivalue-based valuations. Small, but arguably reasonable changes to the utility function can induce substantial shifts in valuations across datapoints. Moreover, these valuation methodologies are also often gameable: low-cost adversarial strategies exist to exploit this ambiguity and systematically redistribute value among datapoints. Through theoretical constructions and empirical examples, we demonstrate that a bad-faith valuator can manipulate utility specifications to favor preferred datapoints, and that a good-faith valuator is left without principled guidance to justify any particular specification. These vulnerabilities raise ethical and epistemic concerns about the use of semivalues in several applications. We conclude by highlighting the burden of justification that semivalue-based approaches place on modelers and discuss important considerations for identifying appropriate uses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12619v1</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hannah Diehl, Ashia C. Wilson</dc:creator>
    </item>
    <item>
      <title>Fast and Furious Symmetric Learning in Zero-Sum Games: Gradient Descent as Fictitious Play</title>
      <link>https://arxiv.org/abs/2506.13086</link>
      <description>arXiv:2506.13086v1 Announce Type: cross 
Abstract: This paper investigates the sublinear regret guarantees of two non-no-regret algorithms in zero-sum games: Fictitious Play, and Online Gradient Descent with constant stepsizes. In general adversarial online learning settings, both algorithms may exhibit instability and linear regret due to no regularization (Fictitious Play) or small amounts of regularization (Gradient Descent). However, their ability to obtain tighter regret bounds in two-player zero-sum games is less understood. In this work, we obtain strong new regret guarantees for both algorithms on a class of symmetric zero-sum games that generalize the classic three-strategy Rock-Paper-Scissors to a weighted, n-dimensional regime. Under symmetric initializations of the players' strategies, we prove that Fictitious Play with any tiebreaking rule has $O(\sqrt{T})$ regret, establishing a new class of games for which Karlin's Fictitious Play conjecture holds. Moreover, by leveraging a connection between the geometry of the iterates of Fictitious Play and Gradient Descent in the dual space of payoff vectors, we prove that Gradient Descent, for almost all symmetric initializations, obtains a similar $O(\sqrt{T})$ regret bound when its stepsize is a sufficiently large constant. For Gradient Descent, this establishes the first "fast and furious" behavior (i.e., sublinear regret without time-vanishing stepsizes) for zero-sum games larger than 2x2.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13086v1</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John Lazarsfeld, Georgios Piliouras, Ryann Sim, Andre Wibisono</dc:creator>
    </item>
    <item>
      <title>Real Time Self-Tuning Adaptive Controllers on Temperature Control Loops using Event-based Game Theory</title>
      <link>https://arxiv.org/abs/2506.13164</link>
      <description>arXiv:2506.13164v1 Announce Type: cross 
Abstract: This paper presents a novel method for enhancing the adaptability of Proportional-Integral-Derivative (PID) controllers in industrial systems using event-based dynamic game theory, which enables the PID controllers to self-learn, optimize, and fine-tune themselves. In contrast to conventional self-learning approaches, our proposed framework offers an event-driven control strategy and game-theoretic learning algorithms. The players collaborate with the PID controllers to dynamically adjust their gains in response to set point changes and disturbances. We provide a theoretical analysis showing sound convergence guarantees for the game given suitable stability ranges of the PID controlled loop. We further introduce an automatic boundary detection mechanism, which helps the players to find an optimal initialization of action spaces and significantly reduces the exploration time. The efficacy of this novel methodology is validated through its implementation in the temperature control loop of a printing press machine. Eventually, the outcomes of the proposed intelligent self-tuning PID controllers are highly promising, particularly in terms of reducing overshoot and settling time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13164v1</guid>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Steve Yuwono, Muhammad Uzair Rana, Dorothea Schwung, Andreas Schwung</dc:creator>
    </item>
    <item>
      <title>A Game-Theoretic Negotiation Framework for Cross-Cultural Consensus in LLMs</title>
      <link>https://arxiv.org/abs/2506.13245</link>
      <description>arXiv:2506.13245v1 Announce Type: cross 
Abstract: The increasing prevalence of large language models (LLMs) is influencing global value systems. However, these models frequently exhibit a pronounced WEIRD (Western, Educated, Industrialized, Rich, Democratic) cultural bias due to lack of attention to minority values. This monocultural perspective may reinforce dominant values and marginalize diverse cultural viewpoints, posing challenges for the development of equitable and inclusive AI systems. In this work, we introduce a systematic framework designed to boost fair and robust cross-cultural consensus among LLMs. We model consensus as a Nash Equilibrium and employ a game-theoretic negotiation method based on Policy-Space Response Oracles (PSRO) to simulate an organized cross-cultural negotiation process. To evaluate this approach, we construct regional cultural agents using data transformed from the World Values Survey (WVS). Beyond the conventional model-level evaluation method, We further propose two quantitative metrics, Perplexity-based Acceptence and Values Self-Consistency, to assess consensus outcomes. Experimental results indicate that our approach generates consensus of higher quality while ensuring more balanced compromise compared to baselines. Overall, it mitigates WEIRD bias by guiding agents toward convergence through fair and gradual negotiation steps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13245v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.GT</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guoxi Zhang, Jiawei Chen, Tianzhuo Yang, Jiaming Ji, Yaodong Yang, Juntao Dai</dc:creator>
    </item>
    <item>
      <title>Deceptive Path Planning: A Bayesian Game Approach</title>
      <link>https://arxiv.org/abs/2506.13650</link>
      <description>arXiv:2506.13650v1 Announce Type: cross 
Abstract: This paper investigates how an autonomous agent can transmit information through its motion in an adversarial setting. We consider scenarios where an agent must reach its goal while deceiving an intelligent observer about its destination. We model this interaction as a dynamic Bayesian game between a mobile Attacker with a privately known goal and a Defender who infers the Attacker's intent to allocate defensive resources effectively. We use Perfect Bayesian Nash Equilibrium (PBNE) as our solution concept and propose a computationally efficient approach to find it. In the resulting equilibrium, the Defender employs a simple Markovian strategy, while the Attacker strategically balances deception and goal efficiency by stochastically mixing shortest and non-shortest paths to manipulate the Defender's beliefs. Numerical experiments demonstrate the advantages of our PBNE-based strategies over existing methods based on one-sided optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13650v1</guid>
      <category>eess.SY</category>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Violetta Rostobaya, James Berneburg, Yue Guan, Michael Dorothy, Daigo Shishika</dc:creator>
    </item>
    <item>
      <title>Online Learning for Equilibrium Pricing in Markets under Incomplete Information</title>
      <link>https://arxiv.org/abs/2303.11522</link>
      <description>arXiv:2303.11522v3 Announce Type: replace 
Abstract: The computation of equilibrium prices at which the supply of goods matches their demand typically relies on complete information on agents' private attributes, e.g., suppliers' cost functions, which are often unavailable in practice. Motivated by this practical consideration, we consider the problem of learning equilibrium prices over a horizon of $T$ periods in the incomplete information setting wherein a market operator seeks to satisfy the customer demand for a commodity by purchasing it from competing suppliers with cost functions unknown to the operator. We first consider the setting when suppliers' cost functions are fixed and develop algorithms that, on three pertinent regret metrics, simultaneously achieve a regret of $O(1)$ when the customer demand is constant over time, and $O(\sqrt{T})$ when the demand varies over time. In the setting when the suppliers' cost functions vary over time, we demonstrate that, in general, no online algorithm can achieve sublinear regret on all three metrics. Thus, we consider an augmented setting wherein the operator has access to hints/contexts that reflect the variation in the cost functions and propose an algorithm with sublinear regret in this augmented setting. Finally, we present numerical experiments that validate our results and discuss various model extensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.11522v3</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Devansh Jalota, Haoyuan Sun, Navid Azizan</dc:creator>
    </item>
    <item>
      <title>Static and Dynamic Jamming Games Over Wireless Channels With Mobile Strategic Players</title>
      <link>https://arxiv.org/abs/2306.10956</link>
      <description>arXiv:2306.10956v2 Announce Type: replace 
Abstract: We study a wireless jamming problem consisting of the competition between a legitimate receiver and a jammer, as a zero-sum game where the value to maximize/minimize is the channel capacity at the receiver's side. Most of the approaches found in the literature consider the two players to be stationary nodes. Instead, we investigate what happens when they can change location, specifically moving along a linear geometry. We frame this at first as a static game, which can be solved in closed form, and subsequently we extend it to a dynamic game under three different versions for what concerns completeness/perfection of mutual information about the adversary's position, corresponding to different assumptions of concealment/sequentiality of the moves, respectively. We first provide some theoretical conditions that hold for the static game and also help identify good strategies valid under any setup, including dynamic games. Since dynamic games, although more realistic, are characterized by a significantly expanded strategy space, we exploit reinforcement learning to obtain efficient strategies that lead to equilibrium outcomes. We show how theoretical findings can be used to train smart agents to play the game and validate our approach in practical settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.10956v2</guid>
      <category>cs.GT</category>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giovanni Perin, Leonardo Badia</dc:creator>
    </item>
    <item>
      <title>Convex Markov Games: A New Frontier for Multi-Agent Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2410.16600</link>
      <description>arXiv:2410.16600v3 Announce Type: replace 
Abstract: Behavioral diversity, expert imitation, fairness, safety goals and others give rise to preferences in sequential decision making domains that do not decompose additively across time. We introduce the class of convex Markov games that allow general convex preferences over occupancy measures. Despite infinite time horizon and strictly higher generality than Markov games, pure strategy Nash equilibria exist. Furthermore, equilibria can be approximated empirically by performing gradient descent on an upper bound of exploitability. Our experiments reveal novel solutions to classic repeated normal-form games, find fair solutions in a repeated asymmetric coordination game, and prioritize safe long-term behavior in a robot warehouse environment. In the prisoner's dilemma, our algorithm leverages transient imitation to find a policy profile that deviates from observed human play only slightly, yet achieves higher per-player utility while also being three orders of magnitude less exploitable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16600v3</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ian Gemp, Andreas Haupt, Luke Marris, Siqi Liu, Georgios Piliouras</dc:creator>
    </item>
    <item>
      <title>Regret Minimization and Convergence to Equilibria in General-sum Markov Games</title>
      <link>https://arxiv.org/abs/2207.14211</link>
      <description>arXiv:2207.14211v3 Announce Type: replace-cross 
Abstract: An abundance of recent impossibility results establish that regret minimization in Markov games with adversarial opponents is both statistically and computationally intractable. Nevertheless, none of these results preclude the possibility of regret minimization under the assumption that all parties adopt the same learning procedure. In this work, we present the first (to our knowledge) algorithm for learning in general-sum Markov games that provides sublinear regret guarantees when executed by all agents. The bounds we obtain are for swap regret, and thus, along the way, imply convergence to a correlated equilibrium. Our algorithm is decentralized, computationally efficient, and does not require any communication between agents. Our key observation is that online learning via policy optimization in Markov games essentially reduces to a form of weighted regret minimization, with unknown weights determined by the path length of the agents' policy sequence. Consequently, controlling the path length leads to weighted regret objectives for which sufficiently adaptive algorithms provide sublinear regret guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.14211v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>stat.ML</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liad Erez, Tal Lancewicki, Uri Sherman, Tomer Koren, Yishay Mansour</dc:creator>
    </item>
    <item>
      <title>Roadmap on Incentive Compatibility for AI Alignment and Governance in Sociotechnical Systems</title>
      <link>https://arxiv.org/abs/2402.12907</link>
      <description>arXiv:2402.12907v3 Announce Type: replace-cross 
Abstract: The burgeoning integration of artificial intelligence (AI) into human society brings forth significant implications for societal governance and safety. While considerable strides have been made in addressing AI alignment challenges, existing methodologies primarily focus on technical facets, often neglecting the intricate sociotechnical nature of AI systems, which can lead to a misalignment between the development and deployment contexts. To this end, we posit a new problem worth exploring: Incentive Compatibility Sociotechnical Alignment Problem (ICSAP). We hope this can call for more researchers to explore how to leverage the principles of Incentive Compatibility (IC) from game theory to bridge the gap between technical and societal components to maintain AI consensus with human societies in different contexts. We further discuss three classical game problems for achieving IC: mechanism design, contract theory, and Bayesian persuasion, in addressing the perspectives, potentials, and challenges of solving ICSAP, and provide preliminary implementation conceptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.12907v3</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.GT</category>
      <category>cs.HC</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhaowei Zhang, Fengshuo Bai, Mingzhi Wang, Haoyang Ye, Chengdong Ma, Yaodong Yang</dc:creator>
    </item>
    <item>
      <title>Metritocracy: Representative Metrics for Lite Benchmarks</title>
      <link>https://arxiv.org/abs/2506.09813</link>
      <description>arXiv:2506.09813v2 Announce Type: replace-cross 
Abstract: A common problem in LLM evaluation is how to choose a subset of metrics from a full suite of possible metrics. Subset selection is usually done for efficiency or interpretability reasons, and the goal is often to select a ``representative'' subset of metrics. However, ``representative'' is rarely clearly defined. In this work, we use ideas from social choice theory to formalize two notions of representation for the selection of a subset of evaluation metrics. We first introduce positional representation, which guarantees every alternative is sufficiently represented at every position cutoff. We then introduce positional proportionality, which guarantees no alternative is proportionally over- or under-represented by more than a small error at any position. We prove upper and lower bounds on the smallest number of metrics needed to guarantee either of these properties in the worst case. We also study a generalized form of each property that allows for additional input on groups of metrics that must be represented. Finally, we tie theory to practice through real-world case studies on both LLM evaluation and hospital quality evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09813v2</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ariel Procaccia, Benjamin Schiffer, Serena Wang, Shirley Zhang</dc:creator>
    </item>
    <item>
      <title>A Benchmark for Generalizing Across Diverse Team Strategies in Competitive Pok\'emon</title>
      <link>https://arxiv.org/abs/2506.10326</link>
      <description>arXiv:2506.10326v2 Announce Type: replace-cross 
Abstract: Developing AI agents that can robustly adapt to dramatically different strategic landscapes without retraining is a central challenge for multi-agent learning. Pok\'emon Video Game Championships (VGC) is a domain with an extraordinarily large space of possible team configurations of approximately $10^{139}$ - far larger than those of Dota or Starcraft. The highly discrete, combinatorial nature of team building in Pok\'emon VGC causes optimal strategies to shift dramatically depending on both the team being piloted and the opponent's team, making generalization uniquely challenging. To advance research on this problem, we introduce VGC-Bench: a benchmark that provides critical infrastructure, standardizes evaluation protocols, and supplies human-play datasets and a range of baselines - from large-language-model agents and behavior cloning to reinforcement learning and empirical game-theoretic methods such as self-play, fictitious play, and double oracle. In the restricted setting where an agent is trained and evaluated on a single-team configuration, our methods are able to win against a professional VGC competitor. We extensively evaluated all baseline methods over progressively larger team sets and find that even the best-performing algorithm in the single-team setting struggles at scaling up as team size grows. Thus, policy generalization across diverse team strategies remains an open challenge for the community. Our code is open sourced at https://github.com/cameronangliss/VGC-Bench.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10326v2</guid>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cameron Angliss, Jiaxun Cui, Jiaheng Hu, Arrasy Rahman, Peter Stone</dc:creator>
    </item>
  </channel>
</rss>
