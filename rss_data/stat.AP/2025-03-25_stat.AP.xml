<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 26 Mar 2025 02:18:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Investigation of FWER and Power for Methodological Changes Introduced in the BDOTS R Package</title>
      <link>https://arxiv.org/abs/2503.17495</link>
      <description>arXiv:2503.17495v1 Announce Type: new 
Abstract: In 2025, we identified a methodological issue in the bootstrapped differences of times series (BDOTS) first introduced in 2017 resulting in a significant inflation of the family-wise error rate. The goal of the present manuscript is threefold: to identify the problem in the original methodology, to present two alternative solutions, and to compare estimates of the FWER and power of each of the considered methods across a variety of experimental conditions. We find conclusive evidence that the original BDOTS method does inflate the FWER, while each of the proposed alternatives maintain a FWER much closer to the nominal rate. Additionally, we demonstrate the relationship between power and effect size for each of the proposed methods. In total, the results presented justify the methodological changes presented in the new iteration of the bdots package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17495v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Collin Nolte</dc:creator>
    </item>
    <item>
      <title>Collective Wisdom: Policy Averaging with an Application to the Newsvendor Problem</title>
      <link>https://arxiv.org/abs/2503.17638</link>
      <description>arXiv:2503.17638v1 Announce Type: new 
Abstract: We propose a Policy Averaging Approach (PAA) that synthesizes the strengths of existing approaches to create more reliable, flexible and justifiable policies for stochastic optimization problems. An important component of the PAA is risk diversification to reduce the randomness of policies. A second component emulates model averaging from statistics. A third component involves using cross-validation to diversify and optimize weights among candidate policies. We demonstrate the use of the PAA for the newsvendor problem. For that problem, model-based approaches typically use specific and potentially unreliable assumptions of either independently and identically distributed (i.i.d.) demand or feature-dependent demand with covariates or autoregressive functions. Data-driven approaches, including sample averaging and the use of functions of covariates to set order quantities, typically suffer from overfitting and provide limited insights to justify recommended policies. By integrating concepts from statistics and finance, the PAA avoids these problems. We show using theoretical analysis, a simulation study, and an empirical study, that the PAA outperforms all those earlier approaches. The demonstrated benefits of the PAA include reduced expected cost, more stable performance, and improved insights to justify recommendations. Extensions to consider tail risk and the use of stratified sampling are discussed. Beyond the newsvendor problem, the PAA is applicable to a wide variety of decision-making problems under uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17638v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiangyu Cui, Nicholas G. Hall, Yun Shi, Tianyuan Su</dc:creator>
    </item>
    <item>
      <title>Estimating the Complier Average Causal Effect in Randomised Controlled Trials with Non-Compliance: A Comparative Simulation Study of the Instrumental Variables and Per-Protocol Analyses</title>
      <link>https://arxiv.org/abs/2503.17692</link>
      <description>arXiv:2503.17692v1 Announce Type: new 
Abstract: Objective: Randomised controlled trials (RCTs) are widely considered as gold standard for assessing the effectiveness of new health interventions. When treatment non-compliance is present in RCTs, the treatment effect in the subgroup of participants who complied with their original treatment allocation, the Complier Average Causal Effect (CACE), is a more representative measure of treatment efficacy than the average treatment effect. Through simulation we aim to compare the two most common methods employed in practice to estimate CACE. Methods: We considered the Per-Protocol and Instrumental Variables (IV) analyses. Based on a real study, we simulated hypothetical trials by varying factors related to non-compliance and compared the two methods by the bias of the estimate, mean squared error and $95\%$ coverage of the true value. Results: For binary compliance, the IV estimator was always unbiased for CACE, while the Per-Protocol estimator was unbiased for random non-compliance or when participants with good or bad conditions always received the treatment. For partial compliance, the IV estimator was less biased when participants with better conditions always received the treatment and those with worse conditions always received the control or vice versa, while the Per-Protocol estimator was less biased when participants with good or bad conditions never received the treatment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17692v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Theodosios Papazoglou, Ed Waddingham, Alastair Young</dc:creator>
    </item>
    <item>
      <title>Probabilistic Assessment of West Nile Virus Spillover Risk Using a Compartmental Mechanistic Model</title>
      <link>https://arxiv.org/abs/2503.18433</link>
      <description>arXiv:2503.18433v1 Announce Type: new 
Abstract: This paper presents a novel probabilistic approach for assessing the risk of West Nile Disease (WND) spillover to the human population. The assessment has been conducted under two different scenarios: (1) assessment of the onset of spillover, and (2) assessment of the severity of the epidemic after the onset of the disease. A compartmental model of differential equations is developed to describe the disease transmission mechanism, and a probability density function for pathogen spillover to humans is derived based on the model for the assessment of the risk of the spillover onset and the severity of the epidemic. The prediction strategy involves making a long-term forecast and then updating it with a short-term (lead time of two weeks or daily). The methodology is demonstrated using detailed outbreak data from high-case counties in California, including Orange County, Los Angeles County, and Kern County. The predicted results are compared with actual infection dates reported by the California Department of Public Health for 2022-2024 to assess prediction accuracy. The performance accuracy is evaluated using a logarithmic scoring system and compared with one of the most renowned predictive models to assess its effectiveness. In all prediction scenarios, the model demonstrated strong performance. Lastly, the method is applied to explore the impact of global warming on spillover risk, revealing an increasing trend in the number of high-risk days and a shift toward a greater proportion of these days over time for the onset of the disease.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18433v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saman Hosseini, Lee W. Cohnstaedt, Matin Marjani, Caterina Scoglio</dc:creator>
    </item>
    <item>
      <title>Stochastic modeling of particle structures in spray fluidized bed agglomeration using methods from machine learning</title>
      <link>https://arxiv.org/abs/2503.18882</link>
      <description>arXiv:2503.18882v1 Announce Type: new 
Abstract: Agglomeration is an industrially relevant process for the production of bulk materials in which the product properties depend on the morphology of the agglomerates, e.g., on the distribution of size and shape descriptors. Thus, accurate characterization and control of agglomerate morphologies is essential to ensure high and consistent product quality. This paper presents a pipeline for image-based inline agglomerate characterization and prediction of their time-dependent multivariate morphology distributions within a spray fluidized bed process with transparent glass beads as primary particles. The framework classifies observed objects in image data into three distinct morphological classes--primary particles, chain-like agglomerates and raspberry-like agglomerates--using various size and shape descriptors. To this end, a fast and robust random forest classifier is trained. Additionally, the fraction of primary particles belonging to each of these classes, either as individual primary particles or as part of a larger structure in the form of chain-like or raspberry-like agglomerates, is described using parametric regression functions. Finally, the temporal evolution of bivariate size and shape descriptor distributions of these classes is modeled using low-parametric regression functions and Archimedean copulas. This approach improves the understanding of agglomerate formation and allows the prediction of process kinetics, facilitating precise control over class fractions and morphology distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18882v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lukas Fuchs, Sabrina Weber, Jialin Men, Niklas Eiermann, Orkun Furat, Andreas B\"uck, Volker Schmidt</dc:creator>
    </item>
    <item>
      <title>Causal Links Between Anthropogenic Emissions and Air Pollution Dynamics in Delhi</title>
      <link>https://arxiv.org/abs/2503.18912</link>
      <description>arXiv:2503.18912v1 Announce Type: new 
Abstract: Air pollution poses significant health and environmental challenges, particularly in rapidly urbanizing regions. Delhi-National Capital Region experiences air pollution episodes due to complex interactions between anthropogenic emissions and meteorological conditions. Understanding the causal drivers of key pollutants such as $PM_{2.5}$ and ground $O_3$ is crucial for developing effective mitigation strategies. This study investigates the causal links of anthropogenic emissions on $PM_{2.5}$ and $O_3$ concentrations using predictive modeling and causal inference techniques. Integrating high-resolution air quality data from Jan 2018 to Aug 2023 across 32 monitoring stations, we develop predictive regression models that incorporate meteorological variables (temperature and relative humidity), pollutant concentrations ($NO_2, SO_2, CO$), and seasonal harmonic components to capture both diurnal and annual cycles. Here, we show that reductions in anthropogenic emissions lead to significant decreases in $PM_{2.5}$ levels, whereas their effect on $O_3$ remains marginal and statistically insignificant. To address spatial heterogeneity, we employ Gaussian Process modeling. Further, we use Granger causality analysis and counterfactual simulation to establish direct causal links. Validation using real-world data from the COVID-19 lockdown confirms that reduced emissions led to a substantial drop in $PM_{2.5}$ but only a slight, insignificant change in $O_3$. The findings highlight the necessity of targeted emission reduction policies while emphasizing the need for integrated strategies addressing both particulate and ozone pollution. These insights are crucial for policymakers designing air pollution interventions in other megacities, and offer a scalable methodology for tackling complex urban air pollution through data-driven decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18912v1</guid>
      <category>stat.AP</category>
      <category>physics.ao-ph</category>
      <category>physics.soc-ph</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sourish Das, Sudeep Shukla, Alka Yadav, Anirban Chakraborti</dc:creator>
    </item>
    <item>
      <title>A Joint Model of Longitudinal CVD Risk Factors, Medication Use, and Time-to-Terminal Events</title>
      <link>https://arxiv.org/abs/2503.17576</link>
      <description>arXiv:2503.17576v1 Announce Type: cross 
Abstract: We introduce a novel Bayesian approach for jointly modeling longitudinal cardiovascular disease (CVD) risk factor trajectories, medication use, and time-to-events. Our methodology incorporates longitudinal risk factor trajectories into the time-to-event model, considers the temporal aspect of medication use, incorporates uncertainty due to missing medication status and medication switching, and analyzes the impact of medications on CVD events. Our research aims to provide a comprehensive understanding of the effect of CVD progression and medication use on time to death, enhancing predictive accuracy and informing personalized intervention strategies. Using data from a cardiovascular cohort study, we demonstrate the model's ability to capture detailed temporal dynamics and enhance predictive accuracy for CVD events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17576v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zeynab Aghabazaz, Michael J Daniels, Donald M Lloyd-Jones, Juned Siddique</dc:creator>
    </item>
    <item>
      <title>Combining longitudinal cohort studies to examine cardiovascular risk factor trajectories across the adult lifespan</title>
      <link>https://arxiv.org/abs/2503.17606</link>
      <description>arXiv:2503.17606v1 Announce Type: cross 
Abstract: We introduce a statistical framework for combining data from multiple large longitudinal cardiovascular cohorts to enable the study of long-term cardiovascular health starting in early adulthood. Using data from seven cohorts belonging to the Lifetime Risk Pooling Project (LRPP), we present a Bayesian hierarchical multivariate approach that jointly models multiple longitudinal risk factors over time and across cohorts. Because few cohorts in our project cover the entire adult lifespan, our strategy uses information from all risk factors to increase precision for each risk factor trajectory and borrows information across cohorts to fill in unobserved risk factors. We develop novel diagnostic testing and model validation methods to ensure that our model robustly captures and maintains critical relationships over time and across risk factors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17606v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zeynab Aghabazaz, Michael J Daniels, Hongyan Ning, Juned Siddique</dc:creator>
    </item>
    <item>
      <title>Bayesian Optimization for CVaR-based portfolio optimization</title>
      <link>https://arxiv.org/abs/2503.17737</link>
      <description>arXiv:2503.17737v1 Announce Type: cross 
Abstract: Optimal portfolio allocation is often formulated as a constrained risk problem, where one aims to minimize a risk measure subject to some performance constraints. This paper presents new Bayesian Optimization algorithms for such constrained minimization problems, seeking to minimize the conditional value-at-risk (a computationally intensive risk measure) under a minimum expected return constraint. The proposed algorithms utilize a new acquisition function, which drives sampling towards the optimal region. Additionally, a new two-stage procedure is developed, which significantly reduces the number of evaluations of the expensive-to-evaluate objective function. The proposed algorithm's competitive performance is demonstrated through practical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17737v1</guid>
      <category>q-fin.PM</category>
      <category>math.OC</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robert Millar, Jinglai Li</dc:creator>
    </item>
    <item>
      <title>Efficient Inference in First Passage Time Models</title>
      <link>https://arxiv.org/abs/2503.18381</link>
      <description>arXiv:2503.18381v1 Announce Type: cross 
Abstract: First passage time models describe the time it takes for a random process to exit a region of interest and are widely used across various scientific fields. Fast and accurate numerical methods for computing the likelihood function in these models are essential for efficient statistical inference. Specifically, in mathematical psychology, generalized drift diffusion models (GDDMs) are an important class of first passage time models that describe the latent psychological processes underlying simple decision-making scenarios. GDDMs model the joint distribution over choices and response times as the first hitting time of a one-dimensional stochastic differential equation (SDE) to possibly time-varying upper and lower boundaries. They are widely applied to extract parameters associated with distinct cognitive and neural mechanisms. However, current likelihood computation methods struggle with common scenarios where drift rates covary dynamically with exogenous covariates in each trial, such as in the attentional drift diffusion model (aDDM). In this work, we propose a fast and flexible algorithm for computing the likelihood function of GDDMs based on a large class of SDEs satisfying the Cherkasov condition. Our method divides each trial into discrete stages, employs fast analytical results to compute stage-wise densities, and integrates these to compute the overall trial-wise likelihood. Numerical examples demonstrate that our method not only yields accurate likelihood evaluations for efficient statistical inference, but also significantly outperforms existing approaches in terms of speed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18381v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sicheng Liu, Alexander Fengler, Michael J. Frank, Matthew T. Harrison</dc:creator>
    </item>
    <item>
      <title>A primer on inference and prediction with epidemic renewal models and sequential Monte Carlo</title>
      <link>https://arxiv.org/abs/2503.18875</link>
      <description>arXiv:2503.18875v1 Announce Type: cross 
Abstract: Renewal models are widely used in statistical epidemiology as semi-mechanistic models of disease transmission. While primarily used for estimating the instantaneous reproduction number, they can also be used for generating projections, estimating elimination probabilities, modelling the effect of interventions, and more. We demonstrate how simple sequential Monte Carlo methods (also known as particle filters) can be used to perform inference on these models. Our goal is to acquaint a reader who has a working knowledge of statistical inference with these methods and models and to provide a practical guide to their implementation. We focus on these methods' flexibility and their ability to handle multiple statistical and other biases simultaneously. We leverage this flexibility to unify existing methods for estimating the instantaneous reproduction number and generating projections. A companion website "SMC and epidemic renewal models" provides additional worked examples, self-contained code to reproduce the examples presented here, and additional materials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18875v1</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Nicholas Steyn, Kris V. Parag, Robin N. Thompson, Christl A. Donnelly</dc:creator>
    </item>
    <item>
      <title>Calibration Bands for Mean Estimates within the Exponential Dispersion Family</title>
      <link>https://arxiv.org/abs/2503.18896</link>
      <description>arXiv:2503.18896v1 Announce Type: cross 
Abstract: A statistical model is said to be calibrated if the resulting mean estimates perfectly match the true means of the underlying responses. Aiming for calibration is often not achievable in practice as one has to deal with finite samples of noisy observations. A weaker notion of calibration is auto-calibration. An auto-calibrated model satisfies that the expected value of the responses being given the same mean estimate matches this estimate. Testing for auto-calibration has only been considered recently in the literature and we propose a new approach based on calibration bands. Calibration bands denote a set of lower and upper bounds such that the probability that the true means lie simultaneously inside those bounds exceeds some given confidence level. Such bands were constructed by Yang-Barber (2019) for sub-Gaussian distributions. Dimitriadis et al. (2023) then introduced narrower bands for the Bernoulli distribution and we use the same idea in order to extend the construction to the entire exponential dispersion family that contains for example the binomial, Poisson, negative binomial, gamma and normal distributions. Moreover, we show that the obtained calibration bands allow us to construct various tests for calibration and auto-calibration, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18896v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>{\L}ukasz Delong, Selim Gatti, Mario V. W\"uthrich</dc:creator>
    </item>
    <item>
      <title>Comparing Clustering Approaches for Smart Meter Time Series: Investigating the Influence of Dataset Properties on Performance</title>
      <link>https://arxiv.org/abs/2412.02026</link>
      <description>arXiv:2412.02026v3 Announce Type: replace 
Abstract: The widespread adoption of smart meters for monitoring energy consumption has generated vast quantities of high-resolution time series data which remains underutilised. While clustering has emerged as a fundamental tool for mining smart meter time series (SMTS) data, selecting appropriate clustering methods remains challenging despite numerous comparative studies. These studies often rely on problematic methodologies and consider a limited scope of methods, frequently overlooking compelling methods from the broader time series clustering literature. Consequently, they struggle to provide dependable guidance for practitioners designing their own clustering approaches.
  This paper presents a comprehensive comparative framework for SMTS clustering methods using expert-informed synthetic datasets that emphasise peak consumption behaviours as fundamental cluster concepts. Using a phased methodology, we first evaluated 31 distance measures and 8 representation methods using leave-one-out classification, then examined the better-suited methods in combination with 11 clustering algorithms. We further assessed the robustness of these combinations to systematic changes in key dataset properties that affect clustering performance on real-world datasets, including cluster balance, noise, and the presence of outliers.
  Our results revealed that methods accommodating local temporal shifts while maintaining amplitude sensitivity, particularly Dynamic Time Warping and $k$-sliding distance, consistently outperformed traditional approaches. Among other key findings, we identified that when combined with $k$-medoids or hierarchical clustering using Ward's linkage, these methods exhibited consistent robustness across varying dataset characteristics without...</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02026v3</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luke W. Yerbury, Ricardo J. G. B. Campello, G. C. Livingston Jr, Mark Goldsworthy, Lachlan O'Neil</dc:creator>
    </item>
    <item>
      <title>Validation of satellite and reanalysis rainfall products against rain gauge observations in Ghana and Zambia</title>
      <link>https://arxiv.org/abs/2501.14829</link>
      <description>arXiv:2501.14829v3 Announce Type: replace 
Abstract: Accurate rainfall data are crucial for effective climate services, especially in Sub-Saharan Africa, where agriculture depends heavily on rain-fed systems. The sparse distribution of rain-gauge networks necessitates reliance on satellite and reanalysis rainfall products (REs). This study evaluated eight REs -- CHIRPS, TAMSAT, CHIRP, ENACTS, ERA5, AgERA5, PERSIANN-CDR, and PERSIANN-CCS-CDR -- in Zambia and Ghana using a point-to-pixel validation approach. The analysis covered spatial consistency, annual rainfall summaries, seasonal patterns, and rainfall intensity detection across 38 ground stations. Results showed no single product performed optimally across all contexts, highlighting the need for application-specific recommendations. All products exhibited a high probability of detection (POD) for dry days in Zambia and northern Ghana (70% &lt; POD &lt; 100%, and 60% &lt; POD &lt; 85%, respectively), suggesting their utility for drought-related studies. However, all products showed limited skill in detecting heavy and violent rains (POD close to 0%), making them unsuitable for analyzing such events (e.g., floods) in their current form. Products integrated with station data (ENACTS, CHIRPS, and TAMSAT) outperformed others in many contexts, emphasizing the importance of local observation calibration. Bias correction is strongly recommended due to varying bias levels across rainfall summaries. A critical area for improvement is the detection of heavy and violent rains, with which REs currently struggle. Future research should focus on this aspect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14829v3</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John Bagiliko, David Stern, Denis Ndanguza, Francis Feehi Torgbor</dc:creator>
    </item>
    <item>
      <title>Markov Renewal Proportional Hazards is All You Need</title>
      <link>https://arxiv.org/abs/2502.03479</link>
      <description>arXiv:2502.03479v5 Announce Type: replace 
Abstract: Transition probability estimation plays a critical role in multi-state modeling, especially in clinical research. This paper investigates the application of semi-Markov and Markov renewal frameworks to the EBMT dataset, focusing on six clinical states encountered during hematopoietic stem cell transplantation. By comparing Aalen-Johansen (AJ) and Dabrowska-Sun-Horowitz (DSH) estimators, we demonstrate that semi-Markov models, which incorporate sojourn times, provide a more nuanced and temporally sensitive depiction of patient trajectories compared to memoryless Markov models. The DSH estimator consistently yields smoother probability curves, particularly for transitions involving prolonged states. We use empirical process theory and Burkholder-Davis-Gundy inequality to show weak convergence of the estimator. Future work includes extending the framework to accommodate advanced covariate structures and non-Markovian dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03479v5</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eliuvish Cuicizion</dc:creator>
    </item>
    <item>
      <title>Community Detection Analysis of Spatial Transcriptomics Data</title>
      <link>https://arxiv.org/abs/2503.12351</link>
      <description>arXiv:2503.12351v2 Announce Type: replace 
Abstract: The spatial transcriptomics (ST) data produced by recent biotechnologies, such as CosMx and Xenium, contain huge amount of information about cancer tissue samples, which has great potential for cancer research via detection of community: a collection of cells with distinct cell-type composition and similar neighboring patterns. But existing clustering methods do not work well for community detection of CosMx ST data, and the commonly used kNN compositional data method shows lack of informative neighboring cell patterns for huge CosMx data. In this article, we propose a novel and more informative disk compositional data (DCD) method, which identifies neighboring patterns of each cell via taking into account of ST data features from recent new technologies. After initial processing ST data into DCD matrix, a new innovative and interpretable DCD-TMHC community detection method is proposed here. Extensive simulation studies and CosMx breast cancer data analysis clearly show that our proposed DCD-TMHC method is superior to other methods. Based on the communities detected by DCD-TMHC method for CosMx breast cancer data, the logistic regression analysis results demonstrate that DCD-TMHC method is clearly interpretable and superior, especially in terms of assessment for different stages of cancer. These suggest that our proposed novel, innovative, informative and interpretable DCD-TMHC method here will be helpful and have impact to future cancer research based on ST data, which can improve cancer diagnosis and monitor cancer treatment progress.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12351v2</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charles Zhao, Susana Garcia-Recio, Brooke M. Felsheim</dc:creator>
    </item>
    <item>
      <title>A continuous multiple hypothesis testing framework for optimal exoplanet detection</title>
      <link>https://arxiv.org/abs/2203.04957</link>
      <description>arXiv:2203.04957v4 Announce Type: replace-cross 
Abstract: When searching for exoplanets, one wants to count how many planets orbit a given star, and to determine what their characteristics are. If the estimated planet characteristics are too far from those of a planet truly present, this should be considered as a false detection. This setting is a particular instance of a general one: aiming to retrieve parametric components in a dataset corrupted by nuisance signals, with a certain accuracy on their parameters. We exhibit a detection criterion minimizing false and missed detections, either as a function of their relative cost or when the expected number of false detections is bounded. If the components can be separated in a technical sense discussed in detail, the optimal detection criterion is a posterior probability obtained as a by-product of Bayesian evidence calculations. Optimality is guaranteed within a model, and we introduce model criticism methods to ensure that the criterion is robust to model errors. We show on two simulations emulating exoplanet searches that the optimal criterion can significantly outperform other criteria. Finally, we show that our framework offers solutions for the identification of components of mixture models and Bayesian false discovery rate control when hypotheses are not discrete.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.04957v4</guid>
      <category>astro-ph.IM</category>
      <category>astro-ph.EP</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1214/23-AOAS1810</arxiv:DOI>
      <arxiv:journal_reference>Ann. Appl. Stat. 18(1): 749-769 (March 2024)</arxiv:journal_reference>
      <dc:creator>Nathan C. Hara, Thibault de Poyferr\'e, Jean-Baptiste Delisle, Marc Hoffmann</dc:creator>
    </item>
    <item>
      <title>High-energy Neutrino Source Cross-correlations with Nearest-neighbor Distributions</title>
      <link>https://arxiv.org/abs/2406.00796</link>
      <description>arXiv:2406.00796v2 Announce Type: replace-cross 
Abstract: The astrophysical origins of the majority of the IceCube neutrinos remain unknown. Effectively characterizing the spatial distribution of the neutrino samples and associating the events with astrophysical source catalogs can be challenging given the large atmospheric neutrino background and underlying non-Gaussian spatial features in the neutrino and source samples. In this paper, we investigate a framework for identifying and statistically evaluating the cross-correlations between IceCube data and an astrophysical source catalog based on the $k$-nearest-neighbor cumulative distribution functions ($k$NN-CDFs). We propose a maximum likelihood estimation procedure for inferring the true proportions of astrophysical neutrinos in the point-source data. We conduct a statistical power analysis of an associated likelihood ratio test with estimations of its sensitivity and discovery potential with synthetic neutrino data samples and a WISE-2MASS galaxy sample. We apply the method to IceCube's public ten-year point-source data and find no statistically significant evidence for spatial cross-correlations with the selected galaxy sample. We discuss possible extensions to the current method and explore the method's potential to identify the cross-correlation signals in data sets with different sample sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00796v2</guid>
      <category>astro-ph.HE</category>
      <category>astro-ph.CO</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3847/1538-4357/ad924c</arxiv:DOI>
      <arxiv:journal_reference>The Astrophysical Journal, Volume 979:194, Number 2, 2025</arxiv:journal_reference>
      <dc:creator>Zhuoyang Zhou, Jessi Cisewski-Kehe, Ke Fang, Arka Banerjee</dc:creator>
    </item>
    <item>
      <title>Leveraging statistical models to improve pre-season forecasting and in-season management of a recreational fishery</title>
      <link>https://arxiv.org/abs/2503.17293</link>
      <description>arXiv:2503.17293v2 Announce Type: replace-cross 
Abstract: Effective management of recreational fisheries requires accurate forecasting of future harvests and real-time monitoring of ongoing harvests. Traditional methods that rely on historical catch data to predict short-term harvests can be unreliable, particularly if changes in management regulations alter angler behavior. In contrast, statistical modeling approaches can provide faster, more flexible, and potentially more accurate predictions, enhancing management outcomes. In this study, we developed and tested models to improve predictions of Gulf of Mexico gag harvests for both pre-season planning and in-season monitoring. Our best-fitting model outperformed traditional methods (i.e., estimates derived from historical average harvest) for both cumulative pre-season projections and in-season monitoring. Notably, our modeling framework appeared to be more accurate in more recent, shorter seasons due to its ability to account for effort compression. A key advantage of our framework is its ability to explicitly quantify the probability of exceeding harvest quotas for any given season duration. This feature enables managers to evaluate trade-offs between season duration and conservation goals. This is especially critical for vulnerable, highly targeted stocks. Our findings also underscore the value of statistical models to complement and advance traditional fisheries management approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17293v2</guid>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. Challen Hyman, Chloe Ramsay, Tiffanie A. Cross, Beverly Sauls, Thomas K. Frazer</dc:creator>
    </item>
  </channel>
</rss>
