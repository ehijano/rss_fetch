<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 01 Sep 2025 04:01:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Detection of collective and point anomalies at the presence of trend and seasonality</title>
      <link>https://arxiv.org/abs/2508.21128</link>
      <description>arXiv:2508.21128v1 Announce Type: new 
Abstract: Detecting anomalies in time series data is a challenging task with broad relevance in many applications. Existing methods work effectively only under idealized conditions, typically focusing on point anomalies or assuming a constant baseline. Our approach overcomes these limitations by detecting both collective and point anomalies, while allowing for polynomial trends and seasonal patterns. We establish statistical theory demonstrating that our method accurately decomposes the time series into anomaly, trend, seasonality, and a remainder component. We further show that it estimates the number of anomalies consistently and their locations with minimal error. Simulation studies confirm its strong detection performance with finite samples, and an application to energy price data illustrates its practical utility. An R package is available on request.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.21128v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiyin Zhang, Florian Pein, Idris Eckley</dc:creator>
    </item>
    <item>
      <title>Bi-SCORE for Weighted Bipartite Networks with Application in Knowledge Source Discovery</title>
      <link>https://arxiv.org/abs/2508.21467</link>
      <description>arXiv:2508.21467v1 Announce Type: new 
Abstract: Community detection in citation networks offers a powerful approach to understanding knowledge flow and identifying core research areas within academic disciplines. This study focuses on knowledge source discovery in statistics by analyzing a weighted bipartite journal citation network constructed from 16,119 articles published in eight core journals from 2001 to 2023. To capture the inherent asymmetry of citation behavior, we explicitly preserve the bipartite structure of the network, distinguishing between citing and cited journals. For this task, we propose Bi-SCORE (Bipartite Spectral Clustering on Ratios-of-Eigenvectors), a computationally efficient and initialization-free spectral method designed for community detection in weighted bipartite networks with degree heterogeneity. We establish rigorous theoretical guarantees for the performance of Bi-SCORE under the weighted bipartite degree-corrected stochastic block model. Furthermore, simulation studies demonstrate its robustness across varying levels of sparsity and degree heterogeneity, where it outperforms existing methods. When applied to the real-world citation network, Bi-SCORE uncovers a six-community structure corresponding to key research areas in statistics, including applied statistics, methodology, theory, computation, and econometrics. These findings provide valuable insights into the intricate citation patterns and knowledge flow among statistical journals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.21467v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zicheng Xie, Rui Pan, Yan Zhang</dc:creator>
    </item>
    <item>
      <title>Triply Robust Panel Estimators</title>
      <link>https://arxiv.org/abs/2508.21536</link>
      <description>arXiv:2508.21536v1 Announce Type: new 
Abstract: This paper studies estimation of causal effects in a panel data setting. We introduce a new estimator, the Triply RObust Panel (TROP) estimator, that combines $(i)$ a flexible model for the potential outcomes based on a low-rank factor structure on top of a two-way-fixed effect specification, with $(ii)$ unit weights intended to upweight units similar to the treated units and $(iii)$ time weights intended to upweight time periods close to the treated time periods. We study the performance of the estimator in a set of simulations designed to closely match several commonly studied real data sets. We find that there is substantial variation in the performance of the estimators across the settings considered. The proposed estimator outperforms two-way-fixed-effect/difference-in-differences, synthetic control, matrix completion and synthetic-difference-in-differences estimators. We investigate what features of the data generating process lead to this performance, and assess the relative importance of the three components of the proposed estimator. We have two recommendations. Our preferred strategy is that researchers use simulations closely matched to the data they are interested in, along the lines discussed in this paper, to investigate which estimators work well in their particular setting. A simpler approach is to use more robust estimators such as synthetic difference-in-differences or the new triply robust panel estimator which we find to substantially outperform two-way fixed effect estimators in many empirically relevant settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.21536v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Susan Athey, Guido Imbens, Zhaonan Qu, Davide Viviano</dc:creator>
    </item>
    <item>
      <title>NExON-Bayes: A Bayesian approach to network estimation informed by ordinal covariates</title>
      <link>https://arxiv.org/abs/2508.21649</link>
      <description>arXiv:2508.21649v1 Announce Type: new 
Abstract: In heterogeneous disease settings, accounting for intrinsic sample variability is crucial for obtaining reliable and interpretable omic network estimates. However, most graphical model analyses of biomedical data assume homogeneous conditional dependence structures, potentially leading to misleading conclusions. To address this, we propose a joint Gaussian graphical model that leverages sample-level ordinal covariates (e.g., disease stage) to account for heterogeneity and improve the estimation of partial correlation structures. Our modelling framework, called NExON-Bayes, extends the graphical spike-and-slab framework to account for ordinal covariates, jointly estimating their relevance to the graph structure and leveraging them to improve the accuracy of network estimation. To scale to high-dimensional omic settings, we develop an efficient variational inference algorithm tailored to our model. Through simulations, we demonstrate that our method outperforms the vanilla graphical spike-and-slab (with no covariate information), as well as other state-of-the-art network approaches which exploit covariate information. Applying our method to reverse phase protein array data from patients diagnosed with stage I, II or III breast carcinoma, we estimate the behaviour of proteomic networks as breast carcinoma progresses. Our model provides insights not only through inspection of the estimated proteomic networks, but also of the estimated ordinal covariate dependencies of key groups of proteins within those networks, offering a comprehensive understanding of how biological pathways shift across disease stages.
  Availability and Implementation: A user-friendly R package for NExON-Bayes with tutorials is available on Github at github.com/jf687/NExON.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.21649v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joseph Feest, H\'el\`ene Ruffieux, Camilla Lingj{\ae}rde, Xiaoyue Xi</dc:creator>
    </item>
    <item>
      <title>Considerations for Estimating Causal Effects of Informatively Timed Treatments</title>
      <link>https://arxiv.org/abs/2508.21804</link>
      <description>arXiv:2508.21804v1 Announce Type: new 
Abstract: Epidemiological studies are often concerned with estimating causal effects of a sequence of treatment decisions on survival outcomes. In many settings, treatment decisions do not occur at fixed, pre-specified followup times. Rather, timing varies across subjects in ways that may be informative of subsequent treatment decisions and potential outcomes. Awareness of the issue and its potential solutions is lacking in the literature, which motivate this work. Here, we formalize the issue of informative timing, problems associated with ignoring it, and show how g-methods can be used to analyze sequential treatments that are informatively timed. As we describe, in such settings, the waiting times between successive treatment decisions may be properly viewed as a time-varying confounders. Using synthetic examples, we illustrate how g-methods that do not adjust for these waiting times may be biased and how adjustment can be done in scenarios where patients may die or be censored in between treatments. We draw connections between adjustment and identification with discrete-time versus continuous-time models. Finally, we provide implementation guidance and examples using publicly available software. Our concluding message is that 1) considering timing is important for valid inference and 2) correcting for informative timing can be done with g-methods that adjust for waiting times between treatments as time-varying confounders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.21804v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arman Oganisian</dc:creator>
    </item>
    <item>
      <title>Practically significant change points in high dimension -- measuring signal strength pro active component</title>
      <link>https://arxiv.org/abs/2508.21520</link>
      <description>arXiv:2508.21520v1 Announce Type: cross 
Abstract: We consider the change point testing problem for high-dimensional time series. Unlike conventional approaches, where one tests whether the difference $\delta$ of the mean vectors before and after the change point is equal to zero, we argue that the consideration of the null hypothesis $H_0:\|\delta\|\le\Delta$, for some norm $\|\cdot\|$ and a threshold $\Delta&gt;0$, is better suited. By the formulation of the null hypothesis as a composite hypothesis, the change point testing problem becomes significantly more challenging. We develop pivotal inference for testing hypotheses of this type in the setting of high-dimensional time series, first, measuring deviations from the null vector by the $\ell_2$-norm $\|\cdot\|_2$ normalized by the dimension. Second, by measuring deviations using a sparsity adjusted $\ell_2$-"norm" $\|\cdot \|_2/\sqrt{\|\cdot\|_0} $, where $\|\cdot\|_0$ denotes the $\ell_0$-"norm," we propose a pivotal test procedure which intrinsically adapts to sparse alternatives in a data-driven way by pivotally estimating the set of nonzero entries of the vector $\delta$. To establish the statistical validity of our approach, we derive tail bounds of certain classes of distributions that frequently appear as limiting distributions of self-normalized statistics. As a theoretical foundation for all results, we develop a general weak invariance principle for the partial sum process $X_1^\top\xi +\cdots +X_{\lfloor\lambda n\rfloor}^\top\xi$ for a time series $(X_j)_{j\in\mathbb{Z}}$ and a contrast vector $\xi\in\mathbb{R}^p$ under increasing dimension $p$, which is of independent interest. Finally, we investigate the finite sample properties of the tests by means of a simulation study and illustrate its application in a data example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.21520v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pascal Quanz, Holger Dette</dc:creator>
    </item>
    <item>
      <title>Treatment effects at the margin: Everyone is marginal</title>
      <link>https://arxiv.org/abs/2508.21583</link>
      <description>arXiv:2508.21583v1 Announce Type: cross 
Abstract: This paper develops a framework for identifying treatment effects when a policy simultaneously alters both the incentive to participate and the outcome of interest -- such as hiring decisions and wages in response to employment subsidies; or working decisions and wages in response to job trainings. This framework was inspired by my PhD project on a Belgian reform that subsidised first-time hiring, inducing entry by marginal firms yet meanwhile changing the wages they pay. Standard methods addressing selection-into-treatment concepts (like Heckman selection equations and local average treatment effects), or before-after comparisons (including simple DiD or RDD), cannot isolate effects at this shifting margin where treatment defines who is observed. I introduce marginality-weighted estimands that recover causal effects among policy-induced entrants, offering a policy-relevant alternative in settings with endogenous selection. This method can thus be applied widely to understanding the economic impacts of public programmes, especially in fields largely relying on reduced-form causal inference estimation (e.g. labour economics, development economics, health economics).</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.21583v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haotian Deng</dc:creator>
    </item>
    <item>
      <title>Quantum Sequential Universal Hypothesis Testing</title>
      <link>https://arxiv.org/abs/2508.21594</link>
      <description>arXiv:2508.21594v1 Announce Type: cross 
Abstract: Quantum hypothesis testing (QHT) concerns the statistical inference of unknown quantum states. In the general setting of composite hypotheses, the goal of QHT is to determine whether an unknown quantum state belongs to one or another of two classes of states based on the measurement of a number of copies of the state. Prior art on QHT with composite hypotheses focused on a fixed-copy two-step protocol, with state estimation followed by an optimized joint measurement. However, this fixed-copy approach may be inefficient, using the same number of copies irrespective of the inherent difficulty of the testing task. To address these limitations, we introduce the quantum sequential universal test (QSUT), a novel framework for sequential QHT in the general case of composite hypotheses. QSUT builds on universal inference, and it alternates between adaptive local measurements aimed at exploring the hypothesis space and joint measurements optimized for maximal discrimination. QSUT is proven to rigorously control the type I error under minimal assumptions about the hypothesis structure. We present two practical instantiations of QSUT, one based on the Helstrom-Holevo test and one leveraging shallow variational quantum circuits. Empirical results across a range of composite QHT tasks demonstrate that QSUT consistently reduces copy complexity relative to state-of-the-art fixed-copy strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.21594v1</guid>
      <category>quant-ph</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matteo Zecchin, Osvaldo Simeone, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Orientability of Causal Relations in Time Series using Summary Causal Graphs and Faithful Distributions</title>
      <link>https://arxiv.org/abs/2508.21742</link>
      <description>arXiv:2508.21742v1 Announce Type: cross 
Abstract: Understanding causal relations between temporal variables is a central challenge in time series analysis, particularly when the full causal structure is unknown. Even when the full causal structure cannot be fully specified, experts often succeed in providing a high-level abstraction of the causal graph, known as a summary causal graph, which captures the main causal relations between different time series while abstracting away micro-level details. In this work, we present conditions that guarantee the orientability of micro-level edges between temporal variables given the background knowledge encoded in a summary causal graph and assuming having access to a faithful and causally sufficient distribution with respect to the true unknown graph. Our results provide theoretical guarantees for edge orientation at the micro-level, even in the presence of cycles or bidirected edges at the macro-level. These findings offer practical guidance for leveraging SCGs to inform causal discovery in complex temporal systems and highlight the value of incorporating expert knowledge to improve causal inference from observational time series data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.21742v1</guid>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Timoth\'ee Loranchet, Charles K. Assaad</dc:creator>
    </item>
    <item>
      <title>Change Point Detection on A Separable Model for Dynamic Networks</title>
      <link>https://arxiv.org/abs/2303.17642</link>
      <description>arXiv:2303.17642v5 Announce Type: replace 
Abstract: This paper studies the unsupervised change point detection problem in time series of networks using the Separable Temporal Exponential-family Random Graph Model (STERGM). Inherently, dynamic network patterns are complex due to dyadic and temporal dependence, and change points detection can identify the discrepancies in the underlying data generating processes to facilitate downstream analysis. In particular, the STERGM that utilizes network statistics and nodal attributes to represent the structural patterns is a flexible and parsimonious model to fit dynamic networks. We propose a new estimator derived from the Alternating Direction Method of Multipliers (ADMM) procedure and Group Fused Lasso (GFL) regularization to simultaneously detect multiple time points where the parameters of a time-heterogeneous STERGM have shifted. Experiments on both simulated and real data show good performance of the proposed framework, and an R package CPDstergm is developed to implement the method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.17642v5</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yik Lun Kei, Hangjian Li, Yanzhen Chen, Oscar Hernan Madrid Padilla</dc:creator>
    </item>
    <item>
      <title>Robust Universal Inference For Misspecified Models</title>
      <link>https://arxiv.org/abs/2307.04034</link>
      <description>arXiv:2307.04034v4 Announce Type: replace 
Abstract: In statistical inference, it is rarely realistic that the hypothesized statistical model is well-specified, and consequently it is important to understand the effects of misspecification on inferential procedures. When the hypothesized statistical model is misspecified, the natural target of inference is a projection of the data generating distribution onto the model. We present a general method for constructing valid confidence sets for such projections, under weak regularity conditions, despite possible model misspecification. Our method builds upon the universal inference method and is based on inverting a family of split-sample tests of relative fit. We study settings in which our methods yield either exact or approximate, finite-sample valid confidence sets for various projection distributions. We study rates at which the resulting confidence sets shrink around their target of inference and complement these results with a simulation study and a study of causal discovery using a linear causal model with the CausalEffectPairs dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.04034v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Beomjo Park, Sivaraman Balakrishnan, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Multiple testing with anytime-valid Monte Carlo p-values</title>
      <link>https://arxiv.org/abs/2404.15586</link>
      <description>arXiv:2404.15586v4 Announce Type: replace 
Abstract: In contemporary problems involving genetic or neuroimaging data, thousands of hypotheses need to be tested. Due to their high power, and finite sample guarantees on type-I error under weak assumptions, Monte Carlo permutation tests are often considered as gold standard for these settings. However, the enormous computational effort required for (thousands of) permutation tests is a major burden. In this paper, we integrate recently constructed anytime-valid permutation p-values into a broad class of multiple testing procedures, including the Benjamini-Hochberg procedure. This allows to fully adapt the number of permutations to the underlying data and thus, for example, to the number of rejections made by the multiple testing procedure. Even though this data-adaptive stopping can induce dependencies between the p-values that violate the usual assumptions of the Benjamini-Hochberg procedure, we prove that our approach controls the false discovery rate under mild assumptions. Furthermore, our method provably decreases the required number of permutations substantially without compromising power. On a real genomics data set, our method reduced the computational time from more than three days to less than four minutes while increasing the number of rejections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15586v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lasse Fischer, Timothy Barry, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Doubly robust estimation and sensitivity analysis with outcomes truncated by death in multi-arm clinical trials</title>
      <link>https://arxiv.org/abs/2410.07483</link>
      <description>arXiv:2410.07483v2 Announce Type: replace 
Abstract: In clinical trials, the observation of participant outcomes may frequently be hindered by death, leading to ambiguity in defining a scientifically meaningful final outcome for those who die. Principal stratification methods are valuable tools for addressing the average causal effect among always-survivors, i.e., the average treatment effect among a subpopulation defined as those who would survive regardless of treatment assignment. Although robust methods for the truncation-by-death problem in two-arm clinical trials have been previously studied, its expansion to multi-arm clinical trials remains elusive. In this article, we study the identification of a class of survivor average causal effect estimands with multiple treatments under monotonicity and principal ignorability, and first propose simple weighting and regression approaches for point estimation. As a further improvement, we derive the efficient influence function to motivate doubly robust estimators for the survivor average causal effects in multi-arm clinical trials. We also propose sensitivity methods under violations of key causal assumptions. Extensive simulations are conducted to investigate the finite-sample performance of the proposed methods against the existing methods, and a real data example is used to illustrate how to operationalize the proposed estimators and the sensitivity methods in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07483v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jiaqi Tong, Chao Cheng, Guangyu Tong, Michael O. Harhay, Fan Li</dc:creator>
    </item>
    <item>
      <title>Multiple Imputation for Nonresponse in Complex Surveys Using Design Weights and Auxiliary Margins</title>
      <link>https://arxiv.org/abs/2412.10988</link>
      <description>arXiv:2412.10988v5 Announce Type: replace 
Abstract: Survey data typically have missing values due to unit and item nonresponse. Sometimes, survey organizations know the marginal distributions of certain categorical variables in the survey. As shown in previous work, survey organizations can leverage these distributions in multiple imputation for nonignorable unit nonresponse, generating imputations that result in plausible completed-data estimates for the variables with known margins. However, this prior work does not use the design weights for unit nonrespondents; rather, it relies on a set of fabricated weights for these units. We extend this previous work to utilize the design weights for all sampled units. We illustrate the approach using simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10988v5</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kewei Xu, Jerome P. Reiter</dc:creator>
    </item>
    <item>
      <title>Bayesian Outlier Detection for Matrix-variate Models</title>
      <link>https://arxiv.org/abs/2503.19515</link>
      <description>arXiv:2503.19515v2 Announce Type: replace 
Abstract: Anomalies in economic and financial data -- often linked to rare yet impactful events -- are of theoretical interest, but can also severely distort inference. Although outlier-robust methodologies can be used, many researchers prefer pre-processing strategies that remove outliers. In this work, an efficient sequential Bayesian framework is proposed for outlier detection based on the predictive Bayes Factor (BF). The proposed method is specifically designed for large, multidimensional datasets and extends univariate Bayesian model outlier detection procedures to the matrix-variate setting. Leveraging power-discounted priors, tractable predictive BF are obtained, thereby avoiding computationally intensive techniques. The BF finite sample distribution, the test critical region, and robust extensions of the test are introduced by exploiting the sampling variability. The framework supports online detection with analytical tractability, ensuring both accuracy and scalability. Its effectiveness is demonstrated through simulations, and three applications to reference datasets in macroeconomics and finance are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19515v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Monica Billio, Roberto Casarin, Fausto Corradin, Antonio Peruzzi</dc:creator>
    </item>
    <item>
      <title>The Multiplicative Instrumental Variable Model</title>
      <link>https://arxiv.org/abs/2507.09302</link>
      <description>arXiv:2507.09302v2 Announce Type: replace 
Abstract: The instrumental variable (IV) design is a common approach to address hidden confounding bias. For validity, an IV must impact the outcome only through its association with the treatment. In addition, IV identification has required a homogeneity condition such as monotonicity or no unmeasured common effect modifier between the additive effect of the treatment on the outcome, and that of the IV on the treatment. In this work, we introduce the Multiplicative Instrumental Variable Model (MIV), which encodes a condition of no multiplicative interaction between the instrument and an unmeasured confounder in the treatment propensity score model. Thus, the MIV provides a novel formalization of the core IV independence condition interpreted as independent mechanisms of action, by which the instrument and hidden confounders influence treatment uptake, respectively. As we formally establish, MIV provides nonparametric identification of the population average treatment effect on the treated (ATT) via a single-arm version of the classical Wald ratio IV estimand, for which we propose a novel class of estimators that are multiply robust and semiparametric efficient. Finally, we illustrate the methods in extended simulations and an application on the causal impact of a job training program on subsequent earnings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09302v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiewen Liu, Chan Park, Yonghoon Lee, Yunshu Zhang, Mengxin Yu, James M. Robins, Eric J. Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Inference on Nonlinear Counterfactual Functionals under a Multiplicative IV Model</title>
      <link>https://arxiv.org/abs/2507.15612</link>
      <description>arXiv:2507.15612v2 Announce Type: replace 
Abstract: Instrumental variable (IV) methods play a central role in causal inference, particularly in settings where treatment assignment is confounded by unobserved variables. IV methods have been extensively developed in recent years and applied across diverse domains, from economics to epidemiology. In this work, we study the recently introduced multiplicative IV (MIV) model and demonstrate its utility for causal inference beyond the average treatment effect. In particular, we show that it enables identification and inference for a broad class of counterfactual functionals characterized by moment equations. This includes, for example, inference on quantile treatment effects. We develop methods for efficient and multiply robust estimation of such functionals, and provide inference procedures with asymptotic validity. Experimental results demonstrate that the proposed procedure performs well even with moderate sample sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15612v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yonghoon Lee, Mengxin Yu, Jiewen Liu, Chan Park, Yunshu Zhang, James M. Robins, Eric J. Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Bayesian Smoothed Quantile Regression</title>
      <link>https://arxiv.org/abs/2508.01738</link>
      <description>arXiv:2508.01738v3 Announce Type: replace 
Abstract: Bayesian quantile regression (BQR) based on the asymmetric Laplace distribution (ALD) has two fundamental limitations: its posterior mean yields biased quantile estimates, and the non-differentiable check loss precludes gradient-based MCMC methods. We propose Bayesian smoothed quantile regression (BSQR), a principled reformulation that constructs a novel, continuously differentiable likelihood from a kernel-smoothed check loss, simultaneously ensuring a consistent posterior by aligning the inferential target with the smoothed objective and enabling efficient Hamiltonian Monte Carlo (HMC) sampling. Our theoretical analysis establishes posterior propriety for various priors and examines the impact of kernel choice. Simulations show BSQR reduces predictive check loss by up to 50% at extreme quantiles over ALD-based methods and improves MCMC efficiency by 20-40% in effective sample size. An application to financial risk during the COVID-19 era demonstrates superior tail risk modeling. The BSQR framework offers a theoretically grounded, computationally efficient solution to longstanding challenges in BQR, with uniform and triangular kernels emerging as highly effective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01738v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bingqi Liu, Kangqiang Li, Tianxiao Pang</dc:creator>
    </item>
    <item>
      <title>Learning covariate importance for matching in policy-relevant observational research</title>
      <link>https://arxiv.org/abs/2403.12367</link>
      <description>arXiv:2403.12367v2 Announce Type: replace-cross 
Abstract: Matching methods are widely used to reduce confounding effects in observational studies, but conventional approaches often treat all covariates as equally important, which can result in poor performance when covariates differ in their relevance to the study. We propose the Priority-Aware one-to-one Matching Algorithm (PAMA), a novel semi-supervised framework that learns a covariate importance measure from a subset data of units that are paired by experts and uses it to match additional units. It optimizes a weighted quadratic score that reflects the relevance between each covariate and the study, and iteratively updates the covariate importance measure in the score function using unlabeled data. PAMA is model-free, but we have established that the covariate importance measure -- the learned weights -- is consistent when the oracle matching rule aligns with the design. In addition, we introduce extensions that address imbalanced data, accommodate temporal covariates, and improve robustness to mispaired observations.
  In simulations, PAMA outperforms standard methods, particularly in high-dimensional settings and under model misspecification. Applied to a real-world study of in-person schooling and COVID-19 transmission, PAMA recovers nearly twice as many expert-designated matches as competing methods using baseline covariates. A self-taught learning extension improves performance in simulations, though its benefit is context-dependent.
  To our knowledge, PAMA is the first framework to apply semi-supervised learning to observational matching with covariates of unequal relevance. It offers a scalable and interpretable tool for incorporating expert insight into policy-relevant observational research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12367v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongzhe Zhang, Jiasheng Shi, Jing Huang</dc:creator>
    </item>
  </channel>
</rss>
