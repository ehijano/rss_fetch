<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 22 Aug 2025 01:23:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>piCurve: an R package for modeling photosynthesis-irradiance curves</title>
      <link>https://arxiv.org/abs/2508.14321</link>
      <description>arXiv:2508.14321v1 Announce Type: new 
Abstract: Photosynthesis-irradiance (PI) curves are foundational for quantifying primary production, parameterizing ecosystem and biogeochemical models, and interpreting physiological acclimation to light. Despite their broad use, researchers lack a unified, reproducible toolkit to fit, compare, and diagnose the many PI formulations that have accumulated over the last century. We introduce piCurve, an R package that standardizes the modeling of PI relationships, with a library of widely used light-limited, light-saturated, and photoinhibited formulations and a consistent statistical framework for estimation and comparison. With the total of 24 PI models, piCurve supports mean squared error (MSE) and maximum likelihood estimation (MLE), provides uncertainty quantification via information matrix (Hessian), and includes automated, data-informed initialization to improve convergence. Utilities classify PI data into light-limited, light-saturated, and photoinhibited regions, while plotting and 'tidy' helpers streamline workflow and reporting. Together, these features enable reproducible analyses and fair model comparisons, including for curves exhibiting a plateau followed by photoinhibition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14321v1</guid>
      <category>stat.AP</category>
      <category>q-bio.QM</category>
      <category>stat.CO</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad M. Amirian, Andrew J. Irwin</dc:creator>
    </item>
    <item>
      <title>Comparative Evaluation of Difference in Differences Methods for Staggered Adoption Interventions</title>
      <link>https://arxiv.org/abs/2508.14365</link>
      <description>arXiv:2508.14365v1 Announce Type: new 
Abstract: Staggered adoption is a common approach for implementing healthcare interventions, where different units adopt the program at different times. Difference-in-differences (DiD) methods are frequently used to evaluate the effects of such interventions. Nonetheless, recent research has shown that classical DiD approaches designed for a single treatment start date can produce biased estimates in staggered adoption settings, particularly due to treatment effect heterogeneity across adoption and calendar time. Several alternative methods have been developed to address these limitations. However, these methods have not been fully systematically compared, and their practical utility remains unclear. Motivated by a payment program implemented by a healthcare provider in Hawaii, we provide a comprehensive review of the staggered adoption setting and a selection of DiD methods suitable for this context. We begin with a theoretical overview of these methods, followed by a simulation study designed to resemble the characteristics of our application, where the intervention is implemented at the cluster level. Our results show that the current methods tend to under-perform when the number of clusters is small, but improve as the number of clusters increases. We then apply the methods to evaluate the real-world payment program intervention and offer practical recommendations for researchers implementing DiD methods for staggered adoption settings. Finally, we translate our findings into practical guidance for applied researchers choosing among DiD methods for staggered adoption settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14365v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ernesto Ulloa-P\'erez, Elizabeth F. Bair, Amol S. Navathe, Kristin A. Linn</dc:creator>
    </item>
    <item>
      <title>Experimental validation of universal filtering and smoothing for linear system identification using adaptive tuning</title>
      <link>https://arxiv.org/abs/2508.14629</link>
      <description>arXiv:2508.14629v1 Announce Type: new 
Abstract: In Kalman filtering, unknown inputs are often estimated by augmenting the state vector, which introduces reliance on fictitious input models. In contrast, minimum-variance unbiased methods estimate inputs and states separately, avoiding fictitious models but requiring strict sensor configurations, such as full-rank feedforward matrices or without direct feedthrough. To address these limitations, two universal approaches have been proposed to handle systems with or without direct feedthrough, including cases of rank-deficient feedforward matrices. Numerical studies have shown their robustness and applicability, however, they have so far relied on offline tuning, and performance under physical sensor noise and structural uncertainties has not yet been experimentally validated. Contributing to this gap, this paper experimentally validates the universal methods on a five-storey shear frame subjected to shake table tests and multi-impact events. Both typical and rank-deficient conditions are considered. Furthermore, a self-tuning mechanism is introduced to replace impractical offline tuning and enable real-time adaptability. The findings of this paper provide strong evidence of the robustness and adaptability of the methods for structural health monitoring applications, particularly when sensor networks deviate from ideal configurations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14629v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zihao Liu, Sima Abolghasemi, Mohsen Ebrahimzadeh Hassanabadi, Nicholas E. Wierschem, Daniel Dias-da-Costa</dc:creator>
    </item>
    <item>
      <title>A Bayesian Semiparametric Mixture Model for Clustering Zero-Inflated Microbiome Data</title>
      <link>https://arxiv.org/abs/2508.14184</link>
      <description>arXiv:2508.14184v1 Announce Type: cross 
Abstract: Microbiome research has immense potential for unlocking insights into human health and disease. A common goal in human microbiome research is identifying subgroups of individuals with similar microbial composition that may be linked to specific health states or environmental exposures. However, existing clustering methods are often not equipped to accommodate the complex structure of microbiome data and typically make limiting assumptions regarding the number of clusters in the data which can bias inference. Designed for zero-inflated multivariate compositional count data collected in microbiome research, we propose a novel Bayesian semiparametric mixture modeling framework that simultaneously learns the number of clusters in the data while performing cluster allocation. In simulation, we demonstrate the clustering performance of our method compared to distance- and model-based alternatives and the importance of accommodating zero-inflation when present in the data. We then apply the model to identify clusters in microbiome data collected in a study designed to investigate the relation between gut microbial composition and enteric diarrheal disease.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14184v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Suppapat Korsurat, Matthew D. Koslovsky</dc:creator>
    </item>
    <item>
      <title>A statistical test for network similarity</title>
      <link>https://arxiv.org/abs/2508.14399</link>
      <description>arXiv:2508.14399v1 Announce Type: cross 
Abstract: In this article, we revisit and expand our prior work on graph similarity. In this version of our work, we offer an extensive array of empirical tests. We also examine the sensitivity of our test to network variations. Our test performs exactly as expected, on synthetic and real-world graphs. It offers a very accurate measure of graph (dis)similarity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14399v1</guid>
      <category>cs.DM</category>
      <category>stat.AP</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Pierre Miasnikof, Alexander Y. Shetopaloff</dc:creator>
    </item>
    <item>
      <title>Non-Existent Outcomes in Research on Inequality: A Causal Approach</title>
      <link>https://arxiv.org/abs/2508.14770</link>
      <description>arXiv:2508.14770v1 Announce Type: cross 
Abstract: Scholars of social stratification often study exposures that shape life outcomes. But some outcomes (such as wage) only exist for some people (such as those who are employed). We show how a common practice -- dropping cases with non-existent outcomes -- can obscure causal effects when a treatment affects both outcome existence and outcome values. The effects of both beneficial and harmful treatments can be underestimated. Drawing on existing approaches for principal stratification, we show how to study (1) the average effect on whether an outcome exists and (2) the average effect on the outcome among the latent subgroup whose outcome would exist in either treatment condition. To extend our approach to the selection-on-observables settings common in applied research, we develop a framework involving regression and simulation to enable principal stratification estimates that adjust for measured confounders. We illustrate through an empirical example about the effects of parenthood on labor market outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14770v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ian Lundberg, Soonhong Cho</dc:creator>
    </item>
    <item>
      <title>Quantifying How Much Has Been Learned from a Research Study</title>
      <link>https://arxiv.org/abs/2508.14789</link>
      <description>arXiv:2508.14789v1 Announce Type: cross 
Abstract: How much does a research study contribute to a scientific literature? We propose a learning metric to quantify how much a research community learns from a given study. To do so, we adopt a Bayesian perspective and assess changes in the community's beliefs once updated with a new study's evidence. We recommend the Wasserstein-2 distance as a way to describe how the research community's prior beliefs change to incorporate a study's findings. We illustrate this approach through stylized examples and empirical applications, showing how it differs from more traditional evaluative standards, such as statistical significance. We then extend the framework to the prospective setting, offering a way for decision-makers to evaluate the expected amount of learning from a proposed study. While assessments about what has or could be learned from a research program are often expressed informally, our learning metric provides a principled tool for judging scientific contributions. By formalizing these judgments, our measure has the potential to allow for more transparent assessments of past and prospective research contributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14789v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas M. Mikhaeil, Donald P. Green</dc:creator>
    </item>
    <item>
      <title>The C-index Multiverse</title>
      <link>https://arxiv.org/abs/2508.14821</link>
      <description>arXiv:2508.14821v1 Announce Type: cross 
Abstract: Quantifying out-of-sample discrimination performance for time-to-event outcomes is a fundamental step for model evaluation and selection in the context of predictive modelling. The concordance index, or C-index, is a widely used metric for this purpose, particularly with the growing development of machine learning methods. Beyond differences between proposed C-index estimators (e.g. Harrell's, Uno's and Antolini's), we demonstrate the existence of a C-index multiverse among available R and python software, where seemingly equal implementations can yield different results. This can undermine reproducibility and complicate fair comparisons across models and studies. Key variation sources include tie handling and adjustment to censoring. Additionally, the absence of a standardised approach to summarise risk from survival distributions, result in another source of variation dependent on input types. We demonstrate the consequences of the C-index multiverse when quantifying predictive performance for several survival models (from Cox proportional hazards to recent deep learning approaches) on publicly available breast cancer data, and semi-synthetic examples. Our work emphasises the need for better reporting to improve transparency and reproducibility. This article aims to be a useful guideline, helping analysts when navigating the multiverse, providing unified documentation and highlighting potential pitfalls of existing software. All code is publicly available at: www.github.com/BBolosSierra/CindexMultiverse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14821v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bego\~na B. Sierra, Colin McLean, Peter S. Hall, Catalina A. Vallejos</dc:creator>
    </item>
    <item>
      <title>Simplifying Random Forests' Probabilistic Forecasts</title>
      <link>https://arxiv.org/abs/2408.12332</link>
      <description>arXiv:2408.12332v4 Announce Type: replace 
Abstract: Since their introduction by Breiman, Random Forests (RFs) have proven to be useful for both classification and regression tasks. The RF prediction of a previously unseen observation can be represented as a weighted sum of all training sample observations. This nearest-neighbor-type representation is useful, among other things, for constructing forecast distributions (Meinshausen, 2006). In this paper, we consider simplifying RF-based forecast distributions by sparsifying them. That is, we focus on a small subset of $k$ nearest neighbors while setting the remaining weights to zero. This simplification, which we refer to as `Top$k$', greatly improves the interpretability of RF predictions. It can be applied to any forecasting task without re-training existing RF models. In empirical experiments, we document that the simplified predictions can be similar to or exceed the original ones in terms of forecasting performance. We explore the statistical sources of this finding via a stylized analytical model of RFs. The model suggests that simplification is particularly promising if the unknown true forecast distribution contains many small weights that are estimated imprecisely.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12332v4</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nils Koster, Fabian Kr\"uger</dc:creator>
    </item>
    <item>
      <title>Gender disparities in rehospitalisations after coronary artery bypass grafting: evidence from a sparse functional causal mediation analysis of the MIMIC-IV data</title>
      <link>https://arxiv.org/abs/2410.22502</link>
      <description>arXiv:2410.22502v3 Announce Type: replace 
Abstract: Hospital readmissions following coronary artery bypass grafting (CABG) not only impose a substantial cost burden on healthcare systems but also serve as a potential indicator of the quality of medical care. Previous studies of gender effects on complications after CABG surgery have consistently revealed that women tend to suffer worse outcomes. To better understand the causal pathway from gender to the number of rehospitalisations, we study the postoperative central venous pressure (CVP), recorded over the first 24 hours of patients' intensive care unit (ICU) stay after the CABG surgery, as sparse observations of a functional mediator. Confronted with time-varying CVP measurements and zero-inflated rehospitalisation counts within 60 days following discharge, we propose a parameter-simulating quasi-Bayesian Monte Carlo approximation method that accommodates a sparse functional mediator and a zero-inflated count outcome for causal mediation analysis. We find a causal relationship between the female gender and increased rehospitalisation counts after CABG, and that time-varying central venous pressure mediates this causal effect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22502v3</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Henan Xu, Yeying Zhu, Donna L. Coffman</dc:creator>
    </item>
    <item>
      <title>Hierarchical Temporal Point Process Modeling of Aggressive Behavior Onset in Psychiatric Inpatient Youth with Autism for Branching Factor Estimation</title>
      <link>https://arxiv.org/abs/2507.12424</link>
      <description>arXiv:2507.12424v3 Announce Type: replace 
Abstract: Aggressive behavior in autistic inpatient youth often arises in temporally clustered bursts complicating efforts to distinguish external triggers from internal escalation. The sample population branching factor-the expected number of new onsets triggered by a given event-is a key summary of self-excitation in behavior dynamics. Prior pooled models overestimate this quantity by ignoring patient-specific variability. We addressed this using a hierarchical Hawkes process with an exponential kernel and edge-effect correction allowing partial pooling across patients. This approach reduces bias from high-frequency individuals and stabilizes estimates for those with sparse data. Bayesian inference was performed using the No U-Turn Sampler with model evaluation via convergence diagnostics, power-scaling sensitivity analysis, and multiple Goodness-of-Fit (GOF) metrics: PSIS-LOO the Lewis test with Durbin's modification and residual analysis based on the Random Time Change Theorem (RTCT). The hierarchical model yielded a significantly lower and more precise branching factor estimate mean (0.742 +- 0.026) than the pooled model (0.899 +- 0.015) and narrower intervals than the unpooled model (0.717 +- 0.139). This led to a threefold smaller cascade of events per onset under the hierarchical model. Sensitivity analyses confirmed robustness to prior and likelihood perturbations while the unpooled model showed instability for sparse individuals. GOF measures consistently favored or on par to the hierarchical model. Hierarchical Hawkes modeling with edge-effect correction provides robust estimation of branching dynamics by capturing both within- and between-patient variability. This enables clearer separation of endogenous from exogenous events supports linkage to physiological signals and enhances early warning systems individualized treatment and resource allocation in inpatient care.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12424v3</guid>
      <category>stat.AP</category>
      <category>math.PR</category>
      <category>stat.OT</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Potter, Michael Everett, Deniz Erdogmus, Yuna Watanabe, Tales Imbiriba, Matthew S. Goodwin</dc:creator>
    </item>
    <item>
      <title>Simulation of extreme functionals in meteoceanic data: Application to surge evolution over tidal cycles</title>
      <link>https://arxiv.org/abs/2508.13687</link>
      <description>arXiv:2508.13687v2 Announce Type: replace 
Abstract: We investigate the influence of time-varying meteoceanic conditions on coastal flooding under the prism of rare events. Focusing on conditions observed over half tidal cycles, we observe that such data fall within the framework of functional extreme value theory, but violate standard assumptions due to temporal dependence and short-tailed behavior.a To address this, we propose a two-stage methodology. First, we introduce an autoregressive model to eliminate temporal dependence between cycles. Second, considering the model residuals, we adapt existing techniques based on Pareto processes. This allows us to build a simulator of extreme scenarios, by applying inverse transformations. These simulations depend on an initial time series, which can be randomly selected to tune the desired level of extremes. We validate the simulator performance by comparing simulated times series with observations, through several criteria, based on principal component analysis, extreme value analysis, and classification algorithms. The approach is applied to the surge data, on the G{\^a}vres site, located in southern Brittany, France.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13687v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nathan Gorse (INSA Toulouse, IMT), Olivier Roustant (INSA Toulouse, IMT), J\'er\'emy Rohmer (BRGM), D\'eborah Idier (BRGM)</dc:creator>
    </item>
    <item>
      <title>Exploring the Difficulty of Estimating Win Probability: A Simulation Study</title>
      <link>https://arxiv.org/abs/2406.16171</link>
      <description>arXiv:2406.16171v5 Announce Type: replace-cross 
Abstract: Estimating win probability is one of the classic modeling tasks of sports analytics. Many widely used win probability estimators use machine learning to fit the relationship between a binary win/loss outcome variable and certain game-state variables. To illustrate just how difficult it is to accurately fit such a model from noisy and highly correlated observational data, in this paper we conduct a simulation study. We create a simplified random walk version of football in which true win probability at each game-state is known, and we see how well a model recovers it. We find that the dependence structure of observational play-by-play data substantially inflates the bias and variance of estimators and lowers the effective sample size. Further, to achieve approximately valid marginal coverage, win probability confidence intervals need to be substantially wide. Concisely, these are high variance estimators subject to substantial uncertainty. Our findings are not unique to the particular application of estimating win probability; they are broadly applicable across sports analytics, as myriad other sports datasets are clustered into groups of observations that share the same outcome.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16171v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan S. Brill, Ronald Yurko, Abraham J. Wyner</dc:creator>
    </item>
    <item>
      <title>Ranking by Lifts: A Cost-Benefit Approach to Large-Scale A/B Tests</title>
      <link>https://arxiv.org/abs/2407.01036</link>
      <description>arXiv:2407.01036v3 Announce Type: replace-cross 
Abstract: A/B testing is a core tool for decision-making in business experimentation, particularly in digital platforms and marketplaces. Practitioners often prioritize lift in performance metrics while seeking to control the costs of false discoveries. This paper develops a decision-theoretic framework for maximizing expected profit subject to a constraint on the cost-weighted false discovery rate (FDR). We propose an empirical Bayes approach that uses a greedy knapsack algorithm to rank experiments based on the ratio of expected lift to cost, incorporating the local false discovery rate (lfdr) as a key statistic. The resulting oracle rule is valid and rank-optimal. In large-scale settings, we establish the asymptotic validity of a data-driven implementation and demonstrate superior finite-sample performance over existing FDR-controlling methods. An application to A/B tests run on the Optimizely platform highlights the business value of the approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01036v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pallavi Basu, Ron Berman</dc:creator>
    </item>
    <item>
      <title>Inequality Restricted Minimum Density Power Divergence Estimation in Panel Count Data</title>
      <link>https://arxiv.org/abs/2503.21534</link>
      <description>arXiv:2503.21534v3 Announce Type: replace-cross 
Abstract: Analysis of panel count data has garnered a considerable amount of attention in the literature, leading to the development of multiple statistical techniques. In inferential analysis, most of the works focus on leveraging estimating equations-based techniques or conventional maximum likelihood estimation. However, the robustness of these methods is largely questionable. In this paper, we present the robust density power divergence estimation for panel count data arising from nonhomogeneous Poisson processes, correlated through a latent frailty variable. In order to cope with real-world incidents, it is often desired to impose certain inequality constraints on the parameter space, giving rise to the restricted minimum density power divergence estimator. The significant contribution of this study lies in deriving its asymptotic properties. The proposed method ensures high efficiency in the model estimation while providing reliable inference despite data contamination. Moreover, the density power divergence measure is governed by a tuning parameter $\gamma$, which controls the trade-off between robustness and efficiency. To effectively determine the optimal value of $\gamma$, this study employs a generalized score-matching technique, marking considerable progress in the data analysis. Simulation studies and real data examples are provided to illustrate the performance of the estimator and to substantiate the theory developed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21534v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Udita Goswami, Shuvashree Mondal</dc:creator>
    </item>
    <item>
      <title>nonprobsvy -- An R package for modern methods for non-probability surveys</title>
      <link>https://arxiv.org/abs/2504.04255</link>
      <description>arXiv:2504.04255v2 Announce Type: replace-cross 
Abstract: The following paper presents nonprobsvy -- an R package for inference based on non-probability samples. The package implements various approaches that can be categorized into three groups: prediction-based approach, inverse probability weighting and doubly robust approach. In the package, we assume the existence of either population-level data or probability-based population information and leverage the survey package for inference. The package implements both analytical and bootstrap variance estimation for the proposed estimators. In the paper we present the theory behind the package, its functionalities and case study that showcases the usage of the package. The package is aimed at scientists and researchers who would like to use non-probability samples (e.g.big data, opt-in web panels, social media) to accurately estimate population characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04255v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>{\L}ukasz Chrostowski, Piotr Chlebicki, Maciej Ber\k{e}sewicz</dc:creator>
    </item>
  </channel>
</rss>
