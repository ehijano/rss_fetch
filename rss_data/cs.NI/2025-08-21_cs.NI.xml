<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 21 Aug 2025 04:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>StarStream: Live Video Analytics over Space Networking</title>
      <link>https://arxiv.org/abs/2508.14222</link>
      <description>arXiv:2508.14222v1 Announce Type: new 
Abstract: Streaming videos from resource-constrained front-end devices over networks to resource-rich cloud servers has long been a common practice for surveillance and analytics. Most existing live video analytics (LVA) systems, however, have been built over terrestrial networks, limiting their applications during natural disasters and in remote areas that desperately call for real-time visual data delivery and scene analysis. With the recent advent of space networking, in particular, Low Earth Orbit (LEO) satellite constellations such as Starlink, high-speed truly global Internet access is becoming available and affordable. This paper examines the challenges and potentials of LVA over modern LEO satellite networking (LSN). Using Starlink as the testbed, we have carried out extensive in-the-wild measurements to gain insights into its achievable performance for LVA. The results reveal that the uplink bottleneck in today's LSN, together with the volatile network conditions, can significantly affect the service quality of LVA and necessitate prompt adaptation. We accordingly develop StarStream, a novel LSN-adaptive streaming framework for LVA. At its core, StarStream is empowered by a Transformer-based network performance predictor tailored for LSN and a content-aware configuration optimizer. We discuss a series of key design and implementation issues of StarStream and demonstrate its effectiveness and superiority through trace-driven experiments with real-world network and video processing data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14222v1</guid>
      <category>cs.NI</category>
      <category>cs.MM</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Miao Zhang, Jiaxing Li, Haoyuan Zhao, Linfeng Shen, Jiangchuan Liu</dc:creator>
    </item>
    <item>
      <title>OmniSense: Towards Edge-Assisted Online Analytics for 360-Degree Videos</title>
      <link>https://arxiv.org/abs/2508.14237</link>
      <description>arXiv:2508.14237v1 Announce Type: new 
Abstract: With the reduced hardware costs of omnidirectional cameras and the proliferation of various extended reality applications, more and more $360^\circ$ videos are being captured. To fully unleash their potential, advanced video analytics is expected to extract actionable insights and situational knowledge without blind spots from the videos. In this paper, we present OmniSense, a novel edge-assisted framework for online immersive video analytics. OmniSense achieves both low latency and high accuracy, combating the significant computation and network resource challenges of analyzing $360^\circ$ videos. Motivated by our measurement insights into $360^\circ$ videos, OmniSense introduces a lightweight spherical region of interest (SRoI) prediction algorithm to prune redundant information in $360^\circ$ frames. Incorporating the video content and network dynamics, it then smartly scales vision models to analyze the predicted SRoIs with optimized resource utilization. We implement a prototype of OmniSense with commodity devices and evaluate it on diverse real-world collected $360^\circ$ videos. Extensive evaluation results show that compared to resource-agnostic baselines, it improves the accuracy by $19.8\%$ -- $114.6\%$ with similar end-to-end latencies. Meanwhile, it hits $2.0\times$ -- $2.4\times$ speedups while keeping the accuracy on par with the highest accuracy of baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14237v1</guid>
      <category>cs.NI</category>
      <category>cs.CV</category>
      <category>cs.MM</category>
      <category>eess.IV</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Miao Zhang, Yifei Zhu, Linfeng Shen, Fangxin Wang, Jiangchuan Liu</dc:creator>
    </item>
    <item>
      <title>A Distributed Learned Hash Table</title>
      <link>https://arxiv.org/abs/2508.14239</link>
      <description>arXiv:2508.14239v1 Announce Type: new 
Abstract: Distributed Hash Tables (DHTs) are pivotal in numerous high-impact key-value applications built on distributed networked systems, offering a decentralized architecture that avoids single points of failure and improves data availability. Despite their widespread utility, DHTs face substantial challenges in handling range queries, which are crucial for applications such as LLM serving, distributed storage, databases, content delivery networks, and blockchains. To address this limitation, we present LEAD, a novel system incorporating learned models within DHT structures to significantly optimize range query performance. LEAD utilizes a recursive machine learning model to map and retrieve data across a distributed system while preserving the inherent order of data. LEAD includes the designs to minimize range query latency and message cost while maintaining high scalability and resilience to network churn. Our comprehensive evaluations, conducted in both testbed implementation and simulations, demonstrate that LEAD achieves tremendous advantages in system efficiency compared to existing range query methods in large-scale distributed systems, reducing query latency and message cost by 80% to 90%+. Furthermore, LEAD exhibits remarkable scalability and robustness against system churn, providing a robust, scalable solution for efficient data retrieval in distributed key-value systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14239v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shengze Wang, Yi Liu, Xiaoxue Zhang, Liting Hu, Chen Qian</dc:creator>
    </item>
    <item>
      <title>DeeP-TE: Data-enabled Predictive Traffic Engineering</title>
      <link>https://arxiv.org/abs/2508.14281</link>
      <description>arXiv:2508.14281v1 Announce Type: new 
Abstract: Routing configurations of a network should constantly adapt to traffic variations to achieve good network performance. Adaptive routing faces two main challenges: 1) how to accurately measure/estimate time-varying traffic matrices? 2) how to control the network and application performance degradation caused by frequent route changes? In this paper, we develop a novel data-enabled predictive traffic engineering (DeeP-TE) algorithm that minimizes the network congestion by gracefully adapting routing configurations over time. Our control algorithm can generate routing updates directly from the historical routing data and the corresponding link rate data, without direct traffic matrix measurement or estimation. Numerical experiments on real network topologies with real traffic matrices demonstrate that the proposed DeeP-TE routing adaptation algorithm can achieve close-to-optimal control effectiveness with significantly lower routing variations than the baseline methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14281v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhun Yin, Xiaotian Li, Lifan Mei, Yong Liu, Zhong-Ping Jiang</dc:creator>
    </item>
    <item>
      <title>Design and Simulation of Fault-Tolerant Network Switching System Using Python-Based Algorithms</title>
      <link>https://arxiv.org/abs/2508.14305</link>
      <description>arXiv:2508.14305v1 Announce Type: new 
Abstract: Ensuring uninterrupted data flow in modern networks requires robust fault-tolerant mechanisms, especially in environments where reliability and responsiveness are critical. This paper presents the design and simulation of a fault-tolerant network switching system using Python-based algorithms. A simulated enterprise-level Local Area Network (LAN) was modeled using NetworkX to represent switch-router interconnectivity with redundant links. Fault scenarios, including link failure and congestion, were injected using Scapy, while automatic failover and rerouting were implemented via custom Python logic. The system demonstrates resilience by dynamically detecting path failures, redistributing network traffic through redundant links, and minimizing downtime. Performance evaluations reveal significant improvements in packet delivery continuity, faster recovery times, and reduced packet loss compared to non-fault-tolerant baselines. The implementation provides a scalable and lightweight approach to integrating fault-tolerance features into mid-scale networks, with potential application in enterprise information technology infrastructures and academic simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14305v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Terlumun Gbaden, Mterorga Ukor, Grace Erdoo Ateata</dc:creator>
    </item>
    <item>
      <title>The Small-World Beneath LEO Satellite Coverage: Ground Hubs in Multi-Shell Constellations</title>
      <link>https://arxiv.org/abs/2508.14335</link>
      <description>arXiv:2508.14335v1 Announce Type: new 
Abstract: In recent years, the emergence of large-scale Low-Earth-Orbit (LEO) satellite constellations has introduced unprecedented opportunities for global connectivity. However, routing efficiency and inter-shell communication remain key challenges in multi-shell architectures. This paper investigates the structural properties and network dynamics of a representative six-shell mega-constellation composed of 10,956 satellites and 198 gateway stations (GSs). Leveraging tools from complex network analysis, we identify several critical findings: (1) the constellation exhibits strong small-world characteristics, enabling efficient routing despite large network diameters; (2) GS relays play a pivotal role in enhancing inter-shell connectivity by bridging otherwise disconnected components; (3) feeder links significantly reduce average path length, making long-haul communication more feasible; (4) betweenness analysis reveals load imbalances among GSs, indicating the need for traffic-aware management strategies; (5) the architecture offers excellent spatial coverage and resilience, maintaining connectivity and low routing costs even under GS failures. These insights not only explain the design rationale behind current mega-constellations like SpaceX Starlink, but also provide valuable guidance for the evolution of future satellite network infrastructures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14335v1</guid>
      <category>cs.NI</category>
      <category>cs.SI</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hailong Su, Jinshu Su, Yusheng Xia, Haibin Li</dc:creator>
    </item>
    <item>
      <title>Availability-Aware VNF Placement and Request Routing in MEC-Enabled 5G Networks</title>
      <link>https://arxiv.org/abs/2508.14435</link>
      <description>arXiv:2508.14435v1 Announce Type: new 
Abstract: In this paper, we study the virtual network function (VNF) placement problem in mobile edge computing (MEC)-enabled 5G networks to meet the stringent reliability and latency requirements of uRLLC applications. We pose it as a constrained optimization problem, which is NP-hard, to maximize the total reward obtained by a network service provider by serving uRLLC service requests. We propose an approximated randomized rounding approach to solve the NP-hard optimization problem in polynomial time. We prove that the proposed randomized approach achieves performance guarantees while violating the resource constraints boundedly. Furthermore, we present a greedy-heuristic approach to tackle the violations of resource constraints.
  Simulation results show that the proposed randomized rounding and greedy approaches achieve a total reward which is within 5% and 10% of the optimal solution, respectively. Furthermore, we compare the proposed greedy approach with the existing schemes that do not consider the availability requirements. We observe that the existing schemes perform poorly in terms of total reward, as negligence to the availability requirements negatively impacts the number of successfully served requests. These findings highlight the trade-off between availability and resource efficiency in latency-sensitive uRLLC applications. We also implement a software prototype of a 5G network using open-source software platforms with redundant placement of VNFs. The results on packet delivery ratio and latency obtained from the prototype implementation are also improved in the redundant VNFs with different failure probabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14435v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aqsa Sayeed, Samaresh Bera</dc:creator>
    </item>
    <item>
      <title>Transforming Next-generation Network Planning assisted by Data Acquisition of Top Three Spanish MNOs</title>
      <link>https://arxiv.org/abs/2508.14445</link>
      <description>arXiv:2508.14445v1 Announce Type: new 
Abstract: In this paper, we address the necessity of data related to mobile traffic of the legacy infrastructure to extract useful information and perform network dimensioning for 5G. These data can help us achieve a more efficient network planning design, especially in terms of topology and cost. To that end, a real open database of top three Spanish mobile network operators (MNOs) is used to estimate the traffic and to identify the area of highest user density for the deployment of new services. We propose the data acquisition procedure described to clean the database, to extract meaningful traffic information and to visualize traffic density patterns for new gNB deployments. We present the state of the art in Network Data. We describe the considered network database in detail. The Network Data Acquisition entity along with the proposed procedure is explained. The corresponding results are discussed, following the conclusions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14445v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M. Umar Khan</dc:creator>
    </item>
    <item>
      <title>Adaptive Network Selection for Latency-Aware V2X Systems under Varying Network and Vehicle Densities</title>
      <link>https://arxiv.org/abs/2508.14471</link>
      <description>arXiv:2508.14471v1 Announce Type: new 
Abstract: This paper presents ANS-V2X, an Adaptive Network Selection framework tailored for latency-aware V2X systems operating under varying vehicle densities and heterogeneous network conditions. Modern vehicular environments demand low-latency and high-throughput communication, yet real-time network selection is hindered by diverse application requirements and the coexistence of multiple Radio Access Technologies (RATs) such as 4G, 5G, and ad hoc links. ANS-V2X employs a heuristic-driven approach to assign vehicles to networks by considering application sensitivity, latency, computational load, and directionality constraints. The framework is benchmarked against a Mixed-Integer Linear Programming (MILP) formulation for optimal solutions and a Q-learning-based method representing reinforcement learning. Simulation results demonstrate that ANS-V2X achieves near-optimal performance, typically within 5 to 10% of the utility achieved by MILP-V2X, while reducing execution time by more than 85%. Although MILP-V2X offers globally optimal results, its computation time often exceeds 100 milliseconds, making it unsuitable for real-time applications. The Q-learning-based method is more adaptable but requires extensive training and converges slowly in dynamic scenarios. In contrast, ANS-V2X completes decisions in under 15 milliseconds and consistently delivers lower latency than both alternatives. This confirms its suitability for real-time, edge-level deployment in latency-critical V2X systems</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14471v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammad Z. Haq, Nadia N. Qadri, Omer Chughtai, Sadiq A. Ahmad, Waqas Khalid, Heejung Yu</dc:creator>
    </item>
    <item>
      <title>Multi-Tier UAV Edge Computing for Low Altitude Networks Towards Long-Term Energy Stability</title>
      <link>https://arxiv.org/abs/2508.14601</link>
      <description>arXiv:2508.14601v1 Announce Type: new 
Abstract: This paper presents a novel multi-tier UAV-assisted edge computing system designed for low-altitude networks. The system comprises vehicle users, lightweight Low-Tier UAVs (L-UAVs), and High-Tier UAV (H-UAV). L-UAVs function as small-scale edge servers positioned closer to vehicle users, while the H-UAV, equipped with more powerful server and larger-capacity battery, serves as mobile backup server to address the limitations in endurance and computing resources of L-UAVs. The primary objective is to minimize task execution delays while ensuring long-term energy stability for L-UAVs. To address this challenge, the problem is first decoupled into a series of deterministic problems for each time slot using Lyapunov optimization. The priorities of task delay and energy consumption for L-UAVs are adaptively adjusted based on real-time energy status. The optimization tasks include assignment of tasks, allocation of computing resources, and trajectory planning for both L-UAVs and H-UAV. Simulation results demonstrate that the proposed approach achieves a reduction of at least 26% in transmission energy for L-UAVs and exhibits superior energy stability compared to existing benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14601v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yufei Ye, Shijian Gao, Xinhu Zheng, Liuqing Yang</dc:creator>
    </item>
    <item>
      <title>Adaptive Vision-Based Coverage Optimization in Mobile Wireless Sensor Networks: A Multi-Agent Deep Reinforcement Learning Approach</title>
      <link>https://arxiv.org/abs/2508.14676</link>
      <description>arXiv:2508.14676v1 Announce Type: new 
Abstract: Traditional Wireless Sensor Networks (WSNs) typically rely on pre-analysis of the target area, network size, and sensor coverage to determine initial deployment. This often results in significant overlap to ensure continued network operation despite sensor energy depletion. With the emergence of Mobile Wireless Sensor Networks (MWSNs), issues such as sensor failure and static coverage limitations can be more effectively addressed through mobility. This paper proposes a novel deployment strategy in which mobile sensors autonomously position themselves to maximize area coverage, eliminating the need for predefined policies. A live camera system, combined with deep reinforcement learning (DRL), monitors the network by detecting sensor LED indicators and evaluating real-time coverage. Rewards based on coverage efficiency and sensor movement are computed at each learning step and shared across the network through a Multi-Agent Reinforcement Learning (MARL) framework, enabling decentralized, cooperative sensor control. Key contributions include a vision-based, low-cost coverage evaluation method; a scalable MARL-DRL framework for autonomous deployment; and a self-reconfigurable system that adjusts sensor positioning in response to energy depletion. Compared to traditional distance-based localization, the proposed method achieves a 26.5% improvement in coverage, a 32% reduction in energy consumption, and a 22% decrease in redundancy, extending network lifetime by 45%. This approach significantly enhances adaptability, energy efficiency, and robustness in MWSNs, offering a practical deployment solution within the IoT framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14676v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Parham Soltani, Mehrshad Eskandarpour, Sina Heidari, Farnaz Alizadeh, Hossein Soleimani</dc:creator>
    </item>
    <item>
      <title>Energy-Efficient Routing Algorithm for Wireless Sensor Networks: A Multi-Agent Reinforcement Learning Approach</title>
      <link>https://arxiv.org/abs/2508.14679</link>
      <description>arXiv:2508.14679v1 Announce Type: new 
Abstract: Efficient energy management is essential in Wireless Sensor Networks (WSNs) to extend network lifetime and ensure reliable data transmission. This paper presents a novel method using reinforcement learning-based cluster-head selection and a hybrid multi-hop routing algorithm, which leverages Q-learning within a multi-agent system to dynamically adapt transmission paths based on the energy distribution across sensor nodes. Each sensor node is modeled as an autonomous agent that observes local state parameters, such as residual energy, distance to sink, hop count, and hotspot proximity, and selects routing actions that maximize long-term energy efficiency. After computing the optimal paths, each sensor aggregates sensed data and forwards it through intermediate nodes to a selected transmitter node, chosen based on the highest remaining State of Charge (SoC), thereby avoiding premature node depletion. To promote efficient learning, a carefully designed reward function incentivizes balanced load distribution, hotspot avoidance, and energy-aware forwarding while maintaining signal quality. The learning process occurs either in a decentralized manner or via a cloud-based controller that offloads computation in large-scale deployments. Moreover, the RL-driven routing decisions are fused with classical graph-based methods, Minimum Energy Routing Algorithm (MERA) and Minimum Spanning Tree (MST), to optimize energy consumption and load balancing. Simulations confirm that the proposed approach significantly improves node survival rate, reduces SoC variance, and enhances network resilience, making it a scalable and adaptive solution for energy-constrained WSNs in dynamic sensor deployments and IoT applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14679v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Parham Soltani, Mehrshad Eskandarpour, Amir Ahmadizad, Hossein Soleimani</dc:creator>
    </item>
    <item>
      <title>MultiFuzz: A Dense Retrieval-based Multi-Agent System for Network Protocol Fuzzing</title>
      <link>https://arxiv.org/abs/2508.14300</link>
      <description>arXiv:2508.14300v1 Announce Type: cross 
Abstract: Traditional protocol fuzzing techniques, such as those employed by AFL-based systems, often lack effectiveness due to a limited semantic understanding of complex protocol grammars and rigid seed mutation strategies. Recent works, such as ChatAFL, have integrated Large Language Models (LLMs) to guide protocol fuzzing and address these limitations, pushing protocol fuzzers to wider exploration of the protocol state space. But ChatAFL still faces issues like unreliable output, LLM hallucinations, and assumptions of LLM knowledge about protocol specifications. This paper introduces MultiFuzz, a novel dense retrieval-based multi-agent system designed to overcome these limitations by integrating semantic-aware context retrieval, specialized agents, and structured tool-assisted reasoning. MultiFuzz utilizes agentic chunks of protocol documentation (RFC Documents) to build embeddings in a vector database for a retrieval-augmented generation (RAG) pipeline, enabling agents to generate more reliable and structured outputs, enhancing the fuzzer in mutating protocol messages with enhanced state coverage and adherence to syntactic constraints. The framework decomposes the fuzzing process into modular groups of agents that collaborate through chain-of-thought reasoning to dynamically adapt fuzzing strategies based on the retrieved contextual knowledge. Experimental evaluations on the Real-Time Streaming Protocol (RTSP) demonstrate that MultiFuzz significantly improves branch coverage and explores deeper protocol states and transitions over state-of-the-art (SOTA) fuzzers such as NSFuzz, AFLNet, and ChatAFL. By combining dense retrieval, agentic coordination, and language model reasoning, MultiFuzz establishes a new paradigm in autonomous protocol fuzzing, offering a scalable and extensible foundation for future research in intelligent agentic-based fuzzing systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14300v1</guid>
      <category>cs.CR</category>
      <category>cs.CL</category>
      <category>cs.MA</category>
      <category>cs.NI</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Youssef Maklad, Fares Wael, Ali Hamdi, Wael Elsersy, Khaled Shaban</dc:creator>
    </item>
    <item>
      <title>CoFacS -- Simulating a Complete Factory to Study the Security of Interconnected Production</title>
      <link>https://arxiv.org/abs/2508.14526</link>
      <description>arXiv:2508.14526v1 Announce Type: cross 
Abstract: While the digitization of industrial factories provides tremendous improvements for the production of goods, it also renders such systems vulnerable to serious cyber-attacks. To research, test, and validate security measures protecting industrial networks against such cyber-attacks, the security community relies on testbeds to simulate industrial systems, as utilizing live systems endangers costly components or even human life. However, existing testbeds focus on individual parts of typically complex production lines in industrial factories. Consequently, the impact of cyber-attacks on industrial networks as well as the effectiveness of countermeasures cannot be evaluated in an end-to-end manner. To address this issue and facilitate research on novel security mechanisms, we present CoFacS, the first COmplete FACtory Simulation that replicates an entire production line and affords the integration of real-life industrial applications. To showcase that CoFacS accurately captures real-world behavior, we validate it against a physical model factory widely used in security research. We show that CoFacS has a maximum deviation of 0.11% to the physical reference, which enables us to study the impact of physical attacks or network-based cyber-attacks. Moreover, we highlight how CoFacS enables security research through two cases studies surrounding attack detection and the resilience of 5G-based industrial communication against jamming.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14526v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stefan Lenz, David Schachtschneider, Simon Jonas, Liam Tirpitz, Sandra Geisler, Martin Henze</dc:creator>
    </item>
    <item>
      <title>Minimizing Task-Oriented Age of Information for Remote Monitoring with Pre-Identification</title>
      <link>https://arxiv.org/abs/2508.14575</link>
      <description>arXiv:2508.14575v1 Announce Type: cross 
Abstract: The emergence of new intelligent applications has fostered the development of a task-oriented communication paradigm, where a comprehensive, universal, and practical metric is crucial for unleashing the potential of this paradigm. To this end, we introduce an innovative metric, the Task-oriented Age of Information (TAoI), to measure whether the content of information is relevant to the system task, thereby assisting the system in efficiently completing designated tasks. We apply TAoI to a wireless monitoring system tasked with identifying targets and transmitting their images for subsequent analysis. To minimize TAoI and determine the optimal transmission policy, we formulate the dynamic transmission problem as a Semi-Markov Decision Process (SMDP) and transform it into an equivalent Markov Decision Process (MDP). Our analysis demonstrates that the optimal policy is threshold-based with respect to TAoI. Building on this, we propose a low-complexity relative value iteration algorithm tailored to this threshold structure to derive the optimal transmission policy. Additionally, we introduce a simpler single-threshold policy, which, despite a slight performance degradation, offers faster convergence. Comprehensive experiments and simulations validate the superior performance of our optimal transmission policy compared to two established baseline approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14575v1</guid>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuying Gan, Xijun Wang, Chao Xu, Xiang Chen</dc:creator>
    </item>
    <item>
      <title>MOHAF: A Multi-Objective Hierarchical Auction Framework for Scalable and Fair Resource Allocation in IoT Ecosystems</title>
      <link>https://arxiv.org/abs/2508.14830</link>
      <description>arXiv:2508.14830v1 Announce Type: cross 
Abstract: The rapid growth of Internet of Things (IoT) ecosystems has intensified the challenge of efficiently allocating heterogeneous resources in highly dynamic, distributed environments. Conventional centralized mechanisms and single-objective auction models, focusing solely on metrics such as cost minimization or revenue maximization, struggle to deliver balanced system performance. This paper proposes the Multi-Objective Hierarchical Auction Framework (MOHAF), a distributed resource allocation mechanism that jointly optimizes cost, Quality of Service (QoS), energy efficiency, and fairness. MOHAF integrates hierarchical clustering to reduce computational complexity with a greedy, submodular optimization strategy that guarantees a (1-1/e) approximation ratio. A dynamic pricing mechanism adapts in real time to resource utilization, enhancing market stability and allocation quality. Extensive experiments on the Google Cluster Data trace, comprising 3,553 requests and 888 resources, demonstrate MOHAF's superior allocation efficiency (0.263) compared to Greedy (0.185), First-Price (0.138), and Random (0.101) auctions, while achieving perfect fairness (Jain's index = 1.000). Ablation studies reveal the critical influence of cost and QoS components in sustaining balanced multi-objective outcomes. With near-linear scalability, theoretical guarantees, and robust empirical performance, MOHAF offers a practical and adaptable solution for large-scale IoT deployments, effectively reconciling efficiency, equity, and sustainability in distributed resource coordination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14830v1</guid>
      <category>cs.DC</category>
      <category>cs.GT</category>
      <category>cs.NE</category>
      <category>cs.NI</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kushagra Agrawal, Polat Goktas, Anjan Bandopadhyay, Debolina Ghosh, Junali Jasmine Jena, Mahendra Kumar Gourisaria</dc:creator>
    </item>
    <item>
      <title>Benchmarking Vector, Graph and Hybrid Retrieval Augmented Generation (RAG) Pipelines for Open Radio Access Networks (ORAN)</title>
      <link>https://arxiv.org/abs/2507.03608</link>
      <description>arXiv:2507.03608v2 Announce Type: replace-cross 
Abstract: Generative AI (GenAI) is expected to play a pivotal role in enabling autonomous optimization in future wireless networks. Within the ORAN architecture, Large Language Models (LLMs) can be specialized to generate xApps and rApps by leveraging specifications and API definitions from the RAN Intelligent Controller (RIC) platform. However, fine-tuning base LLMs for telecom-specific tasks remains expensive and resource-intensive. Retrieval-Augmented Generation (RAG) offers a practical alternative through in-context learning, enabling domain adaptation without full retraining. While traditional RAG systems rely on vector-based retrieval, emerging variants such as GraphRAG and Hybrid GraphRAG incorporate knowledge graphs or dual retrieval strategies to support multi-hop reasoning and improve factual grounding. Despite their promise, these methods lack systematic, metric-driven evaluations, particularly in high-stakes domains such as ORAN. In this study, we conduct a comparative evaluation of Vector RAG, GraphRAG, and Hybrid GraphRAG using ORAN specifications. We assess performance across varying question complexities using established generation metrics: faithfulness, answer relevance, context relevance, and factual correctness. Results show that both GraphRAG and Hybrid GraphRAG outperform traditional RAG. Hybrid GraphRAG improves factual correctness by 8%, while GraphRAG improves context relevance by 11%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03608v2</guid>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <category>cs.NI</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarat Ahmad, Zeinab Nezami, Maryam Hafeez, Syed Ali Raza Zaidi</dc:creator>
    </item>
  </channel>
</rss>
