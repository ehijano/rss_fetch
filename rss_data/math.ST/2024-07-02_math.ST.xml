<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 02 Jul 2024 04:00:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 02 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Exact mean and covariance formulas after diagonal transformations of a multivariate normal</title>
      <link>https://arxiv.org/abs/2407.00240</link>
      <description>arXiv:2407.00240v1 Announce Type: new 
Abstract: Consider $\boldsymbol X \sim \mathcal{N}(\boldsymbol 0, \boldsymbol \Sigma)$ and $\boldsymbol Y = (f_1(X_1), f_2(X_2),\dots, f_d(X_d))$. We call this a diagonal transformation of a multivariate normal. In this paper we compute exactly the mean vector and covariance matrix of the random vector $\boldsymbol Y.$ This is done two different ways: One approach uses a series expansion for the function $f_i$ and the other a transform method. We compute several examples, show how the covariance entries can be estimated, and compare the theoretical results with numerical ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00240v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rebecca Morrison, Estelle Basor</dc:creator>
    </item>
    <item>
      <title>Geometric and Harmonic Aging Intensity function and a Reliability Perspective</title>
      <link>https://arxiv.org/abs/2407.00712</link>
      <description>arXiv:2407.00712v1 Announce Type: new 
Abstract: In this paper, we introduce some new notions of aging based on geometric, harmonic means of failure rate and aging intensity function. We define a generalized version of aging functions called specific interval-average geometric hazard rate, specific interval-average harmonic hazard rate. We focus on some characterization results and their inter-relationships among the resulting non-parametric classes of distributions. Monotonic nature of so defined aging classes are exhibited by some well known probability distributions. Probabilistic orders based on these functions are taken up for further study. The work is illustrated through case studies and a simulated data having applications in reliability/survival analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00712v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subarna Bhattacharjee, Ananda Sen, Sabana Anwar, Aninda Kumar Nanda</dc:creator>
    </item>
    <item>
      <title>Estimation for the damping factor of the driving process of an SPDE in two space dimensions</title>
      <link>https://arxiv.org/abs/2407.00953</link>
      <description>arXiv:2407.00953v1 Announce Type: new 
Abstract: We study parametric estimation for a second order linear parabolic stochastic partial differential equation (SPDE) in two space dimensions driven by a $Q$-Wiener process based on high frequency spatio-temporal data. We give an estimator of the damping parameter of the $Q$-Wiener process of the SPDE based on quadratic variations with temporal and spatial increments. We also provide simulation results of the proposed estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00953v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yozo Tonaki, Yusuke Kaino, Masayuki Uchida</dc:creator>
    </item>
    <item>
      <title>QBIC of SEM for diffusion processes from discrete observations</title>
      <link>https://arxiv.org/abs/2407.01040</link>
      <description>arXiv:2407.01040v1 Announce Type: new 
Abstract: We deal with a model selection problem for structural equation modeling (SEM) with latent variables for diffusion processes. Based on the asymptotic expansion of the marginal quasi-log likelihood, we propose two types of quasi-Bayesian information criteria of the SEM. It is shown that the information criteria have model selection consistency. Furthermore, we examine the finite-sample performance of the proposed information criteria by numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01040v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shogo Kusano, Masayuki Uchida</dc:creator>
    </item>
    <item>
      <title>Detecting and Identifying Selection Structure in Sequential Data</title>
      <link>https://arxiv.org/abs/2407.00529</link>
      <description>arXiv:2407.00529v1 Announce Type: cross 
Abstract: We argue that the selective inclusion of data points based on latent objectives is common in practical situations, such as music sequences. Since this selection process often distorts statistical analysis, previous work primarily views it as a bias to be corrected and proposes various methods to mitigate its effect. However, while controlling this bias is crucial, selection also offers an opportunity to provide a deeper insight into the hidden generation process, as it is a fundamental mechanism underlying what we observe. In particular, overlooking selection in sequential data can lead to an incomplete or overcomplicated inductive bias in modeling, such as assuming a universal autoregressive structure for all dependencies. Therefore, rather than merely viewing it as a bias, we explore the causal structure of selection in sequential data to delve deeper into the complete causal process. Specifically, we show that selection structure is identifiable without any parametric assumptions or interventional experiments. Moreover, even in cases where selection variables coexist with latent confounders, we still establish the nonparametric identifiability under appropriate structural conditions. Meanwhile, we also propose a provably correct algorithm to detect and identify selection structures as well as other types of dependencies. The framework has been validated empirically on both synthetic data and real-world music.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00529v1</guid>
      <category>cs.LG</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yujia Zheng, Zeyu Tang, Yiwen Qiu, Bernhard Sch\"olkopf, Kun Zhang</dc:creator>
    </item>
    <item>
      <title>Proper Scoring Rules for Multivariate Probabilistic Forecasts based on Aggregation and Transformation</title>
      <link>https://arxiv.org/abs/2407.00650</link>
      <description>arXiv:2407.00650v1 Announce Type: cross 
Abstract: Proper scoring rules are an essential tool to assess the predictive performance of probabilistic forecasts. However, propriety alone does not ensure an informative characterization of predictive performance and it is recommended to compare forecasts using multiple scoring rules. With that in mind, interpretable scoring rules providing complementary information are necessary. We formalize a framework based on aggregation and transformation to build interpretable multivariate proper scoring rules. Aggregation-and-transformation-based scoring rules are able to target specific features of the probabilistic forecasts; which improves the characterization of the predictive performance. This framework is illustrated through examples taken from the literature and studied using numerical experiments showcasing its benefits. In particular, it is shown that it can help bridge the gap between proper scoring rules and spatial verification tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00650v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Romain Pic, Cl\'ement Dombry, Philippe Naveau, Maxime Taillardat</dc:creator>
    </item>
    <item>
      <title>Neural Conditional Probability for Inference</title>
      <link>https://arxiv.org/abs/2407.01171</link>
      <description>arXiv:2407.01171v1 Announce Type: cross 
Abstract: We introduce NCP (Neural Conditional Probability), a novel operator-theoretic approach for learning conditional distributions with a particular focus on inference tasks. NCP can be used to build conditional confidence regions and extract important statistics like conditional quantiles, mean, and covariance. It offers streamlined learning through a single unconditional training phase, facilitating efficient inference without the need for retraining even when conditioning changes. By tapping into the powerful approximation capabilities of neural networks, our method efficiently handles a wide variety of complex probability distributions, effectively dealing with nonlinear relationships between input and output variables. Theoretical guarantees ensure both optimization consistency and statistical accuracy of the NCP method. Our experiments show that our approach matches or beats leading methods using a simple Multi-Layer Perceptron (MLP) with two hidden layers and GELU activations. This demonstrates that a minimalistic architecture with a theoretically grounded loss function can achieve competitive results without sacrificing performance, even in the face of more complex architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01171v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vladimir R. Kostic, Karim Lounici, Gregoire Pacreau, Pietro Novelli, Giacomo Turri, Massimiliano Pontil</dc:creator>
    </item>
    <item>
      <title>Linear parametric model checks for functional time series</title>
      <link>https://arxiv.org/abs/2303.09644</link>
      <description>arXiv:2303.09644v4 Announce Type: replace 
Abstract: The presented methodology for testing the goodness-of-fit of an Autoregressive Hilbertian model (ARH(1) model) provides an infinite-dimensional formulation of the approach proposed in Koul and Stute (1999), based on empirical process marked by residuals. Applying a central and functional central limit result for Hilbert-valued martingale difference sequences, the asymptotic behavior of the formulated H-valued empirical process, also indexed by H, is obtained under the null hypothesis. The limiting process is H-valued generalized (i.e., indexed by H) Wiener process, leading to an asymptotically distribution free test. Consistency of the test is also proved. The case of misspecified autocorrelation operator of the ARH(1) process is addressed. The asymptotic equivalence in probability, uniformly in the norm of H, of the empirical processes formulated under known and unknown autocorrelation operator is obtained. Beyond the Euclidean setting, this approach allows to implement goodness of fit testing in the context of manifold and spherical functional autoregressive processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.09644v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>W. Gonz\'alez-Manteiga, M. D. Ruiz-Medina, M. Febrero-Bande</dc:creator>
    </item>
    <item>
      <title>Entrywise Inference for Missing Panel Data: A Simple and Instance-Optimal Approach</title>
      <link>https://arxiv.org/abs/2401.13665</link>
      <description>arXiv:2401.13665v2 Announce Type: replace 
Abstract: Longitudinal or panel data can be represented as a matrix with rows indexed by units and columns indexed by time. We consider inferential questions associated with the missing data version of panel data induced by staggered adoption. We propose a computationally efficient procedure for estimation, involving only simple matrix algebra and singular value decomposition, and prove non-asymptotic and high-probability bounds on its error in estimating each missing entry. By controlling proximity to a suitably scaled Gaussian variable, we develop and analyze a data-driven procedure for constructing entrywise confidence intervals with pre-specified coverage. Despite its simplicity, our procedure turns out to be instance-optimal: we prove that the width of our confidence intervals match a non-asymptotic instance-wise lower bound derived via a Bayesian Cram\'{e}r-Rao argument. We illustrate the sharpness of our theoretical characterization on a variety of numerical examples. Our analysis is based on a general inferential toolbox for SVD-based algorithm applied to the matrix denoising model, which might be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.13665v2</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuling Yan, Martin J. Wainwright</dc:creator>
    </item>
    <item>
      <title>Beyond boundaries: Gary Lorden's groundbreaking contributions to sequential analysis</title>
      <link>https://arxiv.org/abs/2403.18782</link>
      <description>arXiv:2403.18782v2 Announce Type: replace 
Abstract: Gary Lorden provided several fundamental and novel insights into sequential hypothesis testing and changepoint detection. In this article, we provide an overview of Lorden's contributions in the context of existing results in those areas, and some extensions made possible by Lorden's work. We also mention some of Lorden's significant consulting work, including as an expert witness and for NASA, the entertainment industry, and Major League Baseball.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18782v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jay Bartroff, Alexander G. Tartakovsky</dc:creator>
    </item>
    <item>
      <title>Causality Pursuit from Heterogeneous Environments via Neural Adversarial Invariance Learning</title>
      <link>https://arxiv.org/abs/2405.04715</link>
      <description>arXiv:2405.04715v2 Announce Type: replace 
Abstract: Pursuing causality from data is a fundamental problem in scientific discovery, treatment intervention, and transfer learning. This paper introduces a novel algorithmic method for addressing nonparametric invariance and causality learning in regression models across multiple environments, where the joint distribution of response variables and covariates varies, but the conditional expectations of outcome given an unknown set of quasi-causal variables are invariant. The challenge of finding such an unknown set of quasi-causal or invariant variables is compounded by the presence of endogenous variables that have heterogeneous effects across different environments, including even one of them in the regression would make the estimation inconsistent. The proposed Focused Adversial Invariant Regularization (FAIR) framework utilizes an innovative minimax optimization approach that breaks down the barriers, driving regression models toward prediction-invariant solutions through adversarial testing. Leveraging the representation power of neural networks, FAIR neural networks (FAIR-NN) are introduced for causality pursuit. It is shown that FAIR-NN can find the invariant variables and quasi-causal variables under a minimal identification condition and that the resulting procedure is adaptive to low-dimensional composition structures in a non-asymptotic analysis. Under a structural causal model, variables identified by FAIR-NN represent pragmatic causality and provably align with exact causal mechanisms under conditions of sufficient heterogeneity. Computationally, FAIR-NN employs a novel Gumbel approximation with decreased temperature and stochastic gradient descent ascent algorithm. The procedures are convincingly demonstrated using simulated and real-data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04715v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yihong Gu, Cong Fang, Peter B\"uhlmann, Jianqing Fan</dc:creator>
    </item>
    <item>
      <title>Stationary Kernels and Gaussian Processes on Lie Groups and their Homogeneous Spaces II: non-compact symmetric spaces</title>
      <link>https://arxiv.org/abs/2301.13088</link>
      <description>arXiv:2301.13088v3 Announce Type: replace-cross 
Abstract: Gaussian processes are arguably the most important class of spatiotemporal models within machine learning. They encode prior information about the modeled function and can be used for exact or approximate Bayesian learning. In many applications, particularly in physical sciences and engineering, but also in areas such as geostatistics and neuroscience, invariance to symmetries is one of the most fundamental forms of prior information one can consider. The invariance of a Gaussian process' covariance to such symmetries gives rise to the most natural generalization of the concept of stationarity to such spaces. In this work, we develop constructive and practical techniques for building stationary Gaussian processes on a very large class of non-Euclidean spaces arising in the context of symmetries. Our techniques make it possible to (i) calculate covariance kernels and (ii) sample from prior and posterior Gaussian processes defined on such spaces, both in a practical manner. This work is split into two parts, each involving different technical considerations: part I studies compact spaces, while part II studies non-compact spaces possessing certain structure. Our contributions make the non-Euclidean Gaussian process models we study compatible with well-understood computational techniques available in standard Gaussian process software packages, thereby making them accessible to practitioners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.13088v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Iskander Azangulov, Andrei Smolensky, Alexander Terenin, Viacheslav Borovitskiy</dc:creator>
    </item>
    <item>
      <title>A Regression-Based Approach to the CO2 Airborne Fraction: Enhancing Statistical Precision and Tackling Zero Emissions</title>
      <link>https://arxiv.org/abs/2311.01053</link>
      <description>arXiv:2311.01053v3 Announce Type: replace-cross 
Abstract: The global fraction of anthropogenically emitted carbon dioxide (CO$_2$) that stays in the atmosphere, the CO$_2$ airborne fraction, has been fluctuating around a constant value over the period 1959 to 2022. The consensus estimate of the airborne fraction is around $44\%$; the remaining $56\%$ is absorbed by the oceanic and terrestrials biospheres. In this study, we show that the conventional estimator of the airborne fraction, based on a ratio of changes in atmospheric CO$_2$ concentrations and CO$_2$ emissions, suffers from a number of statistical deficiencies, such as non-existence of moments and a non-Gaussian limiting distribution. We propose an alternative regression-based estimator of the airborne fraction that does not suffer from these deficiencies. We show that the regression-based estimator has a Gaussian limiting distribution and reduces estimation uncertainty substantially. Our empirical analysis leads to an estimate of the airborne fraction over 1959--2022 of $47.0\%$ ($\pm 1.1\%$; $1 \sigma$), implying a higher, and better constrained, estimate than the current consensus. Using climate model output, we show that a regression-based approach provides sensible estimates of the airborne fraction, also in future scenarios where emissions are at or near zero.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.01053v3</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mikkel Bennedsen, Eric Hillebrand, Siem Jan Koopman</dc:creator>
    </item>
    <item>
      <title>Elicitability and identifiability of tail risk measures</title>
      <link>https://arxiv.org/abs/2404.14136</link>
      <description>arXiv:2404.14136v2 Announce Type: replace-cross 
Abstract: Tail risk measures are fully determined by the distribution of the underlying loss beyond its quantile at a certain level, with Value-at-Risk and Expected Shortfall being prime examples. They are induced by law-based risk measures, called their generators, evaluated on the tail distribution. This paper establishes joint identifiability and elicitability results of tail risk measures together with the corresponding quantile, provided that their generators are identifiable and elicitable, respectively. As an example, we establish the joint identifiability and elicitability of the tail expectile together with the quantile. The corresponding consistent scores constitute a novel class of weighted scores, nesting the known class of scores of Fissler and Ziegel for the Expected Shortfall together with the quantile. For statistical purposes, our results pave the way to easier model fitting for tail risk measures via regression and the generalized method of moments, but also model comparison and model validation in terms of established backtesting procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14136v2</guid>
      <category>q-fin.ST</category>
      <category>math.ST</category>
      <category>q-fin.RM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tobias Fissler, Fangda Liu, Ruodu Wang, Linxiao Wei</dc:creator>
    </item>
    <item>
      <title>Empirical Bayes for Dynamic Bayesian Networks Using Generalized Variational Inference</title>
      <link>https://arxiv.org/abs/2406.17831</link>
      <description>arXiv:2406.17831v2 Announce Type: replace-cross 
Abstract: In this work, we demonstrate the Empirical Bayes approach to learning a Dynamic Bayesian Network. By starting with several point estimates of structure and weights, we can use a data-driven prior to subsequently obtain a model to quantify uncertainty. This approach uses a recent development of Generalized Variational Inference, and indicates the potential of sampling the uncertainty of a mixture of DAG structures as well as a parameter posterior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17831v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vyacheslav Kungurtsev,  Apaar, Aarya Khandelwal, Parth Sandeep Rastogi, Bapi Chatterjee, Jakub Mare\v{c}ek</dc:creator>
    </item>
  </channel>
</rss>
