<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 17 Sep 2025 01:40:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Outlier-Resistant Heterogeneous Treatment Effect Estimation in HDLSS Settings via GAT--CVAE Framework</title>
      <link>https://arxiv.org/abs/2509.10787</link>
      <description>arXiv:2509.10787v1 Announce Type: new 
Abstract: We introduce a robust framework for heterogeneous treatment effect (HTE) estimation tailored to high-dimensional low sample size (HDLSS) settings. By combining Graph Attention Networks (GAT) to capture structural dependencies among confounders with a Conditional Variational Autoencoder (CVAE) for latent representation learning, our method expands the sample space and performs clustering that integrates even outlier sets into coherent subgroups. Clusterwise causal effects are then estimated using a doubly robust outlier-resistant estimator, yielding stable and generalizable results. Simulations and real-world applications confirm superior performance compared with existing HTE methods, highlighting the framework's potential for precision medicine and policy evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10787v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Byeonghee Lee, Joonsung Kang</dc:creator>
    </item>
    <item>
      <title>Closed-form parameter estimation for the bivariate gamma distribution: New approaches</title>
      <link>https://arxiv.org/abs/2509.10794</link>
      <description>arXiv:2509.10794v1 Announce Type: new 
Abstract: We propose new closed-form estimators for the parameters of McKay's bivariate gamma distribution by exploiting monotone transformations of the likelihood equations. As a special case, our framework recovers the estimators recently introduced by Zhao et al. (2022) [Zhao, J., Jang, Y.-H., and Kim, H. (2022). Closed-form and bias-corrected estimators for the bivariate gamma distribution. Journal of Multivariate Analysis, 191:105009]. Theoretical properties, including strong consistency and asymptotic normality, are established. We further introduce a second family of closed-form estimators that is explicitly built from the stochastic relationship between gamma random variables. Our second approach encompasses the estimators of Nawa and Nadarajah (2023) [Nawa, V. M. and Nadarajah, S. (2023). New closed form estimators for a bivariate gamma distribution. Statistics, 57(1):150-160]. Monte Carlo experiments are conducted to assess finite-sample performance, showing that the new estimators perform comparably to maximum likelihood estimators while avoiding iterative optimization, and improve upon the existing closed-form approach by Zhao et al. (2022) and Nawa and Nadarajah (2023). A real hydrological data set is analyzed to illustrate the proposed approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10794v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Vila, Helton Saulo</dc:creator>
    </item>
    <item>
      <title>Two-Stage Least Squares Instrumental Variable Estimation for Semiparametric Accelerated Failure Time Models with Right-Censored Data</title>
      <link>https://arxiv.org/abs/2509.10905</link>
      <description>arXiv:2509.10905v1 Announce Type: new 
Abstract: Instrumental variable (IV) analysis is widely used in fields such as economics and epidemiology to address unobserved confounding and measurement error when estimating the causal effects of intermediate covariates on outcomes. However, extending the commonly used two-stage least squares (TSLS) approach to survival settings is nontrivial due to censoring. This paper introduces a novel extension of TSLS to the semiparametric accelerated failure time (AFT) model with right-censored data, supported by rigorous theoretical justification. Specifically, we propose an iterative reweighted generalized estimating equation (GEE) approach that incorporates Leurgans' synthetic variable method, establish the asymptotic properties of the resulting estimator, and derive a consistent variance estimator, enabling valid causal inference. Simulation studies are conducted to evaluate the finite-sample performance of the proposed method across different scenarios. The results show that it outperforms the naive unweighted GEE method, a parametric IV approach, and a one-stage estimator without IV. The proposed method is also highly scalable to large datasets, achieving a 300- to 1500-fold speedup relative to a Bayesian parametric IV approach in both simulations and the real-data example. We further illustrate the utility of the proposed method through a real-data application using the UK Biobank data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10905v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zian Zhuang, Hua Zhou, Jin Zhou, Gang Li</dc:creator>
    </item>
    <item>
      <title>A Tutorial on Conducting Mediation Analysis with Exposure Mixtures</title>
      <link>https://arxiv.org/abs/2509.10916</link>
      <description>arXiv:2509.10916v1 Announce Type: new 
Abstract: Causal mediation analysis is a powerful tool in environmental health research, allowing researchers to uncover the pathways through which exposures influence health outcomes. While traditional mediation methods have been widely applied to individual exposures, real-world scenarios often involve complex mixtures. Such mixtures introduce unique methodological challenges, including multicollinearity, sparsity of active exposures, and potential nonlinear and interactive effects. This paper provides an overview of several commonly used approaches for mediation analysis under exposure mixture settings with clear strategies and code for implementation. The methods include: single exposure mediation analysis (SE-MA), principal component-based mediation analysis, environmental risk score-based mediation analysis, and Bayesian kernel machine regression causal mediation analysis. While SE-MA serves as a baseline that analyzes each exposure individually, the other methods are designed to address the correlation and complexity inherent in exposure mixtures. For each method, we aim to clarify the target estimand and the assumptions that each method is making to render a causal interpretation of the estimates obtained. We conduct a simulation study to systematically evaluate the operating characteristics of these four methods to estimate global indirect effects and to identify individual exposures contributing to the global mediation under varying sample sizes, effect sizes, and exposure-mediator-outcome structures. We also illustrate their real-world applicability by examining data from the PROTECT birth cohort, specifically analyzing the relationship between prenatal exposure to phthalate mixtures and neonatal head circumference Z-score, with leukotriene E4 as a mediator. This example offers practical guidance for conducting mediation analysis in complex environmental contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10916v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiran Wang, Yi-Ting Lin, Sean McGrath, John D. Meeker, Sung Kyun Park, Joshua L. Warren, Bhramar Mukherjee</dc:creator>
    </item>
    <item>
      <title>A Latent Factor Panel Approach to Spatiotemporal Causal Inference</title>
      <link>https://arxiv.org/abs/2509.10974</link>
      <description>arXiv:2509.10974v1 Announce Type: new 
Abstract: Unmeasured confounding can severely bias causal effect estimates from spatiotemporal observational data, especially when the confounders do not vary smoothly in time and space. In this work, we develop a method for addressing unmeasured confounding in spatiotemporal contexts by building on models from the panel data literature and methods in multivariate causal inference. Our method is based on a factor confounding assumption, which posits that effects of unmeasured confounders on exposures and outcomes can be captured by a shared latent factor model. Factor confounding is sufficient to partially identify causal effects, even when there is interference between units. Additional assumptions that limit the degree of spatiotemporal interference, reasonable in most applications, are sufficient to point identify the effects. Simulation studies demonstrate that the proposed approach can substantially reduce omitted variable bias relative to other spatial smoothing and panel data baselines. We illustrate our method in a case study of the effect of prenatal PM2.5 exposure on birth weight in California.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10974v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaxi Wu, Alexander Franks</dc:creator>
    </item>
    <item>
      <title>Varying-Coefficient Fr\'echet Regression</title>
      <link>https://arxiv.org/abs/2509.11061</link>
      <description>arXiv:2509.11061v1 Announce Type: new 
Abstract: As a growing number of problems involve variables that are random objects, the development of models for such data has become increasingly important. This paper introduces a novel varying-coefficient Fr\'echet regression model that extends the classical varying-coefficient framework to accommodate random objects as responses. The proposed model provides a unified methodology for analyzing both Euclidean and non-Euclidean response variables. We develop a comprehensive estimation procedure that accommodates diverse predictor settings. Specifically, the model allows the effect-modifier variable U to be either Euclidean or non-Euclidean, while the predictors X are assumed to be Euclidean. Tailored estimation methods are provided for each scenario. To examine the asymptotic properties of the estimators, we introduce a smoothed version of the model and establish convergence rates through separate theoretical analyses of the bias and stochastic terms. The effectiveness and practical utility of the proposed methodology are demonstrated through extensive simulation studies and a real-data application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11061v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanzhao Wang, Jianqiang Zhang, Wangli Xu</dc:creator>
    </item>
    <item>
      <title>KOO Method-based Consistent Clustering for Group-wise Linear Regression with Graph Structure</title>
      <link>https://arxiv.org/abs/2509.11103</link>
      <description>arXiv:2509.11103v1 Announce Type: new 
Abstract: The kick-one-out (KOO) method is a variable selection method based on a model selection criterion. The method is very simple, and yet it has consistency in variable selection under a high-dimensional asymptotic framework with a specific model selection criterion. This paper proposes the join-twotogether (JTT) method, which is a clustering method based on the KOO method for group-wise linear regression with graph structure. The JTT method formulates the clustering problem as an edge selection problem for a graph and determines whether to select each edge based on the KOO method. We can employ network Lasso to perform such a clustering. However, network Lasso is somewhat cumbersome because there is no good algorithm for solving the associated optimization problem and the tuning is complicated. Therefore, by deriving a model selection criterion such that the JTT method has consistency in clustering under a high-dimensional asymptotic framework, we propose a simple yet powerful method that outperforms network Lasso.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11103v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M. Ohishi, R. Oda</dc:creator>
    </item>
    <item>
      <title>BE-BOIN: A Dose Optimization Design Accommodating Backfill and Late-Onset Toxicity</title>
      <link>https://arxiv.org/abs/2509.11333</link>
      <description>arXiv:2509.11333v1 Announce Type: new 
Abstract: The US Food and Drug Administration (FDA) launched Project Optimus and issued guidance to reform dose-finding and selection trials, shifting the paradigm from identifying the maximum tolerable dose (MTD) to determining the optimal biological dose (OBD), which optimizes the risk and benefit of treatments. The FDA's guidance emphasizes the importance of collecting sufficient toxicity and efficacy data across multiple doses and considering late-onset cumulative toxicity that often results in tolerability issues. To address these challenges, we propose the BE-BOIN (Backfill time-to-Event Bayesian Optimal INterval) design, which allows backfilling patients into safe and effective doses during dose escalation and accommodates late-onset toxicities. BE-BOIN enables the collection of additional safety and efficacy data to enhance the accuracy and reliability of OBD selection and supports real-time dose decisions for new patients. Our simulation studies show that BE-BOIN accurately identifies the MTD and OBD while significantly reducing trial duration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11333v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kai Chen, Yixuan Zhao, Kentaro Takeda, Ying Yuan</dc:creator>
    </item>
    <item>
      <title>Unified Distributed Estimation Framework for Sufficient Dimension Reduction Based on Conditional Moments</title>
      <link>https://arxiv.org/abs/2509.11455</link>
      <description>arXiv:2509.11455v1 Announce Type: new 
Abstract: Nowadays, massive datasets are typically dispersed across multiple locations, encountering dual challenges of high dimensionality and huge sample size. Therefore, it is necessary to explore sufficient dimension reduction (SDR) methods for distributed data. In this paper, we first propose an exact distributed estimation of sliced inverse regression, which substantially improves computational efficiency while obtaining identical estimation as that on the full sample. Then, we propose a unified distributed framework for general conditional-moment-based inverse regression methods. This framework allows for distinct population structure for data distributed at different locations, thus addressing the issue of heterogeneity. To assess the effectiveness of our proposed methods, we conduct simulations incorporating various data generation mechanisms, and examine scenarios where samples are homogeneous equally, heterogeneous equally, and heterogeneous unequally scattered across local nodes. Our findings highlight the versatility and applicability of the unified framework. Meanwhile, the communication cost is practically acceptable and the computation cost is greatly reduced. Sensitivity analysis verifies the robustness of the algorithm under extreme conditions where the SDR method locally fails on some nodes. A real data analysis also demonstrates the superior performance of the algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11455v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongying Li, Minyi Zhu, Yaqi Cao, Xinyi Xu</dc:creator>
    </item>
    <item>
      <title>A New Class of Mark-Specific Proportional Hazards Models for Recurrent Events: Application to Opioid Refills Among Post-Surgical Patients</title>
      <link>https://arxiv.org/abs/2509.11472</link>
      <description>arXiv:2509.11472v1 Announce Type: new 
Abstract: Prescription opioids relieve moderate-to-severe pain after surgery, but overprescription can lead to misuse and overdose. Understanding factors associated with post-surgical opioid refills is crucial for improving pain management and reducing opioid-related harms. Conventional methods often fail to account for refill size or dosage and capture patient risk dynamics. We address this gap by treating dosage as a continuously varying mark for each refill event and proposing a new class of mark-specific proportional hazards models for recurrent events. Our marginal model, developed on the gap-time scale with a dual weighting scheme, accommodates event proximity to dosage of interest while accounting for the informative number of recurrences. We establish consistency and asymptotic normality of the estimator and provide a sandwich variance estimator for robust inference. Simulations show improved finite-sample performance over competing methods. We apply the model to data from the Michigan Surgical Quality Collaborative and Michigan Automated Prescription System. Results show that high BMI, smoking, cancer, and open surgery increase hazards of high-dosage refills, while inpatient surgeries elevate refill hazards across all dosages. Black race is associated with higher hazards of low-dosage but lower hazards of high-dosage refills. These findings may inform personalized, dosage-specific pain management strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11472v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eileen Yang, Donglin Zeng, Mark Bicket, Yi Li</dc:creator>
    </item>
    <item>
      <title>Learn-As-you-GO (LAGO) Trials: Optimizing Trials for Effectiveness and Power to Prevent Failed Trials</title>
      <link>https://arxiv.org/abs/2509.11479</link>
      <description>arXiv:2509.11479v1 Announce Type: new 
Abstract: The Learn-As-you-GO (LAGO) design provides a rigorous framework for adapting the intervention package based on accumulating data while the trial is ongoing. This article improves the flexibility of the LAGO design by incorporating statistical power as an optimization criterion (power goal) in LAGO optimizations. We propose the unconditional and conditional power approaches to add a power goal. Both approaches estimate the power at the end of the LAGO trial using data from prior stages, and increase the power at the end of the LAGO trial when the original trial was underpowered. Including a power goal maintains the asymptotic properties of the estimators of the treatment effect while preserving the asymptotic level of the statistical test at the end of the trial. We illustrate the benefits of our methods through a retrospective application to the BetterBirth Study, a large-scale study of maternal-newborn care that failed to show a significant effect on its primary outcome. This analysis demonstrates how our methods could have led to more intensive interventions and potentially significant results. The LAGO design with power goal optimizations provides investigators with a powerful tool to reduce the risk of failed trials due to insufficient power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11479v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ante Bing (Department of Mathematics,Statistics, Boston University), Donna Spiegelman (Department of Biostatistics, Yale University), Judith J. Lok (Department of Mathematics,Statistics, Boston University)</dc:creator>
    </item>
    <item>
      <title>Mendelian Randomization Methods for Causal Inference: Estimands, Identification and Inference</title>
      <link>https://arxiv.org/abs/2509.11519</link>
      <description>arXiv:2509.11519v1 Announce Type: new 
Abstract: Mendelian randomization (MR) has become an essential tool for causal inference in biomedical and public health research. By using genetic variants as instrumental variables, MR helps address unmeasured confounding and reverse causation, offering a quasi-experimental framework to evaluate causal effects of modifiable exposures on health outcomes. Despite its promise, MR faces substantial methodological challenges, including invalid instruments, weak instrument bias, and design complexities across different data structures. In this tutorial review, we provide a comprehensive overview of MR methods for causal inference, emphasizing clarity of causal interpretation, study design comparisons, availability of software tools, and practical guidance for applied scientists. We organize the review around causal estimands, ensuring that analyses are anchored to well-defined causal questions. We discuss the problems of invalid and weak instruments, comparing available strategies for their detection and correction. We integrate discussions of population-based versus family-based MR designs, analyses based on individual-level versus summary-level data, and one-sample versus two-sample MR designs, highlighting their relative advantages and limitations. We also summarize recent methodological advances and software developments that extend MR to settings with many weak or invalid instruments and to modern high-dimensional omics data. Real-data applications, including UK Biobank and Alzheimer's disease proteomics studies, illustrate the use of these methods in practice. This review aims to serve as a tutorial-style reference for both methodologists and applied scientists.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11519v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minhao Yao, Anqi Wang, Xihao Li, Zhonghua Liu</dc:creator>
    </item>
    <item>
      <title>Scalable Variable Selection and Model Averaging for Latent Regression Models Using Approximate Variational Bayes</title>
      <link>https://arxiv.org/abs/2509.11751</link>
      <description>arXiv:2509.11751v1 Announce Type: new 
Abstract: We propose a fast and theoretically grounded method for Bayesian variable selection and model averaging in latent variable regression models. Our framework addresses three interrelated challenges: (i) intractable marginal likelihoods, (ii) exponentially large model spaces, and (iii) computational costs in large samples. We introduce a novel integrated likelihood approximation based on mean-field variational posterior approximations and establish its asymptotic model selection consistency under broad conditions. To reduce the computational burden, we develop an approximate variational Bayes scheme that fixes the latent regression outcomes for all models at initial estimates obtained under a baseline null model. Despite its simplicity, this approach locally and asymptotically preserves the model-selection behavior of the full variational Bayes approach to first order, at a fraction of the computational cost. Extensive numerical studies - covering probit, tobit, semi-parametric count data models and Poisson log-normal regression - demonstrate accurate inference and large speedups, often reducing runtime from days to hours with comparable accuracy. Applications to real-world data further highlight the practical benefits of the methods for Bayesian inference in large samples and under model uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11751v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gregor Zens, Mark F. J. Steel</dc:creator>
    </item>
    <item>
      <title>Covering Unknown Correlations in Bayesian Priors by Inflating Uncertainties</title>
      <link>https://arxiv.org/abs/2509.11821</link>
      <description>arXiv:2509.11821v1 Announce Type: new 
Abstract: Bayesian analyses require that all variable model parameters are given a prior probability distribution. This can pose a challenge for analyses where multiple experiments are combined if these experiments use different parametrisations for their nuisance parameters. If the parameters in the two models describe exactly the same physics, they should be 100% correlated in the prior. If the parameters describe independent physics, they should be uncorrelated. But if they describe related or overlapping physics, it is not trivial to determine what the joint prior distribution should look like. Even if the priors for each experiment are well motivated, the unknown correlations between them can have unintended consequences for the posterior probability of the parameters of interest, potentially leading to underestimated uncertainties. In this paper we show that it is possible to choose a prior parametrisation that ensures conservative posterior uncertainties for the parameters of interest under some very general assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11821v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lukas Koch</dc:creator>
    </item>
    <item>
      <title>Statistical Model Checking Beyond Means: Quantiles, CVaR, and the DKW Inequality (extended version)</title>
      <link>https://arxiv.org/abs/2509.11859</link>
      <description>arXiv:2509.11859v1 Announce Type: new 
Abstract: Statistical model checking (SMC) randomly samples probabilistic models to approximate quantities of interest with statistical error guarantees. It is traditionally used to estimate probabilities and expected rewards, i.e. means of different random variables on paths. In this paper, we develop methods using the Dvoretzky-Kiefer-Wolfowitz-Massart inequality (DKW) to extend SMC beyond means to compute quantities such as quantiles, conditional value-at-risk, and entropic risk. The DKW provides confidence bounds on the random variable's entire cumulative distribution function, a much more versatile guarantee compared to the statistical methods prevalent in SMC today. We have implemented support for computing new quantities via the DKW in the 'modes' simulator of the Modest Toolset. We highlight the implementation and its versatility on benchmarks from the quantitative verification literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11859v1</guid>
      <category>stat.ME</category>
      <category>cs.DM</category>
      <category>cs.LO</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlos E. Budde, Arnd Hartmanns, Tobias Meggendorfer, Maximilian Weininger, Patrick Wienh\"oft</dc:creator>
    </item>
    <item>
      <title>Modeling Non-Uniform Hypergraphs Using Determinantal Point Processes</title>
      <link>https://arxiv.org/abs/2509.12028</link>
      <description>arXiv:2509.12028v1 Announce Type: new 
Abstract: Most statistical models for networks focus on pairwise interactions between nodes. However, many real-world networks involve higher-order interactions among multiple nodes, such as co-authors collaborating on a paper. Hypergraphs provide a natural representation for these networks, with each hyperedge representing a set of nodes. The majority of existing hypergraph models assume uniform hyperedges (i.e., edges of the same size) or rely on diversity among nodes. In this work, we propose a new hypergraph model based on non-symmetric determinantal point processes. The proposed model naturally accommodates non-uniform hyperedges, has tractable probability mass functions, and accounts for both node similarity and diversity in hyperedges. For model estimation, we maximize the likelihood function under constraints using a computationally efficient projected adaptive gradient descent algorithm. We establish the consistency and asymptotic normality of the estimator. Simulation studies confirm the efficacy of the proposed model, and its utility is further demonstrated through edge predictions on several real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12028v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yichao Chen, Jingfei Zhang, Ji Zhu</dc:creator>
    </item>
    <item>
      <title>Least squares-based methods to bias adjustment in scalar-on-function regression model using a functional instrumental variable</title>
      <link>https://arxiv.org/abs/2509.12122</link>
      <description>arXiv:2509.12122v1 Announce Type: new 
Abstract: Instrumental variables are widely used to adjust for measurement error bias when assessing associations of health outcomes with ME prone independent variables. IV approaches addressing ME in longitudinal models are well established, but few methods exist for functional regression. We develop two methods to adjust for ME bias in scalar on function linear models. We regress a scalar outcome on an ME prone functional variable using a functional IV for model identification and propose two least squares based methods to adjust for ME bias. Our methods alleviate potential computational challenges encountered when applying classical regression calibration methods for bias adjustment in high dimensional settings and adjust for potential serial correlations across time. Simulations demonstrate faster run times, lower bias, and lower AIMSE for the proposed methods when compared to existing approaches. The proposed methods were applied to investigate the association between body mass index and wearable device-based physical activity intensity among community dwelling adults living in the United States.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12122v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiwei Chen, Ufuk Beyaztas, Caihong Qin, Heyang Ji, Gilson Honvoh, Roger S. Zoh, Lan Xue, Carmen D. Tekwe</dc:creator>
    </item>
    <item>
      <title>An Interpretable Ensemble Framework for Multi-Omics Dementia Biomarker Discovery Under HDLSS Conditions</title>
      <link>https://arxiv.org/abs/2509.10527</link>
      <description>arXiv:2509.10527v1 Announce Type: cross 
Abstract: Biomarker discovery in neurodegenerative diseases requires robust, interpretable frameworks capable of integrating high-dimensional multi-omics data under low-sample conditions. We propose a novel ensemble approach combining Graph Attention Networks (GAT), MultiOmics Variational AutoEncoder (MOVE), Elastic-net sparse regression, and Storey's False Discovery Rate (FDR). This framework is benchmarked against state-of-the-art methods including DIABLO, MOCAT, AMOGEL, and MOMLIN. We evaluate performance using both simulated multi-omics data and the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. Our method demonstrates superior predictive accuracy, feature selection precision, and biological relevance. Biomarker gene maps derived from both datasets are visualized and interpreted, offering insights into latent molecular mechanisms underlying dementia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10527v1</guid>
      <category>eess.IV</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Byeonghee Lee, Joonsung Kang</dc:creator>
    </item>
    <item>
      <title>Efficient High-Dimensional Conditional Independence Testing</title>
      <link>https://arxiv.org/abs/2509.10817</link>
      <description>arXiv:2509.10817v1 Announce Type: cross 
Abstract: This article deals with the problem of testing conditional independence between two random vectors ${\bf X}$ and ${\bf Y}$ given a confounding random vector ${\bf Z}$. Several authors have considered this problem for multivariate data. However, most of the existing tests has poor performance against local contiguous alternatives beyond linear dependency. In this article, an Energy distance type measure of conditional dependence is developed, borrowing ideas from the model-X framework. A consistent estimator of the measure is proposed, and its theoretical properties are studied under general assumptions. Using the estimator as a test statistic a test of conditional independence is developed, and a suitable resampling algorithm is designed to calibrate the test. The test turns out to be not only large sample consistent, but also Pitman efficient against local contiguous alternatives, and is provably consistent when the dimension of the data diverges to infinity with the sample size. Several empirical studies are conducted to demonstrate the efficacy of the test against state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10817v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bilol Banerjee</dc:creator>
    </item>
    <item>
      <title>Online simplex-structured matrix factorization</title>
      <link>https://arxiv.org/abs/2509.10857</link>
      <description>arXiv:2509.10857v1 Announce Type: cross 
Abstract: Simplex-structured matrix factorization (SSMF) is a common task encountered in signal processing and machine learning. Minimum-volume constrained unmixing (MVCU) algorithms are among the most widely used methods to perform this task. While MVCU algorithms generally perform well in an offline setting, their direct application to online scenarios suffers from scalability limitations due to memory and computational demands. To overcome these limitations, this paper proposes an approach which can build upon any off-the-shelf MVCU algorithm to operate sequentially, i.e., to handle one observation at a time. The key idea of the proposed method consists in updating the solution of MVCU only when necessary, guided by an online check of the corresponding optimization problem constraints. It only stores and processes observations identified as informative with respect to the geometrical constraints underlying SSMF. We demonstrate the effectiveness of the approach when analyzing synthetic and real datasets, showing that it achieves estimation accuracy comparable to the offline MVCU method upon which it relies, while significantly reducing the computational cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10857v1</guid>
      <category>eess.SP</category>
      <category>physics.chem-ph</category>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hugues Kouakou, Jos\'e Henrique de Morais Goulart, Raffaele Vitale, Thomas Oberlin, David Rousseau, Cyril Ruckebusch, Nicolas Dobigeon</dc:creator>
    </item>
    <item>
      <title>Testing for LLM response differences: the case of a composite null consisting of semantically irrelevant query perturbations</title>
      <link>https://arxiv.org/abs/2509.10963</link>
      <description>arXiv:2509.10963v1 Announce Type: cross 
Abstract: Given an input query, generative models such as large language models produce a random response drawn from a response distribution. Given two input queries, it is natural to ask if their response distributions are the same. While traditional statistical hypothesis testing is designed to address this question, the response distribution induced by an input query is often sensitive to semantically irrelevant perturbations to the query, so much so that a traditional test of equality might indicate that two semantically equivalent queries induce statistically different response distributions. As a result, the outcome of the statistical test may not align with the user's requirements. In this paper, we address this misalignment by incorporating into the testing procedure consideration of a collection of semantically similar queries. In our setting, the mapping from the collection of user-defined semantically similar queries to the corresponding collection of response distributions is not known a priori and must be estimated, with a fixed budget. Although the problem we address is quite general, we focus our analysis on the setting where the responses are binary, show that the proposed test is asymptotically valid and consistent, and discuss important practical considerations with respect to power and computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10963v1</guid>
      <category>math.ST</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aranyak Acharyya, Carey E. Priebe, Hayden S. Helm</dc:creator>
    </item>
    <item>
      <title>Large-Scale Curve Time Series with Common Stochastic Trends</title>
      <link>https://arxiv.org/abs/2509.11060</link>
      <description>arXiv:2509.11060v1 Announce Type: cross 
Abstract: This paper studies high-dimensional curve time series with common stochastic trends. A dual functional factor model structure is adopted with a high-dimensional factor model for the observed curve time series and a low-dimensional factor model for the latent curves with common trends. A functional PCA technique is applied to estimate the common stochastic trends and functional factor loadings. Under some regularity conditions we derive the mean square convergence and limit distribution theory for the developed estimates, allowing the dimension and sample size to jointly diverge to infinity. We propose an easy-to-implement criterion to consistently select the number of common stochastic trends and further discuss model estimation when the nonstationary factors are cointegrated. Extensive Monte-Carlo simulations and two empirical applications to large-scale temperature curves in Australia and log-price curves of S&amp;P 500 stocks are conducted, showing finite-sample performance and providing practical implementations of the new methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11060v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Degui Li, Yu-Ning Li, Peter C. B. Phillips</dc:creator>
    </item>
    <item>
      <title>Contrastive Network Representation Learning</title>
      <link>https://arxiv.org/abs/2509.11316</link>
      <description>arXiv:2509.11316v1 Announce Type: cross 
Abstract: Network representation learning seeks to embed networks into a low-dimensional space while preserving the structural and semantic properties, thereby facilitating downstream tasks such as classification, trait prediction, edge identification, and community detection. Motivated by challenges in brain connectivity data analysis that is characterized by subject-specific, high-dimensional, and sparse networks that lack node or edge covariates, we propose a novel contrastive learning-based statistical approach for network edge embedding, which we name as Adaptive Contrastive Edge Representation Learning (ACERL). It builds on two key components: contrastive learning of augmented network pairs, and a data-driven adaptive random masking mechanism. We establish the non-asymptotic error bounds, and show that our method achieves the minimax optimal convergence rate for edge representation learning. We further demonstrate the applicability of the learned representation in multiple downstream tasks, including network classification, important edge detection, and community detection, and establish the corresponding theoretical guarantees. We validate our method through both synthetic data and real brain connectivities studies, and show its competitive performance compared to the baseline method of sparse principal components analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11316v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zihan Dong, Xin Zhou, Ryumei Nakada, Lexin Li, Linjun Zhang</dc:creator>
    </item>
    <item>
      <title>The Honest Truth About Causal Trees: Accuracy Limits for Heterogeneous Treatment Effect Estimation</title>
      <link>https://arxiv.org/abs/2509.11381</link>
      <description>arXiv:2509.11381v1 Announce Type: cross 
Abstract: Recursive decision trees have emerged as a leading methodology for heterogeneous causal treatment effect estimation and inference in experimental and observational settings. These procedures are fitted using the celebrated CART (Classification And Regression Tree) algorithm [Breiman et al., 1984], or custom variants thereof, and hence are believed to be "adaptive" to high-dimensional data, sparsity, or other specific features of the underlying data generating process. Athey and Imbens [2016] proposed several "honest" causal decision tree estimators, which have become the standard in both academia and industry. We study their estimators, and variants thereof, and establish lower bounds on their estimation error. We demonstrate that these popular heterogeneous treatment effect estimators cannot achieve a polynomial-in-$n$ convergence rate under basic conditions, where $n$ denotes the sample size. Contrary to common belief, honesty does not resolve these limitations and at best delivers negligible logarithmic improvements in sample size or dimension. As a result, these commonly used estimators can exhibit poor performance in practice, and even be inconsistent in some settings. Our theoretical insights are empirically validated through simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11381v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Jason M. Klusowski, Ruiqi Rae Yu</dc:creator>
    </item>
    <item>
      <title>Association between Air Pollutants and Hospitalizations for Cardiovascular Diseases: Time-Series Analysis in S\~ao Paulo, 2010-2019</title>
      <link>https://arxiv.org/abs/2509.11546</link>
      <description>arXiv:2509.11546v1 Announce Type: cross 
Abstract: Cardiovascular diseases (CVD) remain one of the leading causes of hospitalization in Brazil. Exposure to air pollutants such as PM$_{10}$ $\mu$m, NO$_2$, and SO$_2$ has been associated with the worsening of these diseases, especially in urban areas. This study evaluated the association between the daily concentration of these pollutants and daily hospitalizations for acute myocardial infarction and cerebrovascular diseases in S\~ao Paulo (2010-2019), using generalized additive models with a lag of 0 to 4 days. Two approaches for choosing the degrees of freedom in temporal smoothing were compared: based on pollutant prediction and based on outcome prediction (hospitalizations). Data were obtained from official government databases. The modeling used the quasi-Poisson family in R software (v. 4.4.0). Models with exposure-based smoothing generated more consistent estimates. For PM10{\mu}m, the cumulative risk estimate for exposure was 1.08%, while for hospitalization, it was 1.20%. For NO$_2$, the estimated risk was 1.47% (exposure) versus 1.33% (hospitalization). For SO$_2$, a striking difference was observed: 7.66% (exposure) versus 14.31% (hospitalization). The significant lags were on days 0, 1, and 2. The results show that smoothing based on outcome prediction can generate bias, masking the true effect of pollutants. The appropriate choice of df in the smoothing function is crucial. Smoothing by the pollutant series was more robust and accurate, contributing to methodological improvements in time-series studies and reinforcing the importance of public policies for pollution control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11546v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlos Souto dos Santos Filho (S\~ao Paulo State University), Ana J\'ulia Alves C\^amara (Federal University of Esp\'irito Santo), Guilherme Aparecido Santos Aguilar (S\~ao Paulo State University)</dc:creator>
    </item>
    <item>
      <title>Wavelet-SARIMA-Transformer: A Hybrid Model for Rainfall Forecasting</title>
      <link>https://arxiv.org/abs/2509.11903</link>
      <description>arXiv:2509.11903v1 Announce Type: cross 
Abstract: This study develops and evaluates a novel hybridWavelet SARIMA Transformer, WST framework to forecast using monthly rainfall across five meteorological subdivisions of Northeast India over the 1971 to 2023 period. The approach employs the Maximal Overlap Discrete Wavelet Transform, MODWT with four wavelet families such as, Haar, Daubechies, Symlet, Coiflet etc. to achieve shift invariant, multiresolution decomposition of the rainfall series. Linear and seasonal components are modeled using Seasonal ARIMA, SARIMA, while nonlinear components are modeled by a Transformer network, and forecasts are reconstructed via inverse MODWT. Comprehensive validation using an 80 is to 20 train test split and multiple performance indices such as, RMSE, MAE, SMAPE, Willmotts d, Skill Score, Percent Bias, Explained Variance, and Legates McCabes E1 demonstrates the superiority of the Haar-based hybrid model, WHST. Across all subdivisions, WHST consistently achieved lower forecast errors, stronger agreement with observed rainfall, and unbiased predictions compared with stand alone SARIMA, stand-alone Transformer, and two-stage wavelet hybrids. Residual adequacy was confirmed through the Ljung Box test, while Taylor diagrams provided an integrated assessment of correlation, variance fidelity, and RMSE, further reinforcing the robustness of the proposed approach. The results highlight the effectiveness of integrating multiresolution signal decomposition with complementary linear and deep learning models for hydroclimatic forecasting. Beyond rainfall, the proposed WST framework offers a scalable methodology for forecasting complex environmental time series, with direct implications for flood risk management, water resources planning, and climate adaptation strategies in data-sparse and climate-sensitive regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11903v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Junmoni Saikia, Kuldeep Goswami, Sarat C. Kakaty</dc:creator>
    </item>
    <item>
      <title>Identifiable Autoregressive Variational Autoencoders for Nonlinear and Nonstationary Spatio-Temporal Blind Source Separation</title>
      <link>https://arxiv.org/abs/2509.11962</link>
      <description>arXiv:2509.11962v1 Announce Type: cross 
Abstract: The modeling and prediction of multivariate spatio-temporal data involve numerous challenges. Dimension reduction methods can significantly simplify this process, provided that they account for the complex dependencies between variables and across time and space. Nonlinear blind source separation has emerged as a promising approach, particularly following recent advances in identifiability results. Building on these developments, we introduce the identifiable autoregressive variational autoencoder, which ensures the identifiability of latent components consisting of nonstationary autoregressive processes. The blind source separation efficacy of the proposed method is showcased through a simulation study, where it is compared against state-of-the-art methods, and the spatio-temporal prediction performance is evaluated against several competitors on air pollution and weather datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11962v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mika Sipil\"a, Klaus Nordhausen, Sara Taskinen</dc:creator>
    </item>
    <item>
      <title>On the universal calibration of Pareto-type linear combination tests</title>
      <link>https://arxiv.org/abs/2509.12066</link>
      <description>arXiv:2509.12066v1 Announce Type: cross 
Abstract: It is often of interest to test a global null hypothesis using multiple, possibly dependent, $p$-values by combining their strengths while controlling the Type I error. Recently, several heavy-tailed combinations tests, such as the harmonic mean test and the Cauchy combination test, have been proposed: they map $p$-values into heavy-tailed random variables before combining them in some fashion into a single test statistic. The resulting tests, which are calibrated under the assumption of independence of the $p$-values, have shown to be rather robust to dependence. The complete understanding of the calibration properties of the resulting combination tests of dependent and possibly tail-dependent $p$-values has remained an important open problem in the area. In this work, we show that the powerful framework of multivariate regular variation (MRV) offers a nearly complete solution to this problem.
  We first show that the precise asymptotic calibration properties of a large class of homogeneous combination tests can be expressed in terms of the angular measure -- a characteristic of the asymptotic tail-dependence under MRV. Consequently, we show that under MRV, the Pareto-type linear combination tests, which are equivalent to the harmonic mean test, are universally calibrated regardless of the tail-dependence structure of the underlying $p$-values. In contrast, the popular Cauchy combination test is shown to be universally honest but often conservative; the Tippet combination test, while being honest, is calibrated if and only if the underlying $p$-values are tail-independent.
  One of our major findings is that the Pareto-type linear combination tests are the only universally calibrated ones among the large family of possibly non-linear homogeneous heavy-tailed combination tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12066v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Parijat Chakraborty, F. Richard Guo, Kerby Shedden, Stilian Stoev</dc:creator>
    </item>
    <item>
      <title>MMM: Clustering Multivariate Longitudinal Mixed-type Data</title>
      <link>https://arxiv.org/abs/2509.12166</link>
      <description>arXiv:2509.12166v1 Announce Type: cross 
Abstract: Multivariate longitudinal data of mixed-type are increasingly collected in many science domains. However, algorithms to cluster this kind of data remain scarce, due to the challenge to simultaneously model the within- and between-time dependence structures for multivariate data of mixed kind. We introduce the Mixture of Mixed-Matrices (MMM) model: reorganizing the data in a three-way structure and assuming that the non-continuous variables are observations of underlying latent continuous variables, the model relies on a mixture of matrix-variate normal distributions to perform clustering in the latent dimension. The MMM model is thus able to handle continuous, ordinal, binary, nominal and count data and to concurrently model the heterogeneity, the association among the responses and the temporal dependence structure in a parsimonious way and without assuming conditional independence. The inference is carried out through an MCMC-EM algorithm, which is detailed. An evaluation of the model through synthetic data shows its inference abilities. A real-world application on financial data is presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12166v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francesco Amato, Julien Jacques</dc:creator>
    </item>
    <item>
      <title>Extrapolation of Tempered Posteriors</title>
      <link>https://arxiv.org/abs/2509.12173</link>
      <description>arXiv:2509.12173v1 Announce Type: cross 
Abstract: Tempering is a popular tool in Bayesian computation, being used to transform a posterior distribution $p_1$ into a reference distribution $p_0$ that is more easily approximated. Several algorithms exist that start by approximating $p_0$ and proceed through a sequence of intermediate distributions $p_t$ until an approximation to $p_1$ is obtained. Our contribution reveals that high-quality approximation of terms up to $p_1$ is not essential, as knowledge of the intermediate distributions enables posterior quantities of interest to be extrapolated. Specifically, we establish conditions under which posterior expectations are determined by their associated tempered expectations on any non-empty $t$ interval. Harnessing this result, we propose novel methodology for approximating posterior expectations based on extrapolation and smoothing of tempered expectations, which we implement as a post-processing variance-reduction tool for sequential Monte Carlo.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12173v1</guid>
      <category>stat.CO</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mengxin Xi, Zheyang Shen, Marina Riabiz, Nicolas Chopin, Chris J. Oates</dc:creator>
    </item>
    <item>
      <title>Semiparametric sensitivity analysis: unmeasured confounding in observational studies</title>
      <link>https://arxiv.org/abs/2104.08300</link>
      <description>arXiv:2104.08300v3 Announce Type: replace 
Abstract: Establishing cause-effect relationships from observational data often relies on untestable assumptions. It is crucial to know whether, and to what extent, the conclusions drawn from non-experimental studies are robust to potential unmeasured confounding. In this paper, we focus on the average causal effect (ACE) as our target of inference. We generalize the sensitivity analysis approach developed by Robins et al. (2000), Franks et al. (2020), and Zhou and Yao (2023). We use semiparametric theory to derive the non-parametric efficient influence function of the ACE, for fixed sensitivity parameters. We use this influence function to construct a one-step, split sample, truncated estimator of the ACE. Our estimator depends on semiparametric models for the distribution of the observed data; importantly, these models do not impose any restrictions on the values of sensitivity analysis parameters. We establish sufficient conditions ensuring that our estimator has root-n asymptotics. We use our methodology to evaluate the causal effect of smoking during pregnancy on birth weight. We also evaluate the performance of estimation procedure in a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2104.08300v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Biometrics, Volume 80, Issue 4, December 2024</arxiv:journal_reference>
      <dc:creator>Razieh Nabi, Matteo Bonvini, Edward H. Kennedy, Ming-Yueh Huang, Marcela Smid, Daniel O. Scharfstein</dc:creator>
    </item>
    <item>
      <title>A Permutation-free Kernel Two-Sample Test</title>
      <link>https://arxiv.org/abs/2211.14908</link>
      <description>arXiv:2211.14908v3 Announce Type: replace 
Abstract: The kernel Maximum Mean Discrepancy~(MMD) is a popular multivariate distance metric between distributions that has found utility in two-sample testing. The usual kernel-MMD test statistic is a degenerate U-statistic under the null, and thus it has an intractable limiting distribution. Hence, to design a level-$\alpha$ test, one usually selects the rejection threshold as the $(1-\alpha)$-quantile of the permutation distribution. The resulting nonparametric test has finite-sample validity but suffers from large computational cost, since every permutation takes quadratic time. We propose the cross-MMD, a new quadratic-time MMD test statistic based on sample-splitting and studentization. We prove that under mild assumptions, the cross-MMD has a limiting standard Gaussian distribution under the null. Importantly, we also show that the resulting test is consistent against any fixed alternative, and when using the Gaussian kernel, it has minimax rate-optimal power against local alternatives. For large sample sizes, our new cross-MMD provides a significant speedup over the MMD, for only a slight loss in power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.14908v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shubhanshu Shekhar, Ilmun Kim, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>A method for empirically assessing small area estimators via bootstrap-weighted k-Nearest-Neighbor artificial populations, with applications to forest inventory</title>
      <link>https://arxiv.org/abs/2306.15607</link>
      <description>arXiv:2306.15607v4 Announce Type: replace 
Abstract: National Forest Inventories (NFIs) monitor forest attributes across a variety of spatial and temporal scales in a given country. Increased interest in reporting and management at smaller scales has driven NFIs to investigate and adopt small area estimation (SAE) due to the promise of increased precision at these scales. However, comparing and evaluating SAE models for a given application is inherently difficult. Typically, many areas lack enough data to check unit-level modeling assumptions or to assess unit-level predictions empirically; and no ground truth is available for checking area-level estimates. Design-based simulation from artificial populations can help with each of these issues, but only if the artificial populations realistically represent the application at hand and are not built using assumptions that inherently favor one SAE model over another. In this paper, we borrow ideas from random hot deck, approximate Bayesian bootstrap (ABB), and k Nearest Neighbor (kNN) imputation methods to propose a kNN-based approximation to ABB (KBAABB), for generating an artificial population when rich unit-level auxiliary data is available. We introduce diagnostic checks on the process of building the artificial population, and we demonstrate how to use such an artificial population for design-based simulation studies to compare and evaluate SAE models, using real data from the United States Department of Agriculture, Forest Service, Forest Inventory and Analysis Program, the NFI of the United States.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15607v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Grayson W. White, Jerzy A. Wieczorek, Zachariah W. Cody, Emily X. Tan, Jacqueline O. Chistolini, Kelly S. McConville, Tracey S. Frescino, Gretchen G. Moisen</dc:creator>
    </item>
    <item>
      <title>Models for temporal clustering of extreme events with applications to mid-latitude winter cyclones</title>
      <link>https://arxiv.org/abs/2308.14625</link>
      <description>arXiv:2308.14625v3 Announce Type: replace 
Abstract: The occurrence of extreme events like heavy precipitation or storms at a certain location often shows a clustering behaviour and is thus not described well by a Poisson process. We construct a general model for the inter-exceedance times in between such events which combines different candidate models for such behaviour. This allows us to distinguish data generating mechanisms leading to clusters of dependent events with exponential inter-exceedance times in between clusters from independent events with heavy-tailed inter-exceedance times, and even allows us to combine these two mechanisms for better descriptions of such occurrences. We propose a modification of the Cram\'er-von Mises distance for model fitting. An application to mid-latitude winter cyclones illustrates the usefulness of our work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.14625v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christina Mathieu, Katharina Hees, Roland Fried</dc:creator>
    </item>
    <item>
      <title>Post-selection inference for causal effects after causal discovery</title>
      <link>https://arxiv.org/abs/2405.06763</link>
      <description>arXiv:2405.06763v3 Announce Type: replace 
Abstract: Algorithms for constraint-based causal discovery select graphical causal models among a space of possible candidates (e.g., all directed acyclic graphs) by executing a sequence of conditional independence tests. These may be used to inform the estimation of causal effects (e.g., average treatment effects) when there is uncertainty about which covariates ought to be adjusted for, or which variables act as confounders versus mediators. However, naively using the data twice, for model selection and estimation, would lead to invalid confidence intervals. Moreover, if the selected graph is incorrect, the inferential claims may apply to a selected functional that is distinct from the actual causal effect. We propose an approach to post-selection inference that is based on a resampling and screening procedure, which essentially performs causal discovery multiple times with randomly varying intermediate test statistics. Then, an estimate of the target causal effect and corresponding confidence sets are constructed from a union of individual graph-based estimates and intervals. We show that this construction has asymptotically correct coverage for the true causal effect parameter. Importantly, the guarantee holds for a fixed population-level effect, not a data-dependent or selection-dependent quantity. Most of our exposition focuses on the PC-algorithm for learning directed acyclic graphs and the multivariate Gaussian case for simplicity, but the approach is general and modular, so it may be used with other conditional independence based discovery algorithms and distributional families.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06763v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ting-Hsuan Chang, Zijian Guo, Daniel Malinsky</dc:creator>
    </item>
    <item>
      <title>Scalable Bayesian inference for heat kernel Gaussian processes on manifolds</title>
      <link>https://arxiv.org/abs/2405.13342</link>
      <description>arXiv:2405.13342v2 Announce Type: replace 
Abstract: We establish a scalable manifold learning method and theory, motivated by the problem of estimating fMRI activation manifolds in the Human Connectome Project (HCP). Our primary contribution is the development of an efficient estimation technique for heat kernel Gaussian processes in the exponential family model. This approach handles large sample sizes $n$, preserves the intrinsic geometry of data, and significantly reduces computational complexity from $\mathcal{O}(n^3)$ to $\mathcal{O}(n)$ via a novel reduced-rank approximation of the graph Laplacian's transition matrix and a Truncated Singular Value Decomposition for the eigenpair computation. The numerical experiments demonstrate the scalability and improved accuracy of our method for manifold learning tasks involving complex large-scale data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13342v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1093/jrsssb/qkaf057</arxiv:DOI>
      <dc:creator>Junhui He, Guoxuan Ma, Jian Kang, Ying Yang</dc:creator>
    </item>
    <item>
      <title>A Flexible Model for Record Linkage</title>
      <link>https://arxiv.org/abs/2407.06835</link>
      <description>arXiv:2407.06835v3 Announce Type: replace 
Abstract: Combining data from various sources empowers researchers to explore innovative questions, for example those raised by conducting healthcare monitoring studies. However, the lack of a unique identifier often poses challenges. Record linkage procedures determine whether pairs of observations collected on different occasions belong to the same individual using partially identifying variables (e.g. birth year, postal code). Existing methodologies typically involve a compromise between computational efficiency and accuracy. Traditional approaches simplify this task by condensing information, yet they neglect dependencies among linkage decisions and disregard the one-to-one relationship required to establish coherent links. Modern approaches offer a comprehensive representation of the data generation process, at the expense of computational overhead and reduced flexibility. We propose a flexible method, that adapts to varying data complexities, addressing registration errors and accommodating changes of the identifying information over time. Our approach balances accuracy and scalability, estimating the linkage using a Stochastic Expectation Maximisation algorithm on a latent variable model. We illustrate the ability of our methodology to connect observations using large real data applications and demonstrate the robustness of our model to the linking variables quality in a simulation study. The proposed algorithm FlexRL is implemented and available in an open source R package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06835v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1093/jrsssc/qlaf016</arxiv:DOI>
      <arxiv:journal_reference>A flexible model for record linkage, JRSSSC, Volume 74, Issue 4, November 2025, Pages 1100-1127</arxiv:journal_reference>
      <dc:creator>Kayan\'e Robach, St\'ephanie L van der Pas, Mark A van de Wiel, Michel H Hof</dc:creator>
    </item>
    <item>
      <title>Raking mortality rates across cause, population group and geography with uncertainty quantification</title>
      <link>https://arxiv.org/abs/2407.20520</link>
      <description>arXiv:2407.20520v4 Announce Type: replace 
Abstract: The Global Burden of Diseases, Injuries, and Risk Factors Study (GBD) is the single largest and most detailed scientific effort ever conducted to quantify levels and trends in health. This global health model to estimate mortality rates and other health metrics is run at different scales, leading to large data sets of results for a global region and its different sub-regions, or for a cause of death and different sub-causes for example. These models do not necessarily lead to consistent data tables where, for instance, the sum of the number of deaths for each of the sub-regions is equal to the number of deaths for the global region. Raking is widely used in survey inference and global health models to adjust the observations in contingency tables to given marginals, in the latter case reconciling estimates between models with different granularities. The results of global health models usually associate to the point estimates an uncertainty, such as standard deviations or confidence intervals. In this paper, we propose an uncertainty propagation approach that obtains, at the cost of a single solve, nearly the same uncertainty estimates as computationally intensive Monte Carlo techniques that pass thousands of observed and marginal samples through the entire raking process. We introduce a convex optimization approach that provides a unified framework to raking extensions such as uncertainty propagation, raking with differential weights, raking with different loss functions in order to ensure that bounds on estimates are respected, verifying the feasibility of the constraints, raking to margins either as hard constraints or as aggregate observations, and handling missing data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20520v4</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ariane Ducellier (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Alexander Hsu (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Parkes Kendrick (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Bill Gustafson (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Laura Dwyer-Lindgren (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Christopher Murray (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Peng Zheng (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Aleksandr Aravkin (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA)</dc:creator>
    </item>
    <item>
      <title>A subcopula characterization of dependence for the Multivariate Bernoulli Distribution</title>
      <link>https://arxiv.org/abs/2410.01133</link>
      <description>arXiv:2410.01133v2 Announce Type: replace 
Abstract: By applying Sklar's theorem to the Multivariate Bernoulli Distribution (MBD), this paper proposes a framework to decouple marginal distributions from the dependence structure, clarifying interactions among binary variables. Explicit formulas are derived under the MBD using subcopulas to introduce dependence measures for interactions of all orders, not just pairwise. A Bayesian inference approach is also applied to estimate the parameters of the MBD, offering practical tools for parameter estimation and dependence analysis in real-world applications. The results obtained contribute to the application of subcopulas of multivariate binary data, with real data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01133v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arturo Erdely</dc:creator>
    </item>
    <item>
      <title>Identifying nonlinear relations among random variables: A network analytic approach</title>
      <link>https://arxiv.org/abs/2411.02763</link>
      <description>arXiv:2411.02763v2 Announce Type: replace 
Abstract: Nonlinear relations, such as the curvilinear relationship between childhood trauma and resilience in patients with schizophrenia and the moderation relationship between mentalizing, and internalizing and externalizing symptoms and quality of life in youths, are more prevalent than our current methods have been able to detect. Although the use of network models has risen, network construction for the standard Gaussian graphical model depends solely upon linearity. While nonlinear models are an active field of study in psychological methodology, many models require the analyst to specify the functional form of the relation. When performing more exploratory modeling, such as with cross-sectional network psychometrics, specifying the functional form a nonlinear relation might take becomes infeasible given the number of possible relations modeled. Here, we apply a novel nonparametric approach to identifying nonlinear relations using partial distance correlations. We found that partial distance correlations excel overall at identifying nonlinear relations regardless of functional form when compared with Pearson's and Spearman's partial correlations and conditional mutual information. Through simulation studies and an empirical example, we show that partial distance correlations as a novel method can be used to identify possible nonlinear relations in psychometric networks, enabling researchers to then explore the shape of these relations with more confirmatory models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02763v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lindley R. Slipetz, Jiaxing Qiu, Siqi Sun, Teague R. Henry</dc:creator>
    </item>
    <item>
      <title>Robust Gradient Descent Estimation for Tensor Models under Heavy-Tailed Distributions</title>
      <link>https://arxiv.org/abs/2412.04773</link>
      <description>arXiv:2412.04773v2 Announce Type: replace 
Abstract: Low-rank tensor models are widely used in statistics. However, most existing methods rely heavily on the assumption that data follows a sub-Gaussian distribution. To address the challenges associated with heavy-tailed distributions encountered in real-world applications, we propose a novel robust estimation procedure based on truncated gradient descent for general low-rank tensor models. We establish the computational convergence of the proposed method and derive optimal statistical rates under heavy-tailed distributional settings of both covariates and noise for various low-rank models. Notably, the statistical error rates are governed by a local moment condition, which captures the distributional properties of tensor variables projected onto certain low-dimensional local regions. Furthermore, we present numerical results to demonstrate the effectiveness of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04773v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoyu Zhang, Di Wang, Guodong Li, Defeng Sun</dc:creator>
    </item>
    <item>
      <title>B-MASTER: Scalable Bayesian Multivariate Regression for Master Predictor Discovery in Colorectal Cancer Microbiome-Metabolite Profiles</title>
      <link>https://arxiv.org/abs/2412.05998</link>
      <description>arXiv:2412.05998v3 Announce Type: replace 
Abstract: The gut microbiome significantly influences responses to cancer therapies, including immunotherapies, primarily through its impact on the metabolome. Despite some studies on effects of specific microbial genera on individual metabolites, there is little prior work identifying key microbiome components at the genus level that shape the overall metabolome profile. To address this gap, we introduce B-MASTER (Bayesian Multivariate regression Analysis for Selecting Targeted Essential Regressors), a fully Bayesian framework with an L1 penalty to promote sparsity and an L2 penalty to shrink coefficients for non-major covariates, thereby isolating essential regressors. The method is paired with a scalable Gibbs sampling algorithm, whose computation grows linearly with the number of parameters and remains largely unaffected by sample size for models of fixed dimensions. Notably, B-MASTER enables full posterior inference for models with up to four million parameters within a practical time-frame. Its theoretical guarantees include posterior contraction, selection consistency, and robustness under mild misspecification. Using this approach, we identify key microbial genera shaping the metabolite profile, analyze their effects on the most abundant metabolites, and investigate metabolites differentially abundant in colorectal cancer (CRC) patients. These results provide foundational insights into microbiome-metabolite relationships relevant to cancer, a connection largely unexplored in existing literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05998v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Priyam Das, Tanujit Dey, Christine Peterson, Sounak Chakraborty</dc:creator>
    </item>
    <item>
      <title>Understanding Model Calibration -- A gentle introduction and visual exploration of calibration and the expected calibration error (ECE)</title>
      <link>https://arxiv.org/abs/2501.19047</link>
      <description>arXiv:2501.19047v5 Announce Type: replace 
Abstract: To be considered reliable, a model must be calibrated so that its confidence in each decision closely reflects its true outcome. In this blogpost we'll take a look at the most commonly used definition for calibration and then dive into a frequently used evaluation measure for model calibration. We'll then cover some of the drawbacks of this measure and how these surfaced the need for additional notions of calibration, which require their own new evaluation measures. This post is not intended to be an in-depth dissection of all works on calibration, nor does it focus on how to calibrate models. Instead, it is meant to provide a gentle introduction to the different notions and their evaluation measures as well as to re-highlight some issues with a measure that is still widely used to evaluate calibration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19047v5</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>https://iclr-blogposts.github.io/2025/blog/calibration/</arxiv:journal_reference>
      <dc:creator>Maja Pavlovic</dc:creator>
    </item>
    <item>
      <title>A Unified Framework for Large-Scale Inference of Classification: Error Rate Control and Optimality</title>
      <link>https://arxiv.org/abs/2504.07321</link>
      <description>arXiv:2504.07321v2 Announce Type: replace 
Abstract: Classification is a fundamental task in supervised learning, while achieving valid misclassification rate control remains challenging due to possibly the limited predictive capability of the classifiers or the intrinsic complexity of the classification task. In this article, we address large-scale multi-class classification problems with general error rate guarantees to enhance algorithmic trustworthiness. To this end, we first introduce a notion of group-wise classification, which unifies the common class-wise and overall classifications as special cases. We then develop a unified inference framework for the general group-wise classification that consists of three steps: Pre-classification, Selective $p$-value construction, and large-scale Post-classification decisions (PSP). Theoretically, PSP is distribution-free and provides valid finite-sample guarantees for controlling general group-wise false decision rates at target levels. To show the power of PSP, we demonstrate that the step of post-classification decisions never degrades the power of pre-classification, provided that pre-classification has been sufficiently powerful to meet the target error levels. We further establish general power optimality theories for PSP from both non-asymptotic and asymptotic perspectives. Numerical results in both simulations and real data analysis validate the performance of the proposed PSP approach. In addition, we introduce an ePSP algorithm that integrates the idea of PSP with selective $e$-values. Finally, extensions of PSP are shown to demonstrate its feasibility and power in broader applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07321v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinrui Sun, Yin Xia</dc:creator>
    </item>
    <item>
      <title>Model-free High Dimensional Mediator Selection with False Discovery Rate Control</title>
      <link>https://arxiv.org/abs/2505.09105</link>
      <description>arXiv:2505.09105v3 Announce Type: replace 
Abstract: There is a challenge in selecting high-dimensional mediators when the mediators have complex correlation structures and interactions. In this work, we frame the high-dimensional mediator selection problem into a series of hypothesis tests with composite nulls, and develop a method to control the false discovery rate (FDR) which has mild assumptions on the mediation model. We show the theoretical guarantee that the proposed method and algorithm achieve FDR control. We present extensive simulation results to demonstrate the power and finite sample performance compared with existing methods. Lastly, we demonstrate the method for analyzing the Alzheimer's Disease Neuroimaging Initiative (ADNI) data, in which the proposed method selects the volume of the hippocampus and amygdala, as well as some other important MRI-derived measures as mediators for the relationship between gender and dementia progression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09105v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Runqiu Wang, Ran Dai, Jieqiong Wang, Kah Meng Soh, Ziyang Xu, Mohamed Azzam, Hongying Dai, Cheng Zheng</dc:creator>
    </item>
    <item>
      <title>Intellectual Up-streams of Percentage Scale ($ps$) and Percentage Coefficient ($b_p$) -- Effect Size Analysis (Theory Paper 2)</title>
      <link>https://arxiv.org/abs/2507.13695</link>
      <description>arXiv:2507.13695v2 Announce Type: replace 
Abstract: Percentage thinking, i.e., assessing quantities as parts per hundred, spread from Roman tax ledgers to modern algorithms. Building on Simon Stevin's La Thiende (1585) and the 19th-century metrication that institutionalized base-10 measurement (Cajori, 1925), this article traces how base-10 normalization, especially the 0-1 percentage scale, became a shared language for human and machine understanding. We retrace 1980s efforts at UW-Madison and UNC Chapel Hill to "percentize" variables to make regression coefficients interpretable, and relate these experiments to established indices, notably the Pearson (1895) correlation r (range -1 to 1) and the coefficient of determination r-squared (Wright, 1920). We also revisit Cohen et al.'s (1999) percent of maximum possible (POMP) metric. The lineage of 0-100 and 0-1 scales includes Roman fiscal practice, early American grading at Yale and Harvard, and recurring analyses of percent (0-100) and percentage (0-1, or -1 to 1) scales that repeatedly reinvent the same indices (Durm, 1993; Schneider and Hutt, 2014). In data mining and machine learning, min-max normalization maps any feature to [0, 1] (i.e., 0-100%), equalizing scale ranges and implied units across percentized variables, which improves comparability of predictors. Under the percentage theory of measurement indices, equality of units is the necessary and sufficient condition for comparing indices (Cohen et al., 1999; Zhao et al., 2024; Zhao and Zhang, 2014). Seen this way, the successes of machine learning and artificial intelligence over the past half century constitute large-scale evidence for the comparability of percentage-based indices, foremost the percentage coefficient (bp).</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13695v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinshu Zhao, Qinru Ruby Ju, Piper Liping Liu, Dianshi Moses Li, Luxi Zhang, Jizhou Francis Ye, Song Harris Ao, Ming Milano Li</dc:creator>
    </item>
    <item>
      <title>Identifying Unmeasured Confounders in Panel Causal Models: A Two-Stage LM-Wald Approach</title>
      <link>https://arxiv.org/abs/2508.10342</link>
      <description>arXiv:2508.10342v2 Announce Type: replace 
Abstract: Panel data are widely used in political science to draw causal inferences. However, these models often rely on the strong and untested assumption of sequential ignorability--that no unmeasured variables influence both the independent and outcome variables across time. Grounded in psychometric literature on latent variable modeling, this paper introduces the Two-Stage LM-Wald (2SLW) approach, a diagnostic tool that extends the Lagrange Multiplier (LM) and Wald tests to detect violations of this assumption in panel causal models. Using Monte Carlo simulations within the Random Intercept Cross-Lagged Panel Model (RI-CLPM), which separates within and between person effects, I demonstrate the 2SLW's ability to detect unmeasured confounding across three key scenarios: biased corrections, distorted direct effects, and altered mediation pathways. I also illustrate the approach with an empirical application to real-world panel data. By providing a practical and theoretically grounded diagnostic, the 2SLW approach enhances the robustness of causal inferences in the presence of potential time-varying confounders. Moreover, it can be readily implemented using the R package lavaan.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10342v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bang Quan Zheng</dc:creator>
    </item>
    <item>
      <title>A Case for a "Refutations and Critiques'' Track in Statistics Journals</title>
      <link>https://arxiv.org/abs/2509.03702</link>
      <description>arXiv:2509.03702v2 Announce Type: replace 
Abstract: The statistics community, which has traditionally lacked a transparent and open peer-review system, faces a challenge of inconsistent paper quality, with some published work containing substantial errors. This problem resonates with concerns raised by Schaeffer et al. (2025) regarding the rapid growth of machine learning research. They argue that peer review has proven insufficient to prevent the publication of ``misleading, incorrect, flawed or perhaps even fraudulent studies'' and that a ``dynamic self-correcting research ecosystem'' is needed. This note provides a concrete illustration of this problem by examining two published papers, Wang, Zhou and Lin (2025) and Liu et al. (2023), and exposing striking and critical errors in their proofs. The presence of such errors in major journals raises a fundamental question about the importance and verification of mathematical proofs in our field. Echoing the proposal from Schaeffer et al. (2025), we argue that reforming the peer-review system itself is likely impractical. Instead, we propose a more viable path forward: the creation of a high-profile, reputable platform, such as a ``Refutations and Critiques'' track on arXiv, to provide visibility to vital research that critically challenges prior work. Such a mechanism would be crucial for enhancing the reliability and credibility of statistical research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03702v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhen Li</dc:creator>
    </item>
    <item>
      <title>Optimal empirical Bayes estimation for the Poisson model via minimum-distance methods</title>
      <link>https://arxiv.org/abs/2209.01328</link>
      <description>arXiv:2209.01328v3 Announce Type: replace-cross 
Abstract: The Robbins estimator is the most iconic and widely used procedure in the empirical Bayes literature for the Poisson model. On one hand, this method has been recently shown to be minimax optimal in terms of the regret (excess risk over the Bayesian oracle that knows the true prior) for various nonparametric classes of priors. On the other hand, it has been long recognized in practice that the Robbins estimator lacks the desired smoothness and monotonicity of Bayes estimators and can be easily derailed by those data points that were rarely observed before. Based on the minimum-distance distance method, we propose a suite of empirical Bayes estimators, including the classical nonparametric maximum likelihood, that outperform the Robbins method in a variety of synthetic and real data sets and retain its optimality in terms of minimax regret.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.01328v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soham Jana, Yury Polyanskiy, Yihong Wu</dc:creator>
    </item>
    <item>
      <title>Identification by non-Gaussianity in structural threshold and smooth transition vector autoregressive models</title>
      <link>https://arxiv.org/abs/2404.19707</link>
      <description>arXiv:2404.19707v5 Announce Type: replace-cross 
Abstract: We show that structural smooth transition vector autoregressive models are statistically identified if the shocks are mutually independent and at most one of them is Gaussian. This extends a known identification result for linear structural vector autoregressions to a time-varying impact matrix. We also propose an estimation method, show how a blended identification strategy can be adopted to address weak identification, and establish a sufficient condition for ergodic stationarity. The introduced methods are implemented in the accompanying R package sstvars. Our empirical application finds that a positive climate policy uncertainty shock reduces production and raises inflation under both low and high economic policy uncertainty, but its effects, particularly on inflation, are stronger during the latter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19707v5</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Savi Virolainen</dc:creator>
    </item>
    <item>
      <title>The multivariate fractional Ornstein-Uhlenbeck process</title>
      <link>https://arxiv.org/abs/2408.03051</link>
      <description>arXiv:2408.03051v2 Announce Type: replace-cross 
Abstract: Starting from the notion of multivariate fractional Brownian Motion introduced in [F. Lavancier, A. Philippe, and D. Surgailis. Covariance function of vector self-similar processes. Statistics &amp; Probability Letters, 2009] we define a multivariate version of the fractional Ornstein-Uhlenbeck process. This multivariate Gaussian process is stationary, ergodic and allows for different Hurst exponents on each component. We characterize its correlation matrix and its short and long time asymptotics. Besides the marginal parameters, the cross correlation between one-dimensional marginal components is ruled by two parameters. We consider the problem of their inference, proposing two types of estimator, constructed from discrete observations of the process. We establish their asymptotic theory, in one case in the long time asymptotic setting, in the other case in the infill and long time asymptotic setting. The limit behavior can be asymptotically Gaussian or non-Gaussian, depending on the values of the Hurst exponents of the marginal components. The technical core of the paper relies on the analysis of asymptotic properties of functionals of Gaussian processes, that we establish using Malliavin calculus and Stein's method. We provide numerical experiments that support our theoretical analysis and also suggest a conjecture on the application of one of these estimators to the multivariate fractional Brownian Motion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03051v2</guid>
      <category>math.PR</category>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ranieri Dugo, Giacomo Giorgio, Paolo Pigato</dc:creator>
    </item>
    <item>
      <title>Deep learning joint extremes of metocean variables using the SPAR model</title>
      <link>https://arxiv.org/abs/2412.15808</link>
      <description>arXiv:2412.15808v3 Announce Type: replace-cross 
Abstract: This paper presents a novel deep learning framework for estimating multivariate joint extremes of metocean variables, based on the Semi-Parametric Angular-Radial (SPAR) model. When considered in polar coordinates, the problem of modelling multivariate extremes is transformed to one of modelling an angular density, and the tail of a univariate radial variable conditioned on angle. In the SPAR approach, the tail of the radial variable is modelled using a generalised Pareto (GP) distribution, providing a natural extension of univariate extreme value theory to the multivariate setting. In this work, we show how the method can be applied in higher dimensions, using a case study for five metocean variables: wind speed, wind direction, wave height, wave period, and wave direction. The angular variable is modelled using a kernel density method, while the parameters of the GP model are approximated using fully-connected deep neural networks. Our approach provides great flexibility in the dependence structures that can be represented, together with computationally efficient routines for training the model. Furthermore, the application of the method requires fewer assumptions about the underlying distribution(s) compared to existing approaches, and an asymptotically justified means for extrapolating outside the range of observations. Using various diagnostic plots, we show that the fitted models provide a good description of the joint extremes of the metocean variables considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15808v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ed Mackay, Callum Murphy-Barltrop, Jordan Richards, Philip Jonathan</dc:creator>
    </item>
    <item>
      <title>Simulation-Based Sensitivity Analysis in Optimal Treatment Regimes and Causal Decomposition with Individualized Interventions</title>
      <link>https://arxiv.org/abs/2506.19010</link>
      <description>arXiv:2506.19010v2 Announce Type: replace-cross 
Abstract: Causal decomposition analysis aims to assess the effect of modifying risk factors on reducing social disparities in outcomes. Recently, this analysis has incorporated individual characteristics when modifying risk factors by utilizing optimal treatment regimes (OTRs). Since the newly defined individualized effects rely on the no omitted confounding assumption, developing sensitivity analyses to account for potential omitted confounding is essential. Moreover, OTRs and individualized effects are primarily based on binary risk factors, and no formal approach currently exists to benchmark the strength of omitted confounding using observed covariates for binary risk factors. To address this gap, we extend a simulation-based sensitivity analysis that simulates unmeasured confounders, addressing two sources of bias emerging from deriving OTRs and estimating individualized effects. Additionally, we propose a formal bounding strategy that benchmarks the strength of omitted confounding for binary risk factors. Using the High School Longitudinal Study 2009 (HSLS:09), we demonstrate this sensitivity analysis and benchmarking method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19010v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Soojin Park, Suyeon Kang, Chioun Lee</dc:creator>
    </item>
    <item>
      <title>Likelihood Ratio Tests by Kernel Gaussian Embedding</title>
      <link>https://arxiv.org/abs/2508.07982</link>
      <description>arXiv:2508.07982v2 Announce Type: replace-cross 
Abstract: We propose a novel kernel-based nonparametric two-sample test, employing the combined use of kernel mean and kernel covariance embedding. Our test builds on recent results showing how such combined embeddings map distinct probability measures to mutually singular Gaussian measures on the kernel's RKHS. Leveraging this ``separation of measure phenomenon", we construct a test statistic based on the relative entropy between the Gaussian embeddings, in effect the likelihood ratio. The likelihood ratio is specifically tailored to detect equality versus singularity of two Gaussians, and satisfies a ``$0/\infty$" law, in that it vanishes under the null and diverges under the alternative. To implement the test in finite samples, we introduce a regularised version, calibrated by way of permutation. We prove consistency, establish uniform power guarantees under mild conditions, and discuss how our framework unifies and extends prior approaches based on spectrally regularized MMD. Empirical results on synthetic and real data demonstrate remarkable gains in power compared to state-of-the-art methods, particularly in high-dimensional and weak-signal regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07982v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leonardo V. Santoro, Victor M. Panaretos</dc:creator>
    </item>
    <item>
      <title>Non-Linear Model-Based Sequential Decision-Making in Agriculture</title>
      <link>https://arxiv.org/abs/2509.01924</link>
      <description>arXiv:2509.01924v2 Announce Type: replace-cross 
Abstract: Sequential decision-making is central to sustainable agricultural management and precision agriculture, where resource inputs must be optimized under uncertainty and over time. However, such decisions must often be made with limited observations, whereas classical bandit and reinforcement learning approaches typically rely on either linear or black-box reward models that may misrepresent domain knowledge or require large amounts of data. We propose a family of \emph{nonlinear, model-based bandit algorithms} that embed domain-specific response curves directly into the exploration-exploitation loop. By coupling (i) principled uncertainty quantification with (ii) closed-form or rapidly computable profit optima, these algorithms achieve sublinear regret and near-optimal sample complexity while preserving interpretability. Theoretical analysis establishes regret and sample complexity bounds, and extensive simulations emulating real-world fertilizer-rate decisions show consistent improvements over both linear and nonparametric baselines (such as linear UCB and $k$-NN UCB) in the low-sample regime, under both well-specified and shape-compatible misspecified models. Because our approach leverages mechanistic insight rather than large data volumes, it is especially suited to resource-constrained settings, supporting sustainable, inclusive, and transparent sequential decision-making across agriculture, environmental management, and allied applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01924v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sakshi Arya, Wentao Lin</dc:creator>
    </item>
  </channel>
</rss>
