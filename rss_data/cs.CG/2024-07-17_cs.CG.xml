<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CG updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CG</link>
    <description>cs.CG updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CG" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 17 Jul 2024 04:01:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 17 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Transforming the Challenge of Constructing Low-Discrepancy Point Sets into a Permutation Selection Problem</title>
      <link>https://arxiv.org/abs/2407.11533</link>
      <description>arXiv:2407.11533v1 Announce Type: new 
Abstract: Low discrepancy point sets have been widely used as a tool to approximate continuous objects by discrete ones in numerical processes, for example in numerical integration. Following a century of research on the topic, it is still unclear how low the discrepancy of point sets can go; in other words, how regularly distributed can points be in a given space. Recent insights using optimization and machine learning techniques have led to substantial improvements in the construction of low-discrepancy point sets, resulting in configurations of much lower discrepancy values than previously known. Building on the optimal constructions, we present a simple way to obtain $L_{\infty}$-optimized placement of points that follow the same relative order as an (arbitrary) input set. Applying this approach to point sets in dimensions 2 and 3 for up to 400 and 50 points, respectively, we obtain point sets whose $L_{\infty}$ star discrepancies are up to 25% smaller than those of the current-best sets, and around 50% better than classical constructions such as the Fibonacci set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11533v1</guid>
      <category>cs.CG</category>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fran\c{c}ois Cl\'ement, Carola Doerr, Kathrin Klamroth, Lu\'is Paquete</dc:creator>
    </item>
    <item>
      <title>Rectangle Tiling Binary Arrays</title>
      <link>https://arxiv.org/abs/2007.14142</link>
      <description>arXiv:2007.14142v2 Announce Type: replace 
Abstract: The problem of rectangle tiling binary arrays is defined as follows. Given an $n \times n$ array $A$ of zeros and ones and a natural number $p$, our task is to partition $A$ into at most $p$ rectangular tiles, so that the maximal weight of a tile is minimized. A tile is any rectangular subarray of $A$. The weight of a tile is the sum of elements that fall within it. We present a linear $(O(n^2))$ time $(\frac{3}{2}+\frac{p^2}{w(A)})$-approximation algorithm (where $\frac{p^2}{w(A)} &lt; \frac{1}{2}$) for this problem, where $w(A)$ denotes the weight of the whole array $A$. This improves on the previously known approximation with the ratio $2$.
  The result is best possible in the following sense. The algorithm employs the lower bound of $L=\lceil \frac{w(A)}{p} \rceil$, which is the only known and used bound on the optimum in all algorithms for rectangle tiling. We prove that a better approximation factor for the binary \RTILE cannot be achieved using $L$, because there exist arrays, whose every partition contains a tile with weight at least $(\frac{3}{2}+\frac{p^2}{w(A)})L$. We also consider the dual problem of rectangle tiling for binary arrays, where we are given an upper bound on the weight of the tiles, and we have to cover the array $A$ with the minimum number of non-overlapping tiles. Both problems have natural extensions to $d$-dimensional versions, for which we provide analogous results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2007.14142v2</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pratik Ghosal, Syed Mohammad Meesum, Katarzyna Paluch</dc:creator>
    </item>
    <item>
      <title>The Maximum Clique Problem in a Disk Graph Made Easy</title>
      <link>https://arxiv.org/abs/2404.03751</link>
      <description>arXiv:2404.03751v2 Announce Type: replace 
Abstract: A disk graph is an intersection graph of disks in $\mathbb{R}^2$. Determining the computational complexity of finding a maximum clique in a disk graph is a long-standing open problem. In 1990, Clark, Colbourn, and Johnson gave a polynomial-time algorithm for computing a maximum clique in a unit disk graph. However, finding a maximum clique when disks are of arbitrary size is widely believed to be a challenging open problem. The problem is open even if we restrict the disks to have at most two different sizes of radii, or restrict the radii to be within $[1,1+\varepsilon]$ for some $\epsilon&gt;0$. In this paper, we provide a new perspective to examine adjacencies in a disk graph that helps obtain the following results.
  - We design an $O(2^k n^{2k} poly(n))$-time algorithm to find a maximum clique in a $n$-vertex disk graph with $k$ different sizes of radii. This is polynomial for every fixed $k$, and thus settles the open question for the case when $k=2$.
  - Given a set of $n$ unit disks, we show how to compute a maximum clique inside each possible axis-aligned rectangle determined by the disk centers in $O(n^5\log n)$-time. This is at least a factor of $n^{4/3}$ faster than applying the fastest known algorithm for finding a maximum clique in a unit disk graph for each rectangle independently.
  - We give an $O(2^kn^{2rk} poly(n,r))$-time algorithm to find a maximum clique in a $n$-vertex ball graph with $k$ different sizes of radii where the ball centers lie on $r$ parallel planes. This is polynomial for every fixed $k$ and $r$, and thus contrasts the previously known NP-hardness result for finding a maximum clique in an arbitrary ball graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03751v2</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J. Mark Keil, Debajyoti Mondal</dc:creator>
    </item>
    <item>
      <title>Edge-Unfolding Polycubes with Orthogonally Convex Layers</title>
      <link>https://arxiv.org/abs/2407.01326</link>
      <description>arXiv:2407.01326v2 Announce Type: replace 
Abstract: A polycube is an orthogonal polyhedron composed of unit cubes glued together along entire faces, homeomorphic to a sphere. A polycube layer is the section of the polycube that lies between two horizontal cross-sections of the polycube at unit distance from each other. An edge unfolding of a polycube involves cutting its surface along any of the constituent cube edges and flattening it into a single, non-overlapping planar piece. We show that any polycube with orthogonally convex layers can be edge unfolded.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01326v2</guid>
      <category>cs.CG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mirela Damian, Henk Meijer</dc:creator>
    </item>
  </channel>
</rss>
