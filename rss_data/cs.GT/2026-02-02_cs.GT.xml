<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 02 Feb 2026 05:00:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Tacit Coordination of Large Language Models</title>
      <link>https://arxiv.org/abs/2601.22184</link>
      <description>arXiv:2601.22184v1 Announce Type: new 
Abstract: In tacit coordination games with multiple outcomes, purely rational solution concepts, such as Nash equilibria, provide no guidance for which equilibrium to choose. Shelling's theory explains how, in these settings, humans coordinate by relying on focal points: solutions or outcomes that naturally arise because they stand out in some way as salient or prominent to all players. This work studies Large Language Models (LLMs) as players in tacit coordination games, and addresses how, when, and why focal points emerge. We compare and quantify the coordination capabilities of LLMs in cooperative and competitive games for which human experiments are available. We also introduce several learning-free strategies to improve the coordination of LLMs, with themselves and with humans. On a selection of heterogeneous open-source models, including Llama, Qwen, and GPT-oss, we discover that LLMs have a remarkable capability to coordinate and often outperform humans, yet fail on common-sense coordination that involves numbers or nuanced cultural archetypes. This paper constitutes the first large-scale assessment of LLMs' tacit coordination within the theoretical and psychological framework of focal points.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22184v1</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ido Aharon, Emanuele La Malfa, Michael Wooldridge, Sarit Kraus</dc:creator>
    </item>
    <item>
      <title>FAIRFORMER: A transformer architecture for discrete fair division</title>
      <link>https://arxiv.org/abs/2601.22346</link>
      <description>arXiv:2601.22346v1 Announce Type: new 
Abstract: We propose a deep neural network-based solution to the problem of allocating indivisible goods under additive subjective valuations without monetary transfers, trading off economic efficiency with envy-based fairness. We introduce FairFormer, an amortized, permutation-equivariant two-tower transformer that encodes items and agents as unordered token sets, applies self-attention within each set, and uses item-to-agent cross-attention to produce per-item assignment distributions in a single forward pass. FairFormer is trained end-to-end to maximize expected log-Nash welfare on sampled instances, requiring no solver supervision, unrolled allocation procedures, or fairness labels. At test time, we discretize by row-wise $\arg\max$ and apply a lightweight post-processing routine that transfers items to eliminate violations of envy-freeness up to one item while prioritizing improvements in Nash welfare. Our approach generalizes beyond its training regime and achieves near-optimal welfare (e.g., for uniformly sampled valuations, $96$--$97\%$ for Nash welfare; $95$--$96\%$ for utilitarian welfare), outperforming strong baselines in solution quality and/or runtime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22346v1</guid>
      <category>cs.GT</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chris Mascioli, Satyam Goyal, Mithun Chakraborty</dc:creator>
    </item>
    <item>
      <title>Dynamic Welfare-Maximizing Pooled Testing</title>
      <link>https://arxiv.org/abs/2601.22419</link>
      <description>arXiv:2601.22419v1 Announce Type: new 
Abstract: Pooled testing is a common strategy for public health disease screening under limited testing resources, allowing multiple biological samples to be tested together with the resources of a single test, at the cost of reduced individual resolution. While dynamic and adaptive strategies have been extensively studied in the classical pooled testing literature, where the goal is to minimize the number of tests required for full diagnosis of a given population, much of the existing work on welfare-maximizing pooled testing adopts static formulations in which all tests are assigned in advance. In this paper, we study dynamic welfare-maximizing pooled testing strategies in which a limited number of tests are performed sequentially to maximize social welfare, defined as the aggregate utility of individuals who are confirmed to be healthy. We formally define the dynamic problem and study algorithmic approaches for sequential test assignment. Because exact dynamic optimization is computationally infeasible beyond small instances, we evaluate a range of strategies (including exact optimization baselines, greedy heuristics, mixed-integer programming relaxations, and learning-based policies) and empirically characterize their performance and tradeoffs using synthetic experiments. Our results show that dynamic testing can yield substantial welfare improvements over static baselines in low-budget regimes. We find that much of the benefit of dynamic testing is captured by simple greedy policies, which substantially outperform static approaches while remaining computationally efficient. Learning-based methods are included as flexible baselines, but in our experiments they do not reliably improve upon these heuristics. Overall, this work provides a principled computational perspective on dynamic pooled testing and clarifies when dynamic assignment meaningfully improves welfare in public health screening.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22419v1</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicholas Lopez, Francisco Marmolejo-Coss\'io, Jose Roberto Tello Ayala, David C. Parkes</dc:creator>
    </item>
    <item>
      <title>Do AI Overviews Benefit Search Engines? An Ecosystem Perspective</title>
      <link>https://arxiv.org/abs/2601.22493</link>
      <description>arXiv:2601.22493v1 Announce Type: new 
Abstract: The integration of AI Overviews into search engines enhances user experience but diverts traffic from content creators, potentially discouraging high-quality content creation and causing user attrition that undermines long-term search engine profit. To address this issue, we propose a game-theoretic model of creator competition with costly effort, characterize equilibrium behavior, and design two incentive mechanisms: a citation mechanism that references sources within an AI Overview, and a compensation mechanism that offers monetary rewards to creators. For both cases, we provide structural insights and near-optimal profit-maximizing mechanisms. Evaluations on real click data show that although AI Overviews harm long-term search engine profit, interventions based on our proposed mechanisms can increase long-term profit across a range of realistic scenarios, pointing toward a more sustainable trajectory for AI-enhanced search ecosystems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22493v1</guid>
      <category>cs.GT</category>
      <category>cs.IR</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yihang Wu, Jiajun Tang, Jinfei Liu, Haifeng Xu, Fan Yao</dc:creator>
    </item>
    <item>
      <title>Greedy Routing Reachability Games</title>
      <link>https://arxiv.org/abs/2601.23126</link>
      <description>arXiv:2601.23126v1 Announce Type: new 
Abstract: Today's networks consist of many autonomous entities that follow their own objectives, i.e., smart devices or parts of large AI systems, that are interconnected. Given the size and complexity of most communication networks, each entity typically only has a local view and thus must rely on a local routing protocol for sending and forwarding packets. A common solution for this is greedy routing, where packets are locally forwarded to a neighbor in the network that is closer to the packet's destination.
  In this paper we investigate a game-theoretic model with autonomous agents that aim at forming a network where greedy routing is enabled. The agents are positioned in a metric space and each agent tries to establish as few links as possible, while maintaining that it can reach every other agent via greedy routing. Thus, this model captures how greedy routing networks are formed without any assumption on the distribution of the agents or the specific employed greedy routing protocol. Hence, it distills the essence that makes greedy routing work.
  We study two variants of the model: with directed edges or with undirected edges. For the former, we show that equilibria exist, have optimal total cost, and that in Euclidean metrics they can be found efficiently. However, even for this simple setting computing optimal strategies is NP-hard. For the much more challenging setting with undirected edges, we show for the realistic setting with agents in 2D Euclidean space that the price of anarchy is between 1.75 and 1.8 and for higher dimensions it is less than 2. Also, we show that best response dynamics may cycle, but that in Euclidean space almost optimal approximate equilibria can be computed in polynomial time. Moreover, for 2D Euclidean space, these approximate equilibria outperform the well-known Delaunay triangulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.23126v1</guid>
      <category>cs.GT</category>
      <category>cs.CG</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pascal Lenzner, Paraskevi Machaira</dc:creator>
    </item>
    <item>
      <title>(Doubly) Exponential Lower Bounds for Follow the Regularized Leader in Potential Games</title>
      <link>https://arxiv.org/abs/2601.23248</link>
      <description>arXiv:2601.23248v1 Announce Type: new 
Abstract: Follow the regularized leader FTRL is the premier algorithm for online optimization. However, despite decades of research on its convergence in constrained optimization -- and potential games in particular -- its behavior remained hitherto poorly understood. In this paper, we establish that FTRL can take exponential time to converge to a Nash equilibrium in two-player potential games for any (permutation-invariant) regularizer and potentially vanishing learning rate. By known equivalences, this translates to an exponential lower bound for certain mirror descent counterparts, most notably multiplicative weights update. On the positive side, we establish the potential property for FTRL and obtain an exponential upper bound $\exp(O_{\epsilon}(1/\epsilon^2))$ for any no-regret dynamics executed in a lazy, alternating fashion, matching our lower bound up to factors in the exponent. Finally, in multi-player potential games, we show that fictitious play -- the extreme version of FTRL -- can take doubly exponential time to reach a Nash equilibrium. This constitutes an exponentially stronger lower bound for the foundational learning algorithm in games.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.23248v1</guid>
      <category>cs.GT</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ioannis Anagnostides, Ioannis Panageas, Nikolas Patris, Tuomas Sandholm</dc:creator>
    </item>
    <item>
      <title>Incentives in Federated Learning with Heterogeneous Agents</title>
      <link>https://arxiv.org/abs/2509.21612</link>
      <description>arXiv:2509.21612v2 Announce Type: replace 
Abstract: Federated learning promises significant sample-efficiency gains by pooling data across multiple agents, yet incentive misalignment is an obstacle: each update is costly to the contributor but boosts every participant. We introduce a game-theoretic framework that captures heterogeneous data: an agent's utility depends on who supplies each sample, not just how many. Agents aim to meet a PAC-style accuracy threshold at minimal personal cost. We show that uncoordinated play yields pathologies: pure equilibria may not exist, and the best equilibrium can be arbitrarily more costly than cooperation. To steer collaboration, we analyze the cost-minimizing contribution vector, prove that computing it is NP-hard, and derive a polynomial-time linear program that achieves a logarithmic approximation. Finally, pairing the LP with a simple pay what you contribute rule, where each agent receives a payment equal to its sample cost, yields a mechanism that is strategy-proof and, within the class of contribution-based transfers, is unique.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21612v2</guid>
      <category>cs.GT</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>International Conference on Learning Representations (ICLR), 2026</arxiv:journal_reference>
      <dc:creator>Ariel D. Procaccia, Han Shao, Itai Shapira</dc:creator>
    </item>
    <item>
      <title>On the Coordination of Value-Maximizing Bidders</title>
      <link>https://arxiv.org/abs/2511.04993</link>
      <description>arXiv:2511.04993v2 Announce Type: replace 
Abstract: While the auto-bidding literature predominantly considers independent bidding, we investigate the coordination problem among multiple auto-bidders in online advertising platforms. Two motivating scenarios are: collaborative bidding among multiple bidders managed by a third-party bidding agent, and strategic bid selection for multiple ad campaigns managed by a single advertiser. We formalize this coordination problem as a theoretical model and investigate the coordination mechanism where only the highest-value bidder competes with outside bidders, while other coordinated bidders refrain from competing. We demonstrate that such a coordination mechanism dominates independent bidding, improving both Return-on-Spend (RoS) compliance and the total value accrued for the participating auto-bidders or ad campaigns, for a broad class of auto-bidding algorithms. Additionally, our simulations on synthetic and real-world datasets support the theoretical result that coordination outperforms independent bidding. These findings highlight both the theoretical potential and the practical robustness of coordinated auto-bidding in online auctions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04993v2</guid>
      <category>cs.GT</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanru Guan, Jiahao Zhang, Zhe Feng, Tao Lin</dc:creator>
    </item>
    <item>
      <title>Multi-agent Adaptive Mechanism Design</title>
      <link>https://arxiv.org/abs/2512.21794</link>
      <description>arXiv:2512.21794v2 Announce Type: replace 
Abstract: We study a sequential mechanism design problem in which a principal seeks to elicit truthful reports from multiple rational agents while starting with no prior knowledge of agents' beliefs. We introduce Distributionally Robust Adaptive Mechanism (DRAM), a general framework combining insights from both mechanism design and online learning to jointly address truthfulness and cost-optimality. Throughout the sequential game, the mechanism estimates agents' beliefs and iteratively updates a distributionally robust linear program with shrinking ambiguity sets to reduce payments while preserving truthfulness. Our mechanism guarantees truthful reporting with high probability while achieving $\tilde{O}(\sqrt{T})$ cumulative regret, and we establish a matching lower bound showing that no truthful adaptive mechanism can asymptotically do better. The framework generalizes to plug-in estimators, supporting structured priors and delayed feedback. To our knowledge, this is the first adaptive mechanism under general settings that maintains truthfulness and achieves optimal regret when incentive constraints are unknown and must be learned.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21794v2</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <category>econ.TH</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiushi Han, David Simchi-Levi, Renfei Tan, Zishuo Zhao</dc:creator>
    </item>
    <item>
      <title>Inequality in Congestion Games with Learning Agents</title>
      <link>https://arxiv.org/abs/2601.20578</link>
      <description>arXiv:2601.20578v2 Announce Type: replace 
Abstract: Who benefits from expanding transport networks? While designed to improve mobility, such interventions can also create inequality. In this paper, we show that disparities arise not only from the structure of the network itself but also from differences in how commuters adapt to it. We model commuters as reinforcement learning agents who adapt their travel choices at different learning rates, reflecting unequal access to resources and information. To capture potential efficiency-fairness tradeoffs, we introduce the Price of Learning (PoL), a measure of inefficiency during learning. We analyze both a stylized network -- inspired in the well-known Braess's paradox, yet with two-source nodes -- and an abstraction of a real-world metro system (Amsterdam). Our simulations show that network expansions can simultaneously increase efficiency and amplify inequality, especially when faster learners disproportionately benefit from new routes before others adapt. These results highlight that transport policies must account not only for equilibrium outcomes but also for the heterogeneous ways commuters adapt, since both shape the balance between efficiency and fairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20578v2</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dimitris Michailidis, Sennay Ghebreab, Fernando P. Santos</dc:creator>
    </item>
    <item>
      <title>A game-theoretic probability approach to loopholes in CHSH experiments</title>
      <link>https://arxiv.org/abs/2601.09339</link>
      <description>arXiv:2601.09339v2 Announce Type: replace-cross 
Abstract: We study the CHSH inequality from an informational, timing-sensitive viewpoint using game-theoretic probability, which avoids assuming an underlying probability space. The locality loophole and the measurement-dependence (``freedom-of-choice'') loophole are reformulated as structural constraints in a sequential hidden-variable game between Scientists and Nature. We construct a loopholes-closed game with capital processes that test (i) convergence of empirical conditional frequencies to the CHSH correlations and (ii) the absence of systematic correlations between measurement settings and Nature's hidden-variable assignments, and prove that Nature cannot satisfy both simultaneously: at least one capital process must diverge. This yields an operational winning strategy for Scientists and a game-theoretic probabilistic interpretation of experimentally observed CHSH violations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09339v2</guid>
      <category>quant-ph</category>
      <category>cs.GT</category>
      <category>math.PR</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takara Nomura, Koichi Yamagata, Akio Fujiwara</dc:creator>
    </item>
  </channel>
</rss>
