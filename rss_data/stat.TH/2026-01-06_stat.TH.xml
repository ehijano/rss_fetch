<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 06 Jan 2026 05:02:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Tessellation Localized Transfer learning for nonparametric regression</title>
      <link>https://arxiv.org/abs/2601.00987</link>
      <description>arXiv:2601.00987v1 Announce Type: new 
Abstract: Transfer learning aims to improve performance on a target task by leveraging information from related source tasks. We propose a nonparametric regression transfer learning framework that explicitly models heterogeneity in the source-target relationship. Our approach relies on a local transfer assumption: the covariate space is partitioned into finitely many cells such that, within each cell, the target regression function can be expressed as a low-complexity transformation of the source regression function. This localized structure enables effective transfer where similarity is present while limiting negative transfer elsewhere. We introduce estimators that jointly learn the local transfer functions and the target regression, together with fully data-driven procedures that adapt to unknown partition structure and transfer strength. We establish sharp minimax rates for target regression estimation, showing that local transfer can mitigate the curse of dimensionality by exploiting reduced functional complexity. Our theoretical guarantees take the form of oracle inequalities that decompose excess risk into estimation and approximation terms, ensuring robustness to model misspecification. Numerical experiments illustrate the benefits of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.00987v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>H\'el\`ene Halconruy, Benjamin Bobbia, Paul Lejamtel</dc:creator>
    </item>
    <item>
      <title>Quotient EM under Misspecification:Tight Local Rates and Finite-Sample Bounds in General Integral Probability Metrics</title>
      <link>https://arxiv.org/abs/2601.01051</link>
      <description>arXiv:2601.01051v1 Announce Type: new 
Abstract: We study the expectation-maximization (EM) algorithm for general latent-variable models under (i) distributional misspecification and (ii) nonidentifiability induced by a group action. We formulate EM on the quotient parameter space and measure error using an arbitrary integral probability metric (IPM). Our main results give (a) a sharp local linear convergence rate for population EM governed by the spectral radius of the linearization on a local slice, and (b) tight finite-sample bounds for sample EM obtained via perturbed contraction inequalities and generic chaining/entropy control of EM-induced empirical processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01051v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Koustav Mallik</dc:creator>
    </item>
    <item>
      <title>Optimal estimators and tests for reciprocal effects</title>
      <link>https://arxiv.org/abs/2601.01325</link>
      <description>arXiv:2601.01325v1 Announce Type: new 
Abstract: The $p_1$ model plays a fundamental role in modeling directed networks, where the reciprocal effect parameter $\rho$ is of special interest in practice. However, due to nonlinear factors in this model, how to estimate $\rho$ efficiently is a long-standing open problem. We tackle the problem by the cycle count approach. The challenge is, due to the nonlinear factors in the model, for any given type of generalized cycles, the expected count is a complicated function of many parameters in the model, so it is unclear how to use cycle counts to estimate $\rho$. However, somewhat surprisingly, we discover that, among many types of generalized cycles with the same length, we can carefully pick a pair of them such that in the ratio between the expected cycle counts of the two types, the non-linear factors cancel out nicely with each other, and as a result, the ratio equals to $\mathrm{exp}(\rho)$ exactly. Therefore, though the expected count of cycles of any type is not tractable, the ratio between the expected cycle counts of a (carefully chosen) pair of generalized cycles may have an utterly simple form. We study to what extent such pairs exist, and use our discovery to derive both an estimate for $\rho$ and a testing procedure for testing $\rho = \rho_0$. In a setting where we allow a wide range of reciprocal effects and a wide variety of network sparsity and degree heterogeneity, we show that our estimator achieves the optimal rate and our test achieves the optimal phase transition. Technically, first, motivated by what we observe on real networks, we do not want to impose strong conditions on reciprocal effects, network sparsity, and degree heterogeneity. Second, our proposed statistic is a type of $U$-statistic, the analysis of which involves complex combinatorics and is error-prone. For these reasons, our analysis is long and delicate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01325v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qunqiang Feng, Jiashun Jin, Yaru Tian, Ting Yan</dc:creator>
    </item>
    <item>
      <title>Uniform Asymptotic Theory for Local Likelihood Estimation of Covariate-Dependent Copula Parameters</title>
      <link>https://arxiv.org/abs/2601.01345</link>
      <description>arXiv:2601.01345v1 Announce Type: new 
Abstract: Conditional copula models allow dependence structures to vary with observed covariates while preserving a separation between marginal behavior and association. We study the uniform asymptotic behavior of kernel-weighted local likelihood estimators for smoothly varying copula parameters in multivariate conditional copula models. Using a local polynomial approximation of a suitably transformed calibration function, we establish uniform convergence rates over compact covariate sets for the local log-likelihood, its score, and its Hessian. These results yield uniform consistency of the local maximum likelihood estimator and of the induced copula parameter function. The analysis is based on empirical process techniques for kernel-indexed classes with shrinking neighborhoods and polynomial entropy bounds, providing theoretical support for global consistency and stable local optimization in covariate-dependent copula models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01345v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mathias Nthiani Muia</dc:creator>
    </item>
    <item>
      <title>SGD with Dependent Data: Optimal Estimation, Regret, and Inference</title>
      <link>https://arxiv.org/abs/2601.01371</link>
      <description>arXiv:2601.01371v1 Announce Type: new 
Abstract: This work investigates the performance of the final iterate produced by stochastic gradient descent (SGD) under temporally dependent data. We consider two complementary sources of dependence: $(i)$ martingale-type dependence in both the covariate and noise processes, which accommodates non-stationary and non-mixing time series data, and $(ii)$ dependence induced by sequential decision making. Our formulation runs in parallel with classical notions of (local) stationarity and strong mixing, while neither framework fully subsumes the other. Remarkably, SGD is shown to automatically accommodate both independent and dependent information under a broad class of stepsize schedules and exploration rate schemes.
  Non-asymptotically, we show that SGD simultaneously achieves statistically optimal estimation error and regret, extending and improving existing results. In particular, our tail bounds remain sharp even for potentially infinite horizon $T=+\infty$. Asymptotically, the SGD iterates converge to a Gaussian distribution with only an $O_{\PP}(1/\sqrt{t})$ remainder, demonstrating that the supposed estimation-regret trade-off claimed in prior work can in fact be avoided. We further propose a new ``conic'' approximation of the decision region that allows the covariates to have unbounded support. For online sparse regression, we develop a new SGD-based algorithm that uses only $d$ units of storage and requires $O(d)$ flops per iteration, achieving the long term statistical optimality. Intuitively, each incoming observation contributes to estimation accuracy, while aggregated summary statistics guide support recovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01371v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yinan Shen, Yichen Zhang, Wen-Xin Zhou</dc:creator>
    </item>
    <item>
      <title>Double Machine Learning of Continuous Treatment Effects with General Instrumental Variables</title>
      <link>https://arxiv.org/abs/2601.01471</link>
      <description>arXiv:2601.01471v1 Announce Type: new 
Abstract: Estimating causal effects of continuous treatments is a common problem in practice, for example, in studying dose-response functions. Classical analyses typically assume that all confounders are fully observed, whereas in real-world applications, unmeasured confounding often persists. In this article, we propose a novel framework for local identification of dose-response functions using instrumental variables, thereby mitigating bias induced by unobserved confounders. We introduce the concept of a uniform regular weighting function and consider covering the treatment space with a finite collection of open sets. On each of these sets, such a weighting function exists, allowing us to identify the dose-response function locally within the corresponding region. For estimation, we develop an augmented inverse probability weighting score for continuous treatments under a debiased machine learning framework with instrumental variables. We further establish the asymptotic properties when the dose-response function is estimated via kernel regression or empirical risk minimization. Finally, we conduct both simulation and empirical studies to assess the finite-sample performance of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01471v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuyuan Chen, Peng Zhang, Yifan Cui</dc:creator>
    </item>
    <item>
      <title>Pathwise Representation of the Smoothing Distribution in Continuous-Time Linear Gaussian Models</title>
      <link>https://arxiv.org/abs/2601.01805</link>
      <description>arXiv:2601.01805v1 Announce Type: new 
Abstract: We study the filtering and smoothing problem for continuous-time linear Gaussian systems. While classical approaches such as the Kalman-Bucy filter and the Rauch-Tung-Striebel (RTS) smoother provide recursive formulas for the conditional mean and covariance, we present a pathwise perspective that characterizes the smoothing error dynamics as an Ornstein-Uhlenbeck process. As an application, we show that standard filtering and smoothing equations can be uniformly derived as corollaries of our main theorem. In particular, we provide the first mathematically rigorous derivation of the Bryson-Frazier smoother in the continuous-time setting. Beyond offering a more transparent understanding of the smoothing distribution, our formulation enables pathwise sampling from it, which facilitates Monte Carlo methods for evaluating nonlinear functionals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01805v1</guid>
      <category>math.ST</category>
      <category>eess.SP</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kurisaki</dc:creator>
    </item>
    <item>
      <title>On lead-lag estimation of non-synchronously observed point processes</title>
      <link>https://arxiv.org/abs/2601.01871</link>
      <description>arXiv:2601.01871v1 Announce Type: new 
Abstract: This paper introduces a new theoretical framework for analyzing lead-lag relationships between point processes, with a special focus on applications to high-frequency financial data. In particular, we are interested in lead-lag relationships between two sequences of order arrival timestamps. The seminal work of Dobrev and Schaumburg proposed model-free measures of cross-market trading activity based on cross-counts of timestamps. While their method is known to yield reliable results, it faces limitations because its original formulation inherently relies on discrete-time observations, an issue we address in this study. Specifically, we formulate the problem of estimating lead-lag relationships in two point processes as that of estimating the shape of the cross-pair correlation function (CPCF) of a bivariate stationary point process, a quantity well-studied in the neuroscience and spatial statistics literature. Within this framework, the prevailing lead-lag time is defined as the location of the CPCF's sharpest peak. Under this interpretation, the peak location in Dobrev and Schaumburg's cross-market activity measure can be viewed as an estimator of the lead-lag time in the aforementioned sense. We further propose an alternative lead-lag time estimator based on kernel density estimation and show that it possesses desirable theoretical properties and delivers superior numerical performance. Empirical evidence from high-frequency financial data demonstrates the effectiveness of our proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01871v1</guid>
      <category>math.ST</category>
      <category>q-fin.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takaaki Shiotani, Takaki Hayashi, Yuta Koike</dc:creator>
    </item>
    <item>
      <title>Modified weighted power variations of the Hermite process and applications to integrated volatility</title>
      <link>https://arxiv.org/abs/2601.02025</link>
      <description>arXiv:2601.02025v1 Announce Type: new 
Abstract: We study the asymptotic behaviour of modified weighted power variations of the Hermite process of arbitrary order. By selecting suitable "good" increments and exploiting their decomposition into dominant independent components, we establish a central limit theorem for weighted $p$-variations using tools from Stein-Malliavin calculus. Our results extend previous works on modified quadratic and wavelet-based variations to general powers and to weighted settings, with explicit bounds in Wasserstein distance. We further apply these limit theorems to construct asymptotically Gaussian estimators of integrated volatility in Hermite-driven models, thereby extending fBm-based methods to non-Gaussian settings. The last part of our work contains numerical simulations which illustrate the practical performance of the proposed estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.02025v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antoine Ayache, laurent Loosveldt, Ciprian Tudor</dc:creator>
    </item>
    <item>
      <title>Convergence of the EM algorithm via proximal techniques</title>
      <link>https://arxiv.org/abs/2601.02252</link>
      <description>arXiv:2601.02252v1 Announce Type: new 
Abstract: We investigate convergence of the expectation maximization algorithm by representing it as a generalized proximal method. Convergence of iterates and not just in value is investigated under natural hypotheses such as definability of the incomplete data log-likelihood in the sense of o-minimal structure theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.02252v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dominikus Noll</dc:creator>
    </item>
    <item>
      <title>On Statistical Inference for Rates of Change in Spatial Processes over Riemannian Manifolds</title>
      <link>https://arxiv.org/abs/2601.02305</link>
      <description>arXiv:2601.02305v1 Announce Type: new 
Abstract: Statistical inference for spatial processes from partially realized or scattered data has seen voluminous developments in diverse areas ranging from environmental sciences to business and economics. Inference on the associated rates of change has seen some recent developments. The literature has been restricted to Euclidean domains, where inference is sought on directional derivatives, rates along a chosen direction of interest, at arbitrary locations. Inference for higher order rates, particularly directional curvature has also proved useful in these settings. Modern spatial data often arise from non-Euclidean domains. This manuscript particularly considers spatial processes defined over compact Riemannian manifolds. We develop a comprehensive inferential framework for spatial rates of change for such processes over vector fields. In doing so, we formalize smoothness of process realizations and construct differential processes -- the derivative and curvature processes. We derive conditions for kernels that ensure the existence of these processes and establish validity of the joint multivariate process consisting of the ``parent'' Gaussian process (GP) over the manifold and the associated differential processes. Predictive inference on these rates is devised conditioned on the realized process over the manifold. Manifolds arise as polyhedral meshes in practice. The success of our simulation experiments for assessing derivatives for processes observed over such meshes validate our theoretical findings. By enhancing our understanding of GPs on manifolds, this manuscript unlocks a variety of potential applications in machine learning and statistics where GPs have seen wide usage. We propose a fully model-based approach to inference on the differential processes arising from a spatial process from partially observed or realized data across scattered location on a manifold.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.02305v1</guid>
      <category>math.ST</category>
      <category>math.DG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Didong Li, Aritra Halder, Sudipto Banerjee</dc:creator>
    </item>
    <item>
      <title>Beyond Demand Estimation: Consumer Surplus Evaluation via Cumulative Propensity Weights</title>
      <link>https://arxiv.org/abs/2601.01029</link>
      <description>arXiv:2601.01029v1 Announce Type: cross 
Abstract: This paper develops a practical framework for using observational data to audit the consumer surplus effects of AI-driven decisions, specifically in targeted pricing and algorithmic lending. Traditional approaches first estimate demand functions and then integrate to compute consumer surplus, but these methods can be challenging to implement in practice due to model misspecification in parametric demand forms and the large data requirements and slow convergence of flexible nonparametric or machine learning approaches. Instead, we exploit the randomness inherent in modern algorithmic pricing, arising from the need to balance exploration and exploitation, and introduce an estimator that avoids explicit estimation and numerical integration of the demand function. Each observed purchase outcome at a randomized price is an unbiased estimate of demand and by carefully reweighting purchase outcomes using novel cumulative propensity weights (CPW), we are able to reconstruct the integral. Building on this idea, we introduce a doubly robust variant named the augmented cumulative propensity weighting (ACPW) estimator that only requires one of either the demand model or the historical pricing policy distribution to be correctly specified. Furthermore, this approach facilitates the use of flexible machine learning methods for estimating consumer surplus, since it achieves fast convergence rates by incorporating an estimate of demand, even when the machine learning estimate has slower convergence rates. Neither of these estimators is a standard application of off-policy evaluation techniques as the target estimand, consumer surplus, is unobserved. To address fairness, we extend this framework to an inequality-aware surplus measure, allowing regulators and firms to quantify the profit-equity trade-off. Finally, we validate our methods through comprehensive numerical studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01029v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zeyu Bian, Max Biggs, Ruijiang Gao, Zhengling Qi</dc:creator>
    </item>
    <item>
      <title>Central limit theorem for a partially observed interacting system of Hawkes processes I: subcritical case</title>
      <link>https://arxiv.org/abs/2601.01189</link>
      <description>arXiv:2601.01189v1 Announce Type: cross 
Abstract: We consider a system of $N$ Hawkes processes and observe the actions of a subpopulation of size $K \le N$ up to time $t$, where $K$ is large. The influence relationships between each pair of individuals are modeled by i.i.d.Bernoulli($p$) random variables, where $p \in [0,1]$ is an unknown parameter. Each individual acts at a {\it baseline} rate $\mu &gt; 0$ and, additionally, at an {\it excitation} rate of the form $N^{-1} \sum_{j=1}^{N} \theta_{ij} \int_{0}^{t} \phi(t-s)\,dZ_s^{j,N}$, which depends on the past actions of all individuals that influence it, scaled by $N^{-1}$ (i.e. the mean-field type), with the influence of older actions discounted through a memory kernel $\phi \colon \mathbb{R}{+} \to \mathbb{R}{+}$. Here, $\mu$ and $\phi$ are treated as nuisance parameters. The aim of this paper is to establish a central limit theorem for the estimator of $p$ proposed in \cite{D}, under the subcritical condition $\Lambda p &lt; 1$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01189v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>q-fin.MF</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenguang Liu, Liping Xu, An Zhang</dc:creator>
    </item>
    <item>
      <title>Order-Constrained Spectral Causality in Multivariate Time Series</title>
      <link>https://arxiv.org/abs/2601.01216</link>
      <description>arXiv:2601.01216v1 Announce Type: cross 
Abstract: We introduce an operator-theoretic framework for causal analysis in multivariate time series based on order-constrained spectral non-invariance. Directional influence is defined as sensitivity of second-order dependence operators to admissible, order-preserving temporal deformations of a designated source component, yielding an intrinsically multivariate causal notion summarized through orthogonally invariant spectral functionals. Under linear Gaussian assumptions, the criterion coincides with linear Granger causality, while beyond this regime it captures collective and nonlinear directional dependence not reflected in pairwise predictability. We establish existence, uniform consistency, and valid inference for the resulting non-smooth supremum--infimum statistics using shift-based randomization that exploits order-induced group invariance, yielding finite-sample exactness under exact invariance and asymptotic validity under weak dependence without parametric assumptions. Simulations demonstrate correct size and strong power against distributed and bulk-dominated alternatives, including nonlinear dependence missed by linear Granger tests with appropriate feature embeddings. An empirical application to a high-dimensional panel of daily financial return series spanning major asset classes illustrates system-level causal monitoring in practice. Directional organization is episodic and stress-dependent, causal propagation strengthens while remaining multi-channel, dominant causal hubs reallocate rapidly, and statistically robust transmission channels are sparse and horizon-heterogeneous even when aggregate lead--lag asymmetry is weak. The framework provides a scalable and interpretable complement to correlation-, factor-, and pairwise Granger-style analyses for complex systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01216v1</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>q-fin.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alejandro Rodriguez Dominguez</dc:creator>
    </item>
    <item>
      <title>A Novel Multiple Imputation Approach For Parameter Estimation in Observation-Driven Time Series Models With Missing Data</title>
      <link>https://arxiv.org/abs/2601.01259</link>
      <description>arXiv:2601.01259v1 Announce Type: cross 
Abstract: Handling missing data in time series is a complex problem due to the presence of temporal dependence. General-purpose imputation methods, while widely used, often distort key statistical properties of the data, such as variance and dependence structure, leading to biased estimation and misleading inference. These issues become more pronounced in models that explicitly rely on capturing serial dependence, as standard imputation techniques fail to preserve the underlying dynamics. This paper proposes a novel multiple imputation method specifically designed for parameter estimation in observation-driven models (ODM). The approach takes advantage of the iterative nature of the systematic component in ODM to propagate the dependence structure through missing data, minimizing its impact on estimation. Unlike traditional imputation techniques, the proposed method accommodates continuous, discrete, and mixed-type data while preserving key distributional and dependence properties. We evaluate its performance through Monte Carlo simulations in the context of GARMA models, considering time series with up to 70\% missing data. An application to the proportion of stocked energy stored in South Brazil further demonstrates its practical utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01259v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Guilherme Pumi, Taiane Schaedler Prassand Douglas Krauthein Verdum</dc:creator>
    </item>
    <item>
      <title>Statistical inference for highly correlated stationary point processes and noisy bivariate Neyman-Scott processes</title>
      <link>https://arxiv.org/abs/2410.05732</link>
      <description>arXiv:2410.05732v3 Announce Type: replace 
Abstract: Motivated by estimating the lead-lag relationships in high-frequency financial data, we propose noisy bivariate Neyman-Scott point processes with gamma kernels (NBNSP-G). NBNSP-G tolerates noises that are not necessarily Poissonian and has an intuitive interpretation. Our experiments suggest that NBNSP-G can explain the correlation of orders of two stocks well. A composite-type quasi-likelihood is employed to estimate the parameters of the model. However, when one tries to prove consistency and asymptotic normality, NBNSP-G breaks the boundedness assumption on the moment density functions commonly assumed in the literature. Therefore, under more relaxed conditions, we show consistency and asymptotic normality for bivariate point process models, which include NBNSP-G. Our numerical simulations also show that the estimator is indeed likely to converge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05732v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takaaki Shiotani, Nakahiro Yoshida</dc:creator>
    </item>
    <item>
      <title>Attainability of Two-Point Testing Rates for Finite-Sample Location Estimation</title>
      <link>https://arxiv.org/abs/2502.05730</link>
      <description>arXiv:2502.05730v3 Announce Type: replace 
Abstract: Le Cam's two-point testing method yields perhaps the simplest lower bound for estimating the mean of a distribution: roughly, if it is impossible to well-distinguish a distribution centered at $\mu$ from the same distribution centered at $\mu+\Delta$, then it is impossible to estimate the mean by better than $\Delta/2$. It is setting-dependent whether or not a nearly matching upper bound is attainable. We study the conditions under which the two-point testing lower bound can be attained for univariate mean estimation; both in the setting of location estimation (where the distribution is known up to translation) and adaptive location estimation (unknown distribution). Roughly, we will say an estimate nearly attains the two-point testing lower bound if it incurs error that is at most polylogarithmically larger than the Hellinger modulus of continuity for $\tilde{\Omega}(n)$ samples.
  Adaptive location estimation is particularly interesting as some distributions admit much better guarantees than sub-Gaussian rates (e.g. $\operatorname{Unif}(\mu-1,\mu+1)$ permits error $\Theta(\frac{1}{n})$, while the sub-Gaussian rate is $\Theta(\frac{1}{\sqrt{n}})$), yet it is not obvious whether these rates may be adaptively attained by one unified approach. Our main result designs an algorithm that nearly attains the two-point testing rate for mixtures of symmetric, log-concave distributions with a common mean. Moreover, this algorithm runs in near-linear time and is parameter-free. In contrast, we show the two-point testing rate is not nearly attainable even for symmetric, unimodal distributions.
  We complement this with results for location estimation, showing the two-point testing rate is nearly attainable for unimodal distributions, but unattainable for symmetric distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05730v3</guid>
      <category>math.ST</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Spencer Compton, Gregory Valiant</dc:creator>
    </item>
    <item>
      <title>Kullback-Leibler Consistency of $p$-dimensional P\'olya Tree Posteriors and Differential Entropy Estimation</title>
      <link>https://arxiv.org/abs/2504.02950</link>
      <description>arXiv:2504.02950v2 Announce Type: replace 
Abstract: We exploit the multiplicative structure of P\'olya Tree priors to establish novel consistency results on $p$-dimensional trees, conditions to obtain Kullback-Leibler minimax contraction rates for univariate density estimation and a representation theorem of entropy functionals of P\'olya Tree posteriors. These results motivate a novel differential entropy estimator that is consistent under mild conditions on large dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02950v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fernando Corr\^ea, Rafael Bassi Stern, Julio Michael Stern</dc:creator>
    </item>
    <item>
      <title>Information geometry of L\'evy processes and financial models</title>
      <link>https://arxiv.org/abs/2507.23646</link>
      <description>arXiv:2507.23646v2 Announce Type: replace 
Abstract: We explore the information geometry of L\'evy processes. As a starting point, we derive the $\alpha$-divergence between two L\'evy processes. Subsequently, the Fisher information matrix and the $\alpha$-connection associated with the geometry of L\'evy processes are computed from the $\alpha$-divergence. In addition, we discuss statistical applications of this information geometry. As illustrative examples, we investigate the differential-geometric structures of various L\'evy processes relevant to financial modeling, including tempered stable processes, the CGMY model, and variance gamma processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23646v2</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.DG</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>q-fin.MF</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jaehyung Choi</dc:creator>
    </item>
    <item>
      <title>Sample Path Regularity of Gaussian Processes from the Covariance Kernel</title>
      <link>https://arxiv.org/abs/2312.14886</link>
      <description>arXiv:2312.14886v3 Announce Type: replace-cross 
Abstract: Gaussian processes (GPs) are the most common formalism for defining probability distributions over spaces of functions. While applications of GPs are myriad, a comprehensive understanding of GP sample paths, i.e. the function spaces over which they define a probability measure, is lacking. In practice, GPs are not constructed through a probability measure, but instead through a mean function and a covariance kernel. In this paper we provide necessary and sufficient conditions on the covariance kernel for the sample paths of the corresponding GP to attain a given regularity. We focus primarily on H\"older regularity as it grants particularly straightforward conditions, which simplify further in the cases of stationary and isotropic GPs. We then demonstrate that our results allow for novel and unusually tight characterisations of the sample path regularities of the GPs commonly used in machine learning applications, such as the Mat\'ern GPs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14886v3</guid>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Natha\"el Da Costa, Marvin Pf\"ortner, Lancelot Da Costa, Philipp Hennig</dc:creator>
    </item>
    <item>
      <title>Geometry-induced Regularization in Deep ReLU Neural Networks</title>
      <link>https://arxiv.org/abs/2402.08269</link>
      <description>arXiv:2402.08269v2 Announce Type: replace-cross 
Abstract: Neural networks with a large number of parameters often do not overfit, owing to implicit regularization that favors \lq good\rq{} networks. Other related and puzzling phenomena include properties of flat minima, saddle-to-saddle dynamics, and neuron alignment. To investigate these phenomena, we study the local geometry of deep ReLU neural networks. We show that, for a fixed architecture, as the weights vary, the image of a sample $X$ forms a set whose local dimension changes. The parameter space is partitioned into regions where this local dimension remains constant. The local dimension is invariant under the natural symmetries of ReLU networks (i.e., positive rescalings and neuron permutations). We establish then that the network's geometry induces a regularization, with the local dimension serving as a key measure of regularity. Moreover, we relate the local dimension to a new notion of flatness of minima and to saddle-to-saddle dynamics. For shallow networks, we also show that the local dimension is connected to the number of linear regions perceived by $X$, offering insight into the effects of regularization. This is further supported by experiments and linked to neuron alignment. Our analysis offers, for the first time, a simple and unified geometric explanation that applies to all learning contexts for these phenomena, which are usually studied in isolation. Finally, we explore the practical computation of the local dimension and present experiments on the MNIST dataset, which highlight geometry-induced regularization in this setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08269v2</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joachim Bona-Pellissier (MaLGA), Fran\c{c}ois Malgouyres (IMT), Fran\c{c}ois Bachoc (LPP)</dc:creator>
    </item>
    <item>
      <title>Consistency for Large Neural Networks: Regression and Classification</title>
      <link>https://arxiv.org/abs/2409.14123</link>
      <description>arXiv:2409.14123v3 Announce Type: replace-cross 
Abstract: Although overparameterized models have achieved remarkable practical success, their theoretical properties, particularly their generalization behavior, remain incompletely understood. The well known double descents phenomenon suggests that the test error curve of neural networks decreases monotonically as model size grows and eventually converges to a non-zero constant. This work aims to explain the theoretical mechanism underlying this tail behavior and study the statistical consistency of deep overparameterized neural networks in many different learning tasks including regression and classification. Firstly, we prove that as the number of parameters increases, the approximation error decreases monotonically, while explicit or implicit regularization (e.g., weight decay) keeps the generalization error existing but bounded. Consequently, the overall error curve eventually converges to a constant determined by the bounded generalization error and the optimization error. Secondly, we prove that deep overparameterized neural networks are statistical consistency across multiple learning tasks if regularization technique is used. Our theoretical findings coincide with numerical experiments and provide a perspective for understanding the generalization behavior of overparameterized neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14123v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoran Zhan, Yingcun Xia</dc:creator>
    </item>
    <item>
      <title>Group lasso based selection for high-dimensional mediation analysis</title>
      <link>https://arxiv.org/abs/2409.20036</link>
      <description>arXiv:2409.20036v2 Announce Type: replace-cross 
Abstract: Mediation analysis aims to identify and estimate the effect of an exposure on an outcome that is mediated through one or more intermediate variables. In the presence of multiple intermediate variables, two pertinent methodological questions arise: estimating mediated effects when mediators are correlated, and performing high-dimensional mediation analyses when the number of mediators exceeds the sample size. This paper presents a two-step procedure for high-dimensional mediation analyses. The first step selects a reduced number of candidate mediators using an ad-hoc lasso penalty. The second step applies a procedure we previously developed to estimate the mediated effects, accounting for the correlation structure among the retained candidate mediators. We compare the performance of the proposed two-step procedure with state-of-the-art methods using simulated data. Additionally, we demonstrate its practical application by estimating the causal role of DNA methylation in the pathway between smoking and rheumatoid arthritis using real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20036v2</guid>
      <category>q-bio.QM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Allan J\'erolon (CIC - Antilles Guyane, MAP5 - UMR 8145), Flora Alarcon (MAP5 - UMR 8145), Florence Pittion (TIMC), Magali Richard (TIMC), Olivier Fran\c{c}ois (TIMC), Etienne E. Birmel\'e (IRMA), Vittorio Perduca (MAP5 - UMR 8145)</dc:creator>
    </item>
    <item>
      <title>Data integration using covariate summaries from external sources</title>
      <link>https://arxiv.org/abs/2411.15691</link>
      <description>arXiv:2411.15691v3 Announce Type: replace-cross 
Abstract: In modern data analysis, information is frequently collected from multiple sources, often leading to challenges such as data heterogeneity and imbalanced sample sizes across datasets. Robust and efficient data integration methods are crucial for improving the generalization and transportability of statistical findings. In this work, we address scenarios where, in addition to having full access to individualized data from a primary source, supplementary covariate information from external sources is also available. While traditional data integration methods typically require individualized covariates from external sources, such requirements can be impractical due to limitations related to accessibility, privacy, storage, and cost. Instead, we propose novel data integration techniques that rely solely on external summary statistics, such as sample means and covariances, to construct robust estimators for the mean outcome under both homogeneous and heterogeneous data settings. Additionally, we extend this framework to causal inference, enabling the estimation of average treatment effects for both generalizability and transportability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15691v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Facheng Yu, Zhen Qi, Yuqian Zhang</dc:creator>
    </item>
    <item>
      <title>Model non-collapse: Minimax bounds for recursive discrete distribution estimation</title>
      <link>https://arxiv.org/abs/2501.19273</link>
      <description>arXiv:2501.19273v4 Announce Type: replace-cross 
Abstract: Learning discrete distributions from i.i.d. samples is a well-understood problem. However, advances in generative machine learning prompt an interesting new, non-i.i.d. setting: after receiving a certain number of samples, an estimated distribution is fixed, and samples from this estimate are drawn and introduced into the sample corpus, undifferentiated from real samples. Subsequent generations of estimators now face contaminated environments, a scenario referred to in the machine learning literature as self-consumption. Empirically, it has been observed that models in fully synthetic self-consuming loops collapse -- their performance deteriorates with each batch of training -- but accumulating data has been shown to prevent complete degeneration. This, in turn, begs the question: What happens when fresh real samples \textit{are} added at every stage? In this paper, we study the minimax loss of self-consuming discrete distribution estimation in such loops. We show that even when model collapse is consciously averted, the ratios between the minimax losses with and without source information can grow unbounded as the batch size increases. In the data accumulation setting, where all batches of samples are available for estimation, we provide minimax lower bounds and upper bounds that are order-optimal under mild conditions for the expected $\ell_2^2$ and $\ell_1$ losses at every stage. We provide conditions for regimes where there is a strict gap in the convergence rates compared to the corresponding oracle-assisted minimax loss where real and synthetic samples are differentiated, and provide examples where this gap is easily observed. We also provide a lower bound on the minimax loss in the data replacement setting, where only the latest batch of samples is available, and use it to find a lower bound for the worst-case loss for bounded estimate trajectories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19273v4</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TIT.2025.3649611</arxiv:DOI>
      <dc:creator>Millen Kanabar, Michael Gastpar</dc:creator>
    </item>
    <item>
      <title>Exploratory Hierarchical Factor Analysis with an Application to Psychological Measurement</title>
      <link>https://arxiv.org/abs/2505.09043</link>
      <description>arXiv:2505.09043v3 Announce Type: replace-cross 
Abstract: Hierarchical factor models, which include the bifactor model as a special case, are useful in social and behavioural sciences for measuring hierarchically structured constructs. Specifying a hierarchical factor model involves imposing hierarchically structured zero constraints on a factor loading matrix, which is often challenging. Therefore, an exploratory analysis is needed to learn the hierarchical factor structure from data. Unfortunately, there does not exist an identifiability theory for the learnability of this hierarchical structure, nor a computationally efficient method with provable performance. The method of Schmid-Leiman transformation, which is often regarded as the default method for exploratory hierarchical factor analysis, is flawed and likely to fail. The contribution of this paper is three-fold. First, an identifiability result is established for general hierarchical factor models, which shows that the hierarchical factor structure is learnable under mild regularity conditions. Second, a computationally efficient divide-and-conquer approach is proposed for learning the hierarchical factor structure. Finally, asymptotic theory is established for the proposed method, showing that it can consistently recover the true hierarchical factor structure as the sample size grows to infinity. The power of the proposed method is shown via simulation studies and a real data application to a personality test. The computation code for the proposed method is publicly available at https://github.com/EmetSelch97/EHFA/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09043v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiawei Qiao, Yunxiao Chen, Zhiliang Ying</dc:creator>
    </item>
    <item>
      <title>A Linear Approach to Data Poisoning</title>
      <link>https://arxiv.org/abs/2505.15175</link>
      <description>arXiv:2505.15175v3 Announce Type: replace-cross 
Abstract: Backdoor and data-poisoning attacks can flip predictions with tiny training corruptions, yet a sharp theory linking poisoning strength, overparameterization, and regularization is lacking. We analyze ridge least squares with an unpenalized intercept in the high-dimensional regime \(p,n\to\infty\), \(p/n\to c\). Targeted poisoning is modelled by shifting a \(\theta\)-fraction of one class by a direction \(\mathbf{v}\) and relabelling. Using resolvent techniques and deterministic equivalents from random matrix theory, we derive closed-form limits for the poisoned score explicit in the model parameters. The formulas yield scaling laws, recover the interpolation threshold as \(c\to1\) in the ridgeless limit, and show that the weights align with the poisoning direction. Synthetic experiments match theory across sweeps of the parameters and MNIST backdoor tests show qualitatively consistent trends. The results provide a tractable framework for quantifying poisoning in linear models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15175v3</guid>
      <category>stat.ML</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Donald Flynn, Diego Granziol</dc:creator>
    </item>
    <item>
      <title>GRAND: Graph Release with Assured Node Differential Privacy</title>
      <link>https://arxiv.org/abs/2507.00402</link>
      <description>arXiv:2507.00402v3 Announce Type: replace-cross 
Abstract: Differential privacy is a well-established framework for safeguarding sensitive information in data. While extensively applied across various domains, its application to network data -- particularly at the node level -- remains underexplored. Existing methods for node-level privacy either focus exclusively on query-based approaches, which restrict output to pre-specified network statistics, or fail to preserve key structural properties of the network. In this work, we propose GRAND (Graph Release with Assured Node Differential privacy), which is, to the best of our knowledge, the first network release mechanism that releases networks while ensuring node-level differential privacy and preserving structural properties. Under a broad class of latent space models, we show that the released network asymptotically follows the same distribution as the original network. The effectiveness of the approach is evaluated through extensive experiments on both synthetic and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00402v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Suqing Liu, Xuan Bi, Tianxi Li</dc:creator>
    </item>
    <item>
      <title>Ideal Observer for Segmentation of Dead Leaves Images</title>
      <link>https://arxiv.org/abs/2512.05539</link>
      <description>arXiv:2512.05539v2 Announce Type: replace-cross 
Abstract: The human visual environment is comprised of different surfaces that are distributed in space. The parts of a scene that are visible at any one time are governed by the occlusion of overlapping objects. In this work we consider "dead leaves" models, which replicate these occlusions when generating images by layering objects on top of each other. A dead leaves model is a generative model comprised of distributions for object position, shape, color and texture. An image is generated from a dead leaves model by sampling objects ("leaves") from these distributions until a stopping criterion is reached, usually when the image is fully covered or until a given number of leaves was sampled. Here, we describe a theoretical approach, based on previous work, to derive a Bayesian ideal observer for the partition of a given set of pixels based on independent dead leaves model distributions. Extending previous work, we provide step-by-step explanations for the computation of the posterior probability as well as describe factors that determine the feasibility of practically applying this computation. The dead leaves image model and the associated ideal observer can be applied to study segmentation decisions in a limited number of pixels, providing a principled upper-bound on performance, to which humans and vision algorithms could be compared.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05539v2</guid>
      <category>cs.CV</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Swantje Mahncke, Malte Ott</dc:creator>
    </item>
    <item>
      <title>Asymptotic Inference for Constrained Regression</title>
      <link>https://arxiv.org/abs/2512.12953</link>
      <description>arXiv:2512.12953v2 Announce Type: replace-cross 
Abstract: We consider statistical inference in high-dimensional regression problems under affine constraints on the parameter space. The theoretical study of this is motivated by the study of genetic determinants of diseases, such as diabetes, using external information from mediating protein expression levels. Specifically, we develop rigorous methods for estimating genetic effects on diabetes-related continuous outcomes when these associations are constrained based on external information about genetic determinants of proteins, and genetic relationships between proteins and the outcome of interest. In this regard, we discuss multiple candidate estimators and study their theoretical properties, sharp large sample optimality, and numerical qualities under a high-dimensional proportional asymptotic framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12953v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Madhav Sankaranarayanan, Yana Hrytsenko, Jerome I. Rotter, Tamar Sofer, Rajarshi Mukherjee</dc:creator>
    </item>
    <item>
      <title>Sharp Structure-Agnostic Lower Bounds for General Linear Functional Estimation</title>
      <link>https://arxiv.org/abs/2512.17341</link>
      <description>arXiv:2512.17341v2 Announce Type: replace-cross 
Abstract: We establish a general statistical optimality theory for estimation problems where the target parameter is a linear functional of an unknown nuisance component that must be estimated from data. This formulation covers many causal and predictive parameters and has applications to numerous disciplines. We adopt the structure-agnostic framework introduced by \citet{balakrishnan2023fundamental}, which poses no structural properties on the nuisance functions other than access to black-box estimators that achieve some statistical estimation rate. This framework is particularly appealing when one is only willing to consider estimation strategies that use non-parametric regression and classification oracles as black-box sub-processes. Within this framework, we first prove the statistical optimality of the celebrated and widely used doubly robust estimators for the Average Treatment Effect (ATE), the most central parameter in causal inference. We then characterize the minimax optimal rate under the general formulation. Notably, we differentiate between two regimes in which double robustness can and cannot be achieved and in which first-order debiasing yields different error rates. Our result implies that first-order debiasing is simultaneously optimal in both regimes. We instantiate our theory by deriving optimal error rates that recover existing results and extend to various settings of interest, including the case when the nuisance is defined by generalized regressions and when covariate shift exists for training and test distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17341v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jikai Jin, Vasilis Syrgkanis</dc:creator>
    </item>
    <item>
      <title>Subgroup Discovery with the Cox Model</title>
      <link>https://arxiv.org/abs/2512.20762</link>
      <description>arXiv:2512.20762v2 Announce Type: replace-cross 
Abstract: We study the problem of subgroup discovery for survival analysis, where the goal is to find an interpretable subset of the data on which a Cox model is highly accurate. Our work is the first to study this particular subgroup problem, for which we make several contributions.
  Subgroup discovery methods generally require a "quality function" in order to sift through and select the most advantageous subgroups. We first examine why existing natural choices for quality functions are insufficient to solve the subgroup discovery problem for the Cox model. To address the shortcomings of existing metrics, we introduce two technical innovations: the *expected prediction entropy (EPE)*, a novel metric for evaluating survival models which predict a hazard function; and the *conditional rank statistics (CRS)*, a statistical object which quantifies the deviation of an individual point to the distribution of survival times in an existing subgroup. We study the EPE and CRS theoretically and show that they can solve many of the problems with existing metrics.
  We introduce a total of eight algorithms for the Cox subgroup discovery problem. The main algorithm is able to take advantage of both the EPE and the CRS, allowing us to give theoretical correctness results for this algorithm in a well-specified setting. We evaluate all of the proposed methods empirically on both synthetic and real data. The experiments confirm our theory, showing that our contributions allow for the recovery of a ground-truth subgroup in well-specified cases, as well as leading to better model fit compared to naively fitting the Cox model to the whole dataset in practical settings. Lastly, we conduct a case study on jet engine simulation data from NASA. The discovered subgroups uncover known nonlinearities/homogeneity in the data, and which suggest design choices which have been mirrored in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20762v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zachary Izzo, Iain Melvin</dc:creator>
    </item>
  </channel>
</rss>
