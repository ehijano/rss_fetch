<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 30 Jan 2025 02:30:37 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Dynamic Metadata Schemes in the Neutron and Photon Science Communities: A Case Study of X-Ray Photon Correlation Spectroscopy</title>
      <link>https://arxiv.org/abs/2501.16814</link>
      <description>arXiv:2501.16814v1 Announce Type: new 
Abstract: Metadata is one of the most important aspects for advancing data management practices within all research communities. Definitions and schemes of metadata are inter alia of particular significance in the domain of neutron and photon scattering experiments covering a broad area of different scientific disciplines. The demand of describing continuously evolving highly nonstandardized experiments, including the resulting processed and published data, constitutes a considerable challenge for a static definition of metadata. Here, we present the concept of dynamic metadata for the neutron and photon scientific community, which enriches a static set of defined basic metadata. We explore the idea of dynamic metadata with the help of the use case of X-ray Photon Correlation Spectroscopy (XPCS), which is a synchrotron-based scattering technique that allows the investigation of nanoscale dynamic processes. It serves here as a demonstrator of how dynamic metadata can improve data acquisition, sharing, and analysis workflows. Our approach enables researchers to tailor metadata definitions dynamically and adapt them to the evolving demands of describing data and results from a diverse set of experiments. We demonstrate that dynamic metadata standards yield advantages that enhance data reproducibility, interoperability, and the dissemination of knowledge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16814v1</guid>
      <category>physics.data-an</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Engineering and Technology International Journal of Computer and Information Engineering, Vol:18, No:5, 2024</arxiv:journal_reference>
      <dc:creator>Amir Tosson, Mohammad Reza, Christian Gutt</dc:creator>
    </item>
    <item>
      <title>Two measurement bases are asymptotically informationally complete for any pure state tomography</title>
      <link>https://arxiv.org/abs/2501.17061</link>
      <description>arXiv:2501.17061v1 Announce Type: cross 
Abstract: One of the fundamental questions in quantum information theory is to find how many measurement bases are required to obtain the full information of a quantum state. While a minimum of four measurement bases is typically required to determine an arbitrary pure state, we prove that for any states generated by finite-depth Clifford + T circuits, just two measurement bases are sufficient. More generally, we prove that two measurement bases are informationally complete for determining algebraic pure states whose state-vector elements represented in the computational basis are algebraic numbers. Since any pure state can be asymptotically approximated by a sequence of algebraic states with arbitrarily high precision, our scheme is referred to as asymptotically informationally complete for pure state tomography. Furthermore, existing works mostly construct the measurements using entangled bases. So far, the best result requires $O(n)$ local measurement bases for $n$-qubit pure-state tomography. Here, we show that two measurement bases that involve polynomial elementary gates are sufficient for uniquely determining sparse algebraic states. Moreover, we prove that two local measurement bases, involving single-qubit local operations only, are informationally complete for certain algebraic states, such as GHZ-like and W-like states. Besides, our two-measurement-bases scheme remains valid for mixed states with certain types of noises. We numerically test the uniqueness of the reconstructed states under two (local) measurement bases with and without measurement and depolarising types of noise. Our scheme provides a theoretical guarantee for pure state tomography in the fault-tolerant quantum computing regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17061v1</guid>
      <category>quant-ph</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianfeng Feng, Tianqi Xiao, Yu Wang, Shengshi Pang, Farhan Hanif, Xiaoqi Zhou, Qi Zhao, M. S. Kim, Jinzhao Sun</dc:creator>
    </item>
    <item>
      <title>Approximation of High-Dimensional Gibbs Distributions with Functional Hierarchical Tensors</title>
      <link>https://arxiv.org/abs/2501.17143</link>
      <description>arXiv:2501.17143v2 Announce Type: cross 
Abstract: The numerical representation of high-dimensional Gibbs distributions is challenging due to the curse of dimensionality manifesting through the intractable normalization constant calculations. This work addresses this challenge by performing a particle-based high-dimensional parametric density estimation subroutine, and the input to the subroutine is Gibbs samples generated by leveraging advanced sampling techniques. Specifically, to generate Gibbs samples, we employ ensemble-based annealed importance sampling, a population-based approach for sampling multimodal distributions. These samples are then processed using functional hierarchical tensor sketching, a tensor-network-based density estimation method for high-dimensional distributions, to obtain the numerical representation of the Gibbs distribution. We successfully apply the proposed approach to complex Ginzburg-Landau models with hundreds of variables. In particular, we show that the approach proposed is successful at addressing the metastability issue under difficult numerical cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17143v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nan Sheng, Xun Tang, Haoxuan Chen, Lexing Ying</dc:creator>
    </item>
    <item>
      <title>A General Track Fit based on Triplets</title>
      <link>https://arxiv.org/abs/2406.05240</link>
      <description>arXiv:2406.05240v2 Announce Type: replace-cross 
Abstract: This paper presents a general three-dimensional track fit based on hit triplets.
  The general track fit considers spatial hit and multiple Coulomb scattering uncertainties, and can also be extended to include energy losses.
  Input to the fit are detector-specific triplet parameters, which contain information about
  the triplet geometry (hit positions),
  the radiation length of the material and the magnetic field.
  Since the solution is given by an analytical closed-form, it is possible to use the same fitting code for all kind of tracking detectors.
  Fitting formulas are given for the global track fit as well as for the local hit triplets.
  The latter allows filtering out triplets with poor fit quality at an early stage of track reconstruction.
  The construction and fit of local triplets is fully parallelizable, enabling accelerated computation with parallel hardware architectures.
  Formulas for the detector-specific triplet parameters are derived for the two most commonly used field configuration for tracking detectors, namely a uniform solenoidal field and gap spectrometer dipole.
  An algorithm to calculate the triplet parameters for an arbitrary magnetic field configuration is presented too.
  This paper also includes a discussion of inherent track fit biases.
  Furthermore, a new method is proposed to accelerate track fitting by classifying tracking regimes and using optimal fit formulas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05240v2</guid>
      <category>physics.ins-det</category>
      <category>hep-ex</category>
      <category>nucl-ex</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andre Sch\"oning</dc:creator>
    </item>
    <item>
      <title>Doppler correlation-driven vetoes for the Frequency Hough analysis in continuous gravitational-wave searches</title>
      <link>https://arxiv.org/abs/2410.19420</link>
      <description>arXiv:2410.19420v3 Announce Type: replace-cross 
Abstract: We present an improved method for vetoing candidates of continuous gravitational-wave sources during all-sky searches utilizing the Frequency Hough pipeline. This approach leverages linear correlations between source parameters induced by the Earth Doppler effect, which can be effectively identified through the Hough Transform. Candidates that do not align with these patterns are considered spurious and can thus be vetoed, enhancing the depth and statistical significance of follow-up analyses. Additionally, we provide a comprehensive explanation of the method calibration, which intrinsically linked to the total duration of the observing run. On average, the procedure successfully vetoes $56\%$ of candidates. To assess the method performance, we conducted a Monte-Carlo simulation injecting fake continuous-wave signals into data from the third observing run of the LIGO detectors. This analysis allowed us to infer strain amplitude upper limits at a $90\%$ confidence level. We found that the optimal sensitivity is $h_0^{90\%} = 3.62^{+0.23}_{-0.22}\times 10^{-26}$ in the [128, 200] Hz band, which is within the most sensible frequency band of the LIGO detectors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19420v3</guid>
      <category>gr-qc</category>
      <category>astro-ph.IM</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matteo Di Giovanni, Paola Leaci, Pia Astone, Stefano Dal Pra, Sabrina D'Antonio, Luca D'Onofrio, Sergio Frasca, Federico Muciaccia, Cristiano Palomba, Lorenzo Pierini, Francesco Safai Tehrani</dc:creator>
    </item>
    <item>
      <title>Extraction of gravitational wave signals from LISA data in the presence of artifacts</title>
      <link>https://arxiv.org/abs/2411.13402</link>
      <description>arXiv:2411.13402v2 Announce Type: replace-cross 
Abstract: The Laser Interferometer Space Antenna (LISA) mission is being developed by ESA with NASA participation. As it has recently passed the Mission Adoption milestone, models of the instruments and noise performance are becoming more detailed, and likewise prototype data analyses must as well. Assumptions such as Gaussianity, stationarity, and data continuity are unrealistic, and must be replaced with physically motivated data simulations, and data analysis methods adapted to accommodate such likely imperfections. To this end, the LISA Data Challenges have produced datasets featuring time-varying and unequal constellation armlength, and measurement artifacts including data interruptions and instrumental transients. In this work, we assess the impact of these data artifacts on the inference of Galactic Binary and Massive Black Hole properties. Our analysis shows that the treatment of noise transients and gaps is necessary for effective parameter estimation, as they substantially corrupt the analysis if unmitigated. We find that straightforward mitigation techniques can significantly if imperfectly suppress artifacts. For the Galactic Binaries, mitigation of glitches was essentially total, while mitigations of the data gaps increased parameter uncertainty by approximately 10%. For the Massive Black Hole binaries the particularly pernicious glitches resulted in a 30% uncertainty increase after mitigations, while the data gaps can increase parameter uncertainty by up to several times. Critically, this underlines the importance of early detection of transient gravitational waves to ensure they are protected from planned data interruptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13402v2</guid>
      <category>gr-qc</category>
      <category>astro-ph.IM</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eleonora Castelli, Quentin Baghi, John G. Baker, Jacob Slutsky, J\'er\^ome Bobin, Nikolaos Karnesis, Antoine Petiteau, Orion Sauter, Peter Wass, William J. Weber</dc:creator>
    </item>
  </channel>
</rss>
