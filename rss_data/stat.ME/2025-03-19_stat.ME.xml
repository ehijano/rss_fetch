<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 20 Mar 2025 01:52:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Exact statistical tests using integer programming: Leveraging an overlooked approach for maximizing power for differences between binomial proportions</title>
      <link>https://arxiv.org/abs/2503.13689</link>
      <description>arXiv:2503.13689v1 Announce Type: new 
Abstract: Traditional hypothesis testing methods for differences in binomial proportions can either be too liberal (Wald test) or overly conservative (Fisher's exact test), especially in small samples. Regulators favour conservative approaches for robust type I error control, though excessive conservatism may significantly reduce statistical power. We offer fundamental theoretical contributions that extend an approach proposed in 1969, resulting in the derivation of a family of exact tests designed to maximize a specific type of power. We establish theoretical guarantees for controlling type I error despite the discretization of the null parameter space. This theoretical advancement is supported by a comprehensive series of experiments to empirically quantify the power advantages compared to traditional hypothesis tests. The approach determines the rejection region through a binary decision for each outcome dataset and uses integer programming to find an optimal decision boundary that maximizes power subject to type I error constraints. Our analysis provides new theoretical properties and insights into this approach's comparative advantages. When optimized for average power over all possible parameter configurations under the alternative, the method exhibits remarkable robustness, performing optimally or near-optimally across specific alternatives while maintaining exact type I error control. The method can be further customized for particular prior beliefs by using a weighted average. The findings highlight both the method's practical utility and how techniques from combinatorial optimization can enhance statistical methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13689v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Stef Baas, Yaron Racah, Elad Berkman, Sof\'ia S. Villar</dc:creator>
    </item>
    <item>
      <title>Minnesota BART</title>
      <link>https://arxiv.org/abs/2503.13759</link>
      <description>arXiv:2503.13759v1 Announce Type: new 
Abstract: Vector autoregression (VAR) models are widely used for forecasting and macroeconomic analysis, yet they remain limited by their reliance on a linear parameterization. Recent research has introduced nonparametric alternatives, such as Bayesian additive regression trees (BART), which provide flexibility without strong parametric assumptions. However, existing BART-based frameworks do not account for time dependency or allow for sparse estimation in the construction of regression tree priors, leading to noisy and inefficient high-dimensional representations. This paper introduces a sparsity-inducing Dirichlet hyperprior on the regression tree's splitting probabilities, allowing for automatic variable selection and high-dimensional VARs. Additionally, we propose a structured shrinkage prior that decreases the probability of splitting on higher-order lags, aligning with the Minnesota prior's principles. Empirical results demonstrate that our approach improves predictive accuracy over the baseline BART prior and Bayesian VAR (BVAR), particularly in capturing time-dependent relationships and enhancing density forecasts. These findings highlight the potential of developing domain-specific nonparametric methods in macroeconomic forecasting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13759v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pedro A. Lima, Carlos M. Carvalho, Hedibert F. Lopes, Andrew Herren</dc:creator>
    </item>
    <item>
      <title>Regularized Parameter Estimation in Mixed Model Trace Regression</title>
      <link>https://arxiv.org/abs/2503.13782</link>
      <description>arXiv:2503.13782v1 Announce Type: new 
Abstract: We introduce mixed model trace regression (MMTR), a mixed model linear regression extension for scalar responses and high-dimensional matrix-valued covariates. MMTR's fixed effects component is equivalent to trace regression, with an element-wise lasso penalty imposed on the regression coefficients matrix to facilitate the estimation of a sparse mean parameter. MMTR's key innovation lies in modeling the covariance structure of matrix-variate random effects as a Kronecker product of low-rank row and column covariance matrices, enabling sparse estimation of the covariance parameter through low-rank constraints. We establish identifiability conditions for the estimation of row and column covariance matrices and use them for rank selection by applying group lasso regularization on the columns of their respective Cholesky factors. We develop an Expectation-Maximization (EM) algorithm extension for numerically stable parameter estimation in high-dimensional applications. MMTR achieves estimation accuracy comparable to leading regularized quasi-likelihood competitors across diverse simulation studies and attains the lowest mean square prediction error compared to its competitors on a publicly available image dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13782v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ian Hultman, Sanvesh Srivastava</dc:creator>
    </item>
    <item>
      <title>Distributions and Direct Parametrization for Stable Stochastic State-Space Models</title>
      <link>https://arxiv.org/abs/2503.14177</link>
      <description>arXiv:2503.14177v1 Announce Type: new 
Abstract: We present a direct parametrization for continuous-time stochastic state-space models that ensures external stability via the stochastic bounded-real lemma. Our formulation facilitates the construction of probabilistic priors that enforce almost-sure stability which are suitable for sampling-based Bayesian inference methods. We validate our work with a simulation example and demonstrate its ability to yield stable predictions with uncertainty quantification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14177v1</guid>
      <category>stat.ME</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohamad Al Ahdab, Zheng-Hua Tan, John Leth</dc:creator>
    </item>
    <item>
      <title>Partially Directed Configuration Model with Homophily and Respondent-Driven Sampling</title>
      <link>https://arxiv.org/abs/2503.14334</link>
      <description>arXiv:2503.14334v1 Announce Type: new 
Abstract: Respondent-driven sampling (RDS) is a sampling scheme used in socially connected human populations lacking a sampling frame. One of the first steps to make design-based inferences from RDS data is to estimate the sampling probabilities. A classical approach for such estimation assumes that a first-order Markov chain over a fully connected and undirected network may adequately represent RDS. This convenient model, however, does not reflect that the network may be directed and homophilous. The methods proposed in this work aim to address this issue. The main methodological contributions of this manuscript are two fold: first, we introduce a partially directed and homophilous network configuration model, and second, we develop two mathematical representations of the RDS sampling process over the proposed configuration model. Our simulation study shows that the resulting sampling probabilities are similar to those of RDS, and they improve the prevalence estimation under various realistic scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14334v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alejandro Sepulveda-Pe\~naloza, Isabelle S. Beaudry</dc:creator>
    </item>
    <item>
      <title>Cross-Validation in Penalized Linear Mixed Models: Addressing Common Implementation Pitfalls</title>
      <link>https://arxiv.org/abs/2503.14374</link>
      <description>arXiv:2503.14374v1 Announce Type: new 
Abstract: In this paper, we develop an implementation of cross-validation for penalized linear mixed models. While these models have been proposed for correlated high-dimensional data, the current literature implicitly assumes that tuning parameter selection procedures developed for independent data will also work well in this context. We argue that such naive assumptions make analysis prone to pitfalls, several of which we will describe. Here we present a correct implementation of cross-validation for penalized linear mixed models, addressing these common pitfalls. We support our methods with mathematical proof, simulation study, and real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14374v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tabitha K. Peter, Patrick J. Breheny</dc:creator>
    </item>
    <item>
      <title>KANITE: Kolmogorov-Arnold Networks for ITE estimation</title>
      <link>https://arxiv.org/abs/2503.13912</link>
      <description>arXiv:2503.13912v1 Announce Type: cross 
Abstract: We introduce KANITE, a framework leveraging Kolmogorov-Arnold Networks (KANs) for Individual Treatment Effect (ITE) estimation under multiple treatments setting in causal inference. By utilizing KAN's unique abilities to learn univariate activation functions as opposed to learning linear weights by Multi-Layer Perceptrons (MLPs), we improve the estimates of ITEs. The KANITE framework comprises two key architectures: 1.Integral Probability Metric (IPM) architecture: This employs an IPM loss in a specialized manner to effectively align towards ITE estimation across multiple treatments. 2. Entropy Balancing (EB) architecture: This uses weights for samples that are learned by optimizing entropy subject to balancing the covariates across treatment groups. Extensive evaluations on benchmark datasets demonstrate that KANITE outperforms state-of-the-art algorithms in both $\epsilon_{\text{PEHE}}$ and $\epsilon_{\text{ATE}}$ metrics. Our experiments highlight the advantages of KANITE in achieving improved causal estimates, emphasizing the potential of KANs to advance causal inference methodologies across diverse application areas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13912v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Eshan Mehendale, Abhinav Thorat, Ravi Kolla, Niranjan Pedanekar</dc:creator>
    </item>
    <item>
      <title>A Note on the Asymptotic Properties of the GLS Estimator in Multivariate Regression with Heteroskedastic and Autocorrelated Errors</title>
      <link>https://arxiv.org/abs/2503.13950</link>
      <description>arXiv:2503.13950v1 Announce Type: cross 
Abstract: We study the asymptotic properties of the GLS estimator in multivariate regression with heteroskedastic and autocorrelated errors. We derive Wald statistics for linear restrictions and assess their performance. The statistics remains robust to heteroskedasticity and autocorrelation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13950v1</guid>
      <category>econ.EM</category>
      <category>q-fin.ST</category>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Koichiro Moriya, Akihiko Noda</dc:creator>
    </item>
    <item>
      <title>Asymptotic properties of the MLE in distributional regression under random censoring</title>
      <link>https://arxiv.org/abs/2503.14311</link>
      <description>arXiv:2503.14311v1 Announce Type: cross 
Abstract: The aim of distributional regression is to find the best candidate in a given parametric family of conditional distributions to model a given dataset. As each candidate in the distribution family can be identified by the corresponding distribution parameters, a common approach for this task is using the maximum likelihood estimator (MLE) for the parameters. In this paper, we establish theoretical results for this estimator in case the response variable is subject to random right censoring. In particular, we provide proofs of almost sure consistency and asymptotic normality of the MLE under censoring. Further, the finite-sample behavior is exemplarily demonstrated in a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14311v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gitte Kremling, Gerhard Dikta</dc:creator>
    </item>
    <item>
      <title>Optimizing High-Dimensional Oblique Splits</title>
      <link>https://arxiv.org/abs/2503.14381</link>
      <description>arXiv:2503.14381v1 Announce Type: cross 
Abstract: Orthogonal-split trees perform well, but evidence suggests oblique splits can enhance their performance. This paper explores optimizing high-dimensional $s$-sparse oblique splits from $\{(\vec{w}, \vec{w}^{\top}\boldsymbol{X}_{i}) : i\in \{1,\dots, n\}, \vec{w} \in \mathbb{R}^p, \| \vec{w} \|_{2} = 1, \| \vec{w} \|_{0} \leq s \}$ for growing oblique trees, where $ s $ is a user-defined sparsity parameter. We establish a connection between SID convergence and $s_0$-sparse oblique splits with $s_0\ge 1$, showing that the SID function class expands as $s_0$ increases, enabling the capture of more complex data-generating functions such as the $s_0$-dimensional XOR function. Thus, $s_0$ represents the unknown potential complexity of the underlying data-generating function. Learning these complex functions requires an $s$-sparse oblique tree with $s \geq s_0$ and greater computational resources. This highlights a trade-off between statistical accuracy, governed by the SID function class size depending on $s_0$, and computational cost. In contrast, previous studies have explored the problem of SID convergence using orthogonal splits with $ s_0 = s = 1 $, where runtime was less critical. Additionally, we introduce a practical framework for oblique trees that integrates optimized oblique splits alongside orthogonal splits into random forests. The proposed approach is assessed through simulations and real-data experiments, comparing its performance against various oblique tree models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14381v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chien-Ming Chi</dc:creator>
    </item>
    <item>
      <title>Doubly robust identification of treatment effects from multiple environments</title>
      <link>https://arxiv.org/abs/2503.14459</link>
      <description>arXiv:2503.14459v1 Announce Type: cross 
Abstract: Practical and ethical constraints often require the use of observational data for causal inference, particularly in medicine and social sciences. Yet, observational datasets are prone to confounding, potentially compromising the validity of causal conclusions. While it is possible to correct for biases if the underlying causal graph is known, this is rarely a feasible ask in practical scenarios. A common strategy is to adjust for all available covariates, yet this approach can yield biased treatment effect estimates, especially when post-treatment or unobserved variables are present. We propose RAMEN, an algorithm that produces unbiased treatment effect estimates by leveraging the heterogeneity of multiple data sources without the need to know or learn the underlying causal graph. Notably, RAMEN achieves doubly robust identification: it can identify the treatment effect whenever the causal parents of the treatment or those of the outcome are observed, and the node whose parents are observed satisfies an invariance assumption. Empirical evaluations on synthetic and real-world datasets show that our approach outperforms existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14459v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Piersilvio De Bartolomeis, Julia Kostin, Javier Abad, Yixin Wang, Fanny Yang</dc:creator>
    </item>
    <item>
      <title>Balanced Subsampling for Big Data with Categorical Covariates</title>
      <link>https://arxiv.org/abs/2212.12595</link>
      <description>arXiv:2212.12595v2 Announce Type: replace 
Abstract: Supervised learning under measurement constraints is a common challenge in statistical and machine learning. In many applications, despite extensive design points, acquiring responses for all points is often impractical due to resource limitations. Subsampling algorithms offer a solution by selecting a subset from the design points for observing the response. Existing subsampling methods primarily assume numerical predictors, neglecting the prevalent occurrence of big data with categorical predictors across various disciplines. This paper proposes a novel balanced subsampling approach tailored for data with categorical predictors. A balanced subsample significantly reduces the cost of observing the response and possesses three desired merits. First, it is nonsingular and, therefore, allows linear regression with all dummy variables encoded from categorical predictors. Second, it offers optimal parameter estimation by minimizing the generalized variance of the estimated parameters. Third, it allows robust prediction in the sense of minimizing the worst-case prediction error. We demonstrate the superiority of balanced subsampling over existing methods through extensive simulation studies and a real-world application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.12595v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5705/ss.202023.0434</arxiv:DOI>
      <dc:creator>Lin Wang</dc:creator>
    </item>
    <item>
      <title>Max-Rank: Efficient Multiple Testing for Conformal Prediction</title>
      <link>https://arxiv.org/abs/2311.10900</link>
      <description>arXiv:2311.10900v4 Announce Type: replace 
Abstract: Multiple hypothesis testing (MHT) frequently arises in scientific inquiries, and concurrent testing of multiple hypotheses inflates the risk of Type-I errors or false positives, rendering MHT corrections essential. This paper addresses MHT in the context of conformal prediction, a flexible framework for predictive uncertainty quantification. Some conformal applications give rise to simultaneous testing, and positive dependencies among tests typically exist. We introduce $\texttt{max-rank}$, a novel correction that exploits these dependencies whilst efficiently controlling the family-wise error rate. Inspired by existing permutation-based corrections, $\texttt{max-rank}$ leverages rank order information to improve performance and integrates readily with any conformal procedure. We establish its theoretical and empirical advantages over the common Bonferroni correction and its compatibility with conformal prediction, highlighting the potential to strengthen predictive uncertainty estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.10900v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Timans, Christoph-Nikolas Straehle, Kaspar Sakmann, Christian A. Naesseth, Eric Nalisnick</dc:creator>
    </item>
    <item>
      <title>Understanding Measurement Precision from a Regression Perspective</title>
      <link>https://arxiv.org/abs/2404.16709</link>
      <description>arXiv:2404.16709v3 Announce Type: replace 
Abstract: We adopt and expand McDonald's (2011) regression framework for measurement precision, integrating two key perspectives: (a) reliability of observed scores and (b) optimal prediction of latent scores. Reliability arises from a measurement decomposition of an observed score into its true score and measurement error. In contrast, proportional reduction in mean squared error (PRMSE) arises from a prediction decomposition of a latent score into its optimal predictor (the observed expected a posteriori [EAP] score) and prediction error. Reliability is the coefficient of determination obtained by two isomorphic regressions: regressing the observed score on its true score or on all the latent variables. Similarly, PRMSE is the coefficient of determination obtained from two isomorphic regressions: regressing the latent score on its observed EAP score or all the manifest variables. A key implication of this regression framework is that both reliability and PRMSE can be estimated using a Monte Carlo (MC) method, which is particularly useful when no analytic formula is available or when the analytic calculation is involved. We illustrate these concepts with a factor analysis model and a two parameter logistic model, in which we compute reliability coefficients for different observed scores and PRMSE for different latent scores. Additionally, we provide a numerical example demonstrating how the MC method can be used to estimate reliability and PRMSE within a two-dimensional item response tree model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16709v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yang Liu, Jolynn Pek, Alberto Maydeu-Olivares</dc:creator>
    </item>
    <item>
      <title>Distributed Tensor Principal Component Analysis with Data Heterogeneity</title>
      <link>https://arxiv.org/abs/2405.11681</link>
      <description>arXiv:2405.11681v2 Announce Type: replace 
Abstract: As tensors become widespread in modern data analysis, Tucker low-rank Principal Component Analysis (PCA) has become essential for dimensionality reduction and structural discovery in tensor datasets. Motivated by the common scenario where large-scale tensors are distributed across diverse geographic locations, this paper investigates tensor PCA within a distributed framework where direct data pooling is impractical.
  We offer a comprehensive analysis of three specific scenarios in distributed Tensor PCA: a homogeneous setting in which tensors at various locations are generated from a single noise-affected model; a heterogeneous setting where tensors at different locations come from distinct models but share some principal components, aiming to improve estimation across all locations; and a targeted heterogeneous setting, designed to boost estimation accuracy at a specific location with limited samples by utilizing transferred knowledge from other sites with ample data.
  We introduce novel estimation methods tailored to each scenario, establish statistical guarantees, and develop distributed inference techniques to construct confidence regions. Our theoretical findings demonstrate that these distributed methods achieve sharp rates of accuracy by efficiently aggregating shared information across different tensors, while maintaining reasonable communication costs. Empirical validation through simulations and real-world data applications highlights the advantages of our approaches, particularly in managing heterogeneous tensor data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11681v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Elynn Chen, Xi Chen, Wenbo Jing, Yichen Zhang</dc:creator>
    </item>
    <item>
      <title>Exact statistical analysis for response-adaptive clinical trials: A general and computationally tractable approach</title>
      <link>https://arxiv.org/abs/2407.01055</link>
      <description>arXiv:2407.01055v2 Announce Type: replace 
Abstract: Response-adaptive clinical trial designs allow targeting a given objective by skewing the allocation of participants to treatments based on observed outcomes. Response-adaptive designs face greater regulatory scrutiny due to potential type I error rate inflation, which limits their uptake in practice. Existing approaches for type I error control either only work for specific designs, have a risk of Monte Carlo/approximation error, are conservative, or computationally intractable. To this end, a general and computationally tractable approach is developed for exact analysis in two-arm response-adaptive designs with binary outcomes. This approach can construct exact tests for designs using either a randomized or deterministic response-adaptive procedure. The constructed conditional and unconditional exact tests generalize Fisher's and Barnard's exact tests, respectively. Furthermore, the approach allows for complexities such as delayed outcomes, early stopping, or allocation of participants in blocks. The efficient implementation of forward recursion allows for testing of two-arm trials with 1,000 participants on a standard computer. Through an illustrative computational study of trials using randomized dynamic programming it is shown that, contrary to what is known for equal allocation, the conditional exact Wald test based on total successes has, almost uniformly, higher power than the unconditional exact Wald test. Two real-world trials with the above-mentioned complexities are re-analyzed to demonstrate the value of the new approach in controlling type I errors and/or improving the statistical power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01055v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Stef Baas, Peter Jacko, Sof\'ia S. Villar</dc:creator>
    </item>
    <item>
      <title>Powerful batch conformal prediction for classification</title>
      <link>https://arxiv.org/abs/2411.02239</link>
      <description>arXiv:2411.02239v2 Announce Type: replace 
Abstract: In a split conformal framework with $K$ classes, a calibration sample of $n$ labeled examples is observed for inference on the label of a new unlabeled example. We explore the setting where a `batch' of $m$ independent such unlabeled examples is given, and the goal is to construct a batch prediction set with 1-$\alpha$ coverage. Unlike individual prediction sets, the batch prediction set is a collection of label vectors of size $m$, while the calibration sample consists of univariate labels. A natural approach is to apply the Bonferroni correction, which concatenates individual prediction sets at level $1-\alpha/m$. We propose a uniformly more powerful solution, based on specific combinations of conformal $p$-values that exploit the Simes inequality. We provide a general recipe for valid inference with any combinations of conformal $p$-values, and compare the performance of several useful choices. Intuitively, the pooled evidence of relatively `easy' examples within the batch can help provide narrower batch prediction sets. Additionally, we introduce a more computationally intensive method that aggregates batch scores and can be even more powerful. The theoretical guarantees are established when all examples are independent and identically distributed (iid), as well as more generally when iid is assumed only conditionally within each class. Notably, our results remain valid under label distribution shift, since the distribution of the labels need not be the same in the calibration sample and in the new batch. The effectiveness of the methods is highlighted through illustrative synthetic and real data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02239v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ulysse Gazin, Ruth Heller, Etienne Roquain, Aldo Solari</dc:creator>
    </item>
    <item>
      <title>Debiased Nonparametric Regression for Statistical Inference and Distributionally Robustness</title>
      <link>https://arxiv.org/abs/2412.20173</link>
      <description>arXiv:2412.20173v3 Announce Type: replace 
Abstract: This study proposes a debiasing method for smooth nonparametric estimators. While machine learning techniques such as random forests and neural networks have demonstrated strong predictive performance, their theoretical properties remain relatively underexplored. In particular, many modern algorithms lack guarantees of pointwise and uniform risk convergence, as well as asymptotic normality. These properties are essential for statistical inference and robust estimation and have been well-established for classical methods such as Nadaraya-Watson regression. To ensure these properties for various nonparametric regression estimators, we introduce a model-free debiasing method. By incorporating a correction term that estimates the conditional expected residual of the original estimator, or equivalently, its estimation error, into the initial nonparametric regression estimator, we obtain a debiased estimator that satisfies pointwise and uniform risk convergence, along with asymptotic normality, under mild smoothness conditions. These properties facilitate statistical inference and enhance robustness to covariate shift, making the method broadly applicable to a wide range of nonparametric regression problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20173v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
    <item>
      <title>Inference for Log-Gaussian Cox Point Processes using Bayesian Deep Learning: Application to Human Oral Microbiome Image Data</title>
      <link>https://arxiv.org/abs/2502.12334</link>
      <description>arXiv:2502.12334v2 Announce Type: replace 
Abstract: It is common in nature to see aggregation of objects in space. Exploring the mechanism associated with the locations of such clustered observations can be essential to understanding the phenomenon, such as the source of spatial heterogeneity, or comparison to other event generating processes in the same domain. Log-Gaussian Cox processes (LGCPs) represent an important class of models for quantifying aggregation in a spatial point pattern. However, implementing likelihood-based Bayesian inference for such models presents many computational challenges, particularly in high dimensions. In this paper, we propose a novel likelihood-free inference approach for LGCPs using the recently developed BayesFlow approach, where invertible neural networks are employed to approximate the posterior distribution of the parameters of interest. BayesFlow is a neural simulation-based method based on "amortized" posterior estimation. That is, after an initial training procedure, fast feed-forward operations allow rapid posterior inference for any data within the same model family. Comprehensive numerical studies validate the reliability of the framework and show that BayesFlow achieves substantial computational gain in repeated application, especially for two-dimensional LGCPs. We demonstrate the utility and robustness of the method by applying it to two distinct oral microbial biofilm images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12334v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuwan Wang, Christopher K. Wikle, Athanasios C. Micheas, Jessica L. Mark Welch, Jacqueline R. Starr, Kyu Ha Lee</dc:creator>
    </item>
    <item>
      <title>Bounds for the regression parameters in dependently censored survival models</title>
      <link>https://arxiv.org/abs/2503.11210</link>
      <description>arXiv:2503.11210v2 Announce Type: replace 
Abstract: We propose a semiparametric model to study the effect of covariates on the distribution of a censored event time while making minimal assumptions about the censoring mechanism. The result is a partially identified model, in the sense that we obtain bounds on the covariate effects, which are allowed to be time-dependent. Moreover, these bounds can be interpreted as classical confidence intervals and are obtained by aggregating information in the conditional Peterson bounds over the entire covariate space. As a special case, our approach can be used to study the popular Cox proportional hazards model while leaving the censoring distribution as well as its dependence with the time of interest completely unspecified. A simulation study illustrates good finite sample performance of the method, and several data applications in both economics and medicine demonstrate its practicability on real data. All developed methodology is implemented in R and made available in the package depCensoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11210v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilias Willems, Jad Beyhum, Ingrid Van Keilegom</dc:creator>
    </item>
    <item>
      <title>Comparative Judgement Modeling to Map Forced Marriage at Local Levels</title>
      <link>https://arxiv.org/abs/2212.01202</link>
      <description>arXiv:2212.01202v4 Announce Type: replace-cross 
Abstract: Forcing someone into marriage against their will is a violation of their human rights. In 2021, the county of Nottinghamshire, UK, launched a strategy to tackle forced marriage and violence against women and girls. However, accessing information about where victims are located in the county could compromise their safety, so it is not possible to develop interventions for different areas of the county. Comparative judgement studies offer a way to map the risk of human rights abuses without collecting data that could compromise victim safety. Current methods require studies to have a large number of participants, so we develop a comparative judgement model that provides a more flexible spatial modelling structure and a mechanism to schedule comparisons more effectively. The methods reduce the data collection burden on participants and make a comparative judgement study feasible with a small number of participants. Underpinning these methods is a latent variable representation that improves on the scalability of previous comparative judgement models. We use these methods to map the risk of forced marriage across Nottinghamshire thereby supporting the county's strategy for tackling violence against women and girls.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.01202v4</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1214/24-AOAS1966</arxiv:DOI>
      <arxiv:journal_reference>Ann. Appl. Stat. 19 (1) 419 - 439, March 2025</arxiv:journal_reference>
      <dc:creator>R. G. Seymour, A. Nyarko-Agyei, H. R. McCabe, K. Severn, T. Kypraios, D. Sirl, A. Taylor</dc:creator>
    </item>
    <item>
      <title>Likelihood ratio tests in random graph models with increasing dimensions</title>
      <link>https://arxiv.org/abs/2311.05806</link>
      <description>arXiv:2311.05806v2 Announce Type: replace-cross 
Abstract: We explore the Wilks phenomena in two random graph models: the $\beta$-model and the Bradley-Terry model. For two increasing dimensional null hypotheses, including a specified null $H_0: \beta_i=\beta_i^0$ for $i=1,\ldots, r$ and a homogenous null $H_0: \beta_1=\cdots=\beta_r$, we reveal high dimensional Wilks' phenomena that the normalized log-likelihood ratio statistic, $[2\{\ell(\widehat{\mathbf{\beta}}) - \ell(\widehat{\mathbf{\beta}}^0)\} - r]/(2r)^{1/2}$, converges in distribution to the standard normal distribution as $r$ goes to infinity. Here, $\ell( \mathbf{\beta})$ is the log-likelihood function on the model parameter $\mathbf{\beta}=(\beta_1, \ldots, \beta_n)^\top$, $\widehat{\mathbf{\beta}}$ is its maximum likelihood estimator (MLE) under the full parameter space, and $\widehat{\mathbf{\beta}}^0$ is the restricted MLE under the null parameter space. For the homogenous null with a fixed $r$, we establish Wilks-type theorems that $2\{\ell(\widehat{\mathbf{\beta}}) - \ell(\widehat{\mathbf{\beta}}^0)\}$ converges in distribution to a chi-square distribution with $r-1$ degrees of freedom, as the total number of parameters, $n$, goes to infinity. When testing the fixed dimensional specified null, we find that its asymptotic null distribution is a chi-square distribution in the $\beta$-model. However, unexpectedly, this is not true in the Bradley-Terry model. By developing several novel technical methods for asymptotic expansion, we explore Wilks type results in a principled manner; these principled methods should be applicable to a class of random graph models beyond the $\beta$-model and the Bradley-Terry model. Simulation studies and real network data applications further demonstrate the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.05806v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ting Yan, Yuanzhang Li, Jinfeng Xu, Yaning Yang, Ji Zhu</dc:creator>
    </item>
    <item>
      <title>Uncertainty Quantification in Machine Learning Based Segmentation: A Post-Hoc Approach for Left Ventricle Volume Estimation in MRI</title>
      <link>https://arxiv.org/abs/2312.02167</link>
      <description>arXiv:2312.02167v2 Announce Type: replace-cross 
Abstract: Recent studies have confirmed cardiovascular diseases remain responsible for highest death toll amongst non-communicable diseases. Accurate left ventricular (LV) volume estimation is critical for valid diagnosis and management of various cardiovascular conditions, but poses significant challenge due to inherent uncertainties associated with segmentation algorithms in magnetic resonance imaging (MRI). Recent machine learning advancements, particularly U-Net-like convolutional networks, have facilitated automated segmentation for medical images, but struggles under certain pathologies and/or different scanner vendors and imaging protocols. This study proposes a novel methodology for post-hoc uncertainty estimation in LV volume prediction using It\^{o} stochastic differential equations (SDEs) to model path-wise behavior for the prediction error. The model describes the area of the left ventricle along the heart's long axis. The method is agnostic to the underlying segmentation algorithm, facilitating its use with various existing and future segmentation technologies. The proposed approach provides a mechanism for quantifying uncertainty, enabling medical professionals to intervene for unreliable predictions. This is of utmost importance in critical applications such as medical diagnosis, where prediction accuracy and reliability can directly impact patient outcomes. The method is also robust to dataset changes, enabling application for medical centers with limited access to labeled data. Our findings highlight the proposed uncertainty estimation methodology's potential to enhance automated segmentation robustness and generalizability, paving the way for more reliable and accurate LV volume estimation in clinical settings as well as opening new avenues for uncertainty quantification in biomedical image segmentation, providing promising directions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.02167v2</guid>
      <category>cs.CV</category>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1137/23M161433X</arxiv:DOI>
      <arxiv:journal_reference>SIAM/ASA Journal on Uncertainty Quantification 13 (1), 2025, 90-113</arxiv:journal_reference>
      <dc:creator>F. Terhag, P. Knechtges, A. Basermann, R. Tempone</dc:creator>
    </item>
    <item>
      <title>Convergence rates of non-stationary and deep Gaussian process regression</title>
      <link>https://arxiv.org/abs/2312.07320</link>
      <description>arXiv:2312.07320v4 Announce Type: replace-cross 
Abstract: The focus of this work is the convergence of non-stationary and deep Gaussian process regression. More precisely, we follow a Bayesian approach to regression or interpolation, where the prior placed on the unknown function $f$ is a non-stationary or deep Gaussian process, and we derive convergence rates of the posterior mean to the true function $f$ in terms of the number of observed training points. In some cases, we also show convergence of the posterior variance to zero. The only assumption imposed on the function $f$ is that it is an element of a certain reproducing kernel Hilbert space, which we in particular cases show to be norm-equivalent to a Sobolev space. Our analysis includes the case of estimated hyper-parameters in the covariance kernels employed, both in an empirical Bayes' setting and the particular hierarchical setting constructed through deep Gaussian processes. We consider the settings of noise-free or noisy observations on deterministic or random training points. We establish general assumptions sufficient for the convergence of deep Gaussian process regression, along with explicit examples demonstrating the fulfilment of these assumptions. Specifically, our examples require that the H\"older or Sobolev norms of the penultimate layer are bounded almost surely.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.07320v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Conor Osborne, Aretha L. Teckentrup</dc:creator>
    </item>
  </channel>
</rss>
