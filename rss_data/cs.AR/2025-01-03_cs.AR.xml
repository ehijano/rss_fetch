<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 03 Jan 2025 05:00:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Enabling New HDLs with Agents</title>
      <link>https://arxiv.org/abs/2501.00642</link>
      <description>arXiv:2501.00642v1 Announce Type: new 
Abstract: Large Language Models (LLMs) based agents are transforming the programming language landscape by facilitating learning for beginners, enabling code generation, and optimizing documentation workflows. Hardware Description Languages (HDLs), with their smaller user community, stand to benefit significantly from the application of LLMs as tools for learning new HDLs. This paper investigates the challenges and solutions of enabling LLMs for HDLs, particularly for HDLs that LLMs have not been previously trained on. This work introduces HDLAgent, an AI agent optimized for LLMs with limited knowledge of various HDLs. It significantly enhances off-the-shelf LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00642v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.PL</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mark Zakharov, Farzaneh Rabiei Kashanaki, Jose Renau</dc:creator>
    </item>
    <item>
      <title>Aligning Netlist to Source Code using SynAlign</title>
      <link>https://arxiv.org/abs/2501.00921</link>
      <description>arXiv:2501.00921v1 Announce Type: new 
Abstract: In current chip design processes, using multiple tools to obtain a gate-level netlist often results in the loss of source code correlation. SynAlign addresses this challenge by automating the alignment process, simplifying iterative design, reducing overhead, and maintaining correlation across various tools. This enhances the efficiency and effectiveness of chip design workflows.
  Improving characteristics such as frequency through iterative design is essential for enhancing accelerators and chip designs. While synthesis tools produce netlists with critical path information, designers often lack the tools to trace these netlist cells back to their original source code. Mapping netlist components to source code provides early feedback on timing and power for frontend designers.
  SynAlign automatically aligns post-optimized netlists with the original source code without altering compilers or synthesis processes. Its alignment strategy relies on the consistent design structure throughout the chip design cycle, even with changes in compiler flow. This consistency allows engineers to maintain a correlation between modified designs and the original source code across various tools. Remarkably, SynAlign can tolerate up to 61\% design net changes without impacting alignment accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00921v1</guid>
      <category>cs.AR</category>
      <category>cs.CL</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sakshi Garg, Jose Renau</dc:creator>
    </item>
    <item>
      <title>Integrated AHB to APB Bridge Using Raspberry Pi and Artix-7 FPGA</title>
      <link>https://arxiv.org/abs/2501.01147</link>
      <description>arXiv:2501.01147v1 Announce Type: new 
Abstract: This project focuses on the design and implementation of an AHB to APB Bridge for efficient communication in System-on-Chip (SoC) architectures. The Advanced High-performance Bus (AHB) is used for high-speed operations, typically connecting processors and memory, while the Advanced Peripheral Bus (APB) is optimized for low-power, low-speed peripheral devices. The AHB to APB Bridge serves as an interface that converts complex, high-speed AHB transactions into simpler, single-cycle APB transactions, enabling seamless data transfer between fast components and slower peripherals. The bridge manages clock domain synchronization, transaction conversion, and flow control, ensuring compatibility between AHB's burst transfers and APB's non-pipelined protocol. Implemented in Verilog and simulated on FPGA using Xilinx Vivado, this bridge design provides a robust solution for integrating high-performance and low-power components within a single SoC. This project also evaluates the bridge's functionality and performance through testbenches covering various operational scenarios, validating its efficiency in handling diverse system requirements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01147v1</guid>
      <category>cs.AR</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gopi Chand Ananthu, Riadul Islam</dc:creator>
    </item>
    <item>
      <title>Adaptive Hybrid FFT: A Novel Pipeline and Memory-Based Architecture for Radix-$2^k$ FFT in Large Size Processing</title>
      <link>https://arxiv.org/abs/2501.01259</link>
      <description>arXiv:2501.01259v1 Announce Type: new 
Abstract: In the field of digital signal processing, the fast Fourier transform (FFT) is a fundamental algorithm, with its processors being implemented using either the pipelined architecture, well-known for high-throughput applications but weak in hardware utilization, or the memory-based architecture, designed for area-constrained scenarios but failing to meet stringent throughput requirements. Therefore, we propose an adaptive hybrid FFT, which leverages the strengths of both pipelined and memory-based architectures. In this paper, we propose an adaptive hybrid FFT processor that combines the advantages of both architectures, and it has the following features. First, a set of radix-$2^k$ multi-path delay commutators (MDC) units are developed to support high-performance large-size processing. Second, a conflict-free memory access scheme is formulated to ensure a continuous data flow without data contention. Third, We demonstrate the existence of a series of bit-dimension permutations for reordering input data, satisfying the generalized constraints of variable-length, high-radix, and any level of parallelism for wide adaptivity. Furthermore, the proposed FFT processor has been implemented on a field-programmable gate array (FPGA). As a result, the proposed work outperforms conventional memory-based FFT processors by requiring fewer computation cycles. It achieves higher hardware utilization than pipelined FFT architectures, making it suitable for highly demanding applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01259v1</guid>
      <category>cs.AR</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fangyu Zhao, Chunhua Xiao, Zhiguo Wang, Xiaohua Du, Bo Dong</dc:creator>
    </item>
    <item>
      <title>Adapting Atmospheric Chemistry Components for Efficient GPU Accelerators</title>
      <link>https://arxiv.org/abs/2501.00011</link>
      <description>arXiv:2501.00011v1 Announce Type: cross 
Abstract: Atmospheric models demand a lot of computational power and solving the chemical processes is one of its most computationally intensive components. This work shows how to improve the computational performance of the Multiscale Online Nonhydrostatic AtmospheRe CHemistry model (MONARCH), a chemical weather prediction system developed by the Barcelona Supercomputing Center. The model implements the new flexible external package Chemistry Across Multiple Phases (CAMP) for the solving of gas- and aerosol-phase chemical processes, that allows multiple chemical processes to be solved simultaneously as a single system. We introduce a novel strategy to simultaneously solve multiple instances of a chemical mechanism, represented in the model as grid-cells, obtaining a speedup up to 9x using thousands of cells. In addition, we present a GPU strategy for the most time-consuming function of CAMP. The GPU version achieves up to 1.2x speedup compared to CPU. Also, we optimize the memory access in the GPU to increase its speedup up to 1.7x.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00011v1</guid>
      <category>physics.comp-ph</category>
      <category>cs.AR</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-981-99-3091-3_11</arxiv:DOI>
      <dc:creator>Christian Guzman Ruiz, Matthew Dawson, Mario C. Acosta, Oriol Jorba, Eduardo Cesar Galobardes, Carlos P\'erez Garc\'ia-Pando, Kim Serradell</dc:creator>
    </item>
    <item>
      <title>Highly Optimized Kernels and Fine-Grained Codebooks for LLM Inference on Arm CPUs</title>
      <link>https://arxiv.org/abs/2501.00032</link>
      <description>arXiv:2501.00032v1 Announce Type: cross 
Abstract: Large language models (LLMs) have transformed the way we think about language understanding and generation, enthralling both researchers and developers. However, deploying LLMs for inference has been a significant challenge due to their unprecedented size and resource requirements. While quantizing model weights to sub-byte precision has emerged as a promising solution to ease memory pressure, the group quantization formats commonly used for LLM quantization have significant compute overheads and a resource-intensive dequantization process. As a result, a higher proportion of compute instructions do not perform multiplies, i.e., real work, rendering them unsuitable for meeting the required latency requirements for LLMs deployed on commodity CPUs. In this work, we propose a set of highly optimized kernels to accelerate LLM inference and unleash the full potential of CPUs, particularly Arm CPUs. These kernels amortize the cost of loading the operands and the cost of weight unpacking across multiple output rows. This, along with the introduction of an optimized interleaved group data layout for weights and decompression path optimizations to reduce unnecessary operations and dequantization overhead while maximizing the use of vector and matrix multiply operations, significantly improves the efficiency of MAC operations. Furthermore, we present a groupwise non-uniform codebook-based quantization method for ultra-low-precision quantization of LLMs to better match non-uniform patterns in their weight distributions, demonstrating better throughput during token generation while ensuring better quality than the state-of-the-art. Applying these improvements to 4-bit LLMs results in a 3-3.2x improvement in prompt processing and a 2x improvement in autoregressive decoding on Arm CPUs, compared to LLaMA.cpp-based solution. The optimized kernels are available at https://github.com/ggerganov/llama.cpp.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00032v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.CL</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dibakar Gope, David Mansell, Danny Loh, Ian Bratt</dc:creator>
    </item>
    <item>
      <title>Debunking the CUDA Myth Towards GPU-based AI Systems</title>
      <link>https://arxiv.org/abs/2501.00210</link>
      <description>arXiv:2501.00210v1 Announce Type: cross 
Abstract: With the rise of AI, NVIDIA GPUs have become the de facto standard for AI system design. This paper presents a comprehensive evaluation of Intel Gaudi NPUs as an alternative to NVIDIA GPUs for AI model serving. First, we create a suite of microbenchmarks to compare Intel Gaudi-2 with NVIDIA A100, showing that Gaudi-2 achieves competitive performance not only in primitive AI compute, memory, and communication operations but also in executing several important AI workloads end-to-end. We then assess Gaudi NPU's programmability by discussing several software-level optimization strategies to employ for implementing critical FBGEMM operators and vLLM, evaluating their efficiency against GPU-optimized counterparts. Results indicate that Gaudi-2 achieves energy efficiency comparable to A100, though there are notable areas for improvement in terms of software maturity. Overall, we conclude that, with effective integration into high-level AI frameworks, Gaudi NPUs could challenge NVIDIA GPU's dominance in the AI server market, though further improvements are necessary to fully compete with NVIDIA's robust software ecosystem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00210v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yunjae Lee, Juntaek Lim, Jehyeon Bang, Eunyeong Cho, Huijong Jeong, Taesu Kim, Hyungjun Kim, Joonhyung Lee, Jinseop Im, Ranggi Hwang, Se Jung Kwon, Dongsoo Lee, Minsoo Rhu</dc:creator>
    </item>
    <item>
      <title>Q3DE: A fault-tolerant quantum computer architecture for multi-bit burst errors by cosmic rays</title>
      <link>https://arxiv.org/abs/2501.00331</link>
      <description>arXiv:2501.00331v1 Announce Type: cross 
Abstract: Demonstrating small error rates by integrating quantum error correction (QEC) into an architecture of quantum computing is the next milestone towards scalable fault-tolerant quantum computing (FTQC). Encoding logical qubits with superconducting qubits and surface codes is considered a promising candidate for FTQC architectures. In this paper, we propose an FTQC architecture, which we call Q3DE, that enhances the tolerance to multi-bit burst errors (MBBEs) by cosmic rays with moderate changes and overhead. There are three core components in Q3DE: in-situ anomaly DEtection, dynamic code DEformation, and optimized error DEcoding. In this architecture, MBBEs are detected only from syndrome values for error correction. The effect of MBBEs is immediately mitigated by dynamically increasing the encoding level of logical qubits and re-estimating probable recovery operation with the rollback of the decoding process. We investigate the performance and overhead of the Q3DE architecture with quantum-error simulators and demonstrate that Q3DE effectively reduces the period of MBBEs by 1000 times and halves the size of their region. Therefore, Q3DE significantly relaxes the requirement of qubit density and qubit chip size to realize FTQC. Our scheme is versatile for mitigating MBBEs, i.e., temporal variations of error properties, on a wide range of physical devices and FTQC architectures since it relies only on the standard features of topological stabilizer codes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00331v1</guid>
      <category>quant-ph</category>
      <category>cs.AR</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/MICRO56248.2022.00079</arxiv:DOI>
      <arxiv:journal_reference>2022 55th IEEE/ACM International Symposium on Microarchitecture (MICRO), Chicago, IL, USA, 2022, pp. 1110-1125</arxiv:journal_reference>
      <dc:creator>Yasunari Suzuki, Takanori Sugiyama, Tomochika Arai, Wang Liao, Koji Inoue, Teruo Tanimoto</dc:creator>
    </item>
    <item>
      <title>12-bit Delta-Sigma ADC operating at a temperature of up to 250C in Standard 0.18 $\mu$m SOI CMOS</title>
      <link>https://arxiv.org/abs/2501.00482</link>
      <description>arXiv:2501.00482v1 Announce Type: cross 
Abstract: Some applications require electronic systems to operate at extremely high temperature. Extending the operating temperature range of automotive-grade CMOS processes -- through the use of dedicated design techniques -- can provide an important cost-effective advantage. We present a second-order discrete-time delta-sigma analog-to-digital converter operating at a temperature of up to 250 $^\circ$C, well beyond the 175 $^\circ$C qualification temperature of the automotive-grade CMOS process used for its fabrication (XFAB XT018). The analog-to-digital converter incorporates design techniques that are effective in mitigating the adverse effects of the high temperature, such as increased leakage currents and electromigration. We use configurations of dummy transistors for leakage compensation, clock-boosting methods to limit pass-gate cross-talk, and we optimized the circuit architecture to ensure stability and accuracy at high temperature. Comprehensive measurements demonstrate that the analog-to-digital converter achieves a signal-to-noise ratio exceeding 93 dB at 250 $^\circ$C, with an effective number of bits of 12, and a power consumption of only 44~mW. The die area of the converter is only 0.065~mm$^2$ and the area overhead of the high-temperature mitigation circuits is only 13.7%. The Schreier Figure of Merit is 140~dB at the maximum temperature of 250 $^\circ$C, proving the potential of the circuit for reliable operation in challenging applications such as gas and oil extraction and aeronautics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00482v1</guid>
      <category>eess.SP</category>
      <category>cs.AR</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Christian Sbrana, Alessandro Catania, Tommaso Toschi, Sebastiano Strangio, Giuseppe Iannaccone</dc:creator>
    </item>
    <item>
      <title>Experimental Demonstration of an Optical Neural PDE Solver via On-Chip PINN Training</title>
      <link>https://arxiv.org/abs/2501.00742</link>
      <description>arXiv:2501.00742v1 Announce Type: cross 
Abstract: Partial differential equation (PDE) is an important math tool in science and engineering. This paper experimentally demonstrates an optical neural PDE solver by leveraging the back-propagation-free on-photonic-chip training of physics-informed neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00742v1</guid>
      <category>cs.LG</category>
      <category>cs.AR</category>
      <category>physics.optics</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yequan Zhao, Xian Xiao, Antoine Descos, Yuan Yuan, Xinling Yu, Geza Kurczveil, Marco Fiorentino, Zheng Zhang, Raymond G. Beausoleil</dc:creator>
    </item>
    <item>
      <title>DMSA: A Decentralized Microservice Architecture for Edge Networks</title>
      <link>https://arxiv.org/abs/2501.00883</link>
      <description>arXiv:2501.00883v1 Announce Type: cross 
Abstract: The dispersed node locations and complex topologies of edge networks, combined with intricate dynamic microservice dependencies, render traditional centralized microservice architectures (MSAs) unsuitable. In this paper, we propose a decentralized microservice architecture (DMSA), which delegates scheduling functions from the control plane to edge nodes. DMSA redesigns and implements three core modules of microservice discovery, monitoring, and scheduling for edge networks to achieve precise awareness of instance deployments, low monitoring overhead and measurement errors, and accurate dynamic scheduling, respectively. Particularly, DMSA has customized a microservice scheduling scheme that leverages multi-port listening and zero-copy forwarding to guarantee high data forwarding efficiency. Moreover, a dynamic weighted multi-level load balancing algorithm is proposed to adjust scheduling dynamically with consideration of reliability, priority, and response delay. Finally, we have implemented a physical verification platform for DMSA. Extensive empirical results demonstrate that compared to state-of-the-art and traditional scheduling schemes, DMSA effectively counteracts link failures and network fluctuations, improving the service response delay and execution success rate by approximately $60\% \sim 75\%$ and $10\%\sim15\%$, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00883v1</guid>
      <category>cs.NI</category>
      <category>cs.AR</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuang Chen, Chengdi Lu, Yongsheng Huang, Chang Wu, Fengqian Guo, Hancheng Lu, Chang Wen Chen</dc:creator>
    </item>
    <item>
      <title>Commercial Evaluation of Zero-Skipping MAC Design for Bit Sparsity Exploitation in DL Inference</title>
      <link>https://arxiv.org/abs/2402.19376</link>
      <description>arXiv:2402.19376v2 Announce Type: replace 
Abstract: General Matrix Multiply (GEMM) units, consisting of multiply-accumulate (MAC) arrays, perform bulk of the computation in deep learning (DL). Recent work has proposed a novel MAC design, Bit-Pragmatic (PRA), capable of dynamically exploiting bit sparsity. This work presents OzMAC (Omit-zero-MAC), a modified re-implementation of PRA, but extends beyond earlier works by performing rigorous post-synthesis evaluation against binary MAC design across multiple bitwidths and clock frequencies using TSMC N5 process node to assess commercial implementation potential. We demonstrate the existence of high bit sparsity in eight pretrained INT8 DL workloads and show that 8-bit OzMAC improves all three metrics of area, power, and energy significantly by 21%, 70%, and 28%, respectively. Similar improvements are achieved when scaling data precisions (4, 8, 16 bits) and clock frequencies (0.5 GHz, 1 GHz, 1.5 GHz). For the 8-bit OzMAC, scaling its frequency to normalize the throughput, it still achieves 30% improvement on both power and energy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.19376v2</guid>
      <category>cs.AR</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/VLSI-SoC62099.2024.10767792</arxiv:DOI>
      <dc:creator>Harideep Nair, Prabhu Vellaisamy, Tsung-Han Lin, Perry Wang, Shawn Blanton, John Paul Shen</dc:creator>
    </item>
    <item>
      <title>ReducedLUT: Table Decomposition with "Don't Care" Conditions</title>
      <link>https://arxiv.org/abs/2412.18579</link>
      <description>arXiv:2412.18579v2 Announce Type: replace 
Abstract: Lookup tables (LUTs) are frequently used to efficiently store arrays of precomputed values for complex mathematical computations. When used in the context of neural networks, these functions exhibit a lack of recognizable patterns which presents an unusual challenge for conventional logic synthesis techniques. Several approaches are known to break down a single large lookup table into multiple smaller ones that can be recombined. Traditional methods, such as plain tabulation, piecewise linear approximation, and multipartite table methods, often yield inefficient hardware solutions when applied to LUT-based NNs.
  This paper introduces ReducedLUT, a novel method to reduce the footprint of the LUTs by injecting don't cares into the compression process. This additional freedom introduces more self-similarities which can be exploited using known decomposition techniques. We then demonstrate a particular application to machine learning; by replacing unobserved patterns within the training data of neural network models with don't cares, we enable greater compression with minimal model accuracy degradation. In practice, we achieve up to $1.63\times$ reduction in Physical LUT utilization, with a test accuracy drop of no more than $0.01$ accuracy points.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18579v2</guid>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706628.3708823</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 2025 ACM/SIGDA International Symposium on Field Programmable Gate Arrays (FPGA '25), February 27--March 1, 2025, Monterey, CA, USA</arxiv:journal_reference>
      <dc:creator>Oliver Cassidy, Marta Andronic, Samuel Coward, George A. Constantinides</dc:creator>
    </item>
  </channel>
</rss>
