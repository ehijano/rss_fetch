<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 17 Jul 2024 01:51:54 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 16 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Positive and Unlabeled Data: Model, Estimation, Inference, and Classification</title>
      <link>https://arxiv.org/abs/2407.09735</link>
      <description>arXiv:2407.09735v1 Announce Type: new 
Abstract: This study introduces a new approach to addressing positive and unlabeled (PU) data through the double exponential tilting model (DETM). Traditional methods often fall short because they only apply to selected completely at random (SCAR) PU data, where the labeled positive and unlabeled positive data are assumed to be from the same distribution. In contrast, our DETM's dual structure effectively accommodates the more complex and underexplored selected at random PU data, where the labeled and unlabeled positive data can be from different distributions. We rigorously establish the theoretical foundations of DETM, including identifiability, parameter estimation, and asymptotic properties. Additionally, we move forward to statistical inference by developing a goodness-of-fit test for the SCAR condition and constructing confidence intervals for the proportion of positive instances in the target domain. We leverage an approximated Bayes classifier for classification tasks, demonstrating DETM's robust performance in prediction. Through theoretical insights and practical applications, this study highlights DETM as a comprehensive framework for addressing the challenges of PU data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09735v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siyan Liu, Chi-Kuang Yeh, Xin Zhang, Qinglong Tian, Pengfei Li</dc:creator>
    </item>
    <item>
      <title>Sparse Asymptotic PCA: Identifying Sparse Latent Factors Across Time Horizon</title>
      <link>https://arxiv.org/abs/2407.09738</link>
      <description>arXiv:2407.09738v1 Announce Type: new 
Abstract: This paper proposes a novel method for sparse latent factor modeling using a new sparse asymptotic Principal Component Analysis (APCA). This approach analyzes the co-movements of large-dimensional panel data systems over time horizons within a general approximate factor model framework. Unlike existing sparse factor modeling approaches based on sparse PCA, which assume sparse loading matrices, our sparse APCA assumes that factor processes are sparse over the time horizon, while the corresponding loading matrices are not necessarily sparse. This development is motivated by the observation that the assumption of sparse loadings may not be appropriate for financial returns, where exposure to market factors is generally universal and non-sparse. We propose a truncated power method to estimate the first sparse factor process and a sequential deflation method for multi-factor cases. Additionally, we develop a data-driven approach to identify the sparsity of risk factors over the time horizon using a novel cross-sectional cross-validation method. Theoretically, we establish that our estimators are consistent under mild conditions. Monte Carlo simulations demonstrate that the proposed method performs well in finite samples. Empirically, we analyze daily stock returns for a balanced panel of S&amp;P 500 stocks from January 2004 to December 2016. Through textual analysis, we examine specific events associated with the identified sparse factors that systematically influence the stock market. Our approach offers a new pathway for economists to study and understand the systematic risks of economic and financial systems over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09738v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>q-fin.ST</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaoxing Gao</dc:creator>
    </item>
    <item>
      <title>Exploring Differences between Two Decades of Mental Health Related Emergency Department Visits by Youth via Recurrent Events Analyses</title>
      <link>https://arxiv.org/abs/2407.09761</link>
      <description>arXiv:2407.09761v1 Announce Type: new 
Abstract: We aim to develop a tool for understanding how the mental health of youth aged less than 18 years evolve over time through administrative records of mental health related emergency department (MHED) visits in two decades. Administrative health data usually contain rich information for investigating public health issues; however, many restrictions and regulations apply to their use. Moreover, the data are usually not in a conventional format since administrative databases are created and maintained to serve non-research purposes and only information for people who seek health services is accessible. Analysis of administrative health data is thus challenging in general. In the MHED data analyses, we are particularly concerned with (i) evaluating dynamic patterns and impacts with doubly-censored recurrent event data, and (ii) re-calibrating estimators developed based on truncated data by leveraging summary statistics from the population. The findings are verified empirically via simulation. We have established the asymptotic properties of the inference procedures. The contributions of this paper are twofold. We present innovative strategies for processing doubly-censored recurrent event data, and overcoming the truncation induced by the data collection. In addition, through exploring the pediatric MHED visit records, we provide new insights into children/youths mental health changes over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09761v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yi Xiong, Joan Hu, Rhonda Rosychuk</dc:creator>
    </item>
    <item>
      <title>Valid standard errors for Bayesian quantile regression with clustered and independent data</title>
      <link>https://arxiv.org/abs/2407.09772</link>
      <description>arXiv:2407.09772v1 Announce Type: new 
Abstract: In Bayesian quantile regression, the most commonly used likelihood is the asymmetric Laplace (AL) likelihood. The reason for this choice is not that it is a plausible data-generating model but that the corresponding maximum likelihood estimator is identical to the classical estimator by Koenker and Bassett (1978), and in that sense, the AL likelihood can be thought of as a working likelihood. AL-based quantile regression has been shown to produce good finite-sample Bayesian point estimates and to be consistent. However, if the AL distribution does not correspond to the data-generating distribution, credible intervals based on posterior standard deviations can have poor coverage. Yang, Wang, and He (2016) proposed an adjustment to the posterior covariance matrix that produces asymptotically valid intervals. However, we show that this adjustment is sensitive to the choice of scale parameter for the AL likelihood and can lead to poor coverage when the sample size is small to moderate. We therefore propose using Infinitesimal Jackknife (IJ) standard errors (Giordano &amp; Broderick, 2023). These standard errors do not require resampling but can be obtained from a single MCMC run. We also propose a version of IJ standard errors for clustered data. Simulations and applications to real data show that the IJ standard errors have good frequentist properties, both for independent and clustered data. We provide an R-package that computes IJ standard errors for clustered or independent data after estimation with the brms wrapper in R for Stan.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09772v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Feng Ji, JoonHo Lee, Sophia Rabe-Hesketh</dc:creator>
    </item>
    <item>
      <title>Identification of Average Causal Effects in Confounded Additive Noise Models</title>
      <link>https://arxiv.org/abs/2407.10014</link>
      <description>arXiv:2407.10014v1 Announce Type: new 
Abstract: Additive noise models (ANMs) are an important setting studied in causal inference. Most of the existing works on ANMs assume causal sufficiency, i.e., there are no unobserved confounders. This paper focuses on confounded ANMs, where a set of treatment variables and a target variable are affected by an unobserved confounder that follows a multivariate Gaussian distribution. We introduce a novel approach for estimating the average causal effects (ACEs) of any subset of the treatment variables on the outcome and demonstrate that a small set of interventional distributions is sufficient to estimate all of them. In addition, we propose a randomized algorithm that further reduces the number of required interventions to poly-logarithmic in the number of nodes. Finally, we demonstrate that these interventions are also sufficient to recover the causal structure between the observed variables. This establishes that a poly-logarithmic number of interventions is sufficient to infer the causal effects of any subset of treatments on the outcome in confounded ANMs with high probability, even when the causal structure between treatments is unknown. The simulation results indicate that our method can accurately estimate all ACEs in the finite-sample regime. We also demonstrate the practical significance of our algorithm by evaluating it on semi-synthetic data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10014v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammad Qasim Elahi, Mahsa Ghasemi, Murat Kocaoglu</dc:creator>
    </item>
    <item>
      <title>The inverse Kalman filter</title>
      <link>https://arxiv.org/abs/2407.10089</link>
      <description>arXiv:2407.10089v1 Announce Type: new 
Abstract: In this study, we introduce a new approach, the inverse Kalman filter (IKF), which enables accurate matrix-vector multiplication between a covariance matrix from a dynamic linear model and any real-valued vector with linear computational cost. We incorporate the IKF with the conjugate gradient algorithm, which substantially accelerates the computation of matrix inversion for a general form of covariance matrices, whereas other approximation approaches may not be directly applicable. We demonstrate the scalability and efficiency of the IKF approach through distinct applications, including nonparametric estimation of particle interaction functions and predicting incomplete lattices of correlated data, using both simulation and real-world observations, including cell trajectory and satellite radar interferogram.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10089v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyi Fang, Mengyang Gu</dc:creator>
    </item>
    <item>
      <title>Semiparametric Efficient Inference for the Probability of Necessary and Sufficient Causation</title>
      <link>https://arxiv.org/abs/2407.10185</link>
      <description>arXiv:2407.10185v1 Announce Type: new 
Abstract: Causal attribution, which aims to explain why events or behaviors occur, is crucial in causal inference and enhances our understanding of cause-and-effect relationships in scientific research. The probabilities of necessary causation (PN) and sufficient causation (PS) are two of the most common quantities for attribution in causal inference. While many works have explored the identification or bounds of PN and PS, efficient estimation remains unaddressed. To fill this gap, this paper focuses on obtaining semiparametric efficient estimators of PN and PS under two sets of identifiability assumptions: strong ignorability and monotonicity, and strong ignorability and conditional independence. We derive efficient influence functions and semiparametric efficiency bounds for PN and PS under the two sets of identifiability assumptions, respectively. Based on this, we propose efficient estimators for PN and PS, and show their large sample properties. Extensive simulations validate the superiority of our estimators compared to competing methods. We apply our methods to a real-world dataset to assess various risk factors affecting stroke.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10185v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaoqing Tian, Peng Wu</dc:creator>
    </item>
    <item>
      <title>Two-way Threshold Matrix Autoregression</title>
      <link>https://arxiv.org/abs/2407.10272</link>
      <description>arXiv:2407.10272v1 Announce Type: new 
Abstract: Matrix-valued time series data are widely available in various applications, attracting increasing attention in the literature. However, while nonlinearity has been recognized, the literature has so far neglected a deeper and more intricate level of nonlinearity, namely the {\it row-level} nonlinear dynamics and the {\it column-level} nonlinear dynamics, which are often observed in economic and financial data. In this paper, we propose a novel two-way threshold matrix autoregression (TWTMAR) model. This model is designed to effectively characterize the threshold structure in both rows and columns of matrix-valued time series. Unlike existing models that consider a single threshold variable or assume a uniform structure change across the matrix, the TWTMAR model allows for distinct threshold effects for rows and columns using two threshold variables. This approach achieves greater dimension reduction and yields better interpretation compared to existing methods. Moreover, we propose a parameter estimation procedure leveraging the intrinsic matrix structure and investigate the asymptotic properties. The efficacy and flexibility of the model are demonstrated through both simulation studies and an empirical analysis of the Fama-French Portfolio dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10272v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cheng Yu, Dong Li, XInyu Zhang, Howell Tong</dc:creator>
    </item>
    <item>
      <title>An integrated perspective of robustness in regression through the lens of the bias-variance trade-off</title>
      <link>https://arxiv.org/abs/2407.10418</link>
      <description>arXiv:2407.10418v1 Announce Type: new 
Abstract: This paper presents an integrated perspective on robustness in regression. Specifically, we examine the relationship between traditional outlier-resistant robust estimation and robust optimization, which focuses on parameter estimation resistant to imaginary dataset-perturbations. While both are commonly regarded as robust methods, these concepts demonstrate a bias-variance trade-off, indicating that they follow roughly converse strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10418v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Akifumi Okuno</dc:creator>
    </item>
    <item>
      <title>Inference at the data's edge: Gaussian processes for modeling and inference under model-dependency, poor overlap, and extrapolation</title>
      <link>https://arxiv.org/abs/2407.10442</link>
      <description>arXiv:2407.10442v1 Announce Type: new 
Abstract: The Gaussian Process (GP) is a highly flexible non-linear regression approach that provides a principled approach to handling our uncertainty over predicted (counterfactual) values. It does so by computing a posterior distribution over predicted point as a function of a chosen model space and the observed data, in contrast to conventional approaches that effectively compute uncertainty estimates conditionally on placing full faith in a fitted model. This is especially valuable under conditions of extrapolation or weak overlap, where model dependency poses a severe threat. We first offer an accessible explanation of GPs, and provide an implementation suitable to social science inference problems. In doing so we reduce the number of user-chosen hyperparameters from three to zero. We then illustrate the settings in which GPs can be most valuable: those where conventional approaches have poor properties due to model-dependency/extrapolation in data-sparse regions. Specifically, we apply it to (i) comparisons in which treated and control groups have poor covariate overlap; (ii) interrupted time-series designs, where models are fitted prior to an event by extrapolated after it; and (iii) regression discontinuity, which depends on model estimates taken at or just beyond the edge of their supporting data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10442v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soonhong Cho, Doeun Kim, Chad Hazlett</dc:creator>
    </item>
    <item>
      <title>Nonparametric Multivariate Profile Monitoring Via Tree Ensembles</title>
      <link>https://arxiv.org/abs/2407.10721</link>
      <description>arXiv:2407.10721v1 Announce Type: new 
Abstract: Monitoring random profiles over time is used to assess whether the system of interest, generating the profiles, is operating under desired conditions at any time-point. In practice, accurate detection of a change-point within a sequence of responses that exhibit a functional relationship with multiple explanatory variables is an important goal for effectively monitoring such profiles. We present a nonparametric method utilizing ensembles of regression trees and random forests to model the functional relationship along with associated Kolmogorov-Smirnov statistic to monitor profile behavior. Through a simulation study considering multiple factors, we demonstrate that our method offers strong performance and competitive detection capability when compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10721v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel A. Timme, Andr\'es F. Barrientos, Eric Chicken, Debajyoti Sinha</dc:creator>
    </item>
    <item>
      <title>Joint Learning from Heterogeneous Rank Data</title>
      <link>https://arxiv.org/abs/2407.10846</link>
      <description>arXiv:2407.10846v1 Announce Type: new 
Abstract: The statistical modelling of ranking data has a long history and encompasses various perspectives on how observed rankings arise. One of the most common models, the Plackett-Luce model, is frequently used to aggregate rankings from multiple rankers into a single ranking that corresponds to the underlying quality of the ranked objects. Given that rankers frequently exhibit heterogeneous preferences, mixture-type models have been developed to group rankers with more or less homogeneous preferences together to reduce bias. However, occasionally, these preference groups are known a-priori. Under these circumstances, current practice consists of fitting Plackett-Luce models separately for each group. Nevertheless, there might be some commonalities between different groups of rankers, such that separate estimation implies a loss of information. We propose an extension of the Plackett-Luce model, the Sparse Fused Plackett-Luce model, that allows for joint learning of such heterogeneous rank data, whereby information from different groups is utilised to achieve better model performance. The observed rankings can be considered a function of variables pertaining to the ranked objects. As such, we allow for these types of variables, where information on the coefficients is shared across groups. Moreover, as not all variables might be relevant for the ranking of an object, we impose sparsity on the coefficients to improve interpretability, estimation and prediction of the model. Simulations studies indicate superior performance of the proposed method compared to existing approaches. To illustrate the usage and interpretation of the method, an application on data consisting of consumer preferences regarding various sweet potato varieties is provided. An R package containing the proposed methodology can be found on https://CRAN.R-project.org/package=SFPL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10846v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sjoerd Hermes, Joost van Heerwaarden, Pariya Behrouzi</dc:creator>
    </item>
    <item>
      <title>Multi-object Data Integration in the Study of Primary Progressive Aphasia</title>
      <link>https://arxiv.org/abs/2407.09542</link>
      <description>arXiv:2407.09542v1 Announce Type: cross 
Abstract: This article focuses on a multi-modal imaging data application where structural/anatomical information from gray matter (GM) and brain connectivity information in the form of a brain connectome network from functional magnetic resonance imaging (fMRI) are available for a number of subjects with different degrees of primary progressive aphasia (PPA), a neurodegenerative disorder (ND) measured through a speech rate measure on motor speech loss. The clinical/scientific goal in this study becomes the identification of brain regions of interest significantly related to the speech rate measure to gain insight into ND patterns. Viewing the brain connectome network and GM images as objects, we develop an integrated object response regression framework of network and GM images on the speech rate measure. A novel integrated prior formulation is proposed on network and structural image coefficients in order to exploit network information of the brain connectome while leveraging the interconnections among the two objects. The principled Bayesian framework allows the characterization of uncertainty in ascertaining a region being actively related to the speech rate measure. Our framework yields new insights into the relationship of brain regions associated with PPA, offering a deeper understanding of neuro-degenerative patterns of PPA. The supplementary file adds details about posterior computation and additional empirical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09542v1</guid>
      <category>q-bio.NC</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rene Gutierrez, Rajarshi Guhaniyogi, Aaron Scheffler, Maria Luisa Gorno-Tempini, Maria Luisa Mandelli, Giovanni Battistella</dc:creator>
    </item>
    <item>
      <title>Granger Causality in Extremes</title>
      <link>https://arxiv.org/abs/2407.09632</link>
      <description>arXiv:2407.09632v1 Announce Type: cross 
Abstract: We introduce a rigorous mathematical framework for Granger causality in extremes, designed to identify causal links from extreme events in time series. Granger causality plays a pivotal role in uncovering directional relationships among time-varying variables. While this notion gains heightened importance during extreme and highly volatile periods, state-of-the-art methods primarily focus on causality within the body of the distribution, often overlooking causal mechanisms that manifest only during extreme events. Our framework is designed to infer causality mainly from extreme events by leveraging the causal tail coefficient. We establish equivalences between causality in extremes and other causal concepts, including (classical) Granger causality, Sims causality, and structural causality. We prove other key properties of Granger causality in extremes and show that the framework is especially helpful under the presence of hidden confounders. We also propose a novel inference method for detecting the presence of Granger causality in extremes from data. Our method is model-free, can handle non-linear and high-dimensional time series, outperforms current state-of-the-art methods in all considered setups, both in performance and speed, and was found to uncover coherent effects when applied to financial and extreme weather observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09632v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juraj Bodik, Olivier Pasche</dc:creator>
    </item>
    <item>
      <title>Molecular clouds: do they deserve a non-Gaussian description?</title>
      <link>https://arxiv.org/abs/2407.09832</link>
      <description>arXiv:2407.09832v1 Announce Type: cross 
Abstract: Molecular clouds show complex structures reflecting their non-linear dynamics. Many studies, investigating the bridge between their morphology and physical properties, have shown the interest provided by non-Gaussian higher-order statistics to grasp physical information. Yet, as this bridge is usually characterized in the supervised world of simulations, transferring it onto observations can be hazardous, especially when the discrepancy between simulations and observations remains unknown. In this paper, we aim at identifying relevant summary statistics directly from the observation data. To do so, we develop a test that compares the informative power of two sets of summary statistics for a given dataset. Contrary to supervised approaches, this test does not require the knowledge of any data label or parameter, but focuses instead on comparing the degeneracy levels of these descriptors, relying on a notion of statistical compatibility. We apply this test to column density maps of 14 nearby molecular clouds observed by Herschel, and iteratively compare different sets of usual summary statistics. We show that a standard Gaussian description of these clouds is highly degenerate but can be substantially improved when being estimated on the logarithm of the maps. This illustrates that low-order statistics, properly used, remain a very powerful tool. We then further show that such descriptions still exhibit a small quantity of degeneracies, some of which are lifted by the higher order statistics provided by reduced wavelet scattering transforms. This property of observations quantitatively differs from state-of-the-art simulations of dense molecular cloud collapse and is not reproduced by logfBm models. Finally we show how the summary statistics identified can be cooperatively used to build a morphological distance, which is evaluated visually, and gives very satisfactory results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09832v1</guid>
      <category>astro-ph.GA</category>
      <category>astro-ph.IM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pablo Richard, Erwan Allys, Fran\c{c}ois Levrier, Antoine Gusdorf, Constant Auclair</dc:creator>
    </item>
    <item>
      <title>Optimal Kernel Choice for Score Function-based Causal Discovery</title>
      <link>https://arxiv.org/abs/2407.10132</link>
      <description>arXiv:2407.10132v1 Announce Type: cross 
Abstract: Score-based methods have demonstrated their effectiveness in discovering causal relationships by scoring different causal structures based on their goodness of fit to the data. Recently, Huang et al. proposed a generalized score function that can handle general data distributions and causal relationships by modeling the relations in reproducing kernel Hilbert space (RKHS). The selection of an appropriate kernel within this score function is crucial for accurately characterizing causal relationships and ensuring precise causal discovery. However, the current method involves manual heuristic selection of kernel parameters, making the process tedious and less likely to ensure optimality. In this paper, we propose a kernel selection method within the generalized score function that automatically selects the optimal kernel that best fits the data. Specifically, we model the generative process of the variables involved in each step of the causal graph search procedure as a mixture of independent noise variables. Based on this model, we derive an automatic kernel selection method by maximizing the marginal likelihood of the variables involved in each search step. We conduct experiments on both synthetic data and real-world benchmarks, and the results demonstrate that our proposed method outperforms heuristic kernel selection methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10132v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenjie Wang, Biwei Huang, Feng Liu, Xinge You, Tongliang Liu, Kun Zhang, Mingming Gong</dc:creator>
    </item>
    <item>
      <title>Hidden Markov models with an unknown number of states and a repulsive prior on the state parameters</title>
      <link>https://arxiv.org/abs/2407.10869</link>
      <description>arXiv:2407.10869v1 Announce Type: cross 
Abstract: Hidden Markov models (HMMs) offer a robust and efficient framework for analyzing time series data, modelling both the underlying latent state progression over time and the observation process, conditional on the latent state. However, a critical challenge lies in determining the appropriate number of underlying states, often unknown in practice. In this paper, we employ a Bayesian framework, treating the number of states as a random variable and employing reversible jump Markov chain Monte Carlo to sample from the posterior distributions of all parameters, including the number of states. Additionally, we introduce repulsive priors for the state parameters in HMMs, and hence avoid overfitting issues and promote parsimonious models with dissimilar state components. We perform an extensive simulation study comparing performance of models with independent and repulsive prior distributions on the state parameters, and demonstrate our proposed framework on two ecological case studies: GPS tracking data on muskox in Antarctica and acoustic data on Cape gannets in South Africa. Our results highlight how our framework effectively explores the model space, defined by models with different latent state dimensions, while leading to latent states that are distinguished better and hence are more interpretable, enabling better understanding of complex dynamic systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10869v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ioannis Rotous, Alex Diana, Alessio Farcomeni, Eleni Material, Andr\'ea Thiebault</dc:creator>
    </item>
    <item>
      <title>Robust inference for matching under rolling enrollment</title>
      <link>https://arxiv.org/abs/2205.01061</link>
      <description>arXiv:2205.01061v3 Announce Type: replace 
Abstract: Matching in observational studies faces complications when units enroll in treatment on a rolling basis. While each treated unit has a specific time of entry into the study, control units each have many possible comparison, or "pseudo-treatment," times. The recent GroupMatch framework (Pimentel et al., 2020) solves this problem by searching over all possible pseudo-treatment times for each control and selecting those permitting the closest matches based on covariate histories. However, valid methods of inference have been described only for special cases of the general GroupMatch design, and these rely on strong assumptions. We provide three important innovations to address these problems. First, we introduce a new design, GroupMatch with instance replacement, that allows additional flexibility in control selection and proves more amenable to analysis. Second, we propose a block bootstrap approach for inference in GroupMatch with instance replacement and demonstrate that it accounts properly for complex correlations across matched sets. Third, we develop a permutation-based falsification test to detect possible violations of the important timepoint agnosticism assumption underpinning GroupMatch, which requires homogeneity of potential outcome means across time. Via simulation and a case study of the impact of short-term injuries on batting performance in major league baseball, we demonstrate the effectiveness of our methods for data analysis in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.01061v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Journal of Causal Inference. 11 (2023)</arxiv:journal_reference>
      <dc:creator>Amanda K. Glazer, Samuel D. Pimentel</dc:creator>
    </item>
    <item>
      <title>Factorized Fusion Shrinkage for Dynamic Relational Data</title>
      <link>https://arxiv.org/abs/2210.00091</link>
      <description>arXiv:2210.00091v3 Announce Type: replace 
Abstract: Modern data science applications often involve complex relational data with dynamic structures. An abrupt change in such dynamic relational data is typically observed in systems that undergo regime changes due to interventions. In such a case, we consider a factorized fusion shrinkage model in which all decomposed factors are dynamically shrunk towards group-wise fusion structures, where the shrinkage is obtained by applying global-local shrinkage priors to the successive differences of the row vectors of the factorized matrices. The proposed priors enjoy many favorable properties in comparison and clustering of the estimated dynamic latent factors. Comparing estimated latent factors involves both adjacent and long-term comparisons, with the time range of comparison considered as a variable. Under certain conditions, we demonstrate that the posterior distribution attains the minimax optimal rate up to logarithmic factors. In terms of computation, we present a structured mean-field variational inference framework that balances optimal posterior inference with computational scalability, exploiting both the dependence among components and across time. The framework can accommodate a wide variety of models, including dynamic matrix factorization, latent space models for networks and low-rank tensors. The effectiveness of our methodology is demonstrated through extensive simulations and real-world data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.00091v3</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Peng Zhao, Anirban Bhattacharya, Debdeep Pati, Bani K. Mallick</dc:creator>
    </item>
    <item>
      <title>Latent process models for functional network data</title>
      <link>https://arxiv.org/abs/2210.07491</link>
      <description>arXiv:2210.07491v3 Announce Type: replace 
Abstract: Network data are often sampled with auxiliary information or collected through the observation of a complex system over time, leading to multiple network snapshots indexed by a continuous variable. Many methods in statistical network analysis are traditionally designed for a single network, and can be applied to an aggregated network in this setting, but that approach can miss important functional structure. Here we develop an approach to estimating the expected network explicitly as a function of a continuous index, be it time or another indexing variable. We parameterize the network expectation through low dimensional latent processes, whose components we represent with a fixed, finite-dimensional functional basis. We derive a gradient descent estimation algorithm, establish theoretical guarantees for recovery of the low dimensional structure, compare our method to competitors, and apply it to a data set of international political interactions over time, showing our proposed method to adapt well to data, outperform competitors, and provide interpretable and meaningful results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.07491v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peter W. MacDonald, Elizaveta Levina, Ji Zhu</dc:creator>
    </item>
    <item>
      <title>Improved LM Test for Robust Model Specification Searches in Covariance Structure Analysis</title>
      <link>https://arxiv.org/abs/2306.14302</link>
      <description>arXiv:2306.14302v4 Announce Type: replace 
Abstract: Model specification searches and modifications are commonly employed in covariance structure analysis (CSA) or structural equation modeling (SEM) to improve the goodness-of-fit. However, these practices can be susceptible to capitalizing on chance, as a model that fits one sample may not generalize to another sample from the same population. This paper introduces the improved Lagrange Multipliers (LM) test, which provides a reliable method for conducting a thorough model specification search and effectively identifying missing parameters. By leveraging the stepwise bootstrap method in the standard LM and Wald tests, our data-driven approach enhances the accuracy of parameter identification. The results from Monte Carlo simulations and two empirical applications in political science demonstrate the effectiveness of the improved LM test, particularly when dealing with small sample sizes and models with large degrees of freedom. This approach contributes to better statistical fit and addresses the issue of capitalization on chance in model specification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.14302v4</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bang Quan Zheng, Peter M. Bentler</dc:creator>
    </item>
    <item>
      <title>UTOPIA: Universally Trainable Optimal Prediction Intervals Aggregation</title>
      <link>https://arxiv.org/abs/2306.16549</link>
      <description>arXiv:2306.16549v2 Announce Type: replace 
Abstract: Uncertainty quantification in prediction presents a compelling challenge with vast applications across various domains, including biomedical science, economics, and weather forecasting. There exists a wide array of methods for constructing prediction intervals, such as quantile regression and conformal prediction. However, practitioners often face the challenge of selecting the most suitable method for a specific real-world data problem. In response to this dilemma, we introduce a novel and universally applicable strategy called Universally Trainable Optimal Predictive Intervals Aggregation (UTOPIA). This technique excels in efficiently aggregating multiple prediction intervals while maintaining a small average width of the prediction band and ensuring coverage. UTOPIA is grounded in linear or convex programming, making it straightforward to train and implement. In the specific case where the prediction methods are elementary basis functions, as in kernel and spline bases, our method becomes the construction of a prediction band. Our proposed methodologies are supported by theoretical guarantees on the coverage probability and the average width of the aggregated prediction interval, which are detailed in this paper. The practicality and effectiveness of UTOPIA are further validated through its application to synthetic data and two real-world datasets in finance and macroeconomics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16549v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jianqing Fan, Jiawei Ge, Debarghya Mukherjee</dc:creator>
    </item>
    <item>
      <title>Quantifying predictive uncertainty of aphasia severity in stroke patients with sparse heteroscedastic Bayesian high-dimensional regression</title>
      <link>https://arxiv.org/abs/2309.08783</link>
      <description>arXiv:2309.08783v4 Announce Type: replace 
Abstract: Sparse linear regression methods for high-dimensional data commonly assume that residuals have constant variance, which can be violated in practice. For example, Aphasia Quotient (AQ) is a critical measure of language impairment and informs treatment decisions, but it is challenging to measure in stroke patients. It is of interest to use high-resolution T2 neuroimages of brain damage to predict AQ. However, sparse regression models show marked evidence of heteroscedastic error even after transformations are applied. This violation of the homoscedasticity assumption can lead to bias in estimated coefficients, prediction intervals (PI) with improper length, and increased type I errors. Bayesian heteroscedastic linear regression models relax the homoscedastic error assumption but can enforce restrictive prior assumptions on parameters, and many are computationally infeasible in the high-dimensional setting. This paper proposes estimating high-dimensional heteroscedastic linear regression models using a heteroscedastic partitioned empirical Bayes Expectation Conditional Maximization (H-PROBE) algorithm. H-PROBE is a computationally efficient maximum a posteriori estimation approach that requires minimal prior assumptions and can incorporate covariates hypothesized to impact heterogeneity. We apply this method by using high-dimensional neuroimages to predict and provide PIs for AQ that accurately quantify predictive uncertainty. Our analysis demonstrates that H-PROBE can provide narrower PI widths than standard methods without sacrificing coverage. Narrower PIs are clinically important for determining the risk of moderate to severe aphasia. Additionally, through extensive simulation studies, we exhibit that H-PROBE results in superior prediction, variable selection, and predictive inference compared to alternative methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.08783v4</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anja Zgodic, Ray Bai, Jiajia Zhang, Yuan Wang, Chris Rorden, Alexander McLain</dc:creator>
    </item>
    <item>
      <title>Universally Optimal Multivariate Crossover Designs</title>
      <link>https://arxiv.org/abs/2311.13556</link>
      <description>arXiv:2311.13556v2 Announce Type: replace 
Abstract: In this article, universally optimal multivariate crossover designs are studied. The multiple response crossover design is motivated by a $3 \times 3$ crossover setup, where the effect of $3$ doses of an oral drug are studied on gene expressions related to mucosal inflammation. Subjects are assigned to three treatment sequences and response measurements on $5$ different gene expressions are taken from each subject in each of the $3$ time periods. To model multiple or $g$ responses, where $g&gt;1$, in a crossover setup, a multivariate fixed effect model with both direct and carryover treatment effects is considered. It is assumed that there are non zero within response correlations, while between response correlations are taken to be zero. The information matrix corresponding to the direct effects is obtained and some results are studied. The information matrix in the multivariate case is shown to differ from the univariate case, particularly in the completely symmetric property. For the $g&gt;1$ case, with $t$ treatments and $p$ periods, for $p=t \geq 3$, the design represented by a Type $\rm{I}$ orthogonal array of strength $2$ is proved to be universally optimal over the class of binary designs, for the direct treatment effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.13556v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shubham Niphadkar, Siuli Mukhopadhyay</dc:creator>
    </item>
    <item>
      <title>Adaptive Matrix Change Point Detection: Leveraging Structured Mean Shifts</title>
      <link>https://arxiv.org/abs/2401.17473</link>
      <description>arXiv:2401.17473v2 Announce Type: replace 
Abstract: In high-dimensional time series, the component processes are often assembled into a matrix to display their interrelationship. We focus on detecting mean shifts with unknown change point locations in these matrix time series. Series that are activated by a change may cluster along certain rows (columns), which forms mode-specific change point alignment. Leveraging mode-specific change point alignments may substantially enhance the power for change point detection. Yet, there may be no mode-specific alignments in the change point structure. We propose a powerful test to detect mode-specific change points, yet robust to non-mode-specific changes. We show the validity of using the multiplier bootstrap to compute the p-value of the proposed methods, and derive non-asymptotic bounds on the size and power of the tests. We also propose a parallel bootstrap, a computationally efficient approach for computing the p-value of the proposed adaptive test. In particular, we show the consistency of the proposed test, under mild regularity conditions. To obtain the theoretical results, we derive new, sharp bounds on Gaussian approximation and multiplier bootstrap approximation, which are of independent interest for high dimensional problems with diverging sparsity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.17473v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyu Zhang, Kung-Sik Chan</dc:creator>
    </item>
    <item>
      <title>Multivariate Bayesian models with flexible shared interactions for analyzing spatio-temporal patterns of rare cancers</title>
      <link>https://arxiv.org/abs/2403.10440</link>
      <description>arXiv:2403.10440v2 Announce Type: replace 
Abstract: Rare cancers affect millions of people worldwide each year. However, estimating incidence or mortality rates associated with rare cancers presents important difficulties and poses new statistical methodological challenges. In this paper, we expand the collection of multivariate spatio-temporal models by introducing adaptable shared spatio-temporal components to enable a comprehensive analysis of both incidence and cancer mortality in rare cancer cases. These models allow the modulation of spatio-temporal effects between incidence and mortality, allowing for changes in their relationship over time. The new models have been implemented in INLA using r-generic constructions. We conduct a simulation study to evaluate the performance of the new spatio-temporal models. Our results show that multivariate spatio-temporal models incorporating a flexible shared spatio-temporal term outperform conventional multivariate spatio-temporal models that include specific spatio-temporal effects for each health outcome. We use these models to analyze incidence and mortality data for pancreatic cancer and leukaemia among males across 142 administrative health care districts of Great Britain over a span of nine biennial periods (2002-2019).</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10440v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Garazi Retegui, Jaione Etxeberria, Mar\'ia Dolores Ugarte</dc:creator>
    </item>
    <item>
      <title>Multiple testing with anytime-valid Monte-Carlo p-values</title>
      <link>https://arxiv.org/abs/2404.15586</link>
      <description>arXiv:2404.15586v2 Announce Type: replace 
Abstract: In contemporary problems involving genetic or neuroimaging data, thousands of hypotheses need to be tested. Due to their high power, and finite sample guarantees on type-1 error under weak assumptions, Monte-Carlo permutation tests are often considered as gold standard for these settings. However, the enormous computational effort required for (thousands of) permutation tests is a major burden. Recently, Fischer and Ramdas (2024) constructed a permutation test for a single hypothesis in which the permutations are drawn sequentially one-by-one and the testing process can be stopped at any point without inflating the type I error. They showed that the number of permutations can be substantially reduced (under null and alternative) while the power remains similar. We show how their approach can be modified to make it suitable for a broad class of multiple testing procedures. In particular, we discuss its use with the Benjamini-Hochberg procedure and illustrate the application on a large dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15586v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lasse Fischer, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Process-based Inference for Spatial Energetics Using Bayesian Predictive Stacking</title>
      <link>https://arxiv.org/abs/2405.09906</link>
      <description>arXiv:2405.09906v2 Announce Type: replace 
Abstract: Rapid developments in streaming data technologies have enabled real-time monitoring of human activity that can deliver high-resolution data on health variables over trajectories or paths carved out by subjects as they conduct their daily physical activities. Wearable devices, such as wrist-worn sensors that monitor gross motor activity, have become prevalent and have kindled the emerging field of ``spatial energetics'' in environmental health sciences. We devise a Bayesian inferential framework for analyzing such data while accounting for information available on specific spatial coordinates comprising a trajectory or path using a Global Positioning System (GPS) device embedded within the wearable device. We offer full probabilistic inference with uncertainty quantification using spatial-temporal process models adapted for data generated from ``actigraph'' units as the subject traverses a path or trajectory in their daily routine. Anticipating the need for fast inference for mobile health data, we pursue exact inference using conjugate Bayesian models and employ predictive stacking to assimilate inference across these individual models. This circumvents issues with iterative estimation algorithms such as Markov chain Monte Carlo. We devise Bayesian predictive stacking in this context for models that treat time as discrete epochs and that treat time as continuous. We illustrate our methods with simulation experiments and analysis of data from the Physical Activity through Sustainable Transport Approaches (PASTA-LA) study conducted by the Fielding School of Public Health at the University of California, Los Angeles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09906v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomoya Wakayama, Sudipto Banerjee</dc:creator>
    </item>
    <item>
      <title>Counterfactual Generative Models for Time-Varying Treatments</title>
      <link>https://arxiv.org/abs/2305.15742</link>
      <description>arXiv:2305.15742v5 Announce Type: replace-cross 
Abstract: Estimating the counterfactual outcome of treatment is essential for decision-making in public health and clinical science, among others. Often, treatments are administered in a sequential, time-varying manner, leading to an exponentially increased number of possible counterfactual outcomes. Furthermore, in modern applications, the outcomes are high-dimensional and conventional average treatment effect estimation fails to capture disparities in individuals. To tackle these challenges, we propose a novel conditional generative framework capable of producing counterfactual samples under time-varying treatment, without the need for explicit density estimation. Our method carefully addresses the distribution mismatch between the observed and counterfactual distributions via a loss function based on inverse probability re-weighting, and supports integration with state-of-the-art conditional generative models such as the guided diffusion and conditional variational autoencoder. We present a thorough evaluation of our method using both synthetic and real-world data. Our results demonstrate that our method is capable of generating high-quality counterfactual samples and outperforms the state-of-the-art baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.15742v5</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3637528.3671950</arxiv:DOI>
      <dc:creator>Shenghao Wu, Wenbin Zhou, Minshuo Chen, Shixiang Zhu</dc:creator>
    </item>
    <item>
      <title>A minimum Wasserstein distance approach to Fisher's combination of independent discrete p-values</title>
      <link>https://arxiv.org/abs/2309.07692</link>
      <description>arXiv:2309.07692v2 Announce Type: replace-cross 
Abstract: This paper introduces a comprehensive framework to adjust a discrete test statistic for improving its hypothesis testing procedure. The adjustment minimizes the Wasserstein distance to a null-approximating continuous distribution, tackling some fundamental challenges inherent in combining statistical significances derived from discrete distributions. The related theory justifies Lancaster's mid-p and mean-value chi-squared statistics for Fisher's combination as special cases. However, in order to counter the conservative nature of Lancaster's testing procedures, we propose an updated null-approximating distribution. It is achieved by further minimizing the Wasserstein distance to the adjusted statistics within a proper distribution family. Specifically, in the context of Fisher's combination, we propose an optimal gamma distribution as a substitute for the traditionally used chi-squared distribution. This new approach yields an asymptotically consistent test that significantly improves type I error control and enhances statistical power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.07692v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Gonzalo Contador, Zheyang Wu</dc:creator>
    </item>
    <item>
      <title>Regressions under Adverse Conditions</title>
      <link>https://arxiv.org/abs/2311.13327</link>
      <description>arXiv:2311.13327v2 Announce Type: replace-cross 
Abstract: We introduce a new regression method that relates the mean of an outcome variable to covariates, given the "adverse condition" that a distress variable falls in its tail. This allows to tailor classical mean regressions to adverse economic scenarios, which receive increasing interest in managing macroeconomic and financial risks, among many others. In the terminology of the systemic risk literature, our method can be interpreted as a regression for the Marginal Expected Shortfall. We propose a two-step procedure to estimate the new models, show consistency and asymptotic normality of the estimator, and propose feasible inference under weak conditions allowing for cross-sectional and time series applications. The accuracy of the asymptotic approximations of the two-step estimator is verified in simulations. Two empirical applications show that our regressions under adverse conditions are valuable in such diverse fields as the study of the relation between systemic risk and asset price bubbles, and dissecting macroeconomic growth vulnerabilities into individual components.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.13327v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Timo Dimitriadis, Yannick Hoga</dc:creator>
    </item>
  </channel>
</rss>
