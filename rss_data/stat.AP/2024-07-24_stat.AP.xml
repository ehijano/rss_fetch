<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 24 Jul 2024 04:00:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 24 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A divide-and-conquer approach for spatio-temporal analysis of large house price data from Greater London</title>
      <link>https://arxiv.org/abs/2407.15905</link>
      <description>arXiv:2407.15905v1 Announce Type: new 
Abstract: Statistical research in real estate markets, particularly in understanding the spatio-temporal dynamics of house prices, has garnered significant attention in recent times. Although Bayesian methods are common in spatio-temporal modeling, standard Markov chain Monte Carlo (MCMC) techniques are usually slow for large datasets such as house price data. To tackle this problem, we propose a divide-and-conquer spatio-temporal modeling approach. This method involves partitioning the data into multiple subsets and applying an appropriate Gaussian process model to each subset in parallel. The results from each subset are then combined using the Wasserstein barycenter technique to obtain the global parameters for the original problem. The proposed methodology allows for multiple observations per spatial and time unit, thereby offering added benefits for practitioners. As a real-life application, we analyze house price data of more than 0.6 million transactions from 983 middle layer super output areas in London over a period of eight years. The methodology provides insightful findings about the effects of various amenities, trend patterns, and the relationship between prices and carbon emissions. Furthermore, as demonstrated through a cross-validation study, it shows good predictive accuracy while balancing computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15905v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kapil Gupta, Soudeep Deb</dc:creator>
    </item>
    <item>
      <title>AI for Handball: predicting and explaining the 2024 Olympic Games tournament with Deep Learning and Large Language Models</title>
      <link>https://arxiv.org/abs/2407.15987</link>
      <description>arXiv:2407.15987v1 Announce Type: new 
Abstract: Over summer 2024, the world will be looking at Paris to encourage their favorite athletes win the Olympic gold medal. In handball, few nations will fight hard to win the precious metal with speculations predicting the victory for France or Denmark for men and France or Norway for women. However, there is so far no scientific method proposed to predict the final results of the competition. In this work, we leverage a deep learning model to predict the results of the handball tournament of the 2024 Olympic Games. This model, coupled with explainable AI (xAI) techniques, allows us to extract insightful information about the main factors influencing the outcome of each match. Notably, xAI helps sports experts understand how factors like match information or individual athlete performance contribute to the predictions. Furthermore, we integrate Large Language Models (LLMs) to generate human-friendly explanations that highlight the most important factors impacting the match results. By providing human-centric explanations, our approach offers a deeper understanding of the AI predictions, making them more actionable for coaches and analysts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15987v1</guid>
      <category>stat.AP</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Florian Felice</dc:creator>
    </item>
    <item>
      <title>Investigating the HIV Epidemic in Miami Using a Novel Approach for Bayesian Inference on Partially Observed Networks</title>
      <link>https://arxiv.org/abs/2407.16135</link>
      <description>arXiv:2407.16135v1 Announce Type: new 
Abstract: Molecular HIV Surveillance (MHS) has been described as key to enabling rapid responses to HIV outbreaks. It operates by linking individuals with genetically similar viral sequences, which forms a network. A major limitation of MHS is that it depends on sequence collection, which very rarely covers the entire population of interest. Ignoring missing data by conducting complete case analysis--which assumes that the observed network is complete--has been shown to result in significantly biased estimates of network properties. We use MHS to investigate disease dynamics of the HIV epidemic in Miami-Dade County (MDC) among men who have sex with men (MSM)--only 30.1% have a reported sequence. To do so, we present an approach for making Bayesian inferences on partially observed networks. Through a simulation study, we demonstrate a reduction in error of 43%-63% between our estimates and complete case analyses. We estimate increased mixing between MSM communities in MDC, defined by race and transmission risk compared to the results based on complete case analysis. Our approach makes use of a flexible network model--congruence class model--to overcome the high computational burden of previously reported Bayesian approaches to estimate network properties from partially observed networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16135v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ravi Goyal, Kevin Nguyen, Victor De Gruttola, Susan J Little, Colby Cohen, Natasha K Martin</dc:creator>
    </item>
    <item>
      <title>Spatially-clustered spatial autoregressive models with application to agricultural market concentration in Europe</title>
      <link>https://arxiv.org/abs/2407.15874</link>
      <description>arXiv:2407.15874v1 Announce Type: cross 
Abstract: In this paper, we present an extension of the spatially-clustered linear regression models, namely, the spatially-clustered spatial autoregression (SCSAR) model, to deal with spatial heterogeneity issues in clustering procedures. In particular, we extend classical spatial econometrics models, such as the spatial autoregressive model, the spatial error model, and the spatially-lagged model, by allowing the regression coefficients to be spatially varying according to a cluster-wise structure. Cluster memberships and regression coefficients are jointly estimated through a penalized maximum likelihood algorithm which encourages neighboring units to belong to the same spatial cluster with shared regression coefficients. Motivated by the increase of observed values of the Gini index for the agricultural production in Europe between 2010 and 2020, the proposed methodology is employed to assess the presence of local spatial spillovers on the market concentration index for the European regions in the last decade. Empirical findings support the hypothesis of fragmentation of the European agricultural market, as the regions can be well represented by a clustering structure partitioning the continent into three-groups, roughly approximated by a division among Western, North Central and Southeastern regions. Also, we detect heterogeneous local effects induced by the selected explanatory variables on the regional market concentration. In particular, we find that variables associated with social, territorial and economic relevance of the agricultural sector seem to act differently throughout the spatial dimension, across the clusters and with respect to the pooled model, and temporal dimension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15874v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roy Cerqueti (Department of Social,Economic Sciences, Sapienza University of Rome, Italy,GRANEM, University of Angers, France), Paolo Maranzano (Department Economics, Management,Statistics), Raffaele Mattera (Department of Social,Economic Sciences, Sapienza University of Rome, Italy)</dc:creator>
    </item>
    <item>
      <title>Evaluation of deep learning models for Australian climate extremes: prediction of streamflow and floods</title>
      <link>https://arxiv.org/abs/2407.15882</link>
      <description>arXiv:2407.15882v1 Announce Type: cross 
Abstract: In recent years, climate extremes such as floods have created significant environmental and economic hazards for Australia, causing damage to the environment and economy and losses of human and animal lives. An efficient method of forecasting floods is crucial to limit this damage. Techniques for flood prediction are currently based on hydrological, and hydrodynamic (physically-based) numerical models. Machine learning methods that include deep learning offer certain advantages over conventional physically based approaches, including flexibility and accuracy. Deep learning methods have been promising for predicting small to medium-sized climate extreme events over a short time horizon; however, large flooding events present a critical challenge. We present an ensemble-based machine learning approach that addresses large-scale extreme flooding challenges using a switching mechanism motivated by extreme-value theory for long-short-term-memory (LSTM) deep learning models. We use a multivariate and multi-step time-series prediction approach to predict streamflow for multiple days ahead in the major catchments of Australia. The ensemble framework also employs static information to enrich the time-series information, allowing for regional modelling across catchments. Our results demonstrate enhanced prediction of streamflow extremes, with notable efficacy for large flooding scenarios in the selected Australian catchments. Through comparative analysis, our methodology underscores the potential for deep learning models to revolutionise flood forecasting across diverse regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15882v1</guid>
      <category>cs.LG</category>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siddharth Khedkar, R. Willem Vervoort, Rohitash Chandra</dc:creator>
    </item>
    <item>
      <title>Improving probabilistic forecasts of extreme wind speeds by training statistical post-processing models with weighted scoring rules</title>
      <link>https://arxiv.org/abs/2407.15900</link>
      <description>arXiv:2407.15900v1 Announce Type: cross 
Abstract: Accurate forecasts of extreme wind speeds are of high importance for many applications. Such forecasts are usually generated by ensembles of numerical weather prediction (NWP) models, which however can be biased and have errors in dispersion, thus necessitating the application of statistical post-processing techniques. In this work we aim to improve statistical post-processing models for probabilistic predictions of extreme wind speeds. We do this by adjusting the training procedure used to fit ensemble model output statistics (EMOS) models - a commonly applied post-processing technique - and propose estimating parameters using the so-called threshold-weighted continuous ranked probability score (twCRPS), a proper scoring rule that places special emphasis on predictions over a threshold. We show that training using the twCRPS leads to improved extreme event performance of post-processing models for a variety of thresholds. We find a distribution body-tail trade-off where improved performance for probabilistic predictions of extreme events comes with worse performance for predictions of the distribution body. However, we introduce strategies to mitigate this trade-off based on weighted training and linear pooling. Finally, we consider some synthetic experiments to explain the training impact of the twCRPS and derive closed-form expressions of the twCRPS for a number of distributions, giving the first such collection in the literature. The results will enable researchers and practitioners alike to improve the performance of probabilistic forecasting models for extremes and other events of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15900v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jakob Benjamin Wessel, Christopher A. T. Ferro, Gavin R. Evans, Frank Kwasniok</dc:creator>
    </item>
    <item>
      <title>Improving the Computational Efficiency of Adaptive Audits of IRV Elections</title>
      <link>https://arxiv.org/abs/2407.16465</link>
      <description>arXiv:2407.16465v1 Announce Type: cross 
Abstract: AWAIRE is one of two extant methods for conducting risk-limiting audits of instant-runoff voting (IRV) elections. In principle AWAIRE can audit IRV contests with any number of candidates, but the original implementation incurred memory and computation costs that grew superexponentially with the number of candidates. This paper improves the algorithmic implementation of AWAIRE in three ways that make it practical to audit IRV contests with 55 candidates, compared to the previous 6 candidates. First, rather than trying from the start to rule out all candidate elimination orders that produce a different winner, the algorithm starts by considering only the final round, testing statistically whether each candidate could have won that round. For those candidates who cannot be ruled out at that stage, it expands to consider earlier and earlier rounds until either it provides strong evidence that the reported winner really won or a full hand count is conducted, revealing who really won. Second, it tests a richer collection of conditions, some of which can rule out many elimination orders at once. Third, it exploits relationships among those conditions, allowing it to abandon testing those that are unlikely to help. We provide real-world examples with up to 36 candidates and synthetic examples with up to 55 candidates, showing how audit sample size depends on the margins and on the tuning parameters. An open-source Python implementation is publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16465v1</guid>
      <category>cs.CY</category>
      <category>cs.CR</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Ek, Michelle Blom, Philip B. Stark, Peter J. Stuckey, Damjan Vukcevic</dc:creator>
    </item>
    <item>
      <title>CASTRO -- Efficient constrained sampling method for material and chemical experimental design</title>
      <link>https://arxiv.org/abs/2407.16567</link>
      <description>arXiv:2407.16567v1 Announce Type: cross 
Abstract: The exploration of multicomponent material composition space requires significant time and financial investments, necessitating efficient use of resources for statistically relevant compositions. This article introduces a novel methodology, implemented in the open-source CASTRO (ConstrAined Sequential laTin hypeRcube sampling methOd) software package, to overcome equality-mixture constraints and ensure comprehensive design space coverage. Our approach leverages Latin hypercube sampling (LHS) and LHS with multidimensional uniformity (LHSMDU) using a divide-and-conquer strategy to manage high-dimensional problems effectively. By incorporating previous experimental knowledge within a limited budget, our method strategically recommends a feasible number of experiments to explore the design space. We validate our methodology with two examples: a four-dimensional problem with near-uniform distributions and a nine-dimensional problem with additional mixture constraints, yielding specific component distributions. Our constrained sequential LHS or LHSMDU approach enables thorough design space exploration, proving robustness for experimental design. This research not only advances material science but also offers promising solutions for efficiency challenges in pharmaceuticals and chemicals. CASTRO and the case studies are available for free download on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16567v1</guid>
      <category>stat.CO</category>
      <category>cond-mat.mtrl-sci</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Christina Schenk, Maciej Haranczyk</dc:creator>
    </item>
    <item>
      <title>Dynamic Factor Analysis with Dependent Gaussian Processes for High-Dimensional Gene Expression Trajectories</title>
      <link>https://arxiv.org/abs/2307.02781</link>
      <description>arXiv:2307.02781v2 Announce Type: replace 
Abstract: The increasing availability of high-dimensional, longitudinal measures of gene expression can facilitate understanding of biological mechanisms, as required for precision medicine. Biological knowledge suggests that it may be best to describe complex diseases at the level of underlying pathways, which may interact with one another. We propose a Bayesian approach that allows for characterising such correlation among different pathways through Dependent Gaussian Processes (DGP) and mapping the observed high-dimensional gene expression trajectories into unobserved low-dimensional pathway expression trajectories via Bayesian Sparse Factor Analysis. Our proposal is the first attempt to relax the classical assumption of independent factors for longitudinal data and has demonstrated a superior performance in recovering the shape of pathway expression trajectories, revealing the relationships between genes and pathways, and predicting gene expressions (closer point estimates and narrower predictive intervals), as demonstrated through simulations and real data analysis. To fit the model, we propose a Monte Carlo Expectation Maximization (MCEM) scheme that can be implemented conveniently by combining a standard Markov Chain Monte Carlo sampler and an R package GPFDA (Konzen and others, 2021), which returns the maximum likelihood estimates of DGP hyperparameters. The modular structure of MCEM makes it generalizable to other complex models involving the DGP model component. Our R package DGP4LCF that implements the proposed approach is available on CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.02781v2</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiachen Cai, Robert J. B. Goudie, Colin Starr, Brian D. M. Tom</dc:creator>
    </item>
    <item>
      <title>A new paradigm of mortality modeling via individual vitality dynamics</title>
      <link>https://arxiv.org/abs/2407.15388</link>
      <description>arXiv:2407.15388v2 Announce Type: replace 
Abstract: The significance of mortality modeling extends across multiple research areas, including life insurance valuation, longevity risk management, life-cycle hypothesis, and retirement income planning. Despite the variety of existing approaches, such as mortality laws and factor-based models, they often lack compatibility or fail to meet specific research needs. To address these shortcomings, this study introduces a novel approach centered on modeling the dynamics of individual vitality and defining mortality as the depletion of vitality level to zero. More specifically, we develop a four-component framework to analyze the initial value, trend, diffusion, and sudden changes in vitality level over an individual's lifetime. We demonstrate the framework's estimation and analytical capabilities in various settings and discuss its practical implications in actuarial problems and other research areas. The broad applicability and interpretability of our vitality-based modeling approach offer an enhanced paradigm for mortality modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15388v2</guid>
      <category>stat.AP</category>
      <category>q-fin.RM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaobai Zhu, Kenneth Q. Zhou, Zijia Wang</dc:creator>
    </item>
    <item>
      <title>Mixture of segmentation for heterogeneous functional data</title>
      <link>https://arxiv.org/abs/2303.10712</link>
      <description>arXiv:2303.10712v3 Announce Type: replace-cross 
Abstract: In this paper we consider functional data with heterogeneity in time and in population. We propose a mixture model with segmentation of time to represent this heterogeneity while keeping the functional structure. Maximum likelihood estimator is considered, proved to be identifiable and consistent. In practice, an EM algorithm is used, combined with dynamic programming for the maximization step, to approximate the maximum likelihood estimator. The method is illustrated on a simulated dataset, and used on a real dataset of electricity consumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.10712v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vincent Brault, \'Emilie Devijver, Charlotte Laclau</dc:creator>
    </item>
  </channel>
</rss>
