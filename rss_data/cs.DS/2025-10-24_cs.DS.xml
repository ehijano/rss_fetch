<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 Oct 2025 04:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>On Hardness and Approximation of Broadcasting in Sparse Graphs</title>
      <link>https://arxiv.org/abs/2510.20026</link>
      <description>arXiv:2510.20026v1 Announce Type: new 
Abstract: We study the Telephone Broadcasting problem in sparse graphs. Given a designated source in an undirected graph, the task is to disseminate a message to all vertices in the minimum number of rounds, where in each round every informed vertex may inform at most one uninformed neighbor. For general graphs with $n$ vertices, the problem is NP-hard. Recent work shows that the problem remains NP-hard even on restricted graph classes such as cactus graphs of pathwidth $2$ [Aminian et al., ICALP 2025] and graphs at distance-1 to a path forest [Egami et al., MFCS 2025].
  In this work, we investigate the problem in several sparse graph families. We first prove NP-hardness for $k$-cycle graphs, namely graphs formed by $k$ cycles sharing a single vertex, as well as $k$-path graphs, namely graphs formed by $k$ paths with shared endpoints. Despite multiple efforts to understand the problem in these simple graph families, the computational complexity of the problem had remained unsettled, and our hardness results answer open questions by Bhabak and Harutyunyan [CALDAM 2015] and Harutyunyan and Hovhannisyan [COCAO 2023] concerning the problem's complexity in $k$-cycle and $k$-path graphs, respectively.
  On the positive side, we present Polynomial-Time Approximation Schemes (PTASs) for $k$-cycle and $k$-path graphs, improving over the best existing approximation factors of $2$ for $k$-cycle graphs and an approximation factor of $4$ for $k$-path graphs. Moreover, we identify a structural frontier for tractability by showing that the problem is solvable in polynomial time on graphs of bounded bandwidth. This result generalizes existing tractability results for special sparse families such as necklace graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20026v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeffrey Bringolf, Hovhannes A. Harutyunyan, Shahin Kamali, Seyed-Mohammad Seyed-Javadi</dc:creator>
    </item>
    <item>
      <title>Parallel Joinable B-Trees in the Fork-Join I/O Model</title>
      <link>https://arxiv.org/abs/2510.20053</link>
      <description>arXiv:2510.20053v1 Announce Type: new 
Abstract: Balanced search trees are widely used in computer science to efficiently maintain dynamic ordered data. To support efficient set operations (e.g., union, intersection, difference) using trees, the join-based framework is widely studied. This framework has received particular attention in the parallel setting, and has been shown to be effective in enabling simple and theoretically efficient set operations on trees. Despite the widespread adoption of parallel join-based trees, a major drawback of previous work on such data structures is the inefficiency of their input/output (I/O) access patterns. Some recent work (e.g., C-trees and PaC-trees) focused on more I/O-friendly implementations of these algorithms. Surprisingly, however, there have been no results on bounding the I/O-costs for these algorithms. It remains open whether these algorithms can provide tight, provable guarantees in I/O-costs on trees.
  This paper studies efficient parallel algorithms for set operations based on search tree algorithms using a join-based framework, with a special focus on achieving I/O efficiency in these algorithms. To better capture the I/O-efficiency in these algorithms in parallel, we introduce a new computational model, Fork-Join I/O Model, to measure the I/O costs in fork-join parallelism. This model measures the total block transfers (I/O work) and their critical path (I/O span). Under this model, we propose our new solution based on B-trees. Our parallel algorithm computes the union, intersection, and difference of two B-trees with $O(m \log_B(n/m))$ I/O work and $O(\log_B m \cdot \log_2 \log_B n + \log_B n)$ I/O span, where $n$ and $m \leq n$ are the sizes of the two trees, and $B$ is the block size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20053v1</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Goodrich, Yan Gu, Ryuto Kitagawa, Yihan Sun</dc:creator>
    </item>
    <item>
      <title>Optimal Rounding for Two-Stage Bipartite Matching</title>
      <link>https://arxiv.org/abs/2510.20153</link>
      <description>arXiv:2510.20153v1 Announce Type: new 
Abstract: We study two-stage bipartite matching, in which the edges of a bipartite graph on vertices $(B_1 \cup B_2, I)$ are revealed in two batches. In stage one, a matching must be selected from among revealed edges $E \subseteq B_1 \times I$. In stage two, edges $E^\theta \subseteq B_2 \times I$ are sampled from a known distribution, and a second matching must be selected between $B_2$ and unmatched vertices in $I$. The objective is to maximize the total weight of the combined matching. We design polynomial-time approximations to the optimum online algorithm, achieving guarantees of $7/8$ for vertex-weighted graphs and $2\sqrt{2}-2 \approx 0.828$ for edge-weighted graphs under arbitrary distributions. Both approximation ratios match known upper bounds on the integrality gap of the natural fractional relaxation, improving upon the best-known approximation of 0.767 by Feng, Niazadeh, and Saberi for unweighted graphs whose second batch consists of independently arriving nodes.
  Our results are obtained via an algorithm that rounds a fractional matching revealed in two stages, aiming to match offline nodes (respectively, edges) with probability proportional to their fractional weights, up to a constant-factor loss. We leverage negative association (NA) among offline node availabilities -- a property induced by dependent rounding -- to derive new lower bounds on the expected size of the maximum weight matching in random graphs where one side is realized via NA binary random variables. Moreover, we extend these results to settings where we have only sample access to the distribution. In particular, $\text{poly}(n,\epsilon^{-1})$ samples suffice to obtain an additive loss of $\epsilon$ in the approximation ratio for the vertex-weighted problem; a similar bound holds for the edge-weighted problem with an additional (unavoidable) dependence on the scale of edge weights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20153v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tristan Pollner, Amin Saberi, Anders Wikum</dc:creator>
    </item>
    <item>
      <title>Smoothed Analysis of Online Metric Matching with a Single Sample: Beyond Metric Distortion</title>
      <link>https://arxiv.org/abs/2510.20288</link>
      <description>arXiv:2510.20288v1 Announce Type: new 
Abstract: In the online metric matching problem, $n$ servers and $n$ requests lie in a metric space. Servers are available upfront, and requests arrive sequentially. An arriving request must be matched immediately and irrevocably to an available server, incurring a cost equal to their distance. The goal is to minimize the total matching cost.
  We study this problem in the Euclidean metric $[0, 1]^d$, when servers are adversarial and requests are independently drawn from distinct distributions that satisfy a mild smoothness condition. Our main result is an $O(1)$-competitive algorithm for $d \neq 2$ that requires no distributional knowledge, relying only on a single sample from each request distribution. To our knowledge, this is the first algorithm to achieve an $o(\log n)$ competitive ratio for non-trivial metrics beyond the i.i.d. setting. Our approach bypasses the $\Omega(\log n)$ barrier introduced by probabilistic metric embeddings: instead of analyzing the embedding distortion and the algorithm separately, we directly bound the cost of the algorithm on the target metric of a simple deterministic embedding. We then combine this analysis with lower bounds on the offline optimum for Euclidean metrics, derived via majorization arguments, to obtain our guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20288v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yingxi Li, Ellen Vitercik, Mingwei Yang</dc:creator>
    </item>
    <item>
      <title>Separations between Oblivious and Adaptive Adversaries for Natural Dynamic Graph Problems</title>
      <link>https://arxiv.org/abs/2510.20341</link>
      <description>arXiv:2510.20341v1 Announce Type: new 
Abstract: We establish the first update-time separation between dynamic algorithms against oblivious adversaries and those against adaptive adversaries in natural dynamic graph problems, based on popular fine-grained complexity hypotheses. Specifically, under the combinatorial BMM hypothesis, we show that every combinatorial algorithm against an adaptive adversary for the incremental maximal independent set problem requires $n^{1-o(1)}$ amortized update time. Furthermore, assuming either the 3SUM or APSP hypotheses, every algorithm for the decremental maximal clique problem needs $\Delta/n^{o(1)}$ amortized update time when the initial maximum degree is $\Delta \le \sqrt{n}$. These lower bounds are matched by existing algorithms against adaptive adversaries. In contrast, both problems admit algorithms against oblivious adversaries that achieve $\operatorname{polylog}(n)$ amortized update time [Behnezhad, Derakhshan, Hajiaghayi, Stein, Sudan; FOCS '19] [Chechik, Zhang; FOCS '19]. Therefore, our separations are exponential. Previously known separations for dynamic algorithms were either engineered for contrived problems and relied on strong cryptographic assumptions [Beimel, Kaplan, Mansour, Nissim, Saranurak, Stemmer; STOC '22], or worked for problems whose inputs are not explicitly given but are accessed through oracle calls [Bateni, Esfandiari, Fichtenberger, Henzinger, Jayaram, Mirrokni, Wiese; SODA '23].
  As a byproduct, we also provide a separation between incremental and decremental algorithms for the triangle detection problem: we show a decremental algorithm with $\tilde{O}(n^{\omega})$ total update time, while every incremental algorithm requires $n^{3-o(1)}$ total update time, assuming the OMv hypothesis. To our knowledge this is the first separation of this kind.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20341v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aaron Bernstein, Sayan Bhattacharya, Nick Fischer, Peter Kiss, Thatchaphol Saranurak</dc:creator>
    </item>
    <item>
      <title>$\ell_2/\ell_2$ Sparse Recovery via Weighted Hypergraph Peeling</title>
      <link>https://arxiv.org/abs/2510.20361</link>
      <description>arXiv:2510.20361v1 Announce Type: new 
Abstract: We demonstrate that the best $k$-sparse approximation of a length-$n$ vector can be recovered within a $(1+\epsilon)$-factor approximation in $O((k/\epsilon) \log n)$ time using a non-adaptive linear sketch with $O((k/\epsilon) \log n)$ rows and $O(\log n)$ column sparsity. This improves the running time of the fastest-known sketch [Nakos, Song; STOC '19] by a factor of $\log n$, and is optimal for a wide range of parameters.
  Our algorithm is simple and likely to be practical, with the analysis built on a new technique we call weighted hypergraph peeling. Our method naturally extends known hypergraph peeling processes (as in the analysis of Invertible Bloom Filters) to a setting where edges and nodes have (possibly correlated) weights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20361v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nick Fischer, Vasileios Nakos</dc:creator>
    </item>
    <item>
      <title>From Incremental Transitive Cover to Strongly Polynomial Maximum Flow</title>
      <link>https://arxiv.org/abs/2510.20368</link>
      <description>arXiv:2510.20368v1 Announce Type: new 
Abstract: We provide faster strongly polynomial time algorithms solving maximum flow in structured $n$-node $m$-arc networks. Our results imply an $n^{\omega + o(1)}$-time strongly polynomial time algorithms for computing a maximum bipartite $b$-matching where $\omega$ is the matrix multiplication constant. Additionally, they imply an $m^{1 + o(1)} W$-time algorithm for solving the problem on graphs with a given tree decomposition of width $W$.
  We obtain these results by strengthening and efficiently implementing an approach in Orlin's (STOC 2013) state-of-the-art $O(mn)$ time maximum flow algorithm. We develop a general framework that reduces solving maximum flow with arbitrary capacities to (1) solving a sequence of maximum flow problems with polynomial bounded capacities and (2) dynamically maintaining a size-bounded supersets of the transitive closure under arc additions; we call this problem \emph{incremental transitive cover}. Our applications follow by leveraging recent weakly polynomial, almost linear time algorithms for maximum flow due to Chen, Kyng, Liu, Peng, Gutenberg, Sachdeva (FOCS 2022) and Brand, Chen, Kyng, Liu, Peng, Gutenberg, Sachdeva, Sidford (FOCS 2023), and by developing incremental transitive cover data structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20368v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Dadush, James B. Orlin, Aaron Sidford, L\'aszl\'o A. V\'egh</dc:creator>
    </item>
    <item>
      <title>Compact representations of pattern-avoiding permutations</title>
      <link>https://arxiv.org/abs/2510.20382</link>
      <description>arXiv:2510.20382v1 Announce Type: new 
Abstract: Pattern-avoiding permutations are a central object of study in both combinatorics and theoretical computer science. In this paper we design a data structure that can store any size-$n$ permutation $\tau$ that avoids an arbitrary (and unknown) fixed pattern $\pi$ in the asymptotically optimal $O(n \lg{s_\pi})$ bits, where $s_\pi$ is the Stanley-Wilf limit of $\pi$. Our data structure supports $\tau(i)$ and $\tau^{-1}(i)$ queries in $O(1)$ time, sidestepping the lower bound of Golynski (SODA 2009) that holds for general permutations. Comparable results were previously known only in more restricted cases, e.g., when $\tau$ is separable, which means avoiding the patterns 2413 and 3142.
  We also extend our data structure to support more complex geometric queries on pattern-avoiding permutations (or planar point sets) such as rectangle range counting in $O(\lg\lg{n})$ time. This result circumvents the lower bound of $\Omega{(\lg{n}/\lg\lg{n})}$ by P\u{a}tra\c{s}cu (STOC 2007) that holds in the general case. For bounded treewidth permutation classes (which include the above-mentioned separable class), we further reduce the space overhead to a lower order additive term, making our data structure succinct. This extends and improves results of Chakraborty et al. (ISAAC 2024) that were obtained for separable permutations via different techniques. All our data structures can be constructed in linear time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20382v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>L\'aszl\'o Kozma, Michal Opler</dc:creator>
    </item>
    <item>
      <title>Parallel $(1+\epsilon)$-Approximate Multi-Commodity Mincost Flow in Almost Optimal Depth and Work</title>
      <link>https://arxiv.org/abs/2510.20456</link>
      <description>arXiv:2510.20456v1 Announce Type: new 
Abstract: We present a parallel algorithm for computing $(1+\epsilon)$-approximate mincost flow on an undirected graph with $m$ edges, where capacities and costs are assigned to both edges and vertices. Our algorithm achieves $\hat{O}(m)$ work and $\hat{O}(1)$ depth when $\epsilon &gt; 1/\mathrm{polylog}(m)$, making both the work and depth almost optimal, up to a subpolynomial factor.
  Previous algorithms with $\hat{O}(m)$ work required $\Omega(m)$ depth, even for special cases of mincost flow with only edge capacities or max flow with vertex capacities. Our result generalizes prior almost-optimal parallel $(1+\epsilon)$-approximation algorithms for these special cases, including shortest paths [Li, STOC'20] [Andoni, Stein, Zhong, STOC'20] [Rozhen, Haeupler, Marinsson, Grunau, Zuzic, STOC'23] and max flow with only edge capacities [Agarwal, Khanna, Li, Patil, Wang, White, Zhong, SODA'24].
  Our key technical contribution is the first construction of length-constrained flow shortcuts with $(1+\epsilon)$ length slack, $\hat{O}(1)$ congestion slack, and $\hat{O}(1)$ step bound. This provides a strict generalization of the influential concept of $(\hat{O}(1),\epsilon)$-hopsets [Cohen, JACM'00], allowing for additional control over congestion. Previous length-constrained flow shortcuts [Haeupler, Hershkowitz, Li, Roeyskoe, Saranurak, STOC'24] incur a large constant in the length slack, which would lead to a large approximation factor. To enable our flow algorithms to work under vertex capacities, we also develop a close-to-linear time algorithm for computing length-constrained vertex expander decomposition.
  Building on Cohen's idea of path-count flows [Cohen, SICOMP'95], we further extend our algorithm to solve $(1+\epsilon)$-approximate $k$-commodity mincost flow problems with almost-optimal $\hat{O}(mk)$ work and $\hat{O}(1)$ depth, independent of the number of commodities $k$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20456v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bernhard Haeupler, Yonggang Jiang, Yaowei Long, Thatchaphol Saranurak, Shengzhe Wang</dc:creator>
    </item>
    <item>
      <title>Provably Small Portfolios for Multiobjective Optimization with Application to Subsidized Facility Location</title>
      <link>https://arxiv.org/abs/2510.20555</link>
      <description>arXiv:2510.20555v1 Announce Type: new 
Abstract: Many multiobjective real-world problems, such as facility location and bus routing, become more complex when optimizing the priorities of multiple stakeholders. These are often modeled using infinite classes of objectives, e.g., $L_p$ norms over group distances induced by feasible solutions in a fixed domain. Traditionally, the literature has considered explicitly balancing `equity' (or min-max) and `efficiency' (or min-sum) objectives to capture this trade-off. However, the structure of solutions obtained by such modeling choices can be very different. Taking a solution-centric approach, we introduce the concept of provably small set of solutions $P$, called a {\it portfolio}, such that for every objective function $h(\cdot)$ in the given class $\mathbf{C}$, there exists some solution in $P$ which is an $\alpha$-approximation for $h(\cdot)$. Constructing such portfolios can help decision-makers understand the impact of balancing across multiple objectives.
  Given a finite set of base objectives $h_1, \ldots, h_N$, we give provable algorithms for constructing portfolios for (1) the class of conic combinations $\mathbf{C} = \{\sum_{j \in [N]}\lambda_j h_j: \lambda \ge 0\}$ and for (2) any class $\mathbf{C}$ of functions that interpolates monotonically between the min-sum efficiency objective (i.e., $h_1 + \ldots + h_N$) and the min-max equity objective (i.e., $\max_{j \in [N]} h_j$). Examples of the latter are $L_p$ norms and top-$\ell$ norms. As an application, we study the Fair Subsidized Facility Location (FSFL) problem, motivated by the crisis of medical deserts caused due to pharmacy closures. FSFL allows subsidizing facilities in underserved areas using revenue from profitable locations. We develop a novel bicriteria approximation algorithm and show a significant reduction of medical deserts across states in the U.S.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20555v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Swati Gupta, Jai Moondra, Mohit Singh</dc:creator>
    </item>
    <item>
      <title>A Deterministic Polylogarithmic Competitive Algorithm for Matching with Delays</title>
      <link>https://arxiv.org/abs/2510.20588</link>
      <description>arXiv:2510.20588v1 Announce Type: new 
Abstract: In the online Min-cost Perfect Matching with Delays (MPMD) problem, $m$ requests in a metric space are submitted at different times by an adversary. The goal is to match all requests while (i) minimizing the sum of the distances between matched pairs as well as (ii) how long each request remained unmatched after it appeared.
  While there exist almost optimal algorithms when the metric space is finite and known a priori, this is not the case when the metric space is infinite or unknown. In this latter case, the best known algorithm, due to Azar and Jacob-Fanani, has competitiveness $\mathcal{O}(m^{0.59})$ which is exponentially worse than the best known lower bound of $\Omega(\log m / \log \log m)$ by Ashlagi et al.
  We present a $\mathcal{O}(\log^5 m)$-competitive algorithm for the MPMD problem. This algorithm is deterministic and does not need to know the metric space or $m$ in advance. This is an exponential improvement over previous results and only a polylogarithmic factor away from the lower bound.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20588v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marc Dufay, Roger Wattenhofer</dc:creator>
    </item>
    <item>
      <title>On Geometric Bipartite Graphs with Asymptotically Smallest Zarankiewicz Numbers</title>
      <link>https://arxiv.org/abs/2510.20737</link>
      <description>arXiv:2510.20737v1 Announce Type: cross 
Abstract: This paper considers the \textit{Zarankiewicz problem} in graphs with low-dimensional geometric representation (i.e., low Ferrers dimension). Our first result reveals a separation between bipartite graphs of Ferrers dimension three and four: while $Z(n;k) \leq 9n(k-1)$ for graphs of Ferrers dimension three, $Z(n;k) \in \Omega\left(n k \cdot \frac{\log n}{\log \log n}\right)$ for Ferrers dimension four graphs (Chan &amp; Har-Peled, 2023) (Chazelle, 1990). To complement this, we derive a tight upper bound of $2n(k-1)$ for chordal bigraphs and $54n(k-1)$ for grid intersection graphs (GIG), a prominent graph class residing in four Ferrers dimensions and capturing planar bipartite graphs as well as bipartite intersection graphs of rectangles. Previously, the best-known bound for GIG was $Z(n;k) \in O(2^{O(k)} n)$, implied by the results of Fox &amp; Pach (2006) and Mustafa &amp; Pach (2016). Our results advance and offer new insights into the interplay between Ferrers dimensions and extremal combinatorics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20737v1</guid>
      <category>math.CO</category>
      <category>cs.DS</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Parinya Chalermsook, Ly Orgo, Minoo Zarsav</dc:creator>
    </item>
    <item>
      <title>Optimizing Feature Ordering in Radar Charts for Multi-Profile Comparison</title>
      <link>https://arxiv.org/abs/2510.20738</link>
      <description>arXiv:2510.20738v1 Announce Type: cross 
Abstract: Radar charts are widely used to visualize multivariate data and compare multiple profiles across features. However, the visual clarity of radar charts can be severely compromised when feature values alternate drastically in magnitude around the circle, causing areas to collapse, which misrepresents relative differences. In the present work we introduce a permutation optimization strategy that reorders features to minimize polygon ``spikiness'' across multiple profiles simultaneously. The method is combinatorial (exhaustive search) for moderate numbers of features and uses a lexicographic minimax criterion that first considers overall smoothness (mean jump) and then the largest single jump as a tie-breaker. This preserves more global information and produces visually balanced arrangements. We discuss complexity, practical bounds, and relations to existing approaches that either change the visualization (e.g., OrigamiPlot) or learn orderings (e.g., Versatile Ordering Network). An example with two profiles and $p=6$ features (before/after ordering) illustrates the qualitative improvement.
  Keywords: data visualization, radar charts, combinatorial optimization, minimax optimization, feature ordering</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20738v1</guid>
      <category>cs.HC</category>
      <category>cs.DS</category>
      <category>cs.GR</category>
      <category>math.OC</category>
      <category>stat.OT</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Albert Dorador</dc:creator>
    </item>
    <item>
      <title>A Freeable Matrix Characterization of Bipartite Graphs of Ferrers Dimension Three</title>
      <link>https://arxiv.org/abs/2510.20744</link>
      <description>arXiv:2510.20744v1 Announce Type: cross 
Abstract: Ferrer dimension, along with the order dimension, is a standard dimensional concept for bipartite graphs. In this paper, we prove that a graph is of Ferrer dimension three (equivalent to the intersection bigraph of orthants and points in ${\mathbb R}^3$) if and only if it admits a biadjacency matrix representation that does not contain $\Gamma= \begin{bmatrix} * &amp; 1 &amp; * \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; * \end{bmatrix}$ and $\Delta = \begin{bmatrix} 1 &amp; * &amp; * \\ 0 &amp; 1 &amp; * \\ 1 &amp; 0 &amp; 1 \end{bmatrix}$, where $*$ denotes zero or one entry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20744v1</guid>
      <category>math.CO</category>
      <category>cs.DS</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Parinya Chalermsook, Ly Orgo, Minoo Zarsav</dc:creator>
    </item>
    <item>
      <title>Isotropic Noise in Stochastic and Quantum Convex Optimization</title>
      <link>https://arxiv.org/abs/2510.20745</link>
      <description>arXiv:2510.20745v1 Announce Type: cross 
Abstract: We consider the problem of minimizing a $d$-dimensional Lipschitz convex function using a stochastic gradient oracle. We introduce and motivate a setting where the noise of the stochastic gradient is isotropic in that it is bounded in every direction with high probability. We then develop an algorithm for this setting which improves upon prior results by a factor of $d$ in certain regimes, and as a corollary, achieves a new state-of-the-art complexity for sub-exponential noise. We give matching lower bounds (up to polylogarithmic factors) for both results. Additionally, we develop an efficient quantum isotropifier, a quantum algorithm which converts a variance-bounded quantum sampling oracle into one that outputs an unbiased estimate with isotropic error. Combining our results, we obtain improved dimension-dependent rates for quantum stochastic convex optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20745v1</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Annie Marsden, Liam O'Carroll, Aaron Sidford, Chenyi Zhang</dc:creator>
    </item>
    <item>
      <title>Balancing Gradient and Hessian Queries in Non-Convex Optimization</title>
      <link>https://arxiv.org/abs/2510.20786</link>
      <description>arXiv:2510.20786v1 Announce Type: cross 
Abstract: We develop optimization methods which offer new trade-offs between the number of gradient and Hessian computations needed to compute the critical point of a non-convex function. We provide a method that for any twice-differentiable $f\colon \mathbb R^d \rightarrow \mathbb R$ with $L_2$-Lipschitz Hessian, input initial point with $\Delta$-bounded sub-optimality, and sufficiently small $\epsilon &gt; 0$, outputs an $\epsilon$-critical point, i.e., a point $x$ such that $\|\nabla f(x)\| \leq \epsilon$, using $\tilde{O}(L_2^{1/4} n_H^{-1/2}\Delta\epsilon^{-9/4})$ queries to a gradient oracle and $n_H$ queries to a Hessian oracle for any positive integer $n_H$. As a consequence, we obtain an improved gradient query complexity of $\tilde{O}(d^{1/3}L_2^{1/2}\Delta\epsilon^{-3/2})$ in the case of bounded dimension and of $\tilde{O}(L_2^{3/4}\Delta^{3/2}\epsilon^{-9/4})$ in the case where we are allowed only a \emph{single} Hessian query. We obtain these results through a more general algorithm which can handle approximate Hessian computations and recovers the state-of-the-art bound of computing an $\epsilon$-critical point with $O(L_1^{1/2}L_2^{1/4}\Delta\epsilon^{-7/4})$
  gradient queries provided that $f$ also has an $L_1$-Lipschitz gradient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20786v1</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Deeksha Adil, Brian Bullins, Aaron Sidford, Chenyi Zhang</dc:creator>
    </item>
    <item>
      <title>Narrowing the LOCAL$\unicode{x2013}$CONGEST Gaps in Sparse Networks via Expander Decompositions</title>
      <link>https://arxiv.org/abs/2205.08093</link>
      <description>arXiv:2205.08093v2 Announce Type: replace 
Abstract: Many combinatorial optimization problems can be approximated within $(1 \pm \epsilon)$ factors in $\text{poly}(\log n, 1/\epsilon)$ rounds in the LOCAL model via network decompositions [Ghaffari, Kuhn, and Maus, STOC 2018]. These approaches require sending messages of unlimited size, so they do not extend to the CONGEST model, which restricts the message size to be $O(\log n)$ bits.
  In this paper, we develop a generic framework for obtaining $\text{poly}(\log n, 1/\epsilon)$-round $(1\pm \epsilon)$-approximation algorithms for many combinatorial optimization problems, including maximum weighted matching, maximum independent set, and correlation clustering, in graphs excluding a fixed minor in the CONGEST model. This class of graphs covers many sparse network classes that have been studied in the literature, including planar graphs, bounded-genus graphs, and bounded-treewidth graphs.
  Furthermore, we show that our framework can be applied to give an efficient distributed property testing algorithm for an arbitrary minor-closed graph property that is closed under taking disjoint union, significantly generalizing the previous distributed property testing algorithm for planarity in [Levi, Medina, and Ron, PODC 2018 &amp; Distributed Computing 2021].
  Our framework uses distributed expander decomposition algorithms [Chang and Saranurak, FOCS 2020] to decompose the graph into clusters of high conductance. We show that any graph excluding a fixed minor admits small edge separators. Using this result, we show the existence of a high-degree vertex in each cluster in an expander decomposition, which allows the entire graph topology of the cluster to be routed to a vertex. Similar to the use of network decompositions in the LOCAL model, the vertex will be able to perform any local computation on the subgraph induced by the cluster and broadcast the result over the cluster.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.08093v2</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yi-Jun Chang, Hsin-Hao Su</dc:creator>
    </item>
    <item>
      <title>Parallel Batch-Dynamic Maximal Matching with Constant Work per Update</title>
      <link>https://arxiv.org/abs/2503.09908</link>
      <description>arXiv:2503.09908v2 Announce Type: replace 
Abstract: We present a work optimal algorithm for parallel fully batch-dynamic maximal matching against an oblivious adversary. It processes batches of updates (either insertions or deletions of edges) in constant expected amortized work per edge update, and in $O(\log^3 m)$ depth per batch whp, where $m$ is the maximum number of edges in the graph over time. This greatly improves on the recent result by Ghaffari and Trygub (2024) that requires $O(\log^9 m)$ amortized work per update and $O(\log^4 m )$ depth per batch, both whp.
  The algorithm can also be used for parallel batch-dynamic hyperedge maximal matching. For hypergraphs with rank $r$ (maximum cardinality of any edge) the algorithm supports batches of updates with $O(r^3)$ expected amortized work per edge update, and $O(\log^3 m)$ depth per batch whp. Ghaffari and Trygub's parallel batch-dynamic algorithm on hypergraphs requires $O(r^8 \log^9 m)$ amortized work per edge update whp. We leverage ideas from the prior algorithms but introduce substantial new ideas. Furthermore, our algorithm is relatively simple, perhaps even simpler than Assadi and Solomon's (2021) sequential dynamic hyperedge algorithm.
  We also present the first work-efficient algorithm for parallel static maximal matching on hypergraphs. For a hypergraph with total cardinality $m'$ (i.e., sum over the cardinality of each edge), the algorithm runs in $O(m')$ work in expectation and $O(\log^2 m)$ depth whp. The algorithm also has some properties that allow us to use it as a subroutine in the dynamic algorithm to select random edges in the graph to add to the matching.
  With a standard reduction from set cover to hyperedge maximal matching, we give state of the art $r$-approximate static and batch-dynamic parallel set cover algorithms, where $r$ is the maximum frequency of any element, and batch-dynamic updates consist of adding or removing batches of elements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09908v2</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guy E. Blelloch, Andrew C. Brady</dc:creator>
    </item>
    <item>
      <title>Engineering Fast and Space-Efficient Recompression from SLP-Compressed Text</title>
      <link>https://arxiv.org/abs/2506.12011</link>
      <description>arXiv:2506.12011v2 Announce Type: replace 
Abstract: Compressed indexing enables powerful queries over massive and repetitive textual datasets using space proportional to the compressed input. While theoretical advances have led to highly efficient index structures, their practical construction remains a bottleneck, especially for complex components like recompression RLSLP, a grammar-based representation crucial for building powerful text indexes that support widely used suffix and LCP array queries.
  In this work, we present the first implementation of recompression RLSLP construction that runs in compressed time, operating on an LZ77-like approximation of the input. Compared to state-of-the-art uncompressed-time methods, our approach achieves up to $46\times$ speedup and $17\times$ lower RAM usage on large, repetitive inputs. These gains unlock scalability to larger datasets and affirm compressed computation as a practical path forward for fast index construction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12011v2</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ankith Reddy Adudodla, Dominik Kempa</dc:creator>
    </item>
    <item>
      <title>Faster Estimation of the Average Degree of a Graph Using Random Edges and Structural Queries</title>
      <link>https://arxiv.org/abs/2507.06925</link>
      <description>arXiv:2507.06925v2 Announce Type: replace 
Abstract: We revisit the problem of designing sublinear algorithms for estimating the average degree of an $n$-vertex graph. The standard access model for graphs allows for the following queries: sampling a uniform random vertex, the degree of a vertex, sampling a uniform random neighbor of a vertex, and ``pair queries'' which determine if a pair of vertices form an edge. In this model, original results [Goldreich-Ron, RSA 2008; Eden-Ron-Seshadhri, SIDMA 2019] on this problem prove that the complexity of getting $(1+\varepsilon)$-multiplicative approximations to the average degree, ignoring $\varepsilon$-dependencies, is $\Theta(\sqrt{n})$. When random edges can be sampled, it is known that the average degree can estimated in $\widetilde{O}(n^{1/3})$ queries, even without pair queries [Motwani-Panigrahy-Xu, ICALP 2007; Beretta-Tetek, TALG 2024].
  We give a nearly optimal algorithm in the standard access model with random edge samples. Our algorithm makes $\widetilde{O}(n^{1/4})$ queries exploiting the power of pair queries. We also analyze the ``full neighborhood access" model wherein the entire adjacency list of a vertex can be obtained with a single query; this model is relevant in many practical applications. In a weaker version of this model, we give an algorithm that makes $\widetilde{O}(n^{1/5})$ queries. Both these results underscore the power of {\em structural queries}, such as pair queries and full neighborhood access queries, for estimating the average degree. We give nearly matching lower bounds, ignoring $\varepsilon$-dependencies, for all our results.
  So far, almost all algorithms for estimating average degree assume that the number of vertices, $n$, is known. Inspired by [Beretta-Tetek, TALG 2024], we study this problem when $n$ is unknown and show that structural queries do not help in estimating average degree in this setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06925v2</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Beretta, Deeparnab Chakrabarty, C. Seshadhri</dc:creator>
    </item>
    <item>
      <title>Efficiently Constructing Sparse Navigable Graphs</title>
      <link>https://arxiv.org/abs/2507.13296</link>
      <description>arXiv:2507.13296v2 Announce Type: replace 
Abstract: Graph-based nearest neighbor search methods have seen a surge of popularity in recent years, offering state-of-the-art performance across a wide variety of applications. Central to these methods is the task of constructing a sparse navigable search graph for a given dataset endowed with a distance function. Unfortunately, doing so is computationally expensive, so heuristics are universally used in practice.
  In this work, we initiate the study of fast algorithms with provable guarantees for search graph construction. For a dataset with $n$ data points, the problem of constructing an optimally sparse navigable graph can be framed as $n$ separate but highly correlated minimum set cover instances. This yields a naive $O(n^3)$ time greedy algorithm that returns a navigable graph whose sparsity is at most $O(\log n)$ higher than optimal. We improve significantly on this baseline, taking advantage of correlation between the set cover instances to leverage techniques from streaming and sublinear-time set cover algorithms. By also introducing problem-specific pre-processing techniques, we obtain an $\tilde{O}(n^2)$ time algorithm for constructing an $O(\log n)$-approximate sparsest navigable graph under any distance function.
  The runtime of our method is optimal up to logarithmic factors under the Strong Exponential Time Hypothesis via a reduction from Monochromatic Closest Pair. Moreover, we prove that, as with general set cover, obtaining better than an $O(\log n)$-approximation is NP-hard, despite the significant additional structure present in the navigable graph problem. Finally, we show that our approach can also beat cubic time for the closely related and practically important problems of constructing $\alpha$-shortcut reachable and $\tau$-monotonic graphs, which are also used for nearest neighbor search. For such graphs, we obtain $\tilde{O}(n^{2.5})$ time or better algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13296v2</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <category>cs.IR</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alex Conway, Laxman Dhulipala, Martin Farach-Colton, Rob Johnson, Ben Landrum, Christopher Musco, Yarin Shechter, Torsten Suel, Richard Wen</dc:creator>
    </item>
    <item>
      <title>Directed and Undirected Vertex Connectivity Problems are Equivalent for Dense Graphs</title>
      <link>https://arxiv.org/abs/2508.20305</link>
      <description>arXiv:2508.20305v2 Announce Type: replace 
Abstract: Vertex connectivity and its variants are among the most fundamental problems in graph theory, with decades of extensive study and numerous algorithmic advances. The directed variants of vertex connectivity are usually solved by manually extending fast algorithms for undirected graphs, which has required considerable effort. In this paper, we present an extremely simple reduction from directed to undirected vertex connectivity for dense graphs. As immediate corollaries, we vastly simplify the proof for directed vertex connectivity in $n^{2+o(1)}$ time [LNPSY25], and obtain a parallel vertex connectivity algorithm for directed graphs with $n^{\omega+o(1)}$ work and $n^{o(1)}$ depth, via the undirected vertex connectivity algorithm of [BJMY25].
  Our reduction further extends to the weighted, all-pairs and Steiner versions of the problem. By combining our reduction with the recent subcubic-time algorithm for undirected weighted vertex cuts [CT25], we obtain a subcubic-time algorithm for weighted directed vertex connectivity, improving upon a three-decade-old bound [HRG00] for dense graphs. For the all-pairs version, by combining the conditional lower bounds on the all-pairs vertex connectivity problem for directed graphs [AGIKPTUW19], we obtain an alternate proof of the conditional lower bound for the all-pairs vertex connectivity problem on undirected graphs, vastly simplifying the proof by [HLSW23].</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20305v2</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Olivier Fischer, Yonggang Jiang, Sagnik Mukhopadhyay, Sorrachai Yingchareonthawornchai</dc:creator>
    </item>
    <item>
      <title>Proper decision trees: An axiomatic framework for solving optimal decision tree problems with arbitrary splitting rules</title>
      <link>https://arxiv.org/abs/2503.01455</link>
      <description>arXiv:2503.01455v2 Announce Type: replace-cross 
Abstract: We present an axiomatic framework for analyzing the algorithmic properties of decision trees. This framework supports the classification of decision tree problems through structural and ancestral constraints within a rigorous mathematical foundation. The central focus of this paper is a special class of decision tree problems-which we term proper decision trees-due to their versatility and effectiveness. In terms of versatility, this class subsumes several well-known data structures, including binary space partitioning trees, K-D trees, and machine learning decision tree models. Regarding effectiveness, we prove that only proper decision trees can be uniquely characterized as K-permutations, whereas typical non-proper decision trees correspond to binary-labeled decision trees with substantially greater complexity. Using this formal characterization, we develop a generic algorithmic approach for solving optimal decision tree problems over arbitrary splitting rules and objective functions for proper decision trees. We constructively derive a generic dynamic programming recursion for solving these problems exactly. However, we show that memoization is generally impractical in terms of space complexity, as both datasets and subtrees must be stored. This result contradicts claims in the literature that suggest a trade-off between memoizing datasets and subtrees. Our framework further accommodates constraints such as tree depth and leaf size, and can be accelerated using techniques such as thinning. Finally, we extend our analysis to several non-proper decision trees, including the commonly studied decision tree over binary feature data, the binary search tree, and the tree structure arising in the matrix chain multiplication problem. We demonstrate how these problems can be solved by appropriately modifying or discarding certain axioms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01455v2</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Xi He, Max A. Little</dc:creator>
    </item>
    <item>
      <title>Selfish, Local and Online Scheduling via Vector Fitting</title>
      <link>https://arxiv.org/abs/2505.10082</link>
      <description>arXiv:2505.10082v2 Announce Type: replace-cross 
Abstract: We provide a dual fitting technique on a semidefinite program yielding simple proofs of tight bounds for the robust price of anarchy of several congestion and scheduling games under the sum of weighted completion times objective. The same approach also allows to bound the approximation ratio of local search algorithms and the competitive ratio of online algorithms for the scheduling problem $R || \sum w_j C_j$. All of our results are obtained through a simple unified dual fitting argument on the same semidefinite programming relaxation, which can essentially be obtained through the first round of the Lasserre/Sum of Squares hierarchy.
  As our main application, we show that the known coordination ratio bounds of respectively $4, (3 + \sqrt{5})/2 \approx 2.618,$ and $32/15 \approx 2.133$ for the scheduling game $R || \sum w_j C_j$ under the coordination mechanisms Smith's Rule, Proportional Sharing and Rand (STOC 2011) can be extended to congestion games and obtained through this approach. For the natural restriction where the weight of each player is proportional to its processing time on every resource, we show that the last bound can be improved from 2.133 to 2. This improvement can also be made for general instances when considering the price of anarchy of the game, rather than the coordination ratio. As a further application of this technique, we show that it recovers the tight bound of $(3 + \sqrt{5})/2$ for the price of anarchy of weighted affine congestion games and the Kawaguchi-Kyan bound of $(1+ \sqrt{2})/2$ for the pure price of anarchy of $P || \sum w_j C_j$. Moreover, this approach can analyze a simple local search algorithm for $R || \sum w_j C_j$, the best currently known combinatorial approximation algorithm for this problem achieving an approximation ratio of $(5 + \sqrt{5})/4 + \varepsilon$ and an online greedy algorithm which is $4$-competitive.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10082v2</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Danish Kashaev</dc:creator>
    </item>
    <item>
      <title>WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables</title>
      <link>https://arxiv.org/abs/2509.16407</link>
      <description>arXiv:2509.16407v2 Announce Type: replace-cross 
Abstract: GPU hash tables are increasingly used to accelerate data processing, but their limited functionality restricts adoption in large-scale data processing applications. Current limitations include incomplete concurrency support and missing compound operations such as upserts.
  This paper presents WarpSpeed, a library of high-performance concurrent GPU hash tables with a unified benchmarking framework for performance analysis. WarpSpeed implements eight state-of-the-art Nvidia GPU hash table designs and provides a rich API designed for modern GPU applications. Our evaluation uses diverse benchmarks to assess both correctness and scalability, and we demonstrate real-world impact by integrating these hash tables into three downstream applications.
  We propose several optimization techniques to reduce concurrency overhead, including fingerprint-based metadata to minimize cache line probes and specialized Nvidia GPU instructions for lock-free queries. Our findings provide new insights into concurrent GPU hash table design and offer practical guidance for developing efficient, scalable data structures on modern GPUs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16407v2</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Hunter McCoy, Prashant Pandey</dc:creator>
    </item>
  </channel>
</rss>
