<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 09 Apr 2024 04:00:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 09 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Algorithmic Misjudgement in Google Search Results: Evidence from Auditing the US Online Electoral Information Environment</title>
      <link>https://arxiv.org/abs/2404.04684</link>
      <description>arXiv:2404.04684v1 Announce Type: new 
Abstract: Google Search is an important way that people seek information about politics, and Google states that it is ``committed to providing timely and authoritative information on Google Search to help voters understand, navigate, and participate in democratic processes''. In this paper, we interrogate the extent to which government-maintained web domains are represented in the online environment of electoral information of the 2022 US midterm elections, as captured through Google Search results in 3.45 million SERPs for 786 locations across the United States between October and November 2022. Although we find that almost 40% of organic results are contributed by the 40% of government domains, this proportional equilibrium hides the fact that most results either belong to a small number of popular domains or are mistargeted (at a rate of 71.18%) with respect to the location of the search. We consider the frequent omission and mistargeting of non-federal websites a form of algorithmic misjudgement that contributes to civic harm, by obscuring the important role that these institutions play in the election information environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04684v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brooke Perreault, Johanna Lee, Ropafadzo Shava, Eni Mustafaraj</dc:creator>
    </item>
    <item>
      <title>Fifth Generation IMC: Expanding the scope to Profit, People, and the Planet</title>
      <link>https://arxiv.org/abs/2404.04740</link>
      <description>arXiv:2404.04740v1 Announce Type: new 
Abstract: This editorial outlines an expanded scope for the next (fifth) generation of integrated marketing communication. It identifies key market forces that gave rise to this evolution and describes a trajectory of where Integrated Marketing Communication (IMC) has been and where it is going. The central shift is moving from primarily focusing on one stakeholder to multiple ones, including people (employees and society), the planet (environment), and profits. It identifies examples from industry that exemplify multi-stakeholder decision-making and uses the examples to suggest research questions that academics and practitioners should address. Examples and research directions are organized around marketing strategy, communication media and messages, and measurement systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04740v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stewart Pearson, Edward Malthouse</dc:creator>
    </item>
    <item>
      <title>Now, Later, and Lasting: Ten Priorities for AI Research, Policy, and Practice</title>
      <link>https://arxiv.org/abs/2404.04750</link>
      <description>arXiv:2404.04750v1 Announce Type: new 
Abstract: Advances in artificial intelligence (AI) will transform many aspects of our lives and society, bringing immense opportunities but also posing significant risks and challenges. The next several decades may well be a turning point for humanity, comparable to the industrial revolution. We write to share a set of recommendations for moving forward from the perspective of the founder and leaders of the One Hundred Year Study on AI. Launched a decade ago, the project is committed to a perpetual series of studies by multidisciplinary experts to evaluate the immediate, longer-term, and far-reaching effects of AI on people and society, and to make recommendations about AI research, policy, and practice. As we witness new capabilities emerging from neural models, it is crucial that we engage in efforts to advance our scientific understanding of these models and their behaviors. We must address the impact of AI on people and society through technical, social, and sociotechnical lenses, incorporating insights from a diverse range of experts including voices from engineering, social, behavioral, and economic disciplines. By fostering dialogue, collaboration, and action among various stakeholders, we can strategically guide the development and deployment of AI in ways that maximize its potential for contributing to human flourishing. Despite the growing divide in the field between focusing on short-term versus long-term implications, we think both are of critical importance. As Alan Turing, one of the pioneers of AI, wrote in 1950, "We can only see a short distance ahead, but we can see plenty there that needs to be done." We offer ten recommendations for action that collectively address both the short- and long-term potential impacts of AI technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04750v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eric Horvitz, Vincent Conitzer, Sheila McIlraith, Peter Stone</dc:creator>
    </item>
    <item>
      <title>The Impact of Virtual Laboratories on Active Learning and Engagement in Cybersecurity Distance Education</title>
      <link>https://arxiv.org/abs/2404.04952</link>
      <description>arXiv:2404.04952v1 Announce Type: new 
Abstract: Virtual Laboratories (V Labs) have in the recent past become part and parcel of remote teaching in practical hands-on approaches, particularly in Cybersecurity distance courses. Their potential is meant to assist learners with hands-on practical laboratory exercises irrespective of geographical location. Nevertheless, adopting V Labs in didactic approaches in higher education has seen both merits and demerits. Based on this premise, this study investigates the impact of V Labs on Active Learning (AL) and engagement in cybersecurity distance education. A survey with a limited number of learners and educators who have had an experience with cybersecurity distance courses that leveraged V Labs in their practical Lab assignment, was conducted at Blekinge Tekniska H\"ogskola, Sweden, to assess the impact of V Labs on AL and engagement in Cybersecurity Distance Education. 29% and 73% of the learners and educators, respectively responded to the survey administered remotely and with good internal consistency of questionnaires based on the Cronbalch Alpha; the results showed that learners and educators had a positive perception of using V Labs to enhance AL in cybersecurity distance education. The key concentration of the study was on AL and engagement and problem-solving abilities when V Labs are used. Both the learners and educators found the V Labs to be engaging, interactive, and effective in improving their understanding of cybersecurity concepts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04952v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Victor R. Kebande</dc:creator>
    </item>
    <item>
      <title>Decisioning Workshop 2023</title>
      <link>https://arxiv.org/abs/2404.05495</link>
      <description>arXiv:2404.05495v1 Announce Type: new 
Abstract: In a knowledge society, the term knowledge must be considered a core resource for organizations. So, beyond being a medium to progress and to innovate, knowledge is one of our most important resources: something necessary to decide.Organizations that are embracing knowledge retention activities are gaining a competitive advantage. Organizational rearrangements from companies, notably outsourcing, increase a possible loss of knowledge, making knowledge retention an essential need for them. When Knowledge is less shared, collaborative decision-making seems harder to obtain insofar as a ``communication breakdown'' characterizes participants' discourse. At best, stakeholders have to finda consensus according to their knowledge. Sharing knowledge ensures its retention and catalyzes the construction of this consensus.
  Our vision of collaborative decision-making aims not only at increasing the quality of the first parts of the decision-making process: intelligence and design, but also at increasing the acceptance of the choice. Intelligence and design will be done by more than one individual and constructed together; the decision is more easily accepted. The decided choice will then be shared. Thereby where decision-making could be seen as a constructed model, collaborative decision-making, for us,is seen as the use of socio-technical media to improve decision-making performance and acceptability. The shared decision making is a core activity in a lot of human activities. For example, the sustainable decision-making is the job of not only governments and institutions but also broader society. Recognizing the urgent need for sustainability, we can argue that to realize sustainable development, it must be considered as a decision-making strategy. The location of knowledge in the realization of collaborative decision-making has to be regarded insofar as knowledge sharing leads to improve collaborative decision-making: a ``static view'' has to be structured and constitutes the ``collaborative knowledge.'' Knowledge has an important role in individual decision-making, and we consider that for collaborative decision-making, knowledge has to be shared. What is required is a better understanding of the nature of group work''. Knowledge has to be shared, but how do we share knowledge?</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05495v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mario Lezoche (CRAN), Sanabria Freddy Mu\~noz (LIFIA, UNQ), Collazos Cesar (LIFIA, UNQ), Torres Diego (LIFIA, UNQ), Agredo Vanessa (LILPA), Ruiz Pablo (LILPA), Hurtado Julio</dc:creator>
    </item>
    <item>
      <title>Ordre public exceptions for algorithmic surveillance patents</title>
      <link>https://arxiv.org/abs/2404.05534</link>
      <description>arXiv:2404.05534v1 Announce Type: new 
Abstract: This chapter explores the role of patent protection in algorithmic surveillance and whether ordre public exceptions from patentability should apply to such patents, due to their potential to enable human rights violations. It concludes that in most cases, it is undesirable to exclude algorithmic surveillance patents from patentability, as the patent system is ill-equipped to evaluate the impacts of the exploitation of such technologies. Furthermore, the disclosure of such patents has positive externalities from the societal perspective by opening the black box of surveillance for public scrutiny.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05534v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-662-68599-0_33</arxiv:DOI>
      <dc:creator>Alina Wernick</dc:creator>
    </item>
    <item>
      <title>The Use of Generative Search Engines for Knowledge Work and Complex Tasks</title>
      <link>https://arxiv.org/abs/2404.04268</link>
      <description>arXiv:2404.04268v1 Announce Type: cross 
Abstract: Until recently, search engines were the predominant method for people to access online information. The recent emergence of large language models (LLMs) has given machines new capabilities such as the ability to generate new digital artifacts like text, images, code etc., resulting in a new tool, a generative search engine, which combines the capabilities of LLMs with a traditional search engine. Through the empirical analysis of Bing Copilot (Bing Chat), one of the first publicly available generative search engines, we analyze the types and complexity of tasks that people use Bing Copilot for compared to Bing Search. Findings indicate that people use the generative search engine for more knowledge work tasks that are higher in cognitive complexity than were commonly done with a traditional search engine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04268v1</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Siddharth Suri, Scott Counts, Leijie Wang, Chacha Chen, Mengting Wan, Tara Safavi, Jennifer Neville, Chirag Shah, Ryen W. White, Reid Andersen, Georg Buscher, Sathish Manivannan, Nagu Rangan, Longqi Yang</dc:creator>
    </item>
    <item>
      <title>Hypothesis Generation with Large Language Models</title>
      <link>https://arxiv.org/abs/2404.04326</link>
      <description>arXiv:2404.04326v1 Announce Type: cross 
Abstract: Effective generation of novel hypotheses is instrumental to scientific progress. So far, researchers have been the main powerhouse behind hypothesis generation by painstaking data analysis and thinking (also known as the Eureka moment). In this paper, we examine the potential of large language models (LLMs) to generate hypotheses. We focus on hypothesis generation based on data (i.e., labeled examples). To enable LLMs to handle arbitrarily long contexts, we generate initial hypotheses from a small number of examples and then update them iteratively to improve the quality of hypotheses. Inspired by multi-armed bandits, we design a reward function to inform the exploitation-exploration tradeoff in the update process. Our algorithm is able to generate hypotheses that enable much better predictive performance than few-shot prompting in classification tasks, improving accuracy by 31.7% on a synthetic dataset and by 13.9%, 3.3% and, 24.9% on three real-world datasets. We also outperform supervised learning by 12.8% and 11.2% on two challenging real-world datasets. Furthermore, we find that the generated hypotheses not only corroborate human-verified theories but also uncover new insights for the tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04326v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, Chenhao Tan</dc:creator>
    </item>
    <item>
      <title>System and Method to Determine ME/CFS and Long COVID Disease Severity Using a Wearable Sensor</title>
      <link>https://arxiv.org/abs/2404.04345</link>
      <description>arXiv:2404.04345v1 Announce Type: cross 
Abstract: Objective: We present a simple parameter, calculated from a single wearable sensor, that can be used to objectively measure disease severity in people with myalgic encephalomyelitis/chronic fatigue syndrome (ME/CFS) or Long COVID. We call this parameter UpTime. Methods: Prior research has shown that the amount of time a person spends upright, defined as lower legs vertical with feet on the floor, correlates strongly with ME/CFS disease severity. We use a single commercial inertial measurement unit (IMU) attached to the ankle to calculate the percentage of time each day that a person spends upright (i.e., UpTime) and number of Steps/Day. As Long COVID shares symptoms with ME/CFS, we also apply this method to determine Long COVID disease severity. We performed a trial with 55 subjects broken into three cohorts, healthy controls, ME/CFS, and Long COVID. Subjects wore the IMU on their ankle for a period of 7 days. UpTime and Steps/Day were calculated each day and results compared between cohorts. Results: UpTime effectively distinguishes between healthy controls and subjects diagnosed with ME/CFS ($\mathbf{p = 0.00004}$) and between healthy controls and subjects diagnosed with Long COVID ($\mathbf{p = 0.01185}$). Steps/Day did distinguish between controls and subjects with ME/CFS ($\mathbf{p = 0.01}$) but did not distinguish between controls and subjects with Long COVID ($\mathbf{p = 0.3}$). Conclusion: UpTime is an objective measure of ME/CFS and Long COVID severity. UpTime can be used as an objective outcome measure in clinical research and treatment trials. Significance: Objective assessment of ME/CFS and Long COVID disease severity using UpTime could spur development of treatments by enabling the effect of those treatments to be easily measured.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04345v1</guid>
      <category>q-bio.QM</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifei Sun, Suzanne D. Vernon, Shad Roundy</dc:creator>
    </item>
    <item>
      <title>Language Models as Critical Thinking Tools: A Case Study of Philosophers</title>
      <link>https://arxiv.org/abs/2404.04516</link>
      <description>arXiv:2404.04516v1 Announce Type: cross 
Abstract: Current work in language models (LMs) helps us speed up or even skip thinking by accelerating and automating cognitive work. But can LMs help us with critical thinking -- thinking in deeper, more reflective ways which challenge assumptions, clarify ideas, and engineer new concepts? We treat philosophy as a case study in critical thinking, and interview 21 professional philosophers about how they engage in critical thinking and on their experiences with LMs. We find that philosophers do not find LMs to be useful because they lack a sense of selfhood (memory, beliefs, consistency) and initiative (curiosity, proactivity). We propose the selfhood-initiative model for critical thinking tools to characterize this gap. Using the model, we formulate three roles LMs could play as critical thinking tools: the Interlocutor, the Monitor, and the Respondent. We hope that our work inspires LM researchers to further develop LMs as critical thinking tools and philosophers and other 'critical thinkers' to imagine intellectually substantive uses of LMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04516v1</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andre Ye, Jared Moore, Rose Novick, Amy X. Zhang</dc:creator>
    </item>
    <item>
      <title>Impact of Fairness Regulations on Institutions' Policies and Population Qualifications</title>
      <link>https://arxiv.org/abs/2404.04534</link>
      <description>arXiv:2404.04534v1 Announce Type: cross 
Abstract: The proliferation of algorithmic systems has fueled discussions surrounding the regulation and control of their social impact. Herein, we consider a system whose primary objective is to maximize utility by selecting the most qualified individuals. To promote demographic parity in the selection algorithm, we consider penalizing discrimination across social groups. We examine conditions under which a discrimination penalty can effectively reduce disparity in the selection. Additionally, we explore the implications of such a penalty when individual qualifications may evolve over time in response to the imposed penalizing policy. We identify scenarios where the penalty could hinder the natural attainment of equity within the population. Moreover, we propose certain conditions that can counteract this undesirable outcome, thus ensuring fairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04534v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hamidreza Montaseri, Amin Gohari</dc:creator>
    </item>
    <item>
      <title>Analyzing LLM Usage in an Advanced Computing Class in India</title>
      <link>https://arxiv.org/abs/2404.04603</link>
      <description>arXiv:2404.04603v1 Announce Type: cross 
Abstract: This paper investigates the usage patterns of undergraduate and graduate students when engaging with large language models (LLMs) to tackle programming assignments in the context of advanced computing courses. Existing work predominantly focuses on the influence of LLMs in introductory programming contexts. Additionally, there is a scarcity of studies analyzing actual conversations between students and LLMs. Our study provides a comprehensive quantitative and qualitative analysis of raw interactions between students and LLMs within an advanced computing course (Distributed Systems) at an Indian University. We further complement this by conducting student interviews to gain deeper insights into their usage patterns. Our study shows that students make use of large language models (LLMs) in various ways: generating code or debugging code by identifying and fixing errors. They also copy and paste assignment descriptions into LLM interfaces for specific solutions, ask conceptual questions about complex programming ideas or theoretical concepts, and generate test cases to check code functionality and robustness. Our analysis includes over 4,000 prompts from 411 students and conducting interviews with 10 students. Our analysis shows that LLMs excel at generating boilerplate code and assisting in debugging, while students handle the integration of components and system troubleshooting. This aligns with the learning objectives of advanced computing courses, which are oriented towards teaching students how to build systems and troubleshoot, with less emphasis on generating code from scratch. Therefore, LLM tools can be leveraged to increase student productivity, as shown by the data we collected. This study contributes to the ongoing discussion on LLM use in education, advocating for their usefulness in advanced computing courses to complement higher-level learning and productivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04603v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chaitanya Arora, Utkarsh Venaik, Pavit Singh, Sahil Goyal, Jatin Tyagi, Shyama Goel, Ujjwal Singhal, Dhruv Kumar</dc:creator>
    </item>
    <item>
      <title>A Bird-Eye view on DNA Storage Simulators</title>
      <link>https://arxiv.org/abs/2404.04877</link>
      <description>arXiv:2404.04877v1 Announce Type: cross 
Abstract: In the current world due to the huge demand for storage, DNA-based storage solution sounds quite promising because of their longevity, low power consumption, and high capacity. However in real life storing data in the form of DNA is quite expensive, and challenging. Therefore researchers and developers develop such kind of software that helps simulate real-life DNA storage without worrying about the cost. This paper aims to review some of the software that performs DNA storage simulations in different domains. The paper also explains the core concepts such as synthesis, sequencing, clustering, reconstruction, GC window, K-mer window, etc and some overview on existing algorithms. Further, we present 3 different softwares on the basis of domain, implementation techniques, and customer/commercial usability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04877v1</guid>
      <category>cs.IT</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <category>math.IT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sanket Doshi, Mihir Gohel, Manish K. Gupta</dc:creator>
    </item>
    <item>
      <title>Exploiting Preference Elicitation in Interactive and User-centered Algorithmic Recourse: An Initial Exploration</title>
      <link>https://arxiv.org/abs/2404.05270</link>
      <description>arXiv:2404.05270v1 Announce Type: cross 
Abstract: Algorithmic Recourse aims to provide actionable explanations, or recourse plans, to overturn potentially unfavourable decisions taken by automated machine learning models. In this paper, we propose an interaction paradigm based on a guided interaction pattern aimed at both eliciting the users' preferences and heading them toward effective recourse interventions. In a fictional task of money lending, we compare this approach with an exploratory interaction pattern based on a combination of alternative plans and the possibility of freely changing the configurations by the users themselves. Our results suggest that users may recognize that the guided interaction paradigm improves efficiency. However, they also feel less freedom to experiment with "what-if" scenarios. Nevertheless, the time spent on the purely exploratory interface tends to be perceived as a lack of efficiency, which reduces attractiveness, perspicuity, and dependability. Conversely, for the guided interface, more time on the interface seems to increase its attractiveness, perspicuity, and dependability while not impacting the perceived efficiency. That might suggest that this type of interfaces should combine these two approaches by trying to support exploratory behavior while gently pushing toward a guided effective solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05270v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seyedehdelaram Esfahani, Giovanni De Toni, Bruno Lepri, Andrea Passerini, Katya Tentori, Massimo Zancanaro</dc:creator>
    </item>
    <item>
      <title>Indexing Analytics to Instances: How Integrating a Dashboard can Support Design Education</title>
      <link>https://arxiv.org/abs/2404.05417</link>
      <description>arXiv:2404.05417v1 Announce Type: cross 
Abstract: We investigate how to use AI-based analytics to support design education. The analytics at hand measure multiscale design, that is, students' use of space and scale to visually and conceptually organize their design work. With the goal of making the analytics intelligible to instructors, we developed a research artifact integrating a design analytics dashboard with design instances, and the design environment that students use to create them. We theorize about how Suchman's notion of mutual intelligibility requires contextualized investigation of AI in order to develop findings about how analytics work for people. We studied the research artifact in 5 situated course contexts, in 3 departments. A total of 236 students used the multiscale design environment. The 9 instructors who taught those students experienced the analytics via the new research artifact.
  We derive findings from a qualitative analysis of interviews with instructors regarding their experiences. Instructors reflected on how the analytics and their presentation in the dashboard have the potential to affect design education. We develop research implications addressing: (1) how indexing design analytics in the dashboard to actual design work instances helps design instructors reflect on what they mean and, more broadly, is a technique for how AI-based design analytics can support instructors' assessment and feedback experiences in situated course contexts; and (2) how multiscale design analytics, in particular, have the potential to support design education. By indexing, we mean linking which provides context, here connecting the numbers of the analytics with visually annotated design work instances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05417v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ajit Jain, Andruid Kerne, Nic Lupfer, Gabriel Britain, Aaron Perrine, Yoonsuck Choe, John Keyser, Ruihong Huang, Jinsil Seo, Annie Sungkajun, Robert Lightfoot, Timothy McGuire</dc:creator>
    </item>
    <item>
      <title>Hook-in Privacy Techniques for gRPC-based Microservice Communication</title>
      <link>https://arxiv.org/abs/2404.05598</link>
      <description>arXiv:2404.05598v1 Announce Type: cross 
Abstract: gRPC is at the heart of modern distributed system architectures. Based on HTTP/2 and Protocol Buffers, it provides highly performant, standardized, and polyglot communication across loosely coupled microservices and is increasingly preferred over REST- or GraphQL-based service APIs in practice. Despite its widespread adoption, gRPC lacks any advanced privacy techniques beyond transport encryption and basic token-based authentication. Such advanced techniques are, however, increasingly important for fulfilling regulatory requirements. For instance, anonymizing or otherwise minimizing (personal) data before responding to requests, or pre-processing data based on the purpose of the access may be crucial in certain usecases. In this paper, we therefore propose a novel approach for integrating such advanced privacy techniques into the gRPC framework in a practically viable way. Specifically, we present a general approach along with a working prototype that implements privacy techniques, such as data minimization and purpose limitation, in a configurable, extensible, and gRPC-native way utilizing a gRPC interceptor. We also showcase how to integrate this contribution into a realistic example of a food delivery use case. Alongside these implementations, a preliminary performance evaluation shows practical applicability with reasonable overheads. Altogether, we present a viable solution for integrating advanced privacy techniques into real-world gRPC-based microservice architectures, thereby facilitating regulatory compliance ``by design''.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05598v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.DC</category>
      <category>cs.SE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Louis Loechel, Siar-Remzi Akbayin, Elias Gr\"unewald, Jannis Kiesel, Inga Strelnikova, Thomas Janke, Frank Pallas</dc:creator>
    </item>
    <item>
      <title>Fairness and Bias in Algorithmic Hiring: a Multidisciplinary Survey</title>
      <link>https://arxiv.org/abs/2309.13933</link>
      <description>arXiv:2309.13933v2 Announce Type: replace 
Abstract: Employers are adopting algorithmic hiring technology throughout the recruitment pipeline. Algorithmic fairness is especially applicable in this domain due to its high stakes and structural inequalities. Unfortunately, most work in this space provides partial treatment, often constrained by two competing narratives, optimistically focused on replacing biased recruiter decisions or pessimistically pointing to the automation of discrimination. Whether, and more importantly what types of, algorithmic hiring can be less biased and more beneficial to society than low-tech alternatives currently remains unanswered, to the detriment of trustworthiness. This multidisciplinary survey caters to practitioners and researchers with a balanced and integrated coverage of systems, biases, measures, mitigation strategies, datasets, and legal aspects of algorithmic hiring and fairness. Our work supports a contextualized understanding and governance of this technology by highlighting current opportunities and limitations, providing recommendations for future work to ensure shared benefits for all stakeholders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.13933v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Alessandro Fabris, Nina Baranowska, Matthew J. Dennis, David Graus, Philipp Hacker, Jorge Saldivar, Frederik Zuiderveen Borgesius, Asia J. Biega</dc:creator>
    </item>
    <item>
      <title>And Then the Hammer Broke: Reflections on Machine Ethics from Feminist Philosophy of Science</title>
      <link>https://arxiv.org/abs/2403.05805</link>
      <description>arXiv:2403.05805v2 Announce Type: replace 
Abstract: Vision is an important metaphor in ethical and political questions of knowledge. The feminist philosopher Donna Haraway points out the ``perverse'' nature of an intrusive, alienating, all-seeing vision (to which we might cry out ``stop looking at me!''), but also encourages us to embrace the embodied nature of sight and its promises for genuinely situated knowledge. Current technologies of machine vision -- surveillance cameras, drones (for war or recreation), iPhone cameras -- are usually construed as instances of the former rather than the latter, and for good reasons. However, although in no way attempting to diminish the real suffering these technologies have brought about in the world, I make the case for understanding technologies of computer vision as material instances of embodied seeing and situated knowing. Furthermore, borrowing from Iris Murdoch's concept of moral vision, I suggest that these technologies direct our labor towards self-reflection in ethically significant ways. My approach draws upon paradigms in computer vision research, phenomenology, and feminist epistemology. Ultimately, this essay is an argument for directing more philosophical attention from merely criticizing technologies of vision as ethically deficient towards embracing them as complex, methodologically and epistemologically important objects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05805v2</guid>
      <category>cs.CY</category>
      <category>cs.CV</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andre Ye</dc:creator>
    </item>
    <item>
      <title>The Journey to Trustworthy AI- Part 1: Pursuit of Pragmatic Frameworks</title>
      <link>https://arxiv.org/abs/2403.15457</link>
      <description>arXiv:2403.15457v2 Announce Type: replace 
Abstract: This paper reviews Trustworthy Artificial Intelligence (TAI) and its various definitions. Considering the principles respected in any society, TAI is often characterized by a few attributes, some of which have led to confusion in regulatory or engineering contexts. We argue against using terms such as Responsible or Ethical AI as substitutes for TAI. And to help clarify any confusion, we suggest leaving them behind. Given the subjectivity and complexity inherent in TAI, developing a universal framework is deemed infeasible. Instead, we advocate for approaches centered on addressing key attributes and properties such as fairness, bias, risk, security, explainability, and reliability. We examine the ongoing regulatory landscape, with a focus on initiatives in the EU, China, and the USA. We recognize that differences in AI regulations based on geopolitical and geographical reasons pose an additional challenge for multinational companies. We identify risk as a core factor in AI regulation and TAI. For example, as outlined in the EU-AI Act, organizations must gauge the risk level of their AI products to act accordingly (or risk hefty fines). We compare modalities of TAI implementation and how multiple cross-functional teams are engaged in the overall process. Thus, a brute force approach for enacting TAI renders its efficiency and agility, moot. To address this, we introduce our framework Set-Formalize-Measure-Act (SFMA). Our solution highlights the importance of transforming TAI-aware metrics, drivers of TAI, stakeholders, and business/legal requirements into actual benchmarks or tests. Finally, over-regulation driven by panic of powerful AI models can, in fact, harm TAI too. Based on GitHub user-activity data, in 2023, AI open-source projects rose to top projects by contributor account. Enabling innovation in TAI hinges on the independent contributions of the open-source community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15457v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mohamad M Nasr-Azadani, Jean-Luc Chatelain</dc:creator>
    </item>
    <item>
      <title>Investigation of the effectiveness of applying ChatGPT in Dialogic Teaching Using Electroencephalography</title>
      <link>https://arxiv.org/abs/2403.16687</link>
      <description>arXiv:2403.16687v2 Announce Type: replace 
Abstract: In recent years, the rapid development of artificial intelligence technology, especially the emergence of large language models (LLMs) such as ChatGPT, has presented significant prospects for application in the field of education. LLMs possess the capability to interpret knowledge, answer questions, and consider context, thus providing support for dialogic teaching to students. Therefore, an examination of the capacity of LLMs to effectively fulfill instructional roles, thereby facilitating student learning akin to human educators within dialogic teaching scenarios, is an exceptionally valuable research topic. This research recruited 34 undergraduate students as participants, who were randomly divided into two groups. The experimental group engaged in dialogic teaching using ChatGPT, while the control group interacted with human teachers. Both groups learned the histogram equalization unit in the information-related course "Digital Image Processing". The research findings show comparable scores between the two groups on the retention test. However, students who engaged in dialogue with ChatGPT exhibited lower performance on the transfer test. Electroencephalography data revealed that students who interacted with ChatGPT exhibited higher levels of cognitive activity, suggesting that ChatGPT could help students establish a knowledge foundation and stimulate cognitive activity. However, its strengths on promoting students. knowledge application and creativity were insignificant. Based upon the research findings, it is evident that ChatGPT cannot fully excel in fulfilling teaching tasks in the dialogue teaching in information related courses. Combining ChatGPT with traditional human teachers might be a more ideal approach. The synergistic use of both can provide students with more comprehensive learning support, thus contributing to enhancing the quality of teaching.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16687v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>physics.ed-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiayue Zhang, Yiheng Liu, Wenqi Cai, Yali Peng, Jingjing Yu, Senqing Qi, Taotao Long, Bao Ge</dc:creator>
    </item>
    <item>
      <title>Power and Play: Investigating "License to Critique" in Teams' AI Ethics Discussions</title>
      <link>https://arxiv.org/abs/2403.19049</link>
      <description>arXiv:2403.19049v2 Announce Type: replace 
Abstract: Past work has sought to design AI ethics interventions--such as checklists or toolkits--to help practitioners design more ethical AI systems. However, other work demonstrates how these interventions may instead serve to limit critique to that addressed within the intervention, while rendering broader concerns illegitimate. In this paper, drawing on work examining how standards enact discursive closure and how power relations affect whether and how people raise critique, we recruit three corporate teams, and one activist team, each with prior context working with one another, to play a game designed to trigger broad discussion around AI ethics. We use this as a point of contrast to trigger reflection on their teams' past discussions, examining factors which may affect their "license to critique" in AI ethics discussions. We then report on how particular affordances of this game may influence discussion, and find that the hypothetical context created in the game is unlikely to be a viable mechanism for real world change. We discuss how power dynamics within a group and notions of "scope" affect whether people may be willing to raise critique in AI ethics discussions, and discuss our finding that games are unlikely to enable direct changes to products or practice, but may be more likely to allow members to find critically-aligned allies for future collective action.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19049v2</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Gray Widder, Laura Dabbish, James Herbsleb, Nikolas Martelaro</dc:creator>
    </item>
    <item>
      <title>Fairness and Unfairness in Binary and Multiclass Classification: Quantifying, Calculating, and Bounding</title>
      <link>https://arxiv.org/abs/2206.03234</link>
      <description>arXiv:2206.03234v2 Announce Type: replace-cross 
Abstract: We propose a new interpretable measure of unfairness, that allows providing a quantitative analysis of classifier fairness, beyond a dichotomous fair/unfair distinction. We show how this measure can be calculated when the classifier's conditional confusion matrices are known. We further propose methods for auditing classifiers for their fairness when the confusion matrices cannot be obtained or even estimated. Our approach lower-bounds the unfairness of a classifier based only on aggregate statistics, which may be provided by the owner of the classifier or collected from freely available data. We use the equalized odds criterion, which we generalize to the multiclass case. We report experiments on data sets representing diverse applications, which demonstrate the effectiveness and the wide range of possible uses of the proposed methodology. An implementation of the procedures proposed in this paper and as the code for running the experiments are provided in https://github.com/sivansabato/unfairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.03234v2</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sivan Sabato, Eran Treister, Elad Yom-Tov</dc:creator>
    </item>
    <item>
      <title>The Minimum Wage as an Anchor: Effects on Determinations of Fairness by Humans and AI</title>
      <link>https://arxiv.org/abs/2210.10585</link>
      <description>arXiv:2210.10585v3 Announce Type: replace-cross 
Abstract: I study the role of minimum wage as an anchor for judgements of the fairness of wages by both human subjects and artificial intelligence (AI). Through surveys of human subjects enrolled in the crowdsourcing platform Prolific.co and queries submitted to the OpenAI's language model GPT-3, I test whether the numerical response for what wage is deemed fair for a particular job description changes when respondents and GPT-3 are prompted with additional information that includes a numerical minimum wage, whether realistic or unrealistic, relative to a control where no minimum wage is stated. I find that the minimum wage influences the distribution of responses for the wage considered fair by shifting the mean response toward the minimum wage, thus establishing the minimum wage's role as an anchor for judgements of fairness. However, for unrealistically high minimum wages, namely $50 and $100, the distribution of responses splits into two distinct modes, one that approximately follows the anchor and one that remains close to the control, albeit with an overall upward shift towards the anchor. The anchor exerts a similar effect on the AI bot; however, the wage that the AI bot perceives as fair exhibits a systematic downward shift compared to human subjects' responses. For unrealistic values of the anchor, the responses of the bot also split into two modes but with a smaller proportion of the responses adhering to the anchor compared to human subjects. As with human subjects, the remaining responses are close to the control group for the AI bot but also exhibit a systematic shift towards the anchor. During experimentation, I noted some variability in the bot responses depending on small perturbations of the prompt, so I also test variability in the bot's responses with respect to more meaningful differences in gender and race cues in the prompt, finding anomalies in the distribution of responses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.10585v3</guid>
      <category>econ.GN</category>
      <category>cs.CY</category>
      <category>q-fin.EC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dario G. Soatto</dc:creator>
    </item>
    <item>
      <title>Cumulative differences between paired samples</title>
      <link>https://arxiv.org/abs/2305.11323</link>
      <description>arXiv:2305.11323v2 Announce Type: replace-cross 
Abstract: The simplest, most common paired samples consist of observations from two populations, with each observed response from one population corresponding to an observed response from the other population at the same value of an ordinal covariate. The pair of observed responses (one from each population) at the same value of the covariate is known as a "matched pair" (with the matching based on the value of the covariate). A graph of cumulative differences between the two populations reveals differences in responses as a function of the covariate. Indeed, the slope of the secant line connecting two points on the graph becomes the average difference over the wide interval of values of the covariate between the two points; i.e., slope of the graph is the average difference in responses. ("Average" refers to the weighted average if the samples are weighted.) Moreover, a simple statistic known as the Kuiper metric summarizes into a single scalar the overall differences over all values of the covariate. The Kuiper metric is the absolute value of the total difference in responses between the two populations, totaled over the interval of values of the covariate for which the absolute value of the total is greatest. The total should be normalized such that it becomes the (weighted) average over all values of the covariate when the interval over which the total is taken is the entire range of the covariate (i.e., the sum for the total gets divided by the total number of observations, if the samples are unweighted, or divided by the total weight, if the samples are weighted). This cumulative approach is fully nonparametric and uniquely defined (with only one right way to construct the graphs and scalar summary statistics), unlike traditional methods such as reliability diagrams or parametric or semi-parametric regressions, which typically obscure significant differences due to their parameter settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.11323v2</guid>
      <category>stat.ME</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Isabel Kloumann, Hannah Korevaar, Chris McConnell, Mark Tygert, Jessica Zhao</dc:creator>
    </item>
    <item>
      <title>Editing Personality for Large Language Models</title>
      <link>https://arxiv.org/abs/2310.02168</link>
      <description>arXiv:2310.02168v3 Announce Type: replace-cross 
Abstract: This paper introduces an innovative task focused on editing the personality traits of Large Language Models (LLMs). This task seeks to adjust the models' responses to opinion-related questions on specified topics since an individual's personality often manifests in the form of their expressed opinions, thereby showcasing different personality traits. Specifically, we construct a new benchmark dataset PersonalityEdit to address this task. Drawing on the theory in Social Psychology, we isolate three representative traits, namely Neuroticism, Extraversion, and Agreeableness, as the foundation for our benchmark. We then gather data using GPT-4, generating responses that not only align with a specified topic but also embody the targeted personality trait. We conduct comprehensive experiments involving various baselines and discuss the representation of personality behavior in LLMs. Our intriguing findings uncover potential challenges of the proposed task, illustrating several remaining issues. We anticipate that our work can provide the NLP community with insights. Code and datasets are available at https://github.com/zjunlp/EasyEdit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.02168v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shengyu Mao, Xiaohan Wang, Mengru Wang, Yong Jiang, Pengjun Xie, Fei Huang, Ningyu Zhang</dc:creator>
    </item>
    <item>
      <title>The Shifting Landscape of Cybersecurity: The Impact of Remote Work and COVID-19 on Data Breach Trends</title>
      <link>https://arxiv.org/abs/2402.06650</link>
      <description>arXiv:2402.06650v2 Announce Type: replace-cross 
Abstract: This study examines the impact of the COVID-19 pandemic on cybersecurity and data breaches, with a specific focus on the shift toward remote work. The study identifies trends and offers insights into cybersecurity incidents by analyzing data breaches two years before and two years after the start of remote work. Data was collected from the Montana Department of Justice Data Breach database and consisted of data breaches that occurred between April 2018 and April 2022. The findings inform best practices for cybersecurity preparedness in remote work environments, aiding organizations to enhance their defenses. Although the study's data is limited to Montana, it offers valuable insights for cybersecurity professionals worldwide. As remote work continues to evolve, organizations must remain adaptable and vigilant in their cybersecurity strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.06650v2</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Murat Ozer, Yasin Kose, Mehmet Bastug, Goksel Kucukkaya, Eva Ruhsar Varlioglu</dc:creator>
    </item>
    <item>
      <title>Leveraging Prototypical Representations for Mitigating Social Bias without Demographic Information</title>
      <link>https://arxiv.org/abs/2403.09516</link>
      <description>arXiv:2403.09516v3 Announce Type: replace-cross 
Abstract: Mitigating social biases typically requires identifying the social groups associated with each data sample. In this paper, we present DAFair, a novel approach to address social bias in language models. Unlike traditional methods that rely on explicit demographic labels, our approach does not require any such information. Instead, we leverage predefined prototypical demographic texts and incorporate a regularization term during the fine-tuning process to mitigate bias in the model's representations. Our empirical results across two tasks and two models demonstrate the effectiveness of our method compared to previous approaches that do not rely on labeled data. Moreover, with limited demographic-annotated data, our approach outperforms common debiasing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09516v3</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shadi Iskander, Kira Radinsky, Yonatan Belinkov</dc:creator>
    </item>
    <item>
      <title>FairRAG: Fair Human Generation via Fair Retrieval Augmentation</title>
      <link>https://arxiv.org/abs/2403.19964</link>
      <description>arXiv:2403.19964v3 Announce Type: replace-cross 
Abstract: Existing text-to-image generative models reflect or even amplify societal biases ingrained in their training data. This is especially concerning for human image generation where models are biased against certain demographic groups. Existing attempts to rectify this issue are hindered by the inherent limitations of the pre-trained models and fail to substantially improve demographic diversity. In this work, we introduce Fair Retrieval Augmented Generation (FairRAG), a novel framework that conditions pre-trained generative models on reference images retrieved from an external image database to improve fairness in human generation. FairRAG enables conditioning through a lightweight linear module that projects reference images into the textual space. To enhance fairness, FairRAG applies simple-yet-effective debiasing strategies, providing images from diverse demographic groups during the generative process. Extensive experiments demonstrate that FairRAG outperforms existing methods in terms of demographic diversity, image-text alignment, and image fidelity while incurring minimal computational overhead during inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19964v3</guid>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robik Shrestha, Yang Zou, Qiuyu Chen, Zhiheng Li, Yusheng Xie, Siqi Deng</dc:creator>
    </item>
    <item>
      <title>TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods</title>
      <link>https://arxiv.org/abs/2403.20150</link>
      <description>arXiv:2403.20150v2 Announce Type: replace-cross 
Abstract: Time series are generated in diverse domains such as economic, traffic, health, and energy, where forecasting of future values has numerous important applications. Not surprisingly, many forecasting methods are being proposed. To ensure progress, it is essential to be able to study and compare such methods empirically in a comprehensive and reliable manner. To achieve this, we propose TFB, an automated benchmark for Time Series Forecasting (TSF) methods. TFB advances the state-of-the-art by addressing shortcomings related to datasets, comparison methods, and evaluation pipelines: 1) insufficient coverage of data domains, 2) stereotype bias against traditional methods, and 3) inconsistent and inflexible pipelines. To achieve better domain coverage, we include datasets from 10 different domains: traffic, electricity, energy, the environment, nature, economic, stock markets, banking, health, and the web. We also provide a time series characterization to ensure that the selected datasets are comprehensive. To remove biases against some methods, we include a diverse range of methods, including statistical learning, machine learning, and deep learning methods, and we also support a variety of evaluation strategies and metrics to ensure a more comprehensive evaluations of different methods. To support the integration of different methods into the benchmark and enable fair comparisons, TFB features a flexible and scalable pipeline that eliminates biases. Next, we employ TFB to perform a thorough evaluation of 21 Univariate Time Series Forecasting (UTSF) methods on 8,068 univariate time series and 14 Multivariate Time Series Forecasting (MTSF) methods on 25 datasets. The benchmark code and data are available at https://github.com/decisionintelligence/TFB.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.20150v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xiangfei Qiu, Jilin Hu, Lekui Zhou, Xingjian Wu, Junyang Du, Buang Zhang, Chenjuan Guo, Aoying Zhou, Christian S. Jensen, Zhenli Sheng, Bin Yang</dc:creator>
    </item>
    <item>
      <title>Metarobotics for Industry and Society: Vision, Technologies, and Opportunities</title>
      <link>https://arxiv.org/abs/2404.00797</link>
      <description>arXiv:2404.00797v2 Announce Type: replace-cross 
Abstract: Metarobotics aims to combine next generation wireless communication, multi-sense immersion, and collective intelligence to provide a pervasive, itinerant, and non-invasive access and interaction with distant robotized applications. Industry and society are expected to benefit from these functionalities. For instance, robot programmers will no longer travel worldwide to plan and test robot motions, even collaboratively. Instead, they will have a personalized access to robots and their environments from anywhere, thus spending more time with family and friends. Students enrolled in robotics courses will be taught under authentic industrial conditions in real-time. This paper describes objectives of Metarobotics in society, industry, and in-between. It identifies and surveys technologies likely to enable their completion and provides an architecture to put forward the interplay of key components of Metarobotics. Potentials for self-determination, self-efficacy, and work-life-flexibility in robotics-related applications in Society 5.0, Industry 4.0, and Industry 5.0 are outlined.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00797v2</guid>
      <category>cs.RO</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TII.2023.3337380</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Industrial Informatics, Volume 20, Issue 4, April 2024</arxiv:journal_reference>
      <dc:creator>Eric Guiffo Kaigom</dc:creator>
    </item>
    <item>
      <title>Guarantees of confidentiality via Hammersley-Chapman-Robbins bounds</title>
      <link>https://arxiv.org/abs/2404.02866</link>
      <description>arXiv:2404.02866v2 Announce Type: replace-cross 
Abstract: Protecting privacy during inference with deep neural networks is possible by adding noise to the activations in the last layers prior to the final classifiers or other task-specific layers. The activations in such layers are known as "features" (or, less commonly, as "embeddings" or "feature embeddings"). The added noise helps prevent reconstruction of the inputs from the noisy features. Lower bounding the variance of every possible unbiased estimator of the inputs quantifies the confidentiality arising from such added noise. Convenient, computationally tractable bounds are available from classic inequalities of Hammersley and of Chapman and Robbins -- the HCR bounds. Numerical experiments indicate that the HCR bounds are on the precipice of being effectual for small neural nets with the data sets, "MNIST" and "CIFAR-10," which contain 10 classes each for image classification. The HCR bounds appear to be insufficient on their own to guarantee confidentiality of the inputs to inference with standard deep neural nets, "ResNet-18" and "Swin-T," pre-trained on the data set, "ImageNet-1000," which contains 1000 classes. Supplementing the addition of noise to features with other methods for providing confidentiality may be warranted in the case of ImageNet. In all cases, the results reported here limit consideration to amounts of added noise that incur little degradation in the accuracy of classification from the noisy features. Thus, the added noise enhances confidentiality without much reduction in the accuracy on the task of image classification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02866v2</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kamalika Chaudhuri, Chuan Guo, Laurens van der Maaten, Saeed Mahloujifar, Mark Tygert</dc:creator>
    </item>
    <item>
      <title>The Impact of Unstated Norms in Bias Analysis of Language Models</title>
      <link>https://arxiv.org/abs/2404.03471</link>
      <description>arXiv:2404.03471v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs), trained on vast datasets, can carry biases that manifest in various forms, from overt discrimination to implicit stereotypes. One facet of bias is performance disparities in LLMs, often harming underprivileged groups, such as racial minorities. A common approach to quantifying bias is to use template-based bias probes, which explicitly state group membership (e.g. White) and evaluate if the outcome of a task, sentiment analysis for instance, is invariant to the change of group membership (e.g. change White race to Black). This approach is widely used in bias quantification. However, in this work, we find evidence of an unexpectedly overlooked consequence of using template-based probes for LLM bias quantification. We find that in doing so, text examples associated with White ethnicities appear to be classified as exhibiting negative sentiment at elevated rates. We hypothesize that the scenario arises artificially through a mismatch between the pre-training text of LLMs and the templates used to measure bias through reporting bias, unstated norms that imply group membership without explicit statement. Our finding highlights the potential misleading impact of varying group membership through explicit mention in bias quantification</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03471v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Farnaz Kohankhaki, Jacob-Junqi Tian, David Emerson, Laleh Seyyed-Kalantari, Faiza Khan Khattak</dc:creator>
    </item>
  </channel>
</rss>
