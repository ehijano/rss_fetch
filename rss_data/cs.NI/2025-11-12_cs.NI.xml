<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 13 Nov 2025 02:35:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Resource Allocation in Hybrid Radio-Optical IoT Networks using GNN with Multi-task Learning</title>
      <link>https://arxiv.org/abs/2511.07428</link>
      <description>arXiv:2511.07428v1 Announce Type: new 
Abstract: This paper addresses the problem of dual-technology scheduling in hybrid Internet of Things (IoT) networks that integrate Optical Wireless Communication (OWC) alongside Radio Frequency (RF). We begin by formulating a Mixed-Integer Nonlinear Programming (MINLP) model that jointly considers throughput maximization and delay minimization between access points and IoT nodes under energy and link availability constraints. However, given the intractability of solving such NP-hard problems at scale and the impractical assumption of full channel observability, we propose the Dual-Graph Embedding with Transformer (DGET) framework, a supervised multi-task learning architecture combining a two-stage Graph Neural Networks (GNNs) with a Transformer-based encoder. The first stage employs a transductive GNN that encodes the known graph topology and initial node and link states. The second stage introduces an inductive GNN for temporal refinement, which learns to generalize these embeddings to the evolved states of the same network, capturing changes in energy and queue dynamics over time, by aligning them with ground-truth scheduling decisions through a consistency loss. These enriched embeddings are then processed by a classifier for the communication links with a Transformer encoder that captures cross-link dependencies through multi-head self-attention via classification loss. Simulation results show that hybrid RF-OWC networks outperform standalone RF systems by handling higher traffic loads more efficiently and reducing the Age of Information (AoI) by up to 20%, all while maintaining comparable energy consumption. The proposed DGET framework, compared to traditional optimization-based methods, achieves near-optimal scheduling with over 90% classification accuracy, reduces computational complexity, and demonstrates higher robustness under partial channel observability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07428v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Aymen Hamrouni, Sofie Pollin, Hazem Sallouha</dc:creator>
    </item>
    <item>
      <title>Pinching Antennas Meet AI in Next-Generation Wireless Networks</title>
      <link>https://arxiv.org/abs/2511.07442</link>
      <description>arXiv:2511.07442v1 Announce Type: new 
Abstract: Next-generation (NG) wireless networks must embrace innate intelligence in support of demanding emerging applications, such as extended reality and autonomous systems, under ultra-reliable and low-latency requirements. Pinching antennas (PAs), a new flexible low-cost technology, can create line-of-sight links by dynamically activating small dielectric pinches along a waveguide on demand. As a compelling complement, artificial intelligence (AI) offers the intelligence needed to manage the complex control of PA activation positions and resource allocation in these dynamic environments. This article explores the "win-win" cooperation between AI and PAs: AI facilitates the adaptive optimization of PA activation positions along the waveguide, while PAs support edge AI tasks such as federated learning and over-the-air aggregation. We also discuss promising research directions including large language model-driven PA control frameworks, and how PA-AI integration can advance semantic communications, and integrated sensing and communication. This synergy paves the way for adaptive, resilient, and self-optimizing NG networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07442v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fang Fang, Zhiguo Ding, Victor C. M. Leung, Lajos Hanzo</dc:creator>
    </item>
    <item>
      <title>Optimal Multi-Constrained Workflow Scheduling for Cyber-Physical Systems in the Edge-Cloud Continuum</title>
      <link>https://arxiv.org/abs/2511.07466</link>
      <description>arXiv:2511.07466v1 Announce Type: new 
Abstract: The emerging edge-hub-cloud paradigm has enabled the development of innovative latency-critical cyber-physical applications in the edge-cloud continuum. However, this paradigm poses multiple challenges due to the heterogeneity of the devices at the edge of the network, their limited computational, communication, and energy capacities, as well as their different sensing and actuating capabilities. To address these issues, we propose an optimal scheduling approach to minimize the overall latency of a workflow application in an edge-hub-cloud cyber-physical system. We consider multiple edge devices cooperating with a hub device and a cloud server. All devices feature heterogeneous multicore processors and various sensing, actuating, or other specialized capabilities. We present a comprehensive formulation based on continuous-time mixed integer linear programming, encapsulating multiple constraints often overlooked by existing approaches. We conduct a comparative experimental evaluation between our method and a well-established and effective scheduling heuristic, which we enhanced to consider the constraints of the specific problem. The results reveal that our technique outperforms the heuristic, achieving an average latency improvement of 13.54% in a relevant real-world use case, under varied system configurations. In addition, the results demonstrate the scalability of our method under synthetic workflows of varying sizes, attaining a 33.03% average latency decrease compared to the heuristic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07466v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/COMPSAC61105.2024.00072</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 2024 IEEE 48th Annual Computers, Software, and Applications Conference (COMPSAC), Osaka, Japan, 2024, pp. 483-492</arxiv:journal_reference>
      <dc:creator>Andreas Kouloumpris, Georgios L. Stavrinides, Maria K. Michael, Theocharis Theocharides</dc:creator>
    </item>
    <item>
      <title>A Large-Scale Dataset and Reproducible Framework for RF Fingerprinting on IEEE 802.11g Same-Model Devices</title>
      <link>https://arxiv.org/abs/2511.07770</link>
      <description>arXiv:2511.07770v1 Announce Type: new 
Abstract: Radio frequency (RF) fingerprinting exploits hardware imperfections for device identification, but distinguishing between same-model devices remains challenging due to their minimal hardware variations. Existing datasets for RF fingerprinting are constrained by small device scales and heterogeneous models, which hinders robust training and fair evaluation for machine learning models. To address this gap, we introduce a large-scale dataset of same-model devices along with a fully reproducible, open-source experimental framework. The dataset is built using 123 identical commercial IEEE 802.11g devices and contains 35.42 million raw I/Q samples from the preambles and corresponding 1.85 million RF features. The open-source framework further ensures full reproducibility from data collection to final evaluation. Within this framework, a Random Forest-based algorithm is proposed to achieve 89.06% identification accuracy on this dataset. Extensive experimental evaluations further confirm the relationships between the extracted features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07770v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zewei Guo, Zhen Jia, JinXiao Zhu, Wenhao Huang, Yin Chen</dc:creator>
    </item>
    <item>
      <title>Argo: An efficient verification framework for distributed in-network computing</title>
      <link>https://arxiv.org/abs/2511.08189</link>
      <description>arXiv:2511.08189v1 Announce Type: new 
Abstract: Distributed in-network programs are increasingly deployed in data centers for their performance benefits, but shifting application logic to switches also enlarges the failure domain. Ensuring their correctness before deployment is thus critical for reliability. While prior verification frameworks can efficiently detect bugs for programs running on a single switch, they overlook the common interactive behaviors in distributed settings, thereby missing related bugs that can cause state inconsistencies and system failures. This paper presents Procurator, a verification framework that efficiently captures interactive behaviors in distributed in-network programs. Procurator introduces a formal model combining the actor paradigm with Communicating Sequential Processes (CSP), translating pipeline execution into reactive, event-driven actors and unifying their interactions as message passing. To support flexible specification of distributed properties, it provides a unified intent language. Additionally, it incorporates a semantic-aware state pruner to reduce verification complexity, thus ensuring system scalability. Evaluation results show that Procurator efficiently uncovers 10 distinct bugs caused by interactive behaviors across five real-world in-network systems. It also reduces verification time by up to 913.2x and memory consumption by up to 1.9x compared to the state-of-the-art verifier.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08189v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingyuan Song, Huan Shen, Jinghui Jiang, Qiang Su, Qingyu Song, Lu Tang, Wanjian Feng, Fei Yuan, Qiao Xiang, Jiwu Shu</dc:creator>
    </item>
    <item>
      <title>SRE-Llama -- Fine-Tuned Meta's Llama LLM, Federated Learning, Blockchain and NFT Enabled Site Reliability Engineering(SRE) Platform for Communication and Networking Software Services</title>
      <link>https://arxiv.org/abs/2511.08282</link>
      <description>arXiv:2511.08282v1 Announce Type: new 
Abstract: Software services are crucial for reliable communication and networking; therefore, Site Reliability Engineering (SRE) is important to ensure these systems stay reliable and perform well in cloud-native environments. SRE leverages tools like Prometheus and Grafana to monitor system metrics, defining critical Service Level Indicators (SLIs) and Service Level Objectives (SLOs) for maintaining high service standards. However, a significant challenge arises as many developers often lack in-depth understanding of these tools and the intricacies involved in defining appropriate SLIs and SLOs. To bridge this gap, we propose a novel SRE platform, called SRE-Llama, enhanced by Generative-AI, Federated Learning, Blockchain, and Non-Fungible Tokens (NFTs). This platform aims to automate and simplify the process of monitoring, SLI/SLO generation, and alert management, offering ease in accessibility and efficy for developers. The system operates by capturing metrics from cloud-native services and storing them in a time-series database, like Prometheus and Mimir. Utilizing this stored data, our platform employs Federated Learning models to identify the most relevant and impactful SLI metrics for different services and SLOs, addressing concerns around data privacy. Subsequently, fine-tuned Meta's Llama-3 LLM is adopted to intelligently generate SLIs, SLOs, error budgets, and associated alerting mechanisms based on these identified SLI metrics. A unique aspect of our platform is the encoding of generated SLIs and SLOs as NFT objects, which are then stored on a Blockchain. This feature provides immutable record-keeping and facilitates easy verification and auditing of the SRE metrics and objectives. The automation of the proposed platform is governed by the blockchain smart contracts. The proposed SRE-Llama platform prototype has been implemented with a use case featuring a customized Open5GS 5G Core.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08282v1</guid>
      <category>cs.NI</category>
      <category>cs.CR</category>
      <category>cs.ET</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eranga Bandara, Safdar H. Bouk, Sachin Shetty, Ravi Mukkamala, Abdul Rahman, Peter Foytik, Ross Gore, Xueping Liang, Ng Wee Keong, Kasun De Zoysa</dc:creator>
    </item>
    <item>
      <title>Demystifying QUIC from the Specifications</title>
      <link>https://arxiv.org/abs/2511.08375</link>
      <description>arXiv:2511.08375v1 Announce Type: new 
Abstract: QUIC is an advanced transport layer protocol whose ubiquity on the Internet is now very apparent. Importantly, QUIC fuels the next generation of web browsing: HTTP/3. QUIC is a stateful and connection oriented protocol which offers similar features (and more) to the combination of TCP and TLS. There are several difficulties which readers may encounter when learning about QUIC: i.) its rapid evolution (particularly, differentiation between the QUIC standard and the now deprecated Google QUIC), ii.) numerous RFCs whose organization, language, and detail may be challenging to the casual reader, and iii.) the nature of QUIC's cross-layer and privacy-centric implementation, making it impossible to understand or debug by looking at packets alone. For these reasons, the aim of this paper is to present QUIC in a complete yet approachable fashion, thereby demystifying the protocol from its specifications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08375v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Darius Saif, Ashraf Matrawy</dc:creator>
    </item>
    <item>
      <title>Fault Tolerant Reconfigurable ML Multiprocessor</title>
      <link>https://arxiv.org/abs/2511.08381</link>
      <description>arXiv:2511.08381v1 Announce Type: new 
Abstract: This paper reports three computational experiments for a von Neumann inspired reconfigurable fault tolerant multiprocessor for neural network (NN) training workflows. The experiments are intended to prove the feasibility of the proposed reconfigurable multiprocessor architecture for non-regular workflows on robustness of adaptability. A potential integration with MLIR compilers is also discussed for integrating diverse accelerator hardware for existing practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08381v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tangrui Li, Justin Y. Shi, Matteo Spatola, Hongzheng Wang</dc:creator>
    </item>
    <item>
      <title>Adaptive Reallocation of RAN Functions for Resilient 6G Networks</title>
      <link>https://arxiv.org/abs/2511.08467</link>
      <description>arXiv:2511.08467v1 Announce Type: new 
Abstract: The disaggregation of base stations into discrete RAN functions introduces new threats to mobile networks, as failures in one RAN function can trigger cascading failures and disrupt the entire functional chain, impacting network performance and leading to outages. In this paper, we propose the first resilience mechanism leveraging the adaptive placement of RAN functions to mitigate disruptions and recover service continuity in the presence of compromised infrastructure. Our model detects disrupted RUs due to cascading failures, reacts by re-instantiating CU and DU in alternative cloud locations, and recovers service continuity by reestablishing functional chains. We formulate this recovery process as an optimization problem that maximizes post-failure network performance while considering computational and communication constraints of the infrastructure. We numerically evaluated our approach on a real-world mobile network topology under multiple failure scenarios, and demonstrated that our solution recovers up to 70% higher throughput compared to conventional resilience mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08467v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriel M. Almeida, Jacek Kibi{\l}da, Joao F. Santos, Kleber Vieira Cardoso</dc:creator>
    </item>
    <item>
      <title>Network and Systems Performance Characterization of MCP-Enabled LLM Agents</title>
      <link>https://arxiv.org/abs/2511.07426</link>
      <description>arXiv:2511.07426v1 Announce Type: cross 
Abstract: Model Context Protocol (MCP) has recently gained increased attention within the AI community for providing a standardized way for large language models (LLMs) to interact with external tools and services, significantly enhancing their capabilities. However, the inclusion of extensive contextual information, including system prompts, MCP tool definitions, and context histories, in MCP-enabled LLM interactions, dramatically inflates token usage. Given that LLM providers charge based on tokens, these expanded contexts can quickly escalate monetary costs and increase the computational load on LLM services. This paper presents a comprehensive measurement-based analysis of MCP-enabled interactions with LLMs, revealing trade-offs between capability, performance, and cost. We explore how different LLM models and MCP configurations impact key performance metrics such as token efficiency, monetary cost, task completion times, and task success rates, and suggest potential optimizations, including enabling parallel tool calls and implementing robust task abort mechanisms. These findings provide useful insights for developing more efficient, robust, and cost-effective MCP-enabled workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07426v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.NI</category>
      <category>cs.SE</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zihao Ding, Mufeng Zhu, Yao Liu</dc:creator>
    </item>
    <item>
      <title>Revisiting Network Traffic Analysis: Compatible network flows for ML models</title>
      <link>https://arxiv.org/abs/2511.08345</link>
      <description>arXiv:2511.08345v1 Announce Type: cross 
Abstract: To ensure that Machine Learning (ML) models can perform a robust detection and classification of cyberattacks, it is essential to train them with high-quality datasets with relevant features. However, it can be difficult to accurately represent the complex traffic patterns of an attack, especially in Internet-of-Things (IoT) networks. This paper studies the impact that seemingly similar features created by different network traffic flow exporters can have on the generalization and robustness of ML models. In addition to the original CSV files of the Bot-IoT, IoT-23, and CICIoT23 datasets, the raw network packets of their PCAP files were analysed with the HERA tool, generating new labelled flows and extracting consistent features for new CSV versions. To assess the usefulness of these new flows for intrusion detection, they were compared with the original versions and were used to fine-tune multiple models. Overall, the results indicate that directly analysing and preprocessing PCAP files, instead of just using the commonly available CSV files, enables the computation of more relevant features to train bagging and gradient boosting decision tree ensembles. It is important to continue improving feature extraction and feature selection processes to make different datasets more compatible and enable a trustworthy evaluation and comparison of the ML models used in cybersecurity solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08345v1</guid>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jo\~ao Vitorino, Daniela Pinto, Eva Maia, Ivone Amorim, Isabel Pra\c{c}a</dc:creator>
    </item>
    <item>
      <title>A CODECO Case Study and Initial Validation for Edge Orchestration of Autonomous Mobile Robots</title>
      <link>https://arxiv.org/abs/2511.08354</link>
      <description>arXiv:2511.08354v1 Announce Type: cross 
Abstract: Autonomous Mobile Robots (AMRs) increasingly adopt containerized micro-services across the Edge-Cloud continuum. While Kubernetes is the de-facto orchestrator for such systems, its assumptions of stable networks, homogeneous resources, and ample compute capacity do not fully hold in mobile, resource-constrained robotic environments.
  This paper describes a case study on smart-manufacturing AMRs and performs an initial comparison between CODECO orchestration and standard Kubernetes using a controlled KinD environment. Metrics include pod deployment and deletion times, CPU and memory usage, and inter-pod data rates. The observed results indicate that CODECO offers reduced CPU consumption and more stable communication patterns, at the cost of modest memory overhead (10-15%) and slightly increased pod lifecycle latency due to secure overlay initialization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08354v1</guid>
      <category>cs.RO</category>
      <category>cs.ET</category>
      <category>cs.NI</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>H. Zhu, T. Samizadeh, R. C. Sofia</dc:creator>
    </item>
    <item>
      <title>Toward Autonomous and Efficient Cybersecurity: A Multi-Objective AutoML-based Intrusion Detection System</title>
      <link>https://arxiv.org/abs/2511.08491</link>
      <description>arXiv:2511.08491v1 Announce Type: cross 
Abstract: With increasingly sophisticated cybersecurity threats and rising demand for network automation, autonomous cybersecurity mechanisms are becoming critical for securing modern networks. The rapid expansion of Internet of Things (IoT) systems amplifies these challenges, as resource-constrained IoT devices demand scalable and efficient security solutions. In this work, an innovative Intrusion Detection System (IDS) utilizing Automated Machine Learning (AutoML) and Multi-Objective Optimization (MOO) is proposed for autonomous and optimized cyber-attack detection in modern networking environments. The proposed IDS framework integrates two primary innovative techniques: Optimized Importance and Percentage-based Automated Feature Selection (OIP-AutoFS) and Optimized Performance, Confidence, and Efficiency-based Combined Algorithm Selection and Hyperparameter Optimization (OPCE-CASH). These components optimize feature selection and model learning processes to strike a balance between intrusion detection effectiveness and computational efficiency. This work presents the first IDS framework that integrates all four AutoML stages and employs multi-objective optimization to jointly optimize detection effectiveness, efficiency, and confidence for deployment in resource-constrained systems. Experimental evaluations over two benchmark cybersecurity datasets demonstrate that the proposed MOO-AutoML IDS outperforms state-of-the-art IDSs, establishing a new benchmark for autonomous, efficient, and optimized security for networks. Designed to support IoT and edge environments with resource constraints, the proposed framework is applicable to a variety of autonomous cybersecurity applications across diverse networked environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08491v1</guid>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TMLCN.2025.3631379</arxiv:DOI>
      <dc:creator>Li Yang, Abdallah Shami</dc:creator>
    </item>
    <item>
      <title>Vision Transformer Based User Equipment Positioning</title>
      <link>https://arxiv.org/abs/2511.08549</link>
      <description>arXiv:2511.08549v1 Announce Type: cross 
Abstract: Recently, Deep Learning (DL) techniques have been used for User Equipment (UE) positioning. However, the key shortcomings of such models is that: i) they weigh the same attention to the entire input; ii) they are not well suited for the non-sequential data e.g., when only instantaneous Channel State Information (CSI) is available. In this context, we propose an attention-based Vision Transformer (ViT) architecture that focuses on the Angle Delay Profile (ADP) from CSI matrix. Our approach, validated on the `DeepMIMO' and `ViWi' ray-tracing datasets, achieves an Root Mean Squared Error (RMSE) of 0.55m indoors, 13.59m outdoors in DeepMIMO, and 3.45m in ViWi's outdoor blockage scenario. The proposed scheme outperforms state-of-the-art schemes by $\sim$ 38\%. It also performs substantially better than other approaches that we have considered in terms of the distribution of error distance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08549v1</guid>
      <category>cs.CV</category>
      <category>cs.NI</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Parshwa Shah, Dhaval K. Patel, Brijesh Soni, Miguel L\'opez-Ben\'itez, Siddhartan Govindasamy</dc:creator>
    </item>
    <item>
      <title>Universal Connection Schedules for Reconfigurable Networking</title>
      <link>https://arxiv.org/abs/2511.08556</link>
      <description>arXiv:2511.08556v1 Announce Type: cross 
Abstract: Reconfigurable networks are a novel communication paradigm in which the pattern of connectivity between hosts varies rapidly over time. Prior theoretical work explored the inherent tradeoffs between throughput (or, hop-count) and latency, and showed the existence of infinitely many Pareto-optimal designs as the network size tends to infinity. Existing Pareto-optimal designs use a connection schedule which is fine-tuned to the desired hop-count $h$, permitting lower latency as $h$ increases. However, in reality datacenter workloads contain a mix of low-latency and high-latency requests. Using a connection schedule fine-tuned for one request type leads to inefficiencies when serving other types.
  A more flexible and efficient alternative is a {\em universal schedule}, a single connection schedule capable of attaining many Pareto-optimal tradeoff points simultaneously, merely by varying the choice of routing paths. In this work we present the first universal schedules for oblivious routing. Our constructions yield universal schedules which are near-optimal for all possible hop-counts $h$. The key technical idea is to specialize to a type of connection schedule based on cyclic permutations and to develop a novel Fourier-analytic method for analyzing randomized routing on these connection schedules. We first show that a uniformly random connection schedule suffices with multiplicative error in throughput, and latency optimal up to a $\log N$ factor. We then show that a more carefully designed random connection schedule suffices with additive error in throughput, but improved latency optimal up to only constant factors. Finally, we show that our first randomized construction can be made deterministic using a derandomized version of the Lovett-Meka discrepancy minimization algorithm to obtain the same result.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08556v1</guid>
      <category>cs.DS</category>
      <category>cs.NI</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shaleen Baral, Robert Kleinberg, Sylvan Martin, Henry Rogers, Tegan Wilson, Ruogu Zhang</dc:creator>
    </item>
    <item>
      <title>EPFL-REMNet: Efficient Personalized Federated Digital Twin Towards 6G Heterogeneous Radio Environment</title>
      <link>https://arxiv.org/abs/2511.05238</link>
      <description>arXiv:2511.05238v2 Announce Type: replace 
Abstract: Radio Environment Map (REM) is transitioning from 5G homogeneous environments to B5G/6G heterogeneous landscapes. However, standard Federated Learning (FL), a natural fit for this distributed task, struggles with performance degradation in accuracy and communication efficiency under the non-independent and identically distributed (Non-IID) data conditions inherent to these new environments. This paper proposes EPFL-REMNet, an efficient personalized federated framework for constructing a high-fidelity digital twin of the 6G heterogeneous radio environment. The proposed EPFL-REMNet employs a"shared backbone + lightweight personalized head" model, where only the compressed shared backbone is transmitted between the server and clients, while each client's personalized head is maintained locally. We tested EPFL-REMNet by constructing three distinct Non-IID scenarios (light, medium, and heavy) based on radio environment complexity, with data geographically partitioned across 90 clients. Experimental results demonstrate that EPFL-REMNet simultaneously achieves higher digital twin fidelity (accuracy) and lower uplink overhead across all Non-IID settings compared to standard FedAvg and recent state-of-the-art methods. Particularly, it significantly reduces performance disparities across datasets and improves local map accuracy for long-tail clients, enhancing the overall integrity of digital twin.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05238v2</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peide Li, Liu Cao, Lyutianyang Zhang, Dongyu Wei, Ye Hu, Qipeng Xie</dc:creator>
    </item>
    <item>
      <title>GeMID: Generalizable Models for IoT Device Identification</title>
      <link>https://arxiv.org/abs/2411.14441</link>
      <description>arXiv:2411.14441v3 Announce Type: replace-cross 
Abstract: With the proliferation of devices on the Internet of Things (IoT), ensuring their security has become paramount. Device identification (DI), which distinguishes IoT devices based on their traffic patterns, plays a crucial role in both differentiating devices and identifying vulnerable ones, closing a serious security gap. However, existing approaches to DI that build machine learning models often overlook the challenge of model generalizability across diverse network environments. In this study, we propose a novel framework to address this limitation and to evaluate the generalizability of DI models across data sets collected within different network environments. Our approach involves a two-step process: first, we develop a feature and model selection method that is more robust to generalization issues by using a genetic algorithm with external feedback and datasets from distinct environments to refine the selections. Second, the resulting DI models are then tested on further independent datasets to robustly assess their generalizability. We demonstrate the effectiveness of our method by empirically comparing it to alternatives, highlighting how fundamental limitations of commonly employed techniques such as sliding window and flow statistics limit their generalizability. Moreover, we show that statistical methods, widely used in the literature, are unreliable for device identification due to their dependence on network-specific characteristics rather than device-intrinsic properties, challenging the validity of a significant portion of existing research. Our findings advance research in IoT security and device identification, offering insight into improving model effectiveness and mitigating risks in IoT networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14441v3</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.iot.2025.101806</arxiv:DOI>
      <arxiv:journal_reference>Internet of Things 34 (2025) 101806</arxiv:journal_reference>
      <dc:creator>Kahraman Kostas, Rabia Yasa Kostas, Mike Just, Michael A. Lones</dc:creator>
    </item>
    <item>
      <title>Federated Learning with Gramian Angular Fields for Privacy-Preserving ECG Classification on Heterogeneous IoT Devices</title>
      <link>https://arxiv.org/abs/2511.03753</link>
      <description>arXiv:2511.03753v2 Announce Type: replace-cross 
Abstract: This study presents a federated learning (FL) framework for privacy-preserving electrocardiogram (ECG) classification in Internet of Things (IoT) healthcare environments. By transforming 1D ECG signals into 2D Gramian Angular Field (GAF) images, the proposed approach enables efficient feature extraction through Convolutional Neural Networks (CNNs) while ensuring that sensitive medical data remain local to each device. This work is among the first to experimentally validate GAF-based federated ECG classification across heterogeneous IoT devices, quantifying both performance and communication efficiency. To evaluate feasibility in realistic IoT settings, we deployed the framework across a server, a laptop, and a resource-constrained Raspberry Pi 4, reflecting edge-cloud integration in IoT ecosystems. Experimental results demonstrate that the FL-GAF model achieves a high classification accuracy of 95.18% in a multi-client setup, significantly outperforming a single-client baseline in both accuracy and training time. Despite the added computational complexity of GAF transformations, the framework maintains efficient resource utilization and communication overhead. These findings highlight the potential of lightweight, privacy-preserving AI for IoT-based healthcare monitoring, supporting scalable and secure edge deployments in smart health systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03753v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.NI</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Youssef Elmir, Yassine Himeur, Abbes Amira</dc:creator>
    </item>
  </channel>
</rss>
