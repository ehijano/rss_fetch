<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 11 Feb 2025 05:00:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Computational Complexity of Polynomial Subalgebras</title>
      <link>https://arxiv.org/abs/2502.05278</link>
      <description>arXiv:2502.05278v1 Announce Type: new 
Abstract: The computational complexity of polynomial ideals and Gr\"obner bases has been studied since the 1980s. In recent years the related notions of polynomial subalgebras and SAGBI bases have gained more and more attention in computational algebra, with a view towards effective algorithms. We investigate the computational complexity of the subalgebra membership problem and degree bounds. In particular, we place these problems in the complexity class EXPSPACE and prove PSPACE-completeness for homogeneous algebras. We highlight parallels and differences compared to the settings of ideals and also look at important classes of polynomials such as monomial algebras.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05278v1</guid>
      <category>cs.CC</category>
      <category>math.AC</category>
      <category>math.AG</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leonie Kayser</dc:creator>
    </item>
    <item>
      <title>Induced Disjoint Paths Without an Induced Minor</title>
      <link>https://arxiv.org/abs/2502.05289</link>
      <description>arXiv:2502.05289v1 Announce Type: new 
Abstract: We exhibit a new obstacle to the nascent algorithmic theory for classes excluding an induced minor. We indeed show that on the class of string graphs -- which avoids the 1-subdivision of, say, $K_5$ as an induced minor -- Induced 2-Disjoint Paths is NP-complete. So, while $k$-Disjoint Paths, for a fixed $k$, is polynomial-time solvable in general graphs, the absence of a graph as an induced minor does not make its induced variant tractable, even for $k=2$. This answers a question of Korhonen and Lokshtanov [SODA '24], and complements a polynomial-time algorithm for Induced $k$-Disjoint Paths in classes of bounded genus by Kobayashi and Kawarabayashi [SODA '09]. In addition to being string graphs, our produced hard instances are subgraphs of a constant power of bounded-degree planar graphs, hence have bounded twin-width and bounded maximum degree.
  We also leverage our new result to show that there is a fixed subcubic graph $H$ such that deciding if an input graph contains $H$ as an induced subdivision is NP-complete. Until now, all the graphs $H$ for which such a statement was known had a vertex of degree at least 4. This answers a question by Chudnovsky, Seymour, and the fourth author [JCTB '13], and by Le [JGT '19]. Finally we resolve another question of Korhonen and Lokshtanov by exhibiting a subcubic graph $H$ without two adjacent degree-3 vertices and such that deciding if an input $n$-vertex graph contains $H$ as an induced minor is NP-complete, and unless the Exponential-Time Hypothesis fails, requires time $2^{\Omega(\sqrt n)}$. This complements an algorithm running in subexponential time $2^{O(n^{2/3} \log n)}$ by these authors [SODA '24] under the same technical condition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05289v1</guid>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pierre Aboulker, \'Edouard Bonnet, Timoth\'e Picavet, Nicolas Trotignon</dc:creator>
    </item>
    <item>
      <title>The Complexity of Blocking All Solutions</title>
      <link>https://arxiv.org/abs/2502.05348</link>
      <description>arXiv:2502.05348v1 Announce Type: new 
Abstract: We consider the general problem of blocking all solutions of some given combinatorial problem with only few elements. For example, the problem of destroying all maximum cliques of a given graph by forbidding only few vertices. Problems of this kind are so fundamental that they have been studied under many different names in many different disjoint research communities already since the 90s. Depending on the context, they have been called the interdiction, most vital vertex, most vital edge, blocker, or vertex deletion problem. Despite their apparent popularity, surprisingly little is known about the computational complexity of interdiction problems in the case where the original problem is already NP-complete. In this paper, we fill that gap of knowledge by showing that a large amount of interdiction problems are even harder than NP-hard. Namely, they are complete for the second stage of Stockmeyer's polynomial hierarchy, the complexity class $\Sigma^p_2$. Such complexity insights are important because they imply that all these problems can not be modelled by a compact integer program (unless the unlikely conjecture NP $= \Sigma_2^p$ holds). Concretely, we prove $\Sigma^p_2$-completeness of the following interdiction problems: satisfiability, 3satisfiability, dominating set, set cover, hitting set, feedback vertex set, feedback arc set, uncapacitated facility location, $p$-center, $p$-median, independent set, clique, subset sum, knapsack, Hamiltonian path/cycle (directed/undirected), TSP, $k$ directed vertex disjoint path ($k \geq 2$), Steiner tree. We show that all of these problems share an abstract property which implies that their interdiction counterpart is $\Sigma_2^p$-complete. Thus, all of these problems are $\Sigma_2^p$-complete \enquote{for the same reason}. Our result extends a recent framework by Gr\"une and Wulf.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05348v1</guid>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>math.OC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christoph Gr\"une, Lasse Wulf</dc:creator>
    </item>
    <item>
      <title>From an odd arity signature to a Holant dichotomy</title>
      <link>https://arxiv.org/abs/2502.05597</link>
      <description>arXiv:2502.05597v1 Announce Type: new 
Abstract: \textsf{Holant} is an essential framework in the field of counting complexity. For over fifteen years, researchers have been clarifying the complexity classification for complex-valued \textsf{Holant} on the Boolean domain, a challenge that remains unresolved. In this article, we prove a complexity dichotomy for complex-valued \textsf{Holant} on Boolean domain when a non-trivial signature of odd arity exists. This dichotomy is based on the dichotomy for \textsf{\#EO}, and consequently is an $\text{FP}^\text{NP}$ vs. \#P dichotomy as well, stating that each problem is either in $\text{FP}^\text{NP}$ or \#P-hard.
  Furthermore, we establish a generalized version of the decomposition lemma for complex-valued \textsf{Holant} on Boolean domain. It asserts that each signature can be derived from its tensor product with other signatures, or conversely, the problem itself is in $\text{FP}^\text{NP}$. We believe that this result is a powerful method for building reductions in complex-valued \textsf{Holant}, as it is also employed as a pivotal technique in the proof of the aforementioned dichotomy in this article.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05597v1</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Boning Meng, Juqiu Wang, Mingji Xia, Jiayi Zheng</dc:creator>
    </item>
    <item>
      <title>FeatPCA: A feature subspace based principal component analysis technique for enhancing clustering of single-cell RNA-seq data</title>
      <link>https://arxiv.org/abs/2502.05647</link>
      <description>arXiv:2502.05647v1 Announce Type: new 
Abstract: Single-cell RNA sequencing (scRNA-seq) has revolutionized our ability to analyze gene expression at the cellular level. By providing data on gene expression for each individual cell, scRNA-seq generates large datasets with thousands of genes. However, handling such high-dimensional data poses computational challenges due to increased complexity. Dimensionality reduction becomes crucial for scRNA-seq analysis. Various dimensionality reduction algorithms, including Principal Component Analysis (PCA), Uniform Manifold Approximation and Projection (UMAP), and t-Distributed Stochastic Neighbor Embedding (t-SNE), are commonly used to address this challenge. These methods transform the original high-dimensional data into a lower-dimensional representation while preserving relevant information. In this paper we propose {\methodname}. Instead of applying dimensionality reduction directly to the entire dataset, we divide it into multiple subspaces. Within each subspace, we apply dimension reduction techniques, and then merge the reduced data. {\methodname} offers four variations for subspacing. Our experimental results demonstrate that clustering based on subspacing yields better accuracy than working with the full dataset. Across a variety of scRNA-seq datasets, {\methodname} consistently outperforms existing state-of-the-art clustering tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05647v1</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Md Romizul Islam, Swakkhar Shatabda</dc:creator>
    </item>
    <item>
      <title>A Parameterized Study of Secluded Structures in Directed Graphs</title>
      <link>https://arxiv.org/abs/2502.06048</link>
      <description>arXiv:2502.06048v1 Announce Type: new 
Abstract: Given an undirected graph $G$ and an integer $k$, the Secluded $\Pi$-Subgraph problem asks you to find a maximum size induced subgraph that satisfies a property $\Pi$ and has at most $k$ neighbors in the rest of the graph. This problem has been extensively studied; however, there is no prior study of the problem in directed graphs. This question has been mentioned by Jansen et al. [ISAAC'23].
  In this paper, we initiate the study of Secluded Subgraph problem in directed graphs by incorporating different notions of neighborhoods: in-neighborhood, out-neighborhood, and their union. Formally, we call these problems {\{In, Out, Total\}-Secluded $\Pi$-Subgraph, where given a directed graph $G$ and integers $k$, we want to find an induced subgraph satisfying $\Pi$ of maximum size that has at most $k$ in/out/total-neighbors in the rest of the graph, respectively. We investigate the parameterized complexity of these problems for different properties $\Pi$. In particular, we prove the following parameterized results: - We design an FPT algorithm for the Total-Secluded Strongly Connected Subgraph problem when parameterized by $k$. - We show that the In/Out-Secluded $\mathcal{F}$-Free Subgraph problem with parameter $k+w$ is W[1]-hard, where $\mathcal{F}$ is a family of directed graphs except any subgraph of a star graph whose edges are directed towards the center. This result also implies that In/Out-Secluded DAG is W[1]-hard, unlike the undirected variants of the two problems, which are FPT. - We design an FPT-algorithm for In/Out/Total-Secluded $\alpha$-Bounded Subgraph when parameterized by $k$, where $\alpha$-bounded graphs are a superclass of tournaments. - For undirected graphs, we improve the best-known FPT algorithm for Secluded Clique by providing a faster FPT algorithm that runs in time $1.6181^kn^{\mathcal{O}(1)}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06048v1</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nadym Mallek, Jonas Schmidt, Shaily Verma</dc:creator>
    </item>
    <item>
      <title>A Quadratic Lower Bound for Stable Roommates Solvability</title>
      <link>https://arxiv.org/abs/2502.06464</link>
      <description>arXiv:2502.06464v1 Announce Type: new 
Abstract: In their seminal work on the Stable Marriage Problem (SM), Gale and Shapley introduced a generalization of SM referred to as the Stable Roommates Problem (SR). An instance of SR consists of a set of $2n$ agents, and each agent has preferences in the form of a ranked list of all other agents. The goal is to find a one-to-one matching between the agents that is stable in the sense that no pair of agents have a mutual incentive to deviate from the matching. Unlike the (bipartite) stable marriage problem, in SR, stable matchings need not exist. Irving devised an algorithm that finds a stable matching or reports that none exists in $O(n^2)$ time. In their influential 1989 text, Gusfield and Irving posed the question of whether $\Omega(n^2)$ time is required for SR solvability -- the task of determining if an SR instance admits a stable matching.
  In this paper we provide an affirmative answer to Gusfield and Irving's question. We show that any (randomized) algorithm that determines SR solvability requires $\Omega(n^2)$ adaptive Boolean queries to the agents' preferences (in expectation). Our argument follows from a reduction from the communication complexity of the set disjointness function. The query lower bound implies quadratic time lower bounds for Turing machines, and memory access lower bounds for random access machines. Thus, we establish that Irving's algorithm is optimal (up to a logarithmic factor) in a very strong sense.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06464v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Will Rosenbaum</dc:creator>
    </item>
    <item>
      <title>Symmetric Algebraic Circuits and Homomorphism Polynomials</title>
      <link>https://arxiv.org/abs/2502.06740</link>
      <description>arXiv:2502.06740v1 Announce Type: new 
Abstract: The central open question of algebraic complexity is whether VP is unequal to VNP, which is saying that the permanent cannot be represented by families of polynomial-size algebraic circuits. For symmetric algebraic circuits, this has been confirmed by Dawar and Wilsenach (2020) who showed exponential lower bounds on the size of symmetric circuits for the permanent. In this work, we set out to develop a more general symmetric algebraic complexity theory. Our main result is that a family of symmetric polynomials admits small symmetric circuits if and only if they can be written as a linear combination of homomorphism counting polynomials of graphs of bounded treewidth. We also establish a relationship between the symmetric complexity of subgraph counting polynomials and the vertex cover number of the pattern graph. As a concrete example, we examine the symmetric complexity of immanant families (a generalisation of the determinant and permanent) and show that a known conditional dichotomy due to Curticapean (2021) holds unconditionally in the symmetric setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06740v1</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anuj Dawar, Benedikt Pago, Tim Seppelt</dc:creator>
    </item>
    <item>
      <title>Equations over Finite Monoids with Infinite Promises</title>
      <link>https://arxiv.org/abs/2502.06762</link>
      <description>arXiv:2502.06762v1 Announce Type: new 
Abstract: Larrauri and \v{Z}ivn\'y recently established a complete complexity classification of the problem of solving a system of equations over a monoid $N$ assuming that a solution exists over a monoid $M$, where both monoids are finite and $M$ admits a homomorphism to $N$. Using the algebraic approach to promise constraint satisfaction problems, we extend their complexity classification in two directions: we obtain a complexity dichotomy in the case where arbitrary relations are added to the monoids, and we moreover allow the monoid $M$ to be finitely generated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06762v1</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Alberto Larrauri, Antoine Mottet, Standa \v{Z}ivn\'y</dc:creator>
    </item>
    <item>
      <title>Benchmarking of Quantum and Classical Computing in Large-Scale Dynamic Portfolio Optimization Under Market Frictions</title>
      <link>https://arxiv.org/abs/2502.05226</link>
      <description>arXiv:2502.05226v1 Announce Type: cross 
Abstract: Quantum computing is poised to transform the financial industry, yet its advantages over traditional methods have not been evidenced. As this technology rapidly evolves, benchmarking is essential to fairly evaluate and compare different computational strategies. This study presents a challenging yet solvable problem of large-scale dynamic portfolio optimization under realistic market conditions with frictions. We frame this issue as a Quadratic Unconstrained Binary Optimization (QUBO) problem, compatible with digital computing and ready for quantum computing, to establish a reliable benchmark. By applying the latest solvers to real data, we release benchmarks that help verify true advancements in dynamic trading strategies, either quantum or digital computing, ensuring that reported improvements in portfolio optimization are based on robust, transparent, and comparable metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05226v1</guid>
      <category>math.OC</category>
      <category>cs.CC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ying Chen, Thorsten Koch, Hanqiu Peng, Hongrui Zhang</dc:creator>
    </item>
    <item>
      <title>Exponential Separation Criteria for Quantum Iterative Power Algorithms</title>
      <link>https://arxiv.org/abs/2502.05506</link>
      <description>arXiv:2502.05506v1 Announce Type: cross 
Abstract: In the vast field of Quantum Optimization, Quantum Iterative Power Algorithms (QIPA) has been introduced recently with a promise of exponential speedup over an already established and well-known method, the variational Quantum Imaginary Time Evolution (varQITE) algorithm. Since the convergence and error of varQITE are known, the promise of QIPA also implied certain collapses in the complexity hierarchy - such as NP $\subseteq$ BQP, as we show in our study. However the original article of QIPA explicitly states the algorithm does not cause any collapses. In this study we prove that these collapses indeed do not occur, and with that, prove that the promised exponential separation is practically unachievable. We do so by introducing criteria for the exponential separation between QIPA that uses a double exponential function and varQITE, and then showing how these criteria require certain properties in problem instances. After that we introduce a preprocessing step that enforces problems to satisfy these criteria, and then we show that the algorithmic error blows up exponentially for these instances, as there is an inverse polynomial term between speedup and the error. Despite the theoretical results, we also show that practically relevant polynomial enhancement is still possible, and show experimental results on a small problem instance, where we used our preprocessing step to achieve the improvement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05506v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andr\'as Cz\'egel, Bogl\'arka G. -T\'oth</dc:creator>
    </item>
    <item>
      <title>Barriers and Pathways to Human-AI Alignment: A Game-Theoretic Approach</title>
      <link>https://arxiv.org/abs/2502.05934</link>
      <description>arXiv:2502.05934v1 Announce Type: cross 
Abstract: Under what conditions can capable AI agents efficiently align their actions with human preferences? More specifically, when they are proficient enough to collaborate with us, how long does coordination take, and when is it computationally feasible? These foundational questions of AI alignment help define what makes an AI agent ``sufficiently safe'' and valuable to humans. Since such generally capable systems do not yet exist, a theoretical analysis is needed to establish when guarantees hold -- and what they even are.
  We introduce a game-theoretic framework that generalizes prior alignment approaches with fewer assumptions, allowing us to analyze the computational complexity of alignment across $M$ objectives and $N$ agents, providing both upper and lower bounds. Unlike previous work, which often assumes common priors, idealized communication, or implicit tractability, our framework formally characterizes the difficulty of alignment under minimal assumptions.
  Our main result shows that even when agents are fully rational and computationally \emph{unbounded}, alignment can be achieved with high probability in time \emph{linear} in the task space size. Therefore, in real-world settings, where task spaces are often \emph{exponential} in input length, this remains impractical. More strikingly, our lower bound demonstrates that alignment is \emph{impossible} to speed up when scaling to exponentially many tasks or agents, highlighting a fundamental computational barrier to scalable alignment.
  Relaxing these idealized assumptions, we study \emph{computationally bounded} agents with noisy messages (representing obfuscated intent), showing that while alignment can still succeed with high probability, it incurs additional \emph{exponential} slowdowns in the task space size, number of agents, and number of tasks.
  We conclude by identifying conditions that make alignment more feasible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05934v1</guid>
      <category>cs.AI</category>
      <category>cs.CC</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aran Nayebi</dc:creator>
    </item>
    <item>
      <title>Provably Overwhelming Transformer Models with Designed Inputs</title>
      <link>https://arxiv.org/abs/2502.06038</link>
      <description>arXiv:2502.06038v1 Announce Type: cross 
Abstract: We develop an algorithm which, given a trained transformer model $\mathcal{M}$ as input, as well as a string of tokens $s$ of length $n_{fix}$ and an integer $n_{free}$, can generate a mathematical proof that $\mathcal{M}$ is ``overwhelmed'' by $s$, in time and space $\widetilde{O}(n_{fix}^2 + n_{free}^3)$. We say that $\mathcal{M}$ is ``overwhelmed'' by $s$ when the output of the model evaluated on this string plus any additional string $t$, $\mathcal{M}(s + t)$, is completely insensitive to the value of the string $t$ whenever length($t$) $\leq n_{free}$. Along the way, we prove a particularly strong worst-case form of ``over-squashing'', which we use to bound the model's behavior. Our technique uses computer-aided proofs to establish this type of operationally relevant guarantee about transformer models. We empirically test our algorithm on a single layer transformer complete with an attention head, layer-norm, MLP/ReLU layers, and RoPE positional encoding. We believe that this work is a stepping stone towards the difficult task of obtaining useful guarantees for trained transformer models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06038v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lev Stambler, Seyed Sajjad Nezhadi, Matthew Coudron</dc:creator>
    </item>
    <item>
      <title>Hardness of Hypergraph Edge Modification Problems</title>
      <link>https://arxiv.org/abs/2502.06045</link>
      <description>arXiv:2502.06045v1 Announce Type: cross 
Abstract: For a fixed graph $F$, let $ex_F(G)$ denote the size of the largest $F$-free subgraph of $G$. Computing or estimating $ex_F(G)$ for various pairs $F,G$ is one of the central problems in extremal combinatorics. It is thus natural to ask how hard is it to compute this function. Motivated by an old problem of Yannakakis from the 80's, Alon, Shapira and Sudakov [ASS'09] proved that for every non-bipartite graph $F$, computing $ex_F(G)$ is NP-hard. Addressing a conjecture of Ailon and Alon (2007), we prove a hypergraph analogue of this theorem, showing that for every $k \geq 3$ and every non-$k$-partite $k$-graph $F$, computing $ex_F(G)$ is NP-hard. Furthermore, we conjecture that our hardness result can be extended to all $k$-graphs $F$ other than a matching of fixed size. If true, this would give a precise characterization of the $k$-graphs $F$ for which computing $ex_F(G)$ is NP-hard, since we also prove that when $F$ is a matching of fixed size, $ex_F(G)$ is computable in polynomial time. This last result can be considered an algorithmic version of the celebrated Erd\H{o}s-Ko-Rado Theorem.
  The proof of [ASS'09] relied on a variety of tools from extremal graph theory, one of them being Tur\'an's theorem. One of the main challenges we have to overcome in order to prove our hypergraph extension is the lack of a Tur\'an-type theorem for $k$-graphs. To circumvent this, we develop a completely new graph theoretic approach for proving such hardness results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06045v1</guid>
      <category>math.CO</category>
      <category>cs.CC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lior Gishboliner, Yevgeny Levanzov, Asaf Shapira</dc:creator>
    </item>
    <item>
      <title>There is a Hyper-Greedoid lurking behind every Graphical Accessible Computational Search Problem solvable in Polynomial Time: $P \not= NP$</title>
      <link>https://arxiv.org/abs/1802.03028</link>
      <description>arXiv:1802.03028v5 Announce Type: replace 
Abstract: Consider $G[X]$, where $G$ is a connected, isthmus-less and labelled graph, and $X$ is the edge-set or the vertex-set of the graph $G$. A Graphical Search Problem (GSP), denoted $\Pi(G[X],\gamma)$, consists of finding $Y$, where $Y \subseteq X$ and $Y$ satisfies the predicate $\gamma$ in $G$. The subset $Y$ is a solution of the problem $\Pi(G[X],\gamma)$. A sub-solution of $\Pi(G[X],\gamma)$ is a subset $Y'$ such that $Y'$ is not a solution of $\Pi(G[X],\gamma)$, but $Y'$ is a solution of the problem $\Pi(H[X'],\gamma)$, where $X' \subset X$ and $H[X']$ is a contraction-minor of $G[X]$. Solutions and sub-solutions are the feasible sets of $\Pi(G[X],\gamma)$.
  Let $\mathfrak{I}$ be the family of all the feasible sets of $\Pi(G[X],\gamma)$. A Hyper-greedoid is a set system $(X, \mathfrak{I})$ satisfying the following axioms.
  A1: Accessibility: if $I \in \mathfrak{I}$, there is an element $x \in I$ such that $I-x \in \mathfrak{I}$
  A2: Augmentability: If $I$ is a sub-solution, there is a polynomial time function $\kappa: \mathfrak{I} \rightarrow \mathfrak{I}$ and there is a element $x \in X-\kappa(I)$ such that $\kappa(I) \cup x \in \mathfrak{I}$. That is, every sub-solution can be augmented using a polynomial time algorithm akin to Edmond Augmenting Path Algorithm.
  Given a graph $G$, the GSP MISP consists of finding an independent set of vertices of $G$. MISP satisfies axioms A1 and A2. Using the P-completeness of the Decision Problem associated to MISP, we prove that every GSP that satisfies A1 is solvable in Polynomial Time if and only if it satisfies A2. On the other hand, let HCP be the GSP that consists of finding a Hamiltonian cycle of the graph $G$. HCP satisfies A1, but does not satisfies A2. Since the Decision Problem associated with HCP is NP-Complete, we get $P \not = NP$.</description>
      <guid isPermaLink="false">oai:arXiv.org:1802.03028v5</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Koko-Kalambay Kalafan Kayibi</dc:creator>
    </item>
    <item>
      <title>Two-Sided Lossless Expanders in the Unbalanced Setting</title>
      <link>https://arxiv.org/abs/2409.04549</link>
      <description>arXiv:2409.04549v3 Announce Type: replace 
Abstract: We present the first explicit construction of two-sided lossless expanders in the unbalanced setting (bipartite graphs that have polynomially many more nodes on the left than on the right).
  Prior to our work, all known explicit constructions in the unbalanced setting achieved only one-sided lossless expansion.
  Specifically, we show that the one-sided lossless expanders constructed by Kalev and Ta-Shma (RANDOM'22) -- that are based on multiplicity codes introduced by Kopparty, Saraf, and Yekhanin (STOC'11) -- are, in fact, two-sided lossless expanders. Moreover, we show that our result is tight, thus completely characterizing the graph of Kalev and Ta-Shma.
  Using our unbalanced bipartite expander, we easily obtain lossless (non-bipartite) expander graphs on $N$ vertices with polynomial degree $\ll N$ and expanding sets of size $N^{0.49}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04549v3</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eshan Chattopadhyay, Mohit Gurumukhani, Noam Ringach, Yunya Zhao</dc:creator>
    </item>
    <item>
      <title>NP-Completeness and Physical Zero-Knowledge Proofs for Zeiger</title>
      <link>https://arxiv.org/abs/2409.14308</link>
      <description>arXiv:2409.14308v2 Announce Type: replace 
Abstract: Zeiger is a pencil puzzle consisting of a rectangular grid, with each cell having an arrow pointing in horizontal or vertical direction. Some cells also contain a positive integer. The objective of this puzzle is to fill a positive integer into every unnumbered cell such that the integer in each cell is equal to the number of different integers in all cells along the direction an arrow in that cell points to. In this paper, we prove that deciding solvability of a given Zeiger puzzle is NP-complete via a reduction from the not-all-equal positive 3SAT (NAE3SAT+) problem. We also construct a card-based physical zero-knowledge proof protocol for Zeiger, which enables a prover to physically show a verifier the existence of the puzzle's solution without revealing it.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14308v2</guid>
      <category>cs.CC</category>
      <category>cs.CR</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Suthee Ruangwises</dc:creator>
    </item>
    <item>
      <title>One-Way Functions and Polynomial Time Dimension</title>
      <link>https://arxiv.org/abs/2411.02392</link>
      <description>arXiv:2411.02392v3 Announce Type: replace 
Abstract: This paper demonstrates a duality between the non-robustness of polynomial time dimension and the existence of one-way functions. Polynomial-time dimension (denoted $\mathrm{cdim}_\mathrm{P}$) quantifies the density of information of infinite sequences using polynomial time betting algorithms called $s$-gales. An alternate quantification of the notion of polynomial time density of information is using polynomial-time Kolmogorov complexity rate (denoted $\mathcal{K}_\text{poly}$). Hitchcock and Vinodchandran (CCC 2004) showed that $\mathrm{cdim}_\mathrm{P}$ is always greater than or equal to $\mathcal{K}_\text{poly}$. We first show that if one-way functions exist then there exists a polynomial-time samplable distribution with respect to which $\mathrm{cdim}_\mathrm{P}$ and $\mathcal{K}_\text{poly}$ are separated by a uniform gap with probability $1$. Conversely, we show that if there exists such a polynomial-time samplable distribution, then (infinitely-often) one-way functions exist.
  Using our main results, we solve a long standing open problem posed by Hitchcock and Vinodchandran (CCC 2004) and Stull under the assumption that one-way functions exist. We demonstrate that if one-way functions exist, then there are individual sequences $X$ whose poly-time dimension strictly exceeds $\mathcal{K}_\text{poly}(X)$, that is $\mathrm{cdim}_\mathrm{P}(X) &gt; \mathcal{K}_\text{poly}(X)$. Further, we show that the gap between these quantities can be made as large as possible (i.e. close to 1). We also establish similar bounds for strong poly-time dimension versus asymptotic upper Kolmogorov complexity rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02392v3</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Satyadev Nandakumar, Subin Pulari, Akhil S, Suronjona Sarma</dc:creator>
    </item>
    <item>
      <title>Randomized Black-Box PIT for Small Depth +-Regular Non-commutative Circuits</title>
      <link>https://arxiv.org/abs/2411.06569</link>
      <description>arXiv:2411.06569v2 Announce Type: replace 
Abstract: We address the black-box polynomial identity testing (PIT) problem for non-commutative polynomials computed by $+$-regular circuits, a class of homogeneous circuits introduced by [AJMR](STOC 2017, Theory of Computing 2019). These circuits can compute polynomials with a number of monomials that are doubly exponential in the circuit size. They gave an efficient randomized PIT algorithm for +-regular circuits of depth 3 and posed the problem of developing an efficient black-box PIT for higher depths as an open problem.
  We present a randomized black-box polynomial-time algorithm for +-regular circuits of any constant depth. Specifically, our algorithm runs in $s^{O(d^2)}$ time, where $s$ and $d$ represent the size and the depth of the $+$-regular circuit, respectively. We combine several key techniques in a novel way. We employ a nondeterministic substitution automaton that transforms the polynomial into a structured form and utilizes polynomial sparsification along with commutative transformations to maintain non-zeroness. Additionally, we introduce matrix composition, coefficient modification via the automaton, and multi-entry outputs-methods that have not previously been applied in the context of black-box PIT. Together, these techniques enable us to effectively handle exponential degrees and doubly exponential sparsity in non-commutative settings, enabling polynomial identity testing for higher-depth circuits. Our work resolves an open problem from [AJMR].
  In particular, we show that if $f$ is a non-zero non-commutative polynomial in $n$ variables over the field $\mathbb{F}$, computed by a depth-$d$ $+$-regular circuit of size $s$, then $f$ cannot be a polynomial identity for the matrix algebra $\mathbb{M}_{N}(\mathbb{F})$, where $N= s^{O(d^2)}$ and the size of the field $\mathbb{F}$ depending on the degree of $f$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06569v2</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>G V Sumukha Bharadwaj, S Raja</dc:creator>
    </item>
    <item>
      <title>New Techniques for Constructing Rare-Case Hard Functions</title>
      <link>https://arxiv.org/abs/2411.09597</link>
      <description>arXiv:2411.09597v2 Announce Type: replace 
Abstract: We say that a function is rare-case hard against a given class of algorithms (the adversary) if all algorithms in the class can compute the function only on an $o(1)$-fraction of instances of size $n$ for large enough $n$. Starting from any NP-complete language, for each $\alpha &gt; 0$, we construct a function that cannot be computed correctly even on a $1/n^\alpha$-fraction of instances for polynomial-sized circuit families if NP $\not \subset$ P/POLY and by polynomial-time algorithms if NP $\not \subset$ BPP - functions that are rare-case hard against polynomial-sized circuits and polynomial-time randomized algorithms. The constructed function is a number-theoretic polynomial evaluated over specific finite fields. For NP-complete languages that admit parsimonious reductions from all of NP (for example, SAT), the constructed functions are hard to compute even on a $1/n^\alpha$-fraction of instances by polynomial-time randomized algorithms and polynomial-sized circuit families simply if P# $\not \subset$ BPP and P# $\not \subset$ P/POLY, respectively. We also show that if the Randomized Exponential Time Hypothesis (RETH) is true, none of these constructed functions can be computed even on a $1/n^\alpha$-fraction of instances in subexponential time. These functions are very hard, almost always.
  While one may not be able to efficiently compute the values of these constructed functions themselves, in polynomial time, one can verify that the evaluation of a function, $s = f(x)$, is correct simply by asking a prover to compute $f(y)$ on targeted queries.
  We have extended our work to give an alternative proof of a variant of Lipton's theorem (Lipton, 1989). We also compare our techniques for constructing rare-case hard functions with two other existing methods in the literature (Sudan et al., 2001; Feige and Lund, 1996).</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09597v2</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tejas Nareddy, Abhishek Mishra</dc:creator>
    </item>
    <item>
      <title>Improved Hardness of Approximation for Geometric Bin Packing</title>
      <link>https://arxiv.org/abs/2301.09272</link>
      <description>arXiv:2301.09272v3 Announce Type: replace-cross 
Abstract: The Geometric Bin Packing (GBP) problem is a generalization of Bin Packing where the input is a set of $d$-dimensional rectangles, and the goal is to pack them into unit $d$-dimensional cubes efficiently. It is NP-Hard to obtain a PTAS for the problem, even when $d=2$. For general $d$, the best-known approximation algorithm has an approximation guarantee exponential in $d$, while the best hardness of approximation is still a small constant inapproximability from the case when $d=2$. In this paper, we show that the problem cannot be approximated within $d^{1-\epsilon}$ factor unless NP=P.
  Recently, $d$-dimensional Vector Bin Packing, a closely related problem to the GBP, was shown to be hard to approximate within $\Omega(\log d)$ when $d$ is a fixed constant, using a notion of Packing Dimension of set families. In this paper, we introduce a geometric analog of it, the Geometric Packing Dimension of set families. While we fall short of obtaining similar inapproximability results for the Geometric Bin Packing problem when $d$ is fixed, we prove a couple of key properties of the Geometric Packing Dimension which highlight fundamental differences between Geometric Bin Packing and Vector Bin Packing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.09272v3</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.CG</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.ipl.2024.106552</arxiv:DOI>
      <arxiv:journal_reference>Information Processing Letters, Volume 189, March 2025, 106552</arxiv:journal_reference>
      <dc:creator>Arka Ray, Sai Sandeep</dc:creator>
    </item>
    <item>
      <title>Complexity of Minimal Faithful Permutation Degree for Fitting-free Groups</title>
      <link>https://arxiv.org/abs/2501.16039</link>
      <description>arXiv:2501.16039v2 Announce Type: replace-cross 
Abstract: In this paper, we investigate the complexity of computing the minimal faithful permutation degree for groups without abelian normal subgroups. When our groups are given as quotients of permutation groups, we establish that this problem is in $\textsf{P}$. Furthermore, in the setting of permutation groups, we obtain an upper bound of $\textsf{NC}$ for this problem. This improves upon the work of Das and Thakkar (STOC 2024), who established a Las Vegas polynomial-time algorithm for this class in the setting of permutation groups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16039v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>math.GR</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Levet, Pranjal Srivastava, Dhara Thakkar</dc:creator>
    </item>
  </channel>
</rss>
