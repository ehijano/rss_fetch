<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 09 Oct 2025 01:42:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Rivaling Transformers: Multi-Scale Structured State-Space Mixtures for Agentic 6G O-RAN</title>
      <link>https://arxiv.org/abs/2510.05255</link>
      <description>arXiv:2510.05255v1 Announce Type: new 
Abstract: In sixth-generation (6G) Open Radio Access Networks (O-RAN), proactive control is preferable. A key open challenge is delivering control-grade predictions within Near-Real-Time (Near-RT) latency and computational constraints under multi-timescale dynamics. We therefore cast RAN Intelligent Controller (RIC) analytics as an agentic perceive-predict xApp that turns noisy, multivariate RAN telemetry into short-horizon per-User Equipment (UE) key performance indicator (KPI) forecasts to drive anticipatory control. In this regard, Transformers are powerful for sequence learning and time-series forecasting, but they are memory-intensive, which limits Near-RT RIC use. Therefore, we need models that maintain accuracy while reducing latency and data movement. To this end, we propose a lightweight Multi-Scale Structured State-Space Mixtures (MS3M) forecaster that mixes HiPPO-LegS kernels to capture multi-timescale radio dynamics. We develop stable discrete state-space models (SSMs) via bilinear (Tustin) discretization and apply their causal impulse responses as per-feature depthwise convolutions. Squeeze-and-Excitation gating dynamically reweights KPI channels as conditions change, and a compact gated channel-mixing layer models cross-feature nonlinearities without Transformer-level cost. The model is KPI-agnostic -- Reference Signal Received Power (RSRP) serves as a canonical use case -- and is trained on sliding windows to predict the immediate next step. Empirical evaluations conducted using our bespoke O-RAN testbed KPI time-series dataset (59,441 windows across 13 KPIs). Crucially for O-RAN constraints, MS3M achieves a 0.057 s per-inference latency with 0.70M parameters, yielding 3-10x lower latency than the Transformer baselines evaluated on the same hardware, while maintaining competitive accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05255v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Farhad Rezazadeh, Hatim Chergui, Merouane Debbah, Houbing Song, Dusit Niyato, Lingjia Liu</dc:creator>
    </item>
    <item>
      <title>Impact of Packet Loss and Timing Errors on Scheduled Periodic Traffic with Time-Aware Shaping (TAS) in Time-Sensitive Networking (TSN)</title>
      <link>https://arxiv.org/abs/2510.05290</link>
      <description>arXiv:2510.05290v1 Announce Type: new 
Abstract: Time-Sensitive Networking (TSN) is a collection of mechanisms to enhance the realtime transmission capability of Ethernet networks. TSN combines priority queuing, traffic scheduling, and the Time-Aware Shaper (TAS) to carry periodic traffic with ultra-low latency and jitter. That is, so-called Talkers send periodic traffic with highest priority according to a schedule. The schedule is designed such that the scheduled traffic is forwarded by the TSN bridges with no or only little queuing delay. To protect that traffic against other frames, the TAS is configured on all interfaces such that lower-priority queues can send only when high-priority traffic is not supposed to be forwarded. In the literature on scheduling algorithms for the TAS there is mostly the explicit or implicit assumption that the TAS also limits transmission slots of high-priority traffic.
  In this paper we show that this assumption can lead to tremendous problems like very long queuing delay or even packet loss in case of faulty frames. A faulty frame arrives too early or too late according to the schedule, it is missing or additional. We construct minimal examples to illustrate basic effects of faulty frames on a single link and demonstrate how this effect can propagate through the networks and cause remote problems. We further show using simulations that a single slightly delayed frame may lead to frame loss on multiple links. We show that these problems can be alleviated or avoided when TAS-based transmission slots for high-priority traffic are configured longer than needed or if they are not limited at all.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05290v1</guid>
      <category>cs.NI</category>
      <category>cs.ET</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Manuel Eppler, Steffen Lindner, Lukas Osswald, Thomas St\"uber, Michael Menth</dc:creator>
    </item>
    <item>
      <title>Generative AI-Driven Hierarchical Multi-Agent Framework for Zero-Touch Optical Networks</title>
      <link>https://arxiv.org/abs/2510.05625</link>
      <description>arXiv:2510.05625v1 Announce Type: new 
Abstract: The rapid development of Generative Artificial Intelligence (GenAI) has catalyzed a transformative technological revolution across all walks of life. As the backbone of wideband communication, optical networks are expecting high-level autonomous operation and zero-touch management to accommodate their expanding network scales and escalating transmission bandwidth. The integration of GenAI is deemed as the pivotal solution for realizing zero-touch optical networks. However, the lifecycle management of optical networks involves a multitude of tasks and necessitates seamless collaboration across multiple layers, which poses significant challenges to the existing single-agent GenAI systems. In this paper, we propose a GenAI-driven hierarchical multi-agent framework designed to streamline multi-task autonomous execution for zero-touch optical networks. We present the architecture, implementation, and applications of this framework. A field-deployed mesh network is utilized to demonstrate three typical scenarios throughout the lifecycle of optical network: quality of transmission estimation in the planning stage, dynamic channel adding/dropping in the operation stage, and system capacity increase in the upgrade stage. The case studies, illustrate the capabilities of multi-agent framework in multi-task allocation, coordination, execution, evaluation, and summarization. This work provides a promising approach for the future development of intelligent, efficient, and collaborative network management solutions, paving the way for more specialized and adaptive zero-touch optical networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05625v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yao Zhang, Yuchen Song, Shengnan Li, Yan Shi, Shikui Shen, Xiongyan Tang, Min Zhang, Danshi Wang</dc:creator>
    </item>
    <item>
      <title>On Enhancing Delay SLAs in TCP Networks through Joint Routing and Transport Assistant Deployment</title>
      <link>https://arxiv.org/abs/2510.05686</link>
      <description>arXiv:2510.05686v1 Announce Type: new 
Abstract: The Transport Control Protocol has long been the primary transport protocol for applications requiring performance and reliability over the Internet. Unfortunately, due its retransmission mechanism, TCP incurs high packet delivery delays when segments are lost. To address this issue, previous research proposed to use a novel network function, namely Transport Assistant, deployed within the network to cache and retransmit lost packets, thus reducing retransmission delays. In this paper, we propose to jointly route the flows and deploy TAs in order to minimize packet delivery delays in best-effort networks (scenario 1) or to satisfy delay-based Service Level Agreements in QoS-based networks (scenario 2). We hence formulate the joint routing and TA deployment problem as Integer Linear Program for the two scenarios and propose a heuristic solution for large-scale instances of the problem. Through extensive simulations, we demonstrate the benefits of performing joint routing flows and TA deployment in reducing packet delivery delays (up to 16.4%) while minimizing deployment costs (up to 60.98%).</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05686v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jos\'e G\'omez-delaHiz, Mohamed Faten Zhani, Jaime Gal\'an-Jim\'enez, John Kaippallimalil</dc:creator>
    </item>
    <item>
      <title>A Deep Q-Network based power control mechanism to Minimize RLF driven Handover Failure in 5G Network</title>
      <link>https://arxiv.org/abs/2510.05762</link>
      <description>arXiv:2510.05762v1 Announce Type: new 
Abstract: The impact of Radio link failure (RLF) has been largely ignored in designing handover algorithms, although RLF is a major contributor towards causing handover failure (HF). RLF can cause HF if it is detected during an ongoing handover. The objective of this work is to propose an efficient power control mechanism based on Deep Q-Network (DQN), considering handover parameters (i.e., time-to-preparation, time-to-execute, preparation offset, execution offset) and radio link monitoring parameters (T310 and N310) as input. The proposed DRL based power control algorithm decides on a possible increase of transmitting power to avoid RLF driven HF. Simulation results show that the traditional conditional handover, when equipped with the proposed DRL based power control algorithm can significantly reduce both RLFs and subsequent HFs, as compared to the existing state of the art approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05762v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kotha Kartheek, Shankar K. Ghosh, Megha Iyengar, Vinod Sharma, Souvik Deb</dc:creator>
    </item>
    <item>
      <title>Leveraging Generative AI for large-scale prediction-based networking</title>
      <link>https://arxiv.org/abs/2510.05797</link>
      <description>arXiv:2510.05797v1 Announce Type: new 
Abstract: The traditional role of the network layer is to create an end-to-end route, through which the intermediate nodes replicate and forward the packets towards the destination. This role can be radically redefined by exploiting the power of Generative AI (GenAI) to pivot towards a prediction-based network layer, which addresses the problems of throughput limits and uncontrollable latency. In the context of real-time delivery of image content, the use of GenAI-aided network nodes has been shown to improve the flow arriving at the destination by more than 100%. However, to successfully exploit GenAI nodes and achieve such transition, we must provide solutions for the problems which arise as we scale the networks to include large amounts of users and multiple data modalities other than images. We present three directions that play a significant role in enabling the use of GenAI as a network layer tool at a large scale. In terms of design, we emphasize the need for initialization protocols to select the prompt size efficiently. Next, we consider the use case of GenAI as a tool to ensure timely delivery of data, as well as an alternative to traditional TCP congestion control algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05797v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mathias Thorsager, Israel Leyva-Mayorga, Petar Popovski</dc:creator>
    </item>
    <item>
      <title>cMPI: Using CXL Memory Sharing for MPI One-Sided and Two-Sided Inter-Node Communications</title>
      <link>https://arxiv.org/abs/2510.05476</link>
      <description>arXiv:2510.05476v1 Announce Type: cross 
Abstract: Message Passing Interface (MPI) is a foundational programming model for high-performance computing. MPI libraries traditionally employ network interconnects (e.g., Ethernet and InfiniBand) and network protocols (e.g., TCP and RoCE) with complex software stacks for cross-node communication. We present cMPI, the first work to optimize MPI point-to-point communication (both one-sided and two-sided) using CXL memory sharing on a real CXL platform, transforming cross-node communication into memory transactions and data copies within CXL memory, bypassing traditional network protocols. We analyze performance across various interconnects and find that CXL memory sharing achieves 7.2x-8.1x lower latency than TCP-based interconnects deployed in small- and medium-scale clusters. We address challenges of CXL memory sharing for MPI communication, including data object management over the dax representation [50], cache coherence, and atomic operations. Overall, cMPI outperforms TCP over standard Ethernet NIC and high-end SmartNIC by up to 49x and 72x in latency and bandwidth, respectively, for small messages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05476v1</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <category>cs.NI</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3712285.3759816</arxiv:DOI>
      <dc:creator>Xi Wang, Bin Ma, Jongryool Kim, Byungil Koh, Hoshik Kim, Dong Li</dc:creator>
    </item>
    <item>
      <title>Dynamic Scheduling in Fiber and Spaceborne Quantum Repeater Networks</title>
      <link>https://arxiv.org/abs/2510.05854</link>
      <description>arXiv:2510.05854v1 Announce Type: cross 
Abstract: The problem of scheduling in quantum networks amounts to choosing which entanglement swapping operations to perform to better serve user demand. The choice can be carried out following a variety of criteria (e.g. ensuring all users are served equally vs. prioritizing specific critical applications, adopting heuristic or optimization-based algorithms...), requiring a method to compare different solutions and choose the most appropriate. We present a framework to mathematically formulate the scheduling problem over quantum networks and benchmark general quantum scheduling policies over arbitrary lossy quantum networks. By leveraging the framework, we apply Lyapunov drift minimization to derive a novel class of quadratic optimization based scheduling policies, which we then analyze and compare with a Max Weight inspired linear class. We then give an overview of the pre-existing fiber quantum simulation tools and report on the development of numerous extensions to QuISP, an established quantum network simulator focused on scalability and accuracy in modeling the underlying classical network infrastructure. To integrate satellite links in the discussion, we derive an analytical model for the entanglement distribution rates for satellite-to-ground and ground-satellite-ground links and discuss different quantum memory allocation policies for the dual link case. Our findings show that classical communication latency is a major limiting factor for satellite communication, and the effects of physical upper bounds such as the speed of light must be taken into account when designing quantum links, limiting the attainable rates to tens of kHz. We conclude by summarizing our findings and highlighting the challenges that still need to be overcome in order to study the quantum scheduling problem over fiber and satellite quantum networks. [Abridged abstract, see PDF for full version]</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05854v1</guid>
      <category>quant-ph</category>
      <category>cs.NI</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Paolo Fittipaldi</dc:creator>
    </item>
    <item>
      <title>DRL-based Latency-Aware Network Slicing in O-RAN with Time-Varying SLAs</title>
      <link>https://arxiv.org/abs/2401.05042</link>
      <description>arXiv:2401.05042v3 Announce Type: replace 
Abstract: The Open Radio Access Network (Open RAN) paradigm, and its reference architecture proposed by the O-RAN Alliance, is paving the way toward open, interoperable, observable and truly intelligent cellular networks. Crucial to this evolution is Machine Learning (ML), which will play a pivotal role by providing the necessary tools to realize the vision of self-organizing O-RAN systems. However, to be actionable, ML algorithms need to demonstrate high reliability, effectiveness in delivering high performance, and the ability to adapt to varying network conditions, traffic demands and performance requirements. To address these challenges, in this paper we propose a novel Deep Reinforcement Learning (DRL) agent design for O-RAN applications that can learn control policies under varying Service Level Agreement (SLAs) with heterogeneous minimum performance requirements. We focus on the case of RAN slicing and SLAs specifying maximum tolerable end-to-end latency levels. We use the OpenRAN Gym open-source environment to train a DRL agent that can adapt to varying SLAs and compare it against the state-of-the-art. We show that our agent maintains a low SLA violation rate that is 8.3x and 14.4x lower than approaches based on Deep Q- Learning (DQN) and Q-Learning while consuming respectively 0.3x and 0.6x fewer resources without the need for re-training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.05042v3</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/ICNC59896.2024.10556357</arxiv:DOI>
      <arxiv:journal_reference>2024 International Conference on Computing, Networking and Communications (ICNC), Big Island, HI, USA, 2024, pp. 737-743</arxiv:journal_reference>
      <dc:creator>Raoul Raftopoulos, Salvatore D'Oro, Tommaso Melodia, Giovanni Schembra</dc:creator>
    </item>
    <item>
      <title>Implementation and Performance Evaluation of TCP over QUIC Tunnels</title>
      <link>https://arxiv.org/abs/2504.10054</link>
      <description>arXiv:2504.10054v2 Announce Type: replace 
Abstract: QUIC, a UDP-based transport protocol, addresses several limitations of TCP by offering built-in encryption, stream multiplexing, and improved loss recovery. To extend these benefits to legacy TCP-based applications, this paper explores the implementation and evaluation of a TCP over QUIC tunneling approach. A lightweight, stream-based tunnel is constructed using the Rust-based Quinn library, enabling TCP traffic to traverse QUIC connections transparently. Performance is evaluated under varying network conditions, including packet loss, high latency, and out-of-order delivery. Results indicate that TCP over QUIC maintains significantly higher throughput than native TCP in lossy or unstable environments, with up to a high improvement under 20\% packet loss. However, under ideal network conditions, tunneling introduces modest overhead due to encryption and user-space processing. These findings provide insights into the trade-offs of TCP over QUIC tunneling and its suitability for deployment in dynamic or impaired networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10054v2</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xuanhong Guo, Zekun Bao, Ying Chen</dc:creator>
    </item>
    <item>
      <title>A Unified Learning-based Optimization Framework for 0-1 Mixed Problems in Wireless Networks</title>
      <link>https://arxiv.org/abs/2509.12664</link>
      <description>arXiv:2509.12664v2 Announce Type: replace 
Abstract: Several wireless networking problems are often posed as 0-1 mixed optimization problems, which involve binary variables (e.g., selection of access points, channels, and tasks) and continuous variables (e.g., allocation of bandwidth, power, and computing resources). Traditional optimization methods as well as reinforcement learning (RL) algorithms have been widely exploited to solve these problems under different network scenarios. However, solving such problems becomes more challenging when dealing with a large network scale, multi-dimensional radio resources, and diversified service requirements. To this end, in this paper, a unified framework that combines RL and optimization theory is proposed to solve 0-1 mixed optimization problems in wireless networks. First, RL is used to capture the process of solving binary variables as a sequential decision-making task. During the decision-making steps, the binary (0-1) variables are relaxed and, then, a relaxed problem is solved to obtain a relaxed solution, which serves as prior information to guide RL searching policy. Then, at the end of decision-making process, the search policy is updated via suboptimal objective value based on decisions made. The performance bound and convergence guarantees of the proposed framework are then proven theoretically. An extension of this approach is provided to solve problems with a non-convex objective function and/or non-convex constraints. Numerical results show that the proposed approach reduces the convergence time by about 30% over B&amp;B in small-scale problems with slightly higher objective values. In large-scale scenarios, it can improve the normalized objective values by 20% over RL with a shorter convergence time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12664v2</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TCOMM.2025.3618171</arxiv:DOI>
      <dc:creator>Kairong Ma, Yao Sun, Shuheng Hua, Muhammad Ali Imran, Walid Saad</dc:creator>
    </item>
  </channel>
</rss>
