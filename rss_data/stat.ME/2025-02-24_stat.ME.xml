<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 24 Feb 2025 05:00:27 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Joint Registration and Conformal Prediction for Partially Observed Functional Data</title>
      <link>https://arxiv.org/abs/2502.15000</link>
      <description>arXiv:2502.15000v1 Announce Type: new 
Abstract: Predicting missing segments in partially observed functions is challenging due to infinite-dimensionality, complex dependence within and across observations, and irregular noise. These challenges are further exacerbated by the existence of two distinct sources of variation in functional data, termed amplitude (variation along the $y$-axis) and phase (variation along the $x$-axis). While registration can disentangle them from complete functional data, the process is more difficult for partial observations. Thus, existing methods for functional data prediction often ignore phase variation. Furthermore, they rely on strong parametric assumptions, and require either precise model specifications or computationally intensive techniques, such as bootstrapping, to construct prediction intervals. To tackle this problem, we propose a unified registration and prediction approach for partially observed functions under the conformal prediction framework, which separately focuses on the amplitude and phase components. By leveraging split conformal methods, our approach integrates registration and prediction while ensuring exchangeability through carefully constructed predictor-response pairs. Using a neighborhood smoothing algorithm, the framework produces pointwise prediction bands with finite-sample marginal coverage guarantees under weak assumptions. The method is easy to implement, computationally efficient, and suitable for parallelization. Numerical studies and real-world data examples clearly demonstrate the effectiveness and practical utility of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15000v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fangyi Wang, Sebastian Kurtek, Yuan Zhang</dc:creator>
    </item>
    <item>
      <title>Bayesian Unit-level Modeling of Categorical Survey Data with a Longitudinal Design</title>
      <link>https://arxiv.org/abs/2502.15112</link>
      <description>arXiv:2502.15112v1 Announce Type: new 
Abstract: Categorical response data are ubiquitous in complex survey applications, yet few methods model the dependence across different outcome categories when the response is ordinal. Likewise, few methods exist for the common combination of a longitudinal design and categorical data. By modeling individual survey responses at the unit-level, it is possible to capture both ordering information in ordinal responses and any longitudinal correlation. However, accounting for a complex survey design becomes more challenging in the unit-level setting. We propose a Bayesian hierarchical, unit-level, model-based approach for categorical data that is able to capture ordering among response categories, can incorporate longitudinal dependence, and accounts for the survey design. To handle computational scalability, we develop efficient Gibbs samplers with appropriate data augmentation as well as variational Bayes algorithms. Using public-use microdata from the Household Pulse Survey, we provide an analysis of an ordinal response that asks about the frequency of anxiety symptoms at the beginning of the COVID-19 pandemic. We compare both design-based and model-based estimators and demonstrate superior performance for the proposed approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15112v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Vedensky, Paul A. Parker, Scott H. Holan</dc:creator>
    </item>
    <item>
      <title>A note on defining positive definite functions</title>
      <link>https://arxiv.org/abs/2502.15146</link>
      <description>arXiv:2502.15146v1 Announce Type: new 
Abstract: A fundamental requirement in spatial statistics is that covariance functions
  are positive definite. While many positive definite functions are known for
  Euclidean spaces, their positive definiteness may not extend to non-Euclidean
  spaces. We present sufficient conditions to derive valid positive definite functions for
  spatial statistics on non-Euclidean geometries. Our approach leverages
  overlooked results due to Schoenberg (1938) to establish conditions
  under which covariance functions that are valid on Euclidean spaces remain
  valid on the domain of interest. This approach provides a more accessible and
  direct framework for spatial statisticians with diverse backgrounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15146v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas da Cunha Godoy, Marcos Oliveira Prates, Fernando Andr\'es Quintana</dc:creator>
    </item>
    <item>
      <title>On a class of binary regression models and their robust estimation</title>
      <link>https://arxiv.org/abs/2502.15220</link>
      <description>arXiv:2502.15220v1 Announce Type: new 
Abstract: A robust estimation framework for binary regression models is studied, aiming to extend traditional approaches like logistic regression models. While previous studies largely focused on logistic models, we explore a broader class of models defined by general link functions. We incorporate various loss functions to improve estimation under model misspecification. Our investigation addresses robustness against outliers and model misspecifications, leveraging divergence-based techniques such as the $\beta$-divergence and $\gamma$-divergence, which generalize the maximum likelihood approach. These divergences introduce loss functions that mitigate the influence of atypical data points while retaining Fisher consistency. We establish a theoretical property of the estimators under both correctly specified and misspecified models, analyzing their robustness through quantifying the effect of outliers in linear predictor. Furthermore, we uncover novel relationships between existing estimators and robust loss functions, identifying previously unexplored classes of robust estimators. Numerical experiments illustrate the efficacy of the proposed methods across various contamination scenarios, demonstrating their potential to enhance reliability in binary classification tasks. By providing a unified framework, this study highlights the versatility and robustness of divergence-based methods, offering insights into their practical application and theoretical underpinnings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15220v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kenichi Hayashi, Shinto Eguchi</dc:creator>
    </item>
    <item>
      <title>Max-Linear Tail Regression</title>
      <link>https://arxiv.org/abs/2502.15310</link>
      <description>arXiv:2502.15310v1 Announce Type: new 
Abstract: The relationship between a response variable and its covariates can vary significantly, especially in scenarios where covariates take on extremely high or low values. This paper introduces a max-linear tail regression model specifically designed to capture such extreme relationships. To estimate the regression coefficients within this framework, we propose a novel M-estimator based on extreme value theory. The consistency and asymptotic normality of our proposed estimator are rigorously established under mild conditions. Simulation results demonstrate that our estimation method outperforms the conditional least squares approach. We validate the practical applicability of our model through two case studies: one using financial data and the other using rainfall data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15310v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liujun Chen, Deyuan Li, Zhengjun Zhang</dc:creator>
    </item>
    <item>
      <title>Modal Clustering for Categorical Data</title>
      <link>https://arxiv.org/abs/2502.15414</link>
      <description>arXiv:2502.15414v1 Announce Type: new 
Abstract: Despite the inherent lack of a ground truth in clustering, a broad consensus is overall acknowledged in defining the concept of cluster in the continuous setting. Conversely, this remains controversial in the presence of categorical data. We propose a novel notion of cluster based on the dual concepts of high frequency and variable association. We show how the concept of high frequency aligns with the cluster notion provided by modal clustering in the continuous setting, which allows us to borrow and adapt existing operational tools to develop a novel procedure. The method is illustrated on some real data and tested via simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15414v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Noemi Corsini, Giovanna Menardi</dc:creator>
    </item>
    <item>
      <title>Covariance Regression based on Basis Expansion</title>
      <link>https://arxiv.org/abs/2502.15416</link>
      <description>arXiv:2502.15416v1 Announce Type: new 
Abstract: This paper presents a study on an $\ell_1$-penalized covariance regression method. Conventional approaches in high-dimensional covariance estimation often lack the flexibility to integrate external information. As a remedy, we adopt the regression-based covariance modeling framework and introduce a linear covariance selection model (LCSM) to encompass a broader spectrum of covariance structures when covariate information is available. Unlike existing methods, we do not assume that the true covariance matrix can be exactly represented by a linear combination of known basis matrices. Instead, we adopt additional basis matrices for a portion of the covariance patterns not captured by the given bases. To estimate high-dimensional regression coefficients, we exploit the sparsity-inducing $\ell_1$-penalization scheme. Our theoretical analyses are based on the (symmetric) matrix regression model with additive random error matrix, which allows us to establish new non-asymptotic convergence rates of the proposed covariance estimator.
  The proposed method is implemented with the coordinate descent algorithm. We conduct empirical evaluation on simulated data to complement theoretical findings and underscore the efficacy of our approach. To show a practical applicability of our method, we further apply it to the co-expression analysis of liver gene expression data where the given basis corresponds to the adjacency matrix of the co-expression network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15416v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kwan-Young Bak, Seongoh Park</dc:creator>
    </item>
    <item>
      <title>SAR models with specific spatial coefficients and heteroskedastic innovations</title>
      <link>https://arxiv.org/abs/2502.15580</link>
      <description>arXiv:2502.15580v1 Announce Type: new 
Abstract: This paper presents an innovative extension of spatial autoregressive (SAR) models, introducing spatial coefficients specific to each spatial region that evolve over time. The proposed estimation methodology covers both homoscedastic and heteroscedastic data, ensuring consistency and efficiency in the estimators of the parameters $\pmb{\rho}$ and $\pmb{\beta}$. The model is based on a robust theoretical framework, supported by the analysis of the asymptotic properties of the estimators, which reinforces its practical implementation. To facilitate its use, an algorithm has been developed in the R software, making it a standard tool for the analysis of complex spatial data. The proposed model proves to be more effective than other similar techniques, especially when modeling data with normal spatial structures and non-normal distributions, even when the residuals are not homoscedastic. Finally, the application of the model to homicide rates in the United States highlights its advantages in both statistical and social analysis, positioning it as a key tool for the analysis of spatial data in various disciplines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15580v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>N. A. Cruz, D. A. Romero, O. O. Melo</dc:creator>
    </item>
    <item>
      <title>A Population Sampling Framework for Claim Reserving in General Insurance</title>
      <link>https://arxiv.org/abs/2502.15598</link>
      <description>arXiv:2502.15598v1 Announce Type: new 
Abstract: Claim reserving in insurance has been studied through two primary frameworks: the macro-level approach, which estimates reserves at an aggregate level (e.g., Chain-Ladder), and the micro-level approach, which estimates reserves at the individual claim level Antonio and Plat (2014). These frameworks are based on fundamentally different theoretical foundations, creating a degree of incompatibility that limits the adoption of more flexible models. This paper introduces a unified statistical framework for claim reserving, grounded in population sampling theory. We show that macro- and micro-level models represent extreme yet natural cases of an augmented inverse probability weighting (AIPW) estimator. This formulation allows for a seamless integration of principles from both aggregate and individual models, enabling more accurate and flexible estimations. Moreover, this paper also addresses critical issues of sampling bias arising from partially observed claims data-an often overlooked challenge in insurance. By adapting advanced statistical methods from the sampling literature, such as double-robust estimators, weighted estimating equations, and synthetic data generation, we improve predictive accuracy and expand the tools available for actuaries. The framework is illustrated using Canadian auto insurance data, highlighting the practical benefits of the sampling-based methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15598v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sebastian Calcetero Vanegas, Andrei L. Badescu, X. Sheldon Lin</dc:creator>
    </item>
    <item>
      <title>Distributed U-net model and Image Segmentation for Lung Cancer Detection</title>
      <link>https://arxiv.org/abs/2502.14928</link>
      <description>arXiv:2502.14928v1 Announce Type: cross 
Abstract: Until now, in the wake of the COVID-19 pandemic in 2019, lung diseases, especially diseases such as lung cancer and chronic obstructive pulmonary disease (COPD), have become an urgent global health issue. In order to mitigate the goal problem, early detection and accurate diagnosis of these conditions are critical for effective treatment and improved patient outcomes. To further research and reduce the error rate of hospital diagnoses, this comprehensive study explored the potential of computer-aided design (CAD) systems, especially utilizing advanced deep learning models such as U-Net. And compared with the literature content of other authors, this study explores the capabilities of U-Net in detail, and enhances the ability to simulate CAD systems through the VGG16 algorithm. An extensive dataset consisting of lung CT images and corresponding segmentation masks, curated collaboratively by multiple academic institutions, serves as the basis for empirical validation. In this paper, the efficiency of U-Net model is evaluated rigorously and precisely under multiple hardware configurations, such as single CPU, single GPU, distributed GPU and federated learning, and the effectiveness and development of the method in the segmentation task of lung disease are demonstrated. Empirical results clearly affirm the robust performance of the U-Net model, most effectively utilizing four GPUs for distributed learning, and these results highlight the potential of U-Net-based CAD systems for accurate and timely lung disease detection and diagnosis huge potential.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14928v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.54254/2755-2721/49/20241066</arxiv:DOI>
      <dc:creator>Tianzuo Hu</dc:creator>
    </item>
    <item>
      <title>Optimal and Provable Calibration in High-Dimensional Binary Classification: Angular Calibration and Platt Scaling</title>
      <link>https://arxiv.org/abs/2502.15131</link>
      <description>arXiv:2502.15131v1 Announce Type: cross 
Abstract: We study the fundamental problem of calibrating a linear binary classifier of the form $\sigma(\hat{w}^\top x)$, where the feature vector $x$ is Gaussian, $\sigma$ is a link function, and $\hat{w}$ is an estimator of the true linear weight $w^\star$. By interpolating with a noninformative $\textit{chance classifier}$, we construct a well-calibrated predictor whose interpolation weight depends on the angle $\angle(\hat{w}, w_\star)$ between the estimator $\hat{w}$ and the true linear weight $w_\star$. We establish that this angular calibration approach is provably well-calibrated in a high-dimensional regime where the number of samples and features both diverge, at a comparable rate. The angle $\angle(\hat{w}, w_\star)$ can be consistently estimated. Furthermore, the resulting predictor is uniquely $\textit{Bregman-optimal}$, minimizing the Bregman divergence to the true label distribution within a suitable class of calibrated predictors. Our work is the first to provide a calibration strategy that satisfies both calibration and optimality properties provably in high dimensions. Additionally, we identify conditions under which a classical Platt-scaling predictor converges to our Bregman-optimal calibrated solution. Thus, Platt-scaling also inherits these desirable properties provably in high dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15131v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yufan Li, Pragya Sur</dc:creator>
    </item>
    <item>
      <title>Dimension-free bounds in high-dimensional linear regression via error-in-operator approach</title>
      <link>https://arxiv.org/abs/2502.15437</link>
      <description>arXiv:2502.15437v1 Announce Type: cross 
Abstract: We consider a problem of high-dimensional linear regression with random design. We suggest a novel approach referred to as error-in-operator which does not estimate the design covariance $\Sigma$ directly but incorporates it into empirical risk minimization. We provide an expansion of the excess prediction risk and derive non-asymptotic dimension-free bounds on the leading term and the remainder. This helps us to show that auxiliary variables do not increase the effective dimension of the problem, provided that parameters of the procedure are tuned properly. We also discuss computational aspects of our method and illustrate its performance with numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15437v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fedor Noskov, Nikita Puchkin, Vladimir Spokoiny</dc:creator>
    </item>
    <item>
      <title>Improving variable selection properties by using external data</title>
      <link>https://arxiv.org/abs/2502.15584</link>
      <description>arXiv:2502.15584v1 Announce Type: cross 
Abstract: Sparse high-dimensional signal recovery is only possible under certain conditions on the number of parameters, sample size, signal strength and underlying sparsity. We show that these mathematical limits can be pushed when one has external information that allow splitting parameters into blocks. This occurs in many applications, including data integration and transfer learning tasks. Specifically, we consider the Gaussian sequence model and linear regression, and show how block-based $\ell_0$ penalties attain model selection consistency under milder conditions than standard $\ell_0$ penalties, and they also attain faster model recovery rates. We first provide results for oracle-based $\ell_0$ penalties that have access to perfect sparsity and signal strength information, and subsequently empirical Bayes-motivated block $\ell_0$ penalties that does not require oracle information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15584v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul Rognon-Vael, David Rossell, Piotr Zwiernik</dc:creator>
    </item>
    <item>
      <title>BARTSIMP: flexible spatial covariate modeling and prediction using Bayesian additive regression trees</title>
      <link>https://arxiv.org/abs/2309.13270</link>
      <description>arXiv:2309.13270v2 Announce Type: replace 
Abstract: Prediction is a classic challenge in spatial statistics and the inclusion of spatial covariates can greatly improve predictive performance when incorporated into a model with latent spatial effects. It is desirable to develop flexible regression models that allow for nonlinearities and interactions in the covariate specification. Existing machine learning approaches that allow for spatial dependence in the residuals fail to provide reliable uncertainty estimates. In this paper, we investigate the combination of a Gaussian process spatial model with a Bayesian Additive Regression Tree (BART) model. The computational burden of the approach is reduced by combining Markov chain Monte Carlo (MCMC) with the Integrated Nested Laplace Approximation (INLA) technique. We study the performance of the method first via simulation. We then use the model to predict anthropometric responses in Kenya, with the data collected via a complex sampling design. In particular, household survey data are collected via stratified two-stage unequal probability cluster sampling, which requires special care when modeled.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.13270v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alex Ziyu Jiang, Jon Wakefield</dc:creator>
    </item>
    <item>
      <title>Asymptotically-exact selective inference for quantile regression</title>
      <link>https://arxiv.org/abs/2404.03059</link>
      <description>arXiv:2404.03059v3 Announce Type: replace 
Abstract: In modern data analysis, it is common to select a model before performing statistical inference. Selective inference tools make adjustments for the model selection process in order to ensure reliable inference post selection. In this paper, we introduce an asymptotic pivot to infer about the effects of selected variables on conditional quantile functions. Utilizing estimators from smoothed quantile regression, our proposed pivot is easy to compute and yields asymptotically-exact selective inference without making strict distributional assumptions about the response variable. At the core of our pivot is the use of external randomization variables, which allows us to utilize all available samples for both selection and inference, without partitioning the data into independent subsets or discarding samples at any step. From simulation studies, we find that: (i) the asymptotic confidence intervals based on our pivot achieve the desired coverage rates, even in cases where sample splitting fails due to insufficient sample size for inference; (ii) our intervals are consistently shorter than those produced by sample splitting across various models and signal settings. We report similar findings when we apply our approach to study risk factors for low birth weights in a publicly accessible dataset of US birth records from 2022.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03059v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yumeng Wang, Snigdha Panigrahi, Xuming He</dc:creator>
    </item>
    <item>
      <title>A Bayesian Hybrid Design with Borrowing from Historical Study</title>
      <link>https://arxiv.org/abs/2404.13177</link>
      <description>arXiv:2404.13177v3 Announce Type: replace 
Abstract: In early phase drug development of combination therapy, the primary objective is to preliminarily assess whether there is additive activity from a novel agent when combined with an established monotherapy. Due to potential feasibility issues for conducting a large randomized study, uncontrolled single-arm trials have been the mainstream approach in cancer clinical trials. However, such trials often present significant challenges in deciding whether to proceed to the next phase of development due to the lack of randomization in traditional two-arm trials. A hybrid design, leveraging data from a completed historical clinical study of the monotherapy, offers a valuable option to enhance study efficiency and improve informed decision-making. Compared to traditional single-arm designs, the hybrid design may significantly enhance power by borrowing external information, enabling a more robust assessment of activity. The primary challenge of hybrid design lies in handling information borrowing. We introduce a Bayesian dynamic power prior (DPP) framework with three components of controlling amount of dynamic borrowing. The framework offers flexible study design options with explicit interpretation of borrowing, allowing customization according to specific needs. Furthermore, the posterior distribution in the proposed framework has a closed form, offering significant advantages in computational efficiency. The proposed framework's utility is demonstrated through simulations and a case study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13177v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1002/pst.2466</arxiv:DOI>
      <arxiv:journal_reference>Pharmaceutical Statistics (2025)</arxiv:journal_reference>
      <dc:creator>Zhaohua Lu, John Toso, Girma Ayele, Philip He</dc:creator>
    </item>
    <item>
      <title>The inverse Kalman filter</title>
      <link>https://arxiv.org/abs/2407.10089</link>
      <description>arXiv:2407.10089v3 Announce Type: replace 
Abstract: We introduce the inverse Kalman filter (IKF), which enables exact matrix-vector multiplication between a covariance matrix from a dynamic linear model and any real-valued vector with linear computational cost. We integrate the IKF with the conjugate gradient algorithm, which substantially accelerates the computation of matrix inversion for a general form of covariance matrix, where other approximation approaches may not be directly applicable. We demonstrate the scalability and efficiency of the IKF approach through applications in nonparametric estimation of particle interaction functions, using both simulations and real cell trajectory data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10089v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyi Fang, Mengyang Gu</dc:creator>
    </item>
    <item>
      <title>A comparison of combined p-value functions for meta-analysis</title>
      <link>https://arxiv.org/abs/2408.08135</link>
      <description>arXiv:2408.08135v2 Announce Type: replace 
Abstract: P-value functions are modern statistical tools that unify effect estimation and hypothesis testing and can provide alternative point and interval estimates compared to standard meta-analysis methods, using any of the many $p$-value combination procedures available (Xie et al., 2011, JASA). We provide a systematic comparison of different combination procedures, both from a theoretical perspective and through simulation. We show that many prominent p-value combination methods (e.g. Fisher's method) are not invariant to the orientation of the underlying one-sided p-values. Only Edgington's method, a lesser-known combination method based on the sum of $p$-values, is orientation-invariant and still provides confidence intervals not restricted to be symmetric around the point estimate. Adjustments for heterogeneity can also be made and results from a simulation study indicate that Edgington's method can compete with more standard meta-analytic methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08135v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leonhard Held, Felix Hofmann, Samuel Pawel</dc:creator>
    </item>
    <item>
      <title>A generalized e-value feature detection method with FDR control at multiple resolutions</title>
      <link>https://arxiv.org/abs/2409.17039</link>
      <description>arXiv:2409.17039v3 Announce Type: replace 
Abstract: Multiple resolutions occur in a range number of explanatory features due to existence of domain-specific structure, which results in groups for the features. Within this context, the simultaneous detection of significant features and groups aimed at a specific response with false discovery rate (FDR) control stands as a crucial issue, such as the spatial genome-wide association studies. Existing methods typically require maintaining the same detection approach at different resolutions to achieve multilayer FDR control, which may be not efficient. For instance, it is unsuitable to apply knockoff method to detect features with high correlations, therefore, the efficiency of multilayer knockoff filter (MKF) is also not guaranteed. To tackle this problem, we introduce a novel method of derandomized flexible e-filter procedure (DFEFP) by developing generalized e-values. This method utilizes a wide variety of base detection procedures that operate effectively across various resolutions to provide stable and consistent results, while controlling the false discovery rate at multiple resolutions simultaneously. Furthermore, we investigate the statistical properties of the DFEFP, encompassing multilayer FDR control, stability guarantee, and solution correctness of algorithm. The DFEFP is initially exemplified to construct an e-value data splitting filter (eDS-filter). Subsequently, the eDS-filter in combination with the group knockoff filter (gKF) is used to develop more flexible methodology which referred to as the eDS+gKF-filter. Simulation studies demonstrate that the eDS+gKF-filter effectively controls FDR at multiple resolutions while either maintaining or enhancing power compared to MKF. The superiority of the eDS+gKF-filter is also demonstrated through the analysis of HIV mutation data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17039v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengyao Yu, Ruixing Ming, Min Xiao, Zhanfeng Wang, Bingyi Jing</dc:creator>
    </item>
    <item>
      <title>Bayesian Calibration for Prediction in a Multi-Output Transposition Context</title>
      <link>https://arxiv.org/abs/2410.00116</link>
      <description>arXiv:2410.00116v2 Announce Type: replace 
Abstract: Numerical simulations are widely used to predict the behavior of physical systems, with Bayesian approaches being particularly well suited for this purpose. However, experimental observations are necessary to calibrate certain simulator parameters for the prediction. In this work, we use a multi-output simulator to predict all its outputs, including those that have never been experimentally observed. This situation is referred to as the transposition context. To accurately quantify the discrepancy between model outputs and real data in this context, conventional methods cannot be applied, and the Bayesian calibration must be augmented by incorporating a joint model error across all outputs.To achieve this, the proposed method is to consider additional numerical input parameters within a hierarchical Bayesian model, which includes hyperparameters for the prior distribution of the calibration variables. This approach is applied on a computer code with three outputs that models the Taylor cylinder impact test with a small number of observations. The outputs are considered as the observed variables one at a time, to work with three different transposition situations. The proposed method is compared with other approaches that embed model errors to demonstrate the significance of the hierarchical formulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00116v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charlie Sire, Josselin Garnier, C\'edric Durantin, Baptiste Kerleguer, Gilles Defaux, Guillaume Perrin</dc:creator>
    </item>
    <item>
      <title>Identification and estimation for matrix time series CP-factor models</title>
      <link>https://arxiv.org/abs/2410.05634</link>
      <description>arXiv:2410.05634v2 Announce Type: replace 
Abstract: We propose a new method for identifying and estimating the CP-factor models for matrix time series. Unlike the generalized eigenanalysis-based method of Chang et al.(2023) for which the convergence rates may suffer from small eigengaps as the asymptotic theory is based on some matrix perturbation analysis, the proposed new method enjoys faster convergence rates which are free from any eigengaps. It achieves this by turning the problem into a joint diagonalization of several matrices whose elements are determined by a basis of a linear system, and by choosing the basis carefully to avoid near co-linearity (see Proposition 5 and Section 4.3 below). Furthermore, unlike Chang et al.(2023) which requires the two factor loading matrices to be full-ranked, the new method can handle rank-deficient factor loading matrices. Illustration with both simulated and real matrix time series data shows the advantages of the proposed new method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05634v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinyuan Chang, Yue Du, Guanglin Huang, Qiwei Yao</dc:creator>
    </item>
    <item>
      <title>Multiple Imputation for Nonresponse in Complex Surveys Using Design Weights and Auxiliary Margins</title>
      <link>https://arxiv.org/abs/2412.10988</link>
      <description>arXiv:2412.10988v4 Announce Type: replace 
Abstract: Survey data typically have missing values due to unit and item nonresponse. Sometimes, survey organizations know the marginal distributions of certain categorical variables in the survey. As shown in previous work, survey organizations can leverage these distributions in multiple imputation for nonignorable unit nonresponse, generating imputations that result in plausible completed-data estimates for the variables with known margins. However, this prior work does not use the design weights for unit nonrespondents; rather, it relies on a set of fabricated weights for these units. We extend this previous work to utilize the design weights for all sampled units. We illustrate the approach using simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10988v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kewei Xu, Jerome P. Reiter</dc:creator>
    </item>
    <item>
      <title>Improvements on Scalable Stochastic Bayesian Inference Methods for Multivariate Hawkes Process</title>
      <link>https://arxiv.org/abs/2309.14658</link>
      <description>arXiv:2309.14658v3 Announce Type: replace-cross 
Abstract: Multivariate Hawkes Processes (MHPs) are a class of point processes that can account for complex temporal dynamics among event sequences. In this work, we study the accuracy and computational efficiency of three classes of algorithms which, while widely used in the context of Bayesian inference, have rarely been applied in the context of MHPs: stochastic gradient expectation-maximization, stochastic gradient variational inference and stochastic gradient Langevin Monte Carlo. An important contribution of this paper is a novel approximation to the likelihood function that allows us to retain the computational advantages associated with conjugate settings while reducing approximation errors associated with the boundary effects. The comparisons are based on various simulated scenarios as well as an application to the study the risk dynamics in the Standard &amp; Poor's 500 intraday index prices among its 11 sectors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.14658v3</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alex Ziyu Jiang, Abel Rodr\'iguez</dc:creator>
    </item>
    <item>
      <title>Time-Uniform Confidence Spheres for Means of Random Vectors</title>
      <link>https://arxiv.org/abs/2311.08168</link>
      <description>arXiv:2311.08168v4 Announce Type: replace-cross 
Abstract: We study sequential mean estimation in $\mathbb{R}^d$. In particular, we derive time-uniform confidence spheres -- confidence sphere sequences (CSSs) -- which contain the mean of random vectors with high probability simultaneously across all sample sizes. Our results include a dimension-free CSS for log-concave random vectors, a dimension-free CSS for sub-Gaussian random vectors, and CSSs for sub-$\psi$ random vectors (which includes sub-gamma, sub-Poisson, and sub-exponential distributions). Many of our results are optimal. For sub-Gaussian distributions we also provide a CSS which tracks a time-varying mean, generalizing Robbins' mixture approach to the multivariate setting. Finally, we provide several CSSs for heavy-tailed random vectors (two moments only). Our bounds hold under a martingale assumption on the mean and do not require that the observations be iid. Our work is based on PAC-Bayesian theory and inspired by an approach of Catoni and Giulini.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.08168v4</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ben Chugg, Hongjian Wang, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>On the choice of the two tuning parameters for nonparametric estimation of an elliptical distribution generator</title>
      <link>https://arxiv.org/abs/2408.17087</link>
      <description>arXiv:2408.17087v2 Announce Type: replace-cross 
Abstract: Elliptical distributions are a simple and flexible class of distributions that depend on a one-dimensional function, called the density generator. In this article, we study the non-parametric estimator of this generator that was introduced by Liebscher (2005). This estimator depends on two tuning parameters: a bandwidth $h$ -- as usual in kernel smoothing -- and an additional parameter $a$ that control the behavior near the center of the distribution. We give an explicit expression for the asymptotic MSE at a point $x$, and derive explicit expressions for the optimal tuning parameters $h$ and $a$. Estimation of the derivatives of the generator is also discussed. A simulation study shows the performance of the new methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17087v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Victor Ryan, Alexis Derumigny</dc:creator>
    </item>
  </channel>
</rss>
