<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.NA</link>
    <description>math.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 21 Mar 2025 04:00:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Numerical Analysis and Dimension Splitting for A Semi-Lagrangian Discontinuous Finite Element Scheme Based on the Characteristic Galerkin Method</title>
      <link>https://arxiv.org/abs/2503.15673</link>
      <description>arXiv:2503.15673v1 Announce Type: new 
Abstract: A characteristic Galerkin-type semi-Lagrangian discontinuous Galerkin methods (CSLDG) is investigated, which directly discretizes an integral invariant model derived from the coupling of a transport equation and its dual equation. First, the existence and uniqueness of the CSLDG numerical solutions are proven, along with the stability of the numerical scheme. Subsequently, in contrast to the commonly used interpolation-based dimensional splitting schemes within the CSLDG framework, a separated-variable dimensional splitting approach based on the tensor product is proposed and applied to the two-dimensional case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15673v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhengrong Xie</dc:creator>
    </item>
    <item>
      <title>Approximation properties of neural ODEs</title>
      <link>https://arxiv.org/abs/2503.15696</link>
      <description>arXiv:2503.15696v1 Announce Type: new 
Abstract: We study the approximation properties of shallow neural networks whose activation function is defined as the flow of a neural ordinary differential equation (neural ODE) at the final time of the integration interval. We prove the universal approximation property (UAP) of such shallow neural networks in the space of continuous functions. Furthermore, we investigate the approximation properties of shallow neural networks whose parameters are required to satisfy some constraints. In particular, we constrain the Lipschitz constant of the flow of the neural ODE to increase the stability of the shallow neural network, and we restrict the norm of the weight matrices of the linear layers to one to make sure that the restricted expansivity of the flow is not compensated by the increased expansivity of the linear layers. For this setting, we prove approximation bounds that tell us the accuracy to which we can approximate a continuous function with a shallow neural network with such constraints. We prove that the UAP holds if we consider only the constraint on the Lipschitz constant of the flow or the unit norm constraint on the weight matrices of the linear layers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15696v1</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arturo De Marinis, Davide Murari, Elena Celledoni, Nicola Guglielmi, Brynjulf Owren, Francesco Tudisco</dc:creator>
    </item>
    <item>
      <title>Computation of whispering gallery modes for spherical symmetric heterogeneous Helmholtz problems with piecewise smooth refractive index</title>
      <link>https://arxiv.org/abs/2503.15765</link>
      <description>arXiv:2503.15765v1 Announce Type: new 
Abstract: In this paper, we develop a numerical method for the computation of (quasi-)resonances in spherical symmetric heterogeneous Helmholtz problems with piecewise smooth refractive index. Our focus lies in resonances very close to the real axis, which characterize the so-called whispering gallery modes. Our method involves a modal equation incorporating fundamental solutions to decoupled problems, extending the known modal equation to the case of piecewise smooth coefficients. We first establish the well-posedeness of the fundamental system, then we formulate the problem of resonances as a nonlinear eigenvalue problem, whose determinant will be the modal equation in the piecewise smooth case. In combination with the numerical approximation of the fundamental solutions using a spectral method, we derive a Newton method to solve the nonlinear modal equation with a proper scaling. We show the local convergence of the algorithm in the piecewise constant case by proving the simplicity of the roots. We confirm our approach through a series of numerical experiments in the piecewise constant and variable case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15765v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bouchra Bensiali, Stefan Sauter</dc:creator>
    </item>
    <item>
      <title>Stable quadratic generalized IsoGeometric analysis for elliptic interface problem</title>
      <link>https://arxiv.org/abs/2503.15786</link>
      <description>arXiv:2503.15786v1 Announce Type: new 
Abstract: Unfitted mesh formulations for interface problems generally adopt two distinct methodologies: (i) penalty-based approaches and (ii) explicit enrichment space techniques. While Stable Generalized Finite Element Method (SGFEM) has been rigorously established for one-dimensional and linear-element cases, the construction of optimal enrichment spaces preserving approximation-theoretic properties within isogeometric analysis (IGA) frameworks remains an open challenge. In this paper, we introduce a stable quadratic generalized isogeometric analysis (SGIGA2) for two-dimensional elliptic interface problems. The method is achieved through two key ideas: a new quasi-interpolation for the function with C0 continuous along interface and a new enrichment space with controlled condition number for the stiffness matrix. We mathematically prove that the present method has optimal convergence rates for elliptic interface problems and demonstrate its stability and robustness through numerical verification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15786v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yin Song, Wenkai Hu, Xin Li</dc:creator>
    </item>
    <item>
      <title>Fourth-order uniformly accurate integrators with long time near conservations for the nonlinear Dirac equation in the nonrelativistic regime</title>
      <link>https://arxiv.org/abs/2503.15826</link>
      <description>arXiv:2503.15826v1 Announce Type: new 
Abstract: In this paper, we propose two novel fourth-order integrators that exhibit uniformly high accuracy and long-term near conservations for solving the nonlinear Dirac equation (NLDE) in the nonrelativistic regime. In this regime, the solution of the NLDE exhibits highly oscillatory behavior in time, characterized by a wavelength of O($\varepsilon^{2}$) with a small parameter $\varepsilon&gt;0$. To ensure uniform temporal accuracy, we employ a two-scale approach in conjunction with exponential integrators, utilizing operator decomposition techniques for the NLDE. The proposed methods are rigorously proved to achieve fourth-order uniform accuracy in time for all $\varepsilon\in (0,1]$. Furthermore, we successfully incorporate symmetry into the integrator, and the long-term near conservation properties are analyzed through the modulated Fourier expansion. The proposed schemes are readily extendable to linear Dirac equations incorporating magnetic potentials, the dynamics of traveling wave solutions and the two/three-dimensional Dirac equations. The validity of all theoretical ndings and extensions is numerically substantiated through a series of numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15826v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lina Wang, Bin Wang, Jiyong Li</dc:creator>
    </item>
    <item>
      <title>A framework for efficient reduced order modelling in the Julia programming language</title>
      <link>https://arxiv.org/abs/2503.15994</link>
      <description>arXiv:2503.15994v1 Announce Type: new 
Abstract: In this paper we propose ROManifolds, a Julia-based package geared towards the numerical approximation of parameterized partial differential equations (PDEs) with a rich set of linear reduced order models (ROMs). The library favors extendibility and productivity, thanks to an expressive high level API, and the efficiency attained by the Julia just-in-time compiler. The implementation of the package is PDE agnostic, meaning that the same code can be used to solve a wide range of equations, including linear, nonlinear, single-field, multi-field, steady and unsteady problems. We highlight the main innovations of ROManifolds, we detail its implementation principles, we introduce its building blocks by providing usage examples, and we solve a fluid dynamics problem described by the Navier-Stokes equations in a 3d geometry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15994v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicholas Mueller, Santiago Badia</dc:creator>
    </item>
    <item>
      <title>Sequential Monte Carlo with Gaussian Mixture Distributions for Infinite-Dimensional Statistical Inverse Problems</title>
      <link>https://arxiv.org/abs/2503.16028</link>
      <description>arXiv:2503.16028v1 Announce Type: new 
Abstract: By formulating the inverse problem of partial differential equations (PDEs) as a statistical inference problem, the Bayesian approach provides a general framework for quantifying uncertainties. In the inverse problem of PDEs, parameters are defined on an infinite-dimensional function space, and the PDEs induce a computationally intensive likelihood function. Additionally, sparse data tends to lead to a multi-modal posterior. These features make it difficult to apply existing sequential Monte Carlo (SMC) algorithms. To overcome these difficulties, we propose new conditions for the likelihood functions, construct a Gaussian mixture based preconditioned Crank-Nicolson transition kernel, and demonstrate the universal approximation property of the infinite-dimensional Gaussian mixture probability measure. By combining these three novel tools, we propose a new SMC algorithm, named SMC-GM. For this new algorithm, we obtain a convergence theorem that allows Gaussian priors, illustrating that the sequential particle filter actually reproduces the true posterior distribution. Furthermore, the proposed new algorithm is rigorously defined on the infinite-dimensional function space, naturally exhibiting the discretization-invariant property. Numerical experiments demonstrate that the new approach has a strong ability to probe the multi-modality of the posterior, significantly reduces the computational burden, and numerically exhibits the discretization-invariant property (important for large-scale problems).</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16028v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoyu Lu, Junxiong Jia, Deyu Meng</dc:creator>
    </item>
    <item>
      <title>Compact implicit high resolution numerical method for solving transport problems with sorption isotherms</title>
      <link>https://arxiv.org/abs/2503.16110</link>
      <description>arXiv:2503.16110v1 Announce Type: new 
Abstract: This study investigates numerical methods to solve nonlinear transport problems characterized by various sorption isotherms with a focus on the Freundlich type of isotherms. We describe and compare second order accurate numerical schemes, focusing on implicit methods, to effectively model transport phenomena without stability restriction on the choice of time steps. Furthermore, a high resolution form of the method is proposed that limits a priori the second order accurate scheme towards first order accuracy to keep the values of numerical solutions in a physically acceptable range.
  Through numerical experiments, we demonstrate the effectiveness of high resolution methods in minimizing oscillations near discontinuities, thereby enhancing solution plausibility. The observed convergence rates confirm that the second order accurate schemes achieve expected accuracy for smooth solutions and that they yield significant improvements when compared with the results of the first order scheme. As the computational cost of the compact implicit method seems to be comparable to similar explicit ones with a clear profit of unconditional stability, this research provides a practical tool toward numerical simulations of nonlinear transport phenomena applicable in various fields such as contaminant transport in porous media or column liquid chromatography.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16110v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dagmar Zakova, Peter Frolkovic</dc:creator>
    </item>
    <item>
      <title>Nonnegative Biquadratic Tensors</title>
      <link>https://arxiv.org/abs/2503.16176</link>
      <description>arXiv:2503.16176v1 Announce Type: new 
Abstract: An M-eigenvalue of a nonnegative biquadratic tensor is referred to as an M$^+$-eigenvalue if it has a pair of nonnegative M-eigenvectors. If furthermore that pair of M-eigenvectors is positive, then that M$^+$-eigenvalue is called an M$^{++}$-eigenvalue. A nonnegative biquadratic tensor always has at least one M$^+$ eigenvalue, and the largest M$^+$-eigenvalue is also the largest M-eigenvalue and the M-spectral radius. In the case of an irreducible nonnegative biquadratic tensor, all the M$^+$-eigenvalues are M$^{++}$-eigenvalues. Although the M$^+$-eigenvalues of irreducible nonnegative biquadratic tensors are not unique in general, we establish a sufficient condition to ensure their uniqueness. For an irreducible nonnegative biquadratic tensor, the largest M$^+$-eigenvalue has a max-min characterization, while the smallest M$^+$-eigenvalue has a min-max characterization. A Collatz algorithm for computing the largest M$^+$-eigenvalues is proposed. Numerical results are reported.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16176v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chunfeng Cui, Liqun Qi</dc:creator>
    </item>
    <item>
      <title>An interior penalty DG method with correct and minimal averages, jumps and penalties for the miscible displacement problem of nonnegative characteristic form, and SUPG-type error estimates under low regularity, dominating Darcy velocity</title>
      <link>https://arxiv.org/abs/2503.16196</link>
      <description>arXiv:2503.16196v1 Announce Type: new 
Abstract: An interior penalty DG method is proposed for the steady-state linear partial differential equations of nonnegative characteristic form, suitable for mixed second-order elliptic-parabolic and first-order hyperbolic equations. Due to the different natures of the elliptic, parabolic, and hyperbolic equations. In the new DG method, the averages, jumps and penalties are minimal, correctly and only imposed on the diffusion-diffusion element boundaries, in addition to the well-known upwind jumps associating with the advection velocity. For the advection-dominated problem, the penalties can be further reduced only being imposed on the diffusion-dominated subset of the diffusion-diffusion element boundaries.This is based on the novel, crucial technique about the multiple partitions of the set of the interelement boundaries into a number of subsets with respect to the diffusion and to the advection and on the consistency result we have proven. The new DG method is the first DG method and the first time that the continuity and discontinuity of the solution are correctly identified and justified of the general steady-state linear partial differential equations of nonnegative characteristic form. The new DG method and its analysis are applied to the miscible displacement problem of vanishing diffusion coefficient and of low regularity, dominating Darcy flow velocity which lives in $H(\operatorname{div};\Omega)\cap \prod_{j=1}^J (H^r(D_j))^d$ for $r&lt;1$ other than the usual assumption $(W^{1,\infty}(\Omega))^d$. We prove the SUPG-type error estimates $\mathcal{O}(h^{\ell+\frac{1}{2}})$ for any element polynomial of degree $\ell\ge 1$ on generally shaped and nonconforming meshes, where the convergence order is independent of the regularity of the advection velocity. The SUPG-type error estimates obtained are new and the first time known under the low regularity of the advection velocity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16196v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhijie Du, Huoyuan Duan, Roger C E Tan, Yuanhong Wei</dc:creator>
    </item>
    <item>
      <title>3D Stochastic Geometry Model for Aerial Vehicle-Relayed Ground-Air-Satellite Connectivity</title>
      <link>https://arxiv.org/abs/2503.16202</link>
      <description>arXiv:2503.16202v1 Announce Type: new 
Abstract: Due to their flexibility, aerial vehicles (AVs), such as unmanned aerial vehicles and airships, are widely employed as relays to assist communications between massive ground users (GUs) and satellites, forming an AV-relayed ground-air-satellite solution (GASS). In GASS, the deployment of AVs is crucial to ensure overall performance from GUs to satellites. This paper develops a stochastic geometry-based analytical model for GASS under Matern hard-core point process (MHCPP) distributed AVs. The 3D distributions of AVs and GUs are modeled by considering their locations on spherical surfaces in the presence of high-altitude satellites. Accordingly, we derive an overall connectivity analytical model for GASS, which includes the average performance of AV-relayed two-hop transmissions. Extensive numerical results validate the accuracy of the connectivity model and provide essential insights for configuring AV deployments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16202v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yulei Wang, Yalin Liu, Yaru Fu, Yujie Qin, Zhongjie Li</dc:creator>
    </item>
    <item>
      <title>Instance optimal function recovery -- samples, decoders and asymptotic performance</title>
      <link>https://arxiv.org/abs/2503.16209</link>
      <description>arXiv:2503.16209v1 Announce Type: new 
Abstract: In this paper we study non-linear sampling recovery of multivariate functions using techniques from compressed sensing. In the first part of the paper we prove that square root Lasso $({\tt rLasso})$ with a particular choice of the regularization parameter $\lambda&gt;0$ as well as orthogonal matching pursuit $({\tt OMP})$ after sufficiently many iterations provide noise blind decoders which efficiently recover multivariate functions from random samples. In contrast to basis pursuit the decoders $({\tt rLasso})$ and $({\tt OMP})$ do not require any additional information on the width of the function class in $L_\infty$ and lead to instance optimal recovery guarantees. In the second part of the paper we relate the findings to linear recovery methods such as least squares $({\tt Lsqr})$ or Smolyak's algorithm $({\tt Smolyak})$ and compare the performance in a model situation, namely periodic multivariate functions with $L_p$-bounded mixed derivative will be approximated in $L_q$. The main observation is the fact, that $({\tt rLasso})$ and $({\tt OMP})$ outperform Smolyak's algorithm (sparse grids) in various situations, where $1&lt;p&lt;2\leq q&lt;\infty$. For $q=2$ they even outperform any linear method including $({\tt Lsqr})$ in combination with recently proposed subsampled random points.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16209v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Moritz Moeller, Kateryna Pozharska, Tino Ullrich</dc:creator>
    </item>
    <item>
      <title>On the convergence of split exponential integrators for semilinear parabolic problems</title>
      <link>https://arxiv.org/abs/2503.16210</link>
      <description>arXiv:2503.16210v1 Announce Type: new 
Abstract: Splitting the exponential-like $\varphi$ functions, which typically appear in exponential integrators, is attractive in many situations since it can dramatically reduce the computational cost of the procedure. However, depending on the employed splitting, this can result in order reduction. The aim of this paper is to analyze different such split approximations. We perform the analysis for semilinear problems in the abstract framework of commuting semigroups and derive error bounds that depend, in particular, on whether the vector (to which the $\varphi$ functions are applied) satisfies appropriate boundary conditions. We then present the convergence analysis for two split versions of a second-order exponential Runge--Kutta integrator in the context of analytic semigroups, and show that one suffers from order reduction while the other does not. Numerical results for semidiscretized parabolic PDEs confirm the theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16210v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marco Caliari, Fabio Cassini, Lukas Einkemmer, Alexander Ostermann</dc:creator>
    </item>
    <item>
      <title>Energy-Adaptive Riemannian Conjugate Gradient Method for Density Functional Theory</title>
      <link>https://arxiv.org/abs/2503.16225</link>
      <description>arXiv:2503.16225v1 Announce Type: new 
Abstract: This paper presents a novel Riemannian conjugate gradient method for the Kohn-Sham energy minimization problem in density functional theory (DFT), with a focus on non-metallic crystal systems. We introduce an energy-adaptive metric that preconditions the Kohn-Sham model, significantly enhancing optimization efficiency. Additionally, a carefully designed shift strategy and several algorithmic improvements make the implementation comparable in performance to highly optimized self-consistent field iterations. The energy-adaptive Riemannian conjugate gradient method has a sound mathematical foundation, including stability and convergence, offering a reliable and efficient alternative for DFT-based electronic structure calculations in computational chemistry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16225v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Peterseim, Jonas P\"uschel, Tatjana Stykel</dc:creator>
    </item>
    <item>
      <title>A Mixed-FEM approximation with uniform conservation of the exponential stability for a class of anisotropic port-Hamiltonian system and its application to LQ control</title>
      <link>https://arxiv.org/abs/2503.16388</link>
      <description>arXiv:2503.16388v1 Announce Type: new 
Abstract: In this manuscript, we present a mixed finite element discretization for a class of boundary-damped anisotropic port-Hamiltonian systems. Using a multiplier method, we demonstrate that the resulting approximation model uniformly preserves the exponential stability of the uncontrolled system, establishing a lower bound for the exponential decay rate that is independent of the mesh size. This property is illustrated through the spatial discretization of a piezoelectric beam. Furthermore, we show how the uniform preservation of exponential stability by the proposed model aids in the convergence of controllers derived from an infinite-time linear quadratic control design, in comparison to models obtained from the standard finite-element method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16388v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.DS</category>
      <category>math.OC</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luis A. Mora, Kirsten Morris</dc:creator>
    </item>
    <item>
      <title>Data-Driven Approximation of Binary-State Network Reliability Function: Algorithm Selection and Reliability Thresholds for Large-Scale Systems</title>
      <link>https://arxiv.org/abs/2503.15545</link>
      <description>arXiv:2503.15545v1 Announce Type: cross 
Abstract: Network reliability assessment is pivotal for ensuring the robustness of modern infrastructure systems, from power grids to communication networks. While exact reliability computation for binary-state networks is NP-hard, existing approximation methods face critical tradeoffs between accuracy, scalability, and data efficiency. This study evaluates 20 machine learning methods across three reliability regimes full range (0.0-1.0), high reliability (0.9-1.0), and ultra high reliability (0.99-1.0) to address these gaps. We demonstrate that large-scale networks with arc reliability larger than or equal to 0.9 exhibit near-unity system reliability, enabling computational simplifications. Further, we establish a dataset-scale-driven paradigm for algorithm selection: Artificial Neural Networks (ANN) excel with limited data, while Polynomial Regression (PR) achieves superior accuracy in data-rich environments. Our findings reveal ANN's Test-MSE of 7.24E-05 at 30,000 samples and PR's optimal performance (5.61E-05) at 40,000 samples, outperforming traditional Monte Carlo simulations. These insights provide actionable guidelines for balancing accuracy, interpretability, and computational efficiency in reliability engineering, with implications for infrastructure resilience and system optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15545v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Wei-Chang Yeh</dc:creator>
    </item>
    <item>
      <title>Mixed precision accumulation for neural network inference guided by componentwise forward error analysis</title>
      <link>https://arxiv.org/abs/2503.15568</link>
      <description>arXiv:2503.15568v1 Announce Type: cross 
Abstract: This work proposes a mathematically founded mixed precision accumulation strategy for the inference of neural networks. Our strategy is based on a new componentwise forward error analysis that explains the propagation of errors in the forward pass of neural networks. Specifically, our analysis shows that the error in each component of the output of a layer is proportional to the condition number of the inner product between the weights and the input, multiplied by the condition number of the activation function. These condition numbers can vary widely from one component to the other, thus creating a significant opportunity to introduce mixed precision: each component should be accumulated in a precision inversely proportional to the product of these condition numbers. We propose a practical algorithm that exploits this observation: it first computes all components in low precision, uses this output to estimate the condition numbers, and recomputes in higher precision only the components associated with large condition numbers. We test our algorithm on various networks and datasets and confirm experimentally that it can significantly improve the cost--accuracy tradeoff compared with uniform precision accumulation baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15568v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>El-Mehdi El Arar (TARAN), Silviu-Ioan Filip (TARAN), Theo Mary (PEQUAN), Elisa Riccietti (ENS de Lyon)</dc:creator>
    </item>
    <item>
      <title>5D free-running, reconstruction, variable projection, ADMM, VPAL</title>
      <link>https://arxiv.org/abs/2503.15711</link>
      <description>arXiv:2503.15711v1 Announce Type: cross 
Abstract: Purpose: Ferumoxytal-enhanced 5D free-running whole heart CMR provides image quality comparable to CTA, but requires hours-long reconstruction time, preventing clinical usage. This study developed a variable projection augmented Lagrangian (VPAL) method for 5D motion-resolved image reconstruction and compared it with alternating direction method of multipliers (ADMM) in five numerical simulations and 15 in-vivo pediatric data set.
  Approach: Relative error of the reconstructed images against the ground-truth images was assessed in numerical simulations. In-vivo analysis compared reconstruction time, mid-short axis (SA) blood-myocardium sharpness, left ventricular ejection fraction (LVEF), and a radiologist's image quality ratings between VPAL and ADMM. A paired t-test (p&lt;0.05) was used to determine statistical significance, while linear regression and Bland-Altman analysis for agreement assessments.
  Results: VPAL and ADMM had similar relative errors compared to the ground truth, p = 0.07. In in-vivo datasets, VPAL reduced the reconstruction time from 16.3 +/- 3.6 hours (ADMM) to 4.7 +/- 1.1 hours (VPAL), p=1e-10. Blood-myocardium border sharpness in VPAL closely correlates to ADMM , R^2 = 0.97. The LVEFs values measured by VPAL and ADMM reconstructions are largely similar, 56 +/- 6 % in ADMM and 56 +/- 6 % in VPAL, p=0.55. Both VPAL and ADMM reconstructions have good to excellent diagnostic ratings (VPAL vs. ADMM: 3.9 +/- 0.3 vs. 3.8 +/- 0.4 in 2-chamber; 3.9 +/- 0.4 vs. 3.9 +/- in 4-chamber; 3.7 +/- 0.5 vs. 3.7 +/- 0.5 in mid-SA reformatted views. Conclusion: VPAL enables faster reconstruction than ADMM while maintaining equivalent image quality for functional assessments, supporting its potential for clinical use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15711v1</guid>
      <category>physics.med-ph</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yitong Yang, Muhammad Naeem, Marly Van Assen, Jerome Yerly, Davide Piccini, Matthias Stuber, John Oshinski, Matthias Chung</dc:creator>
    </item>
    <item>
      <title>Patch-based learning of adaptive Total Variation parameter maps for blind image denoising</title>
      <link>https://arxiv.org/abs/2503.16010</link>
      <description>arXiv:2503.16010v1 Announce Type: cross 
Abstract: We consider a patch-based learning approach defined in terms of neural networks to estimate spatially adaptive regularisation parameter maps for image denoising with weighted Total Variation and test it to situations when the noise distribution is unknown. As an example, we consider situations where noise could be either Gaussian or Poisson and perform preliminary model selection by a standard binary classification network. Then, we define a patch-based approach where at each image pixel an optimal weighting between TV regularisation and the corresponding data fidelity is learned in a supervised way using reference natural image patches upon optimisation of SSIM and in a sliding window fashion. Extensive numerical results are reported for both noise models, showing significant improvement w.r.t. results obtained by means of optimal scalar regularisation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16010v1</guid>
      <category>eess.IV</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Claudio Fantasia, Luca Calatroni, Xavier Descombes, Rim Rekik</dc:creator>
    </item>
    <item>
      <title>Efficient Bayesian Computation Using Plug-and-Play Priors for Poisson Inverse Problems</title>
      <link>https://arxiv.org/abs/2503.16222</link>
      <description>arXiv:2503.16222v1 Announce Type: cross 
Abstract: This paper introduces a novel plug-and-play (PnP) Langevin sampling methodology for Bayesian inference in low-photon Poisson imaging problems, a challenging class of problems with significant applications in astronomy, medicine, and biology. PnP Langevin sampling algorithms offer a powerful framework for Bayesian image restoration, enabling accurate point estimation as well as advanced inference tasks, including uncertainty quantification and visualization analyses, and empirical Bayesian inference for automatic model parameter tuning. However, existing PnP Langevin algorithms are not well-suited for low-photon Poisson imaging due to high solution uncertainty and poor regularity properties, such as exploding gradients and non-negativity constraints. To address these challenges, we propose two strategies for extending Langevin PnP sampling to Poisson imaging models: (i) an accelerated PnP Langevin method that incorporates boundary reflections and a Poisson likelihood approximation and (ii) a mirror sampling algorithm that leverages a Riemannian geometry to handle the constraints and the poor regularity of the likelihood without approximations. The effectiveness of these approaches is demonstrated through extensive numerical experiments and comparisons with state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16222v1</guid>
      <category>stat.CO</category>
      <category>cs.CV</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Teresa Klatzer, Savvas Melidonis, Marcelo Pereyra, Konstantinos C. Zygalakis</dc:creator>
    </item>
    <item>
      <title>Near-Linear Runtime for a Classical Matrix Preconditioning Algorithm</title>
      <link>https://arxiv.org/abs/2503.16312</link>
      <description>arXiv:2503.16312v1 Announce Type: cross 
Abstract: In 1960, Osborne proposed a simple iterative algorithm for matrix balancing with outstanding numerical performance. Today, it is the default preconditioning procedure before eigenvalue computation and other linear algebra subroutines in mainstream software packages such as Python, Julia, MATLAB, EISPACK, LAPACK, and more. Despite its widespread usage, Osborne's algorithm has long resisted theoretical guarantees for its runtime: the first polynomial-time guarantees were obtained only in the past decade, and recent near-linear runtimes remain confined to variants of Osborne's algorithm with important differences that make them simpler to analyze but empirically slower. In this paper, we address this longstanding gap between theory and practice by proving that Osborne's original algorithm -- the de facto preconditioner in practice -- in fact has a near-linear runtime. This runtime guarantee (1) is optimal in the input size up to at most a single logarithm, (2) is the first runtime for Osborne's algorithm that does not dominate the runtime of downstream tasks like eigenvalue computation, and (3) improves upon the theoretical runtimes for all other variants of Osborne's algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16312v1</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xufeng Cai, Jason M. Altschuler, Jelena Diakonikolas</dc:creator>
    </item>
    <item>
      <title>Multilevel Picard approximations overcome the curse of dimensionality in the numerical approximation of general semilinear PDEs with gradient-dependent nonlinearities</title>
      <link>https://arxiv.org/abs/2311.11579</link>
      <description>arXiv:2311.11579v4 Announce Type: replace 
Abstract: Neufeld and Wu (arXiv:2310.12545) developed a multilevel Picard (MLP) algorithm which can approximately solve general semilinear parabolic PDEs with gradient-dependent nonlinearities, allowing also for coefficient functions of the corresponding PDE to be non-constant. By introducing a particular stochastic fixed-point equation (SFPE) motivated by the Feynman-Kac representation and the Bismut-Elworthy-Li formula and identifying the first and second component of the unique fixed-point of the SFPE with the unique viscosity solution of the PDE and its gradient, they proved convergence of their algorithm. However, it remained an open question whether the proposed MLP schema in arXiv:2310.12545 does not suffer from the curse of dimensionality. In this paper, we prove that the MLP algorithm in arXiv:2310.12545 indeed can overcome the curse of dimensionality, i.e. that its computational complexity only grows polynomially in the dimension $d\in \mathbb{N}$ and the reciprocal of the accuracy $\varepsilon$, under some suitable assumptions on the nonlinear part of the corresponding PDE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.11579v4</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.PR</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ariel Neufeld, Tuan Anh Nguyen, Sizhou Wu</dc:creator>
    </item>
    <item>
      <title>A fast fully discrete mixed finite element scheme for fractional viscoelastic models of wave propagation</title>
      <link>https://arxiv.org/abs/2410.01467</link>
      <description>arXiv:2410.01467v5 Announce Type: replace 
Abstract: Due to the nonlocal feature of fractional differential operators, the numerical solution to fractional partial differential equations usually requires expensive memory and computation costs. This paper develops a fast scheme for fractional viscoelastic models of wave propagation.
  We first apply the Laplace transform to convert the time-fractional constitutive equation into an integro-differential form that involves the Mittag-Leffler function as a convolution kernel. Then we construct an efficient sum-of-exponentials (SOE) approximation for the Mittag-Leffler function. We use mixed finite elements for the spatial discretization and the Newmark scheme for the temporal discretization of the second time-derivative of the displacement variable in the kinematical equation and finally obtain the fast algorithm. Compared with the traditional L1 scheme for time fractional derivative, our fast scheme reduces the memory complexity from $\mathcal O(N_sN) $ to $\mathcal O(N_sN_{exp})$ and the computation complexity from $\mathcal O(N_sN^2)$ to $\mathcal O(N_sN_{exp}N)$, where $N$ denotes the total number of temporal grid points, $N_{exp}$ the number of exponentials in SOE, and $N_s$ the complexity of memory and computation related to the spatial discretization. Numerical experiments confirm the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01467v5</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hao Yuan, Xiaoping Xie</dc:creator>
    </item>
    <item>
      <title>Two-level hybrid Schwarz preconditioners with piecewise-polynomial coarse spaces for the high-frequency Helmholtz equation</title>
      <link>https://arxiv.org/abs/2501.15976</link>
      <description>arXiv:2501.15976v2 Announce Type: replace 
Abstract: We analyse two-level hybrid Schwarz domain-decomposition GMRES preconditioners for finite-element discretisations of the Helmholtz equation with wavenumber $k$, where the coarse space consists of piecewise polynomials.
  We prove results for fixed polynomial degree (in both the fine and coarse spaces), as well as for polynomial degree increasing like $\log k$. In the latter case, we exhibit choices of fine and coarse spaces such that, modulo factors of $\log k$, the fine and coarse spaces are both pollution free (with the ratio of the coarse-space dimension to the fine-space dimension arbitrarily small), the number of degrees of freedom per subdomain is constant, and the number of GMRES iterations is constant; i.e., modulo the important question of how to efficiently solve the coarse problem, this is the (arguably) theoretically ideal situation.
  Along with the results in the companion paper [Galkowski, Spence, arXiv:2501.11060] (which cover only fixed polynomial degree), these are the first rigorous convergence results about a two-level Schwarz preconditioner applied to the high-frequency Helmholtz equation with a coarse space that does not consist of problem-adapted basis functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15976v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ivan G. Graham, Euan A. Spence</dc:creator>
    </item>
    <item>
      <title>A shape-optimization approach for inverse diffusion problems using a single boundary measurement</title>
      <link>https://arxiv.org/abs/2503.14764</link>
      <description>arXiv:2503.14764v2 Announce Type: replace 
Abstract: This paper explores the reconstruction of a space-dependent parameter in inverse diffusion problems, proposing a shape-optimization-based approach. The main objective is to recover the absorption coefficient from a single boundary measurement. While conventional gradient-based methods rely on the Fr\'{e}chet derivative of a cost functional with respect to the unknown parameter, we also utilize its shape derivative with respect to the unknown boundary interface for recovery. This non-conventional approach addresses the problem of parameter recovery from a single measurement, which represents the key innovation of this work. Numerical experiments confirm the effectiveness of the proposed method, even for intricate and non-convex boundary interfaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14764v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Manabu Machida, Hirofumi Notsu, Julius Fergy Tiongson Rabago</dc:creator>
    </item>
    <item>
      <title>A Generalized Primal-Dual Correction Method for Saddle-Point Problems with a Nonlinear Coupling Operator</title>
      <link>https://arxiv.org/abs/2308.05388</link>
      <description>arXiv:2308.05388v2 Announce Type: replace-cross 
Abstract: The saddle-point problems (SPPs) with nonlinear coupling operators frequently arise in various control systems, such as dynamic programming optimization, H-infinity control, and Lyapunov stability analysis. However, traditional primal-dual methods are constrained by fixed regularization factors. In this paper, a novel generalized primal-dual correction method (GPD-CM) is proposed to adjust the values of regularization factors dynamically. It turns out that this method can achieve the minimum theoretical lower bound of regularization factors, allowing for larger step sizes under the convergence condition being satisfied. The convergence of the GPD-CM is directly achieved through a unified variational framework. Theoretical analysis shows that the proposed method can achieve an ergodic convergence rate of $O(1/t)$. Numerical results support our theoretical analysis for an SPP with an exponential coupling operator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.05388v2</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s12555-024-0453-8</arxiv:DOI>
      <dc:creator>Sai Wang, Yi Gong</dc:creator>
    </item>
    <item>
      <title>Proposal for the Application of Fractional Operators in Polynomial Regression Models to Enhance the Determination Coefficient $R^2$ on Unseen Data</title>
      <link>https://arxiv.org/abs/2503.11749</link>
      <description>arXiv:2503.11749v2 Announce Type: replace-cross 
Abstract: Since polynomial regression models are generally quite reliable for data with a linear trend, it is important to note that, in some cases, they may encounter overfitting issues during the training phase, which could result in negative values of the coefficient of determination $R^2$ for unseen data. For this reason, this work proposes the partial implementation of fractional operators in polynomial regression models to generate a fractional regression model. The goal of this proposal is to attempt to mitigate overfitting, which could improve the value of the coefficient of determination for unseen data, compared to the polynomial model, under the assumption that this would contribute to generating predictive models with better performance. The methodology for constructing these fractional regression models is detailed, and examples applicable to both Riemann-Liouville and Caputo fractional operators are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11749v2</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Anthony Torres-Hernandez</dc:creator>
    </item>
  </channel>
</rss>
