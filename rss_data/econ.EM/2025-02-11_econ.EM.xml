<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 11 Feb 2025 05:03:41 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Point-Identifying Semiparametric Sample Selection Models with No Excluded Variable</title>
      <link>https://arxiv.org/abs/2502.05353</link>
      <description>arXiv:2502.05353v1 Announce Type: new 
Abstract: Sample selection is pervasive in applied economic studies. This paper develops semiparametric selection models that achieve point identification without relying on exclusion restrictions, an assumption long believed necessary for identification in semiparametric selection models. Our identification conditions require at least one continuously distributed covariate and certain nonlinearity in the selection process. We propose a two-step plug-in estimator that is root-n-consistent, asymptotically normal, and computationally straightforward (readily available in statistical software), allowing for heteroskedasticity. Our approach provides a middle ground between Lee (2009)'s nonparametric bounds and Honor\'e and Hu (2020)'s linear selection bounds, while ensuring point identification. Simulation evidence confirms its excellent finite-sample performance. We apply our method to estimate the racial and gender wage disparity using data from the US Current Population Survey. Our estimates tend to lie outside the Honor\'e and Hu bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05353v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dongwoo Kim, Young Jun Lee</dc:creator>
    </item>
    <item>
      <title>Grouped fixed effects regularization for binary choice models</title>
      <link>https://arxiv.org/abs/2502.06446</link>
      <description>arXiv:2502.06446v1 Announce Type: new 
Abstract: We study the application of the Grouped Fixed Effects (GFE) estimator (Bonhomme et al., ECMTA 90(2):625-643, 2022) to binary choice models for network and panel data. This approach discretizes unobserved heterogeneity via k-means clustering and performs maximum likelihood estimation, reducing the number of fixed effects in finite samples. This regularization helps analyze small/sparse networks and rare events by mitigating complete separation, which can lead to data loss. We focus on dynamic models with few state transitions and network formation models for sparse networks. The effectiveness of this method is demonstrated through simulations and real data applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06446v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Claudia Pigini, Alessandro Pionati, Francesco Valentini</dc:creator>
    </item>
    <item>
      <title>Loss Functions for Inventory Control</title>
      <link>https://arxiv.org/abs/2502.05212</link>
      <description>arXiv:2502.05212v1 Announce Type: cross 
Abstract: In this paper, we provide analytic expressions for the first-order loss function, the complementary loss function and the second-order loss function for several probability distributions. These loss functions are important functions in inventory optimization and other quantitative fields. For several reasons, which will become apparent throughout this paper, the implementation of these loss functions prefers the use of an analytic expression, only using standard probability functions. However, complete and consistent references of analytic expressions for these loss functions are lacking in literature. This paper aims to close this gap and can serve as a reference for researchers, software engineers and practitioners that are concerned with the optimization of a quantitative system. This should lead directly to easily using different probability distributions in quantitive models which is at the core of optimization. Also, this paper serves as a broad introduction to loss functions and their use in inventory control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05212v1</guid>
      <category>math.OC</category>
      <category>econ.EM</category>
      <category>math.PR</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Steven R. Pauly</dc:creator>
    </item>
    <item>
      <title>Dynamic Pricing with Adversarially-Censored Demands</title>
      <link>https://arxiv.org/abs/2502.06168</link>
      <description>arXiv:2502.06168v1 Announce Type: cross 
Abstract: We study an online dynamic pricing problem where the potential demand at each time period $t=1,2,\ldots, T$ is stochastic and dependent on the price. However, a perishable inventory is imposed at the beginning of each time $t$, censoring the potential demand if it exceeds the inventory level. To address this problem, we introduce a pricing algorithm based on the optimistic estimates of derivatives. We show that our algorithm achieves $\tilde{O}(\sqrt{T})$ optimal regret even with adversarial inventory series. Our findings advance the state-of-the-art in online decision-making problems with censored feedback, offering a theoretically optimal solution against adversarial observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06168v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.OC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianyu Xu, Yining Wang, Xi Chen, Yu-Xiang Wang</dc:creator>
    </item>
    <item>
      <title>Comment on "Generic machine learning inference on heterogeneous treatment effects in randomized experiments."</title>
      <link>https://arxiv.org/abs/2502.06758</link>
      <description>arXiv:2502.06758v1 Announce Type: cross 
Abstract: We analyze the split-sample robust inference (SSRI) methodology proposed by Chernozhukov, Demirer, Duflo, and Fernandez-Val (CDDF) for quantifying uncertainty in heterogeneous treatment effect estimation. While SSRI effectively accounts for randomness in data splitting, its computational cost can be prohibitive when combined with complex machine learning (ML) models. We present an alternative randomization inference (RI) approach that maintains SSRI's generality without requiring repeated data splitting. By leveraging cross-fitting and design-based inference, RI achieves valid confidence intervals while significantly reducing computational burden. We compare the two methods through simulation, demonstrating that RI retains statistical efficiency while being more practical for large-scale applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06758v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kosuke Imai, Michael Lingzhi Li</dc:creator>
    </item>
    <item>
      <title>Pairwise Valid Instruments</title>
      <link>https://arxiv.org/abs/2203.08050</link>
      <description>arXiv:2203.08050v5 Announce Type: replace 
Abstract: Finding valid instruments is difficult. We propose Validity Set Instrumental Variable (VSIV) estimation, a method for estimating local average treatment effects (LATEs) in heterogeneous causal effect models when the instruments are partially invalid. We consider settings with pairwise valid instruments, that is, instruments that are valid for a subset of instrument value pairs. VSIV estimation exploits testable implications of instrument validity to remove invalid pairs and provides estimates of the LATEs for all remaining pairs, which can be aggregated into a single parameter of interest using researcher-specified weights. We show that the proposed VSIV estimators are asymptotically normal under weak conditions and remove or reduce the asymptotic bias relative to standard LATE estimators (that is, LATE estimators that do not use testable implications to remove invalid variation). We evaluate the finite sample properties of VSIV estimation in application-based simulations and apply our method to estimate the returns to college education using parental education as an instrument.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.08050v5</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenting Sun, Kaspar W\"uthrich</dc:creator>
    </item>
    <item>
      <title>Assessing Heterogeneity of Treatment Effects</title>
      <link>https://arxiv.org/abs/2306.15048</link>
      <description>arXiv:2306.15048v2 Announce Type: replace 
Abstract: Treatment effect heterogeneity is of major interest in economics, but its assessment is often hindered by the fundamental lack of identification of the individual treatment effects. For example, we may want to assess the effect of a poverty reduction measure at different levels of poverty, but the causal effects on wealth at different wealth levels are not identified. Or, we may be interested in the proportion of workers who benefit from the minimum wage increase, but the proportion is not identified in the absence of counterfactuals. This paper derives bounds useful in such situations, which only depend on the marginal distributions of the outcomes. The bounds are nonparametrically sharp, making clear the maximum extent to which the data can speak about the heterogeneity of the treatment effects. An application to microfinance shows that the bounds can be informative even when the average treatment effects are not significant. Another application to the welfare reform identifies a nonnegligible portion of workers who increased and decreased working hours due to the reform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15048v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tetsuya Kaji, Jianfei Cao</dc:creator>
    </item>
    <item>
      <title>Kernel Three Pass Regression Filter</title>
      <link>https://arxiv.org/abs/2405.07292</link>
      <description>arXiv:2405.07292v3 Announce Type: replace 
Abstract: We forecast a single time series using a high-dimensional set of predictors. When these predictors share common underlying dynamics, an approximate latent factor model provides a powerful characterization of their co-movements Bai(2003). These latent factors succinctly summarize the data and can also be used for prediction, alleviating the curse of dimensionality in high-dimensional prediction exercises, see Stock &amp; Watson (2002a). However, forecasting using these latent factors suffers from two potential drawbacks. First, not all pervasive factors among the set of predictors may be relevant, and using all of them can lead to inefficient forecasts. The second shortcoming is the assumption of linear dependence of predictors on the underlying factors. The first issue can be addressed by using some form of supervision, which leads to the omission of irrelevant information. One example is the three-pass regression filter proposed by Kelly &amp; Pruitt (2015). We extend their framework to cases where the form of dependence might be nonlinear by developing a new estimator, which we refer to as the Kernel Three-Pass Regression Filter (K3PRF). This alleviates the aforementioned second shortcoming. The estimator is computationally efficient and performs well empirically. The short-term performance matches or exceeds that of established models, while the long-term performance shows significant improvement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07292v3</guid>
      <category>econ.EM</category>
      <category>q-fin.ST</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rajveer Jat, Daanish Padha</dc:creator>
    </item>
    <item>
      <title>Estimating Nonseparable Selection Models: A Functional Contraction Approach</title>
      <link>https://arxiv.org/abs/2411.01799</link>
      <description>arXiv:2411.01799v2 Announce Type: replace 
Abstract: We propose a novel method for estimating nonseparable selection models. We show that, given the selection rule and the observed selected outcome distribution, the potential outcome distribution can be characterized as the fixed point of an operator, which we prove to be a functional contraction. We propose a two-step semiparametric maximum likelihood estimator to estimate the selection model and the potential outcome distribution. The consistency and asymptotic normality of the estimator are established. Our approach performs well in Monte Carlo simulations and is applicable in a variety of empirical settings where only a selected sample of outcomes is observed. Examples include consumer demand models with only transaction prices, auctions with incomplete bid data, and Roy models with data on accepted wages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01799v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fan Wu, Yi Xin</dc:creator>
    </item>
    <item>
      <title>Lee Bounds with a Continuous Treatment in Sample Selection</title>
      <link>https://arxiv.org/abs/2411.04312</link>
      <description>arXiv:2411.04312v3 Announce Type: replace 
Abstract: Sample selection bias arises in causal inference when a treatment affects both the outcome and the researcher's ability to observe it. This paper generalizes the sharp bounds in Lee (2009) for the average treatment effect of a binary treatment to a continuous/multivalued treatment. We revisit the Imbens, Rubin, and Sacerdote (2001) lottery data to study the effect of the prize on earnings that are only observed for the employed and the survey respondents. We evaluate the Job Crops program to study the effect of training hours on wages. To identify the average treatment effect of always-takers who are selected into samples with observed outcomes regardless of the treatment value they receive, we assume that if a subject is selected at some sufficient treatment values, then it remains selected at all treatment values. For example, if program participants are employed with one week of training, then they remain employed with any training hours. This sufficient treatment values assumption includes the monotone assumption on the treatment effect on selection as a special case. We further allow the conditional independence assumption and subjects with different pretreatment covariates to have different sufficient treatment values. The practical estimation and inference theory utilize the orthogonal moment function and cross-fitting for double debiased machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04312v3</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ying-Ying Lee, Chu-An Liu</dc:creator>
    </item>
    <item>
      <title>A Kernel Score Perspective on Forecast Disagreement and the Linear Pool</title>
      <link>https://arxiv.org/abs/2412.09430</link>
      <description>arXiv:2412.09430v2 Announce Type: replace 
Abstract: The variance of a linearly combined forecast distribution (or linear pool) consists of two components: The average variance of the component distributions (`average uncertainty'), and the average squared difference between the components' means and the pool's mean (`disagreement'). This paper shows that similar decompositions hold for a class of uncertainty measures that can be constructed as entropy functions of kernel scores. The latter are a rich family of scoring rules that covers point and distribution forecasts for univariate and multivariate, discrete and continuous settings. We further show that the disagreement term is useful for understanding the ex-post performance of the linear pool (as compared to the component distributions), and motivates using the linear pool instead of other forecast combination techniques. From a practical perspective, the results in this paper suggest principled measures of forecast disagreement in a wide range of applied settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09430v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabian Kr\"uger</dc:creator>
    </item>
    <item>
      <title>Deep self-consistent learning of local volatility</title>
      <link>https://arxiv.org/abs/2201.07880</link>
      <description>arXiv:2201.07880v3 Announce Type: replace-cross 
Abstract: We present an algorithm for the calibration of local volatility from market option prices through deep self-consistent learning, by approximating both market option prices and local volatility using deep neural networks. Our method uses the initial-boundary value problem of the underlying Dupire's partial differential equation solved by the parameterized option prices to bring corrections to the parameterization in a self-consistent way. By exploiting the differentiability of neural networks, we can evaluate Dupire's equation locally at each strike-maturity pair; while by exploiting their continuity, we sample strike-maturity pairs uniformly from a given domain, going beyond the discrete points where the options are quoted. Moreover, the absence of arbitrage opportunities are imposed by penalizing an associated loss function as a soft constraint. For comparison with existing approaches, the proposed method is tested on both synthetic and market option prices, which shows an improved performance in terms of reduced interpolation and reprice errors, as well as the smoothness of the calibrated local volatility. An ablation study has been performed, asserting the robustness and significance of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.07880v3</guid>
      <category>q-fin.CP</category>
      <category>econ.EM</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zhe Wang, Ameir Shaa, Nicolas Privault, Claude Guet</dc:creator>
    </item>
    <item>
      <title>Measuring the Quality of Answers in Political Q&amp;As with Large Language Models</title>
      <link>https://arxiv.org/abs/2404.08816</link>
      <description>arXiv:2404.08816v4 Announce Type: replace-cross 
Abstract: This article proposes a new approach for assessing the quality of answers in political question-and-answer sessions. Our methodology consists of measuring the quality of an answer based on how easily and accurately it can be recognized in a random set of candidate answers given the question's text. This measure reflects the answer's relevance and depth of engagement with the question. Like semantic search, this approach can be implemented by training a language model on the corpus of observed questions and answers without additional human-labeled data. We showcase and validate our methodology within the context of the Question Period in the Canadian House of Commons. Our analysis reveals that while some answers have a weak semantic connection to questions, suggesting some evasion or obfuscation, answers are generally at least moderately relevant, far surpassing what would be expected from random replies. Our analysis also provides valuable insights into the correlates of answer quality: we find significant correlations with the party affiliation of the members of Parliament asking the questions and the topic of the questions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08816v4</guid>
      <category>cs.CL</category>
      <category>econ.EM</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>R. Michael Alvarez, Jacob Morrier</dc:creator>
    </item>
  </channel>
</rss>
