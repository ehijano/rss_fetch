<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 30 Dec 2025 05:00:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Study of NP-Completeness and Undecidable Word Problems in Semigroups</title>
      <link>https://arxiv.org/abs/2512.22123</link>
      <description>arXiv:2512.22123v1 Announce Type: new 
Abstract: In this paper we explore fundamental concepts in computational complexity theory and the boundaries of algorithmic decidability. We examine the relationship between complexity classes \textbf{P} and \textbf{NP}, where $L \in \textbf{P}$ implies the existence of a deterministic Turing machine solving $L$ in polynomial time $O(n^k)$. Central to our investigation is polynomial reducibility. Also, we demonstrate the existence of an associative calculus $A(\mathfrak{T})$ with an algorithmically undecidable word problem, where for a Turing machine $\mathfrak{T}$ computing a non-recursive function $E(x)$, we establish that $q_1 01^x v \equiv q_0 01^i v \Leftrightarrow x \in M_i$ for $i \in \{0,1\}$, where $M_i = \{x \mid E(x) = i\}$. This connection between computational complexity and algebraic undecidability illuminates the fundamental limits of algorithmic solutions in mathematics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.22123v1</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Duaa Abdullah, Jasem Hamoud</dc:creator>
    </item>
    <item>
      <title>Lower bounds on pure dynamic programming for connectivity problems on graphs of bounded path-width</title>
      <link>https://arxiv.org/abs/2512.23121</link>
      <description>arXiv:2512.23121v1 Announce Type: new 
Abstract: We give unconditional parameterized complexity lower bounds on pure dynamic programming algorithms - as modeled by tropical circuits - for connectivity problems such as the Traveling Salesperson Problem. Our lower bounds are higher than the currently fastest algorithms that rely on algebra and give evidence that these algebraic aspects are unavoidable for competitive worst case running times. Specifically, we study input graphs with a small width parameter such as treewidth and pathwidth and show that for any $k$ there exists a graph $G$ of pathwidth at most $k$ and $k^{O(1)}$ vertices such that any tropical circuit calculating the optimal value of a Traveling Salesperson round tour uses at least $2^{\Omega(k \log \log k)}$ gates. We establish this result by linking tropical circuit complexity to the nondeterministic communication complexity of specific compatibility matrices. These matrices encode whether two partial solutions combine into a full solution, and Raz and Spieker [Combinatorica 1995] previously proved a lower bound for this complexity measure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23121v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kacper Kluk, Jesper Nederlof</dc:creator>
    </item>
    <item>
      <title>Coloring Hardness on Low Twin-Width Graphs</title>
      <link>https://arxiv.org/abs/2512.23680</link>
      <description>arXiv:2512.23680v1 Announce Type: new 
Abstract: As the class $\mathcal T_4$ of graphs of twin-width at most 4 contains every finite subgraph of the infinite grid and every graph obtained by subdividing each edge of an $n$-vertex graph at least $2 \log n$ times, most NP-hard graph problems, like Max Independent Set, Dominating Set, Hamiltonian Cycle, remain so on $\mathcal T_4$. However, Min Coloring and k-Coloring are easy on both families because they are 2-colorable and 3-colorable, respectively.
  We show that Min Coloring is NP-hard on the class $\mathcal T_3$ of graphs of twin-width at most 3. This is the first hardness result on $\mathcal T_3$ for a problem that is easy on cographs (twin-width 0), on trees (whose twin-width is at most 2), and on unit circular-arc graphs (whose twin-width is at most 3). We also show that for every $k \geqslant 3$, k-Coloring is NP-hard on $\mathcal T_4$. We finally make two observations: (1) there are currently very few problems known to be in P on $\mathcal T_d$ (graphs of twin-width at most $d$) and NP-hard on $\mathcal T_{d+1}$ for some nonnegative integer $d$, and (2) unlike $\mathcal T_4$, which contains every graph as an induced minor, the class $\mathcal T_3$ excludes a fixed planar graph as an induced minor; thus it may be viewed as a special case (or potential counterexample) for conjectures about classes excluding a (planar) induced minor. These observations are accompanied by several open questions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23680v1</guid>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>\'Edouard Bonnet</dc:creator>
    </item>
    <item>
      <title>Pseudodeterministic Algorithms for Minimum Cut Problems</title>
      <link>https://arxiv.org/abs/2512.23468</link>
      <description>arXiv:2512.23468v1 Announce Type: cross 
Abstract: In this paper, we present efficient pseudodeterministic algorithms for both the global minimum cut and minimum s-t cut problems. The running time of our algorithm for the global minimum cut problem is asymptotically better than the fastest sequential deterministic global minimum cut algorithm (Henzinger, Li, Rao, Wang; SODA 2024).
  Furthermore, we implement our algorithm in sequential, streaming, PRAM, and cut-query models, where no efficient deterministic global minimum cut algorithms are known.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23468v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aryan Agarwala, Nithin Varma</dc:creator>
    </item>
    <item>
      <title>Positive Univariate Polynomials: SOS certificates, algorithms, bit complexity, and T-systems</title>
      <link>https://arxiv.org/abs/2510.01701</link>
      <description>arXiv:2510.01701v3 Announce Type: replace 
Abstract: We consider certificates of positivity for univariate polynomials with rational coefficients that are positive over (an interval of)~$\mathbb{R}$. Such certificates take the form of weighted sums of squares (SOS) of polynomials with rational coefficients. We build on the algorithm of Chevillard, Harrison, Jolde{\c{s}}, and Lauter~\cite{chml-usos-alg-11}, and we introduce a variant that we refer to as \usos. Given a polynomial of degree~$d$ with maximum coefficient bitsize~$\tau$, we show that \usos computes a rational weighted SOS representation in $\widetilde{\mathcal{O}}_B(d^3 + d^2 \tau)$ bit operations; the resulting certificate of posivitity involves rationals of bitsize $\widetilde{\mathcal{O}}(d^2 \tau)$. This improves the best-known complexity bounds by a factor of~$d$ and completes previous analyses. We also extend these results to certificates of positivity over arbitrary rational intervals, via a simple transformation. In this case as well, our techniques yield a factor-$d$ improvement in the complexity bounds. Along the same line, for univariate polynomials with rational coefficients, we introduce a new class of certificates, which we call \emph{perturbed SOS certificates}. They consist of a sum of two rational squares that approximates the input polynomial closely enough so that nonnegativity of the approximation implies the nonnegativity of the original polynomial. This computation has the same bit complexity and yields certificates of the same bitsize as in the weighted SOS case. We further investigate structural properties of these SOS decompositions. Relying on the classical result that any nonnegative univariate real polynomial is the sum of two squares of real polynomials, we show that the summands form an interlacing pair. Consequently, their real roots correspond to the Karlin points of the original polynomial on~$\mathbb{R}$, establishing a new connection with the T-systems studied by Karlin~\cite{Karlin-repr-pos-63}. This connection enables us to compute such decompositions explicitly. Previously, only existential results were known for T-systems. We obtain analogous results for positivity over $(0, \infty)$, and hence over arbitrary real intervals. Finally, we present our open-source Maple implementation of the \usos algorithm, together with experiments on various data sets demonstrating the efficiency of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01701v3</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mat\'ias Bender (TROPICAL), Philipp Di Dio (OURAGAN), Elias Tsigaridas (OURAGAN)</dc:creator>
    </item>
    <item>
      <title>Graded Projection Recursion (GPR): A Framework for Controlling Bit-Complexity of Algebraic Packing</title>
      <link>https://arxiv.org/abs/2511.11988</link>
      <description>arXiv:2511.11988v2 Announce Type: replace 
Abstract: We present Graded Projection Recursion (GPR), a framework for converting certain blocked recursive algorithms into model-honest (i.e., reflects full bit complexity) near-quadratic procedures under bounded intermediate budgets. Part I gives a proof-complete {\em integral specification} of a three-band packing identity and a two-round middle-band extractor, and shows how per-node centering and sqrt-free dyadic l2 normalization (recursive invariant amplification) gives a sufficient packing base that grows linearly with the scaling depth (i.e., logarithmic bit-growth) in exact arithmetic.
  On fixed-width hardware, exact evaluation of the packed recursion generally requires either an extended-precision path or digit-band (slice) staging} that emulates b(n) bits of mantissa precision using w-bit words, incurring only polylogarithmic overhead; This leads to a soft-quadratic bit cost O(n^2) when b(n)=\Theta(\log n) in the basic 2x2 recursion). Part II introduces execution-format comparators (e.g., IEEE-754), a drift ledger, and a decision-invariance theorem that supports commensurate-accuracy claims in floating arithmetic (and that cleanly accounts for any staged/truncated auxiliary drift). Part III provides case-study reductions (LUP/solve/det/inv, LDL^T, blocked QR, SOI/SPD functions, GSEVP, dense LP/SDP IPM kernels, Gaussian process regression, and representative semiring problems) showing how to export the kernel advantage without reintroducing uncontrolled intermediate growth. Part IV abstracts admissible packings and extractors via a master condition and an easily checkable BWBM sufficient condition, and sketches extensions to multilinear/multigraded kernels and non-rounding extractors (e.g., CRT and semiring bucket projections).</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11988v2</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeffrey Uhlmann</dc:creator>
    </item>
    <item>
      <title>On the Incompressibility of Truth With Application to Circuit Complexity</title>
      <link>https://arxiv.org/abs/2511.21738</link>
      <description>arXiv:2511.21738v4 Announce Type: replace 
Abstract: We revisit the fundamentals of Circuit Complexity and the nature of efficient computation from a fresh perspective. We present a framework for understanding Circuit Complexity through the lens of Information Theory with analogies to results in Kolmogorov Complexity, viewing circuits as descriptions of truth tables, encoded in logical gates and wires, rather than purely computational devices. From this framework, we re-prove some existing Circuit Complexity bounds, explain what the optimal circuits for most boolean functions look like structurally, give an explicit boolean function family that requires exponential circuits, and explain the aforementioned results in a unifying intuition that re-frames time entirely.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21738v4</guid>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luke Tonon</dc:creator>
    </item>
    <item>
      <title>Toward P vs NP: An Observer-Theoretic Separation via SPDP Rank and a ZFC-Equivalent Foundation within the N-Frame Model</title>
      <link>https://arxiv.org/abs/2512.11820</link>
      <description>arXiv:2512.11820v4 Announce Type: replace 
Abstract: We present a self-contained separation framework for P vs NP developed entirely within ZFC. The approach consists of: (i) a deterministic, radius-1 compilation from uniform polynomial-time Turing computation to local sum-of-squares (SoS) polynomials with polylogarithmic contextual entanglement width (CEW); (ii) a formal Width-to-Rank upper bound for the resulting SPDP matrices at matching parameters; (iii) an NP-side identity-minor lower bound in the same encoding; and (iv) a rank-monotone, instance-uniform extraction map from the compiled P-side polynomials to the NP family. Together these yield a contradiction under the assumption P = NP, establishing a separation.
  We develop a correspondence between CEW, viewed as a quantitative measure of computational contextuality, and SPDP rank, yielding a unified criterion for complexity separation. We prove that bounded-CEW observers correspond to polynomial-rank computations (the class P), while unbounded CEW characterizes the class NP. This implies that exponential SPDP rank for #3SAT and related hard families forces P != NP within the standard framework of complexity theory.
  Key technical components include: (1) constructive lower bounds on SPDP rank via Ramanujan-Tseitin expander families; (2) a non-circular reduction from Turing-machine computation to low-rank polynomial evaluation; (3) a codimension-collapse lemma ensuring that rank amplification cannot occur within polynomial resources; and (4) proofs of barrier immunity against relativization, natural proofs, and algebrization. The result is a complete ZFC proof architecture whose primitives and compositions are fully derived, with community verification and machine-checked formalization left as future work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11820v4</guid>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Darren J. Edwards</dc:creator>
    </item>
    <item>
      <title>Exponential Lower Bounds for Locally Decodable and Correctable Codes for Insertions and Deletions</title>
      <link>https://arxiv.org/abs/2111.01060</link>
      <description>arXiv:2111.01060v3 Announce Type: replace-cross 
Abstract: Locally Decodable Codes (LDCs) are error-correcting codes for which individual message symbols can be quickly recovered despite errors in the codeword. LDCs for Hamming errors have been studied extensively in the past few decades, where a major goal is to understand the amount of redundancy that is necessary and sufficient to decode from large amounts of error, with small query complexity.
  In this work, we study LDCs for insertion and deletion errors, called Insdel LDCs. Their study was initiated by Ostrovsky and Paskin-Cherniavsky (Information Theoretic Security, 2015), who gave a reduction from Hamming LDCs to Insdel LDCs with a small blowup in the code parameters. On the other hand, the only known lower bounds for Insdel LDCs come from those for Hamming LDCs, thus there is no separation between them. Here we prove new, strong lower bounds for the existence of Insdel LDCs. In particular, we show that $2$-query linear Insdel LDCs do not exist, and give an exponential lower bound for the length of all $q$-query Insdel LDCs with constant $q$. For $q \ge 3$ our bounds are exponential in the existing lower bounds for Hamming LDCs. Furthermore, our exponential lower bounds continue to hold for adaptive decoders, and even in private-key settings where the encoder and decoder share secret randomness. This exhibits a strict separation between Hamming LDCs and Insdel LDCs.
  Our strong lower bounds also hold for the related notion of Insdel LCCs (except in the private-key setting), due to an analogue to the Insdel notions of a reduction from Hamming LCCs to LDCs.
  Our techniques are based on a delicate design and analysis of hard distributions of insertion and deletion errors, which depart significantly from typical techniques used in analyzing Hamming LDCs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.01060v3</guid>
      <category>cs.IT</category>
      <category>cs.CC</category>
      <category>math.IT</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeremiah Blocki, Kuan Cheng, Elena Grigorescu, Xin Li, Yu Zheng, Minshen Zhu</dc:creator>
    </item>
    <item>
      <title>Computing Balanced Solutions for Large International Kidney Exchange Schemes When Cycle Length Is Unbounded</title>
      <link>https://arxiv.org/abs/2312.16653</link>
      <description>arXiv:2312.16653v3 Announce Type: replace-cross 
Abstract: In kidney exchange programmes (KEP) patients may swap their incompatible donors leading to cycles of kidney transplants. Nowadays, countries try to merge their national patient-donor pools leading to international KEPs (IKEPs). As shown in the literature, long-term stability of an IKEP can be achieved through a credit-based system. In each round, every country is prescribed a "fair" initial allocation of kidney transplants. The initial allocation, which we obtain by using solution concepts from cooperative game theory, is adjusted by incorporating credits from the previous round, yielding the target allocation. The goal is to find, in each round, an optimal solution that closely approximates this target allocation. There is a known polynomial-time algorithm for finding an optimal solution that lexicographically minimizes the country deviations from the target allocation if only $2$-cycles (matchings) are permitted. In practice, kidney swaps along longer cycles may be performed. However, the problem of computing optimal solutions for maximum cycle length $\ell$ is NP-hard for every $\ell\geq 3$. This situation changes back to polynomial time once we allow unbounded cycle length. However, in contrast to the case where $\ell=2$, we show that for $\ell=\infty$, lexicographical minimization is only polynomial-time solvable under additional conditions (assuming P $\neq$ NP). Nevertheless, the fact that the optimal solutions themselves can be computed in polynomial time if $\ell=\infty$ still enables us to perform a large scale experimental study for showing how stability and total social welfare are affected when we set $\ell=\infty$ instead of $\ell=2$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.16653v3</guid>
      <category>cs.GT</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M\'arton Benedek, P\'eter Bir\'o, Gergely Cs\'aji, Matthew Johnson, Dani\"el Paulusma, Xin Ye</dc:creator>
    </item>
    <item>
      <title>Learning to erase quantum states: thermodynamic implications of quantum learning theory</title>
      <link>https://arxiv.org/abs/2504.07341</link>
      <description>arXiv:2504.07341v2 Announce Type: replace-cross 
Abstract: The energy cost of erasing quantum states depends on our knowledge of the states. We show that learning algorithms can acquire such knowledge to erase many copies of an unknown state at the optimal energy cost. This is proved by showing that learning can be made fully reversible and has no fundamental energy cost itself. With simple counting arguments, we relate the energy cost of erasing quantum states to their complexity, entanglement, and magic. We further show that the constructed erasure protocol is computationally efficient when learning is efficient. Conversely, under standard cryptographic assumptions, we prove that the optimal energy cost cannot be achieved efficiently in general. These results also enable efficient work extraction based on learning. Together, our results establish a concrete connection between quantum learning theory and thermodynamics, highlighting the physical significance of learning processes and enabling provably-efficient learning-based protocols for thermodynamic tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07341v2</guid>
      <category>quant-ph</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.CC</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haimeng Zhao, Yuzhen Zhang, John Preskill</dc:creator>
    </item>
    <item>
      <title>Tight Success Probabilities for Quantum Period Finding and Phase Estimation</title>
      <link>https://arxiv.org/abs/2506.20527</link>
      <description>arXiv:2506.20527v3 Announce Type: replace-cross 
Abstract: Period finding and phase estimation are fundamental in quantum computing. Prior work has established lower bounds on their success probabilities. Such quantum algorithms measure a state $|\hat\ell\rangle$ in an $n$-qubit computational basis, $\hat\ell \in [0, 2^n - 1]$, and then post-process this measurement to produce the final output, in the case of period finding, a divisor of the period $r$. We consider a general post-processing algorithm which succeeds whenever the measured $\hat\ell$ is within some tolerance $M$ of a positive integer multiple of $2^n / r$. We give new (tight) lower and upper bounds on the success probability that converge to 1. The parameter $n$ captures the complexity of the quantum circuit. The parameter $M$ can be tuned by varying the post-processing algorithm (e.g., additional brute-force search, lattice methods). Our tight analysis allows for the careful exploitation of the tradeoffs between the complexity of the quantum circuit and the effort spent in classical processing when optimizing the probability of success. We note that the most recent prior work in most recent work does not give tight bounds for general $M$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20527v3</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1088/1402-4896/ae20cc</arxiv:DOI>
      <dc:creator>Malik Magdon-Ismail, Khai Dong</dc:creator>
    </item>
    <item>
      <title>Classical billiards can compute</title>
      <link>https://arxiv.org/abs/2512.19156</link>
      <description>arXiv:2512.19156v2 Announce Type: replace-cross 
Abstract: We show that two-dimensional billiard systems are Turing complete by encoding their dynamics within the framework of Topological Kleene Field Theory. Billiards serve as idealized models of particle motion with elastic reflections and arise naturally as limits of smooth Hamiltonian systems under steep confining potentials. Our results establish the existence of undecidable trajectories in physically natural billiard-type models, including billiard-type models arising in hard-sphere gases and in collision-chain limits of celestial mechanics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19156v2</guid>
      <category>math.DS</category>
      <category>cs.CC</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <pubDate>Tue, 30 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eva Miranda, Isaac Ramos</dc:creator>
    </item>
  </channel>
</rss>
