<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 23 Oct 2024 02:08:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Differentially Private Covariate Balancing Causal Inference</title>
      <link>https://arxiv.org/abs/2410.14789</link>
      <description>arXiv:2410.14789v1 Announce Type: new 
Abstract: Differential privacy is the leading mathematical framework for privacy protection, providing a probabilistic guarantee that safeguards individuals' private information when publishing statistics from a dataset. This guarantee is achieved by applying a randomized algorithm to the original data, which introduces unique challenges in data analysis by distorting inherent patterns. In particular, causal inference using observational data in privacy-sensitive contexts is challenging because it requires covariate balance between treatment groups, yet checking the true covariates is prohibited to prevent leakage of sensitive information. In this article, we present a differentially private two-stage covariate balancing weighting estimator to infer causal effects from observational data. Our algorithm produces both point and interval estimators with statistical guarantees, such as consistency and rate optimality, under a given privacy budget.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14789v1</guid>
      <category>stat.ME</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuki Ohnishi, Jordan Awan</dc:creator>
    </item>
    <item>
      <title>A New One Parameter Unit Distribution: Median Based Unit Rayleigh (MBUR): Parametric Quantile Regression Model</title>
      <link>https://arxiv.org/abs/2410.14857</link>
      <description>arXiv:2410.14857v1 Announce Type: new 
Abstract: Parametric quantile regression is illustrated for the one parameter new unit Rayleigh distribution called Median Based Unit Rayleigh distribution (MBUR) distribution. The estimation process using re-parameterized maximum likelihood function is highlighted with real dataset example. The inference and goodness of fit is also explored.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14857v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iman Mohamed Attia</dc:creator>
    </item>
    <item>
      <title>Fast and Optimal Changepoint Detection and Localization using Bonferroni Triplets</title>
      <link>https://arxiv.org/abs/2410.14866</link>
      <description>arXiv:2410.14866v1 Announce Type: new 
Abstract: The paper considers the problem of detecting and localizing changepoints in a sequence of independent observations. We propose to evaluate a local test statistic on a triplet of time points, for each such triplet in a particular collection. This collection is sparse enough so that the results of the local tests can simply be combined with a weighted Bonferroni correction. This results in a simple and fast method, {\sl Lean Bonferroni Changepoint detection} (LBD), that provides finite sample guarantees for the existance of changepoints as well as simultaneous confidence intervals for their locations. LBD is free of tuning parameters, and we show that LBD allows optimal inference for the detection of changepoints. To this end, we provide a lower bound for the critical constant that measures the difficulty of the changepoint detection problem, and we show that LBD attains this critical constant. We illustrate LBD for a number of distributional settings, namely when the observations are homoscedastic normal with known or unknown variance, for observations from a natural exponential family, and in a nonparametric setting where we assume only exchangeability for segments without a changepoint.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14866v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jayoon Jang, Guenther Walther</dc:creator>
    </item>
    <item>
      <title>Stochastic Loss Reserving: Dependence and Estimation</title>
      <link>https://arxiv.org/abs/2410.14985</link>
      <description>arXiv:2410.14985v1 Announce Type: new 
Abstract: Nowadays insurers have to account for potentially complex dependence between risks. In the field of loss reserving, there are many parametric and non-parametric models attempting to capture dependence between business lines. One common approach has been to use additive background risk models (ABRMs) which provide rich and interpretable dependence structures via a common shock model. Unfortunately, ABRMs are often restrictive. Models that capture necessary features may have impractical to estimate parameters. For example models without a closed-form likelihood function for lack of a probability density function (e.g. some Tweedie, Stable Distributions, etc).
  We apply a modification of the continuous generalised method of moments (CGMM) of [Carrasco and Florens, 2000] which delivers comparable estimators to the MLE to loss reserving. We examine models such as the one proposed by [Avanzi et al., 2016] and a related but novel one derived from the stable family of distributions. Our CGMM method of estimation provides conventional non-Bayesian estimates in the case where MLEs are impractical.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14985v1</guid>
      <category>stat.ME</category>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Andrew Fleck, Edward Furman, Yang Shen</dc:creator>
    </item>
    <item>
      <title>Bayesian-based Propensity Score Subclassification Estimator</title>
      <link>https://arxiv.org/abs/2410.15102</link>
      <description>arXiv:2410.15102v1 Announce Type: new 
Abstract: Subclassification estimators are one of the methods used to estimate causal effects of interest using the propensity score. This method is more stable compared to other weighting methods, such as inverse probability weighting estimators, in terms of the variance of the estimators. In subclassification estimators, the number of strata is traditionally set at five, and this number is not typically chosen based on data information. Even when the number of strata is selected, the uncertainty from the selection process is often not properly accounted for. In this study, we propose a novel Bayesian-based subclassification estimator that can assess the uncertainty in the number of strata, rather than selecting a single optimal number, using a Bayesian paradigm. To achieve this, we apply a general Bayesian procedure that does not rely on a likelihood function. This procedure allows us to avoid making strong assumptions about the outcome model, maintaining the same flexibility as traditional causal inference methods. With the proposed Bayesian procedure, it is expected that uncertainties from the design phase can be appropriately reflected in the analysis phase, which is sometimes overlooked in non-Bayesian contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15102v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shunichiro Orihara, Tomotaka Momozaki</dc:creator>
    </item>
    <item>
      <title>High-dimensional prediction for count response via sparse exponential weights</title>
      <link>https://arxiv.org/abs/2410.15381</link>
      <description>arXiv:2410.15381v1 Announce Type: new 
Abstract: Count data is prevalent in various fields like ecology, medical research, and genomics. In high-dimensional settings, where the number of features exceeds the sample size, feature selection becomes essential. While frequentist methods like Lasso have advanced in handling high-dimensional count data, Bayesian approaches remain under-explored with no theoretical results on prediction performance. This paper introduces a novel probabilistic machine learning framework for high-dimensional count data prediction. We propose a pseudo-Bayesian method that integrates a scaled Student prior to promote sparsity and uses an exponential weight aggregation procedure. A key contribution is a novel risk measure tailored to count data prediction, with theoretical guarantees for prediction risk using PAC-Bayesian bounds. Our results include non-asymptotic oracle inequalities, demonstrating rate-optimal prediction error without prior knowledge of sparsity. We implement this approach efficiently using Langevin Monte Carlo method. Simulations and a real data application highlight the strong performance of our method compared to the Lasso in various settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15381v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>The Tien Mai</dc:creator>
    </item>
    <item>
      <title>Probabilities for asymmetric p-outside values</title>
      <link>https://arxiv.org/abs/2410.15383</link>
      <description>arXiv:2410.15383v1 Announce Type: new 
Abstract: In 2017-2020 Jordanova and co-authors investigate probabilities for p-outside values and determine them in many particular cases. They show that these probabilities are closely related to the concept for heavy tails. Tukey's boxplots are very popular and useful in practice. Analogously to the chi-square-criterion, the relative frequencies of the events an observation to fall in different their parts, compared with the corresponding probabilities an observation of a fixed probability distribution to fall in the same parts, help the practitioners to find the accurate probability distribution of the observed random variable. These open the door to work with the distribution sensitive estimators which in many cases are more accurate, especially for small sample investigations. All these methods, however, suffer from the disadvantage that they use inter quantile range in a symmetric way. The concept for outside values should take into account the form of the distribution. Therefore, here, we give possibility for more asymmetry in analysis of the tails of the distributions. We suggest new theoretical and empirical box-plots and characteristics of the tails of the distributions. These are theoretical asymmetric p-outside values functions. We partially investigate some of their properties and give some examples. It turns out that they do not depend on the center and the scaling factor of the distribution. Therefore, they are very appropriate for comparison of the tails of the distribution, and later on, for estimation of the parameters, which govern the tail behaviour of the cumulative distribution function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15383v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pavlina K. Jordanova</dc:creator>
    </item>
    <item>
      <title>A New Framework for Bayesian Function Registration</title>
      <link>https://arxiv.org/abs/2410.15421</link>
      <description>arXiv:2410.15421v1 Announce Type: new 
Abstract: Function registration, also referred to as alignment, has been one of the fundamental problems in the field of functional data analysis. Classical registration methods such as the Fisher-Rao alignment focus on estimating optimal time warping function between functions. In recent studies, a model on time warping has attracted more attention, and it can be used as a prior term to combine with the classical method (as a likelihood term) in a Bayesian framework. The Bayesian approaches have been shown improvement over the classical methods. However, its prior model on time warping is often based a nonlinear approximation, which may introduce inaccuracy and inefficiency. To overcome these problems, we propose a new Bayesian approach by adopting a prior which provides a linear representation and various stochastic processes (Gaussian or non-Gaussian) can be effectively utilized on time warping. No linearization approximation is needed in the time warping computation, and the posterior can be obtained via a conventional Markov Chain Monte Carlo approach. We thoroughly investigate the impact of the prior on the performance of functional registration with multiple simulation examples, which demonstrate the superiority of the new framework over the previous methods. We finally utilize the new method in a real dataset and obtain desirable alignment result.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15421v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yijia Ma, Wei Wu</dc:creator>
    </item>
    <item>
      <title>Randomization Inference for Before-and-After Studies with Multiple Units: An Application to a Criminal Procedure Reform in Uruguay</title>
      <link>https://arxiv.org/abs/2410.15477</link>
      <description>arXiv:2410.15477v1 Announce Type: new 
Abstract: We study the immediate impact of a new code of criminal procedure on crime. In November 2017, Uruguay switched from an inquisitorial system (where a single judge leads the investigation and decides the appropriate punishment for a particular crime) to an adversarial system (where the investigation is now led by prosecutors and the judge plays an overseeing role). To analyze the short-term effects of this reform, we develop a randomization-based approach for before-and-after studies with multiple units. Our framework avoids parametric time series assumptions and eliminates extrapolation by basing statistical inferences on finite-sample methods that rely only on the time periods closest to the time of the policy intervention. A key identification assumption underlying our method is that there would have been no time trends in the absence of the intervention, which is most plausible in a small window around the time of the reform. We also discuss several falsification methods to assess the plausibility of this assumption. Using our proposed inferential approach, we find statistically significant short-term causal effects of the crime reform. Our unbiased estimate shows an average increase of approximately 25 police reports per day in the week following the implementation of the new adversarial system in Montevideo, representing an 8 percent increase compared to the previous week under the old system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15477v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Carlos Diaz, Rocio Titiunik</dc:creator>
    </item>
    <item>
      <title>Simultaneous Inference in Multiple Matrix-Variate Graphs for High-Dimensional Neural Recordings</title>
      <link>https://arxiv.org/abs/2410.15530</link>
      <description>arXiv:2410.15530v1 Announce Type: new 
Abstract: As large-scale neural recordings become common, many neuroscientific investigations are focused on identifying functional connectivity from spatio-temporal measurements in two or more brain areas across multiple sessions. Spatial-temporal data in neural recordings can be represented as matrix-variate data, with time as the first dimension and space as the second. In this paper, we exploit the multiple matrix-variate Gaussian Graphical model to encode the common underlying spatial functional connectivity across multiple sessions of neural recordings. By effectively integrating information across multiple graphs, we develop a novel inferential framework that allows simultaneous testing to detect meaningful connectivity for a target edge subset of arbitrary size. Our test statistics are based on a group penalized regression approach and a high-dimensional Gaussian approximation technique. The validity of simultaneous testing is demonstrated theoretically under mild assumptions on sample size and non-stationary autoregressive temporal dependence. Our test is nearly optimal in achieving the testable region boundary. Additionally, our method involves only convex optimization and parametric bootstrap, making it computationally attractive. We demonstrate the efficacy of the new method through both simulations and an experimental study involving multiple local field potential (LFP) recordings in the Prefrontal Cortex (PFC) and visual area V4 during a memory-guided saccade task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15530v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zongge Liu, Heejong Bong, Zhao Ren, Matthew A. Smith, Robert E. Kass</dc:creator>
    </item>
    <item>
      <title>Ablation Studies for Novel Treatment Effect Estimation Models</title>
      <link>https://arxiv.org/abs/2410.15560</link>
      <description>arXiv:2410.15560v1 Announce Type: new 
Abstract: Ablation studies are essential for understanding the contribution of individual components within complex models, yet their application in nonparametric treatment effect estimation remains limited. This paper emphasizes the importance of ablation studies by examining the Bayesian Causal Forest (BCF) model, particularly the inclusion of the estimated propensity score $\hat{\pi}(x_i)$ intended to mitigate regularization-induced confounding (RIC). Through a partial ablation study utilizing five synthetic data-generating processes with varying baseline and propensity score complexities, we demonstrate that excluding $\hat{\pi}(x_i)$ does not diminish the model's performance in estimating average and conditional average treatment effects or in uncertainty quantification. Moreover, omitting $\hat{\pi}(x_i)$ reduces computational time by approximately 21\%. These findings suggest that the BCF model's inherent flexibility suffices in adjusting for confounding without explicitly incorporating the propensity score. The study advocates for the routine use of ablation studies in treatment effect estimation to ensure model components are essential and to prevent unnecessary complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15560v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hugo Gobato Souto, Francisco Louzada</dc:creator>
    </item>
    <item>
      <title>Assessing mediation in cross-sectional stepped wedge cluster randomized trials</title>
      <link>https://arxiv.org/abs/2410.15596</link>
      <description>arXiv:2410.15596v1 Announce Type: new 
Abstract: Mediation analysis has been comprehensively studied for independent data but relatively little work has been done for correlated data, especially for the increasingly adopted stepped wedge cluster randomized trials (SW-CRTs). Motivated by challenges in underlying the effect mechanisms in pragmatic and implementation science clinical trials, we develop new methods for mediation analysis in SW-CRTs. Specifically, based on a linear and generalized linear mixed models, we demonstrate how to estimate the natural indirect effect and mediation proportion in typical SW-CRTs with four data types, including both continuous and binary mediators and outcomes. Furthermore, to address the emerging challenges in exposure-time treatment effect heterogeneity, we derive the mediation expressions in SW-CRTs when the total effect varies as a function of the exposure time. The cluster jackknife approach is considered for inference across all data types and treatment effect structures. We conduct extensive simulations to evaluate the finite-sample performances of proposed mediation estimators and demonstrate the proposed approach in a real data example. A user-friendly R package mediateSWCRT has been developed to facilitate the practical implementation of the estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15596v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhiqiang Cao, Fan Li</dc:creator>
    </item>
    <item>
      <title>Variable screening for covariate dependent extreme value index estimation</title>
      <link>https://arxiv.org/abs/2410.15705</link>
      <description>arXiv:2410.15705v1 Announce Type: new 
Abstract: One of the main topics of extreme value analysis is to estimate the extreme value index, an important parameter that controls the tail behavior of the distribution. In many cases, estimating the extreme value index of the target variable associated with covariates is useful. Although the estimation of the covariate-dependent extreme value index has been developed by numerous researchers, no results have been presented regarding covariate selection. This paper proposes a sure independence screening method for covariate-dependent extreme value index estimation. For the screening, the marginal utility between the target variable and each covariate is calculated using the conditional Pickands estimator. A single-index model that uses the covariates selected by screening is further provided to estimate the extreme value index after screening. Monte Carlo simulations confirmed the finite sample performance of the proposed method. In addition, a real-data application is presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15705v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takuma Yoshida, Yuta Umezu</dc:creator>
    </item>
    <item>
      <title>Nonparametric method of structural break detection in stochastic time series regression model</title>
      <link>https://arxiv.org/abs/2410.15713</link>
      <description>arXiv:2410.15713v1 Announce Type: new 
Abstract: We propose a nonparametric algorithm to detect structural breaks in the conditional mean and/or variance of a time series. Our method does not assume any specific parametric form for the dependence structure of the regressor, the time series model, or the distribution of the model noise. This flexibility allows our algorithm to be applicable to a wide range of time series structures commonly encountered in financial econometrics. The effectiveness of the proposed algorithm is validated through an extensive simulation study and a real data application in detecting structural breaks in the mean and volatility of Bitcoin returns. The algorithm's ability to identify structural breaks in the data highlights its practical utility in econometric analysis and financial modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15713v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Archi Roy, Moumanti Podder, Soudeep Deb</dc:creator>
    </item>
    <item>
      <title>A measure of departure from symmetry via the Fisher-Rao distance for contingency tables</title>
      <link>https://arxiv.org/abs/2410.15874</link>
      <description>arXiv:2410.15874v1 Announce Type: new 
Abstract: A measure of asymmetry is a quantification method that allows for the comparison of categorical evaluations before and after treatment effects or among different target populations, irrespective of sample size. We focus on square contingency tables that summarize survey results between two time points or cohorts, represented by the same categorical variables. We propose a measure to evaluate the degree of departure from a symmetry model using cosine similarity. This proposal is based on the Fisher-Rao distance, allowing asymmetry to be interpreted as a geodesic distance between two distributions. Various measures of asymmetry have been proposed, but visualizing the relationship of these quantification methods on a two-dimensional plane demonstrates that the proposed measure provides the geometrically simplest and most natural quantification. Moreover, the visualized figure indicates that the proposed method for measuring departures from symmetry is less affected by very few cells with extreme asymmetry. A simulation study shows that for square contingency tables with an underlying asymmetry model, our method can directly extract and quantify only the asymmetric structure of the model, and can more sensitively detect departures from symmetry than divergence-type measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15874v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wataru Urasaki, Go Kawamitsu, Tomoyuki Nakagawa, Kouji Tahata</dc:creator>
    </item>
    <item>
      <title>A Causal Transformation Model for Time-to-Event Data Affected by Unobserved Confounding: Revisiting the Illinois Reemployment Bonus Experiment</title>
      <link>https://arxiv.org/abs/2410.15968</link>
      <description>arXiv:2410.15968v1 Announce Type: new 
Abstract: Motivated by studies investigating causal effects in survival analysis, we propose a transformation model to quantify the impact of a binary treatment on a time-to-event outcome. The approach is based on a flexible linear transformation structural model that links a monotone function of the time-to-event with the propensity for treatment through a bivariate Gaussian distribution. The model equations are specified as functions of additive predictors, allowing the impacts of observed confounders to be accounted for flexibly. Furthermore, the effect of the instrumental variable may be regularized through a ridge penalty, while interactions between the treatment and modifier variables can be incorporated into the model to acknowledge potential variations in treatment effects across different subgroups. The baseline survival function is estimated in a flexible manner using monotonic P-splines, while unobserved confounding is captured through the dependence parameter of the bivariate Gaussian. Parameter estimation is achieved via a computationally efficient and stable penalized maximum likelihood estimation approach and intervals constructed using the related inferential results. We revisit a dataset from the Illinois Reemployment Bonus Experiment to estimate the causal effect of a cash bonus on unemployment duration, unveiling new insights. The modeling framework is incorporated into the R package GJRM, enabling researchers and practitioners to fit the proposed causal survival model and obtain easy-to-interpret numerical and visual summaries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15968v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giampiero Marra, Rosalba Radice</dc:creator>
    </item>
    <item>
      <title>Improving the (approximate) sequential probability ratio test by avoiding overshoot</title>
      <link>https://arxiv.org/abs/2410.16076</link>
      <description>arXiv:2410.16076v1 Announce Type: new 
Abstract: The sequential probability ratio test (SPRT) by Wald (1945) is a cornerstone of sequential analysis. Based on desired type-I, II error levels $\alpha, \beta \in (0,1)$, it stops when the likelihood ratio statistic crosses certain upper and lower thresholds, guaranteeing optimality of the expected sample size. However, these thresholds are not closed form and the test is often applied with approximate thresholds $(1-\beta)/\alpha$ and $\beta/(1-\alpha)$ (approximate SPRT). When $\beta &gt; 0$, this neither guarantees type I,II error control at $\alpha,\beta$ nor optimality. When $\beta=0$ (power-one SPRT), it guarantees type I error control at $\alpha$ that is in general conservative, and thus not optimal. The looseness in both cases is caused by overshoot: the test statistic overshoots the thresholds at the stopping time. One standard way to address this is to calculate the right thresholds numerically, but many papers and software packages do not do this. In this paper, we describe a different way to improve the approximate SPRT: we change the test statistic to avoid overshoot. Our technique uniformly improves power-one SPRTs $(\beta=0)$ for simple nulls and alternatives, or for one-sided nulls and alternatives in exponential families. When $\beta &gt; 0$, our techniques provide valid type I error guarantees, lead to similar type II error as Wald's, but often needs less samples. These improved sequential tests can also be used for deriving tighter parametric confidence sequences, and can be extended to nontrivial settings like sampling without replacement and conformal martingales.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16076v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lasse Fischer, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Dynamic Time Warping-based imputation of long gaps in human mobility trajectories</title>
      <link>https://arxiv.org/abs/2410.16096</link>
      <description>arXiv:2410.16096v1 Announce Type: new 
Abstract: Individual mobility trajectories are difficult to measure and often incur long periods of missingness. Aggregation of this mobility data without accounting for the missingness leads to erroneous results, underestimating travel behavior. This paper proposes Dynamic Time Warping-Based Multiple Imputation (DTWBMI) as a method of filling long gaps in human mobility trajectories in order to use the available data to the fullest extent. This method reduces spatiotemporal trajectories to time series of particular travel behavior, then selects candidates for multiple imputation on the basis of the dynamic time warping distance between the potential donor series and the series preceding and following the gap in the recipient series and finally imputes values multiple times. A simulation study designed to establish optimal parameters for DTWBMI provides two versions of the method. These two methods are applied to a real-world dataset of individual mobility trajectories with simulated missingness and compared against other methods of handling missingness. Linear interpolation outperforms DTWBMI and other methods when gaps are short and data are limited. DTWBMI outperforms other methods when gaps become longer and when more data are available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16096v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Danielle McCool, Peter Lugtig, Barry Schouten</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Tensor Discriminant Analysis with Incomplete Tensors</title>
      <link>https://arxiv.org/abs/2410.14783</link>
      <description>arXiv:2410.14783v1 Announce Type: cross 
Abstract: Tensor classification has gained prominence across various fields, yet the challenge of handling partially observed tensor data in real-world applications remains largely unaddressed. This paper introduces a novel approach to tensor classification with incomplete data, framed within the tensor high-dimensional linear discriminant analysis. Specifically, we consider a high-dimensional tensor predictor with missing observations under the Missing Completely at Random (MCR) assumption and employ the Tensor Gaussian Mixture Model to capture the relationship between the tensor predictor and class label. We propose the Tensor LDA-MD algorithm, which manages high-dimensional tensor predictors with missing entries by leveraging the low-rank structure of the discriminant tensor. A key feature of our approach is a novel covariance estimation method under the tensor-based MCR model, supported by theoretical results that allow for correlated entries under mild conditions. Our work establishes the convergence rate of the estimation error of the discriminant tensor with incomplete data and minimax optimal bounds for the misclassification rate, addressing key gaps in the literature. Additionally, we derive large deviation results for the generalized mode-wise (separable) sample covariance matrix and its inverse, which are crucial tools in our analysis and hold independent interest. Our method demonstrates excellent performance in simulations and real data analysis, even with significant proportions of missing data. This research advances high-dimensional LDA and tensor learning, providing practical tools for applications with incomplete data and a solid theoretical foundation for classification accuracy in complex settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14783v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elynn Chen, Yuefeng Han, Jiayu Li</dc:creator>
    </item>
    <item>
      <title>Isolated Causal Effects of Natural Language</title>
      <link>https://arxiv.org/abs/2410.14812</link>
      <description>arXiv:2410.14812v1 Announce Type: cross 
Abstract: As language technologies become widespread, it is important to understand how variations in language affect reader perceptions -- formalized as the isolated causal effect of some focal language-encoded intervention on an external outcome. A core challenge of estimating isolated effects is the need to approximate all non-focal language outside of the intervention. In this paper, we introduce a formal estimation framework for isolated causal effects and explore how different approximations of non-focal language impact effect estimates. Drawing on the principle of omitted variable bias, we present metrics for evaluating the quality of isolated effect estimation and non-focal language approximation along the axes of fidelity and overlap. In experiments on semi-synthetic and real-world data, we validate the ability of our framework to recover ground truth isolated effects, and we demonstrate the utility of our proposed metrics as measures of quality for both isolated effect estimates and non-focal language approximations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14812v1</guid>
      <category>cs.CL</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Victoria Lin, Louis-Philippe Morency, Eli Ben-Michael</dc:creator>
    </item>
    <item>
      <title>Predictive variational inference: Learn the predictively optimal posterior distribution</title>
      <link>https://arxiv.org/abs/2410.14843</link>
      <description>arXiv:2410.14843v1 Announce Type: cross 
Abstract: Vanilla variational inference finds an optimal approximation to the Bayesian posterior distribution, but even the exact Bayesian posterior is often not meaningful under model misspecification. We propose predictive variational inference (PVI): a general inference framework that seeks and samples from an optimal posterior density such that the resulting posterior predictive distribution is as close to the true data generating process as possible, while this this closeness is measured by multiple scoring rules. By optimizing the objective, the predictive variational inference is generally not the same as, or even attempting to approximate, the Bayesian posterior, even asymptotically. Rather, we interpret it as implicit hierarchical expansion. Further, the learned posterior uncertainty detects heterogeneity of parameters among the population, enabling automatic model diagnosis. This framework applies to both likelihood-exact and likelihood-free models. We demonstrate its application in real data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14843v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinlin Lai, Yuling Yao</dc:creator>
    </item>
    <item>
      <title>Learning the Effect of Persuasion via Difference-In-Differences</title>
      <link>https://arxiv.org/abs/2410.14871</link>
      <description>arXiv:2410.14871v1 Announce Type: cross 
Abstract: The persuasion rate is a key parameter for measuring the causal effect of a directional message on influencing the recipient's behavior. Its identification analysis has largely relied on the availability of credible instruments, but the requirement is not always satisfied in observational studies. Therefore, we develop a framework for identifying, estimating, and conducting inference for the average persuasion rates on the treated using a difference-in-differences approach. The average treatment effect on the treated is a standard parameter with difference-in-differences, but it underestimates the persuasion rate in our setting. Our estimation and inference methods include regression-based approaches and semiparametrically efficient estimators. Beginning with the canonical two-period case, we extend the framework to staggered treatment settings, where we show how to conduct rich analyses like the event-study design. We revisit previous studies of the British election and the Chinese curriculum reform to illustrate the usefulness of our methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14871v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sung Jae Jun, Sokbae Lee</dc:creator>
    </item>
    <item>
      <title>Modeling Time-Varying Effects of Mobile Health Interventions Using Longitudinal Functional Data from HeartSteps Micro-Randomized Trial</title>
      <link>https://arxiv.org/abs/2410.15049</link>
      <description>arXiv:2410.15049v1 Announce Type: cross 
Abstract: To optimize mobile health interventions and advance domain knowledge on intervention design, it is critical to understand how the intervention effect varies over time and with contextual information. This study aims to assess how a push notification suggesting physical activity influences individuals' step counts using data from the HeartSteps micro-randomized trial (MRT). The statistical challenges include the time-varying treatments and longitudinal functional step count measurements. We propose the first semiparametric causal excursion effect model with varying coefficients to model the time-varying effects within a decision point and across decision points in an MRT. The proposed model incorporates double time indices to accommodate the longitudinal functional outcome, enabling the assessment of time-varying effect moderation by contextual variables. We propose a two-stage causal effect estimator that is robust against a misspecified high-dimensional outcome regression nuisance model. We establish asymptotic theory and conduct simulation studies to validate the proposed estimator. Our analysis provides new insights into individuals' change in response profiles (such as how soon a response occurs) due to the activity suggestions, how such changes differ by the type of suggestions received, and how such changes depend on other contextual information such as being recently sedentary and the day being a weekday.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15049v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiaxin Yu, Tianchen Qian</dc:creator>
    </item>
    <item>
      <title>Asymptotic Time-Uniform Inference for Parameters in Averaged Stochastic Approximation</title>
      <link>https://arxiv.org/abs/2410.15057</link>
      <description>arXiv:2410.15057v1 Announce Type: cross 
Abstract: We study time-uniform statistical inference for parameters in stochastic approximation (SA), which encompasses a bunch of applications in optimization and machine learning. To that end, we analyze the almost-sure convergence rates of the averaged iterates to a scaled sum of Gaussians in both linear and nonlinear SA problems. We then construct three types of asymptotic confidence sequences that are valid uniformly across all times with coverage guarantees, in an asymptotic sense that the starting time is sufficiently large. These coverage guarantees remain valid if the unknown covariance matrix is replaced by its plug-in estimator, and we conduct experiments to validate our methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15057v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chuhan Xie, Kaicheng Jin, Jiadong Liang, Zhihua Zhang</dc:creator>
    </item>
    <item>
      <title>Joint Probability Estimation of Many Binary Outcomes via Localized Adversarial Lasso</title>
      <link>https://arxiv.org/abs/2410.15166</link>
      <description>arXiv:2410.15166v1 Announce Type: cross 
Abstract: In this work we consider estimating the probability of many (possibly dependent) binary outcomes which is at the core of many applications, e.g., multi-level treatments in causal inference, demands for bundle of products, etc. Without further conditions, the probability distribution of an M dimensional binary vector is characterized by exponentially in M coefficients which can lead to a high-dimensional problem even without the presence of covariates. Understanding the (in)dependence structure allows us to substantially improve the estimation as it allows for an effective factorization of the probability distribution. In order to estimate the probability distribution of a M dimensional binary vector, we leverage a Bahadur representation that connects the sparsity of its coefficients with independence across the components. We propose to use regularized and adversarial regularized estimators to obtain an adaptive estimator with respect to the dependence structure which allows for rates of convergence to depend on this intrinsic (lower) dimension. These estimators are needed to handle several challenges within this setting, including estimating nuisance parameters, estimating covariates, and nonseparable moment conditions. Our main results consider the presence of (low dimensional) covariates for which we propose a locally penalized estimator. We provide pointwise rates of convergence addressing several issues in the theoretical analyses as we strive for making a computationally tractable formulation. We apply our results in the estimation of causal effects with multiple binary treatments and show how our estimators can improve the finite sample performance when compared with non-adaptive estimators that try to estimate all the probabilities directly. We also provide simulations that are consistent with our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15166v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandre Belloni, Yan Chen, Matthew Harding</dc:creator>
    </item>
    <item>
      <title>HACSurv: A Hierarchical Copula-based Approach for Survival Analysis with Dependent Competing Risks</title>
      <link>https://arxiv.org/abs/2410.15180</link>
      <description>arXiv:2410.15180v1 Announce Type: cross 
Abstract: In survival analysis, subjects often face competing risks; for example, individuals with cancer may also suffer from heart disease or other illnesses, which can jointly influence the prognosis of risks and censoring. Traditional survival analysis methods often treat competing risks as independent and fail to accommodate the dependencies between different conditions. In this paper, we introduce HACSurv, a survival analysis method that learns Hierarchical Archimedean Copulas structures and cause-specific survival functions from data with competing risks. HACSurv employs a flexible dependency structure using hierarchical Archimedean copulas to represent the relationships between competing risks and censoring. By capturing the dependencies between risks and censoring, HACSurv achieves better survival predictions and offers insights into risk interactions. Experiments on synthetic datasets demonstrate that our method can accurately identify the complex dependency structure and precisely predict survival distributions, whereas the compared methods exhibit significant deviations between their predictions and the true distributions. Experiments on multiple real-world datasets also demonstrate that our method achieves better survival prediction compared to previous state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15180v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Liu, Weijia Zhang, Min-Ling Zhang</dc:creator>
    </item>
    <item>
      <title>Extensions on low-complexity DCT approximations for larger blocklengths based on minimal angle similarity</title>
      <link>https://arxiv.org/abs/2410.15244</link>
      <description>arXiv:2410.15244v1 Announce Type: cross 
Abstract: The discrete cosine transform (DCT) is a central tool for image and video coding because it can be related to the Karhunen-Lo\`eve transform (KLT), which is the optimal transform in terms of retained transform coefficients and data decorrelation. In this paper, we introduce 16-, 32-, and 64-point low-complexity DCT approximations by minimizing individually the angle between the rows of the exact DCT matrix and the matrix induced by the approximate transforms. According to some classical figures of merit, the proposed transforms outperformed the approximations for the DCT already known in the literature. Fast algorithms were also developed for the low-complexity transforms, asserting a good balance between the performance and its computational cost. Practical applications in image encoding showed the relevance of the transforms in this context. In fact, the experiments showed that the proposed transforms had better results than the known approximations in the literature for the cases of 16, 32, and 64 blocklength.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15244v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.NA</category>
      <category>eess.SP</category>
      <category>math.NA</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s11265-023-01848-w</arxiv:DOI>
      <arxiv:journal_reference>J Sign Process Syst 95, 495-516 (2023)</arxiv:journal_reference>
      <dc:creator>A. P. Rad\"unz, L. Portella, R. S. Oliveira, F. M. Bayer, R. J. Cintra</dc:creator>
    </item>
    <item>
      <title>Structural Causality-based Generalizable Concept Discovery Models</title>
      <link>https://arxiv.org/abs/2410.15491</link>
      <description>arXiv:2410.15491v1 Announce Type: cross 
Abstract: The rising need for explainable deep neural network architectures has utilized semantic concepts as explainable units. Several approaches utilizing disentangled representation learning estimate the generative factors and utilize them as concepts for explaining DNNs. However, even though the generative factors for a dataset remain fixed, concepts are not fixed entities and vary based on downstream tasks. In this paper, we propose a disentanglement mechanism utilizing a variational autoencoder (VAE) for learning mutually independent generative factors for a given dataset and subsequently learning task-specific concepts using a structural causal model (SCM). Our method assumes generative factors and concepts to form a bipartite graph, with directed causal edges from generative factors to concepts. Experiments are conducted on datasets with known generative factors: D-sprites and Shapes3D. On specific downstream tasks, our proposed method successfully learns task-specific concepts which are explained well by the causal edges from the generative factors. Lastly, separate from current causal concept discovery methods, our methodology is generalizable to an arbitrary number of concepts and flexible to any downstream tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15491v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sanchit Sinha, Guangzhi Xiong, Aidong Zhang</dc:creator>
    </item>
    <item>
      <title>Reward Maximization for Pure Exploration: Minimax Optimal Good Arm Identification for Nonparametric Multi-Armed Bandits</title>
      <link>https://arxiv.org/abs/2410.15564</link>
      <description>arXiv:2410.15564v1 Announce Type: cross 
Abstract: In multi-armed bandits, the tasks of reward maximization and pure exploration are often at odds with each other. The former focuses on exploiting arms with the highest means, while the latter may require constant exploration across all arms. In this work, we focus on good arm identification (GAI), a practical bandit inference objective that aims to label arms with means above a threshold as quickly as possible. We show that GAI can be efficiently solved by combining a reward-maximizing sampling algorithm with a novel nonparametric anytime-valid sequential test for labeling arm means. We first establish that our sequential test maintains error control under highly nonparametric assumptions and asymptotically achieves the minimax optimal e-power, a notion of power for anytime-valid tests. Next, by pairing regret-minimizing sampling schemes with our sequential test, we provide an approach that achieves minimax optimal stopping times for labeling arms with means above a threshold, under an error probability constraint. Our empirical results validate our approach beyond the minimax setting, reducing the expected number of samples for all stopping times by at least 50% across both synthetic and real-world settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15564v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brian Cho, Dominik Meier, Kyra Gan, Nathan Kallus</dc:creator>
    </item>
    <item>
      <title>Linking Model Intervention to Causal Interpretation in Model Explanation</title>
      <link>https://arxiv.org/abs/2410.15648</link>
      <description>arXiv:2410.15648v1 Announce Type: cross 
Abstract: Intervention intuition is often used in model explanation where the intervention effect of a feature on the outcome is quantified by the difference of a model prediction when the feature value is changed from the current value to the baseline value. Such a model intervention effect of a feature is inherently association. In this paper, we will study the conditions when an intuitive model intervention effect has a causal interpretation, i.e., when it indicates whether a feature is a direct cause of the outcome. This work links the model intervention effect to the causal interpretation of a model. Such an interpretation capability is important since it indicates whether a machine learning model is trustworthy to domain experts. The conditions also reveal the limitations of using a model intervention effect for causal interpretation in an environment with unobserved features. Experiments on semi-synthetic datasets have been conducted to validate theorems and show the potential for using the model intervention effect for model interpretation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15648v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Debo Cheng, Ziqi Xu, Jiuyong Li, Lin Liu, Kui Yu, Thuc Duy Le, Jixue Liu</dc:creator>
    </item>
    <item>
      <title>Accounting for Missing Covariates in Heterogeneous Treatment Estimation</title>
      <link>https://arxiv.org/abs/2410.15655</link>
      <description>arXiv:2410.15655v1 Announce Type: cross 
Abstract: Many applications of causal inference require using treatment effects estimated on a study population to make decisions in a separate target population. We consider the challenging setting where there are covariates that are observed in the target population that were not seen in the original study. Our goal is to estimate the tightest possible bounds on heterogeneous treatment effects conditioned on such newly observed covariates. We introduce a novel partial identification strategy based on ideas from ecological inference; the main idea is that estimates of conditional treatment effects for the full covariate set must marginalize correctly when restricted to only the covariates observed in both populations. Furthermore, we introduce a bias-corrected estimator for these bounds and prove that it enjoys fast convergence rates and statistical guarantees (e.g., asymptotic normality). Experimental results on both real and synthetic data demonstrate that our framework can produce bounds that are much tighter than would otherwise be possible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15655v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Khurram Yamin, Vibhhu Sharma, Ed Kennedy, Bryan Wilder</dc:creator>
    </item>
    <item>
      <title>Quantiles and Quantile Regression on Riemannian Manifolds: a measure-transportation-based approach</title>
      <link>https://arxiv.org/abs/2410.15711</link>
      <description>arXiv:2410.15711v1 Announce Type: cross 
Abstract: Increased attention has been given recently to the statistical analysis of variables with values on nonlinear manifolds. A natural but nontrivial problem in that context is the definition of quantile concepts. We are proposing a solution for compact Riemannian manifolds without boundaries; typical examples are polyspheres, hyperspheres, and toro\"{\i}dal manifolds equipped with their Riemannian metrics. Our concept of quantile function comes along with a concept of distribution function and, in the empirical case, ranks and signs. The absence of a canonical ordering is offset by resorting to the data-driven ordering induced by optimal transports. Theoretical properties, such as the uniform convergence of the empirical distribution and conditional (and unconditional) quantile functions and distribution-freeness of ranks and signs, are established. Statistical inference applications, from goodness-of-fit to distribution-free rank-based testing, are without number. Of particular importance is the case of quantile regression with directional or toro\"{\i}dal multiple output, which is given special attention in this paper. Extensive simulations are carried out to illustrate these novel concepts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15711v1</guid>
      <category>math.ST</category>
      <category>math.GT</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marc Hallin, Hang Liu</dc:creator>
    </item>
    <item>
      <title>A Kernelization-Based Approach to Nonparametric Binary Choice Models</title>
      <link>https://arxiv.org/abs/2410.15734</link>
      <description>arXiv:2410.15734v1 Announce Type: cross 
Abstract: We propose a new estimator for nonparametric binary choice models that does not impose a parametric structure on either the systematic function of covariates or the distribution of the error term. A key advantage of our approach is its computational efficiency. For instance, even when assuming a normal error distribution as in probit models, commonly used sieves for approximating an unknown function of covariates can lead to a large-dimensional optimization problem when the number of covariates is moderate. Our approach, motivated by kernel methods in machine learning, views certain reproducing kernel Hilbert spaces as special sieve spaces, coupled with spectral cut-off regularization for dimension reduction. We establish the consistency of the proposed estimator for both the systematic function of covariates and the distribution function of the error term, and asymptotic normality of the plug-in estimator for weighted average partial derivatives. Simulation studies show that, compared to parametric estimation methods, the proposed method effectively improves finite sample performance in cases of misspecification, and has a rather mild efficiency loss if the model is correctly specified. Using administrative data on the grant decisions of US asylum applications to immigration courts, along with nine case-day variables on weather and pollution, we re-examine the effect of outdoor temperature on court judges' "mood", and thus, their grant decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15734v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guo Yan</dc:creator>
    </item>
    <item>
      <title>Towards more realistic climate model outputs: A multivariate bias correction based on zero-inflated vine copulas</title>
      <link>https://arxiv.org/abs/2410.15931</link>
      <description>arXiv:2410.15931v1 Announce Type: cross 
Abstract: Climate model large ensembles are an essential research tool for analysing and quantifying natural climate variability and providing robust information for rare extreme events. The models simulated representations of reality are susceptible to bias due to incomplete understanding of physical processes. This paper aims to correct the bias of five climate variables from the CRCM5 Large Ensemble over Central Europe at a 3-hourly temporal resolution. At this high temporal resolution, two variables, precipitation and radiation, exhibit a high share of zero inflation. We propose a novel bias-correction method, VBC (Vine copula bias correction), that models and transfers multivariate dependence structures for zero-inflated margins in the data from its error-prone model domain to a reference domain. VBC estimates the model and reference distribution using vine copulas and corrects the model distribution via (inverse) Rosenblatt transformation. To deal with the variables' zero-inflated nature, we develop a new vine density decomposition that accommodates such variables and employs an adequately randomized version of the Rosenblatt transform. This novel approach allows for more accurate modelling of multivariate zero-inflated climate data. Compared with state-of-the-art correction methods, VBC is generally the best-performing correction and the most accurate method for correcting zero-inflated events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15931v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Henri Funk, Ralf Ludwig, Helmut Kuechenhoff, Thomas Nagler</dc:creator>
    </item>
    <item>
      <title>Sequential and Simultaneous Distance-based Dimension Reduction</title>
      <link>https://arxiv.org/abs/1903.00037</link>
      <description>arXiv:1903.00037v3 Announce Type: replace 
Abstract: This paper introduces a method called Sequential and Simultaneous Distance-based Dimension Reduction ($S^2D^2R$) that performs simultaneous dimension reduction for a pair of random vectors based on Distance Covariance (dCov). Compared with Sufficient Dimension Reduction (SDR) and Canonical Correlation Analysis (CCA)-based approaches, $S^2D^2R$ is a model-free approach that does not impose dimensional or distributional restrictions on variables and is more sensitive to nonlinear relationships. Theoretically, we establish a non-asymptotic error bound to guarantee the performance of $S^2D^2R$. Numerically, $S^2D^2R$ performs comparable to or better than other state-of-the-art algorithms and is computationally faster. All codes of our $S^2D^2R$ method can be found on Github, including an R package named S2D2R.</description>
      <guid isPermaLink="false">oai:arXiv.org:1903.00037v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yijin Ni, Chuanping Yu, Andy Ko, Xiaoming Huo</dc:creator>
    </item>
    <item>
      <title>Optimization-based Causal Estimation from Heterogenous Environments</title>
      <link>https://arxiv.org/abs/2109.11990</link>
      <description>arXiv:2109.11990v4 Announce Type: replace 
Abstract: This paper presents a new optimization approach to causal estimation. Given data that contains covariates and an outcome, which covariates are causes of the outcome, and what is the strength of the causality? In classical machine learning (ML), the goal of optimization is to maximize predictive accuracy. However, some covariates might exhibit a non-causal association with the outcome. Such spurious associations provide predictive power for classical ML, but they prevent us from causally interpreting the result. This paper proposes CoCo, an optimization algorithm that bridges the gap between pure prediction and causal inference. CoCo leverages the recently-proposed idea of environments, datasets of covariates/response where the causal relationships remain invariant but where the distribution of the covariates changes from environment to environment. Given datasets from multiple environments-and ones that exhibit sufficient heterogeneity-CoCo maximizes an objective for which the only solution is the causal solution. We describe the theoretical foundations of this approach and demonstrate its effectiveness on simulated and real datasets. Compared to classical ML and existing methods, CoCo provides more accurate estimates of the causal model and more accurate predictions under interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2109.11990v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingzhang Yin, Yixin Wang, David M. Blei</dc:creator>
    </item>
    <item>
      <title>Multi-Model Subset Selection</title>
      <link>https://arxiv.org/abs/2204.08100</link>
      <description>arXiv:2204.08100v3 Announce Type: replace 
Abstract: The two primary approaches for high-dimensional regression problems are sparse methods (e.g., best subset selection, which uses the L0-norm in the penalty) and ensemble methods (e.g., random forests). Although sparse methods typically yield interpretable models, in terms of prediction accuracy they are often outperformed by "blackbox" multi-model ensemble methods. A regression ensemble is introduced which combines the interpretability of sparse methods with the high prediction accuracy of ensemble methods. An algorithm is proposed to solve the joint optimization of the corresponding L0-penalized regression models by extending recent developments in L0-optimization for sparse methods to multi-model regression ensembles. The sparse and diverse models in the ensemble are learned simultaneously from the data. Each of these models provides an explanation for the relationship between a subset of predictors and the response variable. Empirical studies and theoretical knowledge about ensembles are used to gain insight into the ensemble method's performance, focusing on the interplay between bias, variance, covariance, and variable selection. In prediction tasks, the ensembles can outperform state-of-the-art competitors on both simulated and real data. Forward stepwise regression is also generalized to multi-model regression ensembles and used to obtain an initial solution for the algorithm. The optimization algorithms are implemented in publicly available software packages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.08100v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anthony-Alexander Christidis, Stefan Van Aelst, Ruben Zamar</dc:creator>
    </item>
    <item>
      <title>Double soft-thresholded model for multi-group scalar on vector-valued image regression</title>
      <link>https://arxiv.org/abs/2206.09819</link>
      <description>arXiv:2206.09819v4 Announce Type: replace 
Abstract: In this paper, we develop a novel spatial variable selection method for scalar on vector-valued image regression in a multi-group setting. Here, 'vector-valued image' refers to the imaging datasets that contain vector-valued information at each pixel/voxel location, such as in RGB color images, multimodal medical images, DTI imaging, etc. The focus of this work is to identify the spatial locations in the image having an important effect on the scalar outcome measure. Specifically, the overall effect of each voxel is of interest. We thus develop a novel shrinkage prior by soft-thresholding the \ell_2 norm of a latent multivariate Gaussian process. It will allow us to estimate sparse and piecewise-smooth spatially varying vector-valued regression coefficient functions. For posterior inference, an efficient MCMC algorithm is developed. We establish the posterior contraction rate for parameter estimation and consistency for variable selection of the proposed Bayesian model, assuming that the true regression coefficients are Holder smooth. Finally, we demonstrate the advantages of the proposed method in simulation studies and further illustrate in an ADNI dataset for modeling MMSE scores based on DTI-based vector-valued imaging markers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.09819v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arkaprava Roy, Zhou Lan</dc:creator>
    </item>
    <item>
      <title>Boosting with copula-based components</title>
      <link>https://arxiv.org/abs/2208.04669</link>
      <description>arXiv:2208.04669v2 Announce Type: replace 
Abstract: The authors propose new additive models for binary outcomes, where the components are copula-based regression models (Noh et al, 2013), and designed such that the model may capture potentially complex interaction effects. The models do not require discretisation of continuous covariates, and are therefore suitable for problems with many such covariates. A fitting algorithm, and efficient procedures for model selection and evaluation of the components are described. Software is provided in the R-package copulaboost. Simulations and illustrations on data sets indicate that the method's predictive performance is either better than or comparable to the other methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.04669v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Boge Brant, Ingrid Hob{\ae}k Haff</dc:creator>
    </item>
    <item>
      <title>Mixed effects models for extreme value index regression</title>
      <link>https://arxiv.org/abs/2305.05106</link>
      <description>arXiv:2305.05106v4 Announce Type: replace 
Abstract: Extreme value theory (EVT) provides an elegant mathematical tool for the statistical analysis of rare events. When data are collected from multiple population subgroups, because some subgroups may have less data available for extreme value analysis, a scientific interest of many researchers would be to improve the estimates obtained directly from each subgroup. To achieve this, we incorporate the mixed effects model (MEM) into the regression technique in EVT. In small area estimation, the MEM has attracted considerable attention as a primary tool for producing reliable estimates for subgroups with small sample sizes, i.e., ``small areas.'' The key idea of MEM is to incorporate information from all subgroups into a single model and to borrow strength from all subgroups to improve estimates for each subgroup. Using this property, in extreme value analysis, the MEM may contribute to reducing the bias and variance of the direct estimates from each subgroup. This prompts us to evaluate the effectiveness of the MEM for EVT through theoretical studies and numerical experiments, including its application to the risk assessment of a number of stocks in the cryptocurrency market.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.05106v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Koki Momoki, Takuma Yoshida</dc:creator>
    </item>
    <item>
      <title>A Statistical View of Column Subset Selection</title>
      <link>https://arxiv.org/abs/2307.12892</link>
      <description>arXiv:2307.12892v2 Announce Type: replace 
Abstract: We consider the problem of selecting a small subset of representative variables from a large dataset. In the computer science literature, this dimensionality reduction problem is typically formalized as Column Subset Selection (CSS). Meanwhile, the typical statistical formalization is to find an information-maximizing set of Principal Variables. This paper shows that these two approaches are equivalent, and moreover, both can be viewed as maximum likelihood estimation within a certain semi-parametric model. Within this model, we establish suitable conditions under which the CSS estimate is consistent in high dimensions, specifically in the proportional asymptotic regime where the number of variables over the sample size converges to a constant. Using these connections, we show how to efficiently (1) perform CSS using only summary statistics from the original dataset; (2) perform CSS in the presence of missing and/or censored data; and (3) select the subset size for CSS in a hypothesis testing framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.12892v2</guid>
      <category>stat.ME</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anav Sood, Trevor Hastie</dc:creator>
    </item>
    <item>
      <title>A Bayesian Nonparametric Method to Adjust for Unmeasured Confounding with Negative Controls</title>
      <link>https://arxiv.org/abs/2309.02631</link>
      <description>arXiv:2309.02631v2 Announce Type: replace 
Abstract: Unmeasured confounding bias threatens the validity of observational studies. While sensitivity analyses and study designs have been proposed to address this issue, they often overlook the growing availability of auxiliary data. Using negative controls from these data is a promising new approach to reduce unmeasured confounding bias. In this article, we develop a Bayesian nonparametric method to estimate a causal exposure-response function (CERF) leveraging information from negative controls to adjust for unmeasured confounding. We model the CERF as a mixture of linear models. This strategy captures the potential nonlinear shape of CERFs while maintaining computational efficiency, and it leverages closed-form results that hold under the linear model assumption. We assess the performance of our method through simulation studies. We found that the proposed method can recover the true shape of the CERF in the presence of unmeasured confounding under assumptions. To show the practical utility of our approach, we apply it to adjust for a possible unmeasured confounder when evaluating the relationship between long-term exposure to ambient $PM_{2.5}$ and cardiovascular hospitalization rates among the elderly in the continental US. We implement our estimation procedure in open-source software and have made the code publicly available to ensure reproducibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.02631v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jie Kate Hu, Dafne Zorzetto, Francesca Dominici</dc:creator>
    </item>
    <item>
      <title>Typical Algorithms for Estimating Hurst Exponent of Time Sequence: A Data Analyst's Perspective</title>
      <link>https://arxiv.org/abs/2310.19051</link>
      <description>arXiv:2310.19051v3 Announce Type: replace 
Abstract: The Hurst exponent is a significant indicator for characterizing the self-similarity and long-term memory properties of time sequences. It has wide applications in physics, technologies, engineering, mathematics, statistics, economics, psychology and so on. Currently, available methods for estimating the Hurst exponent of time sequences can be divided into different categories: time-domain methods and spectrum-domain methods based on the representation of time sequence, linear regression methods and Bayesian methods based on parameter estimation methods. Although various methods are discussed in literature, there are still some deficiencies: the descriptions of the estimation algorithms are just mathematics-oriented and the pseudo-codes are missing; the effectiveness and accuracy of the estimation algorithms are not clear; the classification of estimation methods is not considered and there is a lack of guidance for selecting the estimation methods. In this work, the emphasis is put on thirteen dominant methods for estimating the Hurst exponent. For the purpose of decreasing the difficulty of implementing the estimation methods with computer programs, the mathematical principles are discussed briefly and the pseudo-codes of algorithms are presented with necessary details. It is expected that the survey could help the researchers to select, implement and apply the estimation algorithms of interest in practical situations in an easy way.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.19051v3</guid>
      <category>stat.ME</category>
      <category>cs.MS</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hong-Yan Zhang, Zhi-Qiang Feng, Si-Yu Feng, Yu Zhou</dc:creator>
    </item>
    <item>
      <title>Modern approaches for evaluating treatment effect heterogeneity from clinical trials and observational data</title>
      <link>https://arxiv.org/abs/2311.14889</link>
      <description>arXiv:2311.14889v3 Announce Type: replace 
Abstract: In this paper we review recent advances in statistical methods for the evaluation of the heterogeneity of treatment effects (HTE), including subgroup identification and estimation of individualized treatment regimens, from randomized clinical trials and observational studies. We identify several types of approaches using the features introduced in Lipkovich, Dmitrienko and D'Agostino (2017) that distinguish the recommended principled methods from basic methods for HTE evaluation that typically rely on rules of thumb and general guidelines (the methods are often referred to as common practices). We discuss the advantages and disadvantages of various principled methods as well as common measures for evaluating their performance. We use simulated data and a case study based on a historical clinical trial to illustrate several new approaches to HTE evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.14889v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1002/sim.10167</arxiv:DOI>
      <dc:creator>Ilya Lipkovich, David Svensson, Bohdana Ratitch, Alex Dmitrienko</dc:creator>
    </item>
    <item>
      <title>Maximum Likelihood Estimation under the Emax Model: Existence, Geometry and Efficiency</title>
      <link>https://arxiv.org/abs/2401.00354</link>
      <description>arXiv:2401.00354v2 Announce Type: replace 
Abstract: This study focuses on the estimation of the Emax dose-response model, a widely utilized framework in clinical trials, agriculture, and environmental experiments. Existing challenges in obtaining maximum likelihood estimates (MLE) for model parameters are often ascribed to computational issues but, in reality, stem from the absence of a MLE. Our contribution provides a new understanding and control of all the experimental situations that practitioners might face, guiding them in the estimation process. We derive the exact MLE for a three-point experimental design and we identify the two scenarios where the MLE fails. To address these challenges, we propose utilizing Firth's modified score, providing its analytical expression as a function of the experimental design. Through a simulation study, we demonstrate that, in one of the problematic cases, the Firth modification yields a finite estimate. For the remaining case, we introduce a design-augmentation strategy akin to a hypothesis test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.00354v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giacomo Aletti, Nancy Flournoy, Caterina May, Chiara Tommasi</dc:creator>
    </item>
    <item>
      <title>Gradient-flow adaptive importance sampling for Bayesian leave one out cross-validation with application to sigmoidal classification models</title>
      <link>https://arxiv.org/abs/2402.08151</link>
      <description>arXiv:2402.08151v2 Announce Type: replace 
Abstract: We introduce gradient-flow-guided adaptive importance sampling (IS) transformations for stabilizing Monte-Carlo approximations of leave-one-out (LOO) cross-validated predictions for Bayesian models. After defining two variational problems, we derive corresponding simple nonlinear transformations that utilize gradient information to shift a model's pre-trained full-data posterior closer to the target LOO posterior predictive distributions. In doing so, the transformations stabilize importance weights. The resulting Monte Carlo integrals depend on Jacobian determinants with respect to the model Hessian. We derive closed-form exact formulae for these Jacobian determinants in the cases of logistic regression and shallow ReLU-activated artificial neural networks, and provide a simple approximation that sidesteps the need to compute full Hessian matrices and their spectra. We test the methodology on an $n\ll p$ dataset that is known to produce unstable LOO IS weights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08151v2</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.SP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua C Chang, Xiangting Li, Shixin Xu, Hao-Ren Yao, Julia Porcino, Carson Chow</dc:creator>
    </item>
    <item>
      <title>Combining Rollout Designs and Clustering for Causal Inference under Low-order Interference</title>
      <link>https://arxiv.org/abs/2405.05119</link>
      <description>arXiv:2405.05119v2 Announce Type: replace 
Abstract: Estimating causal effects under interference is pertinent to many real-world settings. Recent work with low-order potential outcomes models uses a rollout design to obtain unbiased estimators that require no interference network information. However, the required extrapolation can lead to prohibitively high variance. To address this, we propose a two-stage experiment that selects a sub-population in the first stage and restricts treatment rollout to this sub-population in the second stage. We explore the role of clustering in the first stage by analyzing the bias and variance of a polynomial interpolation-style estimator under this experimental design. Bias increases with the number of edges cut in the clustering of the interference network, but variance depends on qualities of the clustering that relate to homophily and covariate balance. There is a tension between clustering objectives that minimize the number of cut edges versus those that maximize covariate balance across clusters. Through simulations, we explore a bias-variance trade-off and compare the performance of the estimator under different clustering strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05119v2</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mayleen Cortez-Rodriguez, Matthew Eichhorn, Christina Lee Yu</dc:creator>
    </item>
    <item>
      <title>Informativeness of Weighted Conformal Prediction</title>
      <link>https://arxiv.org/abs/2405.06479</link>
      <description>arXiv:2405.06479v3 Announce Type: replace 
Abstract: Weighted conformal prediction (WCP), a recently proposed framework, provides uncertainty quantification with the flexibility to accommodate different covariate distributions between training and test data. However, it is pointed out in this paper that the effectiveness of WCP heavily relies on the overlap between covariate distributions; insufficient overlap can lead to uninformative prediction intervals. To enhance the informativeness of WCP, we propose two methods for scenarios involving multiple sources with varied covariate distributions. We establish theoretical guarantees for our proposed methods and demonstrate their efficacy through simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06479v3</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mufang Ying, Wenge Guo, Koulik Khamaru, Ying Hung</dc:creator>
    </item>
    <item>
      <title>Preferential Latent Space Models for Networks with Textual Edges</title>
      <link>https://arxiv.org/abs/2405.15038</link>
      <description>arXiv:2405.15038v2 Announce Type: replace 
Abstract: Many real-world networks contain rich textual information in the edges, such as email networks where an edge between two nodes is an email exchange. The useful textual information carried in the edges is often discarded in most network analyses, resulting in an incomplete view of the relationships between nodes. In this work, we represent each text document as a generalized multi-layer network, and introduce a new and flexible preferential latent space network model that can capture how node-layer preferences directly modulate edge probabilities. We establish identifiability conditions for the proposed model and tackle model estimation with a computationally efficient projected gradient descent algorithm. We further derive the non-asymptotic error bound of the estimator from each step of the algorithm. The efficacy of our proposed method is demonstrated through simulations and an analysis of the Enron email network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15038v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Maoyu Zhang, Biao Cai, Dong Li, Xiaoyue Niu, Jingfei Zhang</dc:creator>
    </item>
    <item>
      <title>Sparse Bayesian multidimensional scaling(s)</title>
      <link>https://arxiv.org/abs/2406.15573</link>
      <description>arXiv:2406.15573v2 Announce Type: replace 
Abstract: Bayesian multidimensional scaling (BMDS) is a probabilistic dimension reduction tool that allows one to model and visualize data consisting of dissimilarities between pairs of objects. Although BMDS has proven useful within, e.g., Bayesian phylogenetic inference, its likelihood and gradient calculations require a burdensome order of $N^2$ floating-point operations, where $N$ is the number of data points. Thus, BMDS becomes impractical as $N$ grows large. We propose and compare two sparse versions of BMDS (sBMDS) that apply log-likelihood and gradient computations to subsets of the observed dissimilarity matrix data. Landmark sBMDS (L-sBMDS) extracts columns, while banded sBMDS (B-sBMDS) extracts diagonals of the data. These sparse variants let one specify a time complexity between $N^2$ and $N$. Under simplified settings, we prove posterior consistency for subsampled distance matrices. Through simulations, we examine the accuracy and computational efficiency across all models using both the Metropolis-Hastings and Hamiltonian Monte Carlo algorithms. We observe approximately 3-fold, 10-fold and 40-fold speedups with negligible loss of accuracy, when applying the sBMDS likelihoods and gradients to 500, 1,000 and 5,000 data points with 50 bands (landmarks); these speedups only increase with the size of data considered. Finally, we apply the sBMDS variants to the phylogeographic modeling of multiple influenza subtypes to better understand how these strains spread through global air transportation networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15573v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ami Sheth, Aaron Smith, Andrew J. Holbrook</dc:creator>
    </item>
    <item>
      <title>Valid standard errors for Bayesian quantile regression with clustered and independent data</title>
      <link>https://arxiv.org/abs/2407.09772</link>
      <description>arXiv:2407.09772v2 Announce Type: replace 
Abstract: In Bayesian quantile regression, the most commonly used likelihood is the asymmetric Laplace (AL) likelihood. The reason for this choice is not that it is a plausible data-generating model but that the corresponding maximum likelihood estimator is identical to the classical estimator by Koenker and Bassett (1978), and in that sense, the AL likelihood can be thought of as a working likelihood. AL-based quantile regression has been shown to produce good finite-sample Bayesian point estimates and to be consistent. However, if the AL distribution does not correspond to the data-generating distribution, credible intervals based on posterior standard deviations can have poor coverage. Yang, Wang, and He (2016) proposed an adjustment to the posterior covariance matrix that produces asymptotically valid intervals. However, we show that this adjustment is sensitive to the choice of scale parameter for the AL likelihood and can lead to poor coverage when the sample size is small to moderate. We therefore propose using Infinitesimal Jackknife (IJ) standard errors (Giordano &amp; Broderick, 2023). These standard errors do not require resampling but can be obtained from a single MCMC run. We also propose a version of IJ standard errors for clustered data. Simulations and applications to real data show that the IJ standard errors have good frequentist properties, both for independent and clustered data. We provide an R-package, IJSE, that computes IJ standard errors for clustered or independent data after estimation with the brms wrapper in R for Stan.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09772v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Feng Ji, JoonHo Lee, Sophia Rabe-Hesketh</dc:creator>
    </item>
    <item>
      <title>Priors for Reducing Asymptotic Bias of the Posterior Mean</title>
      <link>https://arxiv.org/abs/2409.19673</link>
      <description>arXiv:2409.19673v3 Announce Type: replace 
Abstract: It is shown that the first-order term of the asymptotic bias of the posterior mean is removed by a suitable choice of a prior density. In regular statistical models including exponential families, and linear and logistic regression models, such a prior is given by the squared Jeffreys prior. We also explain the relationship between the proposed prior distribution, the moment matching prior, and the prior distribution that reduces the bias term of the posterior mode.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19673v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miyata Yoichi, Yanagimoto Takemi</dc:creator>
    </item>
    <item>
      <title>Nonparametric tests of treatment effect homogeneity for policy-makers</title>
      <link>https://arxiv.org/abs/2410.00985</link>
      <description>arXiv:2410.00985v2 Announce Type: replace 
Abstract: Recent work has focused on nonparametric estimation of conditional treatment effects, but inference has remained relatively unexplored. We propose a class of nonparametric tests for both quantitative and qualitative treatment effect heterogeneity. The tests can incorporate a variety of structured assumptions on the conditional average treatment effect, allow for both continuous and discrete covariates, and do not require sample splitting. Furthermore, we show how the tests are tailored to detect alternatives where the population impact of adopting a personalized decision rule differs from using a rule that discards covariates. The proposal is thus relevant for guiding treatment policies. The utility of the proposal is borne out in simulation studies and a re-analysis of an AIDS clinical trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00985v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oliver Dukes, Mats J. Stensrud, Riccardo Brioschi, Aaron Hudson</dc:creator>
    </item>
    <item>
      <title>Exact Bayesian Inference for Multivariate Spatial Data of Any Size with Application to Air Pollution Monitoring</title>
      <link>https://arxiv.org/abs/2410.02655</link>
      <description>arXiv:2410.02655v2 Announce Type: replace 
Abstract: Fine particulate matter and aerosol optical thickness are of interest to atmospheric scientists for understanding air quality and its various health/environmental impacts. The available data are extremely large, making uncertainty quantification in a fully Bayesian framework quite difficult, as traditional implementations do not scale reasonably to the size of the data. We specifically consider roughly 8 million observations obtained from NASA's Moderate Resolution Imaging Spectroradiometer (MODIS) instrument. To analyze data on this scale, we introduce Scalable Multivariate Exact Posterior Regression (SM-EPR) which combines the recently introduced data subset approach and Exact Posterior Regression (EPR). EPR is a new Bayesian hierarchical model where it is possible to sample independent replicates of fixed and random effects directly from the posterior without the use of Markov chain Monte Carlo (MCMC) or approximate Bayesian techniques. We extend EPR to the multivariate spatial context, where the multiple variables may be distributed according to different distributions. The combination of the data subset approach with EPR allows one to perform exact Bayesian inference without MCMC for effectively any sample size. We demonstrate our new SM-EPR method using this motivating big remote sensing data application and provide several simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02655v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Madelyn Clinch, Jonathan R. Bradley</dc:creator>
    </item>
    <item>
      <title>Functional Singular Value Decomposition</title>
      <link>https://arxiv.org/abs/2410.03619</link>
      <description>arXiv:2410.03619v2 Announce Type: replace 
Abstract: Heterogeneous functional data are commonly seen in time series and longitudinal data analysis. To capture the statistical structures of such data, we propose the framework of Functional Singular Value Decomposition (FSVD), a unified framework with structure-adaptive interpretability for the analysis of heterogeneous functional data. We establish the mathematical foundation of FSVD by proving its existence and providing its fundamental properties using operator theory. We then develop an implementation approach for noisy and irregularly observed functional data based on a novel joint kernel ridge regression scheme and provide theoretical guarantees for its convergence and estimation accuracy. The framework of FSVD also introduces the concepts of intrinsic basis functions and intrinsic basis vectors, which represent two fundamental statistical structures for random functions and connect FSVD to various tasks including functional principal component analysis, factor models, functional clustering, and functional completion. We compare the performance of FSVD with existing methods in several tasks through extensive simulation studies. To demonstrate the value of FSVD in real-world datasets, we apply it to extract temporal patterns from a COVID-19 case count dataset and perform data completion on an electronic health record dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03619v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jianbin Tan, Pixu Shi, Anru R. Zhang</dc:creator>
    </item>
    <item>
      <title>Experimenting under Stochastic Congestion</title>
      <link>https://arxiv.org/abs/2302.12093</link>
      <description>arXiv:2302.12093v4 Announce Type: replace-cross 
Abstract: We study randomized experiments in a service system when stochastic congestion can arise from temporarily limited supply or excess demand. Such congestion gives rise to cross-unit interference between the waiting customers, and analytic strategies that do not account for this interference may be biased. In current practice, one of the most widely used ways to address stochastic congestion is to use switchback experiments that alternatively turn a target intervention on and off for the whole system. We find, however, that under a queueing model for stochastic congestion, the standard way of analyzing switchbacks is inefficient, and that estimators that leverage the queueing model can be materially more accurate. Additionally, we show how the queueing model enables estimation of total policy gradients from unit-level randomized experiments, thus giving practitioners an alternative experimental approach they can use without needing to pre-commit to a fixed switchback length before data collection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.12093v4</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuangning Li, Ramesh Johari, Xu Kuang, Stefan Wager</dc:creator>
    </item>
    <item>
      <title>Privacy-Preserving Community Detection for Locally Distributed Multiple Networks</title>
      <link>https://arxiv.org/abs/2306.15709</link>
      <description>arXiv:2306.15709v2 Announce Type: replace-cross 
Abstract: Modern multi-layer networks are commonly stored and analyzed in a local and distributed fashion because of the privacy, ownership, and communication costs. The literature on the model-based statistical methods for community detection based on these data is still limited. This paper proposes a new method for consensus community detection and estimation in a multi-layer stochastic block model using locally stored and computed network data with privacy protection. A novel algorithm named privacy-preserving Distributed Spectral Clustering (ppDSC) is developed. To preserve the edges' privacy, we adopt the randomized response (RR) mechanism to perturb the network edges, which satisfies the strong notion of differential privacy. The ppDSC algorithm is performed on the squared RR-perturbed adjacency matrices to prevent possible cancellation of communities among different layers. To remove the bias incurred by RR and the squared network matrices, we develop a two-step bias-adjustment procedure. Then we perform eigen-decomposition on the debiased matrices, aggregation of the local eigenvectors using an orthogonal Procrustes transformation, and k-means clustering. We provide theoretical analysis on the statistical errors of ppDSC in terms of eigen-vector estimation. In addition, the blessings and curses of network heterogeneity are well-explained by our bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15709v2</guid>
      <category>cs.SI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiao Guo, Xiang Li, Xiangyu Chang, Shujie Ma</dc:creator>
    </item>
    <item>
      <title>Randomization-based confidence sets for the local average treatment effect</title>
      <link>https://arxiv.org/abs/2404.18786</link>
      <description>arXiv:2404.18786v2 Announce Type: replace-cross 
Abstract: We consider the problem of generating confidence sets in randomized experiments with noncompliance. We show that a refinement of a randomization-based procedure proposed by Imbens and Rosenbaum (2005) has desirable properties. Namely, we show that using a studentized Anderson-Rubin-type statistic as a test statistic yields confidence intervals that are finite-sample exact under treatment effect homogeneity, and remain asymptotically valid for the Local Average Treatment Effect when the treatment effect is heterogeneous. We provide a uniform analysis of this procedure. An algorithm is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18786v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>P. M. Aronow, Haoge Chang, Patrick Lopatto</dc:creator>
    </item>
    <item>
      <title>Watermarking Counterfactual Explanations</title>
      <link>https://arxiv.org/abs/2405.18671</link>
      <description>arXiv:2405.18671v2 Announce Type: replace-cross 
Abstract: Counterfactual (CF) explanations for ML model predictions provide actionable recourse recommendations to individuals adversely impacted by predicted outcomes. However, despite being preferred by end-users, CF explanations have been shown to pose significant security risks in real-world applications; in particular, malicious adversaries can exploit CF explanations to perform query-efficient model extraction attacks on the underlying proprietary ML model. To address this security challenge, we propose CFMark, a novel model-agnostic watermarking framework for detecting unauthorized model extraction attacks relying on CF explanations. CFMark involves a novel bi-level optimization problem to embed an indistinguishable watermark into the generated CF explanation such that any future model extraction attacks using these watermarked CF explanations can be detected using a null hypothesis significance testing (NHST) scheme. At the same time, the embedded watermark does not compromise the quality of the CF explanations. We evaluate CFMark across diverse real-world datasets, CF explanation methods, and model extraction techniques. Our empirical results demonstrate CFMark's effectiveness, achieving an F-1 score of ~0.89 in identifying unauthorized model extraction attacks using watermarked CF explanations. Importantly, this watermarking incurs only a negligible degradation in the quality of generated CF explanations (i.e., ~1.3% degradation in validity and ~1.6% in proximity). Our work establishes a critical foundation for the secure deployment of CF explanations in real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18671v2</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hangzhi Guo, Firdaus Ahmed Choudhury, Tinghua Chen, Amulya Yadav</dc:creator>
    </item>
    <item>
      <title>Overcoming Common Flaws in the Evaluation of Selective Classification Systems</title>
      <link>https://arxiv.org/abs/2407.01032</link>
      <description>arXiv:2407.01032v2 Announce Type: replace-cross 
Abstract: Selective Classification, wherein models can reject low-confidence predictions, promises reliable translation of machine-learning based classification systems to real-world scenarios such as clinical diagnostics. While current evaluation of these systems typically assumes fixed working points based on pre-defined rejection thresholds, methodological progress requires benchmarking the general performance of systems akin to the $\mathrm{AUROC}$ in standard classification. In this work, we define 5 requirements for multi-threshold metrics in selective classification regarding task alignment, interpretability, and flexibility, and show how current approaches fail to meet them. We propose the Area under the Generalized Risk Coverage curve ($\mathrm{AUGRC}$), which meets all requirements and can be directly interpreted as the average risk of undetected failures. We empirically demonstrate the relevance of $\mathrm{AUGRC}$ on a comprehensive benchmark spanning 6 data sets and 13 confidence scoring functions. We find that the proposed metric substantially changes metric rankings on 5 out of the 6 data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01032v2</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeremias Traub, Till J. Bungert, Carsten T. L\"uth, Michael Baumgartner, Klaus H. Maier-Hein, Lena Maier-Hein, Paul F Jaeger</dc:creator>
    </item>
    <item>
      <title>Adaptive Contrastive Search: Uncertainty-Guided Decoding for Open-Ended Text Generation</title>
      <link>https://arxiv.org/abs/2407.18698</link>
      <description>arXiv:2407.18698v2 Announce Type: replace-cross 
Abstract: Decoding from the output distributions of large language models to produce high-quality text is a complex challenge in language modeling. Various approaches, such as beam search, sampling with temperature, $k-$sampling, nucleus $p-$sampling, typical decoding, contrastive decoding, and contrastive search, have been proposed to address this problem, aiming to improve coherence, diversity, as well as resemblance to human-generated text. In this study, we introduce adaptive contrastive search, a novel decoding strategy extending contrastive search by incorporating an adaptive degeneration penalty, guided by the estimated uncertainty of the model at each generation step. This strategy is designed to enhance both the creativity and diversity of the language modeling process while at the same time producing coherent and high-quality generated text output. Our findings indicate performance enhancement in both aspects, across different model architectures and datasets, underscoring the effectiveness of our method in text generation tasks. Our code base, datasets, and models are publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18698v2</guid>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Esteban Garces Arias, Julian Rodemann, Meimingwei Li, Christian Heumann, Matthias A{\ss}enmacher</dc:creator>
    </item>
    <item>
      <title>Towards Optimal Environmental Policies: Policy Learning under Arbitrary Bipartite Network Interference</title>
      <link>https://arxiv.org/abs/2410.08362</link>
      <description>arXiv:2410.08362v2 Announce Type: replace-cross 
Abstract: The substantial effect of air pollution on cardiovascular disease and mortality burdens is well-established. Emissions-reducing interventions on coal-fired power plants -- a major source of hazardous air pollution -- have proven to be an effective, but costly, strategy for reducing pollution-related health burdens. Targeting the power plants that achieve maximum health benefits while satisfying realistic cost constraints is challenging. The primary difficulty lies in quantifying the health benefits of intervening at particular plants. This is further complicated because interventions are applied on power plants, while health impacts occur in potentially distant communities, a setting known as bipartite network interference (BNI). In this paper, we introduce novel policy learning methods based on Q- and A-Learning to determine the optimal policy under arbitrary BNI. We derive asymptotic properties and demonstrate finite sample efficacy in simulations. We apply our novel methods to a comprehensive dataset of Medicare claims, power plant data, and pollution transport networks. Our goal is to determine the optimal strategy for installing power plant scrubbers to minimize ischemic heart disease (IHD) hospitalizations under various cost constraints. We find that annual IHD hospitalization rates could be reduced in a range from 20.66-44.51 per 10,000 person-years through optimal policies under different cost constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08362v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Raphael C. Kim, Falco J. Bargagli-Stoffi, Kevin L. Chen, Rachel C. Nethery</dc:creator>
    </item>
  </channel>
</rss>
