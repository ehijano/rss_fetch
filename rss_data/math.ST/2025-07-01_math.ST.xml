<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 01 Jul 2025 04:01:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Anytime-Valid Tests for Sparse Anomalies</title>
      <link>https://arxiv.org/abs/2506.22588</link>
      <description>arXiv:2506.22588v1 Announce Type: new 
Abstract: We consider the problem of detection of sparse anomalies when monitoring a large number of data streams continuously in time. This problem is addressed using anytime-valid tests. In the context of a normal-means model and for a fixed sample, this problem is known to exhibit a nontrivial phase transition that characterizes when anomalies can and cannot be detected. We show, for the anytime-valid version of the problem, testing procedures that can detect the presence of anomalies quickly. Given that the goal is quick detection, existing approaches to anytime-valid testing that study how evidence accumulates for large times through log-optimality criteria is insufficient. This issue is addressed in this context by studying log-optimal procedures for a fixed moment in time, but as the number of streams grows larger. The resulting characterization is related to, but not implied by the existing results for fixed-sample tests. In addition, we also construct and analyze tests that are parameter-adaptive and exhibit optimal performance (in a well defined sense) even when the hypothesized model parameters are unknown. Numerical results illustrate the behavior of the proposed tests in comparison with oracle tests and suitable benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22588v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Muriel F. P\'erez-Ortiz, Rui M. Castro</dc:creator>
    </item>
    <item>
      <title>Lower bounds for trace estimation via Block Krylov and other methods</title>
      <link>https://arxiv.org/abs/2506.22701</link>
      <description>arXiv:2506.22701v1 Announce Type: new 
Abstract: This paper studies theoretical lower bounds for estimating the trace of a matrix function, $\text{tr}(f(A))$, focusing on methods that use Hutchinson's method along with Block Krylov techniques. These methods work by approximating matrix-vector products like $f(A)V$ using a Block Krylov subspace. This is closely related to approximating functions with polynomials. We derive theoretical upper bounds on how many Krylov steps are needed for functions such as $A^{-1/2}$ and $A^{-1}$ by analyzing the upper bounds from the polynomial approximation of their scalar equivalent. In addition, we also develop lower limits on the number of queries needed for trace estimation, specifically for $\text{tr}(W^{-p})$ where $W$ is a Wishart matrix. Our study clarifies the connection between the number of steps in Block Krylov methods and the degree of the polynomial used for approximation. This links the total cost of trace estimation to basic limits in polynomial approximation and how much information is needed for the computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22701v1</guid>
      <category>math.ST</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shi Jie Yu</dc:creator>
    </item>
    <item>
      <title>On the Study of Weighted Fractional Cumulative Residual Inaccuracy and its Dynamical Version with Applications</title>
      <link>https://arxiv.org/abs/2506.22975</link>
      <description>arXiv:2506.22975v1 Announce Type: new 
Abstract: In recent years, there has been a growing interest in information measures that quantify inaccuracy and uncertainty in systems. In this paper, we introduce a novel concept called the Weighted Fractional Cumulative Residual Inaccuracy (WFCRI). We develop several fundamental properties of WFCRI and establish important bounds that reveal its analytical behavior. Further, we examine the behavior of WFCRI under a mixture hazard model. A dynamic version of WFCRI also proposed and studied its behavior under proportional hazard rate model. An empirical estimation method for WFCRI under the proportional hazard rate model framework is also proposed, and its performance is evaluated through simulation studies. Finally, we demonstrate the utility of WFCRI measure in characterizing chaotic dynamics by applying it to the Ricker and cubic maps. The proposed measure is also applied to real data to assess the uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22975v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aman Pandey, Chanchal Kundu</dc:creator>
    </item>
    <item>
      <title>Some results about varextropy and weighted varextropy functions</title>
      <link>https://arxiv.org/abs/2506.22996</link>
      <description>arXiv:2506.22996v1 Announce Type: new 
Abstract: In this paper, we investigate several properties of the weighted varextropy measure and obtain it for specific distribution functions, such as the equilibrium and weighted distributions. We also obtain bounds for the weighted varextropy, as well as for weighted residual varextropy and weighted past varextropy. Additionally, we derive an expression for the varextropy of the lifetime of coherent systems. A new stochastic ordering, referred to as weighted varextopy orderind, is introduced, and some of its key properties are explored. Furtheremore, we propose two nonparametric estimators for the weighted varextropy function. A simulation study is conducted to evaluate the performance of these estimators in terms of mean squared error(MSE) and bias. Finally, we provide a characterization of the reciprocal distribution based on the weighted varextropy measure. Some tests for reciprocal distribution are constructed by using the proposed estimators and the powers of the tests are compared with the powers of Kolmogorov-Smirnov (KS) test. application to real data is also reported.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22996v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Faranak Goodarzi</dc:creator>
    </item>
    <item>
      <title>On Universality of Non-Separable Approximate Message Passing Algorithms</title>
      <link>https://arxiv.org/abs/2506.23010</link>
      <description>arXiv:2506.23010v1 Announce Type: new 
Abstract: Mean-field characterizations of first-order iterative algorithms -- including Approximate Message Passing (AMP), stochastic and proximal gradient descent, and Langevin diffusions -- have enabled a precise understanding of learning dynamics in many statistical applications. For algorithms whose non-linearities have a coordinate-separable form, it is known that such characterizations enjoy a degree of universality with respect to the underlying data distribution. However, mean-field characterizations of non-separable algorithm dynamics have largely remained restricted to i.i.d. Gaussian or rotationally-invariant data.
  In this work, we initiate a study of universality for non-separable AMP algorithms. We identify a general condition for AMP with polynomial non-linearities, in terms of a Bounded Composition Property (BCP) for their representing tensors, to admit a state evolution that holds universally for matrices with non-Gaussian entries. We then formalize a condition of BCP-approximability for Lipschitz AMP algorithms to enjoy a similar universal guarantee. We demonstrate that many common classes of non-separable non-linearities are BCP-approximable, including local denoisers, spectral denoisers for generic signals, and compositions of separable functions with generic linear maps, implying the universality of state evolution for AMP algorithms employing these non-linearities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23010v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Max Lovig, Tianhao Wang, Zhou Fan</dc:creator>
    </item>
    <item>
      <title>Average quantile regression: a new non-mean regression model and coherent risk measure</title>
      <link>https://arxiv.org/abs/2506.23059</link>
      <description>arXiv:2506.23059v1 Announce Type: new 
Abstract: Regression models that go beyond the mean, alongside coherent risk measures, have been important tools in modern data analysis. This paper introduces the innovative concept of Average Quantile Regression (AQR), which is smooth at the quantile-like level, comonotonically additive, and explicitly accounts for the severity of tail losses relative to quantile regression. AQR serves as a versatile regression model capable of describing distributional information across all positions, akin to quantile regression, yet offering enhanced interpretability compared to expectiles. Numerous traditional regression models and coherent risk measures can be regarded as special cases of AQR. As a flexible non-parametric regression model, AQR demonstrates outstanding performance in analyzing high-dimensional and large datasets, particularly those generated by distributed systems, and provides a convenient framework for their statistical analysis. The corresponding estimators are rigorously derived, and their asymptotic properties are thoroughly developed. In a risk management context, the case study confirms AQR's effectiveness in risk assessment and portfolio optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23059v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rong Jiang, M. C. Jones, Keming Yu, Jiangfeng Wang</dc:creator>
    </item>
    <item>
      <title>Nuisance parameters and elliptically symmetric distributions: a geometric approach to parametric and semiparametric efficiency</title>
      <link>https://arxiv.org/abs/2506.23213</link>
      <description>arXiv:2506.23213v1 Announce Type: new 
Abstract: Elliptically symmetric distributions are a classic example of a semiparametric model where the location vector and the scatter matrix (or a parameterization of them) are the two finite-dimensional parameters of interest, while the density generator represents an \textit{infinite-dimensional nuisance} term. This basic representation of the elliptic model can be made more accurate, rich, and flexible by considering additional \textit{finite-dimensional nuisance} parameters. Our aim is therefore to investigate the deep and counter-intuitive links between statistical efficiency in estimating the parameters of interest in the presence of both finite and infinite-dimensional nuisance parameters. Unlike previous works that addressed this problem using Le Cam's asymptotic theory, our approach here is purely geometric: efficiency will be analyzed using tools such as projections and tangent spaces embedded in the relevant Hilbert space. This allows us to obtain original results also for the case where the location vector and the scatter matrix are parameterized by a finite-dimensional vector that can be partitioned in two sub-vectors: one containing the parameters of interest and the other containing the nuisance parameters. As an example, we illustrate how the obtained results can be applied to the well-known \virg{low-rank} parameterization. Furthermore, while the theoretical analysis will be developed for Real Elliptically Symmetric (RES) distributions, we show how to extend our results to the case of Circular and Non-Circular Complex Elliptically Symmetric (C-CES and NC-CES) distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23213v1</guid>
      <category>math.ST</category>
      <category>eess.SP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stefano Fortunati, Jean-Pierre Delmas, Esa Ollila</dc:creator>
    </item>
    <item>
      <title>Numerical computation of the Rosenblatt distribution and applications</title>
      <link>https://arxiv.org/abs/2506.23337</link>
      <description>arXiv:2506.23337v1 Announce Type: new 
Abstract: The Rosenblatt distribution plays a key role in the limit theorems for non-linear functionals of stationary Gaussian processes with long-range dependence. We derive new expressions for the characteristic function of the Rosenblatt distribution. Also we present a novel accurate approximation of all eigenvalues of the Riesz integral operator associated with the correlation function of the Gaussian process and propose an efficient algorithm for computation of the density of the Rosenblatt distribution. We perform Monte-Carlo simulation for small sample sizes to demonstrate the appearance of the Rosenblatt distribution for several functionals of stationary Gaussian processes with long-range dependence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23337v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikolai N. Leonenko, Andrey Pepelyshev</dc:creator>
    </item>
    <item>
      <title>Sampling and Identity-Testing Without Approximate Tensorization of Entropy</title>
      <link>https://arxiv.org/abs/2506.23456</link>
      <description>arXiv:2506.23456v1 Announce Type: new 
Abstract: Certain tasks in high-dimensional statistics become easier when the underlying distribution satisfies a local-to-global property called approximate tensorization of entropy (ATE). For example, the Glauber dynamics Markov chain of an ATE distribution mixes fast and can produce approximate samples in a small amount of time, since such a distribution satisfies a modified log-Sobolev inequality. Moreover, identity-testing for an ATE distribution requires few samples if the tester is given coordinate conditional access to the unknown distribution, as shown by Blanca, Chen, \v{S}tefankovi\v{c}, and Vigoda (COLT 2023).
  A natural class of distributions that do not satisfy ATE consists of mixtures of (few) distributions that do satisfy ATE. We study the complexity of identity-testing and sampling for these distributions. Our main results are the following:
  1. We show fast mixing of Glauber dynamics from a data-based initialization, with optimal sample complexity, for mixtures of distributions satisfying modified log-Sobolev inequalities. This extends work of Huang, Koehler, Lee, Mohanty, Rajaraman, Vuong, and Wu (STOC 2025, COLT 2025) for mixtures of distributions satisfying Poincar\'e inequalities.
  2. Answering an open question posed by Blanca et al., we give efficient identity-testers for mixtures of ATE distributions in the coordinate-conditional sampling access model. We also give some simplifications and improvements to the original algorithm of Blanca et al.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23456v1</guid>
      <category>math.ST</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William Gay, William He, Nicholas Kocurek, Ryan O'Donnell</dc:creator>
    </item>
    <item>
      <title>Doubly robust estimation of causal effects for random object outcomes with continuous treatments</title>
      <link>https://arxiv.org/abs/2506.22754</link>
      <description>arXiv:2506.22754v1 Announce Type: cross 
Abstract: Causal inference is central to statistics and scientific discovery, enabling researchers to identify cause-and-effect relationships beyond associations. While traditionally studied within Euclidean spaces, contemporary applications increasingly involve complex, non-Euclidean data structures that reside in abstract metric spaces, known as random objects, such as images, shapes, networks, and distributions. This paper introduces a novel framework for causal inference with continuous treatments applied to non-Euclidean data. To address the challenges posed by the lack of linear structures, we leverage Hilbert space embeddings of the metric spaces to facilitate Fr\'echet mean estimation and causal effect mapping. Motivated by a study on the impact of exposure to fine particulate matter on age-at-death distributions across U.S. counties, we propose a nonparametric, doubly-debiased causal inference approach for outcomes as random objects with continuous treatments. Our framework can accommodate moderately high-dimensional vector-valued confounders and derive efficient influence functions for estimation to ensure both robustness and interpretability. We establish rigorous asymptotic properties of the cross-fitted estimators and employ conformal inference techniques for counterfactual outcome prediction. Validated through numerical experiments and applied to real-world environmental data, our framework extends causal inference methodologies to complex data structures, broadening its applicability across scientific disciplines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22754v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Satarupa Bhattacharjee, Bing Li, Xiao Wu, Lingzhou Xue</dc:creator>
    </item>
    <item>
      <title>Semi-tail Units: A Universal Scale for Test Statistics and Efficiency</title>
      <link>https://arxiv.org/abs/2506.22910</link>
      <description>arXiv:2506.22910v1 Announce Type: cross 
Abstract: We introduce $\zeta$- and $s$-values as quantile-based standardizations that are particularly suited for hypothesis testing. Unlike p-values, which express tail probabilities, $s$-values measure the number of semi-tail units into a distribution's tail, where each unit represents a halving of the tail area. This logarithmic scale provides intuitive interpretation: $s=3.3$ corresponds to the 10th percentile, $s=4.3$ to the 5th percentile, and $s=5.3$ to the 2.5th percentile. For two-tailed tests, $\zeta$-values extend this concept symmetrically around the median.
  We demonstrate how these measures unify the interpretation of all test statistics on a common scale, eliminating the need for distribution-specific tables. The approach offers practical advantages: critical values follow simple arithmetic progressions, combining evidence from independent studies reduces to the addition of $s$-values, and semi-tail units provide the natural scale for expressing Bahadur slopes. This leads to a new asymptotic efficiency measure based on differences rather than ratios of slopes, where a difference of 0.15 semi-tail units means that the more efficient test moves samples 10\% farther into the tail. Through examples ranging from standardized test scores to poker hand rankings, we show how semi-tail units provide a natural and interpretable scale for quantifying extremeness in any ordered distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22910v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul W. Vos</dc:creator>
    </item>
    <item>
      <title>Confidence sequences with informative, bounded-influence priors</title>
      <link>https://arxiv.org/abs/2506.22925</link>
      <description>arXiv:2506.22925v1 Announce Type: cross 
Abstract: Confidence sequences are collections of confidence regions that simultaneously cover the true parameter for every sample size at a prescribed confidence level. Tightening these sequences is of practical interest and can be achieved by incorporating prior information through the method of mixture martingales. However, confidence sequences built from informative priors are vulnerable to misspecification and may become vacuous when the prior is poorly chosen. We study this trade-off for Gaussian observations with known variance. By combining the method of mixtures with a global informative prior whose tails are polynomial or exponential and the extended Ville's inequality, we construct confidence sequences that are sharper than their non-informative counterparts whenever the prior is well specified, yet remain bounded under arbitrary misspecification. The theory is illustrated with several classical priors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22925v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Stefano Cortinovis, Valentin Kilian, Fran\c{c}ois Caron</dc:creator>
    </item>
    <item>
      <title>Shifted Composition IV: Underdamped Langevin and Numerical Discretizations with Partial Acceleration</title>
      <link>https://arxiv.org/abs/2506.23062</link>
      <description>arXiv:2506.23062v1 Announce Type: cross 
Abstract: Quantifying the convergence rate of the underdamped Langevin dynamics (ULD) is a classical topic, in large part due to the possibility for diffusive-to-ballistic speedups -- as was recently established for the continuous-time dynamics via space-time Poincare inequalities. A central challenge for analyzing ULD is that its degeneracy necessitates the development of new analysis approaches, e.g., the theory of hypocoercivity. In this paper, we give a new coupling-based framework for analyzing ULD and its numerical discretizations. First, in the continuous-time setting, we use this framework to establish new parabolic Harnack inequalities for ULD. These are the first Harnack inequalities that decay to zero in contractive settings, thereby reflecting the convergence properties of ULD in addition to just its regularity properties.
  Second, we build upon these Harnack inequalities to develop a local error framework for analyzing discretizations of ULD in KL divergence. This extends our framework in part III from uniformly elliptic diffusions to degenerate diffusions, and shares its virtues: the framework is user-friendly, applies to sophisticated discretization schemes, and does not require contractivity. Applying this framework to the randomized midpoint discretization of ULD establishes (i) the first ballistic acceleration result for log-concave sampling (i.e., sublinear dependence on the condition number), and (ii) the first $d^{1/3}$ iteration complexity guarantee for sampling to constant total variation error in dimension $d$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23062v1</guid>
      <category>math.PR</category>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jason M. Altschuler, Sinho Chewi, Matthew S. Zhang</dc:creator>
    </item>
    <item>
      <title>Simultaneous Sieve Estimation and Inference for Time-Varying Nonlinear Time Series Regression</title>
      <link>https://arxiv.org/abs/2506.23069</link>
      <description>arXiv:2506.23069v1 Announce Type: cross 
Abstract: In this paper, we investigate time-varying nonlinear time series regression for a broad class of locally stationary time series. First, we propose sieve nonparametric estimators for the time-varying regression functions that achieve uniform consistency. Second, we develop a unified simultaneous inferential theory to conduct both structural and exact form tests on these functions. Additionally, we introduce a multiplier bootstrap procedure for practical implementation. Our methodology and theory require only mild assumptions on the regression functions, allow for unbounded domain support, and effectively address the issue of identifiability for practical interpretation. Technically, we establish sieve approximation theory for 2-D functions in unbounded domains, prove two Gaussian approximation results for affine forms of high-dimensional locally stationary time series, and calculate critical values for the maxima of the Gaussian random field arising from locally stationary time series, which may be of independent interest. Numerical simulations and two data analyses support our results, and we have developed an $\mathtt{R}$ package, $\mathtt{SIMle}$, to facilitate implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23069v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiucai Ding, Zhou Zhou</dc:creator>
    </item>
    <item>
      <title>Inertia indices of signed graphs with given cyclomatic number and given number of pendant vertices</title>
      <link>https://arxiv.org/abs/2506.23112</link>
      <description>arXiv:2506.23112v1 Announce Type: cross 
Abstract: Let $\Gamma=(G, \sigma)$ be a signed graph of order $n$ with underlying graph $G$ and a sign function $\sigma: E(G)\rightarrow \{+, -\}$. Denoted by $i_+(\Gamma)$, $\theta(\Gamma)$ and $p(\Gamma)$ the positive inertia index, the cyclomatic number and the number of pendant vertices of $\Gamma$, respectively. In this article, we prove that $i_+(\Gamma)$, $\theta(\Gamma)$ and $p(\Gamma)$ are related by the inequality $i_+(\Gamma)\geq \frac{n-p(\Gamma)}{2}-\theta(\Gamma)$. Furthermore, we completely characterize the signed graph $\Gamma$ for which $i_+(\Gamma)=\frac{n-p(\Gamma)}{2}-\theta(\Gamma)$. As a by-product, the inequalities $i_-(\Gamma)\geq \frac{n-p(\Gamma)}{2}-\theta(\Gamma)$ and $\eta(\Gamma)\leq p(\Gamma)+2\theta(\Gamma)$ are also obtained, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23112v1</guid>
      <category>math.SP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jie Pu, Fang Duan</dc:creator>
    </item>
    <item>
      <title>Breaking a Logarithmic Barrier in the Stopping Time Convergence Rate of Stochastic First-order Methods</title>
      <link>https://arxiv.org/abs/2506.23335</link>
      <description>arXiv:2506.23335v1 Announce Type: cross 
Abstract: This work provides a novel convergence analysis for stochastic optimization in terms of stopping times, addressing the practical reality that algorithms are often terminated adaptively based on observed progress. Unlike prior approaches, our analysis: 1. Directly characterizes convergence in terms of stopping times adapted to the underlying stochastic process. 2. Breaks a logarithmic barrier in existing results. Key to our results is the development of a Gr\"onwall-type argument tailored to such stochastic processes. This tool enables sharper bounds without restrictive assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23335v1</guid>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yasong Feng, Yifan Jiang, Tianyu Wang, Zhiliang Ying</dc:creator>
    </item>
    <item>
      <title>An easily verifiable dispersion order for discrete distributions</title>
      <link>https://arxiv.org/abs/2506.23677</link>
      <description>arXiv:2506.23677v1 Announce Type: cross 
Abstract: Dispersion is a fundamental concept in statistics, yet standard approaches to measuring it - especially via stochastic orders - face limitations in the discrete setting. In particular, the classical dispersive order, while well-established for continuous distributions, becomes overly restrictive when applied to discrete random variables due to support inclusion requirements. To address this, we propose a novel weak dispersive order tailored for discrete distributions. This order retains desirable properties while relaxing structural constraints, thereby broadening applicability. We further introduce a class of variability measures grounded in the notion of probability concentration, offering robust and interpretable alternatives that conform to classical axioms. Several empirical illustrations highlight the practical relevance of the proposed framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23677v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andreas Eberl, Bernhard Klar, Alfonso Su\'arez-Llorens</dc:creator>
    </item>
    <item>
      <title>Upgrading survival models with CARE</title>
      <link>https://arxiv.org/abs/2506.23870</link>
      <description>arXiv:2506.23870v1 Announce Type: cross 
Abstract: Clinical risk prediction models are regularly updated as new data, often with additional covariates, become available. We propose CARE (Convex Aggregation of relative Risk Estimators) as a general approach for combining existing "external" estimators with a new data set in a time-to-event survival analysis setting. Our method initially employs the new data to fit a flexible family of reproducing kernel estimators via penalised partial likelihood maximisation. The final relative risk estimator is then constructed as a convex combination of the kernel and external estimators, with the convex combination coefficients and regularisation parameters selected using cross-validation. We establish high-probability bounds for the $L_2$-error of our proposed aggregated estimator, showing that it achieves a rate of convergence that is at least as good as both the optimal kernel estimator and the best external model. Empirical results from simulation studies align with the theoretical results, and we illustrate the improvements our methods provide for cardiovascular disease risk modelling. Our methodology is implemented in the Python package care-survival.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23870v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>William G. Underwood, Henry W. J. Reeve, Oliver Y. Feng, Samuel A. Lambert, Bhramar Mukherjee, Richard J. Samworth</dc:creator>
    </item>
    <item>
      <title>Rothman diagrams: the geometry of association measure modification and collapsibility</title>
      <link>https://arxiv.org/abs/2506.23927</link>
      <description>arXiv:2506.23927v1 Announce Type: cross 
Abstract: Here, we outline how Rothman diagrams provide a geometric perspective that can help epidemiologists understand the relationships between effect measure modification (which we call association measure modification), collapsibility, and confounding. A Rothman diagram plots the risk of disease in the unexposed on the x-axis and the risk in the exposed on the y-axis. Crude and stratum-specific risks in the two exposure groups define points in the unit square. When there is modification of a measure of association $M$ by a covariate $C$, the stratum-specific values of $M$ differ across strata defined by $C$, so the stratum-specific points are on different contour lines of $M$. We show how collapsibility can be defined in terms of standardization instead of no confounding, and we show that a measure of association is collapsible if and only if all its contour lines are straight. We illustrate these ideas using data from a study in Newcastle, United Kingdom, where the causal effect of smoking on 20-year mortality was confounded by age. From this perspective, it is clear that association measure modification and collapsibility are logically independent of confounding. This distinction can be obscured when these concepts are taught using regression models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23927v1</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eben Kenah</dc:creator>
    </item>
    <item>
      <title>Linear relations of colored Gaussian cycles</title>
      <link>https://arxiv.org/abs/2506.23936</link>
      <description>arXiv:2506.23936v1 Announce Type: cross 
Abstract: A colored Gaussian graphical model is a linear concentration model in which equalities among the concentrations are specified by a coloring of an underlying graph. Marigliano and Davies conjectured that every linear binomial that appears in the vanishing ideal of an undirected colored cycle corresponds to a graph symmetry. We prove this conjecture for 3,5, and 7 cycles and disprove it for colored cycles of any other length. We construct the counterexamples by proving the fact that the determinant of the concentration matrices of two colored paths can be equal even when they are not identical or reflection of each other. We also explore the potential strengthening of the conjecture and prove a revised version of the conjecture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23936v1</guid>
      <category>math.CO</category>
      <category>math.AG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hannah G\"obel, Pratik Misra</dc:creator>
    </item>
    <item>
      <title>Minimax and Bayes Optimal Best-arm Identification: Adaptive Experimental Design for Treatment Choice</title>
      <link>https://arxiv.org/abs/2506.24007</link>
      <description>arXiv:2506.24007v1 Announce Type: cross 
Abstract: This study investigates adaptive experimental design for treatment choice, also known as fixed-budget best-arm identification. We consider an adaptive procedure consisting of a treatment-allocation phase followed by a treatment-choice phase, and we design an adaptive experiment for this setup to efficiently identify the best treatment arm, defined as the one with the highest expected outcome. In our designed experiment, the treatment-allocation phase consists of two stages. The first stage is a pilot phase, where we allocate each treatment arm uniformly with equal proportions to eliminate clearly suboptimal arms and estimate outcome variances. In the second stage, we allocate treatment arms in proportion to the variances estimated in the first stage. After the treatment-allocation phase, the procedure enters the treatment-choice phase, where we choose the treatment arm with the highest sample mean as our estimate of the best treatment arm. We prove that this single design is simultaneously asymptotically minimax and Bayes optimal for the simple regret, with upper bounds that match our lower bounds up to exact constants. Therefore, our designed experiment achieves the sharp efficiency limits without requiring separate tuning for minimax and Bayesian objectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.24007v1</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
    <item>
      <title>Faster Diffusion Models via Higher-Order Approximation</title>
      <link>https://arxiv.org/abs/2506.24042</link>
      <description>arXiv:2506.24042v1 Announce Type: cross 
Abstract: In this paper, we explore provable acceleration of diffusion models without any additional retraining. Focusing on the task of approximating a target data distribution in $\mathbb{R}^d$ to within $\varepsilon$ total-variation distance, we propose a principled, training-free sampling algorithm that requires only the order of
  $$ d^{1+2/K} \varepsilon^{-1/K} $$
  score function evaluations (up to log factor) in the presence of accurate scores, where $K$ is an arbitrarily large fixed integer. This result applies to a broad class of target data distributions, without the need for assumptions such as smoothness or log-concavity. Our theory is robust vis-a-vis inexact score estimation, degrading gracefully as the score estimation error increases -- without demanding higher-order smoothness on the score estimates as assumed in previous work. The proposed algorithm draws insight from high-order ODE solvers, leveraging high-order Lagrange interpolation and successive refinement to approximate the integral derived from the probability flow ODE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.24042v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gen Li, Yuchen Zhou, Yuting Wei, Yuxin Chen</dc:creator>
    </item>
    <item>
      <title>Controlling the false discovery rate under a non-parametric graphical dependence model</title>
      <link>https://arxiv.org/abs/2506.24126</link>
      <description>arXiv:2506.24126v1 Announce Type: cross 
Abstract: We propose sufficient conditions and computationally efficient procedures for false discovery rate control in multiple testing when the $p$-values are related by a known \emph{dependency graph} -- meaning that we assume independence of $p$-values that are not within each other's neighborhoods, but otherwise leave the dependence unspecified. Our methods' rejection sets coincide with that of the Benjamini--Hochberg (BH) procedure whenever there are no edges between BH rejections, and we find in simulations and a genomics data example that their power approaches that of the BH procedure when there are few such edges, as is commonly the case. Because our methods ignore all hypotheses not in the BH rejection set, they are computationally efficient whenever that set is small. Our fastest method, the IndBH procedure, typically finishes within seconds even in simulations with up to one million hypotheses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.24126v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Drew T. Nguyen, William Fithian</dc:creator>
    </item>
    <item>
      <title>Robust estimation of parameters in logistic regression via solving the Cramer-von Mises type L2 optimization problem</title>
      <link>https://arxiv.org/abs/1703.07044</link>
      <description>arXiv:1703.07044v4 Announce Type: replace 
Abstract: This paper proposes a novel method to estimate parameters in a logistic regression model. After obtaining the estimators, their asymptotic properties are rigorously investigated.</description>
      <guid isPermaLink="false">oai:arXiv.org:1703.07044v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiwoong Kim</dc:creator>
    </item>
    <item>
      <title>Asymptotic Theory of Geometric and Adaptive $k$-Means Clustering</title>
      <link>https://arxiv.org/abs/2202.13423</link>
      <description>arXiv:2202.13423v2 Announce Type: replace 
Abstract: We revisit Pollard's classical result on consistency for $k$-means clustering in Euclidean space, with a focus on extensions in two directions: first, to problems where the data may come from interesting geometric settings (e.g., Riemannian manifolds, reflexive Banach spaces, or the Wasserstein space); second, to problems where some parameters are chosen adaptively from the data (e.g., $k$-medoids or elbow-method $k$-means). Towards this end, we provide a general theory which shows that all clustering procedures described above are strongly consistent. In fact, our method of proof allows us to derive many asymptotic limit theorems beyond strong consistency. We also remove all assumptions about uniqueness of the set of optimal cluster centers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.13423v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adam Quinn Jaffe</dc:creator>
    </item>
    <item>
      <title>BELIEF in Dependence: Leveraging Atomic Linearity in Data Bits for Rethinking Generalized Linear Models</title>
      <link>https://arxiv.org/abs/2210.10852</link>
      <description>arXiv:2210.10852v3 Announce Type: replace 
Abstract: Two linearly uncorrelated binary variables must be also independent because non-linear dependence cannot manifest with only two possible states. This inherent linearity is the atom of dependency constituting any complex form of relationship. Inspired by this observation, we develop a framework called binary expansion linear effect (BELIEF) for understanding arbitrary relationships with a binary outcome. Models from the BELIEF framework are easily interpretable because they describe the association of binary variables in the language of linear models, yielding convenient theoretical insight and striking Gaussian parallels. With BELIEF, one may study generalized linear models (GLM) through transparent linear models, providing insight into how the choice of link affects modeling. For example, setting a GLM interaction coefficient to zero does not necessarily lead to the kind of no-interaction model assumption as understood under their linear model counterparts. Furthermore, for a binary response, maximum likelihood estimation for GLMs paradoxically fails under complete separation, when the data are most discriminative, whereas BELIEF estimation automatically reveals the perfect predictor in the data that is responsible for complete separation. We explore these phenomena and provide related theoretical results. We also provide preliminary empirical demonstration of some theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.10852v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Annals of Statistics 2025, Vol. 53, No. 3, 1068-1094</arxiv:journal_reference>
      <dc:creator>Benjamin Brown, Kai Zhang, Xiao-Li Meng</dc:creator>
    </item>
    <item>
      <title>Algorithmic stability implies training-conditional coverage for distribution-free prediction methods</title>
      <link>https://arxiv.org/abs/2311.04295</link>
      <description>arXiv:2311.04295v3 Announce Type: replace 
Abstract: In a supervised learning problem, given a predicted value that is the output of some trained model, how can we quantify our uncertainty around this prediction? Distribution-free predictive inference aims to construct prediction intervals around this output, with valid coverage that does not rely on assumptions on the distribution of the data or the nature of the model training algorithm. Existing methods in this area, including conformal prediction and jackknife+, offer theoretical guarantees that hold marginally (i.e., on average over a draw of training and test data). In contrast, training-conditional coverage is a stronger notion of validity that ensures predictive coverage of the test point for most draws of the training data, and is thus a more desirable property in practice. Training-conditional coverage was shown by Vovk [2012] to hold for the split conformal method, but recent work by Bian and Barber [2023] proves that such validity guarantees are not possible for the full conformal and jackknife+ methods without further assumptions. In this paper, we show that an assumption of algorithmic stability ensures that the training-conditional coverage property holds for the full conformal and jackknife+ methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.04295v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruiting Liang, Rina Foygel Barber</dc:creator>
    </item>
    <item>
      <title>Characterizing the minimax rate of nonparametric regression under bounded star-shaped constraints</title>
      <link>https://arxiv.org/abs/2401.07968</link>
      <description>arXiv:2401.07968v5 Announce Type: replace 
Abstract: We quantify the minimax rate for a nonparametric regression model over a star-shaped function class $\mathcal{F}$ with bounded diameter. We obtain a minimax rate of ${\varepsilon^{\ast}}^2\wedge\mathrm{diam}(\mathcal{F})^2$ where \[\varepsilon^{\ast} =\sup\{\varepsilon\ge 0:n\varepsilon^2 \le \log M_{\mathcal{F}}^{\operatorname{loc}}(\varepsilon,c)\},\] where $\log M_{\mathcal{F}}^{\operatorname{loc}}(\cdot, c)$ is the local metric entropy of $\mathcal{F}$, $c$ is some absolute constant scaling down the entropy radius, and our loss function is the squared population $L_2$ distance over our input space $\mathcal{X}$. In contrast to classical works on the topic [cf. Yang and Barron, 1999], our results do not require functions in $\mathcal{F}$ to be uniformly bounded in sup-norm. In fact, we propose a condition that simultaneously generalizes boundedness in sup-norm and the so-called $L$-sub-Gaussian assumption that appears in the prior literature. In addition, we prove that our estimator is adaptive to the true point in the convex-constrained case, and to the best of our knowledge this is the first such estimator in this general setting. This work builds on the Gaussian sequence framework of Neykov [2022] using a similar algorithmic scheme to achieve the minimax rate. Our algorithmic rate also applies with sub-Gaussian noise. We illustrate the utility of this theory with examples including multivariate monotone functions, linear functionals over ellipsoids, and Lipschitz classes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.07968v5</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akshay Prasadan, Matey Neykov</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Privatized Data with Unknown Sample Size</title>
      <link>https://arxiv.org/abs/2406.06231</link>
      <description>arXiv:2406.06231v2 Announce Type: replace 
Abstract: We develop both theory and algorithms to analyze privatized data in the unbounded differential privacy(DP), where even the sample size is considered a sensitive quantity that requires privacy protection. We show that the distance between the sampling distributions under unbounded DP and bounded DP goes to zero as the sample size $n$ goes to infinity, provided that the noise used to privatize $n$ is at an appropriate rate; we also establish that Approximate Bayesian Computation (ABC)-type posterior distributions converge under similar assumptions. We further give asymptotic results in the regime where the privacy budget for $n$ goes to zero, establishing similarity of sampling distributions as well as showing that the MLE in the unbounded setting converges to the bounded-DP MLE. In order to facilitate valid, finite-sample Bayesian inference on privatized data in the unbounded DP setting, we propose a reversible jump MCMC algorithm which extends the data augmentation MCMC of Ju et al. (2022). We also propose a Monte Carlo EM algorithm to compute the MLE from privatized data in both bounded and unbounded DP. We apply our methodology to analyze a linear regression model as well as a 2019 American Time Use Survey Microdata File which we model using a Dirichlet distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06231v2</guid>
      <category>math.ST</category>
      <category>cs.CR</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jordan Awan, Andres Felipe Barrientos, Nianqiao Ju</dc:creator>
    </item>
    <item>
      <title>Optimal Estimation of Structured Covariance Operators</title>
      <link>https://arxiv.org/abs/2408.02109</link>
      <description>arXiv:2408.02109v2 Announce Type: replace 
Abstract: This paper establishes optimal convergence rates for estimation of structured covariance operators of Gaussian processes. We study banded operators with kernels that decay rapidly off-the-diagonal and $L^q$-sparse operators with an unordered sparsity pattern. For these classes of operators, we find the minimax optimal rate of estimation in operator norm, identifying the fundamental dimension-free quantities that determine the sample complexity. In addition, we prove that tapering and thresholding estimators attain the optimal rate. The proof of the upper bound for tapering estimators requires novel techniques to circumvent the issue that discretization of a banded operator does not result, in general, in a banded covariance matrix. To derive lower bounds for banded and $L^q$-sparse classes, we introduce a general framework to lift theory from high-dimensional matrix estimation to the operator setting. Our work contributes to the growing literature on operator estimation and learning, building on ideas from high-dimensional statistics while also addressing new challenges that emerge in infinite dimension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02109v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Omar Al-Ghattas, Jiaheng Chen, Daniel Sanz-Alonso, Nathan Waniorek</dc:creator>
    </item>
    <item>
      <title>Distribution of singular values in large sample cross-covariance matrices</title>
      <link>https://arxiv.org/abs/2502.05254</link>
      <description>arXiv:2502.05254v3 Announce Type: replace 
Abstract: For two large matrices ${\mathbf X}$ and ${\mathbf Y}$ with Gaussian i.i.d.\ entries and dimensions $T\times N_X$ and $T\times N_Y$, respectively, we derive the probability distribution of the singular values of $\mathbf{X}^T \mathbf{Y}$ in different parameter regimes. This extends the Marchenko-Pastur result for the distribution of eigenvalues of empirical sample covariance matrices to singular values of empirical cross-covariances. Our results will help to establish statistical significance of cross-correlations in many data-science applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05254v3</guid>
      <category>math.ST</category>
      <category>cond-mat.dis-nn</category>
      <category>physics.data-an</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arabind Swain, Sean Alexander Ridout, Ilya Nemenman</dc:creator>
    </item>
    <item>
      <title>Testing Thresholds and Spectral Properties of High-Dimensional Random Toroidal Graphs via Edgeworth-Style Expansions</title>
      <link>https://arxiv.org/abs/2502.18346</link>
      <description>arXiv:2502.18346v2 Announce Type: replace 
Abstract: We study high-dimensional random geometric graphs (RGGs) of edge-density $p$ with vertices uniformly distributed on the $d$-dimensional torus and edges inserted between sufficiently close vertices with respect to an $L_q$-norm. We focus on distinguishing an RGG from an Erd\H{o}s--R\'enyi (ER) graph if both models have edge probability $p$. So far, most results considered either spherical RGGs with $L_2$-distance or toroidal RGGs under $L_\infty$-distance. However, for general $L_q$-distances, many questions remain open, especially if $p$ is allowed to depend on $n$. The main reason for this is that RGGs under $L_q$-distances can not easily be represented as the logical AND of their 1-dimensional counterparts, as for $L_\infty$ geometries. To overcome this, we devise a novel technique for quantifying the dependence between edges based on modified Edgeworth expansions.
  Our technique yields the first tight algorithmic upper bounds for distinguishing toroidal RGGs under general $L_q$ norms from ER-graphs for fixed $p$ and $q$. We achieve this by showing that signed triangles can distinguish the two models when $d\ll n^3p^3$ for the whole regime of $c/n&lt;p&lt;1$. Additionally, our technique yields an improved information-theoretic lower bound for this task, showing that the two distributions converge whenever $d=\tilde{\Omega}(n^3p^2)$, which is just as strong as the currently best known lower bound for spherical RGGs in case of general $p$ from Liu et al. [STOC'22]. Finally, our expansions allow us to tightly characterize the spectral properties of toroidal RGGs both under $L_q$-distances for fixed $1\le q&lt;\infty$, and $L_\infty$-distance. Our results partially resolve a conjecture of Bangachev and Bresler [COLT'24] and prove that the distance metric, rather than the underlying space, is responsible for the observed differences in the behavior of spherical and toroidal RGGs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18346v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel Baguley, Andreas G\"obel, Marcus Pappik, Leon Schiller</dc:creator>
    </item>
    <item>
      <title>E-variables for hypotheses generated by constraints</title>
      <link>https://arxiv.org/abs/2504.02974</link>
      <description>arXiv:2504.02974v3 Announce Type: replace 
Abstract: E-variables are nonnegative random variables with expected value at most one under any distribution from a given null hypothesis. E-variables have been recently recognized as fundamental objects in hypothesis testing, and a key open problem is to characterize their form. We provide a complete solution to this problem for hypotheses generated by constraints, a broad and natural framework that encompasses many hypothesis classes occurring in practice. Our main result is an abstract representation theorem that describes all e-variables for any hypothesis defined by an arbitrary collection of measurable constraints. We instantiate this general theory for three important classes: hypotheses generated by finitely many constraints, one-sided sub-$\psi$ distributions (including sub-Gaussian distributions), and distributions constrained by group symmetries. In each case, we explicitly characterize all e-variables as well as all admissible e-variables. Building on these results we prove existence and uniqueness of optimal e-variables under a large class of expected utility-based objective functions, covering all criteria studied in the e-variable literature to date.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02974v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Larsson, Aaditya Ramdas, Johannes Ruf</dc:creator>
    </item>
    <item>
      <title>Multi-Environment GLAMP: Approximate Message Passing for Transfer Learning with Applications to Lasso-based Estimators</title>
      <link>https://arxiv.org/abs/2505.22594</link>
      <description>arXiv:2505.22594v2 Announce Type: replace 
Abstract: Approximate Message Passing (AMP) algorithms enable precise characterization of certain classes of random objects in the high-dimensional limit, and have found widespread applications in fields such as signal processing, statistics, and communications. In this work, we introduce Multi-Environment Generalized Long AMP, a novel AMP framework that applies to transfer learning problems with multiple data sources and distribution shifts. We rigorously establish state evolution for multi-environment GLAMP. We demonstrate the utility of this framework by precisely characterizing the risk of three Lasso-based transfer learning estimators for the first time: the Stacked Lasso, the Model Averaging Estimator, and the Second Step Estimator. We also demonstrate the remarkable finite sample accuracy of our theory via extensive simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22594v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Longlin Wang, Yanke Song, Kuanhao Jiang, Pragya Sur</dc:creator>
    </item>
    <item>
      <title>What Makes Treatment Effects Identifiable? Characterizations and Estimators Beyond Unconfoundedness</title>
      <link>https://arxiv.org/abs/2506.04194</link>
      <description>arXiv:2506.04194v2 Announce Type: replace 
Abstract: Most of the widely used estimators of the average treatment effect (ATE) in causal inference rely on the assumptions of unconfoundedness and overlap. Unconfoundedness requires that the observed covariates account for all correlations between the outcome and treatment. Overlap requires the existence of randomness in treatment decisions for all individuals. Nevertheless, many types of studies frequently violate unconfoundedness or overlap, for instance, observational studies with deterministic treatment decisions - popularly known as Regression Discontinuity designs - violate overlap.
  In this paper, we initiate the study of general conditions that enable the identification of the average treatment effect, extending beyond unconfoundedness and overlap. In particular, following the paradigm of statistical learning theory, we provide an interpretable condition that is sufficient and necessary for the identification of ATE. Moreover, this condition also characterizes the identification of the average treatment effect on the treated (ATT) and can be used to characterize other treatment effects as well. To illustrate the utility of our condition, we present several well-studied scenarios where our condition is satisfied and, hence, we prove that ATE can be identified in regimes that prior works could not capture. For example, under mild assumptions on the data distributions, this holds for the models proposed by Tan (2006) and Rosenbaum (2002), and the Regression Discontinuity design model introduced by Thistlethwaite and Campbell (1960). For each of these scenarios, we also show that, under natural additional assumptions, ATE can be estimated from finite samples.
  We believe these findings open new avenues for bridging learning-theoretic insights and causal inference methodologies, particularly in observational studies with complex treatment mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04194v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Cai, Alkis Kalavasis, Katerina Mamali, Anay Mehrotra, Manolis Zampetakis</dc:creator>
    </item>
    <item>
      <title>Extended UCB Policies for Frequentist Multi-armed Bandit Problems</title>
      <link>https://arxiv.org/abs/1112.1768</link>
      <description>arXiv:1112.1768v4 Announce Type: replace-cross 
Abstract: The multi-armed bandit (MAB) problem is a widely studied model in the field of operations research for sequential decision making and reinforcement learning. This paper mainly considers the classical MAB model with the heavy-tailed reward distributions. We introduce the extended robust UCB policy, which is an extension of the pioneering UCB policies proposed by Bubeck et al. [5] and Lattimore [22]. The previous UCB policies require some strict conditions on the reward distributions, which can be hard to guarantee in practical scenarios. Our extended robust UCB generalizes Lattimore's seminary work (for moments of orders $p=4$ and $q=2$) to arbitrarily chosen $p&gt;q&gt;1$ as long as the two moments have a known controlled relationship, while still achieving the optimal regret growth order $O(log T)$, thus providing a broadened application area of the UCB policies for the heavy-tailed reward distributions. Furthermore, we achieve a near-optimal regret order without any knowledge of the reward distributions as long as their $p$-th moments exist for some $p&gt;1$.</description>
      <guid isPermaLink="false">oai:arXiv.org:1112.1768v4</guid>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keqin Liu, Tianshuo Zheng, Zhi-Hua Zhou</dc:creator>
    </item>
    <item>
      <title>A Nonparametric Maximum Likelihood Approach to Mixture of Regression</title>
      <link>https://arxiv.org/abs/2108.09816</link>
      <description>arXiv:2108.09816v2 Announce Type: replace-cross 
Abstract: We study mixture of linear regression (random coefficient) models, which capture population heterogeneity by allowing the regression coefficients to follow an unknown distribution $G^*$. In contrast to common parametric methods that fix the mixing distribution form and rely on the EM algorithm, we develop a fully nonparametric maximum likelihood estimator (NPMLE). We show that this estimator exists under broad conditions and can be computed via a discrete approximation procedure inspired by the exemplar method. We further establish theoretical guarantees demonstrating that the NPMLE achieves near-parametric rates in estimating the conditional density of $Y|X$, both for fixed and random designs, when $\sigma$ is known and $G^*$ has compact support. In the random design setting, we also prove consistency of the estimated mixing distribution in the L\'evy-Prokhorov distance. Numerical experiments indicate that our approach performs well and additionally enables posterior-based individualized coefficient inference through an empirical Bayes framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2108.09816v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hansheng Jiang, Adityanand Guntuboyina</dc:creator>
    </item>
    <item>
      <title>Symmetry Lie Algebras of Varieties with Applications to Algebraic Statistics</title>
      <link>https://arxiv.org/abs/2309.10741</link>
      <description>arXiv:2309.10741v4 Announce Type: replace-cross 
Abstract: The motivation for this paper is to detect when an irreducible projective variety V is not toric. We do this by analyzing a Lie group and a Lie algebra associated to V. If the dimension of V is strictly less than the dimension of the above mentioned objects, then V is not a toric variety. We provide an algorithm to compute the Lie algebra of an irreducible variety and use it to provide examples of non-toric statistical models in algebraic statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.10741v4</guid>
      <category>math.AG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Aida Maraj, Arpan Pal</dc:creator>
    </item>
    <item>
      <title>Aligning Multiple Inhomogeneous Random Graphs: Fundamental Limits of Exact Recovery</title>
      <link>https://arxiv.org/abs/2405.12293</link>
      <description>arXiv:2405.12293v2 Announce Type: replace-cross 
Abstract: This work studies fundamental limits for recovering the underlying correspondence among multiple correlated graphs. In the setting of inhomogeneous random graphs, we present and analyze a matching algorithm: first partially match the graphs pairwise and then combine the partial matchings by transitivity. Our analysis yields a sufficient condition on the problem parameters to exactly match all nodes across all the graphs. In the setting of homogeneous (Erd\H{o}s-R\'enyi) graphs, we show that this condition is also necessary, i.e. the algorithm works down to the information theoretic threshold. This reveals a scenario where exact matching between two graphs alone is impossible, but leveraging more than two graphs allows exact matching among all the graphs. Converse results are also given in the inhomogeneous setting and transitivity again plays a role. Along the way, we derive independent results about the k-core of inhomogeneous random graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12293v2</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Taha Ameen, Bruce Hajek</dc:creator>
    </item>
    <item>
      <title>A General Framework on Conditions for Constraint-based Causal Learning</title>
      <link>https://arxiv.org/abs/2408.07575</link>
      <description>arXiv:2408.07575v2 Announce Type: replace-cross 
Abstract: Most constraint-based causal learning algorithms provably return the correct causal graph under certain correctness conditions, such as faithfulness. By representing any constraint-based causal learning algorithm using the notion of a property, we provide a general framework to obtain and study correctness conditions for these algorithms. From the framework, we provide exact correctness conditions for the PC algorithm, which are then related to the correctness conditions of some other existing causal discovery algorithms. The framework also suggests a paradigm for designing causal learning algorithms which allows for the correctness conditions of algorithms to be controlled for before designing the actual algorithm, and has the following implications. We show that the sparsest Markov representation condition is the weakest correctness condition for algorithms that output ancestral graphs or directed acyclic graphs satisfying any existing notions of minimality. We also reason that Pearl-minimality is necessary for meaningful causal learning but not sufficient to relax the faithfulness condition and, as such, has to be strengthened, such as by including background knowledge, for causal learning beyond faithfulness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07575v2</guid>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Kai Z. Teh, Kayvan Sadeghi, Terry Soo</dc:creator>
    </item>
    <item>
      <title>Neymanian inference in randomized experiments</title>
      <link>https://arxiv.org/abs/2409.12498</link>
      <description>arXiv:2409.12498v2 Announce Type: replace-cross 
Abstract: In his seminal 1923 work, Neyman studied the variance estimation problem for the difference-in-means estimator of the average treatment effect in completely randomized experiments. He proposed a variance estimator that is conservative in general and unbiased under homogeneous treatment effects. While widely used under complete randomization, there is no unique or natural way to extend this estimator to more complex designs. To this end, we show that Neyman's estimator can be alternatively derived in two ways, leading to two novel variance estimation approaches: the imputation approach and the contrast approach. While both approaches recover Neyman's estimator under complete randomization, they yield fundamentally different variance estimators for more general designs. In the imputation approach, the variance is expressed in terms of observed and missing potential outcomes and then estimated by imputing the missing potential outcomes, akin to Fisherian inference. In the contrast approach, the variance is expressed in terms of unobservable contrasts of potential outcomes and then estimated by exchanging each unobservable contrast with an observable contrast. We examine the properties of both approaches, showing that for a large class of designs, each produces non-negative, conservative variance estimators that are unbiased in finite samples or asymptotically under homogeneous treatment effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12498v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ambarish Chattopadhyay, Guido W. Imbens</dc:creator>
    </item>
    <item>
      <title>Optimization, Isoperimetric Inequalities, and Sampling via Lyapunov Potentials</title>
      <link>https://arxiv.org/abs/2410.02979</link>
      <description>arXiv:2410.02979v4 Announce Type: replace-cross 
Abstract: In this paper, we prove that optimizability of any function F using Gradient Flow from all initializations implies a Poincar\'e Inequality for Gibbs measures mu_{beta} = e^{-beta F}/Z at low temperature. In particular, under mild regularity assumptions on the convergence rate of Gradient Flow, we establish that mu_{beta} satisfies a Poincar\'e Inequality with constant O(C'+1/beta) for beta &gt;= Omega(d), where C' is the Poincar\'e constant of mu_{beta} restricted to a neighborhood of the global minimizers of F. Under an additional mild condition on F, we show that mu_{beta} satisfies a Log-Sobolev Inequality with constant O(beta max(S, 1) max(C', 1)) where S denotes the second moment of mu_{beta}. Here asymptotic notation hides F-dependent parameters. At a high level, this establishes that optimizability via Gradient Flow from every initialization implies a Poincar\'e and Log-Sobolev Inequality for the low-temperature Gibbs measure, which in turn imply sampling from all initializations.
  Analogously, we establish that under the same assumptions, if F can be initialized from everywhere except some set S, then mu_{beta} satisfies a Weak Poincar\'e Inequality with parameters (O(C'+1/beta), O(mu_{beta}(S))) for \beta = Omega(d). At a high level, this shows while optimizability from 'most' initializations implies a Weak Poincar\'e Inequality, which in turn implies sampling from suitable warm starts. Our regularity assumptions are mild and as a consequence, we show we can efficiently sample from several new natural and interesting classes of non-log-concave densities, an important setting with relatively few examples. As another corollary, we obtain efficient discrete-time sampling results for log-concave measures satisfying milder regularity conditions than smoothness, similar to Lehec (2023).</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02979v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>August Y. Chen, Karthik Sridharan</dc:creator>
    </item>
    <item>
      <title>Doubly robust inference via calibration</title>
      <link>https://arxiv.org/abs/2411.02771</link>
      <description>arXiv:2411.02771v2 Announce Type: replace-cross 
Abstract: Doubly robust estimators are widely used for estimating average treatment effects and other linear summaries of regression functions. While consistency requires only one of two nuisance functions to be estimated consistently, asymptotic normality typically require sufficiently fast convergence of both. In this work, we correct this mismatch: we show that calibrating the nuisance estimators within a doubly robust procedure yields doubly robust asymptotic normality for linear functionals. We introduce a general framework, calibrated debiased machine learning (calibrated DML), and propose a specific estimator that augments standard DML with a simple isotonic regression adjustment. Our theoretical analysis shows that the calibrated DML estimator remains asymptotically normal if either the regression or the Riesz representer of the functional is estimated sufficiently well, allowing the other to converge arbitrarily slowly or even inconsistently. We further propose a simple bootstrap method for constructing confidence intervals, enabling doubly robust inference without additional nuisance estimation. In a range of semi-synthetic benchmark datasets, calibrated DML reduces bias and improves coverage relative to standard DML. Our method can be integrated into existing DML pipelines by adding just a few lines of code to calibrate cross-fitted estimates via isotonic regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02771v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lars van der Laan, Alex Luedtke, Marco Carone</dc:creator>
    </item>
    <item>
      <title>Mixing Time of the Proximal Sampler in Relative Fisher Information via Strong Data Processing Inequality</title>
      <link>https://arxiv.org/abs/2502.05623</link>
      <description>arXiv:2502.05623v2 Announce Type: replace-cross 
Abstract: We study the mixing time guarantee for sampling in relative Fisher information via the Proximal Sampler algorithm, which is an approximate proximal discretization of the Langevin dynamics. We show that when the target probability distribution is strongly log-concave, the relative Fisher information converges exponentially fast along the Proximal Sampler; this matches the exponential convergence rate of the relative Fisher information along the continuous-time Langevin dynamics for strongly log-concave target. When combined with a standard implementation of the Proximal Sampler via rejection sampling, this exponential convergence rate provides a high-accuracy iteration complexity guarantee for the Proximal Sampler in relative Fisher information when the target distribution is strongly log-concave and log-smooth. Our proof proceeds by establishing a strong data processing inequality for relative Fisher information along the Gaussian channel under strong log-concavity, and a data processing inequality along the reverse Gaussian channel for a special distribution. The forward and reverse Gaussian channels compose to form the Proximal Sampler, and these data processing inequalities imply the exponential convergence rate of the relative Fisher information along the Proximal Sampler.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05623v2</guid>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andre Wibisono</dc:creator>
    </item>
    <item>
      <title>Recovering a (1+1)-dimensional wave equation from a single white noise boundary measurement</title>
      <link>https://arxiv.org/abs/2503.18515</link>
      <description>arXiv:2503.18515v2 Announce Type: replace-cross 
Abstract: We consider the following inverse problem: Suppose a $(1+1)$-dimensional wave equation on $\mathbb{R}_+$ with zero initial conditions is excited with a Neumann boundary data modelled as a white noise process. Given also the Dirichlet data at the same point, determine the unknown first order coefficient function of the system.
  We first establish that direct problem is well-posed. The inverse problem is then solved by showing that correlations of the boundary data determine the Neumann-to-Dirichlet operator in the sense of distributions, which is known to uniquely identify the coefficient. This approach has applications in acoustic measurements of internal cross-sections of fluid pipes such as pressurised water supply pipes and vocal tract shape determination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18515v2</guid>
      <category>math.AP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emilia L. K. Bl{\aa}sten, Tapio Helin, Antti Kujanp\"a\"a, Lauri Oksanen, Jesse Railo</dc:creator>
    </item>
    <item>
      <title>Exploratory Hierarchical Factor Analysis with an Application to Psychological Measurement</title>
      <link>https://arxiv.org/abs/2505.09043</link>
      <description>arXiv:2505.09043v2 Announce Type: replace-cross 
Abstract: Hierarchical factor models, which include the bifactor model as a special case, are useful in social and behavioural sciences for measuring hierarchically structured constructs. Specifying a hierarchical factor model involves imposing hierarchically structured zero constraints on a factor loading matrix, which is often challenging. Therefore, an exploratory analysis is needed to learn the hierarchical factor structure from data. Unfortunately, there does not exist an identifiability theory for the learnability of this hierarchical structure and a computationally efficient method with provable performance. The method of Schmid-Leiman transformation, which is often regarded as the default method for exploratory hierarchical factor analysis, is flawed and likely to fail. The contribution of this paper is three-fold. First, an identifiability result is established for general hierarchical factor models, which shows that the hierarchical factor structure is learnable under mild regularity conditions. Second, a computationally efficient divide-and-conquer approach is proposed for learning the hierarchical factor structure. Finally, asymptotic theory is established for the proposed method, showing that it can consistently recover the true hierarchical factor structure as the sample size grows to infinity. The power of the proposed method is shown via simulation studies and a real data application to a personality test. The computation code for the proposed method is publicly available at https://anonymous.4open.science/r/Exact-Exploratory-Hierarchical-Factor-Analysis-F850.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09043v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiawei Qiao, Yunxiao Chen, Zhiliang Ying</dc:creator>
    </item>
    <item>
      <title>Constrained Denoising, Empirical Bayes, and Optimal Transport</title>
      <link>https://arxiv.org/abs/2506.09986</link>
      <description>arXiv:2506.09986v2 Announce Type: replace-cross 
Abstract: In the statistical problem of denoising, Bayes and empirical Bayes methods can "overshrink" their output relative to the latent variables of interest. This work is focused on constrained denoising problems which mitigate such phenomena. At the oracle level, i.e., when the latent variable distribution is assumed known, we apply tools from the theory of optimal transport to characterize the solution to (i) variance-constrained, (ii) distribution-constrained, and (iii) general-constrained denoising problems. At the empirical level, i.e., when the latent variable distribution is not known, we use empirical Bayes methodology to estimate these oracle denoisers. Our approach is modular, and transforms any suitable (unconstrained) empirical Bayes denoiser into a constrained empirical Bayes denoiser. We prove explicit rates of convergence for our proposed methodologies, which both extend and sharpen existing asymptotic results that have previously considered only variance constraints. We apply our methodology in two applications: one in astronomy concerning the relative chemical abundances in a large catalog of red-clump stars, and one in baseball concerning minor- and major league batting skill for rookie players.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09986v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adam Quinn Jaffe, Nikolaos Ignatiadis, Bodhisattva Sen</dc:creator>
    </item>
  </channel>
</rss>
