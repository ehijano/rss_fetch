<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 31 Dec 2024 05:00:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Sychronous vs. asynchronous coalitions in multiplayer games, with applications to guts poker</title>
      <link>https://arxiv.org/abs/2412.19855</link>
      <description>arXiv:2412.19855v1 Announce Type: new 
Abstract: We study the issue introduced by Buck-Lee-Platnick-Wheeler-Zumbrun of synchronous vs. asynchronous coalitions in multiplayer games, that is, the difference between coalitions with full and partial communication, with a specific interest in the context of continuous Guts poker where this problem was originally formulated. We observe for general symmetric multiplayer games, with players 2-n in coalition against player 1, that there are three values, corresponding to symmetric Nash equilibrium, optimal asynchronous, and optimal synchronous strategies, in that order, for which inequalities may for different examples be strict or nonstrict (i.e., equality) in any combination. Different from Nash equilibria and synchronous optima, which may be phrased as convex optimization problems, or classical 2-player games, determination of asynchronous optima is a nonconvex optimization problem. We discuss methods of numerical approximation of this optimum, and examine performance on 3-player rock-paper-scissors and discretized Guts poker. Finally, we present sufficient conditions guaranteeing different possibilities for behavior, based on concave/convexity properties of the payoff function. These answer in the affirmative the open problem posed by Buck-Lee-Platnick-Wheeler-Zumbrun whether the optimal asynchronous coalition value for 3-player guts is equal to the Nash equilibrium value zero. At the same time, we present a number of new results regarding synchronous coalition play for continuous $3$-player guts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19855v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jessica Babyak, Kevin Buck, Leah Dichter, David Jiang, Kevin Zumbrun</dc:creator>
    </item>
    <item>
      <title>The Degree of (Extended) Justified Representation and Its Optimization</title>
      <link>https://arxiv.org/abs/2412.19933</link>
      <description>arXiv:2412.19933v1 Announce Type: new 
Abstract: Justified Representation (JR)/Extended Justified Representation (EJR) is a desirable axiom in multiwinner approval voting. In contrast to (E)JR only requires at least \emph{one} voter to be represented in every cohesive group, we study its optimization version that maximizes the \emph{number} of represented voters in each group. Given an instance, we say a winning committee provides an (E)JR degree of $c$ if at least $c$ voters in each $\ell$-cohesive group have approved $\ell$ winning candidates. Hence, every (E)JR committee provides the (E)JR degree of at least $1$. Besides proposing this new property, we propose the optimization problem of finding a winning committee that achieves the maximum possible (E)JR degree, called MDJR and MDEJR, corresponding to JR and EJR respectively.
  We study the computational complexity and approximability of MDJR of MDEJR. An (E)JR committee, which can be found in polynomial time, straightforwardly gives a $(k/n)$-approximation. On the other hand, we show that it is NP-hard to approximate MDJR and MDEJR to within a factor of $\left(k/n\right)^{1-\epsilon}$, for any $\epsilon&gt;0$, which complements the approximation. Next, we study the fixed-parameter-tractability of this problem. We show that both problems are W[2]-hard if $k$, the size of the winning committee, is specified as the parameter. However, when $c_{\text{max}}$, the maximum value of $c$ such that a committee that provides an (E)JR degree of $c$ exists, is additionally given as a parameter, we show that both MDJR and MDEJR are fixed-parameter-tractable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19933v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Biaoshuai Tao, Chengkai Zhang, Houyu Zhou</dc:creator>
    </item>
    <item>
      <title>Multi-agent reinforcement learning in the all-or-nothing public goods game on networks</title>
      <link>https://arxiv.org/abs/2412.20116</link>
      <description>arXiv:2412.20116v1 Announce Type: new 
Abstract: We study interpersonal trust by means of the all-or-nothing public goods game between agents on a network. The agents are endowed with the simple yet adaptive learning rule, exponential moving average, by which they estimate the behavior of their neighbors in the network. Theoretically we show that in the long-time limit this multi-agent reinforcement learning process always eventually results in indefinite contribution to the public good or indefinite defection (no agent contributing to the public good). However, by simulation of the pre-limit behavior, we see that on complex network structures there may be mixed states in which the process seems to stabilize before actual convergence to states in which agent beliefs and actions are all the same. In these metastable states the local network characteristics can determine whether agents have high or low trust in their neighbors. More generally it is found that more dense networks result in lower rates of contribution to the public good. This has implications for how one can spread global contribution toward a public good by enabling smaller local interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20116v1</guid>
      <category>cs.GT</category>
      <category>physics.soc-ph</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Benedikt Valentin Meylahn</dc:creator>
    </item>
    <item>
      <title>No-regret learning in harmonic games: Extrapolation in the face of conflicting interests</title>
      <link>https://arxiv.org/abs/2412.20203</link>
      <description>arXiv:2412.20203v1 Announce Type: new 
Abstract: The long-run behavior of multi-agent learning - and, in particular, no-regret learning - is relatively well-understood in potential games, where players have aligned interests. By contrast, in harmonic games - the strategic counterpart of potential games, where players have conflicting interests - very little is known outside the narrow subclass of 2-player zero-sum games with a fully-mixed equilibrium. Our paper seeks to partially fill this gap by focusing on the full class of (generalized) harmonic games and examining the convergence properties of follow-the-regularized-leader (FTRL), the most widely studied class of no-regret learning schemes. As a first result, we show that the continuous-time dynamics of FTRL are Poincar\'e recurrent, that is, they return arbitrarily close to their starting point infinitely often, and hence fail to converge. In discrete time, the standard, "vanilla" implementation of FTRL may lead to even worse outcomes, eventually trapping the players in a perpetual cycle of best-responses. However, if FTRL is augmented with a suitable extrapolation step - which includes as special cases the optimistic and mirror-prox variants of FTRL - we show that learning converges to a Nash equilibrium from any initial condition, and all players are guaranteed at most O(1) regret. These results provide an in-depth understanding of no-regret learning in harmonic games, nesting prior work on 2-player zero-sum games, and showing at a high level that harmonic games are the canonical complement of potential games, not only from a strategic, but also from a dynamic viewpoint.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20203v1</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <category>math.OC</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Davide Legacci, Panayotis Mertikopoulos, Christos H. Papadimitriou, Georgios Piliouras, Bary S. R. Pradelski</dc:creator>
    </item>
    <item>
      <title>Efficient Learning and Computation of Linear Correlated Equilibrium in General Convex Games</title>
      <link>https://arxiv.org/abs/2412.20291</link>
      <description>arXiv:2412.20291v1 Announce Type: new 
Abstract: We propose efficient no-regret learning dynamics and ellipsoid-based methods for computing linear correlated equilibria$\unicode{x2014}$a relaxation of correlated equilibria and a strengthening of coarse correlated equilibria$\unicode{x2014}$in general convex games. These are games where the number of pure strategies is potentially exponential in the natural representation of the game, such as extensive-form games. Our work identifies linear correlated equilibria as the tightest known notion of equilibrium that is computable in polynomial time and is efficiently learnable for general convex games. Our results are enabled by a generalization of the seminal framework of of Gordon et al. [2008] for $\Phi$-regret minimization, providing extensions to this framework that can be used even when the set of deviations $\Phi$ is intractable to separate/optimize over. Our polynomial-time algorithms are similarly enabled by extending the Ellipsoid-Against-Hope approach of Papadimitriou and Roughgarden [2008] and its generalization to games of non-polynomial type proposed by Farina and Pipis [2024a]. We provide an extension to these approaches when we do not have access to the separation oracles required by these works for the dual player.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20291v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Constantinos Daskalakis, Gabriele Farina, Maxwell Fishelson, Charilaos Pipis, Jon Schneider</dc:creator>
    </item>
    <item>
      <title>Accelerated regularized learning in finite N-person games</title>
      <link>https://arxiv.org/abs/2412.20365</link>
      <description>arXiv:2412.20365v1 Announce Type: new 
Abstract: Motivated by the success of Nesterov's accelerated gradient algorithm for convex minimization problems, we examine whether it is possible to achieve similar performance gains in the context of online learning in games. To that end, we introduce a family of accelerated learning methods, which we call "follow the accelerated leader" (FTXL), and which incorporates the use of momentum within the general framework of regularized learning - and, in particular, the exponential/multiplicative weights algorithm and its variants. Drawing inspiration and techniques from the continuous-time analysis of Nesterov's algorithm, we show that FTXL converges locally to strict Nash equilibria at a superlinear rate, achieving in this way an exponential speed-up over vanilla regularized learning methods (which, by comparison, converge to strict equilibria at a geometric, linear rate). Importantly, FTXL maintains its superlinear convergence rate in a broad range of feedback structures, from deterministic, full information models to stochastic, realization-based ones, and even when run with bandit, payoff-based information, where players are only able to observe their individual realized payoffs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20365v1</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kyriakos Lotidis, Angeliki Giannou, Panayotis Mertikopoulos, Nicholas Bambos</dc:creator>
    </item>
    <item>
      <title>Convergence of the Min-Max Langevin Dynamics and Algorithm for Zero-Sum Games</title>
      <link>https://arxiv.org/abs/2412.20471</link>
      <description>arXiv:2412.20471v1 Announce Type: new 
Abstract: We study zero-sum games in the space of probability distributions over the Euclidean space $\mathbb{R}^d$ with entropy regularization, in the setting when the interaction function between the players is smooth and strongly convex-concave. We prove an exponential convergence guarantee for the mean-field min-max Langevin dynamics to compute the equilibrium distribution of the zero-sum game. We also study the finite-particle approximation of the mean-field min-max Langevin dynamics, both in continuous and discrete times. We prove biased convergence guarantees for the continuous-time finite-particle min-max Langevin dynamics to the stationary mean-field equilibrium distribution with an explicit bias estimate which does not scale with the number of particles. We also prove biased convergence guarantees for the discrete-time finite-particle min-max Langevin algorithm to the stationary mean-field equilibrium distribution with an additional bias term which scales with the step size and the number of particles. This provides an explicit iteration complexity for the average particle along the finite-particle algorithm to approximately compute the equilibrium distribution of the zero-sum game.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20471v1</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yang Cai, Siddharth Mitra, Xiuyuan Wang, Andre Wibisono</dc:creator>
    </item>
    <item>
      <title>Incentive-Compatible Collusion-Resistance via Posted Prices</title>
      <link>https://arxiv.org/abs/2412.20853</link>
      <description>arXiv:2412.20853v1 Announce Type: new 
Abstract: We consider a refinement to the notions of collusion-resistance in transaction fee mechanisms. In particular, we require that the collusion is by itself incentive-compatible and individually rational to all of its participants. We then study the structural properties of these notions, and importantly, characterize the class of collusion-resistant and incentive-compatible transaction fee mechanisms in the single bidder case, and show that this is exactly the class of posted-price where the price is not too prohibitive. We analyze welfare and revenue implications, as well as the shape of the solution space, for both regular and non-regular distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20853v1</guid>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matheus V. X. Ferreira, Yotam Gafni, Max Resnick</dc:creator>
    </item>
    <item>
      <title>A market-based efficient matching mechanism for crowdsourced delivery systems with demand/supply elasticities</title>
      <link>https://arxiv.org/abs/2412.20395</link>
      <description>arXiv:2412.20395v1 Announce Type: cross 
Abstract: Crowdsourced delivery (CSD) is an emerging business model that leverages the underutilized or excess capacity of individual drivers to fulfill delivery tasks. This paper presents a general formulation of a larege-scale two-sided CSD matching problem, considering demand/supply elasticity, heterogeneous preferences of both shippers and drivers, and task-bundling. We propose a set of methodologies to solve this problem. First, we reveal that the fluid-particle decomposition approach of Akamatsu and Oyama (2024) can be extended to our general formulation. This approach decomposes the original large-scale matching problem into a fluidly-approximated task partition problem (master problem) and small-scale particle matching problems (sub-problems). We propose to introduce a truthful auction mechanism to sub-problems, which enables the observation of privately perceived costs for each shipper/driver. Furthermore, by finding a theoretical link between auction problems and parturbed utility theory, we succeed in accurately reflecting the information collected from auctions to the master problem. This reduces the master problem to a smooth convex optimization problem, theoretically guaranteeing the computational efficiency and solution accuracy of the fluid approximation. Second, we transform the master problem into a traffic assignment problem (TAP) based on a task-chain network. This transformation overcomes the difficulty in enumerating task bundles. Finally, we formulate the dual problem of the TAP, whose decision variable is only a price/reward pattern at market equilibrium, and develop an efficient accelerated gradient descent method. The numerical experiments clarify that our approach drastically reduces the computational cost of the matching problem (~700 times faster than a naive method) without sacrificing accuracy of the optimal solution (mostly within 0.5% errors).</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20395v1</guid>
      <category>math.OC</category>
      <category>cs.GT</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yuki Oyama, Takashi Akamatsu</dc:creator>
    </item>
    <item>
      <title>Game Theory and Multi-Agent Reinforcement Learning : From Nash Equilibria to Evolutionary Dynamics</title>
      <link>https://arxiv.org/abs/2412.20523</link>
      <description>arXiv:2412.20523v1 Announce Type: cross 
Abstract: This paper explores advanced topics in complex multi-agent systems building upon our previous work. We examine four fundamental challenges in Multi-Agent Reinforcement Learning (MARL): non-stationarity, partial observability, scalability with large agent populations, and decentralized learning. The paper provides mathematical formulations and analysis of recent algorithmic advancements designed to address these challenges, with a particular focus on their integration with game-theoretic concepts. We investigate how Nash equilibria, evolutionary game theory, correlated equilibrium, and adversarial dynamics can be effectively incorporated into MARL algorithms to improve learning outcomes. Through this comprehensive analysis, we demonstrate how the synthesis of game theory and MARL can enhance the robustness and effectiveness of multi-agent systems in complex, dynamic environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20523v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Neil De La Fuente, Miquel Noguer i Alonso, Guim Casadell\`a</dc:creator>
    </item>
    <item>
      <title>How to Balance the Load Online When Jobs and Machines Are Both Selfish?</title>
      <link>https://arxiv.org/abs/2412.20711</link>
      <description>arXiv:2412.20711v1 Announce Type: cross 
Abstract: In this paper, we study the classic optimization problem of Related Machine Online Load Balancing under the conditions of selfish machines and selfish jobs. We have $m$ related machines with varying speeds and $n$ jobs arriving online with different sizes. Our objective is to design an online truthful algorithm that minimizes the makespan while ensuring that jobs and machines report their true sizes and speeds.
  Previous studies in the online scenario have primarily focused on selfish jobs, beginning with the work of Aspnes et al. (JACM 1997). An $O(1)$-competitive online mechanism for selfish jobs was discovered by Feldman, Fiat, and Roytman (EC 2017). For selfish machines, truthful mechanisms have only been explored in offline settings, starting with Archer and Tardos (FOCS 2001). The best-known results are two PTAS mechanisms by Christodoulou and Kov\'{a}cs (SICOMP 2013) and Epstein et al. (MOR 2016).
  We design an online mechanism that is truthful for both machines and jobs, achieving a competitive ratio of $O(\log m)$. This is the first non-trivial two-sided truthful mechanism for online load balancing and also the first non-trivial machine-side truthful mechanism. Furthermore, we extend our mechanism to the $\ell_q$ norm variant of load balancing, maintaining two-sided truthfulness with a competitive ratio of $\tilde{O}(m^{\frac{1}{q}(1-\frac{1}{q})})$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20711v1</guid>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenqian Wang, Chenyang Xu, Yuhao Zhang</dc:creator>
    </item>
    <item>
      <title>Sponsored Search Auction Design Beyond Single Utility Maximization</title>
      <link>https://arxiv.org/abs/2406.05988</link>
      <description>arXiv:2406.05988v2 Announce Type: replace 
Abstract: Auction design for the modern advertising market has gained significant prominence in the field of game theory. With the recent rise of auto-bidding tools, an increasing number of advertisers in the market are utilizing these tools for auctions. The diverse array of auto-bidding tools has made auction design more challenging. Various types of bidders, such as quasi-linear utility maximizers and constrained value maximizers, coexist within this dynamic gaming environment. We study sponsored search auction design in such a mixed-bidder world and aim to design truthful mechanisms that maximize the total social welfare. To simultaneously capture the classical utility and the value-max utility, we introduce an allowance utility model. In this model, each bidder is endowed with an additional allowance parameter, signifying the threshold up to which the bidder can maintain a value-max strategy. The paper distinguishes two settings based on the accessibility of the allowance information. In the case where each bidder's allowance is public, we demonstrate the existence of a truthful mechanism achieving an approximation ratio of $(1+\epsilon)$ for any $\epsilon &gt; 0$. In the more challenging private allowance setting, we establish that a truthful mechanism can achieve a constant approximation. Further, we consider uniform-price auction design in large markets and give a truthful mechanism that sets a uniform price in a random manner and admits bounded approximation in expectation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05988v2</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changfeng Xu, Chao Peng, Chenyang Xu, Zhengfeng Yang</dc:creator>
    </item>
    <item>
      <title>Game-Theoretic Joint Incentive and Cut Layer Selection Mechanism in Split Federated Learning</title>
      <link>https://arxiv.org/abs/2412.07813</link>
      <description>arXiv:2412.07813v2 Announce Type: replace 
Abstract: To alleviate the training burden in federated learning while enhancing convergence speed, Split Federated Learning (SFL) has emerged as a promising approach by combining the advantages of federated and split learning. However, recent studies have largely overlooked competitive situations. In this framework, the SFL model owner can choose the cut layer to balance the training load between the server and clients, ensuring the necessary level of privacy for the clients. Additionally, the SFL model owner sets incentives to encourage client participation in the SFL process. The optimization strategies employed by the SFL model owner influence clients' decisions regarding the amount of data they contribute, taking into account the shared incentives over clients and anticipated energy consumption during SFL. To address this framework, we model the problem using a hierarchical decision-making approach, formulated as a single-leader multi-follower Stackelberg game. We demonstrate the existence and uniqueness of the Nash equilibrium among clients and analyze the Stackelberg equilibrium by examining the leader's game. Furthermore, we discuss privacy concerns related to differential privacy and the criteria for selecting the minimum required cut layer. Our findings show that the Stackelberg equilibrium solution maximizes the utility for both the clients and the SFL model owner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07813v2</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joohyung Lee, Jungchan Cho, Wonjun Lee, Mohamed Seif, H. Vincent Poor</dc:creator>
    </item>
    <item>
      <title>Nash CoT: Multi-Path Inference with Preference Equilibrium</title>
      <link>https://arxiv.org/abs/2407.07099</link>
      <description>arXiv:2407.07099v3 Announce Type: replace-cross 
Abstract: Chain of thought (CoT) is a reasoning framework that can enhance the performance of Large Language Models (LLMs) on complex inference tasks. In particular, among various studies related to CoT, multi-path inference stands out as a simple yet effective improvement. However, there is no optimal setting for the number of inference paths. Therefore, we have to increase the number of inference paths to obtain better results, which in turn increases the inference cost. To address this limitation, we can utilize question-related role templates to guide LLMs into relevant roles, thereby increasing the possibility of correct inferences for each path and further reducing dependence on the number of inference paths while improving reasoning accuracy. However, placing LLMs into specific roles may reduce their reasoning diversity and performance on a few tasks where role dependence is low. To alleviate the excessive immersion of the LLM into a specific role, we propose Nash CoT by constructing a game system on each path that balances the generation from role-specific LLMs' and the general LLMs' generation, thereby ensuring both effective role adoption and diversity in LLM generation further maintaining the performance of multi-path inference while reducing the requirement of the number of inference paths. We evaluate Nash CoT across various inference tasks, including Arabic Reasoning, Commonsense Question Answering, and Symbolic Inference, achieving results that are comparable to or better than those of multi-path CoT with the equal number of inference paths.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07099v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>EMNLP 2024</arxiv:journal_reference>
      <dc:creator>Ziqi Zhang, Cunxiang Wang, Xiong Xiao, Yue Zhang, Donglin Wang</dc:creator>
    </item>
  </channel>
</rss>
