<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 17 Jan 2025 02:33:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Detecting Abrupt Changes in Point Processes: Fundamental Limits and Applications</title>
      <link>https://arxiv.org/abs/2501.08392</link>
      <description>arXiv:2501.08392v1 Announce Type: new 
Abstract: We consider the problem of detecting abrupt changes (i.e., large jump discontinuities) in the rate function of a point process. The rate function is assumed to be fully unknown, non-stationary, and may itself be a random process that depends on the history of event times. We show that abrupt changes can be accurately identified from observations of the point process, provided the changes are sharper than the "smoothness'' of the rate function before the abrupt change. This condition is also shown to be necessary from an information-theoretic point of view. We then apply our theory to several special cases of interest, including the detection of significant changes in piecewise smooth rate functions and detecting super-spreading events in epidemic models on graphs. Finally, we confirm the effectiveness of our methods through a detailed empirical analysis of both synthetic and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08392v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anna Brandenberger, Elchanan Mossel, Anirudh Sridhar</dc:creator>
    </item>
    <item>
      <title>On the Asymptotics of Importance Weighted Variational Inference</title>
      <link>https://arxiv.org/abs/2501.08477</link>
      <description>arXiv:2501.08477v1 Announce Type: new 
Abstract: For complex latent variable models, the likelihood function is not available in closed form. In this context, a popular method to perform parameter estimation is Importance Weighted Variational Inference. It essentially maximizes the expectation of the logarithm of an importance sampling estimate of the likelihood with respect to both the latent variable model parameters and the importance distribution parameters, the expectation being itself with respect to the importance samples. Despite its great empirical success in machine learning, a theoretical analysis of the limit properties of the resulting estimates is still lacking. We fill this gap by establishing consistency when both the Monte Carlo and the observed data sample sizes go to infinity simultaneously. We also establish asymptotic normality and efficiency under additional conditions relating the rate of growth between the Monte Carlo and the observed data samples sizes. We distinguish several regimes related to the smoothness of the importance ratio.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08477v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Badr-Eddine Cherief-Abdellatif, Randal Douc, Arnaud Doucet, Hugo Marival</dc:creator>
    </item>
    <item>
      <title>Nonparametric inference for Poisson-Laguerre tessellations</title>
      <link>https://arxiv.org/abs/2501.08810</link>
      <description>arXiv:2501.08810v1 Announce Type: new 
Abstract: In this paper, we consider statistical inference for Poisson-Laguerre tessellations in $\mathbb{R}^d$. The object of interest is a distribution function $F$ which uniquely determines the intensity measure of the underlying Poisson process. Two nonparametric estimators for $F$ are introduced which depend only on the points of the Poisson process which generate non-empty cells and the actual cells corresponding to these points. The proposed estimators are proven to be strongly consistent, as the observation window expands unboundedly to the whole space. We also consider a stereological setting, where one is interested in estimating the distribution function associated with the Poisson process of a higher dimensional Poisson-Laguerre tessellation, given that a corresponding sectional Poisson-Laguerre tessellation is observed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08810v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas van der Jagt, Geurt Jongbloed, Martina Vittorietti</dc:creator>
    </item>
    <item>
      <title>Total positivity and dependence of order statistics</title>
      <link>https://arxiv.org/abs/2501.08939</link>
      <description>arXiv:2501.08939v1 Announce Type: new 
Abstract: In this comprehensive study, we delve deeply into the concept of multivariate total positivity, defining it in accordance with a direction. We rigorously explore numerous salient properties, shedding light on the nuances that characterize this notion. Furthermore, our research extends to establishing distinct forms of dependence among the order statistics of a sample from a distribution function. Our analysis aims to provide a nuanced understanding of the interrelationships within multivariate total positivity and its implications for statistical analysis and probability theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08939v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3934/math.20231570</arxiv:DOI>
      <arxiv:journal_reference>AIMS Mathematics, volume 8, number 12 (2023), 30717-30730</arxiv:journal_reference>
      <dc:creator>Enrique de Amo, Jos\'e Juan Quesada-Molina, Manuel \'Ubeda-Flores</dc:creator>
    </item>
    <item>
      <title>Bayesian Sphere-on-Sphere Regression with Optimal Transport Maps</title>
      <link>https://arxiv.org/abs/2501.08492</link>
      <description>arXiv:2501.08492v1 Announce Type: cross 
Abstract: Spherical regression, where both covariate and response variables are defined on the sphere, is a required form of data analysis in several scientific disciplines, and has been the subject of substantial methodological development in recent years. Yet, it remains a challenging problem due to the complexities involved in constructing valid and expressive regression models between spherical domains, and the difficulties involved in quantifying uncertainty of estimated regression maps. To address these challenges, we propose casting spherical regression as a problem of optimal transport within a Bayesian framework. Through this approach, we obviate the need for directly parameterizing a spherical regression map, and are able to quantify uncertainty on the inferred map. We derive posterior contraction rates for the proposed model under two different prior specifications and, in doing so, obtain a result on the quantitative stability of optimal transport maps on the sphere, one that may be useful in other contexts. The utility of our approach is demonstrated empirically through a simulation study and through its application to real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08492v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tin Lok James Ng, Kwok-Kun Kwong, Jiakun Liu, Andrew Zammit-Mangion</dc:creator>
    </item>
    <item>
      <title>Differentiability and overlap concentration in optimal Bayesian inference</title>
      <link>https://arxiv.org/abs/2501.08786</link>
      <description>arXiv:2501.08786v1 Announce Type: cross 
Abstract: In this short note, we consider models of optimal Bayesian inference of finite-rank tensor products. We add to the model a linear channel parametrized by $h$. We show that at every interior differentiable point $h$ of the free energy (associated with the model), the overlap concentrates at the gradient of the free energy and the minimum mean-square error converges to a related limit. In other words, the model is replica-symmetric at every differentiable point. At any signal-to-noise ratio, such points $h$ form a full-measure set (hence $h=0$ belongs to the closure of these points). For a sufficiently low signal-to-noise ratio, we show that every interior point is a differentiable point.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08786v1</guid>
      <category>math.PR</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Hong-Bin Chen (IHES), Victor Issa (ENS de Lyon)</dc:creator>
    </item>
    <item>
      <title>The Berry-Esseen Bound for High-dimensional Self-normalized Sums</title>
      <link>https://arxiv.org/abs/2501.08979</link>
      <description>arXiv:2501.08979v1 Announce Type: cross 
Abstract: This manuscript studies the Gaussian approximation of the coordinate-wise maximum of self-normalized statistics in high-dimensional settings. We derive an explicit Berry-Esseen bound under weak assumptions on the absolute moments. When the third absolute moment is finite, our bound scales as $\log^{5/4}(d)/n^{1/8}$ where $n$ is the sample size and $d$ is the dimension. Hence, our bound tends to zero as long as $\log(d)=o(n^{1/10})$. Our results on self-normalized statistics represent substantial advancements, as such a bound has not been previously available in the high-dimensional central limit theorem (CLT) literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08979v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Woonyoung Chang, Kenta Takatsu, Konrad Urban, Arun Kumar Kuchibhotla</dc:creator>
    </item>
    <item>
      <title>What Is a Good Imputation Under MAR Missingness?</title>
      <link>https://arxiv.org/abs/2403.19196</link>
      <description>arXiv:2403.19196v3 Announce Type: replace 
Abstract: Missing values pose a persistent challenge in modern data science. Consequently, there is an ever-growing number of publications introducing new imputation methods in various fields. The present paper attempts to take a step back and provide a more systematic analysis. Starting from an in-depth discussion of the Missing at Random (MAR) condition for nonparametric imputation, we first develop an identification result, showing that the widely used Multiple Imputation by Chained Equations (MICE) approach indeed identifies the right conditional distributions. Building on this analysis, we propose three essential properties a successful imputation method should meet, thus enabling a more principled evaluation of existing methods and more targeted development of new methods. In particular, we introduce a new imputation method, denoted mice-DRF, that meets two out of the three criteria. We then discuss and refine ways to rank imputation methods, developing a powerful, easy-to-use scoring algorithm to rank missing value imputations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19196v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeffrey N\"af (PREMEDICAL), Erwan Scornet (USPC, CNRS), Julie Josse (PREMEDICAL)</dc:creator>
    </item>
    <item>
      <title>Azadkia-Chatterjee's dependence coefficient for infinite dimensional data</title>
      <link>https://arxiv.org/abs/2405.07732</link>
      <description>arXiv:2405.07732v3 Announce Type: replace 
Abstract: We extend the scope of Azadkia-Chatterjee's dependence coefficient between a scalar response $Y$ and a multivariate covariate $X$ to the case where $X$ takes values in a general metric space. Particular attention is paid to the case where $X$ is a curve. Although extending this framework at the population level is relatively straightforward, analyzing the asymptotic behavior of the estimator proves to be complex. This complexity is largely related to the nearest neighbor structure of the infinite-dimensional covariate sample, leading us to explore a topic that has not been previously addressed in the literature. The primary contribution of this paper is to provide insights into this issue and propose strategies to address it. Our findings also have significant implications for other graph-based methods facing similar challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07732v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siegfried H\"ormann, Daniel Strenger</dc:creator>
    </item>
    <item>
      <title>On determinantal point processes with nonsymmetric kernels</title>
      <link>https://arxiv.org/abs/2406.03360</link>
      <description>arXiv:2406.03360v2 Announce Type: replace 
Abstract: Determinantal point processes (DPPs for short) are a class of repulsive point processes. They have found some statistical applications to model spatial point pattern datasets with repulsion between close points. In the case of DPPs on finite sets, they are defined by a matrix called the DPP kernel which is usually assumed to be symmetric. While there are a few known examples of DPPs with nonsymmetric kernels, not much is known on how this affects their usual properties. In this paper, we demonstrate how to adapt the results on $P_0$ matrices to the DPP setting in order to get necessary and sufficient conditions for the well-definedness of DPPs with nonsymmetric kernels. We also generalize various common results on DPPs. We then show how to use these results to construct attractive couplings of regular DPPs with symmetric kernels in order to model spatial marked point patterns with repulsion between points of the same mark and attraction between points of different marks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03360v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Poinas Arnaud</dc:creator>
    </item>
    <item>
      <title>Consistency of MLE in partially observed diffusion models on a torus</title>
      <link>https://arxiv.org/abs/2412.03380</link>
      <description>arXiv:2412.03380v3 Announce Type: replace 
Abstract: In this paper, we consider a general partially observed diffusion model with periodic coefficients and with non-degenerate diffusion component. The coefficients of such a model depend on an unknown (static and deterministic) parameter which needs to be estimated based on the observed component of the diffusion process. We show that, given enough regularity of the diffusion coefficients, a maximum likelihood estimator of the unknown parameter converges to the true parameter value as the sample size grows to infinity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03380v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ibrahim Ekren, Sergey Nadtochiy</dc:creator>
    </item>
    <item>
      <title>Optimal Federated Learning for Functional Mean Estimation under Heterogeneous Privacy Constraints</title>
      <link>https://arxiv.org/abs/2412.18992</link>
      <description>arXiv:2412.18992v2 Announce Type: replace 
Abstract: Federated learning (FL) is a distributed machine learning technique designed to preserve data privacy and security, and it has gained significant importance due to its broad range of applications. This paper addresses the problem of optimal functional mean estimation from discretely sampled data in a federated setting.
  We consider a heterogeneous framework where the number of individuals, measurements per individual, and privacy parameters vary across one or more servers, under both common and independent design settings. In the common design setting, the same design points are measured for each individual, whereas in the independent design, each individual has their own random collection of design points. Within this framework, we establish minimax upper and lower bounds for the estimation error of the underlying mean function, highlighting the nuanced differences between common and independent designs under distributed privacy constraints.
  We propose algorithms that achieve the optimal trade-off between privacy and accuracy and provide optimality results that quantify the fundamental limits of private functional mean estimation across diverse distributed settings. These results characterize the cost of privacy and offer practical insights into the potential for privacy-preserving statistical analysis in federated environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18992v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.TH</category>
      <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tony Cai, Abhinav Chakraborty, Lasse Vuursteen</dc:creator>
    </item>
    <item>
      <title>Sobol' Matrices For Multi-Output Models With Quantified Uncertainty</title>
      <link>https://arxiv.org/abs/2501.04602</link>
      <description>arXiv:2501.04602v2 Announce Type: replace 
Abstract: Variance based global sensitivity analysis measures the relevance of inputs to a single output using Sobol' indices. This paper extends the definition in a natural way to multiple outputs, directly measuring the relevance of inputs to the linkages between outputs in a correlation-like matrix of indices. The usual Sobol' indices constitute the diagonal of this matrix. Existence, uniqueness and uncertainty quantification are established by developing the indices from a putative multi-output model with quantified uncertainty. Sobol' matrices and their standard errors are related to the moments of the multi-output model, to enable calculation. These are benchmarked numerically against test functions (with added noise) whose Sobol' matrices are calculated analytically.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04602v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robert A. Milton, Solomon F. Brown</dc:creator>
    </item>
    <item>
      <title>Augmentation Invariant Manifold Learning</title>
      <link>https://arxiv.org/abs/2211.00460</link>
      <description>arXiv:2211.00460v3 Announce Type: replace-cross 
Abstract: Data augmentation is a widely used technique and an essential ingredient in the recent advance in self-supervised representation learning. By preserving the similarity between augmented data, the resulting data representation can improve various downstream analyses and achieve state-of-the-art performance in many applications. Despite the empirical effectiveness, most existing methods lack theoretical understanding under a general nonlinear setting. To fill this gap, we develop a statistical framework on a low-dimension product manifold to model the data augmentation transformation. Under this framework, we introduce a new representation learning method called augmentation invariant manifold learning and design a computationally efficient algorithm by reformulating it as a stochastic optimization problem. Compared with existing self-supervised methods, the new method simultaneously exploits the manifold's geometric structure and invariant property of augmented data and has an explicit theoretical guarantee. Our theoretical investigation characterizes the role of data augmentation in the proposed method and reveals why and how the data representation learned from augmented data can improve the $k$-nearest neighbor classifier in the downstream analysis, showing that a more complex data augmentation leads to more improvement in downstream analysis. Finally, numerical experiments on simulated and real data sets are presented to demonstrate the merit of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.00460v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shulei Wang</dc:creator>
    </item>
    <item>
      <title>Quantification and cross-fitting inference of asymmetric relations under generative exposure mapping models</title>
      <link>https://arxiv.org/abs/2311.04696</link>
      <description>arXiv:2311.04696v5 Announce Type: replace-cross 
Abstract: In many practical studies, learning directionality between a pair of variables is of great interest while notoriously hard, especially for mechanistic relationships. This paper presents a method that examines directionality in exposure-outcome pairs when a priori assumptions about their relative ordering are unavailable. We propose a coefficient of asymmetry to quantify directional asymmetry using Shannon's entropy and propose a statistical estimation and inference framework for said estimand. Large-sample theoretical guarantees are established through data-splitting and cross-fitting techniques. The proposed methodology is extended to allow both measured confounders and contamination in outcome measurements. The methodology is extensively evaluated through extensive simulation studies, a benchmark dataset, and a real data application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.04696v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soumik Purkayastha, Peter X. -K. Song</dc:creator>
    </item>
    <item>
      <title>Supervised Kernel Thinning</title>
      <link>https://arxiv.org/abs/2410.13749</link>
      <description>arXiv:2410.13749v2 Announce Type: replace-cross 
Abstract: The kernel thinning algorithm of Dwivedi &amp; Mackey (2024) provides a better-than-i.i.d. compression of a generic set of points. By generating high-fidelity coresets of size significantly smaller than the input points, KT is known to speed up unsupervised tasks like Monte Carlo integration, uncertainty quantification, and non-parametric hypothesis testing, with minimal loss in statistical accuracy. In this work, we generalize the KT algorithm to speed up supervised learning problems involving kernel methods. Specifically, we combine two classical algorithms--Nadaraya-Watson (NW) regression or kernel smoothing, and kernel ridge regression (KRR)--with KT to provide a quadratic speed-up in both training and inference times. We show how distribution compression with KT in each setting reduces to constructing an appropriate kernel, and introduce the Kernel-Thinned NW and Kernel-Thinned KRR estimators. We prove that KT-based regression estimators enjoy significantly superior computational efficiency over the full-data estimators and improved statistical efficiency over i.i.d. subsampling of the training data. En route, we also provide a novel multiplicative error guarantee for compressing with KT. We validate our design choices with both simulations and real data experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13749v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Albert Gong, Kyuseong Choi, Raaz Dwivedi</dc:creator>
    </item>
    <item>
      <title>A Parameter-Efficient Quantum Anomaly Detection Method on a Superconducting Quantum Processor</title>
      <link>https://arxiv.org/abs/2412.16867</link>
      <description>arXiv:2412.16867v2 Announce Type: replace-cross 
Abstract: Quantum machine learning has gained attention for its potential to address computational challenges. However, whether those algorithms can effectively solve practical problems and outperform their classical counterparts, especially on current quantum hardware, remains a critical question. In this work, we propose a novel quantum machine learning method, called Quantum Support Vector Data Description (QSVDD), for practical image anomaly detection, which aims to achieve both parameter efficiency and superior accuracy compared to classical models. Emulation results indicate that QSVDD demonstrates favourable recognition capabilities compared to classical baselines, achieving an average accuracy of over 90% on benchmarks with significantly fewer trainable parameters. Theoretical analysis confirms that QSVDD has a comparable expressivity to classical counterparts while requiring only a fraction of the parameters. Furthermore, we demonstrate the first implementation of a quantum anomaly detection method for general image datasets on a superconducting quantum processor. Specifically, we achieve an accuracy of over 80% with only 16 parameters on the device, providing initial evidence of QSVDD's practical viability in the noisy intermediate-scale quantum era and highlighting its significant reduction in parameter requirements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16867v2</guid>
      <category>quant-ph</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maida Wang, Jinyang Jiang, Peter V. Coveney</dc:creator>
    </item>
  </channel>
</rss>
