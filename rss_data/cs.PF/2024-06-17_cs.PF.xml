<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PF updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PF</link>
    <description>cs.PF updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PF" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Jun 2024 04:01:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 18 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Comparison of the Performance of the Molecular Dynamics Simulation Package GROMACS Implemented in the SYCL and CUDA Programming Models</title>
      <link>https://arxiv.org/abs/2406.10362</link>
      <description>arXiv:2406.10362v1 Announce Type: cross 
Abstract: For many years, systems running Nvidia-based GPU architectures have dominated the heterogeneous supercomputer landscape. However, recently GPU chipsets manufactured by Intel and AMD have cut into this market and can now be found in some of the worlds fastest supercomputers. The June 2023 edition of the TOP500 list of supercomputers ranks the Frontier supercomputer at the Oak Ridge National Laboratory in Tennessee as the top system in the world. This system features AMD Instinct 250 X GPUs and is currently the only true exascale computer in the world.The first framework that enabled support for heterogeneous platforms across multiple hardware vendors was OpenCL, in 2009. Since then a number of frameworks have been developed to support vendor agnostic heterogeneous environments including OpenMP, OpenCL, Kokkos, and SYCL. SYCL, which combines the concepts of OpenCL with the flexibility of single-source C++, is one of the more promising programming models for heterogeneous computing devices. One key advantage of this framework is that it provides a higher-level programming interface that abstracts away many of the hardware details than the other frameworks. This makes SYCL easier to learn and to maintain across multiple architectures and vendors. In n recent years, there has been growing interest in using heterogeneous computing architectures to accelerate molecular dynamics simulations. Some of the more popular molecular dynamics simulations include Amber, NAMD, and Gromacs. However, to the best of our knowledge, only Gromacs has been successfully ported to SYCL to date. In this paper, we compare the performance of GROMACS compiled using the SYCL and CUDA frameworks for a variety of standard GROMACS benchmarks. In addition, we compare its performance across three different Nvidia GPU chipsets, P100, V100, and A100.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10362v1</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>L. Apanasevich, Yogesh Kale, Himanshu Sharma, Ana Marija Sokovic</dc:creator>
    </item>
    <item>
      <title>High-Performance Hardware Accelerator with Medium Granularity Dataflow for SpTRSV</title>
      <link>https://arxiv.org/abs/2406.10511</link>
      <description>arXiv:2406.10511v1 Announce Type: cross 
Abstract: Sparse triangular solve (SpTRSV) is widely used in various domains. Numerous studies have been conducted using CPUs, GPUs, and specific hardware accelerators, where dataflow can be categorized into coarse and fine granularity. Coarse dataflow offers good spatial locality but suffers from low parallelism, while fine dataflow provides high parallelism but disrupts the spatial structure, leading to increased nodes and poor data reuse. This paper proposes a novel hardware accelerator for SpTRSV or SpTRSV-like DAG. The accelerator implements a medium granularity dataflow through hardware-software codesign and achieves both excellent spatial locality and high parallelism. Additionally, a partial sum caching mechanism is introduced to reduce the blocking frequency of processing elements (PEs), and a reordering algorithm of intra-node edges computation is developed to enhance data reuse. Experimental results on 264 benchmarks with node counts reaching up to 85,392 demonstrate that this work achieves average performance improvements of 12.2x (up to 874.5x) over CPU and 10.1x (up to 740.4x) over GPU. Compared to the state-of-the-art technique (DPU-v2), this work shows a 2.5x (up to 5.9x) average performance improvement and 1.8x (up to 4.1x) average energy efficiency enhancement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10511v1</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <category>cs.NA</category>
      <category>cs.PF</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qian Chen, Xiaofeng Yang, Shengli Lu</dc:creator>
    </item>
    <item>
      <title>Optimization of Armv9 architecture general large language model inference performance based on Llama.cpp</title>
      <link>https://arxiv.org/abs/2406.10816</link>
      <description>arXiv:2406.10816v1 Announce Type: cross 
Abstract: This article optimizes the inference performance of the Qwen-1.8B model by performing Int8 quantization, vectorizing some operators in llama.cpp, and modifying the compilation script to improve the compiler optimization level. On the Yitian 710 experimental platform, the prefill performance is increased by 1.6 times, the decoding performance is increased by 24 times, the memory usage is reduced to 1/5 of the original, and the accuracy loss is almost negligible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10816v1</guid>
      <category>cs.PL</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.PF</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Longhao Chen, Yina Zhao, Qiangjun Xie, Qinghua Sheng</dc:creator>
    </item>
    <item>
      <title>LibProf: A Python Profiler for Improving Cold Start Performance in Serverless Applications</title>
      <link>https://arxiv.org/abs/2406.11734</link>
      <description>arXiv:2406.11734v1 Announce Type: cross 
Abstract: Serverless computing abstracts away server management, enabling automatic scaling and efficient resource utilization. However, cold-start latency remains a significant challenge, affecting end-to-end performance. Our preliminary study reveals that inefficient library initialization and usage are major contributors to this latency in Python-based serverless applications. We introduce LibProf, a Python profiler that uses dynamic program analysis to identify inefficient library initializations. LibProf collects library usage data through statistical sampling and call-path profiling, then generates a report to guide developers in addressing four types of inefficiency patterns. Systematic evaluations on 15 serverless applications demonstrate that LibProf effectively identifies inefficiencies. LibProf guided optimization results up to 2.26x speedup in cold-start execution time and 1.51x reduction in memory usage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11734v1</guid>
      <category>cs.SE</category>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Syed Salauddin Mohammad Tariq, Ali Al Zein, Soumya Sripad Vaidya, Arati Khanolkar, Probir Roy</dc:creator>
    </item>
    <item>
      <title>Selective Parallel Loading of Large-Scale Compressed Graphs with ParaGrapher</title>
      <link>https://arxiv.org/abs/2404.19735</link>
      <description>arXiv:2404.19735v2 Announce Type: replace-cross 
Abstract: Comprehensive evaluation is one of the basis of experimental science. In High-Performance Graph Processing, a thorough evaluation of contributions becomes more achievable by supporting common input formats over different frameworks. However, each framework creates its specific format, which may not support reading large-scale real-world graph datasets. This shows a demand for high-performance libraries capable of loading graphs to (i) accelerate designing new graph algorithms, (ii) to evaluate the contributions on a wide range of graph algorithms, and (iii) to facilitate easy and fast comparison over different graph frameworks.
  To that end, we present ParaGrapher, a high-performance API and library for loading large-scale and compressed graphs. ParaGrapher supports different types of requests for accessing graphs in shared- and distributed-memory and out-of-core graph processing. We explain the design of ParaGrapher and present a performance model of graph decompression, which is used for evaluation of ParaGrapher over three storage types. Our evaluation shows that by decompressing compressed graphs in WebGraph format, ParaGrapher delivers up to 3.2 times speedup in loading and up to 5.2 times speedup in end-to-end execution in comparison to the binary and textual formats.
  ParaGrapher is available online on https://blogs.qub.ac.uk/DIPSA/ParaGrapher/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19735v2</guid>
      <category>cs.AR</category>
      <category>cs.PF</category>
      <category>cs.SE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohsen Koohi Esfahani, Marco D'Antonio, Syed Ibtisam Tauhidi, Thai Son Mai, Hans Vandierendonck</dc:creator>
    </item>
  </channel>
</rss>
