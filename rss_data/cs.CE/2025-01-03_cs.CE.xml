<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 03 Jan 2025 05:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Mathematical modelling of flow and adsorption in a gas chromatograph</title>
      <link>https://arxiv.org/abs/2501.00001</link>
      <description>arXiv:2501.00001v1 Announce Type: new 
Abstract: In this paper, a mathematical model is developed to describe the evolution of the concentration of compounds through a gas chromatography column. The model couples mass balances and kinetic equations for all components. Both single and multiple-component cases are considered with constant or variable velocity. Non-dimensionalisation indicates the small effect of diffusion. The system where diffusion is neglected is analysed using Laplace transforms. In the multiple-component case, it is demonstrated that the competition between the compounds is negligible and the equations may be decoupled. This reduces the problem to solving a single integral equation to determine the concentration profile for all components (since they are scaled versions of each other). For a given analyte, we then only two parameters need to be fitted to the data. To verify this approach, the full governing equations are also solved numerically using the finite difference method and a global adaptive quadrature method to integrate the Laplace transformation. Comparison with the Laplace solution verifies the high degree of accuracy of the simpler Laplace form. The Laplace solution is then verified against experimental data from BTEX chromatography. This novel method, which involves solving a single equation and fitting parameters in pairs for individual components, is highly efficient. It is significantly faster and simpler than the full numerical solution and avoids the computationally expensive methods that would normally be used to fit all curves at the same time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00001v1</guid>
      <category>cs.CE</category>
      <category>physics.chem-ph</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>A. Cabrera-Codony, A. Valverde, K. Born, O. A. I. Noreldin, T. G. Myers</dc:creator>
    </item>
    <item>
      <title>Optimal design of frame structures with mixed categorical and continuous design variables using the Gumbel-Softmax method</title>
      <link>https://arxiv.org/abs/2501.00258</link>
      <description>arXiv:2501.00258v1 Announce Type: new 
Abstract: In optimizing real-world structures, due to fabrication or budgetary restraints, the design variables may be restricted to a set of standard engineering choices. Such variables, commonly called categorical variables, are discrete and unordered in essence, precluding the utilization of gradient-based optimizers for the problems containing them. In this paper, incorporating the Gumbel-Softmax (GSM) method, we propose a new gradient-based optimizer for handling such variables in the optimal design of large-scale frame structures. The GSM method provides a means to draw differentiable samples from categorical distributions, thereby enabling sensitivity analysis for the variables generated from such distributions. The sensitivity information can greatly reduce the computational cost of traversing high-dimensional and discrete design spaces in comparison to employing gradient-free optimization methods. In addition, since the developed optimizer is gradient-based, it can naturally handle the simultaneous optimization of categorical and continuous design variables. Through three numerical case studies, different aspects of the proposed optimizer are studied and its advantages over population-based optimizers, specifically a genetic algorithm, are demonstrated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00258v1</guid>
      <category>cs.CE</category>
      <category>math.OC</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s00158-024-03745-7</arxiv:DOI>
      <arxiv:journal_reference>Structural and Multidisciplinary Optimization 67.3 (2024): 31</arxiv:journal_reference>
      <dc:creator>Mehran Ebrahimi, Hyunmin Cheong, Pradeep Kumar Jayaraman, Farhad Javid</dc:creator>
    </item>
    <item>
      <title>A low order, torsion deformable spatial beam element based on the absolute nodal coordinate formulation and Bishop frame</title>
      <link>https://arxiv.org/abs/2501.00267</link>
      <description>arXiv:2501.00267v1 Announce Type: new 
Abstract: Heretofore, the Serret-Frenet frame has been the ubiquitous choice for analyzing the elastic deformations of beam elements. It is well known that this frame is undefined at the inflection points and straight segments of the beam where its curvature is zero, leading to singularities and errors in their numerical analysis. On the other hand, there exists a lesser-known frame called Bishop which does not have the caveats of the Serret-Frenet frame and is well-defined everywhere along the beam centerline. Leveraging the Bishop frame, in this paper, we propose a new spatial, singularity-free low-order beam element based on the absolute nodal coordinate formulation for both small and large deformation applications. This element, named ANCF14, has a constant mass matrix and can capture longitudinal, transverse (bending) and torsional deformations. It is a two-noded element with 7 degrees of freedom per node, which are global nodal coordinates, nodal slopes and their cross-sectional rotation about the centerline. The newly developed element is tested through four complex benchmarks. Comparing the ANCF14 results with theoretical and numerical results provided in other studies confirms the efficiency and accuracy of the proposed element.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00267v1</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s11044-020-09765-7</arxiv:DOI>
      <arxiv:journal_reference>Multibody System Dynamics 51.3 (2021): 247-278</arxiv:journal_reference>
      <dc:creator>Mehran Ebrahimi, Adrian Butscher, Hyunmin Cheong</dc:creator>
    </item>
    <item>
      <title>Conditional Uncertainty Quantification of Stochastic Dynamical Structures Considering Measurement Conditions</title>
      <link>https://arxiv.org/abs/2501.00310</link>
      <description>arXiv:2501.00310v1 Announce Type: new 
Abstract: How to accurately quantify the uncertainty of stochastic dynamical responses affected by uncertain loads and structural parameters is an important issue in structural safety and reliability analysis. In this paper, the conditional uncertainty quantification analysis for the dynamical response of stochastic structures considering the measurement data with random error is studied in depth. A method for extracting the key measurement condition, which holds the most reference value for the uncertainty quantification of response, from the measurement data is proposed. Considering the key measurement condition and employing the principle of probability conservation and conditional probability theory, the quotient-form expressions for the conditional mean, conditional variance, and conditional probability density function of the stochastic structural dynamical response are derived and are referred to as the key conditional quotients (KCQ). A numerical method combining the non-equal weighted generalized Monte Carlo method, Dirac function smoothing technique, and online-offline coupled computational strategy is developed for calculating KCQs. Three linear/nonlinear stochastic dynamical examples are used to verify that the proposed KCQ method can efficiently and accurately quantify the uncertainty of the structural response considering measurement conditions. The examples also compare the traditional non-conditional uncertainty quantification results with the conditional uncertainty quantification results given by KCQs, indicating that considering measurement conditions can significantly reduce the uncertainty of the stochastic dynamical responses, providing a more refined statistical basis for structural safety and reliability analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00310v1</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Feng Wu, Yuelin Zhao, Li Zhu</dc:creator>
    </item>
    <item>
      <title>Optimal design of triply-periodic minimal surface implants for bone repair</title>
      <link>https://arxiv.org/abs/2501.00611</link>
      <description>arXiv:2501.00611v1 Announce Type: new 
Abstract: This work proposes a gradient-based method to design bone implants using triply-periodic minimal surfaces (TPMS) of spatially varying thickness to maximize bone in-growth. Bone growth into the implant is estimated using a finite element based mechanobiological model considering the magnitude and frequency of in vivo loads, as well as the density distribution of the surrounding bone. The wall thicknesses of the implant unit cells are determined via linear interpolation of the thicknesses over a user defined grid of control points, avoiding mesh dependency and providing control over the sensitivity computation costs. The TPMS structure is modeled as a homogenized material to reduce computational cost. Local properties of the implant are determined at run-time on an element-by-element basis using a pre-constructed surrogate model of the TPMS's physical and geometric properties as a function of the local wall thickness and the density of in-grown bone. Design sensitivities of the bone growth within the implant are computed using the direct sensitivity method. The methodology is demonstrated on a cementless hip, optimizing the implant for bone growth subject to wall thickness constraints to ensure manufacturability and allow cell infiltration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00611v1</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Cohen (School of Mechanical, Aerospace, and Manufacturing Engineering, University of Connecticut), Juli\'an A. Norato (School of Mechanical, Aerospace, and Manufacturing Engineering, University of Connecticut)</dc:creator>
    </item>
    <item>
      <title>Effects of Turbulence Modeling and Parcel Approach on Dispersed Two-Phase Swirling Flow</title>
      <link>https://arxiv.org/abs/2501.00037</link>
      <description>arXiv:2501.00037v1 Announce Type: cross 
Abstract: Several numerical simulations of a co-axial particle-laden swirling air flow in a vertical circular pipe were performed. The air flow was modeled using the unsteady Favre-averaged Navier-Stokes equations. A Lagrangian model was used for the particle motion. The gas and particles are coupled through two-way momentum exchange. The results of the simulations using three versions of the k-epsilon turbulence model (standard, re-normalization group (RNG), and realizable) are compared with experimental mean velocity profiles. The standard model achieved the best overall performance. The realizable model was unable to satisfactorily predict the radial velocity; it is also the most computationally-expensive model. The simulations using the RNG model predicted additional recirculation zones. We also compared the particle and parcel approaches in solving the particle motion. In the latter, multiple similar particles are grouped in a single parcel, thereby reducing the amount of computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00037v1</guid>
      <category>physics.flu-dyn</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Osama A. Marzouk, E. David Huckaby</dc:creator>
    </item>
    <item>
      <title>An OpenFOAM face-centred solver for incompressible flows robust to mesh distortion</title>
      <link>https://arxiv.org/abs/2501.00450</link>
      <description>arXiv:2501.00450v1 Announce Type: cross 
Abstract: This work presents an overview of mesh-induced errors commonly experienced by cell-centred finite volumes (CCFV), for which the face-centred finite volume (FCFV) paradigm offers competitive solutions. In particular, a robust FCFV solver for incompressible laminar flows is integrated in OpenFOAM and tested on a set of steady-state and transient benchmarks. The method outperforms standard simpleFoam and pimpleFoam algorithms in terms of optimal convergence, accuracy, stability, and robustness. Special attention is devoted to motivate and numerically demonstrate the ability of the FCFV method to treat non-orthogonal, stretched, and skewed meshes, where CCFV schemes exhibit shortcomings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00450v1</guid>
      <category>physics.flu-dyn</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Davide Cortellessa, Matteo Giacomini, Antonio Huerta</dc:creator>
    </item>
    <item>
      <title>Design optimization of dynamic flexible multibody systems using the discrete adjoint variable method</title>
      <link>https://arxiv.org/abs/2501.00643</link>
      <description>arXiv:2501.00643v1 Announce Type: cross 
Abstract: The design space of dynamic multibody systems (MBSs), particularly those with flexible components, is considerably large. Consequently, having a means to efficiently explore this space and find the optimum solution within a feasible timeframe is crucial. It is well known that for problems with several design variables, sensitivity analysis using the adjoint variable method extensively reduces the computational costs. This paper presents the novel extension of the discrete adjoint variable method to the design optimization of dynamic flexible MBSs. The extension involves deriving the adjoint equations directly from the discrete, rather than the continuous, equations of motion. This results in a system of algebraic equations that is computationally less demanding to solve compared to the system of differential algebraic equations produced by the continuous adjoint variable method. To describe the proposed method, it is integrated with a numerical time-stepping algorithm based on geometric variational integrators. The developed technique is then applied to the optimization of MBSs composed of springs, dampers, beams and rigid bodies, considering both geometrical (e.g., positions of joints) and non-geometrical (e.g., mechanical properties of components) design variables. To validate the developed methods and show their applicability, three numerical examples are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00643v1</guid>
      <category>math.OC</category>
      <category>cs.CE</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.compstruc.2018.12.007</arxiv:DOI>
      <arxiv:journal_reference>Computers &amp; Structures 213 (2019): 82-99</arxiv:journal_reference>
      <dc:creator>Mehran Ebrahimi, Adrian Butscher, Hyunmin Cheong, Francesco Iorio</dc:creator>
    </item>
    <item>
      <title>CryptoMamba: Leveraging State Space Models for Accurate Bitcoin Price Prediction</title>
      <link>https://arxiv.org/abs/2501.01010</link>
      <description>arXiv:2501.01010v1 Announce Type: cross 
Abstract: Predicting Bitcoin price remains a challenging problem due to the high volatility and complex non-linear dynamics of cryptocurrency markets. Traditional time-series models, such as ARIMA and GARCH, and recurrent neural networks, like LSTMs, have been widely applied to this task but struggle to capture the regime shifts and long-range dependencies inherent in the data. In this work, we propose CryptoMamba, a novel Mamba-based State Space Model (SSM) architecture designed to effectively capture long-range dependencies in financial time-series data. Our experiments show that CryptoMamba not only provides more accurate predictions but also offers enhanced generalizability across different market conditions, surpassing the limitations of previous models. Coupled with trading algorithms for real-world scenarios, CryptoMamba demonstrates its practical utility by translating accurate forecasts into financial outcomes. Our findings signal a huge advantage for SSMs in stock and cryptocurrency price forecasting tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01010v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad Shahab Sepehri, Asal Mehradfar, Mahdi Soltanolkotabi, Salman Avestimehr</dc:creator>
    </item>
    <item>
      <title>Adaptive posterior distributions for uncertainty analysis of covariance matrices in Bayesian inversion problems for multioutput signals</title>
      <link>https://arxiv.org/abs/2501.01148</link>
      <description>arXiv:2501.01148v1 Announce Type: cross 
Abstract: In this paper we address the problem of performing Bayesian inference for the parameters of a nonlinear multi-output model and the covariance matrix of the different output signals. We propose an adaptive importance sampling (AIS) scheme for multivariate Bayesian inversion problems, which is based in two main ideas: the variables of interest are split in two blocks and the inference takes advantage of known analytical optimization formulas. We estimate both the unknown parameters of the multivariate non-linear model and the covariance matrix of the noise. In the first part of the proposed inference scheme, a novel AIS technique called adaptive target adaptive importance sampling (ATAIS) is designed, which alternates iteratively between an IS technique over the parameters of the non-linear model and a frequentist approach for the covariance matrix of the noise. In the second part of the proposed inference scheme, a prior density over the covariance matrix is considered and the cloud of samples obtained by ATAIS are recycled and re-weighted to obtain a complete Bayesian study over the model parameters and covariance matrix. ATAIS is the main contribution of the work. Additionally, the inverted layered importance sampling (ILIS) is presented as a possible compelling algorithm (but based on a conceptually simpler idea). Different numerical examples show the benefits of the proposed approaches</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01148v1</guid>
      <category>stat.CO</category>
      <category>cs.CE</category>
      <category>stat.ML</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jfranklin.2024.107441</arxiv:DOI>
      <arxiv:journal_reference>Journal of the Franklin Institute, Volume 362, Issue 2, 2024. 107441</arxiv:journal_reference>
      <dc:creator>E. Curbelo, L. Martino, F. Llorente, D. Delgado-Gomez</dc:creator>
    </item>
    <item>
      <title>Multi-layer diffusion model of photovoltaic installations</title>
      <link>https://arxiv.org/abs/2408.09904</link>
      <description>arXiv:2408.09904v3 Announce Type: replace 
Abstract: Nowadays, harmful effects of climate change are becoming increasingly apparent. A vital issue that must be addressed is the generation of energy from non-renewable and often polluting sources. For this reason, the development of renewable energy sources is of great importance. Unfortunately, too rapid spread of renewables can disrupt stability of the power system and lead to energy blackouts. One should not simply support it, without ensuring sustainability and understanding of the diffusion process. In this research, we propose a new agent-based model of diffusion of photovoltaic panels. It is an extension of the q-voter model that utilizes a multi-layer network structure. The novelty is that both opinion dynamics and diffusion of innovation are studied simultaneously on a multidimensional structure. The model is analyzed using Monte Carlo simulations and the mean-field approximation. The impact of parameters and specifications on the basic properties of the model is discussed. Firstly, we show that for a certain range of parameters, innovation always succeeds, regardless of the initial conditions. Secondly, that the mean-field approximation gives qualitatively the same results as computer simulations, even though it does not utilize knowledge of the network structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09904v3</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomasz Weron</dc:creator>
    </item>
    <item>
      <title>Hydrogen reaction rate modeling based on convolutional neural network for large eddy simulation</title>
      <link>https://arxiv.org/abs/2408.16709</link>
      <description>arXiv:2408.16709v2 Announce Type: replace 
Abstract: This paper establishes a data-driven modeling framework for lean Hydrogen (H2)-air reaction rates for the Large Eddy Simulation (LES) of turbulent reactive flows. This is particularly challenging since H2 molecules diffuse much faster than heat, leading to large variations in burning rates, thermodiffusive instabilities at the subfilter scale, and complex turbulence-chemistry interactions. Our data-driven approach leverages a Convolutional Neural Network (CNN), trained to approximate filtered burning rates from emulated LES data. First, five different lean premixed turbulent H2-air flame Direct Numerical Simulations (DNSs) are computed each with a unique global equivalence ratio. Second, DNS snapshots are filtered and downsampled to emulate LES data. Third, a CNN is trained to approximate the filtered burning rates as a function of LES scalar quantities: progress variable, local equivalence ratio and flame thickening due to filtering. Finally, the performances of the CNN model are assessed on test solutions never seen during training. The model retrieves burning rates with very high accuracy. It is also tested on two filter and downsampling parameters and two global equivalence ratios between those used during training. For these interpolation cases, the model approximates burning rates with low error even though the cases were not included in the training dataset. This a priori study shows that the proposed data-driven machine learning framework is able to address the challenge of modeling lean premixed H2-air burning rates. It paves the way for a new modeling paradigm for the simulation of carbon-free hydrogen combustion systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16709v2</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Quentin Mal\'e, Corentin J Lapeyre, Nicolas Noiray</dc:creator>
    </item>
    <item>
      <title>Macroscopic stress, couple stress and flux tensors derived through energetic equivalence from microscopic continuous and discrete heterogeneous finite representative volumes</title>
      <link>https://arxiv.org/abs/2412.12616</link>
      <description>arXiv:2412.12616v2 Announce Type: replace 
Abstract: This paper presents a rigorous derivation of equations to evaluate the macroscopic stress tensor, the couple stress tensor, and the flux vector equivalent to underlying microscopic fields in continuous and discrete heterogeneous systems with independent displacements and rotations. Contrary to the classical asymptotic expansion homogenization, finite size representative volume is considered. First, the macroscopic quantities are derived for a heterogeneous Cosserat continuum. The resulting continuum equations are discretized to provide macroscopic quantities in discrete heterogeneous systems. Finally, the expressions for discrete system are derived once again, this time considering the discrete nature directly.
  The formulations are presented in two variants, considering either internal or external forces, couples, and fluxes. The derivation is based on the virtual work equivalence and elucidates the fundamental significance of the couple stress tensor in the context of balance equations and admissible virtual deformation modes. Notably, an additional term in the couple stress tensor formula emerges, explaining its dependence on the reference system and position of the macroscopic point. The resulting equations are verified by comparing their predictions with known analytical solutions and results of other numerical models under both steady state and transient conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12616v2</guid>
      <category>cs.CE</category>
      <category>cond-mat.other</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.cma.2024.117688</arxiv:DOI>
      <arxiv:journal_reference>Computer Methods in Applied Mechanics and Engineering 436, article 117688, 2025, ISSN: 0045-7825</arxiv:journal_reference>
      <dc:creator>Jan Eli\'a\v{s}, Gianluca Cusatis</dc:creator>
    </item>
    <item>
      <title>PDE-constrained Gaussian process surrogate modeling with uncertain data locations</title>
      <link>https://arxiv.org/abs/2305.11586</link>
      <description>arXiv:2305.11586v3 Announce Type: replace-cross 
Abstract: Gaussian process regression is widely applied in computational science and engineering for surrogate modeling owning to its kernel-based and probabilistic nature. In this work, we propose a Bayesian approach that integrates the variability of input data into the Gaussian process regression for function and partial differential equation approximation. Leveraging two types of observables -- noise-corrupted outputs with certain inputs and those with prior-distribution-defined uncertain inputs, a posterior distribution of uncertain inputs is estimated via Bayesian inference. Thereafter, such quantified uncertainties of inputs are incorporated into Gaussian process predictions by means of marginalization. The setting of two types of data aligned with common scenarios of constructing surrogate models for the solutions of partial differential equations, where the data of boundary conditions and initial conditions are typically known while the data of solution may involve uncertainties due to the measurement or stochasticity. The effectiveness of the proposed method is demonstrated through several numerical examples including multiple one-dimensional functions, the heat equation and Allen-Cahn equation. A consistently good performance of generalization is observed, and a substantial reduction in the predictive uncertainties is achieved by the Bayesian inference of uncertain inputs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.11586v3</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>stat.ML</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dongwei Ye, Weihao Yan, Christoph Brune, Mengwu Guo</dc:creator>
    </item>
    <item>
      <title>Baichuan4-Finance Technical Report</title>
      <link>https://arxiv.org/abs/2412.15270</link>
      <description>arXiv:2412.15270v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated strong capabilities in language understanding, generation, and reasoning, yet their potential in finance remains underexplored due to the complexity and specialization of financial knowledge. In this work, we report the development of the Baichuan4-Finance series, including a comprehensive suite of foundational Baichuan4-Finance-Base and an aligned language model Baichuan4-Finance, which are built upon Baichuan4-Turbo base model and tailored for finance domain. Firstly, we have dedicated significant effort to building a detailed pipeline for improving data quality. Moreover, in the continual pre-training phase, we propose a novel domain self-constraint training strategy, which enables Baichuan4-Finance-Base to acquire financial knowledge without losing general capabilities. After Supervised Fine-tuning and Reinforcement Learning from Human Feedback and AI Feedback, the chat model Baichuan4-Finance is able to tackle various financial certification questions and real-world scenario applications. We evaluate Baichuan4-Finance on many widely used general datasets and two holistic financial benchmarks. The evaluation results show that Baichuan4-Finance-Base surpasses almost all competitive baselines on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. At the same time, Baichuan4-Finance demonstrates even more impressive performance on financial application scenarios, showcasing its potential to foster community innovation in the financial LLM field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15270v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Fri, 03 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hanyu Zhang, Boyu Qiu, Yuhao Feng, Shuqi Li, Qian Ma, Xiyuan Zhang, Qiang Ju, Dong Yan, Jian Xie</dc:creator>
    </item>
  </channel>
</rss>
