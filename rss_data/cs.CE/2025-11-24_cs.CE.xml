<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 25 Nov 2025 03:52:37 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Multivariate Sensitivity Analysis of Electric Machine Efficiency Maps and Profiles Under Design Uncertainty</title>
      <link>https://arxiv.org/abs/2511.17099</link>
      <description>arXiv:2511.17099v1 Announce Type: new 
Abstract: This work proposes the use of multivariate global sensitivity analysis for assessing the impact of uncertain electric machine design parameters on efficiency maps and profiles. Contrary to the common approach of applying variance-based (Sobol') sensitivity analysis elementwise, multivariate sensitivity analysis provides a single sensitivity index per parameter, thus allowing for a holistic estimation of parameter importance over the full efficiency map or profile. Its benefits are demonstrated on permanent magnet synchronous machine models of different fidelity. Computations based on Monte Carlo sampling and polynomial chaos expansions are compared in terms of computational cost. The sensitivity analysis results are subsequently used to simplify the models, by fixing non-influential parameters to their nominal values and allowing random variations only for influential parameters. Uncertainty estimates obtained with the full and reduced models confirm the validity of model simplification guided by multivariate sensitivity analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17099v1</guid>
      <category>cs.CE</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aylar Partovizadeh, Sebastian Sch\"ops, Dimitrios Loukrezis</dc:creator>
    </item>
    <item>
      <title>Towards Generative Design Using Optimal Transport for Shape Exploration and Solution Field Interpolation</title>
      <link>https://arxiv.org/abs/2511.17111</link>
      <description>arXiv:2511.17111v1 Announce Type: new 
Abstract: Generative Design (GD) combines artificial intelligence (AI), physics-based modeling, and multi-objective optimization to autonomously explore and refine engineering designs. Despite its promise in aerospace, automotive, and other high-performance applications, current GD methods face critical challenges: AI approaches require large datasets and often struggle to generalize; topology optimization is computationally intensive and difficult to extend to multiphysics problems; and model order reduction for evolving geometries remains underdeveloped. To address these challenges, we introduce a unified, structure-preserving framework for GD based on optimal transport (OT), enabling simultaneous interpolation of complex geometries and their associated physical solution fields across evolving design spaces, even with non-matching meshes and substantial shape changes. This capability leverages Gaussian splatting to provide a continuous, mesh-independent representation of the solution and Wasserstein barycenters to enable smooth, mathematically ''mass''-preserving blending of geometries, offering a major advance over surrogate models tied to static meshes. Our framework efficiently interpolates positive scalar fields across arbitrarily shaped, evolving geometries without requiring identical mesh topology or dimensionality. OT also naturally preserves localized physical features -- such as stress concentrations or sharp gradients -- by conserving the spatial distribution of quantities, interpreted as ''mass'' in a mathematical sense, rather than averaging them, avoiding artificial smoothing. Preliminary extensions to signed and vector fields are presented. Representative test cases demonstrate enhanced efficiency, adaptability, and physical fidelity, establishing a foundation for future foundation-model-powered generative design workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17111v1</guid>
      <category>cs.CE</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sergio Torregrosa, David Munoz, Hector Navarro, Charbel Farhat, Francisco Chinesta</dc:creator>
    </item>
    <item>
      <title>Randomness as Reference: Benchmark Metric for Optimization in Engineering</title>
      <link>https://arxiv.org/abs/2511.17226</link>
      <description>arXiv:2511.17226v1 Announce Type: new 
Abstract: Benchmarking optimization algorithms is fundamental for the advancement of computational intelligence. However, widely adopted artificial test suites exhibit limited correspondence with the diversity and complexity of real-world engineering optimization tasks. This paper presents a new benchmark suite comprising 231 bounded, continuous, unconstrained optimization problems, the majority derived from engineering design and simulation scenarios, including computational fluid dynamics and finite element analysis models. In conjunction with this suite, a novel performance metric is introduced, which employs random sampling as a statistical reference, providing nonlinear normalization of objective values and enabling unbiased comparison of algorithmic efficiency across heterogeneous problems. Using this framework, 20 deterministic and stochastic optimization methods were systematically evaluated through hundreds of independent runs per problem, ensuring statistical robustness. The results indicate that only a few of the tested optimization methods consistently achieve excellent performance, while several commonly used metaheuristics exhibit severe efficiency loss on engineering-type problems, emphasizing the limitations of conventional benchmarks. Furthermore, the conducted tests are used for analyzing various features of the optimization methods, providing practical guidelines for their application. The proposed test suite and metric together offer a transparent, reproducible, and practically relevant platform for evaluating and comparing optimization methods, thereby narrowing the gap between the available benchmark tests and realistic engineering applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17226v1</guid>
      <category>cs.CE</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Stefan Ivi\'c, Sini\v{s}a Dru\v{z}eta, Luka Grb\v{c}i\'c</dc:creator>
    </item>
    <item>
      <title>OmniScientist: Toward a Co-evolving Ecosystem of Human and AI Scientists</title>
      <link>https://arxiv.org/abs/2511.16931</link>
      <description>arXiv:2511.16931v1 Announce Type: cross 
Abstract: With the rapid development of Large Language Models (LLMs), AI agents have demonstrated increasing proficiency in scientific tasks, ranging from hypothesis generation and experimental design to manuscript writing. Such agent systems are commonly referred to as "AI Scientists." However, existing AI Scientists predominantly formulate scientific discovery as a standalone search or optimization problem, overlooking the fact that scientific research is inherently a social and collaborative endeavor. Real-world science relies on a complex scientific infrastructure composed of collaborative mechanisms, contribution attribution, peer review, and structured scientific knowledge networks. Due to the lack of modeling for these critical dimensions, current systems struggle to establish a genuine research ecosystem or interact deeply with the human scientific community. To bridge this gap, we introduce OmniScientist, a framework that explicitly encodes the underlying mechanisms of human research into the AI scientific workflow. OmniScientist not only achieves end-to-end automation across data foundation, literature review, research ideation, experiment automation, scientific writing, and peer review, but also provides comprehensive infrastructural support by simulating the human scientific system, comprising: (1) a structured knowledge system built upon citation networks and conceptual correlations; (2) a collaborative research protocol (OSP), which enables seamless multi-agent collaboration and human researcher participation; and (3) an open evaluation platform (ScienceArena) based on blind pairwise user voting and Elo rankings. This infrastructure empowers agents to not only comprehend and leverage human knowledge systems but also to collaborate and co-evolve, fostering a sustainable and scalable innovation ecosystem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16931v1</guid>
      <category>cs.CY</category>
      <category>cs.CE</category>
      <category>cs.CL</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenyang Shao, Dehao Huang, Yu Li, Keyu Zhao, Weiquan Lin, Yining Zhang, Qingbin Zeng, Zhiyu Chen, Tianxing Li, Yifei Huang, Taozhong Wu, Xinyang Liu, Ruotong Zhao, Mengsheng Zhao, Xuhua Zhang, Yue Wang, Yuanyi Zhen, Fengli Xu, Yong Li, Tie-Yan Liu</dc:creator>
    </item>
    <item>
      <title>Parrot: Persuasion and Agreement Robustness Rating of Output Truth -- A Sycophancy Robustness Benchmark for LLMs</title>
      <link>https://arxiv.org/abs/2511.17220</link>
      <description>arXiv:2511.17220v1 Announce Type: cross 
Abstract: This study presents PARROT (Persuasion and Agreement Robustness Rating of Output Truth), a robustness focused framework designed to measure the degradation in accuracy that occurs under social pressure exerted on users through authority and persuasion in large language models (LLMs) the phenomenon of sycophancy (excessive conformity). PARROT (i) isolates causal effects by comparing the neutral version of the same question with an authoritatively false version using a double-blind evaluation, (ii) quantifies confidence shifts toward the correct and imposed false responses using log-likelihood-based calibration tracking, and (iii) systematically classifies failure modes (e.g., robust correct, sycophantic agreement, reinforced error, stubborn error, self-correction, etc.) using an eight-state behavioral taxonomy. We evaluated 22 models using 1,302 MMLU-style multiple-choice questions across 13 domains and domain-specific authority templates. Findings show marked heterogeneity: advanced models (e.g., GPT-5, GPT-4.1, Claude Sonnet 4.5) exhibit low "follow rates" ($\leq 11\%$, GPT-5: 4\%) and minimal accuracy loss, while older/smaller models show severe epistemic collapse (GPT-4: 80\%, Qwen 2.5-1.5B: 94\%). The danger is not limited to response changes; weak models reduce confidence in the correct response while increasing confidence in the imposed incorrect response. While international law and global knowledge at the domain level exhibit high fragility, elementary mathematics is relatively resilient. Consequently, we argue that the goal of "resistance to overfitting pressure" should be addressed as a primary objective alongside accuracy, harm avoidance, and privacy for safe deployment in the real world.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17220v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yusuf \c{C}elebi, Mahmoud El Hussieni, \"Ozay Ezerceli</dc:creator>
    </item>
    <item>
      <title>Fundamental Limitations of QAOA on Constrained Problems and a Route to Exponential Enhancement</title>
      <link>https://arxiv.org/abs/2511.17259</link>
      <description>arXiv:2511.17259v1 Announce Type: cross 
Abstract: We study fundamental limitations of the generic Quantum Approximate Optimization Algorithm (QAOA) on constrained problems where valid solutions form a low dimensional manifold inside the Boolean hypercube, and we present a provable route to exponential improvements via constraint embedding. Focusing on permutation constrained objectives, we show that the standard generic QAOA ansatz, with a transverse field mixer and diagonal r local cost, faces an intrinsic feasibility bottleneck: even after angle optimization, circuits whose depth grows at most linearly with n cannot raise the total probability mass on the feasible manifold much above the uniform baseline suppressed by the size of the full Hilber space. Against this envelope we introduce a minimal constraint enhanced kernel (CE QAOA) that operates directly inside a product one hot subspace and mixes with a block local XY Hamiltonian. For permutation constrained problems, we prove an angle robust, depth matched exponential enhancement where the ratio between the feasible mass from CE QAOA and generic QAOA grows exponentially in $n^2$ for all depths up to a linear fraction of n, under a mild polynomial growth condition on the interaction hypergraph. Thanks to the problem algorithm co design in the kernel construction, the techniques and guarantees extend beyond permutations to a broad class of NP-Hard constrained optimization problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17259v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.CE</category>
      <category>cs.DM</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chinonso Onah, Kristel Michielsen</dc:creator>
    </item>
    <item>
      <title>FORWARD: Dataset of a forwarder operating in rough terrain</title>
      <link>https://arxiv.org/abs/2511.17318</link>
      <description>arXiv:2511.17318v1 Announce Type: cross 
Abstract: We present FORWARD, a high-resolution multimodal dataset of a cut-to-length forwarder operating in rough terrain on two harvest sites in the middle part of Sweden. The forwarder is a large Komatsu model equipped with a variety of sensors, including RTK-GNSS, 360-camera, operator vibration sensors, internal CAN-bus signal recording, and multiple IMUs. The data includes event time logs recorded in 5 Hz with e.g., driving speed, fuel consumption, vehicle position with centimeter accuracy, and crane use while the vehicle operates in forest areas laser-scanned with very high-resolution, $\sim$1500 points per square meter. Production log files (StanForD standard) with time-stamped machine events, extensive video material, and terrain data in various formats are included as well. About 18 hours of regular wood extraction work during three days is annotated from 360-video material into individual work elements and included in the dataset. We also include scenario specifications of conducted experiments on forest roads and in terrain. Scenarios include repeatedly driving the same routes with and without steel tracks, different load weight, and different target driving speeds. The dataset is intended for developing models and algorithms for trafficability, perception, and autonomous control of forest machines using artificial intelligence, simulation, and experiments on physical testbeds. In part, we focus on forwarders traversing terrain, avoiding obstacles, and loading or unloading logs, with consideration for efficiency, fuel consumption, safety, and environmental impact. Other benefits of the open dataset include the ability to explore auto-generation and calibration of forestry machine simulators and automation scenario descriptions using the data recorded in the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17318v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>physics.app-ph</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mikael Lundb\"ack, Erik Wallin, Carola H\"aggstr\"om, Mattias Nystr\"om, Andreas Gr\"onlund, Mats Richardson, Petrus J\"onsson, William Arnvik, Lucas Hedstr\"om, Arvid F\"alldin, Martin Servin</dc:creator>
    </item>
    <item>
      <title>Self-Supervised Learning for Glass Composition Screening</title>
      <link>https://arxiv.org/abs/2410.24083</link>
      <description>arXiv:2410.24083v3 Announce Type: replace 
Abstract: Glass composition screening is essential for advancing new glass materials, yet the inherent complexity of multicomponent systems presents significant challenges. Current supervised learning methods for this task rely heavily on large amounts of high-quality data and are prone to overfitting on noisy samples, which limits their generalization ability. In this work, we propose a novel self-supervised learning framework designed specifically for screening glass compositions within pre-defined glass transition temperature (Tg) ranges. We reformulate the screening task as a classification problem, aiming to predict whether the glass transition temperature of a given composition falls within a target interval. To improve the model's robustness to noise, we introduce an innovative data augmentation strategy grounded in asymptotic theory. Additionally, we present DeepGlassNet, a dedicated network architecture developed to capture and analyze the complex interactions among constituent elements in glass compositions. Experimental results demonstrate that DeepGlassNet achieves superior screening accuracy compared to traditional methods and exhibits strong adaptability to other composition-related screening tasks. This study not only provides an efficient methodology for designing multicomponent glasses but also establishes a foundation for applying self-supervised learning in material discovery. Code and data are available at: https://github.com/liubin06/DeepGlassNet</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.24083v3</guid>
      <category>cs.CE</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.actamat.2025.121509</arxiv:DOI>
      <arxiv:journal_reference>Acta Materialia, 301, 121509 (2025)</arxiv:journal_reference>
      <dc:creator>Meijing Chen, Bin Liu, Ying Liu, Tianrui Li</dc:creator>
    </item>
    <item>
      <title>Sensor Informativeness, Identifiability, and Uncertainty in Bayesian Inverse Problems for Structural Health Monitoring</title>
      <link>https://arxiv.org/abs/2511.16628</link>
      <description>arXiv:2511.16628v2 Announce Type: replace 
Abstract: In Structural Health Monitoring (SHM), the recovery of distributed mechanical parameters from sparse data is often ill-posed, raising critical questions about identifiability and the reliability of inferred states. While deterministic regularization methods such as Tikhonov stabilise the inversion, they provide little insight into the spatial limits of resolution or the inherent uncertainty of the solution. This paper presents a Bayesian inverse framework that rigorously quantifies these limits, using the identification of distributed flexural rigidity from rotation (tilt) influence lines as a primary case study. Fisher information is employed as a diagnostic metric to quantify sensor informativeness, revealing how specific sensor layouts and load paths constrain the recoverable spatial features of the parameter field.
  The methodology is applied to the full-scale openLAB research bridge (TU Dresden) using data from controlled vehicle passages. Beyond estimating the flexural rigidity profile, the Bayesian formulation produces credible intervals that expose regions of practical non-identifiability, which deterministic methods may obscure. The results demonstrate that while the measurement data carry high information content for the target parameters, their utility is spatially heterogeneous and strictly bounded by the experiment design. The proposed framework unifies identification with uncertainty quantification, providing a rigorous basis for optimising sensor placement and interpreting the credibility of SHM diagnostics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16628v2</guid>
      <category>cs.CE</category>
      <category>stat.AP</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tammam Bakeer, Max Herbers, Steffen Marx</dc:creator>
    </item>
    <item>
      <title>A Detailed Comparative Analysis of Blockchain Consensus Mechanisms</title>
      <link>https://arxiv.org/abs/2511.15730</link>
      <description>arXiv:2511.15730v2 Announce Type: replace-cross 
Abstract: This paper presents a comprehensive comparative analysis of two dominant blockchain consensus mechanisms, Proof of Work (PoW) and Proof of Stake (PoS), evaluated across seven critical metrics: energy use, security, transaction speed, scalability, centralization risk, environmental impact, and transaction fees. Utilizing recent academic research and real-world blockchain data, the study highlights that PoW offers robust, time-tested security but suffers from high energy consumption, slower throughput, and centralization through mining pools. In contrast, PoS demonstrates improved scalability and efficiency, significantly reduced environmental impact, and more stable transaction fees, however it raises concerns over validator centralization and long-term security maturity. The findings underscore the trade-offs inherent in each mechanism and suggest hybrid designs may combine PoW's security with PoS's efficiency and sustainability. The study aims to inform future blockchain infrastructure development by striking a balance between decentralization, performance, and ecological responsibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15730v2</guid>
      <category>cs.CR</category>
      <category>cs.CE</category>
      <category>cs.CY</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaeli Andrews, Linh B. Ngo, Md Amiruzzaman</dc:creator>
    </item>
  </channel>
</rss>
