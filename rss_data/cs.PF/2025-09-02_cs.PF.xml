<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PF updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PF</link>
    <description>cs.PF updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PF" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 03 Sep 2025 04:00:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Improving Nonpreemptive Multiserver Job Scheduling with Quickswap</title>
      <link>https://arxiv.org/abs/2509.01893</link>
      <description>arXiv:2509.01893v1 Announce Type: new 
Abstract: Modern data center workloads are composed of multiserver jobs, computational jobs that require multiple CPU cores in order to run. A data center server can run many multiserver jobs in parallel, as long as it has sufficient resources to meet their demands. However, multiserver jobs are generally stateful, meaning that job preemptions incur significant overhead from saving and reloading the state associated with running jobs. Hence, most systems try to avoid these costly job preemptions altogether. Given these constraints, a scheduling policy must determine what set of jobs to run in parallel at each moment in time to minimize the mean response time across a stream of arriving jobs. Unfortunately, simple non-preemptive policies such as FCFS may leave many cores idle, resulting in high mean response times or even system instability. Our goal is to design and analyze non-preemptive scheduling policies for multiserver jobs that maintain high system utilization to achieve low mean response time.
  One well-known non-preemptive policy, Most Servers First (MSF), prioritizes jobs with higher core requirements and achieves high resource utilization. However, MSF causes extreme variability in job waiting times, and can perform significantly worse than FCFS in practice. To address this, we propose and analyze a class of scheduling policies called MSF-Quick Swap (MSFQ) that performs well. MSFQ reduces the variability of job waiting times by periodically granting priority to other jobs in the system. We provide both stability results and an analysis of mean response time under MSFQ to prove that our policy dramatically outperforms MSF in the case where jobs request one core or all the cores. In more complex cases, we evaluate MSFQ in simulation. We show that, with some additional optimization, variants of the MSFQ policy can greatly outperform MSF and FCFS on real-world multiserver job workloads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01893v1</guid>
      <category>cs.PF</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhongrui Chen, Adityo Anggraito, Diletta Olliaro, Andrea Marin, Marco Ajmone Marsan, Benjamin Berg, Isaac Grosof</dc:creator>
    </item>
    <item>
      <title>Non-Asymptotic Performance Analysis of DOA Estimation Based on Real-Valued Root-MUSIC</title>
      <link>https://arxiv.org/abs/2509.01999</link>
      <description>arXiv:2509.01999v1 Announce Type: new 
Abstract: This paper presents a systematic theoretical performance analysis of the Real-Valued root-MUSIC (RV-root-MUSIC) algorithm under non-asymptotic conditions. However, RV-root-MUSIC suffers from the problem of estimation ambiguity for the mirror roots, therefore the conventional beamforming (CBF) technique is typically employed to filter out the mirror roots. Through the equivalent subspace based on the conjugate extension method and the equivalence of perturbations for both true roots and mirror roots , this paper provides a comprehensive investigation of three critical aspects: noise subspace perturbation, true root perturbation, and mirror root perturbation characteristics in the RV-root-MUSIC algorithm. The statistical model is established and the generalized expression of perturbation is further developed. The simulation results show the correctness and validity of the derived statistical characteristics. The results provide a solid theoretical foundation for optimizing the parameter selection of DOA estimation in practical applications, particularly in radar systems, communication networks, and intelligent sensing technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01999v1</guid>
      <category>cs.PF</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junyang Liu, Weicheng Zhao, Qingping Wang, Xiangtian Meng, Maria Greco, Fulvio Gini</dc:creator>
    </item>
    <item>
      <title>Performance is not All You Need: Sustainability Considerations for Algorithms</title>
      <link>https://arxiv.org/abs/2509.00045</link>
      <description>arXiv:2509.00045v1 Announce Type: cross 
Abstract: This work focuses on the high carbon emissions generated by deep learning model training, specifically addressing the core challenge of balancing algorithm performance and energy consumption. It proposes an innovative two-dimensional sustainability evaluation system. Different from the traditional single performance-oriented evaluation paradigm, this study pioneered two quantitative indicators that integrate energy efficiency ratio and accuracy: the sustainable harmonic mean (FMS) integrates accumulated energy consumption and performance parameters through the harmonic mean to reveal the algorithm performance under unit energy consumption; the area under the sustainability curve (ASC) constructs a performance-power consumption curve to characterize the energy efficiency characteristics of the algorithm throughout the cycle. To verify the universality of the indicator system, the study constructed benchmarks in various multimodal tasks, including image classification, segmentation, pose estimation, and batch and online learning. Experiments demonstrate that the system can provide a quantitative basis for evaluating cross-task algorithms and promote the transition of green AI research from theory to practice. Our sustainability evaluation framework code can be found here, providing methodological support for the industry to establish algorithm energy efficiency standards.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00045v1</guid>
      <category>cs.CV</category>
      <category>cs.PF</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiang Li, Chong Zhang, Hongpeng Wang, Shreyank Narayana Gowda, Yushi Li, Xiaobo Jin</dc:creator>
    </item>
    <item>
      <title>ReLATE: Learning Efficient Sparse Encoding for High-Performance Tensor Decomposition</title>
      <link>https://arxiv.org/abs/2509.00280</link>
      <description>arXiv:2509.00280v1 Announce Type: cross 
Abstract: Tensor decomposition (TD) is essential for analyzing high-dimensional sparse data, yet its irregular computations and memory-access patterns pose major performance challenges on modern parallel processors. Prior works rely on expert-designed sparse tensor formats that fail to adapt to irregular tensor shapes and/or highly variable data distributions. We present the reinforcement-learned adaptive tensor encoding (ReLATE) framework, a novel learning-augmented method that automatically constructs efficient sparse tensor representations without labeled training samples. ReLATE employs an autonomous agent that discovers optimized tensor encodings through direct interaction with the TD environment, leveraging a hybrid model-free and model-based algorithm to learn from both real and imagined actions. Moreover, ReLATE introduces rule-driven action masking and dynamics-informed action filtering mechanisms that ensure functionally correct tensor encoding with bounded execution time, even during early learning stages. By automatically adapting to both irregular tensor shapes and data distributions, ReLATE generates sparse tensor representations that consistently outperform expert-designed formats across diverse sparse tensor data sets, achieving up to 2X speedup compared to the best sparse format, with a geometric-mean speedup of 1.4-1.46X.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00280v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ahmed E. Helal, Fabio Checconi, Jan Laukemann, Yongseok Soh, Jesmin Jahan Tithi, Fabrizio Petrini, Jee Choi</dc:creator>
    </item>
    <item>
      <title>An Efficient GNNs-to-KANs Distillation via Self-Attention Dynamic Sampling with Potential for Consumer Electronics Edge Deployment</title>
      <link>https://arxiv.org/abs/2509.00560</link>
      <description>arXiv:2509.00560v1 Announce Type: cross 
Abstract: Knowledge distillation (KD) is crucial for deploying deep learning models in resource-constrained edge environments, particularly within the consumer electronics sector, including smart home devices, wearable technology, and mobile terminals. These applications place higher demands on model compression and inference speed, necessitating the transfer of knowledge from Graph Neural Networks (GNNs) to more efficient Multi-Layer Perceptron (MLP) models. However, due to their fixed activation functions and fully connected architecture, MLPs face challenges in rapidly capturing the complex neighborhood dependencies learned by GNNs, thereby limiting their performance in edge environments. To address these limitations, this paper introduces an innovative from GNNs to Kolmogorov-Arnold Networks (KANs) knowledge distillation framework-Self Attention Dynamic Sampling Distillation (SA-DSD). This study improved Fourier KAN (FR-KAN) and replaced MLP with the improved FR-KAN+ as the student model. Through the incorporation of learnable frequency bases and phase-shift mechanisms, along with algorithmic optimization, FR-KAN significantly improves its nonlinear fitting capability while effectively reducing computational complexity. Building on this, a margin-level sampling probability matrix, based on teacher-student prediction consistency, is constructed, and an adaptive weighted loss mechanism is designed to mitigate performance degradation in the student model due to the lack of explicit neighborhood aggregation. Extensive experiments conducted on six real-world datasets demonstrate that SA-DSD achieves performance improvements of 3.05%-3.62% over three GNN teacher models and 15.61% over the FR-KAN+ model. Moreover, when compared with key benchmark models, SA-DSD achieves a 16.96x reduction in parameter count and a 55.75% decrease in inference time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00560v1</guid>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Can Cui, Zilong Fu, Penghe Huang, Yuanyuan Li, Wu Deng, Dongyan Li</dc:creator>
    </item>
    <item>
      <title>GeneTEK: Low-power, high-performance and scalable genome sequence matching in FPGAs</title>
      <link>https://arxiv.org/abs/2509.01020</link>
      <description>arXiv:2509.01020v1 Announce Type: cross 
Abstract: The advent of next-generation sequencing (NGS) has revolutionized genomic research by enabling high-throughput data generation through parallel sequencing of a diverse range of organisms at significantly reduced costs. This breakthrough has unleashed a "Cambrian explosion" in genomic data volume and diversity. This volume of workloads places genomics among the top four big data challenges anticipated for this decade. In this context, pairwise sequence alignment represents a very time- and energy-consuming step in common bioinformatics pipelines. Speeding up this step requires the implementation of heuristic approaches, optimized algorithms, and/or hardware acceleration.
  Whereas state-of-the-art CPU and GPU implementations have demonstrated significant performance gains, recent field programmable gate array (FPGA) implementations have shown improved energy efficiency. However, the latter often suffer from limited scalability due to constraints on hardware resources when aligning longer sequences. In this work, we present a scalable and flexible FPGA-based accelerator template that implements Myers's algorithm using high-level synthesis and a worker-based architecture. GeneTEK, an instance of this accelerator template in a Xilinx Zynq UltraScale+ FPGA, outperforms state-of-the-art CPU and GPU implementations in both speed and energy efficiency, while overcoming scalability limitations of current FPGA approaches. Specifically, GeneTEK achieves at least a 19.4% increase in execution speed and up to 62x reduction in energy consumption compared to leading CPU and GPU solutions, while fitting comparison matrices up to 72% larger compared to previous FPGA solutions. These results reaffirm the potential of FPGAs as an energy-efficient platform for scalable genomic workloads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01020v1</guid>
      <category>cs.AR</category>
      <category>cs.PF</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elena Espinosa, Rub\'en Rodr\'iguez \'Alvarez, Jos\'e Miranda, Rafael Larrosa, Miguel Pe\'on-Quir\'os, Oscar Plata, David Atienza</dc:creator>
    </item>
    <item>
      <title>Optimal Parallel Scheduling under Concave Speedup Functions</title>
      <link>https://arxiv.org/abs/2509.01811</link>
      <description>arXiv:2509.01811v1 Announce Type: cross 
Abstract: Efficient scheduling of parallel computation resources across multiple jobs is a fundamental problem in modern cloud/edge computing systems for many AI-based applications. Allocating more resources to a job accelerates its completion, but with diminishing returns. Prior work (heSRPT) solved this problem only for some specific speedup functions with an exponential form, providing a closed-form solution. However, the general case with arbitrary concave speedup functions -- which more accurately capture real-world workloads -- has remained open.
  In this paper, we solve this open problem by developing optimal scheduling algorithms for parallel jobs under general concave speedup functions. We first discover a fundamental and broadly-applicable rule for optimal parallel scheduling, namely the Consistent Derivative Ratio (CDR) Rule, which states that the ratio of the derivatives of the speedup functions across active jobs remains constant over time. To efficiently compute the optimal allocations that satisfy the CDR Rule, we propose the General Water-Filling (GWF) method, a more general version of classical water-filling in wireless communications. Combining these insights, we design the SmartFill Algorithm to solve the general scheduling problem. Unlike heSRPT, which always allocates resources to all active jobs, SmartFill selectively determines which jobs should receive resources and how much they should be allocated. For a broad class of so-called \emph{regular} speedup functions, SmartFill yields closed-form optimal solutions, while for non-regular functions it efficiently computes the optimum with low complexity. Numerical evaluations show that SmartFill can substantially outperform heSRPT across a wide range of concave speedup functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01811v1</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengzhang Li, Peizhong Ju, Atilla Eryilmaz, Ness Shroff</dc:creator>
    </item>
    <item>
      <title>A Diffusion-Based Framework for Configurable and Realistic Multi-Storage Trace Generation</title>
      <link>https://arxiv.org/abs/2509.01919</link>
      <description>arXiv:2509.01919v1 Announce Type: cross 
Abstract: We propose DiTTO, a novel diffusion-based framework for generating realistic, precisely configurable, and diverse multi-device storage traces. Leveraging advanced diffusion tech- niques, DiTTO enables the synthesis of high-fidelity continuous traces that capture temporal dynamics and inter-device dependencies with user-defined configurations. Our experimental results demonstrate that DiTTO can generate traces with high fidelity and diversity while aligning closely with guided configurations with only 8% errors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01919v1</guid>
      <category>cs.CV</category>
      <category>cs.PF</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seohyun Kim, Junyoung Lee, Jongho Park, Jinhyung Koo, Sungjin Lee, Yeseong Kim</dc:creator>
    </item>
    <item>
      <title>DaCe AD: Unifying High-Performance Automatic Differentiation for Machine Learning and Scientific Computing</title>
      <link>https://arxiv.org/abs/2509.02197</link>
      <description>arXiv:2509.02197v1 Announce Type: cross 
Abstract: Automatic differentiation (AD) is a set of techniques that systematically applies the chain rule to compute the gradients of functions without requiring human intervention. Although the fundamentals of this technology were established decades ago, it is experiencing a renaissance as it plays a key role in efficiently computing gradients for backpropagation in machine learning algorithms. AD is also crucial for many applications in scientific computing domains, particularly emerging techniques that integrate machine learning models within scientific simulations and schemes. Existing AD frameworks have four main limitations: limited support of programming languages, requiring code modifications for AD compatibility, limited performance on scientific computing codes, and a naive store-all solution for forward-pass data required for gradient calculations. These limitations force domain scientists to manually compute the gradients for large problems. This work presents DaCe AD, a general, efficient automatic differentiation engine that requires no code modifications. DaCe AD uses a novel ILP-based algorithm to optimize the trade-off between storing and recomputing to achieve maximum performance within a given memory constraint. We showcase the generality of our method by applying it to NPBench, a suite of HPC benchmarks with diverse scientific computing patterns, where we outperform JAX, a Python framework with state-of-the-art general AD capabilities, by more than 92 times on average without requiring any code changes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02197v1</guid>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <category>cs.PL</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Afif Boudaoud, Alexandru Calotoiu, Marcin Copik, Torsten Hoefler</dc:creator>
    </item>
    <item>
      <title>Safe Memory Reclamation Techniques</title>
      <link>https://arxiv.org/abs/2509.02457</link>
      <description>arXiv:2509.02457v1 Announce Type: cross 
Abstract: Safe memory reclamation is crucial to memory safety for optimistic and lock-free concurrent data structures in non garbage collected programming languages. However, several challenges arise in designing an ideal safe memory reclamation algorithm, including achieving high speed and scalability, easy of use for programmers, applicability to wide class of data structures, managing the large memory footprint caused by delayed freeing of memory for safety and performance, and avoiding asymmetric overhead on data structure operations. Several approaches to designing safe memory reclamation algorithms are studied by blending ideas and tools from across the hardware-software stack. These solutions cross traditional boundaries and exploit features exposed at different layers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02457v1</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <category>cs.PF</category>
      <category>cs.PL</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ajay Singh</dc:creator>
    </item>
    <item>
      <title>GPU-Accelerated Parallel Selected Inversion for Structured Matrices Using sTiles</title>
      <link>https://arxiv.org/abs/2504.19171</link>
      <description>arXiv:2504.19171v2 Announce Type: replace 
Abstract: Selected inversion is essential for applications such as Bayesian inference, electronic structure calculations, and inverse covariance estimation, where computing only specific elements of large sparse matrix inverses significantly reduces computational and memory overhead. We present an efficient implementation of a two-phase parallel algorithm for computing selected elements of the inverse of a sparse symmetric matrix A, which can be expressed as A = LL^T through sparse Cholesky factorization. Our approach leverages a tile-based structure, focusing on selected dense tiles to optimize computational efficiency and parallelism. While the focus is on arrowhead matrices, the method can be extended to handle general structured matrices. Performance evaluations on a dual-socket 26-core Intel Xeon CPU server demonstrate that sTiles outperforms state-of-the-art direct solvers such as Panua-PARDISO, achieving up to 13X speedup on large-scale structured matrices. Additionally, our GPU implementation using an NVIDIA A100 GPU demonstrates substantial acceleration over its CPU counterpart, achieving up to 5X speedup for large, high-bandwidth matrices with high computational intensity. These results underscore the robustness and versatility of sTiles, validating its effectiveness across various densities and problem configurations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19171v2</guid>
      <category>cs.PF</category>
      <category>stat.CO</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Esmail Abdul Fattah, Hatem Ltaief, Havard Rue, David Keyes</dc:creator>
    </item>
    <item>
      <title>Adversarial Attacks and Defenses in Multivariate Time-Series Forecasting for Smart and Connected Infrastructures</title>
      <link>https://arxiv.org/abs/2408.14875</link>
      <description>arXiv:2408.14875v2 Announce Type: replace-cross 
Abstract: The emergence of deep learning models has revolutionized various industries over the last decade, leading to a surge in connected devices and infrastructures. However, these models can be tricked into making incorrect predictions with high confidence, leading to disastrous failures and security concerns. To this end, we explore the impact of adversarial attacks on multivariate time-series forecasting and investigate methods to counter them. Specifically, we employ untargeted white-box attacks, namely the Fast Gradient Sign Method (FGSM) and the Basic Iterative Method (BIM), to poison the inputs to the training process, effectively misleading the model. We also illustrate the subtle modifications to the inputs after the attack, which makes detecting the attack using the naked eye quite difficult. Having demonstrated the feasibility of these attacks, we develop robust models through adversarial training and model hardening. We are among the first to showcase the transferability of these attacks and defenses by extrapolating our work from the benchmark electricity data to a larger, 10-year real-world data used for predicting the time-to-failure of hard disks. Our experimental results confirm that the attacks and defenses achieve the desired security thresholds, leading to a 72.41% and 94.81% decrease in RMSE for the electricity and hard disk datasets respectively after implementing the adversarial defenses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14875v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.PF</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pooja Krishan, Rohan Mohapatra, Sanchari Das, Saptarshi Sengupta</dc:creator>
    </item>
    <item>
      <title>Leveraging Approximate Caching for Faster Retrieval-Augmented Generation</title>
      <link>https://arxiv.org/abs/2503.05530</link>
      <description>arXiv:2503.05530v2 Announce Type: replace-cross 
Abstract: Retrieval-augmented generation (RAG) improves the reliability of large language model (LLM) answers by integrating external knowledge. However, RAG increases the end-to-end inference time since looking for relevant documents from large vector databases is computationally expensive. To address this, we introduce Proximity, an approximate key-value cache that optimizes the RAG workflow by leveraging similarities in user queries. Instead of treating each query independently, Proximity reuses previously retrieved documents when similar queries appear, substantially reducing reliance on expensive vector database lookups. To scale efficiently, Proximity employs a locality-sensitive hashing (LSH) scheme that enables fast cache lookups while preserving retrieval accuracy. We evaluate Proximity using the MMLU and MedRAG question answering benchmarks. Our experiments demonstrate that Proximity with our LSH scheme and a realistically skewed MedRAG workload reduces database calls by 78.9% while maintaining database recall and test accuracy. We experiment with different similarity tolerances and cache capacities, and show that the time spent within the Proximity cache remains low and constant (4.8 microseconds) even as the cache grows substantially in size. Our work highlights that approximate caching is a viable and effective strategy for optimizing RAG-based systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05530v2</guid>
      <category>cs.DB</category>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3721146.3721941</arxiv:DOI>
      <dc:creator>Shai Bergman, Zhang Ji, Anne-Marie Kermarrec, Diana Petrescu, Rafael Pires, Mathis Randl, Martijn de Vos</dc:creator>
    </item>
    <item>
      <title>Disaggregated Design for GPU-Based Volumetric Data Structures</title>
      <link>https://arxiv.org/abs/2503.07898</link>
      <description>arXiv:2503.07898v2 Announce Type: replace-cross 
Abstract: Volumetric data structures typically prioritize data locality, focusing on efficient memory access patterns. This singular focus can neglect other critical performance factors, such as occupancy, communication, and kernel fusion. We introduce a novel \emph{disaggregated} design that rebalances trade-offs between locality and these objectives -- reducing communication overhead on distributed memory architectures, mitigating register pressure in complex boundary conditions, and enabling kernel fusion. We provide a thorough analysis of its benefits on a single-node multi-GPU Lattice Boltzmann Method (LBM) solver. Our evaluation spans dense, block-sparse, and multi-resolution discretizations, demonstrating our design's flexibility and efficiency. Leveraging this approach, we achieve up to a $3\times$ speedup over state-of-the-art solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07898v2</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Massimiliano Meneghin, Ahmed H. Mahmoud</dc:creator>
    </item>
    <item>
      <title>Performance and Storage Analysis of CRYSTALS Kyber as a Post Quantum Replacement for RSA and ECC</title>
      <link>https://arxiv.org/abs/2508.01694</link>
      <description>arXiv:2508.01694v5 Announce Type: replace-cross 
Abstract: The steady advancement in quantum computer error correction technology has pushed the current record to 48 stable logical qubits, bringing us closer to machines capable of running Shor's algorithm at scales that threaten RSA and ECC cryptography. While the timeline for developing such quantum computers remains uncertain, the cryptographic community must prepare for the transition to quantum-resistant algorithms. CRYSTALS-Kyber, standardized by NIST in 2022, represents a leading post-quantum cryptographic solution, but widespread adoption faces significant challenges. If this migration follows patterns similar to the SHA-1 to SHA-2 transition, organizations may experience prolonged periods of vulnerability, with substantial security and economic consequences. This study evaluates Kyber's practical viability through performance testing across various implementation schemes, utilizing only standard built-in processor acceleration features, some of which include AES-NI and ASIMD, without any specialized hardware additions. Our findings demonstrate that Kyber provides robust security guarantees against quantum attacks while maintaining acceptable performance profiles for most contemporary applications, utilizing only commodity hardware with manufacturer-provided acceleration capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01694v5</guid>
      <category>cs.CR</category>
      <category>cs.PF</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolas Rodriguez-Alvarez (IES Parquesol, Valladolid, Spain), Fernando Rodriguez-Merino (Department of Theoretical, Atomic and Optical Physics, University of Valladolid, Valladolid, Spain)</dc:creator>
    </item>
    <item>
      <title>Hierarchical Evaluation Function: A Multi-Metric Approach for Optimizing Demand Forecasting Models</title>
      <link>https://arxiv.org/abs/2508.13057</link>
      <description>arXiv:2508.13057v2 Announce Type: replace-cross 
Abstract: Inventory management in dynamic and competitive business environments presents multidimensional challenges, particularly in the face of demand uncertainty and logistical and financial constraints. In this context, accurate demand forecasting is critical for optimizing resources and anticipating market fluctuations. However, the isolated use of traditional metrics such as Mean Absolute Error (MAE) or Root Mean Squared Error (RMSE) can lead to biased evaluations and limit model robustness. To address this limitation, we propose the Hierarchical Evaluation Function (HEF), a composite function that integrates R2, MAE, and RMSE under a hierarchical and dynamic framework, complemented by adaptive penalties. The study implements HEF in the optimization of multiple prediction models, applying Grid Search, Particle Swarm Optimization (PSO), and Optuna, and evaluating their performance on reference databases (Walmart, M3, M4, and M5). The results, validated using statistical tests, confirm that HEF consistently outperforms the MAE used as the evaluation function in global metrics such as R2, Global Relative Precision, RMSE, and RMSSE, improving explanatory power and stability against extreme errors. In contrast, the MAE retains advantages in simplicity and computational efficiency. In summary, HEF constitutes a robust and adaptive alternative for highly variable environments, providing a solid framework for model selection and hyperparameter optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13057v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.PF</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Adolfo Gonz\'alez, V\'ictor Parada</dc:creator>
    </item>
    <item>
      <title>GPT-OSS-20B: A Comprehensive Deployment-Centric Analysis of OpenAI's Open-Weight Mixture of Experts Model</title>
      <link>https://arxiv.org/abs/2508.16700</link>
      <description>arXiv:2508.16700v2 Announce Type: replace-cross 
Abstract: We present a single-GPU (H100, bf16) evaluation of GPT-OSS-20B (Mixture-of-Experts; 20.9B total, approx. 3.61B active) against dense baselines Qwen3-32B and Yi-34B across multiple dimensions. We measure true time-to-first-token (TTFT), full-decode throughput (TPOT), end-to-end latency percentiles, peak VRAM with past key values (PKV) held, and energy via a consistent nvidia-smi-based sampler. At a 2048-token context with 64-token decode, GPT-OSS-20B delivers higher decode throughput and tokens per Joule than dense baselines Qwen3-32B and Yi-34B, while substantially reducing peak VRAM and energy per 1000 generated tokens; its TTFT is higher due to MoE routing overhead. With only 17.3% of parameters active (3.61B of 20.9B), GPT-OSS-20B provides about 31.8% higher decode throughput and 25.8% lower energy per 1000 generated tokens than Qwen3-32B at 2048/64, while using 31.7% less peak VRAM. Normalized by active parameters, GPT-OSS-20B shows markedly stronger per-active-parameter efficiency (APE), underscoring MoE's deployment advantages. We do not evaluate accuracy; this is a deployment-focused study. We release code and consolidated results to enable replication and extension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16700v2</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Deepak Kumar, Divakar Yadav, Yash Patel</dc:creator>
    </item>
  </channel>
</rss>
