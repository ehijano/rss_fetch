<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 08 Aug 2025 04:01:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A near-exact linear mixed model for genome-wide association studies</title>
      <link>https://arxiv.org/abs/2508.05278</link>
      <description>arXiv:2508.05278v1 Announce Type: new 
Abstract: Linear mixed models (LMM) are widely adopted in genome-wide association studies (GWAS) to account for population stratification and cryptic relatedness. However, the parameter estimation of LMMs imposes substantial computational burdens due to large-scale operations on genetic similarity matrices (GSM). We introduced the near-exact linear mixed model (NExt-LMM), a novel LMM framework that overcomes critical computational bottlenecks in GWAS through the following key innovations. Firstly, we exploit the inherent low-rank structure of the GSM iteratively with the Hierarchical Off-Diagonal Low-Rank (HODLR) format, which is much faster than traditional decomposition methods. Secondly, we leverage the HODLR-approximated GSM to dramatically accelerate the further maximum likelihood estimation with the shared heritability ratios. Moreover, we establish rigorous error bounds for the NExt-LMM estimator, proving that Kullback-Leibler divergence between the approximated and exact estimators can be arbitrarily small. Consequently, our proposed dual approach accelerates inference of LMMs while guaranteeing low approximation errors. We use numerical experiments to demonstrate that the NExt-LMM significantly improves inference efficiency compared to existing methods. We develop a Python package that is available at https://github.com/ZhibinPU/NExt-LMM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05278v1</guid>
      <category>stat.CO</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zhibin Pu, Shufei Ge, Shijia Wang</dc:creator>
    </item>
    <item>
      <title>Piecewise Deterministic Sampling for Constrained Distributions</title>
      <link>https://arxiv.org/abs/2508.05462</link>
      <description>arXiv:2508.05462v1 Announce Type: new 
Abstract: In this paper, we propose a novel class of Piecewise Deterministic Markov Processes (PDMP) that are designed to sample from constrained probability distributions $\pi$ supported on a convex set $\mathcal{M}$. This class of PDMPs adapts the concept of a mirror map from convex optimisation to address sampling problems. Such samplers provides unbiased algorithms that respect the constraints and, moreover, allow for exact subsampling. We demonstrate the advantages of these algorithms on a range of constrained sampling problems where the proposed algorithm outperforms state of the art stochastic differential equation-based methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05462v1</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jo\"el Tatang Demano, Paul Dobson, Konstantinos Zygalakis</dc:creator>
    </item>
    <item>
      <title>Modelling the emergence of open-ended technological evolution</title>
      <link>https://arxiv.org/abs/2508.04828</link>
      <description>arXiv:2508.04828v1 Announce Type: cross 
Abstract: Humans stand alone in terms of their potential to collectively and cumulatively improve technologies in an open-ended manner. This open-endedness provides societies with the ability to continually expand their resources and to increase their capacity to store, transmit and process information at a collective-level. Here, we propose that the production of resources arises from the interaction between technological systems (a society's repertoire of interdependent skills, techniques and artifacts) and search spaces (the aggregate collection of needs, problems and goals within a society). Starting from this premise we develop a macro-level model wherein both technological systems and search spaces are subject to cultural evolutionary dynamics. By manipulating the extent to which these dynamics are characterised by stochastic or selection-like processes, we demonstrate that open-ended growth is extremely rare, historically contingent and only possible when technological systems and search spaces co-evolve. Here, stochastic factors must be strong enough to continually perturb the dynamics into a far-from-equilibrium state, whereas selection-like factors help maintain effectiveness and ensure the sustained production of resources. Only when this co-evolutionary dynamic maintains effective technological systems, supports the ongoing expansion of the search space and leads to an increased provision of resources do we observe open-ended technological evolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04828v1</guid>
      <category>cs.NE</category>
      <category>q-bio.PE</category>
      <category>stat.CO</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Winters, Mathieu Charbonneau</dc:creator>
    </item>
    <item>
      <title>L1-Regularized Functional Support Vector Machine</title>
      <link>https://arxiv.org/abs/2508.05567</link>
      <description>arXiv:2508.05567v1 Announce Type: cross 
Abstract: In functional data analysis, binary classification with one functional covariate has been extensively studied. We aim to fill in the gap of considering multivariate functional covariates in classification. In particular, we propose an $L_1$-regularized functional support vector machine for binary classification. An accompanying algorithm is developed to fit the classifier. By imposing an $L_1$ penalty, the algorithm enables us to identify relevant functional covariates of the binary response. Numerical results from simulations and one real-world application demonstrate that the proposed classifier enjoys good performance in both prediction and feature selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05567v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.4310/22-SII773</arxiv:DOI>
      <dc:creator>Bingfan Liu, Peijun Sang</dc:creator>
    </item>
    <item>
      <title>inrep: A Comprehensive Framework for Adaptive Testing in R</title>
      <link>https://arxiv.org/abs/2507.15893</link>
      <description>arXiv:2507.15893v2 Announce Type: replace 
Abstract: The inrep package provides a comprehensive framework for implementing computerized adaptive testing (CAT) in R. Building upon established psychometric foundations from TAM, the package enables researchers to deploy production-ready adaptive assessments through an integrated shiny interface. The framework supports all major item response theory models (1PL, 2PL, 3PL, GRM) with real-time ability estimation, multiple item selection algorithms, and sophisticated stopping criteria. Key innovations include dual estimation engines for optimal speed-accuracy balance, comprehensive multilingual support, GDPR-compliant data management, and seamless integration with external platforms. Empirical validation demonstrates measurement accuracy within established benchmarks while reducing test length efficiently. The package addresses critical barriers to CAT adoption by providing a complete solution from study configuration through deployment and analysis, making adaptive testing accessible to researchers across educational, psychological, and clinical domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15893v2</guid>
      <category>stat.CO</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Clievins Selva</dc:creator>
    </item>
    <item>
      <title>gcor: A Python Implementation of Categorical Gini Correlation and Its Inference</title>
      <link>https://arxiv.org/abs/2506.19230</link>
      <description>arXiv:2506.19230v3 Announce Type: replace-cross 
Abstract: Categorical Gini Correlation (CGC), introduced by Dang et al. (2020), is a novel dependence measure designed to quantify the association between a numerical variable and a categorical variable. It has appealing properties compared to existing dependence measures, such as zero correlation mutually implying independence between the variables. It has also shown superior performance over existing methods when applied to feature screening for classification. This article presents a Python implementation for computing CGC, constructing confidence intervals, and performing independence tests based on it. Efficient algorithms have been implemented for all procedures, and they have been optimized using vectorization and parallelization to enhance computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19230v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sameera Hewage</dc:creator>
    </item>
    <item>
      <title>Efficient optimization of expensive black-box simulators via marginal means, with application to neutrino detector design</title>
      <link>https://arxiv.org/abs/2508.01834</link>
      <description>arXiv:2508.01834v2 Announce Type: replace-cross 
Abstract: With advances in scientific computing, computer experiments are increasingly used for optimizing complex systems. However, for modern applications, e.g., the optimization of nuclear physics detectors, each experiment run can require hundreds of CPU hours, making the optimization of its black-box simulator over a high-dimensional space a challenging task. Given limited runs at inputs $\mathbf{x}_1, \cdots, \mathbf{x}_n$, the best solution from these evaluated inputs can be far from optimal, particularly as dimensionality increases. Existing black-box methods, however, largely employ this ''pick-the-winner'' (PW) solution, which leads to mediocre optimization performance. To address this, we propose a new Black-box Optimization via Marginal Means (BOMM) approach. The key idea is a new estimator of a global optimizer $\mathbf{x}^*$ that leverages the so-called marginal mean functions, which can be efficiently inferred with limited runs in high dimensions. Unlike PW, this estimator can select solutions beyond evaluated inputs for improved optimization performance. Assuming the objective function follows a generalized additive model with unknown link function and under mild conditions, we prove that the BOMM estimator not only is consistent for optimization, but also has an optimization rate that tempers the ''curse-of-dimensionality'' faced by existing methods, thus enabling better performance as dimensionality increases. We present a practical framework for implementing BOMM using the transformed additive Gaussian process surrogate model. Finally, we demonstrate the effectiveness of BOMM in numerical experiments and an application on neutrino detector optimization in nuclear physics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01834v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hwanwoo Kim, Simon Mak, Ann-Kathrin Schuetz, Alan Poon</dc:creator>
    </item>
  </channel>
</rss>
