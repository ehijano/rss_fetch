<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 05 Nov 2024 04:09:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Adaptive minimax estimation for discretely observed L\'evy processes</title>
      <link>https://arxiv.org/abs/2411.00253</link>
      <description>arXiv:2411.00253v1 Announce Type: new 
Abstract: In this paper, we study the nonparametric estimation of the density $f_\Delta$ of an increment of a L\'evy process $X$ based on $n$ observations with a sampling rate $\Delta$. The class of L\'evy processes considered is broad, including both processes with a Gaussian component and pure jump processes. A key focus is on processes where $f_\Delta$ is smooth for all $\Delta$. We introduce a spectral estimator of $f_\Delta$ and derive both upper and lower bounds, showing that the estimator is minimax optimal in both low- and high-frequency regimes. Our results differ from existing work by offering weaker, easily verifiable assumptions and providing non-asymptotic results that explicitly depend on $\Delta$. In low-frequency settings, we recover parametric convergence rates, while in high-frequency settings, we identify two regimes based on whether the Gaussian or jump components dominate. The rates of convergence are closely tied to the jump activity, with continuity between the Gaussian case and more general jump processes. Additionally, we propose a fully data-driven estimator with proven simplicity and rapid implementation, supported by numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00253v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>C\'eline Duval, Taher Jalal, Ester Mariucci</dc:creator>
    </item>
    <item>
      <title>Blocked Gibbs Sampling for Improved Convergence in Finite Mixture Models</title>
      <link>https://arxiv.org/abs/2411.00371</link>
      <description>arXiv:2411.00371v1 Announce Type: new 
Abstract: Gibbs sampling is a common procedure used to fit finite mixture models. However, it is known to be slow to converge when exploring correlated regions of a parameter space and so blocking correlated parameters is sometimes implemented in practice. This is straightforward to visualize in contexts like low-dimensional multivariate Gaussian distributions, but more difficult for mixture models because of the way latent variable assignment and cluster-specific parameters influence one another. Here we analyze correlation in the space of latent variables and show that latent variables of outlier observations equidistant between component distributions can exhibit significant correlation that is not bounded away from one, suggesting they can converge very slowly to their stationary distribution. We provide bounds on convergence rates to a modification of the stationary distribution and propose a blocked sampling procedure that significantly reduces autocorrelation in the latent variable Markov chain, which we demonstrate in simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00371v1</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Michael Swanson</dc:creator>
    </item>
    <item>
      <title>Dependent and Independent Time Series</title>
      <link>https://arxiv.org/abs/2411.00525</link>
      <description>arXiv:2411.00525v1 Announce Type: new 
Abstract: The time series theory is set in this work under the domain of general elliptically contoured distributions. The advent of a time series approach that is in accordance with the expected reality of dependence between errors, transfers the increasingly complex and difficult to handle correlation analysis into a discipline that models volatility from a new view of a likelihood based on dependent probabilistic samples. The equally important problem of model selection is strengthened, but at the same time criticized with the introduction of degrees of evidence of significant difference in the modified BIC criterion . The demanding scale of differentiation puts a well-known database in trouble by observing insignificant relevance between the hierarchical models most used in the theory of time series under independence, such as Arch, Garch, Tgarch and Egarch. For extreme cases where the probabilistic independence of the samples is exceptionally demonstrated by an expert, the article also proposes the theory of time series under elliptical models, but with the same demanding comparison of the degrees of evidence of differences of the modified BIC. The example studied under this approach also does not denote any advantage of the hierarchical models studied. Such a new perspective for a likelihood based on dependent probabilistic samples has arisen naturally in similar context in finance under the setting of multivector variate distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00525v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fredy O. Perez-Ramirez, Francisco J. Caro-Lopera, Jose A. Diaz-Garcia, Graciela Gonzalez-Farias</dc:creator>
    </item>
    <item>
      <title>Characterizing extremal dependence on a hyperplane</title>
      <link>https://arxiv.org/abs/2411.00573</link>
      <description>arXiv:2411.00573v1 Announce Type: new 
Abstract: Quantifying the risks of extreme scenarios requires understanding the tail behaviours of variables of interest. While the tails of individual variables can be characterized parametrically, the extremal dependence across variables can be complex and its modeling remains one of the core problems in extreme value analysis. Notably, existing measures for extremal dependence, e.g., spectral measures and spectral random vectors, reside on nonlinear supports, such that statistical models and methods designed for linear vector spaces cannot be readily applied. In this paper, we show that the extremal dependence of $d$ asymptotically dependent variables can be characterized by a class of random vectors residing on a $(d-1)$-dimensional hyperplane. This translates the analyses on multivariate extremes to that on a linear vector space, opening up the potentials for the application of existing statistical techniques, particularly in statistical learning and dimension reduction. As an example, we show that a lower-dimensional approximation of multivariate extremes can be achieved through principal component analysis on the hyperplane. Additionally, through this framework, the widely used H\"usler-Reiss family for modelling extremes is characterized by the Gaussian family residing on the hyperplane, thereby justifying its status as the Gaussian counterpart for extremes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00573v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Phyllis Wan</dc:creator>
    </item>
    <item>
      <title>Sharp Bounds on the Variance of General Regression Adjustment in Randomized Experiments</title>
      <link>https://arxiv.org/abs/2411.00191</link>
      <description>arXiv:2411.00191v1 Announce Type: cross 
Abstract: Building on statistical foundations laid by Neyman [1923] a century ago, a growing literature focuses on problems of causal inference that arise in the context of randomized experiments where the target of inference is the average treatment effect in a finite population and random assignment determines which subjects are allocated to one of the experimental conditions. In this framework, variances of average treatment effect estimators remain unidentified because they depend on the covariance between treated and untreated potential outcomes, which are never jointly observed. Aronow et al. [2014] provide an estimator for the variance of the difference-in-means estimator that is asymptotically sharp. In practice, researchers often use some form of covariate adjustment, such as linear regression when estimating the average treatment effect. Here we extend the Aronow et al. [2014] result, providing asymptotically sharp variance bounds for general regression adjustment. We apply these results to linear regression adjustment and show benefits both in a simulation as well as an empirical application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00191v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas M. Mikhaeil, Donald P. Green</dc:creator>
    </item>
    <item>
      <title>On optimal prediction of missing functional data with memory</title>
      <link>https://arxiv.org/abs/2208.09925</link>
      <description>arXiv:2208.09925v2 Announce Type: replace 
Abstract: This paper considers the problem of reconstructing missing parts of functions based on their observed segments. It provides, for Gaussian processes and arbitrary bijective transformations thereof, theoretical expressions for the $L^2$-optimal reconstruction of the missing parts. These functions are obtained as solutions of explicit integral equations. In the discrete case, approximations of the solutions provide consistent expressions of all missing values of the processes. Rates of convergence of these approximations, under extra assumptions on the transformation function, are provided. In the case of Gaussian processes with a parametric covariance structure, the estimation can be conducted separately for each function, and yields nonlinear solutions in presence of memory. Simulated examples show that the proposed reconstruction indeed fares better than the conventional interpolation methods in various situations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.09925v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pauliina Ilmonen, Nourhan Shafik, Tommi Sottinen, Germain Van Bever, Lauri Viitasaari</dc:creator>
    </item>
    <item>
      <title>Nonparametric Estimation of the Transition Density Function for Diffusion Processes</title>
      <link>https://arxiv.org/abs/2404.00157</link>
      <description>arXiv:2404.00157v2 Announce Type: replace 
Abstract: We assume that we observe $N$ independent copies of a diffusion process on a time-interval $[0,2T]$. For a given time $t$, we estimate the transition density $p_t(x,y)$, namely the conditional density of $X_{t + s}$ given $X_s = x$, under conditions on the diffusion coefficients ensuring that this quantity exists. We use a least squares projection method on a product of finite dimensional spaces, prove risk bounds for the estimator and propose an anisotropic model selection method, relying on several reference norms. A simulation study illustrates the theoretical part for Ornstein-Uhlenbeck or square-root (Cox-Ingersoll-Ross) processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00157v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabienne Comte, Nicolas Marie</dc:creator>
    </item>
    <item>
      <title>Asymptotics for irregularly observed long memory processes</title>
      <link>https://arxiv.org/abs/2409.09498</link>
      <description>arXiv:2409.09498v2 Announce Type: replace 
Abstract: We study the effect of observing a stationary process at irregular time points via a renewal process. We establish a sharp difference in the asymptotic behaviour of the self-normalized sample mean of the observed process depending on the renewal process. In particular, we show that if the renewal process has a moderate heavy tail distribution then the limit is a so-called Normal Variance Mixture (NVM) and we characterize the randomized variance part of the limiting NVM as an integral function of a L\'evy stable motion. Otherwise, the normalized sample mean will be asymptotically normal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09498v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohamedou Ould-Haye, Anne Philippe</dc:creator>
    </item>
    <item>
      <title>Statistical Inference in Tensor Completion: Optimal Uncertainty Quantification and Statistical-to-Computational Gaps</title>
      <link>https://arxiv.org/abs/2410.11225</link>
      <description>arXiv:2410.11225v2 Announce Type: replace 
Abstract: This paper presents a simple yet efficient method for statistical inference of tensor linear forms using incomplete and noisy observations. Under the Tucker low-rank tensor model and the missing-at-random assumption, we utilize an appropriate initial estimate along with a debiasing technique followed by a one-step power iteration to construct an asymptotically normal test statistic. This method is suitable for various statistical inference tasks, including constructing confidence intervals, inference under heteroskedastic and sub-exponential noise, and simultaneous testing. We demonstrate that the estimator achieves the Cram\'er-Rao lower bound on Riemannian manifolds, indicating its optimality in uncertainty quantification. We comprehensively examine the statistical-to-computational gaps and investigate the impact of initialization on the minimal conditions regarding sample size and signal-to-noise ratio required for accurate inference. Our findings show that with independent initialization, statistically optimal sample sizes and signal-to-noise ratios are sufficient for accurate inference. Conversely, if only dependent initialization is available, computationally optimal sample sizes and signal-to-noise ratio conditions still guarantee asymptotic normality without the need for data-splitting. We present the phase transition between computational and statistical limits. Numerical simulation results align with the theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11225v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wanteng Ma, Dong Xia</dc:creator>
    </item>
    <item>
      <title>Functional Limit Theorems for Hawkes Processes</title>
      <link>https://arxiv.org/abs/2401.11495</link>
      <description>arXiv:2401.11495v2 Announce Type: replace-cross 
Abstract: We prove that the long-run behavior of Hawkes processes is fully determined by the average number and the dispersion of child events. For subcritical processes we provide FLLNs and FCLTs under minimal conditions on the kernel of the process with the precise form of the limit theorems depending strongly on the dispersion of child events. For a critical Hawkes process with weakly dispersed child events, functional central limit theorems do not hold. Instead, we prove that the rescaled intensity processes and rescaled Hawkes processes behave like CIR-processes without mean-reversion, respectively integrated CIR-processes. We provide the rate of convergence by establishing an upper bound on the Wasserstein distance between the distributions of rescaled Hawkes process and the corresponding limit process. By contrast, critical Hawkes process with heavily dispersed child events share many properties of subcritical ones. In particular, functional limit theorems hold. However, unlike subcritical processes critical ones with heavily dispersed child events display long-range dependencies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.11495v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>q-fin.MF</category>
      <category>stat.TH</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ulrich Horst, Wei Xu</dc:creator>
    </item>
    <item>
      <title>Asymptotically-exact selective inference for quantile regression</title>
      <link>https://arxiv.org/abs/2404.03059</link>
      <description>arXiv:2404.03059v2 Announce Type: replace-cross 
Abstract: In modern data analysis, it is common to select a model before performing statistical inference. Selective inference tools make adjustments for the model selection process in order to ensure reliable inference post selection. In this paper, we introduce an asymptotic pivot to infer about the effects of selected variables on conditional quantile functions. Utilizing estimators from smoothed quantile regression, our proposed pivot is easy to compute and yields asymptotically-exact selective inference without making strict distributional assumptions about the response variable. At the core of our pivot is the use of external randomization variables, which allows us to utilize all available samples for both selection and inference without partitioning the data into independent subsets or discarding any samples at any step. From simulation studies, we find that: (i) the asymptotic confidence intervals based on our pivot achieve the desired coverage rates, even in cases where sample splitting fails due to insufficient sample size for inference; (ii) our intervals are consistently shorter than those produced by sample splitting across various models and signal settings. We report similar findings when we apply our approach to study risk factors for low birth weights in a publicly accessible dataset of US birth records from 2022.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03059v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yumeng Wang, Snigdha Panigrahi, Xuming He</dc:creator>
    </item>
    <item>
      <title>Nystr\"om Kernel Stein Discrepancy</title>
      <link>https://arxiv.org/abs/2406.08401</link>
      <description>arXiv:2406.08401v3 Announce Type: replace-cross 
Abstract: Kernel methods underpin many of the most successful approaches in data science and statistics, and they allow representing probability measures as elements of a reproducing kernel Hilbert space without loss of information. Recently, the kernel Stein discrepancy (KSD), which combines Stein's method with the flexibility of kernel techniques, gained considerable attention. Through the Stein operator, KSD allows the construction of powerful goodness-of-fit tests where it is sufficient to know the target distribution up to a multiplicative constant. However, the typical U- and V-statistic-based KSD estimators suffer from a quadratic runtime complexity, which hinders their application in large-scale settings. In this work, we propose a Nystr\"om-based KSD acceleration -- with runtime $\mathcal O\left(mn+m^3\right)$ for $n$ samples and $m\ll n$ Nystr\"om points -- , show its $\sqrt{n}$-consistency with a classical sub-Gaussian assumption, and demonstrate its applicability for goodness-of-fit testing on a suite of benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08401v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian Kalinke, Zoltan Szabo, Bharath K. Sriperumbudur</dc:creator>
    </item>
    <item>
      <title>New matrix perturbation bounds via combinatorial expansion I: Perturbation of eigenspaces</title>
      <link>https://arxiv.org/abs/2409.20207</link>
      <description>arXiv:2409.20207v4 Announce Type: replace-cross 
Abstract: Matrix perturbation bounds (such as Weyl and Davis-Kahan) are frequently used in many branches of mathematics. Most of the classical results in this area are optimal, in the worst case analysis. However, in modern applications, both the ground and the nose matrices frequently have extra structural properties. For instance, it is often assumed that the ground matrix is essentially low rank, and the nose matrix is random or pseudo-random. We aim to rebuild a part of perturbation theory, adapting to these modern assumptions. The key idea is to exploit the skewness between the leading eigenvectors of the ground matrix and the noise matrix. We will do this by combining the classical contour integration method with combinatorial ideas, resulting in a new machinery, which has a wide range of applications. Our new bounds are optimal under mild assumptions, with direct applications to central problems in many different areas. Among others, we derive a sharp result for the perturbation of a low rank matrix with random perturbation, answering an open question in this area. Next, we derive new, optimal, results concerning covariance estimator of the spiked model, an important model in statistics, bridging two different directions of current research. Finally, and somewhat unexpectedly, we can use our results on the perturbation of eigenspaces to derive new results concerning eigenvalues of deterministic and random matrices. In particular, we obtain new results concerning the outliers in the deformed Wigner model and the least singular value of random matrices with non-zero mean.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20207v4</guid>
      <category>math.SP</category>
      <category>math.CO</category>
      <category>math.FA</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Phuc Tran, Van Vu</dc:creator>
    </item>
  </channel>
</rss>
