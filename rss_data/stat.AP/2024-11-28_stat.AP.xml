<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 28 Nov 2024 05:01:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>GeneralizIT: A Python Solution for Generalizability Theory Computations</title>
      <link>https://arxiv.org/abs/2411.17880</link>
      <description>arXiv:2411.17880v1 Announce Type: new 
Abstract: GeneralizIT is a Python package designed to streamline the application of Generalizability Theory (G-Theory) in research and practice. G-Theory extends classical test theory by estimating multiple sources of error variance, providing a more flexible and detailed approach to reliability assessment. Despite its advantages, G-Theory's complexity can present a significant barrier to researchers. GeneralizIT addresses this challenge by offering an intuitive, user-friendly mechanism to calculate variance components, generalizability coefficients E*rho^2 and dependability Phi and to perform decision (D) studies. D-Studies allow users to make decisions about potential study designs and target improvements in the reliability of certain facets. The package supports both fully crossed and nested designs, enabling users to perform in-depth reliability analysis with minimal coding effort. With built-in visualization tools and detailed reporting functions, GeneralizIT empowers researchers across disciplines, such as education, psychology, healthcare, and the social sciences, to harness the power of G-Theory for robust evidence-based insights. Whether applied to small or large datasets, GeneralizIT offers an accessible and computationally efficient solution to improve measurement reliability in complex data environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17880v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tyler J. Smith, Theresa Kline, Adrienne Kline</dc:creator>
    </item>
    <item>
      <title>Defective regression models for cure rate modeling in Marshall-Olkin family</title>
      <link>https://arxiv.org/abs/2411.17841</link>
      <description>arXiv:2411.17841v1 Announce Type: cross 
Abstract: Regression model have a substantial impact on interpretation of treatments, genetic characteristics and other covariates in survival analysis. In many datasets, the description of censoring and survival curve reveals the presence of cure fraction on data, which leads to alternative modelling. The most common approach to introduce covariates under a parametric estimation are the cure rate models and their variations, although the use of defective distributions have introduced a more parsimonious and integrated approach. Defective distributions is given by a density function whose integration is not one after changing the domain of one the parameters. In this work, we introduce two new defective regression models for long-term survival data in the Marshall-Olkin family: the Marshall-Olkin Gompertz and the Marshall-Olkin inverse Gaussian. The estimation process is conducted using the maximum likelihood estimation and Bayesian inference. We evaluate the asymptotic properties of the classical approach in Monte Carlo studies as well as the behavior of Bayes estimates with vague information. The application of both models under classical and Bayesian inferences is provided in an experiment of time until death from colon cancer with a dichotomous covariate. The Marshall-Olkin Gompertz regression presented the best adjustment and we present some global diagnostic and residual analysis for this proposal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17841v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dionisio Alves da Silva Neto, Vera Lucia Damasceno Tomazella</dc:creator>
    </item>
    <item>
      <title>Bayesian Variable Selection for High-Dimensional Mediation Analysis: Application to Metabolomics Data in Epidemiological Studies</title>
      <link>https://arxiv.org/abs/2411.17910</link>
      <description>arXiv:2411.17910v1 Announce Type: cross 
Abstract: In epidemiological research, causal models incorporating potential mediators along a pathway are crucial for understanding how exposures influence health outcomes. This work is motivated by integrated epidemiological and blood biomarker studies, investigating the relationship between long-term adherence to a Mediterranean diet and cardiometabolic health, with plasma metabolomes as potential mediators. Analyzing causal mediation in such high-dimensional omics data presents substantial challenges, including complex dependencies among mediators and the need for advanced regularization or Bayesian techniques to ensure stable and interpretable estimation and selection of indirect effects. To this end, we propose a novel Bayesian framework for identifying active pathways and estimating indirect effects in the presence of high-dimensional multivariate mediators. Our approach adopts a multivariate stochastic search variable selection method, tailored for such complex mediation scenarios. Central to our method is the introduction of a set of priors for the selection: a Markov random field prior and sequential subsetting Bernoulli priors. The first prior's Markov property leverages the inherent correlations among mediators, thereby increasing power to detect mediated effects. The sequential subsetting aspect of the second prior encourages the simultaneous selection of relevant mediators and their corresponding indirect effects from the two model parts, providing a more coherent and efficient variable selection framework, specific to mediation analysis. Comprehensive simulation studies demonstrate that the proposed method provides superior power in detecting active mediating pathways. We further illustrate the practical utility of the method through its application to metabolome data from two cohort studies, highlighting its effectiveness in real data setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17910v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Youngho Bae, Chanmin Kim, Fenglei Wang, Qi Sun, Kyu Ha Lee</dc:creator>
    </item>
    <item>
      <title>Enhancing Project Performance Forecasting using Machine Learning Techniques</title>
      <link>https://arxiv.org/abs/2411.17914</link>
      <description>arXiv:2411.17914v1 Announce Type: cross 
Abstract: Accurate forecasting of project performance metrics is crucial for successfully managing and delivering urban road reconstruction projects. Traditional methods often rely on static baseline plans and fail to consider the dynamic nature of project progress and external factors. This research proposes a machine learning-based approach to forecast project performance metrics, such as cost variance and earned value, for each Work Breakdown Structure (WBS) category in an urban road reconstruction project. The proposed model utilizes time series forecasting techniques, including Autoregressive Integrated Moving Average (ARIMA) and Long Short-Term Memory (LSTM) networks, to predict future performance based on historical data and project progress. The model also incorporates external factors, such as weather patterns and resource availability, as features to enhance the accuracy of forecasts. By applying the predictive power of machine learning, the performance forecasting model enables proactive identification of potential deviations from the baseline plan, which allows project managers to take timely corrective actions. The research aims to validate the effectiveness of the proposed approach using a case study of an urban road reconstruction project, comparing the model's forecasts with actual project performance data. The findings of this research contribute to the advancement of project management practices in the construction industry, offering a data-driven solution for improving project performance monitoring and control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17914v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soheila Sadeghi</dc:creator>
    </item>
    <item>
      <title>Ridge Regression for Manifold-valued Time-Series with Application to Meteorological Forecast</title>
      <link>https://arxiv.org/abs/2411.18339</link>
      <description>arXiv:2411.18339v1 Announce Type: cross 
Abstract: We propose a natural intrinsic extension of the ridge regression from Euclidean spaces to general manifolds, which relies on Riemannian least-squares fitting, empirical covariance, and Mahalanobis distance. We utilize it for time-series prediction and apply the approach to forecast hurricane tracks and their wind speeds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18339v1</guid>
      <category>math.DG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Esfandiar Nava-Yazdani</dc:creator>
    </item>
    <item>
      <title>Bhirkuti's Test of Bias Acceptance: Examining in Psychometric Simulations</title>
      <link>https://arxiv.org/abs/2411.18481</link>
      <description>arXiv:2411.18481v1 Announce Type: cross 
Abstract: This study introduces Bhirkuti's Test of Bias Acceptance, a systematic graphical framework for evaluating bias and determining its acceptability under varying experimental conditions. Absolute Relative Bias (ARB), while useful for understanding bias, is sensitive to outliers and population parameter magnitudes, often overstating bias for small values and understating it for larger ones. Similarly, Relative Efficiency (RE) can be influenced by variance differences and outliers, occasionally producing counterintuitive values exceeding 100%, which complicates interpretation. By addressing the limitations of traditional metrics such as Absolute Relative Bias (ARB) and Relative Efficiency (RE), the proposed graphical methodology framework leverages ridgeline plots and standardized estimate to provide a comprehensive visualization of parameter estimate distributions. Ridgeline plots done this way offer a robust alternative by visualizing full distributions, highlighting variability, trends, outliers, descriptive and facilitating more informed decision-making. This study employs multivariate Latent Growth Models (LGM) and Monte Carlo simulations to examine the performance of growth curve modeling under planned missing data designs, focusing on parameter estimate recovery and efficiency. By combining innovative visualization techniques with rigorous simulation methods, Bhirkuti's Test of Bias Acceptance provides a versatile and interpretable toolset for advancing quantitative research in bias evaluation and efficiency assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18481v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.OT</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aneel Bhusal, Todd D. Little</dc:creator>
    </item>
    <item>
      <title>Functional relevance based on the continuous Shapley value</title>
      <link>https://arxiv.org/abs/2411.18575</link>
      <description>arXiv:2411.18575v1 Announce Type: cross 
Abstract: The presence of Artificial Intelligence (AI) in our society is increasing, which brings with it the need to understand the behaviour of AI mechanisms, including machine learning predictive algorithms fed with tabular data, text, or images, among other types of data. This work focuses on interpretability of predictive models based on functional data. Designing interpretability methods for functional data models implies working with a set of features whose size is infinite. In the context of scalar on function regression, we propose an interpretability method based on the Shapley value for continuous games, a mathematical formulation that allows to fairly distribute a global payoff among a continuous set players. The method is illustrated through a set of experiments with simulated and real data sets. The open source Python package ShapleyFDA is also presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18575v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pedro Delicado, Cristian Pach\'on-Garc\'ia</dc:creator>
    </item>
    <item>
      <title>Digital Twin-Centered Hybrid Data-Driven Multi-Stage Deep Learning Framework for Enhanced Nuclear Reactor Power Prediction</title>
      <link>https://arxiv.org/abs/2211.13157</link>
      <description>arXiv:2211.13157v4 Announce Type: replace 
Abstract: The accurate and efficient modeling of nuclear reactor transients is crucial for ensuring safe and optimal reactor operation. Traditional physics-based models, while valuable, can be computationally intensive and may not fully capture the complexities of real-world reactor behavior. This paper introduces a novel hybrid digital twin-focused multi-stage deep learning framework that addresses these limitations, offering a faster and more robust solution for predicting the final steady-state power of reactor transients. By leveraging a combination of feed-forward neural networks with both classification and regression stages, and training on a unique dataset that integrates real-world measurements of reactor power and controls state from the Missouri University of Science and Technology Reactor (MSTR) with noise-enhanced simulated data, our approach achieves remarkable accuracy (96% classification, 2.3% MAPE). The incorporation of simulated data with noise significantly improves the model's generalization capabilities, mitigating the risk of overfitting. Designed as a digital twin supporting system, this framework integrates real-time, synchronized predictions of reactor state transitions, enabling dynamic operational monitoring and optimization. This innovative solution not only enables rapid and precise prediction of reactor behavior but also has the potential to revolutionize nuclear reactor operations, facilitating enhanced safety protocols, optimized performance, and streamlined decision-making processes. By aligning data-driven insights with the principles of digital twins, this work lays the groundwork for adaptable and scalable solutions in nuclear system management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.13157v4</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Daniell, Kazuma Kobayashi, Ayodeji Alajo, Syed Bahauddin Alam</dc:creator>
    </item>
    <item>
      <title>Joint modeling of wind speed and wind direction through a conditional approach</title>
      <link>https://arxiv.org/abs/2211.13612</link>
      <description>arXiv:2211.13612v3 Announce Type: replace 
Abstract: Atmospheric near surface wind speed and wind direction play an important role in many applications, ranging from air quality modeling, building design, wind turbine placement to climate change research. It is therefore crucial to accurately estimate the joint probability distribution of wind speed and direction. In this work we develop a conditional approach to model these two variables, where the joint distribution is decomposed into the product of the marginal distribution of wind direction and the conditional distribution of wind speed given wind direction. To accommodate the circular nature of wind direction a von Mises mixture model is used; the conditional wind speed distribution is modeled as a directional dependent Weibull distribution via a two-stage estimation procedure, consisting of a directional binned Weibull parameter estimation, followed by a harmonic regression to estimate the dependence of the Weibull parameters on wind direction. A Monte Carlo simulation study indicates that our method outperforms an alternative method that uses periodic spline quantile regression in terms of estimation efficiency. We illustrate our method by using the output from a regional climate model to investigate how the joint distribution of wind speed and direction may change under some future climate scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.13612v3</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eva Murphy, Whitney Huang, Julie Bessac, Jiali Wang, Rao Kotamarthi</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference of Time-Varying Origin-Destination Matrices from Boarding/Alighting Counts for Transit Services</title>
      <link>https://arxiv.org/abs/2403.04742</link>
      <description>arXiv:2403.04742v2 Announce Type: replace 
Abstract: Origin-destination (OD) demand matrices are crucial for transit agencies to design and operate transit systems. This paper presents a novel temporal Bayesian model designed to estimate transit OD matrices at the individual bus-journey level from boarding/alighting counts at bus stops. Our approach begins by modeling the number of alighting passengers at subsequent bus stops, given a boarding stop, through a multinomial distribution parameterized by alighting probabilities. Given the large scale of the problem, we generate alighting probabilities with a latent variable matrix and factorize it into a mapping matrix and a temporal matrix, thereby substantially reducing the number of parameters. To further encode a temporally-smooth structure in the parameters, we impose a Gaussian process prior on the columns of the temporal factor matrix. For model inference, we develop a two-stage algorithm with the Markov chain Monte Carlo (MCMC) method. In the first stage, latent OD matrices are sampled conditional on model parameters using a Metropolis-Hastings sampling algorithm with a Markov model-based proposal distribution. In the second stage, we sample model parameters conditional on latent OD matrices using slice and elliptical slice sampling algorithms. We assess the proposed model using real-world data collected from three bus routes with varying numbers of stops, and the results demonstrate that our model achieves accurate posterior mean estimation and outperforms the widely used iterative proportional fitting (IPF) method. Additionally, our model can provide uncertainty quantification for the OD demand matrices, thus benefiting many downstream planning/operational tasks that require robust decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04742v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoxu Chen, Zhanhong Cheng, Lijun Sun</dc:creator>
    </item>
    <item>
      <title>Bayesian Hierarchical Probabilistic Forecasting of Intraday Electricity Prices</title>
      <link>https://arxiv.org/abs/2403.05441</link>
      <description>arXiv:2403.05441v3 Announce Type: replace 
Abstract: We address the need for forecasting methodologies that handle large uncertainties in electricity prices for continuous intraday markets by incorporating parameter uncertainty and using a broad set of covariables. This study presents the first Bayesian forecasting of electricity prices traded on the German intraday market. Endogenous and exogenous covariables are handled via Orthogonal Matching Pursuit (OMP) and regularising priors. The target variable is the IDFull price index, with forecasts given as posterior predictive distributions. Validation uses the highly volatile 2022 electricity prices, which have seldom been studied. As a benchmark, we use all intraday transactions at the time of forecast to compute a live IDFull value. According to market efficiency, it should not be possible to improve on this last-price benchmark. However, we observe significant improvements in point measures and probability scores, including an average reduction of $5.9\,\%$ in absolute errors and an average increase of $1.7\,\%$ in accuracy when forecasting whether the IDFull exceeds the day-ahead price. Finally, we challenge the use of LASSO in electricity price forecasting, showing that OMP results in superior performance, specifically an average reduction of $22.7\,\%$ in absolute error and $20.2\,\%$ in the continuous ranked probability score.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05441v3</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Daniel Nickelsen, Gernot M\"uller</dc:creator>
    </item>
    <item>
      <title>Scalable Spatiotemporal Prediction with Bayesian Neural Fields</title>
      <link>https://arxiv.org/abs/2403.07657</link>
      <description>arXiv:2403.07657v3 Announce Type: replace-cross 
Abstract: Spatiotemporal datasets, which consist of spatially-referenced time series, are ubiquitous in diverse applications, such as air pollution monitoring, disease tracking, and cloud-demand forecasting. As the scale of modern datasets increases, there is a growing need for statistical methods that are flexible enough to capture complex spatiotemporal dynamics and scalable enough to handle many observations. This article introduces the Bayesian Neural Field (BayesNF), a domain-general statistical model that infers rich spatiotemporal probability distributions for data-analysis tasks including forecasting, interpolation, and variography. BayesNF integrates a deep neural network architecture for high-capacity function estimation with hierarchical Bayesian inference for robust predictive uncertainty quantification. Evaluations against prominent baselines show that BayesNF delivers improvements on prediction problems from climate and public health data containing tens to hundreds of thousands of measurements. Accompanying the paper is an open-source software package (https://github.com/google/bayesnf) that runs on GPU and TPU accelerators through the JAX machine learning platform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07657v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1038/s41467-024-51477-5</arxiv:DOI>
      <arxiv:journal_reference>Nature Communications 15(7942), 2024</arxiv:journal_reference>
      <dc:creator>Feras Saad, Jacob Burnim, Colin Carroll, Brian Patton, Urs K\"oster, Rif A. Saurous, Matthew Hoffman</dc:creator>
    </item>
    <item>
      <title>Spatial-temporal evolution characteristics and driving factors of carbon emission prediction in China-research on ARIMA-BP neural network algorithm</title>
      <link>https://arxiv.org/abs/2409.00039</link>
      <description>arXiv:2409.00039v2 Announce Type: replace-cross 
Abstract: China accounts for one-third of the world's total carbon emissions. How to reach the peak of carbon emissions by 2030 and achieve carbon neutrality by 2060 to ensure the effective realization of the "dual-carbon" target is an important policy orientation at present. Based on the provincial panel data of ARIMA-BP model, this paper shows that the effect of energy consumption intensity effect is the main factor driving the growth of carbon emissions, per capita GDP and energy consumption structure effect are the main factors to inhibit carbon emissions, and the effect of industrial structure and population size effect is relatively small. Based on the research conclusion, the policy suggestions are put forward from the aspects of energy structure, industrial structure, new quality productivity and digital economy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00039v2</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Thu, 28 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3389/fenvs.2024.1497941</arxiv:DOI>
      <arxiv:journal_reference>Frontiers in Environmental Science(2024)</arxiv:journal_reference>
      <dc:creator>Zhao Sanglin, Li Zhetong, Deng Hao, You Xing, Tong Jiaang, Yuan Bingkun, Zeng Zihao</dc:creator>
    </item>
  </channel>
</rss>
