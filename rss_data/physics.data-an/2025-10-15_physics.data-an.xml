<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 16 Oct 2025 01:46:41 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A mathematical theory for understanding when abstract representations emerge in neural networks</title>
      <link>https://arxiv.org/abs/2510.09816</link>
      <description>arXiv:2510.09816v1 Announce Type: cross 
Abstract: Recent experiments reveal that task-relevant variables are often encoded in approximately orthogonal subspaces of the neural activity space. These disentangled low-dimensional representations are observed in multiple brain areas and across different species, and are typically the result of a process of abstraction that supports simple forms of out-of-distribution generalization. The mechanisms by which such geometries emerge remain poorly understood, and the mechanisms that have been investigated are typically unsupervised (e.g., based on variational auto-encoders). Here, we show mathematically that abstract representations of latent variables are guaranteed to appear in the last hidden layer of feedforward nonlinear networks when they are trained on tasks that depend directly on these latent variables. These abstract representations reflect the structure of the desired outputs or the semantics of the input stimuli. To investigate the neural representations that emerge in these networks, we develop an analytical framework that maps the optimization over the network weights into a mean-field problem over the distribution of neural preactivations. Applying this framework to a finite-width ReLU network, we find that its hidden layer exhibits an abstract representation at all global minima of the task objective. We further extend these analyses to two broad families of activation functions and deep feedforward architectures, demonstrating that abstract representations naturally arise in all these scenarios. Together, these results provide an explanation for the widely observed abstract representations in both the brain and artificial neural networks, as well as a mathematically tractable toolkit for understanding the emergence of different kinds of representations in task-optimized, feature-learning network models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09816v1</guid>
      <category>q-bio.NC</category>
      <category>math.OC</category>
      <category>physics.bio-ph</category>
      <category>physics.data-an</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bin Wang, W. Jeffrey Johnston, Stefano Fusi</dc:creator>
    </item>
    <item>
      <title>Non-Normal Eigenvector Amplification in Multi-Dimensional Kesten Processes</title>
      <link>https://arxiv.org/abs/2510.11763</link>
      <description>arXiv:2510.11763v1 Announce Type: cross 
Abstract: Heavy-tailed fluctuations and power law statistics pervade physics, finance, and economics, yet their origin is often ascribed to systems poised near criticality. Here we show that such behavior can emerge far from instability through a universal mechanism of non-normal eigenvector amplification in multidimensional Kesten processes $x_{t+1}=A_t x_t+\eta_t$, where $A_t$ are random interaction matrices and $\eta_t$ represents external inputs, capturing the evolving interdependence among $N$ coupled components. Even when each random multiplicative matrix is spectrally stable, non-orthogonal eigenvectors generate transient growth that renormalizes the Lyapunov exponent and lowers the tail exponent, producing stationary power laws without eigenvalues crossing the stability boundary. We derive explicit relations linking the Lyapunov exponent and the tail index to the statistics of the condition number, $\gamma\!\sim\!\gamma_0+\ln\kappa$ and $\alpha\!\sim\!-2\gamma/\sigma_\kappa^2$, confirmed by numerical simulations. This framework offers a unifying geometric perspective that help interpret diverse phenomena, including polymer stretching in turbulence, magnetic field amplification in dynamos, volatility clustering and wealth inequality in financial systems. Non-normal interactions provide a collective route to scale-free behavior in globally stable systems, defining a new universality class where multiplicative feedback and transient amplification generate critical-like statistics without spectral criticality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11763v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Virgile Troude, Didier Sornette</dc:creator>
    </item>
    <item>
      <title>Maximum entropy temporal networks</title>
      <link>https://arxiv.org/abs/2509.02098</link>
      <description>arXiv:2509.02098v3 Announce Type: replace-cross 
Abstract: Temporal networks consist of timestamped directed interactions that may appear continuously in time, yet few studies have directly tackled the continuous-time modeling of networks. Here, we introduce a maximum-entropy approach to temporal networks and with basic assumptions on constraints, the corresponding network ensembles admit a modular and interpretable representation: a set of global time processes and a static maximum-entropy edge, e.g. node pair, probability. This time-edge labels factorization yields closed-form log-likelihoods, degree, clustering and motif expectations, and yields a whole class of effective generative models. We provide maximum-entropy derivation of an inhomogeneous Poisson edge intensity for temporal networks via functional optimization over path entropy, connecting NHPP modeling to maximum-entropy network ensembles. NHPP consistently improve log-likelihood over generic Poisson processes, while the maximum-entropy edge labels recover strength constraints and reproduce expected unique-degree curves. We discuss the limitations of this framework and how it can be integrated with multivariate Hawkes calibration procedures, renewal theory, and neural kernel estimation in graph neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02098v3</guid>
      <category>cs.SI</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paolo Barucca</dc:creator>
    </item>
  </channel>
</rss>
