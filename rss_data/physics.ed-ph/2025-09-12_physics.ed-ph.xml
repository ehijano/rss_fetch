<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.ed-ph updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.ed-ph</link>
    <description>physics.ed-ph updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.ed-ph" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 12 Sep 2025 04:03:42 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Examining teaching assistant pedagogies in traditional laboratories and recitations</title>
      <link>https://arxiv.org/abs/2509.09136</link>
      <description>arXiv:2509.09136v1 Announce Type: new 
Abstract: Physics education research has consistently shown that students have higher learning outcomes when enrolled in active learning courses. However, while there is a lot of literature describing the difference between the two extremes of traditional vs. active learning courses, research has also shown that many classrooms actually lie on a spectrum rather than firmly falling into one category or the other. Understanding the pedagogical landscape is important for curricular development and dissemination, as well as targeted professional development efforts. Replicating and expanding on a study done by West et al., we observe graduate student Teaching Assistants (TAs) facilitating introductory physics labs and recitations using the Real-time Instructor Observing Tool (RIOT). We confirm West's finding of large variation between TAs' interactions during recitation sessions, but we also find that TAs facilitating traditional labs display fairly similar interaction profiles to each other. Additionally, we find that both the recitations and lab sessions we studied displayed very different interaction patterns from the CLASP ``Discussion/Labs'' studied by West et al. Specifically, we find that the amount of time instructors spend observing students is a key distinguishing characteristic between the traditional settings and the CLASP curriculum. We discuss the pedagogical features of each of the different learning environments as captured by RIOT. We share our results as a snapshot of the interactive elements of an introductory physics course at a four-year, public, master's granting institution situated in a discussion on implications for reform efforts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09136v1</guid>
      <category>physics.ed-ph</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Eric M. Hickok, Cassandra A. Paul</dc:creator>
    </item>
    <item>
      <title>Are arXiv submissions on Wednesday better cited? Introducing Big Data methods in undergraduate courses on scientific computing</title>
      <link>https://arxiv.org/abs/2509.09601</link>
      <description>arXiv:2509.09601v1 Announce Type: new 
Abstract: Extracting information from big data sets, both real and simulated, is a modern hallmark of the physical sciences. In practice, students face barriers to learning ``Big Data'' methods in undergraduate physics and astronomy curricula. As an attempt to alleviate some of these challenges, we present a simple, farm-to-table data analysis pipeline that can collect, process, and plot data from the 800k entries common to the arXiv preprint repository and the bibliographical database inSpireHEP. The pipeline employs contemporary research practices and can be implemented using open-sourced Python libraries common to undergraduate courses on Scientific Computing. To support the use such pipelines in classroom contexts, we make public an example implementation, authored by two undergraduate physics students, that runs on off-the-shelf laptops. For advanced students, we discuss applications of the pipeline, including for online DAQ monitoring and commercialization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09601v1</guid>
      <category>physics.ed-ph</category>
      <category>hep-ex</category>
      <category>physics.comp-ph</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>St\'ephane Delorme, Leon Mach, Hubert Paszkiewicz, Richard Ruiz</dc:creator>
    </item>
    <item>
      <title>Potential Pitfalls in Visual Models of Tipping Points -- And How to Fix Them</title>
      <link>https://arxiv.org/abs/2410.22171</link>
      <description>arXiv:2410.22171v2 Announce Type: replace 
Abstract: Visual models play a crucial role in both science and science communication. However, the distinction between mere analogies and mathematically sound graphical representations is not easy and can be misunderstood not only by laypeople but also within academic literature itself. Moreover, even when the graphical representation exactly corresponds to the mathematical model, its interpretation is often far from obvious. In this paper we discuss the potential landscape visualization commonly used for tipping points in the context of nonlinear dynamics and reveal potential pitfalls, in particular when distinguishing bifurcation induced tipping (B-tipping) from noise-induced tipping (N-tipping). We propose new visualization techniques for tipping dynamics, carefully distinguishing between B- and N-tipping as well as between single systems and ensembles of systems. Explicitly, we apply these visualizations both to molecular cell biology and to climate science in order to reveal the crucial differences in the interpretation of the visual models. We find that it is crucial to explicitly discuss the assumptions made within the visual model and to be aware of the risk of misinterpretation. These findings apply to a wide range of readership, from graduate students - as some general knowledge of nonlinear systems is required - to research professionals working in the field of nonlinear sciences. This paper provides the theoretical groundwork for these new visualizations. As a next step, we propose to investigate the individual mental models that might be induced by these visualizations using empirical research that builds upon these findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22171v2</guid>
      <category>physics.ed-ph</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.63562/2577-8439.1113</arxiv:DOI>
      <arxiv:journal_reference>Northeast Journal of Complex Systems (NEJCS), Vol. 7, No. 3, Article 3 (2025)</arxiv:journal_reference>
      <dc:creator>Jonathan Dechert, Svetlana Gurevich, Stefan Heusler</dc:creator>
    </item>
    <item>
      <title>Multimodal large language models and physics visual tasks: comparative analysis of performance and costs</title>
      <link>https://arxiv.org/abs/2506.19662</link>
      <description>arXiv:2506.19662v2 Announce Type: replace 
Abstract: Multimodal large language models (MLLMs) capable of processing both text and visual inputs are increasingly being explored for uses in physics education, such as tutoring, formative assessment, and grading. This study evaluates a range of publicly available MLLMs on a set of standardized, image-based physics research-based conceptual assessments (concept inventories). We benchmark 15 models from three major providers (Anthropic, Google, and OpenAI) across 102 physics items, focusing on two main questions: (1) How well do these models perform on conceptual physics tasks involving visual representations? and (2) What are the financial costs associated with their use? The results show high variability in both performance and cost. The performance of the tested models ranges from 81.5% to as low as 21%. We also found that expensive models do not always outperform cheaper ones and that, depending on the demands of the context, cheaper models may be sufficiently capable for some tasks. This is especially relevant in contexts where financial resources are limited or for large-scale educational implementation of MLLMs. By providing these analyses, our aim is to inform teachers, institutions, and other educational stakeholders so that they can make evidence-based decisions about the selection of models for use in AI-supported physics education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19662v2</guid>
      <category>physics.ed-ph</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1088/1361-6404/ae03f8</arxiv:DOI>
      <dc:creator>Giulia Polverini, Bor Gregorcic</dc:creator>
    </item>
  </channel>
</rss>
