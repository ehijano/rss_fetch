<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>eess.IV updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/eess.IV</link>
    <description>eess.IV updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/eess.IV" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 18 Jul 2024 01:45:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 17 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Survey of Distance-Based Vessel Trajectory Clustering: Data Pre-processing, Methodologies, Applications, and Experimental Evaluation</title>
      <link>https://arxiv.org/abs/2407.11084</link>
      <description>arXiv:2407.11084v1 Announce Type: new 
Abstract: Vessel trajectory clustering, a crucial component of the maritime intelligent transportation systems, provides valuable insights for applications such as anomaly detection and trajectory prediction. This paper presents a comprehensive survey of the most prevalent distance-based vessel trajectory clustering methods, which encompass two main steps: trajectory similarity measurement and clustering. Initially, we conducted a thorough literature review using relevant keywords to gather and summarize pertinent research papers and datasets. Then, this paper discussed the principal methods of data pre-processing that prepare data for further analysis. The survey progresses to detail the leading algorithms for measuring vessel trajectory similarity and the main clustering techniques used in the field today. Furthermore, the various applications of trajectory clustering within the maritime context are explored. Finally, the paper evaluates the effectiveness of different algorithm combinations and pre-processing methods through experimental analysis, focusing on their impact on the performance of distance-based trajectory clustering algorithms. The experimental results demonstrate the effectiveness of various trajectory clustering algorithms and notably highlight the significant improvements that trajectory compression techniques contribute to the efficiency and accuracy of trajectory clustering. This comprehensive approach ensures a deep understanding of current capabilities and future directions in vessel trajectory clustering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11084v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Maohan Liang, Ryan Wen Liu, Ruobin Gao, Zhe Xiao, Xiaocai Zhang, Hua Wang</dc:creator>
    </item>
    <item>
      <title>Restore-RWKV: Efficient and Effective Medical Image Restoration with RWKV</title>
      <link>https://arxiv.org/abs/2407.11087</link>
      <description>arXiv:2407.11087v1 Announce Type: new 
Abstract: Transformers have revolutionized medical image restoration, but the quadratic complexity still poses limitations for their application to high-resolution medical images. The recent advent of RWKV in the NLP field has attracted much attention as it can process long sequences efficiently. To leverage its advanced design, we propose Restore-RWKV, the first RWKV-based model for medical image restoration. Since the original RWKV model is designed for 1D sequences, we make two necessary modifications for modeling spatial relations in 2D images. First, we present a recurrent WKV (Re-WKV) attention mechanism that captures global dependencies with linear computational complexity. Re-WKV incorporates bidirectional attention as basic for a global receptive field and recurrent attention to effectively model 2D dependencies from various scan directions. Second, we develop an omnidirectional token shift (Omni-Shift) layer that enhances local dependencies by shifting tokens from all directions and across a wide context range. These adaptations make the proposed Restore-RWKV an efficient and effective model for medical image restoration. Extensive experiments demonstrate that Restore-RWKV achieves superior performance across various medical image restoration tasks, including MRI image super-resolution, CT image denoising, PET image synthesis, and all-in-one medical image restoration. Code is available at: \href{https://github.com/Yaziwel/Restore-RWKV.git}{https://github.com/Yaziwel/Restore-RWKV}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11087v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiwen Yang, Hui Zhang, Dan Zhao, Bingzheng Wei, Yan Xu</dc:creator>
    </item>
    <item>
      <title>DD_RoTIR: Dual-Domain Image Registration via Image Translation and Hierarchical Feature-matching</title>
      <link>https://arxiv.org/abs/2407.11223</link>
      <description>arXiv:2407.11223v2 Announce Type: new 
Abstract: Microscopy images obtained from multiple camera lenses or sensors in biological experiments provide a comprehensive understanding of objects from diverse perspectives. However, using multiple microscope setups increases the risk of misalignment of identical target features across different modalities, making multimodal image registration crucial. In this work, we build upon previous successes in biological image translation (XAcGAN) and mono-modal image registration (RoTIR) to develop a deep learning model, Dual-Domain RoTIR (DD_RoTIR), specifically designed to address these challenges. While GAN-based translation models are often considered inadequate for multimodal image registration, we enhance registration accuracy by employing a feature-matching algorithm based on Transformers and rotation equivariant networks. Additionally, hierarchical feature matching is utilized to tackle the complexities of multimodal image registration. Our results demonstrate that the DD_RoTIR model exhibits strong applicability and robustness across multiple microscopy image datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11223v2</guid>
      <category>eess.IV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruixiong Wang, Stephen Cross, Alin Achim</dc:creator>
    </item>
    <item>
      <title>Distributed Semantic Segmentation with Efficient Joint Source and Task Decoding</title>
      <link>https://arxiv.org/abs/2407.11224</link>
      <description>arXiv:2407.11224v1 Announce Type: new 
Abstract: Distributed computing in the context of deep neural networks (DNNs) implies the execution of one part of the network on edge devices and the other part typically on a large-scale cloud platform. Conventional methods propose to employ a serial concatenation of a learned image and source encoder, the latter projecting the image encoder output (bottleneck features) into a quantized representation for bitrate-efficient transmission. In the cloud, a respective source decoder reprojects the quantized representation to the original feature representation, serving as an input for the downstream task decoder performing, e.g., semantic segmentation. In this work, we propose joint source and task decoding, as it allows for a smaller network size in the cloud. This further enables the scalability of such services in large numbers without requiring extensive computational load on the cloud per channel. We demonstrate the effectiveness of our method by achieving a distributed semantic segmentation SOTA over a wide range of bitrates on the mean intersection over union metric, while using only $9.8 \%$ ... $11.59 \%$ of cloud DNN parameters used in the previous SOTA on the COCO and Cityscapes datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11224v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Danish Nazir, Timo Bartels, Jan Piewek, Thorsten Bagdonat, Tim Fingscheidt</dc:creator>
    </item>
    <item>
      <title>A Self-Correcting Strategy of the Digital Volume Correlation Displacement Field Based on Image Matching: Application to Poor Speckles Quality and Complex-Large Deformation</title>
      <link>https://arxiv.org/abs/2407.11287</link>
      <description>arXiv:2407.11287v1 Announce Type: new 
Abstract: Digital Volume Correlation (DVC) is widely used for the analysis of three-dimensional displacement and strain fields based on CT scans. However, the applicability of DVC methods is limited when it comes to geomaterials: CT speckles are directly correlated with the material's microstructure, and the speckle structure cannot be artificially altered, with generally poor speckle quality. Additionally, most geomaterials exhibit elastoplastic properties and will undergo complex-large deformations under external loading, sometimes leading to strain localization phenomena. These factors contribute to inaccuracies in the displacement field obtained through DVC, and at present, there is a shortage of correction methods and accuracy assessment techniques for the displacement field. If the accuracy of the DVC displacement field is sufficiently high, the gray residue of the two volume images before and after deformation should be minimal, utilizing this characteristic to develop a correction method for the displacement field is feasible. The proposed self-correcting strategy of the DVC displacement field based on image matching, which from the experimental measurement error. We demonstrated the effectiveness of the proposed method by CT triaxial tests of granite residual soil. Without adding other parameters or adjusting the original parameters of DVC, the gray residue showed that the proposed method can effectively improve the accuracy of the displacement field. Additionally, the accuracy evaluation method can reasonably estimate the accuracy of the displacement field. The proposed method can effectively improve the accuracy of DVC three-dimensional displacement field for the state of speckles with poor quality and complex-large deformation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11287v1</guid>
      <category>eess.IV</category>
      <category>cs.GR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chengsheng Li, Zhijun Liu</dc:creator>
    </item>
    <item>
      <title>Zero-Shot Adaptation for Approximate Posterior Sampling of Diffusion Models in Inverse Problems</title>
      <link>https://arxiv.org/abs/2407.11288</link>
      <description>arXiv:2407.11288v1 Announce Type: new 
Abstract: Diffusion models have emerged as powerful generative techniques for solving inverse problems. Despite their success in a variety of inverse problems in imaging, these models require many steps to converge, leading to slow inference time. Recently, there has been a trend in diffusion models for employing sophisticated noise schedules that involve more frequent iterations of timesteps at lower noise levels, thereby improving image generation and convergence speed. However, application of these ideas for solving inverse problems with diffusion models remain challenging, as these noise schedules do not perform well when using empirical tuning for the forward model log-likelihood term weights. To tackle these challenges, we propose zero-shot approximate posterior sampling (ZAPS) that leverages connections to zero-shot physics-driven deep learning. ZAPS fixes the number of sampling steps, and uses zero-shot training with a physics-guided loss function to learn log-likelihood weights at each irregular timestep. We apply ZAPS to the recently proposed diffusion posterior sampling method as baseline, though ZAPS can also be used with other posterior sampling diffusion models. We further approximate the Hessian of the logarithm of the prior using a diagonalization approach with learnable diagonal entries for computational efficiency. These parameters are optimized over a fixed number of epochs with a given computational budget. Our results for various noisy inverse problems, including Gaussian and motion deblurring, inpainting, and super-resolution show that ZAPS reduces inference time, provides robustness to irregular noise schedules and improves reconstruction quality. Code is available at https://github.com/ualcalar17/ZAPS</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11288v1</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ya\c{s}ar Utku Al\c{c}alar, Mehmet Ak\c{c}akaya</dc:creator>
    </item>
    <item>
      <title>BUSClean: Open-source software for breast ultrasound image pre-processing and knowledge extraction for medical AI</title>
      <link>https://arxiv.org/abs/2407.11316</link>
      <description>arXiv:2407.11316v1 Announce Type: new 
Abstract: Development of artificial intelligence (AI) for medical imaging demands curation and cleaning of large-scale clinical datasets comprising hundreds of thousands of images. Some modalities, such as mammography, contain highly standardized imaging. In contrast, breast ultrasound imaging (BUS) can contain many irregularities not indicated by scan metadata, such as enhanced scan modes, sonographer annotations, or additional views. We present an open-source software solution for automatically processing clinical BUS datasets. The algorithm performs BUS scan filtering, cleaning, and knowledge extraction from sonographer annotations. Its modular design enables users to adapt it to new settings. Experiments on an internal testing dataset of 430 clinical BUS images achieve &gt;95% sensitivity and &gt;98% specificity in detecting every type of text annotation, &gt;98% sensitivity and specificity in detecting scans with blood flow highlighting, alternative scan modes, or invalid scans. A case study on a completely external, public dataset of BUS scans found that BUSClean identified text annotations and scans with blood flow highlighting with 88.6% and 90.9% sensitivity and 98.3% and 99.9% specificity, respectively. Adaptation of the lesion caliper detection method to account for a type of caliper specific to the case study demonstrates intended use of BUSClean in new data distributions and improved performance in lesion caliper detection from 43.3% and 93.3% out-of-the-box to 92.1% and 92.3% sensitivity and specificity, respectively. Source code, example notebooks, and sample data are available at https://github.com/hawaii-ai/bus-cleaning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11316v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arianna Bunnell, Kailee Hung, John A. Shepherd, Peter Sadowski</dc:creator>
    </item>
    <item>
      <title>Scalable Extraction Based Semantic Communication for 6G Wireless Networks</title>
      <link>https://arxiv.org/abs/2407.11334</link>
      <description>arXiv:2407.11334v1 Announce Type: new 
Abstract: Due to the challenges of satisfying the demands for communication efficiency and intelligent connectivity, sixth-generation (6G) wireless network requires new communication frameworks to enable effective information exchange and the integrated Artificial Intelligence (AI) and communication. The Deep Learning (DL) based semantic communication, which can integrate application requirements and the data meanings into data processing and transmission, is expected to become a new paradigm in 6G wireless networks. However, existing semantic communications frameworks rely on sending full semantic feature, which can maximize the semantic fidelity but fail to achieve the efficient semantic communications. In this article, we introduce a novel Scalable Extraction based Semantic Communication (SE-SC) model to support the potential applications in 6G wireless networks and then analyze its feasibility. Then, we propose a promising the SE-SC framework to highlight the potentials of SE-SC model in 6G wireless networks. Numerical results show that our proposed SE-SC scheme can offer an identical Quality of Service (QoS) for the downstream task with much fewer transmission symbols than the full semantic feature transmission and the traditional codec scheme. Finally, we discuss several challenges for further investigating the scalable extraction based semantic communications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11334v1</guid>
      <category>eess.IV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuzhou Fu, Wenchi Cheng, Wei Zhang, Jingqing Wang</dc:creator>
    </item>
    <item>
      <title>ReLaX-VQA: Residual Fragment and Layer Stack Extraction for Enhancing Video Quality Assessment</title>
      <link>https://arxiv.org/abs/2407.11496</link>
      <description>arXiv:2407.11496v1 Announce Type: new 
Abstract: With the rapid growth of User-Generated Content (UGC) exchanged between users and sharing platforms, the need for video quality assessment in the wild has emerged. UGC is mostly acquired using consumer devices and undergoes multiple rounds of compression or transcoding before reaching the end user. Therefore, traditional quality metrics that require the original content as a reference cannot be used. In this paper, we propose ReLaX-VQA, a novel No-Reference Video Quality Assessment (NR-VQA) model that aims to address the challenges of evaluating the diversity of video content and the assessment of its quality without reference videos. ReLaX-VQA uses fragments of residual frames and optical flow, along with different expressions of spatial features of the sampled frames, to enhance motion and spatial perception. Furthermore, the model enhances abstraction by employing layer-stacking techniques in deep neural network features (from Residual Networks and Vision Transformers). Extensive testing on four UGC datasets confirms that ReLaX-VQA outperforms existing NR-VQA methods with an average SRCC value of 0.8658 and PLCC value of 0.8872. We will open source the code and trained models to facilitate further research and applications of NR-VQA: https://github.com/xinyiW915/ReLaX-VQA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11496v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.MM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xinyi Wang, Angeliki Katsenou, David Bull</dc:creator>
    </item>
    <item>
      <title>Cross-Phase Mutual Learning Framework for Pulmonary Embolism Identification on Non-Contrast CT Scans</title>
      <link>https://arxiv.org/abs/2407.11529</link>
      <description>arXiv:2407.11529v1 Announce Type: new 
Abstract: Pulmonary embolism (PE) is a life-threatening condition where rapid and accurate diagnosis is imperative yet difficult due to predominantly atypical symptomatology. Computed tomography pulmonary angiography (CTPA) is acknowledged as the gold standard imaging tool in clinics, yet it can be contraindicated for emergency department (ED) patients and represents an onerous procedure, thus necessitating PE identification through non-contrast CT (NCT) scans. In this work, we explore the feasibility of applying a deep-learning approach to NCT scans for PE identification. We propose a novel Cross-Phase Mutual learNing framework (CPMN) that fosters knowledge transfer from CTPA to NCT, while concurrently conducting embolism segmentation and abnormality classification in a multi-task manner. The proposed CPMN leverages the Inter-Feature Alignment (IFA) strategy that enhances spatial contiguity and mutual learning between the dual-pathway network, while the Intra-Feature Discrepancy (IFD) strategy can facilitate precise segmentation of PE against complex backgrounds for single-pathway networks. For a comprehensive assessment of the proposed approach, a large-scale dual-phase dataset containing 334 PE patients and 1,105 normal subjects has been established. Experimental results demonstrate that CPMN achieves the leading identification performance, which is 95.4\% and 99.6\% in patient-level sensitivity and specificity on NCT scans, indicating the potential of our approach as an economical, accessible, and precise tool for PE identification in clinical practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11529v1</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bizhe Bai, Yan-Jie Zhou, Yujian Hu, Tony C. W. Mok, Yilang Xiang, Le Lu, Hongkun Zhang, Minfeng Xu</dc:creator>
    </item>
    <item>
      <title>Uniformly Accelerated Motion Model for Inter Prediction</title>
      <link>https://arxiv.org/abs/2407.11541</link>
      <description>arXiv:2407.11541v1 Announce Type: new 
Abstract: Inter prediction is a key technology to reduce the temporal redundancy in video coding. In natural videos, there are usually multiple moving objects with variable velocity, resulting in complex motion fields that are difficult to represent compactly. In Versatile Video Coding (VVC), existing inter prediction methods usually assume uniform speed motion between consecutive frames and use the linear models for motion estimation (ME) and motion compensation (MC), which may not well handle the complex motion fields in the real world. To address these issues, we introduce a uniformly accelerated motion model (UAMM) to exploit motion-related elements (velocity, acceleration) of moving objects between the video frames, and further combine them to assist the inter prediction methods to handle the variable motion in the temporal domain. Specifically, first, the theory of UAMM is mentioned. Second, based on that, we propose the UAMM-based parameter derivation and extrapolation schemes in the coding process. Third, we integrate the UAMM into existing inter prediction modes (Merge, MMVD, CIIP) to achieve higher prediction accuracy. The proposed method is implemented into the VVC reference software, VTM version 12.0. Experimental results show that the proposed method achieves up to 0.38% and on average 0.13% BD-rate reduction compared to the VTM anchor, under the Low-delay P configuration, with a slight increase of time complexity on the encoding/decoding side.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11541v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhuoyuan Li, Yao Li, Chuanbo Tang, Li Li, Dong Liu, Feng Wu</dc:creator>
    </item>
    <item>
      <title>Rethinking Learned Image Compression: Context is All You Need</title>
      <link>https://arxiv.org/abs/2407.11590</link>
      <description>arXiv:2407.11590v1 Announce Type: new 
Abstract: Since LIC has made rapid progress recently compared to traditional methods, this paper attempts to discuss the question about 'Where is the boundary of Learned Image Compression(LIC)?' with regard to subjective matrics. Thus this paper splits the above problem into two sub-problems:1)Where is the boundary of rate-distortion performance of PSNR? 2)How to further improve the compression gain and achieve the boundary? Therefore this paper analyzes the effectiveness of scaling parameters for encoder, decoder and context model, which are the three components of LIC. Then we conclude that scaling for LIC is to scale for context model and decoder within LIC. Extensive experiments demonstrate that overfitting can actually serve as an effective context. By optimizing the context, this paper further improves PSNR and achieves state-of-the-art performance, showing a performance gain of 14.39% with BD-RATE over VVC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11590v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jixiang Luo</dc:creator>
    </item>
    <item>
      <title>Compressive Electron Backscatter Diffraction Imaging</title>
      <link>https://arxiv.org/abs/2407.11724</link>
      <description>arXiv:2407.11724v1 Announce Type: new 
Abstract: Electron backscatter diffraction (EBSD) has developed over the last few decades into a valuable crystallographic characterisation method for a wide range of sample types. Despite these advances, issues such as the complexity of sample preparation, relatively slow acquisition, and damage in beam-sensitive samples, still limit the quantity and quality of interpretable data that can be obtained. To mitigate these issues, here we propose a method based on the subsampling of probe positions and subsequent reconstruction of an incomplete dataset. The missing probe locations (or pixels in the image) are recovered via an inpainting process using a dictionary-learning based method called beta-process factor analysis (BPFA). To investigate the robustness of both our inpainting method and Hough-based indexing, we simulate subsampled and noisy EBSD datasets from a real fully sampled Ni-superalloy dataset for different subsampling ratios of probe positions using both Gaussian and Poisson noise models. We find that zero solution pixel detection (inpainting un-indexed pixels) enables higher quality reconstructions to be obtained. Numerical tests confirm high quality reconstruction of band contrast and inverse pole figure maps from only 10% of the probe positions, with the potential to reduce this to 5% if only inverse pole figure maps are needed. These results show the potential application of this method in EBSD, allowing for faster analysis and extending the use of this technique to beam sensitive materials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11724v1</guid>
      <category>eess.IV</category>
      <category>cond-mat.mtrl-sci</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zo\"e Broad, Alex W. Robinson, Jack Wells, Daniel Nicholls, Amirafshar Moshtaghpour, Angus I. Kirkland, Nigel D. Browning</dc:creator>
    </item>
    <item>
      <title>Novel Hybrid Integrated Pix2Pix and WGAN Model with Gradient Penalty for Binary Images Denoising</title>
      <link>https://arxiv.org/abs/2407.11865</link>
      <description>arXiv:2407.11865v1 Announce Type: new 
Abstract: This paper introduces a novel approach to image denoising that leverages the advantages of Generative Adversarial Networks (GANs). Specifically, we propose a model that combines elements of the Pix2Pix model and the Wasserstein GAN (WGAN) with Gradient Penalty (WGAN-GP). This hybrid framework seeks to capitalize on the denoising capabilities of conditional GANs, as demonstrated in the Pix2Pix model, while mitigating the need for an exhaustive search for optimal hyperparameters that could potentially ruin the stability of the learning process. In the proposed method, the GAN's generator is employed to produce denoised images, harnessing the power of a conditional GAN for noise reduction. Simultaneously, the implementation of the Lipschitz continuity constraint during updates, as featured in WGAN-GP, aids in reducing susceptibility to mode collapse. This innovative design allows the proposed model to benefit from the strong points of both Pix2Pix and WGAN-GP, generating superior denoising results while ensuring training stability. Drawing on previous work on image-to-image translation and GAN stabilization techniques, the proposed research highlights the potential of GANs as a general-purpose solution for denoising. The paper details the development and testing of this model, showcasing its effectiveness through numerical experiments. The dataset was created by adding synthetic noise to clean images. Numerical results based on real-world dataset validation underscore the efficacy of this approach in image-denoising tasks, exhibiting significant enhancements over traditional techniques. Notably, the proposed model demonstrates strong generalization capabilities, performing effectively even when trained with synthetic noise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11865v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luca Tirel, Ali Mohamed Ali, Hashim A. Hashim</dc:creator>
    </item>
    <item>
      <title>dAJC: A 2.02mW 50Mbps Direct Analog to MJPEG Converter for Video Sensor Node using Low-Noise Switched Capacitor MAC-Quantizer with Auto-Calibration and Sparsity-Aware ADC</title>
      <link>https://arxiv.org/abs/2407.11023</link>
      <description>arXiv:2407.11023v1 Announce Type: cross 
Abstract: With the advancement in the field of the Internet of Things(IoT) and Internet of Bodies(IoB), video camera applications using Video Sensor Nodes(VSNs) have gained importance in the field of autonomous driving, health monitoring, robot control, and security camera applications. However, these applications typically involve high data rates due to the transmission of high-resolution video signals, resulting from high data volume generated from the analog-to-digital converters (ADCs). This significant data deluge poses processing and storage overheads, exacerbating the problem. To address this challenge, we propose a low-power solution aimed at reducing the power consumption in Video Sensor Nodes (VSNs) by shifting the computation from the digital domain to the inherently energy-efficient analog domain. Unlike standard architectures where computation and processing are typically performed in digital signal processing (DSP) blocks after the ADCs, our approach eliminates the need for such blocks. Instead, we leverage a switched capacitor-based computation unit in the analog domain, resulting in a reduction in power consumption. We achieve a $\sim4X$ reduction in power consumption compared to digital implementations. Furthermore, we employ a sparsity-aware ADC, which is enabled only for significant compressed samples that contribute to a small fraction ($\le5\%$) of the total captured analog samples, we achieve a $\sim20X$ lower ADC conversion energy without any considerable degradation, contributing to the overall energy savings in the system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11023v1</guid>
      <category>eess.SP</category>
      <category>eess.IV</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gourab Barik (Purdue University, West Lafayette, USA), Gaurav Kumar K (Purdue University, West Lafayette, USA), Baibhab Chatterjee (Purdue University, West Lafayette, USA, University of Florida, Gainsville, USA), Shovan Maity (Quasistatics Inc. West Lafayette), Sumon Bose (Quasistatics Inc. West Lafayette), Shreyas Sen (Purdue University, West Lafayette, USA)</dc:creator>
    </item>
    <item>
      <title>TLRN: Temporal Latent Residual Networks For Large Deformation Image Registration</title>
      <link>https://arxiv.org/abs/2407.11219</link>
      <description>arXiv:2407.11219v1 Announce Type: cross 
Abstract: This paper presents a novel approach, termed {\em Temporal Latent Residual Network (TLRN)}, to predict a sequence of deformation fields in time-series image registration. The challenge of registering time-series images often lies in the occurrence of large motions, especially when images differ significantly from a reference (e.g., the start of a cardiac cycle compared to the peak stretching phase). To achieve accurate and robust registration results, we leverage the nature of motion continuity and exploit the temporal smoothness in consecutive image frames. Our proposed TLRN highlights a temporal residual network with residual blocks carefully designed in latent deformation spaces, which are parameterized by time-sequential initial velocity fields. We treat a sequence of residual blocks over time as a dynamic training system, where each block is designed to learn the residual function between desired deformation features and current input accumulated from previous time frames. We validate the effectivenss of TLRN on both synthetic data and real-world cine cardiac magnetic resonance (CMR) image videos. Our experimental results shows that TLRN is able to achieve substantially improved registration accuracy compared to the state-of-the-art. Our code is publicly available at https://github.com/nellie689/TLRN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11219v1</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nian Wu, Jiarui Xing, Miaomiao Zhang</dc:creator>
    </item>
    <item>
      <title>UP-Diff: Latent Diffusion Model for Remote Sensing Urban Prediction</title>
      <link>https://arxiv.org/abs/2407.11578</link>
      <description>arXiv:2407.11578v2 Announce Type: cross 
Abstract: This study introduces a novel Remote Sensing (RS) Urban Prediction (UP) task focused on future urban planning, which aims to forecast urban layouts by utilizing information from existing urban layouts and planned change maps. To address the proposed RS UP task, we propose UP-Diff, which leverages a Latent Diffusion Model (LDM) to capture positionaware embeddings of pre-change urban layouts and planned change maps. In specific, the trainable cross-attention layers within UP-Diff's iterative diffusion modules enable the model to dynamically highlight crucial regions for targeted modifications. By utilizing our UP-Diff, designers can effectively refine and adjust future urban city plans by making modifications to the change maps in a dynamic and adaptive manner. Compared with conventional RS Change Detection (CD) methods, the proposed UP-Diff for the RS UP task avoids the requirement of paired prechange and post-change images, which enhances the practical usage in city development. Experimental results on LEVIRCD and SYSU-CD datasets show UP-Diff's ability to accurately predict future urban layouts with high fidelity, demonstrating its potential for urban planning. Code and model weights are available at https://github.com/zeyuwang-zju/UP-Diff.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11578v2</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zeyu Wang, Zecheng Hao, Jingyu Lin, Yuchao Feng, Yufei Guo</dc:creator>
    </item>
    <item>
      <title>Observation of Aerosolization-induced Morphological Changes in Viral Capsids</title>
      <link>https://arxiv.org/abs/2407.11687</link>
      <description>arXiv:2407.11687v1 Announce Type: cross 
Abstract: Single-stranded RNA viruses co-assemble their capsid with the genome and variations in capsid structures can have significant functional relevance. In particular, viruses need to respond to a dehydrating environment to prevent genomic degradation and remain active upon rehydration. Theoretical work has predicted low-energy buckling transitions in icosahedral capsids which could protect the virus from further dehydration. However, there has been no direct experimental evidence, nor molecular mechanism, for such behaviour. Here we observe this transition using X-ray single particle imaging of MS2 bacteriophages after aerosolization. Using a combination of machine learning tools, we classify hundreds of thousands of single particle diffraction patterns to learn the structural landscape of the capsid morphology as a function of time spent in the aerosol phase. We found a previously unreported compact conformation as well as intermediate structures which suggest an incoherent buckling transition which does not preserve icosahedral symmetry. Finally, we propose a mechanism of this buckling, where a single 19-residue loop is destabilised, leading to the large observed morphology change. Our results provide experimental evidence for a mechanism by which viral capsids protect themselves from dehydration. In the process, these findings also demonstrate the power of single particle X-ray imaging and machine learning methods in studying biomolecular structural dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11687v1</guid>
      <category>q-bio.BM</category>
      <category>eess.IV</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abhishek Mall, Anna Munke, Zhou Shen, Parichita Mazumder, Johan Bielecki, Juncheng E, Armando Estillore, Chan Kim, Romain Letrun, Jannik L\"ubke, Safi Rafie-Zinedine, Adam Round, Ekaterina Round, Michael R\"utten, Amit K. Samanta, Abhisakh Sarma, Tokushi Sato, Florian Schulz, Carolin Seuring, Tamme Wollweber, Lena Worbs, Patrik Vagovic, Richard Bean, Adrian P. Mancuso, Ne-Te Duane Loh, Tobias Beck, Jochen K\"upper, Filipe R. N. C. Maia, Henry N. Chapman, Kartik Ayyer</dc:creator>
    </item>
    <item>
      <title>Rate-Distortion-Cognition Controllable Versatile Neural Image Compression</title>
      <link>https://arxiv.org/abs/2407.11700</link>
      <description>arXiv:2407.11700v2 Announce Type: cross 
Abstract: Recently, the field of Image Coding for Machines (ICM) has garnered heightened interest and significant advances thanks to the rapid progress of learning-based techniques for image compression and analysis. Previous studies often require training separate codecs to support various bitrate levels, machine tasks, and networks, thus lacking both flexibility and practicality. To address these challenges, we propose a rate-distortion-cognition controllable versatile image compression, which method allows the users to adjust the bitrate (i.e., Rate), image reconstruction quality (i.e., Distortion), and machine task accuracy (i.e., Cognition) with a single neural model, achieving ultra-controllability. Specifically, we first introduce a cognition-oriented loss in the primary compression branch to train a codec for diverse machine tasks. This branch attains variable bitrate by regulating quantization degree through the latent code channels. To further enhance the quality of the reconstructed images, we employ an auxiliary branch to supplement residual information with a scalable bitstream. Ultimately, two branches use a `$\beta x + (1 - \beta) y$' interpolation strategy to achieve a balanced cognition-distortion trade-off. Extensive experiments demonstrate that our method yields satisfactory ICM performance and flexible Rate-Distortion-Cognition controlling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11700v2</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinming Liu, Ruoyu Feng, Yunpeng Qi, Qiuyu Chen, Zhibo Chen, Wenjun Zeng, Xin Jin</dc:creator>
    </item>
    <item>
      <title>Evaluation of Extra Pixel Interpolation with Mask Processing for Medical Image Segmentation with Deep Learning</title>
      <link>https://arxiv.org/abs/2302.11522</link>
      <description>arXiv:2302.11522v4 Announce Type: replace 
Abstract: Current mask processing operations rely on interpolation algorithms that do not produce extra pixels, such as nearest neighbor (NN) interpolation, as opposed to algorithms that do produce extra pixels, like bicubic (BIC) or bilinear (BIL) interpolation. In our previous study, the author proposed an alternative approach to NN-based mask processing and evaluated its effects on deep learning training outcomes. In this study, the author evaluated the effects of both BIC-based image and mask processing and BIC-and-NN-based image and mask processing versus NN-based image and mask processing. The evaluation revealed that the BIC-BIC model/network was an 8.9578 % (with image size 256 x 256) and a 1.0496 % (with image size 384 x 384) increase of the NN-NN network compared to the NN-BIC network which was an 8.3127 % (with image size 256 x 256) and a 0.2887 % (with image size 384 x 384) increase of the NN-NN network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.11522v4</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s11760-024-03421-9</arxiv:DOI>
      <arxiv:journal_reference>SIViP (2024)</arxiv:journal_reference>
      <dc:creator>Olivier Rukundo</dc:creator>
    </item>
    <item>
      <title>ParamNet: A Dynamic Parameter Network for Fast Multi-to-One Stain Normalization</title>
      <link>https://arxiv.org/abs/2305.06511</link>
      <description>arXiv:2305.06511v3 Announce Type: replace 
Abstract: In practice, digital pathology images are often affected by various factors, resulting in very large differences in color and brightness. Stain normalization can effectively reduce the differences in color and brightness of digital pathology images, thus improving the performance of computer-aided diagnostic systems. Conventional stain normalization methods rely on one or several reference images, but one or several images may not adequately represent the entire dataset. Although learning-based stain normalization methods are a general approach, they use complex deep networks, which not only greatly reduce computational efficiency, but also risk introducing artifacts. Some studies use specialized network structures to enhance computational efficiency and reliability, but these methods are difficult to apply to multi-to-one stain normalization due to insufficient network capacity. In this study, we introduced dynamic-parameter network and proposed a novel method for stain normalization, called ParamNet. ParamNet addresses the challenges of limited network capacity and computational efficiency by introducing dynamic parameters (weights and biases of convolutional layers) into the network design. By effectively leveraging these parameters, ParamNet achieves superior performance in stain normalization while maintaining computational efficiency. Results show ParamNet can normalize one whole slide image (WSI) of 100,000x100,000 within 25s. The code is available at: https://github.com/khtao/ParamNet.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.06511v3</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongtao Kang, Die Luo, Li Chen, Junbo Hu, Tingwei Quan, Shaoqun Zeng, Shenghua Cheng, Xiuli Liu</dc:creator>
    </item>
    <item>
      <title>Temporal-spatial Adaptation of Promptable SAM Enhance Accuracy and Generalizability of cine CMR Segmentation</title>
      <link>https://arxiv.org/abs/2403.10009</link>
      <description>arXiv:2403.10009v2 Announce Type: replace 
Abstract: Accurate myocardium segmentation across all phases in one cardiac cycle in cine cardiac magnetic resonance (CMR) scans is crucial for comprehensively cardiac function analysis. Despite advancements in deep learning (DL) for automatic cine CMR segmentation, generalizability on unseen data remains a significant challenge. Recently, the segment-anything-model (SAM) has been invented as a segmentation foundation model, known for its accurate segmentation and more importantly, zero-shot generalization. SAM was trained on two-dimensional (2D) natural images; to adapt it for comprehensive cine CMR segmentation, we propose cineCMR-SAM which incorporates both temporal and spatial information through a modified model architecture. Compared to other state-of-the-art (SOTA) methods, our model achieved superior data-specific model segmentation accuracy on the STACOM2011 when fine-tuned on this dataset and demonstrated superior zero-shot generalization on two other large public datasets (ACDC and M&amp;Ms) unseen during fine-tuning. Additionally, we introduced a text prompt feature in cineCMR-SAM to specify the view type of input slices (short-axis or long-axis), enhancing performance across all view types.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10009v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhennong Chen, Sekeun Kim, Hui Ren, Quanzheng Li, Xiang Li</dc:creator>
    </item>
    <item>
      <title>A Foundation Model for Brain Lesion Segmentation with Mixture of Modality Experts</title>
      <link>https://arxiv.org/abs/2405.10246</link>
      <description>arXiv:2405.10246v2 Announce Type: replace 
Abstract: Brain lesion segmentation plays an essential role in neurological research and diagnosis. As brain lesions can be caused by various pathological alterations, different types of brain lesions tend to manifest with different characteristics on different imaging modalities. Due to this complexity, brain lesion segmentation methods are often developed in a task-specific manner. A specific segmentation model is developed for a particular lesion type and imaging modality. However, the use of task-specific models requires predetermination of the lesion type and imaging modality, which complicates their deployment in real-world scenarios. In this work, we propose a universal foundation model for 3D brain lesion segmentation, which can automatically segment different types of brain lesions for input data of various imaging modalities. We formulate a novel Mixture of Modality Experts (MoME) framework with multiple expert networks attending to different imaging modalities. A hierarchical gating network combines the expert predictions and fosters expertise collaboration. Furthermore, we introduce a curriculum learning strategy during training to avoid the degeneration of each expert network and preserve their specialization. We evaluated the proposed method on nine brain lesion datasets, encompassing five imaging modalities and eight lesion types. The results show that our model outperforms state-of-the-art universal models and provides promising generalization to unseen datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10246v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinru Zhang, Ni Ou, Berke Doga Basaran, Marco Visentin, Mengyun Qiao, Renyang Gu, Cheng Ouyang, Yaou Liu, Paul M. Matthew, Chuyang Ye, Wenjia Bai</dc:creator>
    </item>
    <item>
      <title>DUCPS: Deep Unfolding the Cauchy Proximal Splitting Algorithm for B-Lines Quantification in Lung Ultrasound Images</title>
      <link>https://arxiv.org/abs/2407.10667</link>
      <description>arXiv:2407.10667v2 Announce Type: replace 
Abstract: The identification of artefacts, particularly B-lines, in lung ultrasound (LUS), is crucial for assisting clinical diagnosis, prompting the development of innovative methodologies. While the Cauchy proximal splitting (CPS) algorithm has demonstrated effective performance in B-line detection, the process is slow and has limited generalization. This paper addresses these issues with a novel unsupervised deep unfolding network structure (DUCPS). The framework utilizes deep unfolding procedures to merge traditional model-based techniques with deep learning approaches. By unfolding the CPS algorithm into a deep network, DUCPS enables the parameters in the optimization algorithm to be learnable, thus enhancing generalization performance and facilitating rapid convergence. We conducted entirely unsupervised training using the Neighbor2Neighbor (N2N) and the Structural Similarity Index Measure (SSIM) losses. When combined with an improved line identification method proposed in this paper, state-of-the-art performance is achieved, with the recall and F2 score reaching 0.70 and 0.64, respectively. Notably, DUCPS significantly improves computational efficiency eliminating the need for extensive data labeling, representing a notable advancement over both traditional algorithms and existing deep learning approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10667v2</guid>
      <category>eess.IV</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tianqi Yang, Oktay Karaku\c{s}, Nantheera Anantrasirichai, Marco Allinovi, Alin Achim</dc:creator>
    </item>
    <item>
      <title>Detection of Malaria Vector Breeding Habitats using Topographic Models</title>
      <link>https://arxiv.org/abs/2011.13714</link>
      <description>arXiv:2011.13714v2 Announce Type: replace-cross 
Abstract: Treatment of stagnant water bodies that act as a breeding site for malarial vectors is a fundamental step in most malaria elimination campaigns. However, identification of such water bodies over large areas is expensive, labour-intensive and time-consuming and hence, challenging in countries with limited resources. Practical models that can efficiently locate water bodies can target the limited resources by greatly reducing the area that needs to be scanned by the field workers. To this end, we propose a practical topographic model based on easily available, global, high-resolution DEM data to predict locations of potential vector-breeding water sites. We surveyed the Obuasi region of Ghana to assess the impact of various topographic features on different types of water bodies and uncover the features that significantly influence the formation of aquatic habitats. We further evaluate the effectiveness of multiple models. Our best model significantly outperforms earlier attempts that employ topographic variables for detection of small water sites, even the ones that utilize additional satellite imagery data and demonstrates robustness across different settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2011.13714v2</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aishwarya Jadhav</dc:creator>
    </item>
    <item>
      <title>Bora: Biomedical Generalist Video Generation Model</title>
      <link>https://arxiv.org/abs/2407.08944</link>
      <description>arXiv:2407.08944v2 Announce Type: replace-cross 
Abstract: Generative models hold promise for revolutionizing medical education, robot-assisted surgery, and data augmentation for medical AI development. Diffusion models can now generate realistic images from text prompts, while recent advancements have demonstrated their ability to create diverse, high-quality videos. However, these models often struggle with generating accurate representations of medical procedures and detailed anatomical structures. This paper introduces Bora, the first spatio-temporal diffusion probabilistic model designed for text-guided biomedical video generation. Bora leverages Transformer architecture and is pre-trained on general-purpose video generation tasks. It is fine-tuned through model alignment and instruction tuning using a newly established medical video corpus, which includes paired text-video data from various biomedical fields. To the best of our knowledge, this is the first attempt to establish such a comprehensive annotated biomedical video dataset. Bora is capable of generating high-quality video data across four distinct biomedical domains, adhering to medical expert standards and demonstrating consistency and diversity. This generalist video generative model holds significant potential for enhancing medical consultation and decision-making, particularly in resource-limited settings. Additionally, Bora could pave the way for immersive medical training and procedure planning. Extensive experiments on distinct medical modalities such as endoscopy, ultrasound, MRI, and cell tracking validate the effectiveness of our model in understanding biomedical instructions and its superior performance across subjects compared to state-of-the-art generation models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08944v2</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weixiang Sun, Xiaocao You, Ruizhe Zheng, Zhengqing Yuan, Xiang Li, Lifang He, Quanzheng Li, Lichao Sun</dc:creator>
    </item>
  </channel>
</rss>
