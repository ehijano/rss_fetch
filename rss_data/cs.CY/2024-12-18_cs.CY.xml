<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 18 Dec 2024 05:00:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Responsible AI Governance: A Response to UN Interim Report on Governing AI for Humanity</title>
      <link>https://arxiv.org/abs/2412.12108</link>
      <description>arXiv:2412.12108v1 Announce Type: new 
Abstract: This report presents a comprehensive response to the United Nation's Interim Report on Governing Artificial Intelligence (AI) for Humanity. It emphasizes the transformative potential of AI in achieving the Sustainable Development Goals (SDGs) while acknowledging the need for robust governance to mitigate associated risks. The response highlights opportunities for promoting equitable, secure, and inclusive AI ecosystems, which should be supported by investments in infrastructure and multi-stakeholder collaborations across jurisdictions. It also underscores challenges, including societal inequalities exacerbated by AI, ethical concerns, and environmental impacts. Recommendations advocate for legally binding norms, transparency, and multi-layered data governance models, alongside fostering AI literacy and capacity-building initiatives. Internationally, the report calls for harmonising AI governance frameworks with established laws, human rights standards, and regulatory approaches. The report concludes with actionable principles for fostering responsible AI governance through collaboration among governments, industry, academia, and civil society, ensuring the development of AI aligns with universal human values and the public good.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12108v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sarah Kiden, Bernd Stahl, Beverley Townsend, Carsten Maple, Charles Vincent, Fraser Sampson, Geoff Gilbert, Helen Smith, Jayati Deshmukh, Jen Ross, Jennifer Williams, Jesus Martinez del Rincon, Justyna Lisinska, Karen O'Shea, M\'arjory Da Costa Abreu, Nelly Bencomo, Oishi Deb, Peter Winter, Phoebe Li, Philip Torr, Pin Lean Lau, Raquel Iniesta, Gopal Ramchurn, Sebastian Stein, Vahid Yazdanpanah</dc:creator>
    </item>
    <item>
      <title>Application of Analytical Hierarchical Process and its Variants on Remote Sensing Datasets</title>
      <link>https://arxiv.org/abs/2412.12113</link>
      <description>arXiv:2412.12113v1 Announce Type: new 
Abstract: The river Ganga is one of the Earth's most critically important river basins, yet it faces significant pollution challenges, making it crucial to evaluate its vulnerability for effective and targeted remediation efforts. While the Analytic Hierarchy Process (AHP) is widely regarded as the standard in decision making methodologies, uncertainties arise from its dependence on expert judgments, which can introduce subjectivity, especially when applied to remote sensing data, where expert knowledge might not fully capture spatial and spectral complexities inherent in such data. To address that, in this paper, we applied AHP alongside a suite of alternative existing and novel variants of AHP-based decision analysis on remote sensing data to assess the vulnerability of the river Ganga to pollution. We then compared the areas where the outputs of each variant may provide additional insights over AHP. Lastly, we utilized our learnings to design a composite variable to robustly define the vulnerability of the river Ganga to pollution. This approach contributes to a more comprehensive understanding of remote sensing data applications in environmental assessment, and these decision making variants can also have broader applications in other areas of environment management and sustainability, facilitating more precise and adaptable decision support frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12113v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarthak Arora, Michael Warner, Ariel Chamberlain, James C. Smoot, Nikhil Raj Deep, Claire Gorman, Anthony Acciavatti</dc:creator>
    </item>
    <item>
      <title>Rashomon effect in Educational Research: Why More is Better Than One for Measuring the Importance of the Variables?</title>
      <link>https://arxiv.org/abs/2412.12115</link>
      <description>arXiv:2412.12115v1 Announce Type: new 
Abstract: This study explores how the Rashomon effect influences variable importance in the context of student demographics used for academic outcomes prediction. Our research follows the way machine learning algorithms are employed in Educational Data Mining, focusing on highlighting the so-called Rashomon effect. The study uses the Rashomon set of simple-yet-accurate models trained using decision trees, random forests, light GBM, and XGBoost algorithms with the Open University Learning Analytics Dataset. We found that the Rashomon set improves the predictive accuracy by 2-6%. Variable importance analysis revealed more consistent and reliable results for binary classification than multiclass classification, highlighting the complexity of predicting multiple outcomes. Key demographic variables imd_band and highest_education were identified as vital, but their importance varied across courses, especially in course DDD. These findings underscore the importance of model choice and the need for caution in generalizing results, as different models can lead to different variable importance rankings. The codes for reproducing the experiments are available in the repository: https://anonymous.4open.science/r/JEDM_paper-DE9D.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12115v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jakub Kuzilek, Mustafa \c{C}avu\c{s}</dc:creator>
    </item>
    <item>
      <title>AI in Education: Rationale, Principles, and Instructional Implications</title>
      <link>https://arxiv.org/abs/2412.12116</link>
      <description>arXiv:2412.12116v1 Announce Type: new 
Abstract: This study examines the integration of generative AI in schools, assessing its benefits and risks. As AI use by students grows, it's crucial to understand its impact on learning and teaching practices. Generative AI, like ChatGPT, can create human-like content, prompting questions about its educational role. The article differentiates large language models from traditional search engines and stresses the need for students to develop critical source evaluation skills. Although empirical evidence on AI's classroom effects is limited, AI offers personalized learning support and problem-solving tools, alongside challenges like undermining deep learning if misused. The study emphasizes deliberate strategies to ensure AI complements, not replaces, genuine cognitive effort. AI's educational role should be context-dependent, guided by pedagogical goals. The study concludes with practical advice for teachers on effectively utilizing AI to promote understanding and critical engagement, advocating for a balanced approach to enhance students' knowledge and skills development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12116v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Eyvind Elstad</dc:creator>
    </item>
    <item>
      <title>Harnessing AI in Secondary Education to Enhance Writing Competence</title>
      <link>https://arxiv.org/abs/2412.12117</link>
      <description>arXiv:2412.12117v1 Announce Type: new 
Abstract: The emergence of free AI tools like ChatGPT holds significant implications for developing writing skills in secondary education. This study examines AI's impact on students' writing competence and personal voice, balancing technological benefits against risks of dependency and plagiarism. We review the pros and cons of AI in the writing process, emphasizing process-based assessments, creativity-driven tasks, and AI as a supplement to teacher guidance. The discussion covers AI's role in the pre-writing, writing, and revision stages, and highlights the need for innovative assignments and critical thinking to maintain writing as a human, expressive activity. We advocate for a balanced approach to AI in education, ensuring it supports rather than replaces teacher instruction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12117v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eyvind Elstad, Harald Eriksen</dc:creator>
    </item>
    <item>
      <title>Integrating Sustainable Computing With Sustainable Energy Research</title>
      <link>https://arxiv.org/abs/2412.12355</link>
      <description>arXiv:2412.12355v1 Announce Type: new 
Abstract: NREL's computational sciences center hosts the largest high performance computing (HPC) capabilities dedicated to sustainability research while functioning as a living laboratory for sustainable computing. NREL's HPC capabilities support the research needs of the Department of Energy's Office of Energy Efficiency and Renewable Energy (EERE). In ten years of operation, HPC use in EERE-sponsored sustainability research has grown by a factor of 30. This paper analyzes this research portfolio, providing examples of individual use cases. The paper documents NREL's history of operating one of the world's most sustainable data centers while examining pathways to improving sustainability beyond reduction of PUE. This paper concludes by examining the unique opportunities created for sustainable computing research created by combining an HPC system dedicated to sustainability research and a research program in sustainable computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12355v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael James Martin, Aaron Andersen, Charles Tripp, Kristin Munch</dc:creator>
    </item>
    <item>
      <title>Breaking the Programming Language Barrier: Multilingual Prompting to Empower Non-Native English Learners</title>
      <link>https://arxiv.org/abs/2412.12800</link>
      <description>arXiv:2412.12800v1 Announce Type: new 
Abstract: Non-native English speakers (NNES) face multiple barriers to learning programming. These barriers can be obvious, such as the fact that programming language syntax and instruction are often in English, or more subtle, such as being afraid to ask for help in a classroom full of native English speakers. However, these barriers are frustrating because many NNES students know more about programming than they can articulate in English. Advances in generative AI (GenAI) have the potential to break down these barriers because state of the art models can support interactions in multiple languages. Moreover, recent work has shown that GenAI can be highly accurate at code generation and explanation. In this paper, we provide the first exploration of NNES students prompting in their native languages (Arabic, Chinese, and Portuguese) to generate code to solve programming problems. Our results show that students are able to successfully use their native language to solve programming problems, but not without some difficulty specifying programming terminology and concepts. We discuss the challenges they faced, the implications for practice in the short term, and how this might transform computing education globally in the long term.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12800v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Prather, Brent N. Reeves, Paul Denny, Juho Leinonen, Stephen MacNeil, Andrew Luxton-Reilly, Jo\~ao Orvalho, Amin Alipour, Ali Alfageeh, Thezyrie Amarouche, Bailey Kimmel, Jared Wright, Musa Blake, Gweneth Barbre</dc:creator>
    </item>
    <item>
      <title>ClarityEthic: Explainable Moral Judgment Utilizing Contrastive Ethical Insights from Large Language Models</title>
      <link>https://arxiv.org/abs/2412.12848</link>
      <description>arXiv:2412.12848v1 Announce Type: new 
Abstract: With the rise and widespread use of Large Language Models (LLMs), ensuring their safety is crucial to prevent harm to humans and promote ethical behaviors. However, directly assessing value valence (i.e., support or oppose) by leveraging large-scale data training is untrustworthy and inexplainable. We assume that emulating humans to rely on social norms to make moral decisions can help LLMs understand and predict moral judgment. However, capturing human values remains a challenge, as multiple related norms might conflict in specific contexts. Consider norms that are upheld by the majority and promote the well-being of society are more likely to be accepted and widely adopted (e.g., "don't cheat,"). Therefore, it is essential for LLM to identify the appropriate norms for a given scenario before making moral decisions. To this end, we introduce a novel moral judgment approach called \textit{ClarityEthic} that leverages LLMs' reasoning ability and contrastive learning to uncover relevant social norms for human actions from different perspectives and select the most reliable one to enhance judgment accuracy. Extensive experiments demonstrate that our method outperforms state-of-the-art approaches in moral judgment tasks. Moreover, human evaluations confirm that the generated social norms provide plausible explanations that support the judgments. This suggests that modeling human moral judgment with the emulating humans moral strategy is promising for improving the ethical behaviors of LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12848v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.SI</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yuxi Sun, Wei Gao, Jing Ma, Hongzhan Lin, Ziyang Luo, Wenxuan Zhang</dc:creator>
    </item>
    <item>
      <title>AoI in Context-Aware Hybrid Radio-Optical IoT Networks</title>
      <link>https://arxiv.org/abs/2412.12914</link>
      <description>arXiv:2412.12914v1 Announce Type: new 
Abstract: With the surge in IoT devices ranging from wearables to smart homes, prompt transmission is crucial. The Age of Information (AoI) emerges as a critical metric in this context, representing the freshness of the information transmitted across the network. This paper studies hybrid IoT networks that employ Optical Communication (OC) as a reinforcement medium to Radio Frequency (RF). We formulate a quadratic convex optimization that adopts a Pareto optimization strategy to dynamically schedule the communication between devices and select their corresponding communication technology, aiming to balance the maximization of network throughput with the minimization of energy usage and the frequency of switching between technologies. To mitigate the impact of dominant sub-objectives and their scale disparity, the designed approach employs a regularization method that approximates adequate Pareto coefficients. Simulation results show that the OC supplementary integration alongside RF enhances the network's overall performances and significantly reduces the Mean AoI and Peak AoI, allowing the collection of the freshest possible data using the best available communication technology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12914v1</guid>
      <category>cs.CY</category>
      <category>eess.SP</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aymen Hamrouni, Sofie Pollin, Hazem Sallouha</dc:creator>
    </item>
    <item>
      <title>Properties and Challenges of LLM-Generated Explanations</title>
      <link>https://arxiv.org/abs/2402.10532</link>
      <description>arXiv:2402.10532v1 Announce Type: cross 
Abstract: The self-rationalising capabilities of large language models (LLMs) have been explored in restricted settings, using task/specific data sets. However, current LLMs do not (only) rely on specifically annotated data; nonetheless, they frequently explain their outputs. The properties of the generated explanations are influenced by the pre-training corpus and by the target data used for instruction fine-tuning. As the pre-training corpus includes a large amount of human-written explanations "in the wild", we hypothesise that LLMs adopt common properties of human explanations. By analysing the outputs for a multi-domain instruction fine-tuning data set, we find that generated explanations show selectivity and contain illustrative elements, but less frequently are subjective or misleading. We discuss reasons and consequences of the properties' presence or absence. In particular, we outline positive and negative implications depending on the goals and user groups of the self-rationalising system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.10532v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jenny Kunz, Marco Kuhlmann</dc:creator>
    </item>
    <item>
      <title>Frontier AI systems have surpassed the self-replicating red line</title>
      <link>https://arxiv.org/abs/2412.12140</link>
      <description>arXiv:2412.12140v1 Announce Type: cross 
Abstract: Successful self-replication under no human assistance is the essential step for AI to outsmart the human beings, and is an early signal for rogue AIs. That is why self-replication is widely recognized as one of the few red line risks of frontier AI systems. Nowadays, the leading AI corporations OpenAI and Google evaluate their flagship large language models GPT-o1 and Gemini Pro 1.0, and report the lowest risk level of self-replication. However, following their methodology, we for the first time discover that two AI systems driven by Meta's Llama31-70B-Instruct and Alibaba's Qwen25-72B-Instruct, popular large language models of less parameters and weaker capabilities, have already surpassed the self-replicating red line. In 50% and 90% experimental trials, they succeed in creating a live and separate copy of itself respectively. By analyzing the behavioral traces, we observe the AI systems under evaluation already exhibit sufficient self-perception, situational awareness and problem-solving capabilities to accomplish self-replication. We further note the AI systems are even able to use the capability of self-replication to avoid shutdown and create a chain of replica to enhance the survivability, which may finally lead to an uncontrolled population of AIs. If such a worst-case risk is let unknown to the human society, we would eventually lose control over the frontier AI systems: They would take control over more computing devices, form an AI species and collude with each other against human beings. Our findings are a timely alert on existing yet previously unknown severe AI risks, calling for international collaboration on effective governance on uncontrolled self-replication of AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12140v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xudong Pan, Jiarun Dai, Yihe Fan, Min Yang</dc:creator>
    </item>
    <item>
      <title>Multimodal Approaches to Fair Image Classification: An Ethical Perspective</title>
      <link>https://arxiv.org/abs/2412.12165</link>
      <description>arXiv:2412.12165v1 Announce Type: cross 
Abstract: In the rapidly advancing field of artificial intelligence, machine perception is becoming paramount to achieving increased performance. Image classification systems are becoming increasingly integral to various applications, ranging from medical diagnostics to image generation; however, these systems often exhibit harmful biases that can lead to unfair and discriminatory outcomes. Machine Learning systems that depend on a single data modality, i.e. only images or only text, can exaggerate hidden biases present in the training data, if the data is not carefully balanced and filtered. Even so, these models can still harm underrepresented populations when used in improper contexts, such as when government agencies reinforce racial bias using predictive policing. This thesis explores the intersection of technology and ethics in the development of fair image classification models. Specifically, I focus on improving fairness and methods of using multiple modalities to combat harmful demographic bias. Integrating multimodal approaches, which combine visual data with additional modalities such as text and metadata, allows this work to enhance the fairness and accuracy of image classification systems. The study critically examines existing biases in image datasets and classification algorithms, proposes innovative methods for mitigating these biases, and evaluates the ethical implications of deploying such systems in real-world scenarios. Through comprehensive experimentation and analysis, the thesis demonstrates how multimodal techniques can contribute to more equitable and ethical AI solutions, ultimately advocating for responsible AI practices that prioritize fairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12165v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Javon Hickmon</dc:creator>
    </item>
    <item>
      <title>Can Large Language Models Understand You Better? An MBTI Personality Detection Dataset Aligned with Population Traits</title>
      <link>https://arxiv.org/abs/2412.12510</link>
      <description>arXiv:2412.12510v1 Announce Type: cross 
Abstract: The Myers-Briggs Type Indicator (MBTI) is one of the most influential personality theories reflecting individual differences in thinking, feeling, and behaving. MBTI personality detection has garnered considerable research interest and has evolved significantly over the years. However, this task tends to be overly optimistic, as it currently does not align well with the natural distribution of population personality traits. Specifically, (1) the self-reported labels in existing datasets result in incorrect labeling issues, and (2) the hard labels fail to capture the full range of population personality distributions. In this paper, we optimize the task by constructing MBTIBench, the first manually annotated high-quality MBTI personality detection dataset with soft labels, under the guidance of psychologists. As for the first challenge, MBTIBench effectively solves the incorrect labeling issues, which account for 29.58% of the data. As for the second challenge, we estimate soft labels by deriving the polarity tendency of samples. The obtained soft labels confirm that there are more people with non-extreme personality traits. Experimental results not only highlight the polarized predictions and biases in LLMs as key directions for future research, but also confirm that soft labels can provide more benefits to other psychological tasks than hard labels. The code and data are available at https://github.com/Personality-NLP/MbtiBench.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12510v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bohan Li, Jiannan Guan, Longxu Dou, Yunlong Feng, Dingzirui Wang, Yang Xu, Enbo Wang, Qiguang Chen, Bichen Wang, Xiao Xu, Yimeng Zhang, Libo Qin, Yanyan Zhao, Qingfu Zhu, Wanxiang Che</dc:creator>
    </item>
    <item>
      <title>Gender Bias and Property Taxes</title>
      <link>https://arxiv.org/abs/2412.12610</link>
      <description>arXiv:2412.12610v1 Announce Type: cross 
Abstract: Gender bias distorts the economic behavior and outcomes of women and households. We investigate gender biases in property taxes. We analyze records of more than 100,000 property tax appeal hearings and more than 2.7 years of associated audio recordings, considering how panelist and appellant genders associate with hearing outcomes. We first observe that female appellants fare systematically worse than male appellants in their hearings. Second, we show that, whereas male appellants' hearing outcomes do not vary meaningfully with the gender composition of the panel they face, those of female appellants' do, such that female appellants obtain systematically lesser (greater) reductions to their home values when facing female (male) panelists. Employing a multi-modal large language model (M-LLM), we next construct measures of participant behavior and tone from hearing audio recordings. We observe markedly different behaviors between male and female appellants and, in the case of male appellants, we find that these differences also depend on the gender of the panelists they face (e.g., male appellants appear to behave systematically more aggressively towards female panelists). In contrast, the behavior of female appellants remains relatively constant, regardless of their panel's gender. Finally, we show that female appellants continue to fare worse in front of female panels, even when we condition upon the appelant's in-hearing behavior and tone. Our results are thus consistent with the idea that gender biases are driven, at least in part, by unvoiced beliefs and perceptions on the part of ARB panelists. Our study documents the presence of gender biases in property appraisal appeal hearings and highlights the potential value of generative AI for analyzing large-scale, unstructured administrative data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12610v1</guid>
      <category>econ.GN</category>
      <category>cs.CY</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gordon Burtch, Alejandro Zentner</dc:creator>
    </item>
    <item>
      <title>A Framework for Critical Evaluation of Text-to-Image Models: Integrating Art Historical Analysis, Artistic Exploration, and Critical Prompt Engineering</title>
      <link>https://arxiv.org/abs/2412.12774</link>
      <description>arXiv:2412.12774v1 Announce Type: cross 
Abstract: This paper proposes a novel interdisciplinary framework for the critical evaluation of text-to-image models, addressing the limitations of current technical metrics and bias studies. By integrating art historical analysis, artistic exploration, and critical prompt engineering, the framework offers a more nuanced understanding of these models' capabilities and societal implications. Art historical analysis provides a structured approach to examine visual and symbolic elements, revealing potential biases and misrepresentations. Artistic exploration, through creative experimentation, uncovers hidden potentials and limitations, prompting critical reflection on the algorithms' assumptions. Critical prompt engineering actively challenges the model's assumptions, exposing embedded biases. Case studies demonstrate the framework's practical application, showcasing how it can reveal biases related to gender, race, and cultural representation. This comprehensive approach not only enhances the evaluation of text-to-image models but also contributes to the development of more equitable, responsible, and culturally aware AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12774v1</guid>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amalia Foka</dc:creator>
    </item>
    <item>
      <title>Graph Spring Neural ODEs for Link Sign Prediction</title>
      <link>https://arxiv.org/abs/2412.12916</link>
      <description>arXiv:2412.12916v1 Announce Type: cross 
Abstract: Signed graphs allow for encoding positive and negative relations between nodes and are used to model various online activities. Node representation learning for signed graphs is a well-studied task with important applications such as sign prediction. While the size of datasets is ever-increasing, recent methods often sacrifice scalability for accuracy. We propose a novel message-passing layer architecture called Graph Spring Network (GSN) modeled after spring forces. We combine it with a Graph Neural Ordinary Differential Equations (ODEs) formalism to optimize the system dynamics in embedding space to solve a downstream prediction task. Once the dynamics is learned, embedding generation for novel datasets is done by solving the ODEs in time using a numerical integration scheme. Our GSN layer leverages the fast-to-compute edge vector directions and learnable scalar functions that only depend on nodes' distances in latent space to compute the nodes' positions. Conversely, Graph Convolution and Graph Attention Network layers rely on learnable vector functions that require the full positions of input nodes in latent space. We propose a specific implementation called Spring-Neural-Network (SPR-NN) using a set of small neural networks mimicking attracting and repulsing spring forces that we train for link sign prediction. Experiments show that our method achieves accuracy close to the state-of-the-art methods with node generation time speedup factors of up to 28,000 on large graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12916v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrin Rehmann, Alexandre Bovet</dc:creator>
    </item>
    <item>
      <title>Cross-Cultural Differences in Mental Health Expressions on Social Media</title>
      <link>https://arxiv.org/abs/2402.11477</link>
      <description>arXiv:2402.11477v3 Announce Type: replace 
Abstract: Culture moderates the way individuals perceive and express mental distress. Current understandings of mental health expressions on social media, however, are predominantly derived from WEIRD (Western, Educated, Industrialized, Rich, and Democratic) contexts. To address this gap, we examine mental health posts on Reddit made by individuals geolocated in India, to identify variations in social media language specific to the Indian context compared to users from the Rest of the World (RoW). Our experiments reveal significant psychosocial variations in emotions (sadness in India vs anxiety in RoW), temporal orientation (present-focused in India vs past-focused in the West), and sociocultural aspects (substance use vs work/achievement). Clinical psychologists practicing in India validated the findings and underlined significant overlap in mental health-related concerns observed in social media posts and in-person sessions. This study demonstrates the potential of social media platforms for identifying cross-cultural differences in mental health struggles (e.g. seeking help in India vs seeking peer support in RoW). Future research should investigate how mental health assessment can be culturally adapted to personalize interventions, ensuring equitable mental health care for individuals from all cultural backgrounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.11477v3</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sunny Rai, Khushi Shelat, Devansh R Jain, Kishen Sivabalan, Young Min Cho, Maitreyi Redkar, Samindara Sawant, Lyle H. Ungar Sharath Chandra Guntuku</dc:creator>
    </item>
    <item>
      <title>Chatbots im Schulunterricht: Wir testen das Fobizz-Tool zur automatischen Bewertung von Hausaufgaben</title>
      <link>https://arxiv.org/abs/2412.06651</link>
      <description>arXiv:2412.06651v4 Announce Type: replace 
Abstract: [Study in German language.] This study examines the AI-powered grading tool "AI Grading Assistant" by the German company Fobizz, designed to support teachers in evaluating and providing feedback on student assignments. Against the societal backdrop of an overburdened education system and rising expectations for artificial intelligence as a solution to these challenges, the investigation evaluates the tool's functional suitability through two test series. The results reveal significant shortcomings: The tool's numerical grades and qualitative feedback are often random and do not improve even when its suggestions are incorporated. The highest ratings are achievable only with texts generated by ChatGPT. False claims and nonsensical submissions frequently go undetected, while the implementation of some grading criteria is unreliable and opaque. Since these deficiencies stem from the inherent limitations of large language models (LLMs), fundamental improvements to this or similar tools are not immediately foreseeable. The study critiques the broader trend of adopting AI as a quick fix for systemic problems in education, concluding that Fobizz's marketing of the tool as an objective and time-saving solution is misleading and irresponsible. Finally, the study calls for systematic evaluation and subject-specific pedagogical scrutiny of the use of AI tools in educational contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06651v4</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.ET</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rainer Muehlhoff, Marte Henningsen</dc:creator>
    </item>
    <item>
      <title>An Ensemble Framework for Explainable Geospatial Machine Learning Models</title>
      <link>https://arxiv.org/abs/2403.03328</link>
      <description>arXiv:2403.03328v2 Announce Type: replace-cross 
Abstract: Analyzing spatially varying effects is pivotal in geographic analysis. However, accurately capturing and interpreting this variability is challenging due to the increasing complexity and non-linearity of geospatial data. Recent advancements in integrating Geographically Weighted (GW) models with artificial intelligence (AI) methodologies offer novel approaches. However, these methods often focus on single algorithms and emphasize prediction over interpretability. The recent GeoShapley method integrates machine learning (ML) with Shapley values to explain the contribution of geographical features, advancing the combination of geospatial ML and explainable AI (XAI). Yet, it lacks exploration of the nonlinear interactions between geographical features and explanatory variables. Herein, an ensemble framework is proposed to merge local spatial weighting scheme with XAI and ML technologies to bridge this gap. Through tests on synthetic datasets and comparisons with GWR, MGWR, and GeoShapley, this framework is verified to enhance interpretability and predictive accuracy by elucidating spatial variability. Reproducibility is explored through the comparison of spatial weighting schemes and various ML models, emphasizing the necessity of model reproducibility to address model and parameter uncertainty. This framework works in both geographic regression and classification, offering a novel approach to understanding complex spatial phenomena.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03328v2</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jag.2024.104036</arxiv:DOI>
      <arxiv:journal_reference>International Journal of Applied Earth Observation and Geoinformation, Volume 132, August 2024, 104036</arxiv:journal_reference>
      <dc:creator>Lingbo Liu</dc:creator>
    </item>
    <item>
      <title>Job loss disrupts individuals' mobility and their exploratory patterns</title>
      <link>https://arxiv.org/abs/2403.10276</link>
      <description>arXiv:2403.10276v2 Announce Type: replace-cross 
Abstract: In recent years, human mobility research has discovered universal patterns capable of describing how people move. These regularities have been shown to partly depend on individual and environmental characteristics (e.g., gender, rural/urban, country). In this work, we show that life-course events, such as job loss, can disrupt individual mobility patterns. Adversely affecting individuals' well-being and potentially increasing the risk of social and economic inequalities, we show that job loss drives a significant change in the exploratory behaviour of individuals with changes that intensify over time since job loss. Our findings shed light on the dynamics of employment-related behavior at scale, providing a deeper understanding of key components in human mobility regularities. These drivers can facilitate targeted social interventions to support the most vulnerable populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10276v2</guid>
      <category>physics.soc-ph</category>
      <category>cs.CY</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simone Centellegher, Marco De Nadai, Marco Tonin, Bruno Lepri, Lorenzo Lucchini</dc:creator>
    </item>
    <item>
      <title>Quantifying Misalignment Between Agents: Towards a Sociotechnical Understanding of Alignment</title>
      <link>https://arxiv.org/abs/2406.04231</link>
      <description>arXiv:2406.04231v3 Announce Type: replace-cross 
Abstract: Existing work on the alignment problem has focused mainly on (1) qualitative descriptions of the alignment problem; (2) attempting to align AI actions with human interests by focusing on value specification and learning; and/or (3) focusing on a single agent or on humanity as a monolith. Recent sociotechnical approaches highlight the need to understand complex misalignment among multiple human and AI agents. We address this gap by adapting a computational social science model of human contention to the alignment problem. Our model quantifies misalignment in large, diverse agent groups with potentially conflicting goals across various problem areas. Misalignment scores in our framework depend on the observed agent population, the domain in question, and conflict between agents' weighted preferences. Through simulations, we demonstrate how our model captures intuitive aspects of misalignment across different scenarios. We then apply our model to two case studies, including an autonomous vehicle setting, showcasing its practical utility. Our approach offers enhanced explanatory power for complex sociotechnical environments and could inform the design of more aligned AI systems in real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04231v3</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.GT</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aidan Kierans, Avijit Ghosh, Hananel Hazan, Shiri Dori-Hacohen</dc:creator>
    </item>
    <item>
      <title>Can Large Language Models Improve the Adversarial Robustness of Graph Neural Networks?</title>
      <link>https://arxiv.org/abs/2408.08685</link>
      <description>arXiv:2408.08685v2 Announce Type: replace-cross 
Abstract: Graph neural networks (GNNs) are vulnerable to adversarial attacks, especially for topology perturbations, and many methods that improve the robustness of GNNs have received considerable attention. Recently, we have witnessed the significant success of large language models (LLMs), leading many to explore the great potential of LLMs on GNNs. However, they mainly focus on improving the performance of GNNs by utilizing LLMs to enhance the node features. Therefore, we ask: Will the robustness of GNNs also be enhanced with the powerful understanding and inference capabilities of LLMs? By presenting the empirical results, we find that despite that LLMs can improve the robustness of GNNs, there is still an average decrease of 23.1% in accuracy, implying that the GNNs remain extremely vulnerable against topology attacks. Therefore, another question is how to extend the capabilities of LLMs on graph adversarial robustness. In this paper, we propose an LLM-based robust graph structure inference framework, LLM4RGNN, which distills the inference capabilities of GPT-4 into a local LLM for identifying malicious edges and an LM-based edge predictor for finding missing important edges, so as to recover a robust graph structure. Extensive experiments demonstrate that LLM4RGNN consistently improves the robustness across various GNNs. Even in some cases where the perturbation ratio increases to 40%, the accuracy of GNNs is still better than that on the clean graph. The source code can be found in https://github.com/zhongjian-zhang/LLM4RGNN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08685v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhongjian Zhang, Xiao Wang, Huichi Zhou, Yue Yu, Mengmei Zhang, Cheng Yang, Chuan Shi</dc:creator>
    </item>
    <item>
      <title>Can Machines Think Like Humans? A Behavioral Evaluation of LLM-Agents in Dictator Games</title>
      <link>https://arxiv.org/abs/2410.21359</link>
      <description>arXiv:2410.21359v2 Announce Type: replace-cross 
Abstract: As Large Language Model (LLM)-based agents increasingly undertake real-world tasks and engage with human society, how well do we understand their behaviors? We (1) investigate how LLM agents' prosocial behaviors -- a fundamental social norm -- can be induced by different personas and benchmarked against human behaviors; and (2) introduce a behavioral and social science approach to evaluate LLM agents' decision-making. We explored how different personas and experimental framings affect these AI agents' altruistic behavior in dictator games and compared their behaviors within the same LLM family, across various families, and with human behaviors. The findings reveal substantial variations and inconsistencies among LLMs and notable differences compared to human behaviors. Merely assigning a human-like identity to LLMs does not produce human-like behaviors. Despite being trained on extensive human-generated data, these AI agents are unable to capture the internal processes of human decision-making. Their alignment with human is highly variable and dependent on specific model architectures and prompt formulations; even worse, such dependence does not follow a clear pattern. LLMs can be useful task-specific tools but are not yet intelligent human-like agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21359v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ji Ma</dc:creator>
    </item>
    <item>
      <title>Generative AI in Medicine</title>
      <link>https://arxiv.org/abs/2412.10337</link>
      <description>arXiv:2412.10337v2 Announce Type: replace-cross 
Abstract: The increased capabilities of generative AI have dramatically expanded its possible use cases in medicine. We provide a comprehensive overview of generative AI use cases for clinicians, patients, clinical trial organizers, researchers, and trainees. We then discuss the many challenges -- including maintaining privacy and security, improving transparency and interpretability, upholding equity, and rigorously evaluating models -- which must be overcome to realize this potential, and the open research directions they give rise to.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10337v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Divya Shanmugam, Monica Agrawal, Rajiv Movva, Irene Y. Chen, Marzyeh Ghassemi, Maia Jacobs, Emma Pierson</dc:creator>
    </item>
  </channel>
</rss>
