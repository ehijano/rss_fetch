<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Jul 2025 01:28:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Sequential Decision Problems with Missing Feedback</title>
      <link>https://arxiv.org/abs/2507.19596</link>
      <description>arXiv:2507.19596v1 Announce Type: new 
Abstract: This paper investigates the challenges of optimal online policy learning under missing data. State-of-the-art algorithms implicitly assume that rewards are always observable. I show that when rewards are missing at random, the Upper Confidence Bound (UCB) algorithm maintains optimal regret bounds; however, it selects suboptimal policies with high probability as soon as this assumption is relaxed. To overcome this limitation, I introduce a fully nonparametric algorithm-Doubly-Robust Upper Confidence Bound (DR-UCB)-which explicitly models the form of missingness through observable covariates and achieves a nearly-optimal worst-case regret rate of $\widetilde{O}(\sqrt{T})$. To prove this result, I derive high-probability bounds for a class of doubly-robust estimators that hold under broad dependence structures. Simulation results closely match the theoretical predictions, validating the proposed framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19596v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Filippo Palomba</dc:creator>
    </item>
    <item>
      <title>Uniform Critical Values for Likelihood Ratio Tests in Boundary Problems</title>
      <link>https://arxiv.org/abs/2507.19603</link>
      <description>arXiv:2507.19603v1 Announce Type: new 
Abstract: Limit distributions of likelihood ratio statistics are well-known to be discontinuous in the presence of nuisance parameters at the boundary of the parameter space, which lead to size distortions when standard critical values are used for testing. In this paper, we propose a new and simple way of constructing critical values that yields uniformly correct asymptotic size, regardless of whether nuisance parameters are at, near or far from the boundary of the parameter space. Importantly, the proposed critical values are trivial to compute and at the same time provide powerful tests in most settings. In comparison to existing size-correction methods, the new approach exploits the monotonicity of the two components of the limiting distribution of the likelihood ratio statistic, in conjunction with rectangular confidence sets for the nuisance parameters, to gain computational tractability. Uniform validity is established for likelihood ratio tests based on the new critical values, and we provide illustrations of their construction in two key examples: (i) testing a coefficient of interest in the classical linear regression model with non-negativity constraints on control coefficients, and, (ii) testing for the presence of exogenous variables in autoregressive conditional heteroskedastic models (ARCH) with exogenous regressors. Simulations confirm that the tests have desirable size and power properties. A brief empirical illustration demonstrates the usefulness of our proposed test in relation to testing for spill-overs and ARCH effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19603v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giuseppe Cavaliere, Adam McCloskey, Rasmus S. Pedersen, Anders Rahbek</dc:creator>
    </item>
    <item>
      <title>Beyond Bonferroni: Hierarchical Multiple Testing in Empirical Research</title>
      <link>https://arxiv.org/abs/2507.19610</link>
      <description>arXiv:2507.19610v1 Announce Type: new 
Abstract: Empirical research in the social and medical sciences frequently involves testing multiple hypotheses simultaneously, increasing the risk of false positives due to chance. Classical multiple testing procedures, such as the Bonferroni correction, control the family-wise error rate (FWER) but tend to be overly conservative, reducing statistical power. Stepwise alternatives like the Holm and Hochberg procedures offer improved power while maintaining error control under certain dependence structures. However, these standard approaches typically ignore hierarchical relationships among hypotheses -- structures that are common in settings such as clinical trials and program evaluations, where outcomes are often logically or causally linked. Hierarchical multiple testing procedures -- including fixed sequence, fallback, and gatekeeping methods -- explicitly incorporate these relationships, providing more powerful and interpretable frameworks for inference. This paper reviews key hierarchical methods, compares their statistical properties and practical trade-offs, and discusses implications for applied empirical research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19610v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sebastian Calonico, Sebastian Galiani</dc:creator>
    </item>
    <item>
      <title>Binary Classification with the Maximum Score Model and Linear Programming</title>
      <link>https://arxiv.org/abs/2507.19654</link>
      <description>arXiv:2507.19654v1 Announce Type: new 
Abstract: This paper presents a computationally efficient method for binary classification using Manski's (1975,1985) maximum score model when covariates are discretely distributed and parameters are partially but not point identified. We establish conditions under which it is minimax optimal to allow for either non-classification or random classification and derive finite-sample and asymptotic lower bounds on the probability of correct classification. We also describe an extension of our method to continuous covariates. Our approach avoids the computational difficulty of maximum score estimation by reformulating the problem as two linear programs. Compared to parametric and nonparametric methods, our method balances extrapolation ability with minimal distributional assumptions. Monte Carlo simulations and empirical applications demonstrate its effectiveness and practical relevance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19654v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joel L. Horowitz, Sokbae Lee</dc:creator>
    </item>
    <item>
      <title>Semiparametric Identification of the Discount Factor and Payoff Function in Dynamic Discrete Choice Models</title>
      <link>https://arxiv.org/abs/2507.19814</link>
      <description>arXiv:2507.19814v1 Announce Type: new 
Abstract: This paper investigates how the discount factor and payoff functions can be identified in stationary infinite-horizon dynamic discrete choice models. In single-agent models, we show that common nonparametric assumptions on per-period payoffs -- such as homogeneity of degree one, monotonicity, concavity, zero cross-differences, and complementarity -- provide identifying restrictions on the discount factor. These restrictions take the form of polynomial equalities and inequalities with degrees bounded by the cardinality of the state space. These restrictions also identify payoff functions under standard normalization at one action. In dynamic game models, we show that firm-specific discount factors can be identified using assumptions such as irrelevance of other firms' lagged actions, exchangeability, and the independence of adjustment costs from other firms' actions. Our results demonstrate that widely used nonparametric assumptions in economic analysis can provide substantial identifying power in dynamic structural models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19814v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yu Hao, Hiroyuki Kasahara, Katsumi Shimotsu</dc:creator>
    </item>
    <item>
      <title>Staggered Adoption DiD Designs with Misclassification and Anticipation</title>
      <link>https://arxiv.org/abs/2507.20415</link>
      <description>arXiv:2507.20415v1 Announce Type: new 
Abstract: This paper examines the identification and estimation of treatment effects in staggered adoption designs -- a common extension of the canonical Difference-in-Differences (DiD) model to multiple groups and time-periods -- in the presence of (time varying) misclassification of the treatment status as well as of anticipation. We demonstrate that standard estimators are biased with respect to commonly used causal parameters of interest under such forms of misspecification. To address this issue, we provide modified estimators that recover the Average Treatment Effect of observed and true switching units, respectively. Additionally, we suggest a testing procedure aimed at detecting the timing and extent of misclassification and anticipation effects. We illustrate the proposed methods with an application to the effects of an anti-cheating policy on school mean test scores in high stakes national exams in Indonesia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20415v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Clara Augustin, Daniel Gutknecht, Cenchen Liu</dc:creator>
    </item>
    <item>
      <title>Policy Learning under Unobserved Confounding: A Robust and Efficient Approach</title>
      <link>https://arxiv.org/abs/2507.20550</link>
      <description>arXiv:2507.20550v1 Announce Type: new 
Abstract: This paper develops a robust and efficient method for policy learning from observational data in the presence of unobserved confounding, complementing existing instrumental variable (IV) based approaches. We employ the marginal sensitivity model (MSM) to relax the commonly used yet restrictive unconfoundedness assumption by introducing a sensitivity parameter that captures the extent of selection bias induced by unobserved confounders. Building on this framework, we consider two distributionally robust welfare criteria, defined as the worst-case welfare and policy improvement functions, evaluated over an uncertainty set of counterfactual distributions characterized by the MSM. Closed-form expressions for both welfare criteria are derived. Leveraging these identification results, we construct doubly robust scores and estimate the robust policies by maximizing the proposed criteria. Our approach accommodates flexible machine learning methods for estimating nuisance components, even when these converge at moderately slow rate. We establish asymptotic regret bounds for the resulting policies, providing a robust guarantee against the most adversarial confounding scenario. The proposed method is evaluated through extensive simulation studies and empirical applications to the JTPA study and Head Start program.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20550v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zequn Jin, Gaoqian Xu, Xi Zheng, Yahong Zhou</dc:creator>
    </item>
    <item>
      <title>Dependency Network-Based Portfolio Design with Forecasting and VaR Constraints</title>
      <link>https://arxiv.org/abs/2507.20039</link>
      <description>arXiv:2507.20039v1 Announce Type: cross 
Abstract: This study proposes a novel portfolio optimization framework that integrates statistical social network analysis with time series forecasting and risk management. Using daily stock data from the S&amp;P 500 (2020-2024), we construct dependency networks via Vector Autoregression (VAR) and Forecast Error Variance Decomposition (FEVD), transforming influence relationships into a cost-based network. Specifically, FEVD breaks down the VAR's forecast error variance to quantify how much each stock's shocks contribute to another's uncertainty information we invert to form influence-based edge weights in our network. By applying the Minimum Spanning Tree (MST) algorithm, we extract the core inter-stock structure and identify central stocks through degree centrality. A dynamic portfolio is constructed using the top-ranked stocks, with capital allocated based on Value at Risk (VaR). To refine stock selection, we incorporate forecasts from ARIMA and Neural Network Autoregressive (NNAR) models. Trading simulations over a one-year period demonstrate that the MST-based strategies outperform a buy-and-hold benchmark, with the tuned NNAR-enhanced strategy achieving a 63.74% return versus 18.00% for the benchmark. Our results highlight the potential of combining network structures, predictive modeling, and risk metrics to improve adaptive financial decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20039v1</guid>
      <category>q-fin.PM</category>
      <category>econ.EM</category>
      <category>q-fin.ST</category>
      <category>stat.ML</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zihan Lin, Haojie Liu, Randall R. Rojas</dc:creator>
    </item>
    <item>
      <title>Peer Effects in Labor Market Training</title>
      <link>https://arxiv.org/abs/2211.12366</link>
      <description>arXiv:2211.12366v3 Announce Type: replace 
Abstract: This paper shows that group composition shapes the effectiveness of labor market training programs for jobseekers. Using rich administrative data from Germany and a novel measure of employability, I find that participants benefit from greater average exposure to highly employable peers through increased long-term employment and earnings. The effects vary significantly by own employability: jobseekers with a low employability experience larger long-term gains, whereas highly employable individuals benefit primarily in the short term through higher entry wages. An analysis of mechanisms suggests that within-group competition in job search attenuates part of the positive effects that operate through knowledge spillovers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.12366v3</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ulrike Unterhofer</dc:creator>
    </item>
    <item>
      <title>Identifying Socially Disruptive Policies</title>
      <link>https://arxiv.org/abs/2306.15000</link>
      <description>arXiv:2306.15000v3 Announce Type: replace 
Abstract: Social disruption occurs when a policy creates or destroys many network connections between agents. It is a costly side effect of many interventions and so a growing empirical literature recommends measuring and accounting for social disruption when evaluating the welfare impact of a policy. However, there is currently little work characterizing what can actually be learned about social disruption from data in practice. In this paper, we consider the problem of identifying social disruption in an experimental setting. We show that social disruption is not generally point identified, but informative bounds can be constructed by rearranging the eigenvalues of the marginal distribution of network connections between pairs of agents identified from the experiment. We apply our bounds to the setting of Banerjee et al. (2021) and find large disruptive effects that the authors miss by only considering regression estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15000v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eric Auerbach, Yong Cai</dc:creator>
    </item>
    <item>
      <title>Identification of Nonlinear Dynamic Panels under Partial Stationarity</title>
      <link>https://arxiv.org/abs/2401.00264</link>
      <description>arXiv:2401.00264v4 Announce Type: replace 
Abstract: This paper provides a general identification approach for a wide range of nonlinear panel data models, including binary choice, ordered response, and other types of limited dependent variable models. Our approach accommodates dynamic models with any number of lagged dependent variables as well as other types of endogenous covariates. Our identification strategy relies on a partial stationarity condition, which allows for not only an unknown distribution of errors, but also temporal dependencies in errors. We derive partial identification results under flexible model specifications and establish sharpness of our identified set in the binary choice setting. We demonstrate the robust finite-sample performance of our approach using Monte Carlo simulations, and apply the approach to analyze the empirical application of income categories using various ordered choice models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.00264v4</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wayne Yuan Gao, Rui Wang</dc:creator>
    </item>
    <item>
      <title>Nonparametric regression for cost-effectiveness analyses with observational data -- a tutorial</title>
      <link>https://arxiv.org/abs/2507.03511</link>
      <description>arXiv:2507.03511v2 Announce Type: replace 
Abstract: Healthcare decision-making often requires selecting among treatment options under budget constraints, particularly when one option is more effective but also more costly. Cost-effectiveness analysis (CEA) provides a framework for evaluating whether the health benefits of a treatment justify its additional costs. A key component of CEA is the estimation of treatment effects on both health outcomes and costs, which becomes challenging when using observational data, due to potential confounding. While advanced causal inference methods exist for use in such circumstances, their adoption in CEAs remains limited, with many studies relying on overly simplistic methods such as linear regression or propensity score matching. We believe that this is mainly due to health economists being generally unfamiliar with superior methodology. In this paper, we address this gap by introducing cost-effectiveness researchers to modern nonparametric regression models, with a particular focus on Bayesian Additive Regression Trees (BART). We provide practical guidance on how to implement BART in CEAs, including code examples, and discuss its advantages in producing more robust and credible estimates from observational data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03511v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas Esser, Mateus Maia, Judith Bosmans, Johanna van Dongen</dc:creator>
    </item>
    <item>
      <title>Breakdown Analysis for Instrumental Variables with Binary Outcomes</title>
      <link>https://arxiv.org/abs/2507.10242</link>
      <description>arXiv:2507.10242v3 Announce Type: replace 
Abstract: This paper studies the partial identification of treatment effects in Instrumental Variables (IV) settings with binary outcomes under violations of independence. I derive the identified sets for the treatment parameters of interest in the setting, as well as breakdown values for conclusions regarding the true treatment effects. I derive $\sqrt{N}$-consistent nonparametric estimators for the bounds of treatment effects and for breakdown values. These results can be used to assess the robustness of empirical conclusions obtained under the assumption that the instrument is independent from potential quantities, which is a pervasive concern in studies that use IV methods with observational data. In the empirical application, I show that the conclusions regarding the effects of family size on female unemployment using same-sex siblings as the instrument are highly sensitive to violations of independence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10242v3</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pedro Picchetti</dc:creator>
    </item>
    <item>
      <title>Testing Clustered Equal Predictive Ability with Unknown Clusters</title>
      <link>https://arxiv.org/abs/2507.14621</link>
      <description>arXiv:2507.14621v2 Announce Type: replace 
Abstract: This paper proposes a selective inference procedure for testing equal predictive ability in panel data settings with unknown heterogeneity. The framework allows predictive performance to vary across unobserved clusters and accounts for the data-driven selection of these clusters using the Panel Kmeans Algorithm. A post-selection Wald-type statistic is constructed, and valid $p$-values are derived under general forms of autocorrelation and cross-sectional dependence in forecast loss differentials. The method accommodates conditioning on covariates or common factors and permits both strong and weak dependence across units. Simulations demonstrate the finite-sample validity of the procedure and show that it has very high power. An empirical application to exchange rate forecasting using machine learning methods illustrates the practical relevance of accounting for unknown clusters in forecast evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14621v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oguzhan Akgun, Alain Pirotte, Giovanni Urga, Zhenlin Yang</dc:creator>
    </item>
    <item>
      <title>A Simulated Reconstruction and Reidentification Attack on the 2010 U.S. Census</title>
      <link>https://arxiv.org/abs/2312.11283</link>
      <description>arXiv:2312.11283v3 Announce Type: replace-cross 
Abstract: We show that individual, confidential microdata records from the 2010 U.S. Census of Population and Housing can be accurately reconstructed from the published tabular summaries. Ninety-seven million person records (every resident in 70% of all census blocks) are exactly reconstructed with provable certainty using only public information. We further show that a hypothetical attacker using our methods can reidentify with 95% accuracy population unique individuals who are perfectly reconstructed and not in the modal race and ethnicity category in their census block (3.4 million persons)--a result that is only possible because their confidential records were used in the published tabulations. Finally, we show that the methods used for the 2020 Census, based on a differential privacy framework, provide better protection against this type of attack, with better published data accuracy, than feasible alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.11283v3</guid>
      <category>stat.AP</category>
      <category>cs.CR</category>
      <category>econ.EM</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1162/99608f92.4a1ebf70</arxiv:DOI>
      <dc:creator>John M. Abowd, Tamara Adams, Robert Ashmead, David Darais, Sourya Dey, Simson L. Garfinkel, Nathan Goldschlag, Michael B. Hawes, Daniel Kifer, Philip Leclerc, Ethan Lew, Scott Moore, Rolando A. Rodr\'iguez, Ramy N. Tadros, Lars Vilhuber</dc:creator>
    </item>
    <item>
      <title>Identification and estimation for matrix time series CP-factor models</title>
      <link>https://arxiv.org/abs/2410.05634</link>
      <description>arXiv:2410.05634v3 Announce Type: replace-cross 
Abstract: We propose a new method for identifying and estimating the CP-factor models for matrix time series. Unlike the generalized eigenanalysis-based method of Chang et al. (2023) for which the convergence rates of the associated estimators may suffer from small eigengaps as the asymptotic theory is based on some matrix perturbation analysis, the proposed new method enjoys faster convergence rates which are free from any eigengaps. It achieves this by turning the problem into a joint diagonalization of several matrices whose elements are determined by a basis of a linear system, and by choosing the basis carefully to avoid near co-linearity (see Proposition 5 and Section 4.3). Furthermore, unlike Chang et al. (2023) which requires the two factor loading matrices to be full-ranked, the proposed new method can handle rank-deficient factor loading matrices. Illustration with both simulated and real matrix time series data shows the advantages of the proposed new method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05634v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinyuan Chang, Yue Du, Guanglin Huang, Qiwei Yao</dc:creator>
    </item>
  </channel>
</rss>
