<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SI</link>
    <description>cs.SI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 05 Dec 2025 02:34:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Accumulated Local Effects and Graph Neural Networks for link prediction</title>
      <link>https://arxiv.org/abs/2512.03061</link>
      <description>arXiv:2512.03061v1 Announce Type: new 
Abstract: We investigate how Accumulated Local Effects (ALE), a model-agnostic explanation method, can be adapted to visualize the influence of node feature values in link prediction tasks using Graph Neural Networks (GNNs), specifically Graph Convolutional Networks and Graph Attention Networks. A key challenge addressed in this work is the complex interactions of nodes during message passing within GNN layers, complicating the direct application of ALE. Since a straightforward solution of modifying only one node at once substantially increases computation time, we propose an approximate method that mitigates this challenge. Our findings reveal that although the approximate method offers computational efficiency, the exact method yields more stable explanations, particularly when smaller data subsets are used. However, the explanations produced with the approximate method are not significantly different from the ones obtained with the exact method. Additionally, we analyze how varying parameters affect the accuracy of ALE estimation for both approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03061v1</guid>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paulina Kaczy\'nska, Julian Sienkiewicz, Dominik \'Sl\k{e}zak</dc:creator>
    </item>
    <item>
      <title>Unsupervised Multimodal Graph-based Model for Geo-social Analysis</title>
      <link>https://arxiv.org/abs/2512.03063</link>
      <description>arXiv:2512.03063v1 Announce Type: new 
Abstract: The systematic analysis of user-generated social media content, especially when enriched with geospatial context, plays a vital role in domains such as disaster management and public opinion monitoring. Although multimodal approaches have made significant progress, most existing models remain fragmented, processing each modality separately rather than integrating them into a unified end-to-end model. To address this, we propose an unsupervised, multimodal graph-based methodology that jointly embeds semantic and geographic information into a shared representation space. The proposed methodology comprises two architectural paradigms: a mono graph (MonoGrah) model that jointly encodes both modalities, and a multi graph (MultiGraph) model that separately models semantic and geographic relationships and subsequently integrates them through multi-head attention mechanisms. A composite loss, combining contrastive, coherence, and alignment objectives, guides the learning process to produce semantically coherent and spatially compact clusters. Experiments on four real-world disaster datasets demonstrate that our models consistently outperform existing baselines in topic quality, spatial coherence, and interpretability. Inherently domain-independent, the framework can be readily extended to diverse forms of multimodal data and a wide range of downstream analysis tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03063v1</guid>
      <category>cs.SI</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ehsaneddin Jalilian, Bernd Resch</dc:creator>
    </item>
    <item>
      <title>Demographic Inference from Social Media Data with Multimodal Foundation Models: Strategies, Evaluation, and Benchmarking</title>
      <link>https://arxiv.org/abs/2512.03064</link>
      <description>arXiv:2512.03064v1 Announce Type: new 
Abstract: Demographic inference plays a crucial role in understanding the representativeness and equity of social media-based research. However, existing methods typically rely on a single modality, such as text, image, or network, and are limited to predicting one or two demographic attributes, constraining their generalizability and robustness across populations. This study leverages GPT-5, a state-of-the-art multimodal foundation model, to infer age, gender, and race from social media profiles. Using a dataset of 263 publicly available X (formerly Twitter) users, we design a progressive multimodal framework that incrementally incorporates usernames, profile descriptions, tweets, and profile images to examine how each information source contributes to inference accuracy. Results show a consistent improvement across all conditions, with the inclusion of textual and visual cues substantially enhancing performance. GPT-5 achieves an overall accuracy of 0.90 for age, 0.98 for gender, and 0.85 for race, outperforming existing models under equivalent inputs. These findings demonstrate the potential of large multimodal foundation models to capture complex, cross-modal demographic cues with minimal task-specific training. The study further highlights a transparent, interpretable approach to multimodal reasoning that advances the accuracy, fairness, and scalability of demographic inference in social data analytics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03064v1</guid>
      <category>cs.SI</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao Yang, Angela Yao, Eric Chang, Hexiang Wang</dc:creator>
    </item>
    <item>
      <title>Quantifying the Potential to Escape Filter Bubbles: A Behavior-Aware Measure via Contrastive Simulation</title>
      <link>https://arxiv.org/abs/2512.03067</link>
      <description>arXiv:2512.03067v1 Announce Type: new 
Abstract: Nowadays, recommendation systems have become crucial to online platforms, shaping user exposure by accurate preference modeling. However, such an exposure strategy can also reinforce users' existing preferences, leading to a notorious phenomenon named filter bubbles. Given its negative effects, such as group polarization, increasing attention has been paid to exploring reasonable measures to filter bubbles. However, most existing evaluation metrics simply measure the diversity of user exposure, failing to distinguish between algorithmic preference modeling and actual information confinement. In view of this, we introduce Bubble Escape Potential (BEP), a behavior-aware measure that quantifies how easily users can escape from filter bubbles. Specifically, BEP leverages a contrastive simulation framework that assigns different behavioral tendencies (e.g., positive vs. negative) to synthetic users and compares the induced exposure patterns. This design enables decoupling the effect of filter bubbles and preference modeling, allowing for more precise diagnosis of bubble severity. We conduct extensive experiments across multiple recommendation models to examine the relationship between predictive accuracy and bubble escape potential across different groups. To the best of our knowledge, our empirical results are the first to quantitatively validate the dilemma between preference modeling and filter bubbles. What's more, we observe a counter-intuitive phenomenon that mild random recommendations are ineffective in alleviating filter bubbles, which can offer a principled foundation for further work in this direction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03067v1</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Difu Feng, Qianqian Xu, Zitai Wang, Cong Hua, Zhiyong Yang, Qingming Huang</dc:creator>
    </item>
    <item>
      <title>Approximate Bayesian Inference on Mechanisms of Network Growth and Evolution</title>
      <link>https://arxiv.org/abs/2512.03092</link>
      <description>arXiv:2512.03092v1 Announce Type: new 
Abstract: Mechanistic models can provide an intuitive and interpretable explanation of network growth by specifying a set of generative rules. These rules can be defined by domain knowledge about real-world mechanisms governing network growth or may be designed to facilitate the appearance of certain network motifs. In the formation of real-world networks, multiple mechanisms may be simultaneously involved; it is then important to understand the relative contribution of each of these mechanisms. In this paper, we propose the use of a conditional density estimator, augmented with a graph neural network, to perform inference on a flexible mixture of network-forming mechanisms. This event-wise mixture-of-mechanisms model assigns mechanisms to each edge formation event rather than stipulating node-level mechanisms, thus allowing for an explanation of the network generation process, as well as the dynamic evolution of the network over time. We demonstrate that our approximate Bayesian approach yields valid inferences for the relative weights of the mechanisms in our model, and we utilize this method to investigate the mechanisms behind the formation of a variety of real-world networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03092v1</guid>
      <category>cs.SI</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Maxwell H Wang, Till Hoffmann, Jukka-Pekka Onnela</dc:creator>
    </item>
    <item>
      <title>Community Quality and Influence Maximization: An Empirical Study</title>
      <link>https://arxiv.org/abs/2512.03095</link>
      <description>arXiv:2512.03095v1 Announce Type: new 
Abstract: Influence maximization in social networks plays a vital role in applications such as viral marketing, epidemiology, product recommendation, opinion mining, and counter-terrorism. A common approach identifies seed nodes by first detecting disjoint communities and subsequently selecting representative nodes from these communities. However, whether the quality of detected communities consistently affects the spread of influence under the Independent Cascade model remains unclear. This paper addresses this question by extending a previously proposed disjoint community detection method, termed $\alpha$-Hierarchical Clustering, to the influence maximization problem under the Independent Cascade model. The proposed method is compared with an alternative approach that employs the same seed selection criteria but relies on communities of lower quality obtained through standard Hierarchical Clustering. The former is referred to as Hierarchical Clustering-based Influence Maximization, while the latter, which leverages higher-quality community structures to guide seed selection, is termed $\alpha$-Hierarchical Clustering-based Influence Maximization. Extensive experiments are performed on multiple real-world datasets to assess the effectiveness of both methods. The results demonstrate that higher-quality community structures substantially improve information diffusion under the Independent Cascade model, particularly when the propagation probability is low. These findings underscore the critical importance of community quality in guiding effective seed selection for influence maximization in complex networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03095v1</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Motaz Ben Hassine (CRIL)</dc:creator>
    </item>
    <item>
      <title>Public Sentiment Analysis of Traffic Management Policies in Knoxville: A Social Media Driven Study</title>
      <link>https://arxiv.org/abs/2512.03103</link>
      <description>arXiv:2512.03103v1 Announce Type: new 
Abstract: This study presents a comprehensive analysis of public sentiment toward traffic management policies in Knoxville, Tennessee, utilizing social media data from Twitter and Reddit platforms. We collected and analyzed 7906 posts spanning January 2022 to December 2023, employing Valence Aware Dictionary and sEntiment Reasoner (VADER) for sentiment analysis and Latent Dirichlet Allocation (LDA) for topic modeling. Our findings reveal predominantly negative sentiment, with significant variations across platforms and topics. Twitter exhibited more negative sentiment compared to Reddit. Topic modeling identified six distinct themes, with construction-related topics showing the most negative sentiment while general traffic discussions were more positive. Spatiotemporal analysis revealed geographic and temporal patterns in sentiment expression. The research demonstrates social media's potential as a real-time public sentiment monitoring tool for transportation planning and policy evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03103v1</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shampa Saha, Shovan Roy</dc:creator>
    </item>
    <item>
      <title>Associating Healthcare Teamwork with Patient Outcomes for Predictive Analysis</title>
      <link>https://arxiv.org/abs/2512.03296</link>
      <description>arXiv:2512.03296v1 Announce Type: new 
Abstract: Cancer treatment outcomes are influenced not only by clinical and demographic factors but also by the collaboration of healthcare teams. However, prior work has largely overlooked the potential role of human collaboration in shaping patient survival. This paper presents an applied AI approach to uncovering the impact of healthcare professionals' (HCPs) collaboration-captured through electronic health record (EHR) systems-on cancer patient outcomes. We model EHR-mediated HCP interactions as networks and apply machine learning techniques to detect predictive signals of patient survival embedded in these collaborations. Our models are cross validated to ensure generalizability, and we explain the predictions by identifying key network traits associated with improved outcomes. Importantly, clinical experts and literature validate the relevance of the identified crucial collaboration traits, reinforcing their potential for real-world applications. This work contributes to a practical workflow for leveraging digital traces of collaboration and AI to assess and improve team-based healthcare. The approach is potentially transferable to other domains involving complex collaboration and offers actionable insights to support data-informed interventions in healthcare delivery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03296v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hsiao-Ying Lu, Kwan-Liu Ma</dc:creator>
    </item>
    <item>
      <title>Epistemic Substitution: How Grokipedia's AI-Generated Encyclopedia Restructures Authority</title>
      <link>https://arxiv.org/abs/2512.03337</link>
      <description>arXiv:2512.03337v1 Announce Type: new 
Abstract: A quarter century ago, Wikipedia's decentralized, crowdsourced, and consensus-driven model replaced the centralized, expert-driven, and authority-based standard for encyclopedic knowledge curation. The emergence of generative AI encyclopedias, such as Grokipedia, possibly presents another potential shift in epistemic evolution. This study investigates whether AI- and human-curated encyclopedias rely on the same foundations of authority. We conducted a multi-scale comparative analysis of the citation networks from 72 matched article pairs, which cite a total of almost 60,000 sources. Using an 8-category epistemic classification, we mapped the "epistemic profiles" of the articles on each platform. Our findings reveal several quantitative and qualitative differences in how knowledge is sourced and encyclopedia claims are epistemologically justified. Grokipedia replaces Wikipedia's heavy reliance on peer-reviewed "Academic &amp; Scholarly" work with a notable increase in "User-generated" and "Civic organization" sources. Comparative network analyses further show that Grokipedia employs very different epistemological profiles when sourcing leisure topics (such as Sports and Entertainment) and more societal sensitive civic topics (such as Politics &amp; Conflicts, Geographical Entities, and General Knowledge &amp; Society). Finally, we find a "scaling-law for AI-generated knowledge sourcing" that shows a linear relationship between article length and citation density, which is distinct from collective human reference sourcing. We conclude that this first implementation of an LLM-based encyclopedia does not merely automate knowledge production but restructures it. Given the notable changes and the important role of encyclopedias, we suggest the continuation and deepening of algorithm audits, such as the one presented here, in order to understand the ongoing epistemological shifts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03337v1</guid>
      <category>cs.SI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aliakbar Mehdizadeh, Martin Hilbert</dc:creator>
    </item>
    <item>
      <title>Inaccessibility in Public Transit Networks</title>
      <link>https://arxiv.org/abs/2512.03766</link>
      <description>arXiv:2512.03766v1 Announce Type: new 
Abstract: The study of networks derived from infrastructure systems has received considerable attention, yet the accessibility of such systems, particularly within public transit networks, remains comparatively underexplored. Accessibility encompasses a broad range of considerations, from infrastructure-based features such as elevators and step-free access to spatial factors such as the geographic distribution of accessible stations. In this work, we investigate infrastructure-based accessibility in two major transit systems: the London Underground and the New York City Subway. We construct network models in which nodes represent accessible stations and edges represent adjacency along transit lines. Using tools from network analysis, we examine the structural properties of these accessibility networks, including clustering patterns and the spatial distribution of accessible nodes. We further employ centrality measures to identify stations that serve as major accessible hubs. Finally, we analyze socioeconomic and tourism-related variables to assess the influence of neighborhood wealth and popularity on the prevalence of accessible stations. Our findings highlight significant disparities in accessibility across both systems and demonstrate the utility of mathematical and network-theoretic methods in understanding and improving modern transit infrastructure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03766v1</guid>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Katherine Betz</dc:creator>
    </item>
    <item>
      <title>DSP: A Statistically-Principled Structural Polarization Measure</title>
      <link>https://arxiv.org/abs/2512.03937</link>
      <description>arXiv:2512.03937v1 Announce Type: new 
Abstract: Social and information networks may become polarized, leading to echo chambers and political gridlock. Accurately measuring this phenomenon is a critical challenge. Existing measures often conflate genuine structural division with random topological features, yielding misleadingly high polarization scores on random networks, and failing to distinguish real-world networks from randomized null models. We introduce DSP, a Diffusion-based Structural Polarization measure designed from first principles to correct for such biases. DSP removes the arbitrary concept of 'influencers' used by the popular Random Walk Controversy (RWC) score, instead treating every node as a potential origin for a random walk. To validate our approach, we introduce a set of desirable properties for polarization measures, expressed through reference topologies with known structural properties. We show that DSP satisfies these desiderata, being near-zero for non-polarized structures such as cliques and random networks, while correctly capturing the expected polarization of reference topologies such as monochromatic-splittable networks. Our method applied to U.S. Congress datasets uncovers trends of increasing polarization in recent years. By integrating a null model into its core definition, DSP provides a reliable and interpretable diagnostic tool, highlighting the necessity of statistically-grounded metrics to analyze societal fragmentation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03937v1</guid>
      <category>cs.SI</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giulia Preti, Matteo Riondato, Aristides Gionis, Gianmarco De Francisci Morales</dc:creator>
    </item>
    <item>
      <title>Model-Agnostic Fairness Regularization for GNNs with Incomplete Sensitive Information</title>
      <link>https://arxiv.org/abs/2512.03074</link>
      <description>arXiv:2512.03074v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) have demonstrated exceptional efficacy in relational learning tasks, including node classification and link prediction. However, their application raises significant fairness concerns, as GNNs can perpetuate and even amplify societal biases against protected groups defined by sensitive attributes such as race or gender. These biases are often inherent in the node features, structural topology, and message-passing mechanisms of the graph itself. A critical limitation of existing fairness-aware GNN methods is their reliance on the strong assumption that sensitive attributes are fully available for all nodes during training--a condition that poses a practical impediment due to privacy concerns and data collection constraints. To address this gap, we propose a novel, model-agnostic fairness regularization framework designed for the realistic scenario where sensitive attributes are only partially available. Our approach formalizes a fairness-aware objective function that integrates both equal opportunity and statistical parity as differentiable regularization terms. Through a comprehensive empirical evaluation across five real-world benchmark datasets, we demonstrate that the proposed method significantly mitigates bias across key fairness metrics while maintaining competitive node classification performance. Results show that our framework consistently outperforms baseline models in achieving a favorable fairness-accuracy trade-off, with minimal degradation in predictive accuracy. The datasets and source code will be publicly released at https://github.com/mtavassoli/GNN-FC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03074v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mahdi Tavassoli Kejani, Fadi Dornaika, Jean-Michel Loubes</dc:creator>
    </item>
    <item>
      <title>Generating a Contact Matrix for Aged Care Settings in Australia: an agent-based model study</title>
      <link>https://arxiv.org/abs/2512.03866</link>
      <description>arXiv:2512.03866v1 Announce Type: cross 
Abstract: This study presents an agent-based model (ABM) developed to simulate staff and resident interactions within a synthetic aged care facility, capturing movement, task execution, and proximity-based contact events across three staff shifts and varying levels of resident care. Contacts were defined by spatial thresholds (1.5 m and 3 m) and cumulative duration, enabling the generation of detailed contact matrices. Simulation results showed that low and medium care residents experienced the highest frequency of interactions, particularly with staff on morning and afternoon shifts, while high care residents and night staff had substantially fewer contacts. Contact rates varied significantly by care level and shift, confirmed through Poisson-based regression modelling. Temporal analyses revealed clustering of high-risk contacts during structured daily routines, especially communal and care activities. An integrated airborne transmission module, seeded with a single infectious staff member, demonstrated that infection risk was highest during high-contact shifts and among medium care residents. Vaccination scenarios reduced predicted transmission by up to 68\%, with the greatest impact observed when both staff and residents were vaccinated. These findings highlight the importance of accounting for contact heterogeneity in aged care and demonstrate the utility of ABMs for evaluating targeted infection control strategies in high-risk, enclosed environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03866v1</guid>
      <category>q-bio.PE</category>
      <category>cs.SI</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haley Stone, C. Raina MacIntyre, Mohana Kunasekaran, Chris Poulos, David Heslop</dc:creator>
    </item>
    <item>
      <title>Aggregating maximal cliques in real-world graphs</title>
      <link>https://arxiv.org/abs/2512.03960</link>
      <description>arXiv:2512.03960v1 Announce Type: cross 
Abstract: Maximal clique enumeration is a fundamental graph mining task, but its utility is often limited by computational intractability and highly redundant output. To address these challenges, we introduce \emph{$\rho$-dense aggregators}, a novel approach that succinctly captures maximal clique structure. Instead of listing all cliques, we identify a small collection of clusters with edge density at least $\rho$ that collectively contain every maximal clique.
  In contrast to maximal clique enumeration, we prove that for all $\rho &lt; 1$, every graph admits a $\rho$-dense aggregator of \emph{sub-exponential} size, $n^{O(\log_{1/\rho}n)}$, and provide an algorithm achieving this bound. For graphs with bounded degeneracy, a typical characteristic of real-world networks, our algorithm runs in near-linear time and produces near-linear size aggregators. We also establish a matching lower bound on aggregator size, proving our results are essentially tight. In an empirical evaluation on real-world networks, we demonstrate significant practical benefits for the use of aggregators: our algorithm is consistently faster than the state-of-the-art clique enumeration algorithm, with median speedups over $6\times$ for $\rho=0.1$ (and over $300\times$ in an extreme case), while delivering a much more concise structural summary.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03960v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>cs.SI</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Noga Alon, Sabyasachi Basu, Shweta Jain, Haim Kaplan, Jakub {\L}\k{a}cki, Blair D. Sullivan</dc:creator>
    </item>
    <item>
      <title>A Novel Discrete-time Model of Information Diffusion on Social Networks Considering Users Behavior</title>
      <link>https://arxiv.org/abs/2510.22501</link>
      <description>arXiv:2510.22501v2 Announce Type: replace 
Abstract: In this paper, we introduce the SDIR (Susceptible-Delayable-Infected-Recovered) model, an extension of the classical SIR epidemic framework, to provide a more explicit characterization of user behavior in online social networks. The newly merged state D (delayable) represents users who have received the information but delayed its spreading and may eventually choose not to share it at all. Based on the mean-field approximation method, we derive the dynamical equations of the model and investigate its convergence and stability conditions. Under these conditions, we further propose an approximation algorithm for the edge-deletion problem, aiming to minimize the influence of information diffusion by identifying approximate solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22501v2</guid>
      <category>cs.SI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tran Van Khanh, Do Xuan Cho, Hoang Phi Dung</dc:creator>
    </item>
    <item>
      <title>Embedding networks with the random walk first return time distribution</title>
      <link>https://arxiv.org/abs/2512.02694</link>
      <description>arXiv:2512.02694v2 Announce Type: replace 
Abstract: We propose the first return time distribution (FRTD) of a random walk as an interpretable and mathematically grounded node embedding. The FRTD assigns a probability mass function to each node, allowing us to define a distance between any pair of nodes using standard metrics for discrete distributions. We present several arguments to motivate the FRTD embedding. First, we show that FRTDs are strictly more informative than eigenvalue spectra, yet insufficient for complete graph identification, thus placing FRTD equivalence between cospectrality and isomorphism. Second, we argue that FRTD equivalence between nodes captures structural similarity. Third, we empirically demonstrate that the FRTD embedding outperforms manually designed graph metrics in network alignment tasks. Finally, we show that random networks that approximately match the FRTD of a desired target also preserve other salient features. Together these results demonstrate the FRTD as a simple and mathematically principled embedding for complex networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02694v2</guid>
      <category>cs.SI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vedanta Thapar, Renaud Lambiotte, George T. Cantwell</dc:creator>
    </item>
    <item>
      <title>Variational Inference of Parameters in Opinion Dynamics Models</title>
      <link>https://arxiv.org/abs/2403.05358</link>
      <description>arXiv:2403.05358v2 Announce Type: replace-cross 
Abstract: Despite the frequent use of agent-based models (ABMs) for studying social phenomena, parameter estimation remains a challenge, often relying on costly simulation-based heuristics. This work uses variational inference to estimate the parameters of an opinion dynamics ABM, by transforming the estimation problem into an optimization task that can be solved directly.
  Our proposal relies on probabilistic generative ABMs (PGABMs): we start by synthesizing a probabilistic generative model from the ABM rules. Then, we transform the inference process into an optimization problem suitable for automatic differentiation. In particular, we use the Gumbel-Softmax reparameterization for categorical agent attributes and stochastic variational inference for parameter estimation. Furthermore, we explore the trade-offs of using variational distributions with different complexity: normal distributions and normalizing flows.
  We validate our method on a bounded confidence model with agent roles (leaders and followers). Our approach estimates both macroscopic (bounded confidence intervals and backfire thresholds) and microscopic ($200$ categorical, agent-level roles) more accurately than simulation-based and MCMC methods. Consequently, our technique enables experts to tune and validate their ABMs against real-world observations, thus providing insights into human behavior in social systems via data-driven analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05358v2</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <category>stat.ML</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jacopo Lenti, Fabrizio Silvestri, Gianmarco De Francisci Morales</dc:creator>
    </item>
  </channel>
</rss>
