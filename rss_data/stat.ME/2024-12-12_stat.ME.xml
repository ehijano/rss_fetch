<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Dec 2024 05:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Improved Small Area Inference from Data Integration Using Global-Local Priors</title>
      <link>https://arxiv.org/abs/2412.07824</link>
      <description>arXiv:2412.07824v1 Announce Type: new 
Abstract: We present and apply methodology to improve inference for small area parameters by using data from several sources. This work extends Cahoy and Sedransk (2023) who showed how to integrate summary statistics from several sources. Our methodology uses hierarchical global-local prior distributions to make inferences for the proportion of individuals in Florida's counties who do not have health insurance. Results from an extensive simulation study show that this methodology will provide improved inference by using several data sources. Among the five model variants evaluated the ones using horseshoe priors for all variances have better performance than the ones using lasso priors for the local variances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07824v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>D Cahoy, J Sedransk</dc:creator>
    </item>
    <item>
      <title>Spectral Differential Network Analysis for High-Dimensional Time Series</title>
      <link>https://arxiv.org/abs/2412.07905</link>
      <description>arXiv:2412.07905v1 Announce Type: new 
Abstract: Spectral networks derived from multivariate time series data arise in many domains, from brain science to Earth science. Often, it is of interest to study how these networks change under different conditions. For instance, to better understand epilepsy, it would be interesting to capture the changes in the brain connectivity network as a patient experiences a seizure, using electroencephalography data. A common approach relies on estimating the networks in each condition and calculating their difference. Such estimates may behave poorly in high dimensions as the networks themselves may not be sparse in structure while their difference may be. We build upon this observation to develop an estimator of the difference in inverse spectral densities across two conditions. Using an L1 penalty on the difference, consistency is established by only requiring the difference to be sparse. We illustrate the method on synthetic data experiments, on experiments with electroencephalography data, and on experiments with optogentic stimulation and micro-electrocorticography data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07905v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Hellstern, Byol Kim, Zaid Harchaoui, Ali Shojaie</dc:creator>
    </item>
    <item>
      <title>Spatial scale-aware tail dependence modeling for high-dimensional spatial extremes</title>
      <link>https://arxiv.org/abs/2412.07957</link>
      <description>arXiv:2412.07957v1 Announce Type: new 
Abstract: Extreme events over large spatial domains may exhibit highly heterogeneous tail dependence characteristics, yet most existing spatial extremes models yield only one dependence class over the entire spatial domain. To accurately characterize "data-level dependence'' in analysis of extreme events, we propose a mixture model that achieves flexible dependence properties and allows high-dimensional inference for extremes of spatial processes. We modify the popular random scale construction that multiplies a Gaussian random field by a single radial variable; we allow the radial variable to vary smoothly across space and add non-stationarity to the Gaussian process. As the level of extremeness increases, this single model exhibits both asymptotic independence at long ranges and either asymptotic dependence or independence at short ranges. We make joint inference on the dependence model and a marginal model using a copula approach within a Bayesian hierarchical model. Three different simulation scenarios show close to nominal frequentist coverage rates. Lastly, we apply the model to a dataset of extreme summertime precipitation over the central United States. We find that the joint tail of precipitation exhibits non-stationary dependence structure that cannot be captured by limiting extreme value models or current state-of-the-art sub-asymptotic models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07957v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muyang Shi, Likun Zhang, Mark D. Risser, Benjamin A. Shaby</dc:creator>
    </item>
    <item>
      <title>Hypothesis Testing for High-Dimensional Matrix-Valued Data</title>
      <link>https://arxiv.org/abs/2412.07987</link>
      <description>arXiv:2412.07987v1 Announce Type: new 
Abstract: This paper addresses hypothesis testing for the mean of matrix-valued data in high-dimensional settings. We investigate the minimum discrepancy test, originally proposed by Cragg (1997), which serves as a rank test for lower-dimensional matrices. We evaluate the performance of this test as the matrix dimensions increase proportionally with the sample size, and identify its limitations when matrix dimensions significantly exceed the sample size. To address these challenges, we propose a new test statistic tailored for high-dimensional matrix rank testing. The oracle version of this statistic is analyzed to highlight its theoretical properties. Additionally, we develop a novel approach for constructing a sparse singular value decomposition (SVD) estimator for singular vectors, providing a comprehensive examination of its theoretical aspects. Using the sparse SVD estimator, we explore the properties of the sample version of our proposed statistic. The paper concludes with simulation studies and two case studies involving surveillance video data, demonstrating the practical utility of our proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07987v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shijie Cui, Danning Li, Runze Li, Lingzhou Xue</dc:creator>
    </item>
    <item>
      <title>Robust and efficient estimation of time-varying treatment effects using marginal structural models dependent on partial treatment history</title>
      <link>https://arxiv.org/abs/2412.08042</link>
      <description>arXiv:2412.08042v1 Announce Type: new 
Abstract: Inverse probability (IP) weighting of marginal structural models (MSMs) can provide consistent estimators of time-varying treatment effects under correct model specifications and identifiability assumptions, even in the presence of time-varying confounding. However, this method has two problems: (i) inefficiency due to IP-weights cumulating all time points and (ii) bias and inefficiency due to the MSM misspecification. To address these problems, we propose new IP-weights for estimating the parameters of the MSM dependent on partial treatment history and closed testing procedures for selecting the MSM under known IP-weights. In simulation studies, our proposed methods outperformed existing methods in terms of both performance in estimating time-varying treatment effects and in selecting the correct MSM. Our proposed methods were also applied to real data of hemodialysis patients with reasonable results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08042v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nodoka Seya, Masataka Taguri, Takeo Ishii</dc:creator>
    </item>
    <item>
      <title>Two-way Node Popularity Model for Directed and Bipartite Networks</title>
      <link>https://arxiv.org/abs/2412.08051</link>
      <description>arXiv:2412.08051v1 Announce Type: new 
Abstract: There has been extensive research on community detection in directed and bipartite networks. However, these studies often fail to consider the popularity of nodes in different communities, which is a common phenomenon in real-world networks. To address this issue, we propose a new probabilistic framework called the Two-Way Node Popularity Model (TNPM). The TNPM also accommodates edges from different distributions within a general sub-Gaussian family. We introduce the Delete-One-Method (DOM) for model fitting and community structure identification, and provide a comprehensive theoretical analysis with novel technical skills dealing with sub-Gaussian generalization. Additionally, we propose the Two-Stage Divided Cosine Algorithm (TSDC) to handle large-scale networks more efficiently. Our proposed methods offer multi-folded advantages in terms of estimation accuracy and computational efficiency, as demonstrated through extensive numerical studies. We apply our methods to two real-world applications, uncovering interesting findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08051v1</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bing-Yi Jing, Ting Li, Jiangzhou Wang, Ya Wang</dc:creator>
    </item>
    <item>
      <title>Dynamic Classification of Latent Disease Progression with Auxiliary Surrogate Labels</title>
      <link>https://arxiv.org/abs/2412.08088</link>
      <description>arXiv:2412.08088v1 Announce Type: new 
Abstract: Disease progression prediction based on patients' evolving health information is challenging when true disease states are unknown due to diagnostic capabilities or high costs. For example, the absence of gold-standard neurological diagnoses hinders distinguishing Alzheimer's disease (AD) from related conditions such as AD-related dementias (ADRDs), including Lewy body dementia (LBD). Combining temporally dependent surrogate labels and health markers may improve disease prediction. However, existing literature models informative surrogate labels and observed variables that reflect the underlying states using purely generative approaches, limiting the ability to predict future states. We propose integrating the conventional hidden Markov model as a generative model with a time-varying discriminative classification model to simultaneously handle potentially misspecified surrogate labels and incorporate important markers of disease progression. We develop an adaptive forward-backward algorithm with subjective labels for estimation, and utilize the modified posterior and Viterbi algorithms to predict the progression of future states or new patients based on objective markers only. Importantly, the adaptation eliminates the need to model the marginal distribution of longitudinal markers, a requirement in traditional algorithms. Asymptotic properties are established, and significant improvement with finite samples is demonstrated via simulation studies. Analysis of the neuropathological dataset of the National Alzheimer's Coordinating Center (NACC) shows much improved accuracy in distinguishing LBD from AD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08088v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zexi Cai, Donglin Zeng, Karen S. Marder, Lawrence S. Honig, Yuanjia Wang</dc:creator>
    </item>
    <item>
      <title>Improving Active Learning with a Bayesian Representation of Epistemic Uncertainty</title>
      <link>https://arxiv.org/abs/2412.08225</link>
      <description>arXiv:2412.08225v1 Announce Type: new 
Abstract: A popular strategy for active learning is to specifically target a reduction in epistemic uncertainty, since aleatoric uncertainty is often considered as being intrinsic to the system of interest and therefore not reducible. Yet, distinguishing these two types of uncertainty remains challenging and there is no single strategy that consistently outperforms the others. We propose to use a particular combination of probability and possibility theories, with the aim of using the latter to specifically represent epistemic uncertainty, and we show how this combination leads to new active learning strategies that have desirable properties. In order to demonstrate the efficiency of these strategies in non-trivial settings, we introduce the notion of a possibilistic Gaussian process (GP) and consider GP-based multiclass and binary classification problems, for which the proposed methods display a strong performance for both simulated and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08225v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jake Thomas, Jeremie Houssineau</dc:creator>
    </item>
    <item>
      <title>Heavy Tail Robust Estimation and Inference for Average Treatment Effects</title>
      <link>https://arxiv.org/abs/2412.08458</link>
      <description>arXiv:2412.08458v1 Announce Type: new 
Abstract: We study the probability tail properties of Inverse Probability Weighting (IPW) estimators of the Average Treatment Effect (ATE) when there is limited overlap between the covariate distributions of the treatment and control groups. Under unconfoundedness of treatment assignment conditional on covariates, such limited overlap is manifested in the propensity score for certain units being very close (but not equal) to 0 or 1. This renders IPW estimators possibly heavy tailed, and with a slower than sqrt(n) rate of convergence. Trimming or truncation is ultimately based on the covariates, ignoring important information about the inverse probability weighted random variable Z that identifies ATE by E[Z]= ATE. We propose a tail-trimmed IPW estimator whose performance is robust to limited overlap. In terms of the propensity score, which is generally unknown, we plug-in its parametric estimator in the infeasible Z, and then negligibly trim the resulting feasible Z adaptively by its large values. Trimming leads to bias if Z has an asymmetric distribution and an infinite variance, hence we estimate and remove the bias using important improvements on existing theory and methods. Our estimator sidesteps dimensionality, bias and poor correspondence properties associated with trimming by the covariates or propensity score. Monte Carlo experiments demonstrate that trimming by the covariates or the propensity score requires the removal of a substantial portion of the sample to render a low bias and close to normal estimator, while our estimator has low bias and mean-squared error, and is close to normal, based on the removal of very few sample extremes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08458v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan B. Hill, Saraswata Chaudhuri</dc:creator>
    </item>
    <item>
      <title>A robust, scalable K-statistic for quantifying immune cell clustering in spatial proteomics data</title>
      <link>https://arxiv.org/abs/2412.08498</link>
      <description>arXiv:2412.08498v1 Announce Type: new 
Abstract: Spatial summary statistics based on point process theory are widely used to quantify the spatial organization of cell populations in single-cell spatial proteomics data. Among these, Ripley's $K$ is a popular metric for assessing whether cells are spatially clustered or are randomly dispersed. However, the key assumption of spatial homogeneity is frequently violated in spatial proteomics data, leading to overestimates of cell clustering and colocalization. To address this, we propose a novel $K$-based method, termed \textit{KAMP} (\textbf{K} adjustment by \textbf{A}nalytical \textbf{M}oments of the \textbf{P}ermutation distribution), for quantifying the spatial organization of cells in spatial proteomics samples. \textit{KAMP} leverages background cells in each sample along with a new closed-form representation of the first and second moments of the permutation distribution of Ripley's $K$ to estimate an empirical null model. Our method is robust to inhomogeneity, computationally efficient even in large datasets, and provides approximate $p$-values for testing spatial clustering and colocalization. Methodological developments are motivated by a spatial proteomics study of 103 women with ovarian cancer, where our analysis using \textit{KAMP} shows a positive association between immune cell clustering and overall patient survival. Notably, we also find evidence that using $K$ without correcting for sample inhomogeneity may bias hazard ratio estimates in downstream analyses. \textit{KAMP} completes this analysis in just 5 minutes, compared to 538 minutes for the only competing method that adequately addresses inhomogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08498v1</guid>
      <category>stat.ME</category>
      <category>q-bio.GN</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julia Wrobel, Hoseung Song</dc:creator>
    </item>
    <item>
      <title>Rate accelerated inference for integrals of multivariate random functions</title>
      <link>https://arxiv.org/abs/2412.08533</link>
      <description>arXiv:2412.08533v1 Announce Type: new 
Abstract: The computation of integrals is a fundamental task in the analysis of functional data, which are typically considered as random elements in a space of squared integrable functions. Borrowing ideas from recent advances in the Monte Carlo integration literature, we propose effective unbiased estimation and inference procedures for integrals of uni- and multivariate random functions. Several applications to key problems in functional data analysis (FDA) involving random design points are studied and illustrated. In the absence of noise, the proposed estimates converge faster than the sample mean and the usual algorithms for numerical integration. Moreover, the proposed estimator facilitates effective inference by generally providing better coverage with shorter confidence and prediction intervals, in both noisy and noiseless setups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08533v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Valentin Patilea, Sunny G. W. Wang</dc:creator>
    </item>
    <item>
      <title>Identifiability of the instrumental variable model with the treatment and outcome missing not at random</title>
      <link>https://arxiv.org/abs/2412.08567</link>
      <description>arXiv:2412.08567v1 Announce Type: new 
Abstract: The instrumental variable model of Imbens and Angrist (1994) and Angrist et al. (1996) allow for the identification of the local average treatment effect, also known as the complier average causal effect. However, many empirical studies are challenged by the missingness in the treatment and outcome. Generally, the complier average causal effect is not identifiable without further assumptions when the treatment and outcome are missing not at random. We study its identifiability even when the treatment and outcome are missing not at random. We review the existing results and provide new findings to unify the identification analysis in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08567v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shuozhi Zuo, Peng Ding, Fan Yang</dc:creator>
    </item>
    <item>
      <title>Dirichlet-Neumann Averaging: The DNA of Efficient Gaussian Process Simulation</title>
      <link>https://arxiv.org/abs/2412.07929</link>
      <description>arXiv:2412.07929v1 Announce Type: cross 
Abstract: Gaussian processes (GPs) and Gaussian random fields (GRFs) are essential for modelling spatially varying stochastic phenomena. Yet, the efficient generation of corresponding realisations on high-resolution grids remains challenging, particularly when a large number of realisations are required. This paper presents two novel contributions. First, we propose a new methodology based on Dirichlet-Neumann averaging (DNA) to generate GPs and GRFs with isotropic covariance on regularly spaced grids. The combination of discrete cosine and sine transforms in the DNA sampling approach allows for rapid evaluations without the need for modification or padding of the desired covariance function. While this introduces an error in the covariance, our numerical experiments show that this error is negligible for most relevant applications, representing a trade-off between efficiency and precision. We provide explicit error estimates for Mat\'ern covariances. The second contribution links our new methodology to the stochastic partial differential equation (SPDE) approach for sampling GRFs. We demonstrate that the concepts developed in our methodology can also guide the selection of boundary conditions in the SPDE framework. We prove that averaging specific GRFs sampled via the SPDE approach yields genuinely isotropic realisations without domain extension, with the error bounds established in the first part remaining valid.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07929v1</guid>
      <category>stat.CO</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robert Kutri (Institute for Mathematics, Heidelberg, Interdisciplinary Center for Scientific Computing), Robert Scheichl (Institute for Mathematics, Heidelberg, Interdisciplinary Center for Scientific Computing)</dc:creator>
    </item>
    <item>
      <title>Statistical Convergence Rates of Optimal Transport Map Estimation between General Distributions</title>
      <link>https://arxiv.org/abs/2412.08064</link>
      <description>arXiv:2412.08064v1 Announce Type: cross 
Abstract: This paper studies the convergence rates of optimal transport (OT) map estimators, a topic of growing interest in statistics, machine learning, and various scientific fields. Despite recent advancements, existing results rely on regularity assumptions that are very restrictive in practice and much stricter than those in Brenier's Theorem, including the compactness and convexity of the probability support and the bi-Lipschitz property of the OT maps. We aim to broaden the scope of OT map estimation and fill this gap between theory and practice. Given the strong convexity assumption on Brenier's potential, we first establish the non-asymptotic convergence rates for the original plug-in estimator without requiring restrictive assumptions on probability measures. Additionally, we introduce a sieve plug-in estimator and establish its convergence rates without the strong convexity assumption on Brenier's potential, enabling the widely used cases such as the rank functions of normal or t-distributions. We also establish new Poincar\'e-type inequalities, which are proved given sufficient conditions on the local boundedness of the probability density and mild topological conditions of the support, and these new inequalities enable us to achieve faster convergence rates for the Donsker function class. Moreover, we develop scalable algorithms to efficiently solve the OT map estimation using neural networks and present numerical experiments to demonstrate the effectiveness and robustness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08064v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yizhe Ding, Runze Li, Lingzhou Xue</dc:creator>
    </item>
    <item>
      <title>Bayesian Analysis of Sigmoidal Gaussian Cox Processes via Data Augmentation</title>
      <link>https://arxiv.org/abs/2203.06743</link>
      <description>arXiv:2203.06743v3 Announce Type: replace 
Abstract: Many models for point process data are defined through a thinning procedure where locations of a base process (often Poisson) are either kept (observed) or discarded (thinned). In this paper, we go back to the fundamentals of the distribution theory for point processes to establish a link between the base thinning mechanism and the joint density of thinned and observed locations in any of such models. In practice, the marginal model of observed points is often intractable, but thinned locations can be instantiated from their conditional distribution and typical data augmentation schemes can be employed to circumvent this problem. Such approaches have been employed in the recent literature, but some inconsistencies have been introduced across the different publications. We concentrate on an example: the so-called sigmoidal Gaussian Cox process. We apply our approach to resolve contradicting viewpoints in the data augmentation step of the inference procedures therein. We also provide a multitype extension to this process and conduct Bayesian inference on data consisting of positions of two different species of trees in Lansing Woods, Michigan. The emphasis is put on intertype dependence modeling with Bayesian uncertainty quantification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.06743v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Renaud Alie, David A. Stephens, Alexandra M. Schmidt</dc:creator>
    </item>
    <item>
      <title>Fast and Optimal Inference for Change Points in Piecewise Polynomials via Differencing</title>
      <link>https://arxiv.org/abs/2307.03639</link>
      <description>arXiv:2307.03639v3 Announce Type: replace 
Abstract: We consider the problem of uncertainty quantification in change point regressions, where the signal can be piecewise polynomial of arbitrary but fixed degree. That is we seek disjoint intervals which, uniformly at a given confidence level, must each contain a change point location. We propose a procedure based on performing local tests at a number of scales and locations on a sparse grid, which adapts to the choice of grid in the sense that by choosing a sparser grid one explicitly pays a lower price for multiple testing. The procedure is fast as its computational complexity is always of the order $\mathcal{O} (n \log (n))$ where $n$ is the length of the data, and optimal in the sense that under certain mild conditions every change point is detected with high probability and the widths of the intervals returned match the mini-max localisation rates for the associated change point problem up to log factors. A detailed simulation study shows our procedure is competitive against state of the art algorithms for similar problems. Our procedure is implemented in the R package ChangePointInference which is available via https://github.com/gaviosha/ChangePointInference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.03639v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shakeel Gavioli-Akilagun, Piotr Fryzlewicz</dc:creator>
    </item>
    <item>
      <title>Causal thinking for decision making on Electronic Health Records: why and how</title>
      <link>https://arxiv.org/abs/2308.01605</link>
      <description>arXiv:2308.01605v4 Announce Type: replace 
Abstract: Accurate predictions, as with machine learning, may not suffice to provide optimal healthcare for every patient. Indeed, prediction can be driven by shortcuts in the data, such as racial biases. Causal thinking is needed for data-driven decisions. Here, we give an introduction to the key elements, focusing on routinely-collected data, electronic health records (EHRs) and claims data. Using such data to assess the value of an intervention requires care: temporal dependencies and existing practices easily confound the causal effect. We present a step-by-step framework to help build valid decision making from real-life patient records by emulating a randomized trial before individualizing decisions, eg with machine learning. Our framework highlights the most important pitfalls and considerations in analysing EHRs or claims data to draw causal conclusions. We illustrate the various choices in studying the effect of albumin on sepsis mortality in the Medical Information Mart for Intensive Care database (MIMIC-IV). We study the impact of various choices at every step, from feature extraction to causal-estimator selection. In a tutorial spirit, the code and the data are openly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.01605v4</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthieu Doutreligne (SODA), Tristan Struja (MIT, USZ), Judith Abecassis (SODA), Claire Morgand (ARS IDF), Leo Anthony Celi (MIT), Ga\"el Varoquaux (SODA)</dc:creator>
    </item>
    <item>
      <title>Methods for Quantifying Dataset Similarity: a Review, Taxonomy and Comparison</title>
      <link>https://arxiv.org/abs/2312.04078</link>
      <description>arXiv:2312.04078v2 Announce Type: replace 
Abstract: Quantifying the similarity between datasets has widespread applications in statistics and machine learning. The performance of a predictive model on novel datasets, referred to as generalizability, depends on how similar the training and evaluation datasets are. Exploiting or transferring insights between similar datasets is a key aspect of meta-learning and transfer-learning. In simulation studies, the similarity between distributions of simulated datasets and real datasets, for which the performance of methods is assessed, is crucial. In two- or $k$-sample testing, it is checked, whether the underlying distributions of two or more datasets coincide.
  Extremely many approaches for quantifying dataset similarity have been proposed in the literature. We examine more than 100 methods and provide a taxonomy, classifying them into ten classes. In an extensive review of these methods the main underlying ideas, formal definitions, and important properties are introduced.
  We compare the 118 methods in terms of their applicability, interpretability, and theoretical properties, in order to provide recommendations for selecting an appropriate dataset similarity measure based on the specific goal of the dataset comparison and on the properties of the datasets at hand. An online tool facilitates the choice of the appropriate dataset similarity measure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.04078v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1214/24-SS149</arxiv:DOI>
      <arxiv:journal_reference>Statist. Surv. 18, 163 - 298, 2024</arxiv:journal_reference>
      <dc:creator>Marieke Stolte, Franziska Kappenberg, J\"org Rahnenf\"uhrer, Andrea Bommert</dc:creator>
    </item>
    <item>
      <title>Sequential Monte-Carlo testing by betting</title>
      <link>https://arxiv.org/abs/2401.07365</link>
      <description>arXiv:2401.07365v5 Announce Type: replace 
Abstract: In a Monte-Carlo test, the observed dataset is fixed, and several resampled or permuted versions of the dataset are generated in order to test a null hypothesis that the original dataset is exchangeable with the resampled/permuted ones. Sequential Monte-Carlo tests aim to save computational resources by generating these additional datasets sequentially one by one, and potentially stopping early. While earlier tests yield valid inference at a particular prespecified stopping rule, our work develops a new anytime-valid Monte-Carlo test that can be continuously monitored, yielding a p-value or e-value at any stopping time possibly not specified in advance. It generalizes the well-known method by Besag and Clifford, allowing it to stop at any time, but also encompasses new sequential Monte-Carlo tests that tend to stop sooner under the null and alternative without compromising power. The core technical advance is the development of new test martingales (nonnegative martingales with initial value one) for testing exchangeability against a very particular alternative. These test martingales are constructed using new and simple betting strategies that smartly bet on whether a generated test statistic is greater or smaller than the observed one. The betting strategies are guided by the derivation of a simple log-optimal betting strategy, have closed form expressions for the wealth process, provable guarantees on resampling risk, and display excellent power in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.07365v5</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lasse Fischer, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Large Row-Constrained Supersaturated Designs for High-throughput Screening</title>
      <link>https://arxiv.org/abs/2407.06173</link>
      <description>arXiv:2407.06173v2 Announce Type: replace 
Abstract: High-throughput screening, in which multiwell plates are used to test large numbers of compounds against specific targets, is widely used across many areas of the biological sciences and most prominently in drug discovery. We propose a statistically principled approach to these screening experiments, using the machinery of supersaturated designs and the Lasso. To accommodate limitations on the number of biological entities that can be applied to a single microplate well, we present a new class of row-constrained supersaturated designs. We develop a computational procedure to construct these designs, provide some initial lower bounds on the average squared off-diagonal values of their main-effects information matrix, and study the impact of the constraint on design quality. We also show via simulation that the proposed constrained row screening method is statistically superior to existing methods and demonstrate the use of the new methodology on a real drug-discovery system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06173v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Byran J. Smucker, Stephen E. Wright, Isaac Williams, Richard C. Page, Andor J. Kiss, Surendra Bikram Silwal, Maria Weese, David J. Edwards</dc:creator>
    </item>
    <item>
      <title>Hypothesis tests and model parameter estimation on data sets with missing correlation information</title>
      <link>https://arxiv.org/abs/2410.22333</link>
      <description>arXiv:2410.22333v2 Announce Type: replace 
Abstract: Ideally, all analyses of normally distributed data should include the full covariance information between all data points. In practice, the full covariance matrix between all data points is not always available. Either because a result was published without a covariance matrix, or because one tries to combine multiple results from separate publications. For simple hypothesis tests, it is possible to define robust test statistics that will behave conservatively in the presence on unknown correlations. For model parameter fits, one can inflate the variance by a factor to ensure that things remain conservative at least up to a chosen confidence level. This paper describes a class of robust test statistics for simple hypothesis tests, as well as an algorithm to determine the necessary inflation factor for model parameter fits and Goodness of Fit tests and composite hypothesis tests. It then presents some example applications of the methods to real neutrino interaction data and model comparisons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22333v2</guid>
      <category>stat.ME</category>
      <category>hep-ph</category>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lukas Koch</dc:creator>
    </item>
    <item>
      <title>Covariate Adjustment in Randomized Experiments Motivated by Higher-Order Influence Functions</title>
      <link>https://arxiv.org/abs/2411.08491</link>
      <description>arXiv:2411.08491v2 Announce Type: replace 
Abstract: Higher-Order Influence Functions (HOIF), developed in a series of papers over the past twenty years, is a fundamental theoretical device for constructing rate-optimal causal-effect estimators from observational studies. However, the value of HOIF for analyzing well-conducted randomized controlled trials (RCTs) has not been explicitly explored. In the recent U.S. Food and Drug Administration (FDA) and European Medicines Agency (EMA) guidelines on the practice of covariate adjustment in analyzing RCTs, in addition to the simple, unadjusted difference-in-mean estimator, it was also recommended to report the estimator adjusting for baseline covariates via a simple parametric working model, such as a linear model. In this paper, we show that a HOIF-motivated estimator for the treatment-specific mean has significantly improved statistical properties compared to popular adjusted estimators in practice when the number of baseline covariates $p$ is relatively large compared to the sample size $n$. We also characterize the conditions under which the HOIF-motivated estimator improves upon the unadjusted one. Furthermore, we demonstrate that a novel debiased adjusted estimator proposed recently by Lu et al. is, in fact, another HOIF-motivated estimator in disguise. Numerical and empirical studies are conducted to corroborate our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08491v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sihui Zhao, Xinbo Wang, Lin Liu, Xin Zhang</dc:creator>
    </item>
    <item>
      <title>Application of generalized linear models in big data: a divide and recombine (D&amp;R) approach</title>
      <link>https://arxiv.org/abs/2412.05018</link>
      <description>arXiv:2412.05018v2 Announce Type: replace 
Abstract: D&amp;R is a statistical approach designed to handle large and complex datasets. It partitions the dataset into several manageable subsets and subsequently applies the analytic method to each subset independently to obtain results. Finally, the results from each subset are combined to yield the results for the entire dataset. D&amp;R strategies can be implemented to fit GLMs to datasets too large for conventional methods. Several D&amp;R strategies are available for different GLMs, some of which are theoretically justified but lack practical validation. A significant limitation is the theoretical and practical justification for estimating combined standard errors and confidence intervals. This paper reviews D&amp;R strategies for GLMs and proposes a method to determine the combined standard error for D&amp;R-based estimators. In addition to the traditional dataset division procedures, we propose a different division method named sequential partitioning for D&amp;R-based estimators on GLMs. We show that the obtained D&amp;R estimator with the proposed standard error attains equivalent efficiency as the full data estimate. We illustrate this on a large synthetic dataset and verify that the results from D&amp;R are accurate and identical to those from other available R packages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05018v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Md. Mahadi Hassan Nayem, Soma Chowdhury Biswas</dc:creator>
    </item>
    <item>
      <title>Energy Based Equality of Distributions Testing for Compositional Data</title>
      <link>https://arxiv.org/abs/2412.05199</link>
      <description>arXiv:2412.05199v2 Announce Type: replace 
Abstract: Not many tests exist for testing the equality for two or more multivariate distributions with compositional data, perhaps due to their constrained sample space. At the moment, there is only one test suggested that relies upon random projections. We propose a novel test termed {\alpha}-Energy Based Test ({\alpha}-EBT) to compare the multivariate distributions of two (or more) compositional data sets. Similar to the aforementioned test, the new test makes no parametric assumptions about the data and, based on simulation studies it exhibits higher power levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05199v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Volkan Sevinc, Michail Tsagris</dc:creator>
    </item>
    <item>
      <title>Robust Estimation and Inference in Panels with Interactive Fixed Effects</title>
      <link>https://arxiv.org/abs/2210.06639</link>
      <description>arXiv:2210.06639v3 Announce Type: replace-cross 
Abstract: We consider estimation and inference for a regression coefficient in panels with interactive fixed effects (i.e., with a factor structure). We demonstrate that existing estimators and confidence intervals (CIs) can be heavily biased and size-distorted when some of the factors are weak. We propose estimators with improved rates of convergence and bias-aware CIs that remain valid uniformly, regardless of factor strength. Our approach applies the theory of minimax linear estimation to form a debiased estimate, using a nuclear norm bound on the error of an initial estimate of the interactive fixed effects. Our resulting bias-aware CIs take into account the remaining bias caused by weak factors. Monte Carlo experiments show substantial improvements over conventional methods when factors are weak, with minimal costs to estimation accuracy when factors are strong.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.06639v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Timothy B. Armstrong, Martin Weidner, Andrei Zeleneev</dc:creator>
    </item>
    <item>
      <title>Empirical Bayes When Estimation Precision Predicts Parameters</title>
      <link>https://arxiv.org/abs/2212.14444</link>
      <description>arXiv:2212.14444v5 Announce Type: replace-cross 
Abstract: Gaussian empirical Bayes methods usually maintain a precision independence assumption: The unknown parameters of interest are independent from the known standard errors of the estimates. This assumption is often theoretically questionable and empirically rejected. This paper proposes to model the conditional distribution of the parameter given the standard errors as a flexibly parametrized location-scale family of distributions, leading to a family of methods that we call CLOSE. The CLOSE framework unifies and generalizes several proposals under precision dependence. We argue that the most flexible member of the CLOSE family is a minimalist and computationally efficient default for accounting for precision dependence. We analyze this method and show that it is competitive in terms of the regret of subsequent decisions rules. Empirically, using CLOSE leads to sizable gains for selecting high-mobility Census tracts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.14444v5</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiafeng Chen</dc:creator>
    </item>
    <item>
      <title>Length Optimization in Conformal Prediction</title>
      <link>https://arxiv.org/abs/2406.18814</link>
      <description>arXiv:2406.18814v3 Announce Type: replace-cross 
Abstract: Conditional validity and length efficiency are two crucial aspects of conformal prediction (CP). Conditional validity ensures accurate uncertainty quantification for data subpopulations, while proper length efficiency ensures that the prediction sets remain informative. Despite significant efforts to address each of these issues individually, a principled framework that reconciles these two objectives has been missing in the CP literature. In this paper, we develop Conformal Prediction with Length-Optimization (CPL) - a novel and practical framework that constructs prediction sets with (near-) optimal length while ensuring conditional validity under various classes of covariate shifts, including the key cases of marginal and group-conditional coverage. In the infinite sample regime, we provide strong duality results which indicate that CPL achieves conditional validity and length optimality. In the finite sample regime, we show that CPL constructs conditionally valid prediction sets. Our extensive empirical evaluations demonstrate the superior prediction set size performance of CPL compared to state-of-the-art methods across diverse real-world and synthetic datasets in classification, regression, and large language model-based multiple choice question answering. An Implementation of our algorithm can be accessed at the following link: https://github.com/shayankiyani98/CP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18814v3</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shayan Kiyani, George Pappas, Hamed Hassani</dc:creator>
    </item>
    <item>
      <title>Practical Performative Policy Learning with Strategic Agents</title>
      <link>https://arxiv.org/abs/2412.01344</link>
      <description>arXiv:2412.01344v2 Announce Type: replace-cross 
Abstract: This paper studies the performative policy learning problem, where agents adjust their features in response to a released policy to improve their potential outcomes, inducing an endogenous distribution shift. There has been growing interest in training machine learning models in strategic environments, including strategic classification and performative prediction. However, existing approaches often rely on restrictive parametric assumptions: micro-level utility models in strategic classification and macro-level data distribution maps in performative prediction, severely limiting scalability and generalizability. We approach this problem as a complex causal inference task, relaxing parametric assumptions on both micro-level agent behavior and macro-level data distribution. Leveraging bounded rationality, we uncover a practical low-dimensional structure in distribution shifts and construct an effective mediator in the causal path from the deployed model to the shifted data. We then propose a gradient-based policy optimization algorithm with a differentiable classifier as a substitute for the high-dimensional distribution map. Our algorithm efficiently utilizes batch feedback and limited manipulation patterns. Our approach achieves high sample efficiency compared to methods reliant on bandit feedback or zero-order optimization. We also provide theoretical guarantees for algorithmic convergence. Extensive and challenging experiments on high-dimensional settings demonstrate our method's practical efficacy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01344v2</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 12 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qianyi Chen, Ying Chen, Bo Li</dc:creator>
    </item>
  </channel>
</rss>
