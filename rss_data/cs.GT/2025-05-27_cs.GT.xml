<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 28 May 2025 01:54:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Efficient Algorithms for Electing Successive Committees</title>
      <link>https://arxiv.org/abs/2505.18287</link>
      <description>arXiv:2505.18287v1 Announce Type: new 
Abstract: In a recently introduced model of successive committee elections (Bredereck et al., AAAI-20) for a given set of ordinal or approval preferences one aims to find a sequence of a given length of "best" same-size committees such that each candidate is a member of a limited number of consecutive committees. However, the practical usability of this model remains limited, as the described task turns out to be NP-hard for most selection criteria already for seeking committees of size three. Non-trivial or somewhat efficient algorithms for these cases are lacking too. Motivated by a desire to unlock the full potential of the described temporal model of committee elections, we devise (parameterized) algorithms that effectively solve the mentioned hard cases in realistic scenarios of a moderate number of candidates or of a limited time horizon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18287v1</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pallavi Jain, Andrzej Kaczmarczyk</dc:creator>
    </item>
    <item>
      <title>Incentivizing High-Quality Human Annotations with Golden Questions</title>
      <link>https://arxiv.org/abs/2505.19134</link>
      <description>arXiv:2505.19134v1 Announce Type: new 
Abstract: Human-annotated data plays a vital role in training large language models (LLMs), such as supervised fine-tuning and human preference alignment. However, it is not guaranteed that paid human annotators produce high-quality data. In this paper, we study how to incentivize human annotators to do so. We start from a principal-agent model to model the dynamics between the company (the principal) and the annotator (the agent), where the principal can only monitor the annotation quality by examining $n$ samples. We investigate the maximum likelihood estimators (MLE) and the corresponding hypothesis testing to incentivize annotators: the agent is given a bonus if the MLE passes the test. By analyzing the variance of the outcome, we show that the strategic behavior of the agent makes the hypothesis testing very different from traditional ones: Unlike the exponential rate proved by the large deviation theory, the principal-agent model's hypothesis testing rate is of $\Theta(1/\sqrt{n \log n})$. Our theory implies two criteria for the \emph{golden questions} to monitor the performance of the annotators: they should be of (1) high certainty and (2) similar format to normal ones. In that light, we select a set of golden questions in human preference data. By doing incentive-compatible experiments, we find out that the annotators' behavior is better revealed by those golden questions, compared to traditional survey techniques such as instructed manipulation checks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19134v1</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shang Liu, Zhongze Cai, Hanzhao Wang, Zhongyao Ma, Xiaocheng Li</dc:creator>
    </item>
    <item>
      <title>Market Clearing with Semi-fungible Assets</title>
      <link>https://arxiv.org/abs/2505.19298</link>
      <description>arXiv:2505.19298v1 Announce Type: new 
Abstract: As markets have digitized, the number of tradable products has skyrocketed. Algorithmically constructed portfolios of these assets now dominate public and private markets, resulting in a combinatorial explosion of tradable assets. In this paper, we provide a simple means to compute market clearing prices for semi-fungible assets which have a partial ordering between them. Such assets are increasingly found in traditional markets (bonds, commodities, ETFs), private markets (private credit, compute markets), and in decentralized finance. We formulate the market clearing problem as an optimization problem over a directed acyclic graph that represents participant preferences. Subsequently, we use convex duality to efficiently estimate market clearing prices, which correspond to particular dual variables. We then describe dominant strategy incentive compatible payment and allocation rules for clearing these markets. We conclude with examples of how this framework can construct prices for a variety of algorithmically constructed, semi-fungible portfolios of practical importance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19298v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Theo Diamandis, Tarun Chitra, Guillermo Angeris</dc:creator>
    </item>
    <item>
      <title>Co-evolutionary Dynamics of Attack and Defence in Cybersecurity</title>
      <link>https://arxiv.org/abs/2505.19338</link>
      <description>arXiv:2505.19338v1 Announce Type: new 
Abstract: In the evolving digital landscape, it is crucial to study the dynamics of cyberattacks and defences. This study uses an Evolutionary Game Theory (EGT) framework to investigate the evolutionary dynamics of attacks and defences in cyberspace. We develop a two-population asymmetric game between attacker and defender to capture the essential factors of costs, potential benefits, and the probability of successful defences. Through mathematical analysis and numerical simulations, we find that systems with high defence intensities show stability with minimal attack frequencies, whereas low-defence environments show instability, and are vulnerable to attacks. Furthermore, we find five equilibria, where the strategy pair always defend and attack emerged as the most likely stable state as cyber domain is characterised by a continuous battle between defenders and attackers. Our theoretical findings align with real-world data from past cyber incidents, demonstrating the interdisciplinary impact, such as fraud detection, risk management and cybersecurity decision-making. Overall, our analysis suggests that adaptive cybersecurity strategies based on EGT can improve resource allocation, enhance system resilience, and reduce the overall risk of cyberattacks. By incorporating real-world data, this study demonstrates the applicability of EGT in addressing the evolving nature of cyber threats and the need for secure digital ecosystems through strategic planning and proactive defence measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19338v1</guid>
      <category>cs.GT</category>
      <category>cs.CR</category>
      <category>nlin.AO</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adeela Bashir, Zia Ush Shamszaman, Zhao Song, The Anh Han</dc:creator>
    </item>
    <item>
      <title>Approximately Optimal Mechanism Design for Competing Sellers</title>
      <link>https://arxiv.org/abs/2505.19453</link>
      <description>arXiv:2505.19453v1 Announce Type: new 
Abstract: Two sellers compete to sell identical products to a single buyer. Each seller chooses an arbitrary mechanism, possibly involving lotteries, to sell their product. The utility-maximizing buyer can choose to participate in one or both mechanisms, resolving them in either order. Given a common prior over buyer values, how should the sellers design their mechanisms to maximize their respective revenues?
  We first consider a Stackelberg setting where one seller (Alice) commits to her mechanism and the other seller (Bob) best-responds. We show how to construct a simple and approximately-optimal single-lottery mechanism for Alice that guarantees her a quarter of the optimal monopolist's revenue, for any regular distribution. Along the way we prove a structural result: for any single-lottery mechanism of Alice, there will always be a best response mechanism for Bob consisting of a single take-it-or-leave-it price. We also show that no mechanism (single-lottery or otherwise) can guarantee Alice more than a 1/e fraction of the monopolist revenue. Finally, we show that our approximation result does not extend to Nash equilibrium: there exist instances in which a monopolist could extract full surplus, but neither competing seller obtains positive revenue at any equilibrium choice of mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19453v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brendan Lucier, Raghuvansh R. Saxena</dc:creator>
    </item>
    <item>
      <title>Continuous-Time Analysis of Heavy Ball Momentum in Min-Max Games</title>
      <link>https://arxiv.org/abs/2505.19537</link>
      <description>arXiv:2505.19537v1 Announce Type: new 
Abstract: Since Polyak's pioneering work, heavy ball (HB) momentum has been widely studied in minimization. However, its role in min-max games remains largely unexplored. As a key component of practical min-max algorithms like Adam, this gap limits their effectiveness. In this paper, we present a continuous-time analysis for HB with simultaneous and alternating update schemes in min-max games. Locally, we prove smaller momentum enhances algorithmic stability by enabling local convergence across a wider range of step sizes, with alternating updates generally converging faster. Globally, we study the implicit regularization of HB, and find smaller momentum guides algorithms trajectories towards shallower slope regions of the loss landscapes, with alternating updates amplifying this effect. Surprisingly, all these phenomena differ from those observed in minimization, where larger momentum yields similar effects. Our results reveal fundamental differences between HB in min-max games and minimization, and numerical experiments further validate our theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19537v1</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yi Feng, Kaito Fujii, Stratis Skoulakis, Xiao Wang, Volkan Cevher</dc:creator>
    </item>
    <item>
      <title>A Framework for Combined Transaction Posting and Pricing for Layer 2 Blockchains</title>
      <link>https://arxiv.org/abs/2505.19556</link>
      <description>arXiv:2505.19556v1 Announce Type: new 
Abstract: This paper presents a comprehensive framework for transaction posting and pricing in Layer 2 (L2) blockchain systems, focusing on challenges stemming from fluctuating Layer 1 (L1) gas fees and the congestion issues within L2 networks. Existing methods have focused on the problem of optimal posting strategies to L1 in isolation, without simultaneously considering the L2 fee mechanism. In contrast, our work offers a unified approach that addresses the complex interplay between transaction queue dynamics, L1 cost variability, and user responses to L2 fees. We contribute by (1) formulating a dynamic model that integrates both posting and pricing strategies, capturing the interplay between L1 gas price fluctuations and L2 queue management, (2) deriving an optimal threshold-based posting policy that guides L2 sequencers in managing transactions based on queue length and current L1 conditions, and (3) establishing theoretical foundations for a dynamic L2 fee mechanism that balances cost recovery with congestion control. We validate our framework through simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19556v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shouqiao Wang, Davide Crapis, Ciamac C. Moallemi</dc:creator>
    </item>
    <item>
      <title>The residual maximin share</title>
      <link>https://arxiv.org/abs/2505.19961</link>
      <description>arXiv:2505.19961v1 Announce Type: new 
Abstract: We consider fair allocations of indivisible goods to agents with general monotone valuations. We observe that it is useful to introduce a new share-based fairness notion, the {\em residual maximin share} (RMMS). This share is {\em feasible} and {\em self maximizing}. Its value is at least as large as the MXS, and at least as large as $\frac{2}{3}$-MMS for additive valuations. Known techniques easily imply the existence of partial allocations that are both RMMS and EFX, and complete allocations that are both RMMS and EFL. This unifies and somewhat improves upon several different results from previous papers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19961v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Uriel Feige</dc:creator>
    </item>
    <item>
      <title>An AI Capability Threshold for Rent-Funded Universal Basic Income in an AI-Automated Economy</title>
      <link>https://arxiv.org/abs/2505.18687</link>
      <description>arXiv:2505.18687v1 Announce Type: cross 
Abstract: We derive the first closed-form condition under which artificial intelligence (AI) capital profits could sustainably finance a universal basic income (UBI) without additional taxes or new job creation. In a Solow-Zeira economy characterized by a continuum of automatable tasks, a constant net saving rate $s$, and task-elasticity $\sigma &lt; 1$, we analyze how the AI capability threshold--defined as the productivity level of AI relative to pre-AI automation--varies under different economic scenarios. At present economic parameters, we find that AI systems must achieve only approximately 5-6 times existing automation productivity to finance an 11\%-of-GDP UBI, in the worst case situation where \emph{no} new jobs or tasks are created.
  Our analysis also reveals some specific policy levers: raising public revenue share (e.g. profit taxation) of AI capital from the current 15\% to about 33\% halves the required AI capability threshold to attain UBI to 3 times existing automotion productivity, but gains diminish beyond 50\% public revenue share, especially if regulatory costs increase. Market structure also strongly affects outcomes: monopolistic or concentrated oligopolistic markets reduce the threshold by increasing economic rents, whereas heightened competition significantly raises it.
  Overall, these results suggest a couple policy recommendations: maximizing public revenue share up to a point so that operating costs are minimized, and strategically managing market competition can ensure AI's growing capabilities translate into meaningful social benefits within realistic technological progress scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18687v1</guid>
      <category>econ.GN</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aran Nayebi</dc:creator>
    </item>
    <item>
      <title>Improved Regret and Contextual Linear Extension for Pandora's Box and Prophet Inequality</title>
      <link>https://arxiv.org/abs/2505.18828</link>
      <description>arXiv:2505.18828v1 Announce Type: cross 
Abstract: We study the Pandora's Box problem in an online learning setting with semi-bandit feedback. In each round, the learner sequentially pays to open up to $n$ boxes with unknown reward distributions, observes rewards upon opening, and decides when to stop. The utility of the learner is the maximum observed reward minus the cumulative cost of opened boxes, and the goal is to minimize regret defined as the gap between the cumulative expected utility and that of the optimal policy. We propose a new algorithm that achieves $\widetilde{O}(\sqrt{nT})$ regret after $T$ rounds, which improves the $\widetilde{O}(n\sqrt{T})$ bound of Agarwal et al. [2024] and matches the known lower bound up to logarithmic factors. To better capture real-life applications, we then extend our results to a natural but challenging contextual linear setting, where each box's expected reward is linear in some known but time-varying $d$-dimensional context and the noise distribution is fixed over time. We design an algorithm that learns both the linear function and the noise distributions, achieving $\widetilde{O}(nd\sqrt{T})$ regret. Finally, we show that our techniques also apply to the online Prophet Inequality problem, where the learner must decide immediately whether or not to accept a revealed reward. In both non-contextual and contextual settings, our approach achieves similar improvements and regret bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18828v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junyan Liu, Ziyun Chen, Kun Wang, Haipeng Luo, Lillian J. Ratliff</dc:creator>
    </item>
    <item>
      <title>Eliciting Informed Preferences</title>
      <link>https://arxiv.org/abs/2505.19570</link>
      <description>arXiv:2505.19570v1 Announce Type: cross 
Abstract: If people find it costly to evaluate the options available to them, their choices may not directly reveal their preferences. Yet, it is conceivable that a researcher can still learn about a population's preferences with careful experiment design. We formalize the researcher's problem in a model of robust mechanism design where it is costly for individuals to learn about how much they value a product. We characterize the statistics that the researcher can identify, and find that they are quite restricted. Finally, we apply our positive results to social choice and propose a way to combat uninformed voting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19570v1</guid>
      <category>econ.TH</category>
      <category>cs.GT</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Modibo K. Camara, Nicole Immorlica, Brendan Lucier</dc:creator>
    </item>
    <item>
      <title>Multi-Agent Reinforcement Learning in Cybersecurity: From Fundamentals to Applications</title>
      <link>https://arxiv.org/abs/2505.19837</link>
      <description>arXiv:2505.19837v1 Announce Type: cross 
Abstract: Multi-Agent Reinforcement Learning (MARL) has shown great potential as an adaptive solution for addressing modern cybersecurity challenges. MARL enables decentralized, adaptive, and collaborative defense strategies and provides an automated mechanism to combat dynamic, coordinated, and sophisticated threats. This survey investigates the current state of research in MARL applications for automated cyber defense (ACD), focusing on intruder detection and lateral movement containment. Additionally, it examines the role of Autonomous Intelligent Cyber-defense Agents (AICA) and Cyber Gyms in training and validating MARL agents. Finally, the paper outlines existing challenges, such as scalability and adversarial robustness, and proposes future research directions. This also discusses how MARL integrates in AICA to provide adaptive, scalable, and dynamic solutions to counter the increasingly sophisticated landscape of cyber threats. It highlights the transformative potential of MARL in areas like intrusion detection and lateral movement containment, and underscores the value of Cyber Gyms for training and validation of AICA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19837v1</guid>
      <category>cs.MA</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christoph R. Landolt, Christoph W\"ursch, Roland Meier, Alain Mermoud, Julian Jang-Jaccard</dc:creator>
    </item>
    <item>
      <title>The Limits of Preference Data for Post-Training</title>
      <link>https://arxiv.org/abs/2505.19964</link>
      <description>arXiv:2505.19964v1 Announce Type: cross 
Abstract: Recent progress in strengthening the capabilities of large language models has stemmed from applying reinforcement learning to domains with automatically verifiable outcomes. A key question is whether we can similarly use RL to optimize for outcomes in domains where evaluating outcomes inherently requires human feedback; for example, in tasks like deep research and trip planning, outcome evaluation is qualitative and there are many possible degrees of success. One attractive and scalable modality for collecting human feedback is preference data: ordinal rankings (pairwise or $k$-wise) that indicate, for $k$ given outcomes, which one is preferred. In this work, we study a critical roadblock: preference data fundamentally and significantly limits outcome-based optimization. Even with idealized preference data (infinite, noiseless, and online), the use of ordinal feedback can prevent obtaining even approximately optimal solutions. We formalize this impossibility using voting theory, drawing an analogy between how a model chooses to answer a query with how voters choose a candidate to elect. This indicates that grounded human scoring and algorithmic innovations are necessary for extending the success of RL post-training to domains demanding human feedback. We also explore why these limitations have disproportionately impacted RLHF when it comes to eliciting reasoning behaviors (e.g., backtracking) versus situations where RLHF has been historically successful (e.g., instruction-tuning and safety training), finding that the limitations of preference data primarily suppress RLHF's ability to elicit robust strategies -- a class that encompasses most reasoning behaviors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19964v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.GT</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eric Zhao, Jessica Dai, Pranjal Awasthi</dc:creator>
    </item>
    <item>
      <title>Optimizing Contracts in Principal-Agent Team Production</title>
      <link>https://arxiv.org/abs/2405.20631</link>
      <description>arXiv:2405.20631v2 Announce Type: replace 
Abstract: We study a principal-agent team production model. The principal hires a team of agents to participate in a common production task. The exact effort of each agent is unobservable and unverifiable, but the total production outcome (e.g. the total revenue) can be observed. The principal incentivizes the agents to exert effort through contracts. Specifically, the principal promises that each agent receives a pre-specified amount of share of the total production output. The principal is interested in finding the optimal profit-sharing rule that maximizes her own utility. We identify a condition under which the principal's optimization problem can be reformulated as solving a family of convex programs, thereby showing the optimal contract can be found efficiently.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20631v2</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shiliang Zuo</dc:creator>
    </item>
    <item>
      <title>Learning in Games with Progressive Hiding</title>
      <link>https://arxiv.org/abs/2409.03875</link>
      <description>arXiv:2409.03875v3 Announce Type: replace 
Abstract: When learning to play an imperfect information game, it is often easier to first start with the basic mechanics of the game rules. For example, one can play several example rounds with private cards revealed to all players to better understand the basic actions and their effects. Building on this intuition, this paper introduces {\it progressive hiding}, an algorithm that balances learning the basic mechanics of an imperfect information game and satisfying the information constraints. Progressive hiding is inspired by methods from stochastic multistage optimization, such as scenario decomposition and progressive hedging. We prove that it enables the adaptation of counterfactual regret minimization to games where perfect recall is not satisfied. Numerical experiments illustrate that progressive hiding produces notable improvements in several settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03875v3</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Heymann, Marc Lanctot</dc:creator>
    </item>
    <item>
      <title>Equilibria under Dynamic Benchmark Consistency in Non-Stationary Multi-Agent Systems</title>
      <link>https://arxiv.org/abs/2501.11897</link>
      <description>arXiv:2501.11897v2 Announce Type: replace 
Abstract: We formulate and study a general time-varying multi-agent system where players repeatedly compete under incomplete information. Our work is motivated by scenarios commonly observed in online advertising and retail marketplaces, where agents and platform designers optimize algorithmic decision-making in dynamic competitive settings. In these systems, no-regret algorithms that provide guarantees relative to \emph{static} benchmarks can perform poorly and the distributions of play that emerge from their interaction do not correspond anymore to static solution concepts such as coarse correlated equilibria. Instead, we analyze the interaction of \textit{dynamic benchmark} consistent policies that have performance guarantees relative to \emph{dynamic} sequences of actions, and through a novel \textit{tracking error} notion we delineate when their empirical joint distribution of play can approximate an evolving sequence of static equilibria. In systems that change sufficiently slowly (sub-linearly in the horizon length), we show that the resulting distributions of play approximate the sequence of coarse correlated equilibria, and apply this result to establish improved welfare bounds for smooth games. On a similar vein, we formulate internal dynamic benchmark consistent policies and establish that they approximate sequences of correlated equilibria. Our findings therefore suggest that in a broad range of multi-agent systems where non-stationarity is prevalent, algorithms designed to compete with dynamic benchmarks can improve both individual and welfare guarantees, and their emerging dynamics approximate a sequence of static equilibrium outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11897v2</guid>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ludovico Crippa, Yonatan Gur, Bar Light</dc:creator>
    </item>
    <item>
      <title>Perpetual Demand Lending Pools</title>
      <link>https://arxiv.org/abs/2502.06028</link>
      <description>arXiv:2502.06028v2 Announce Type: replace 
Abstract: Decentralized perpetuals protocols have collectively reached billions of dollars of daily trading volume, yet are still not serious competitors on the basis of trading volume with centralized venues such as Binance. One of the main reasons for this is the high cost of capital for market makers and sophisticated traders in decentralized settings. Recently, numerous decentralized finance protocols have been used to improve borrowing costs for perpetual futures traders. We formalize this class of mechanisms utilized by protocols such as Jupiter, Hyperliquid, and GMX, which we term~\emph{Perpetual Demand Lending Pools} (PDLPs). We then formalize a general target weight mechanism that generalizes what GMX and Jupiter are using in practice. We explicitly describe pool arbitrage and expected payoffs for arbitrageurs and liquidity providers within these mechanisms. Using this framework, we show that under general conditions, PDLPs are easy to delta hedge, partially explaining the proliferation of live hedged PDLP strategies. Our results suggest directions to improve capital efficiency in PDLPs via dynamic parametrization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06028v2</guid>
      <category>cs.GT</category>
      <category>q-fin.PM</category>
      <category>q-fin.RM</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tarun Chitra, Theo Diamandis, Nathan Sheng, Luke Sterle, Kamil Yusubov</dc:creator>
    </item>
    <item>
      <title>Incentivizing Truthful Data Contributions in a Marketplace for Mean Estimation</title>
      <link>https://arxiv.org/abs/2502.16052</link>
      <description>arXiv:2502.16052v2 Announce Type: replace 
Abstract: We study a data marketplace where a broker intermediates between buyers, who seek to estimate the mean \(\mu\) of an unknown normal distribution \(\Ncal(\mu, \sigma^2)\), and contributors, who can collect data from this distribution at a cost. The broker delegates data collection work to contributors, aggregates reported datasets, sells it to buyers, and redistributes revenue as payments to contributors. We aim to maximize welfare or profit under key constraints: individual rationality for buyers and contributors, incentive compatibility (contributors are incentivized to comply with data collection instructions and truthfully report the collected data), and budget balance (total contributor payments equals total revenue). We first compute welfare/profit-optimal prices under truthful reporting; however, to incentivize data collection and truthful data reporting, we adjust them based on discrepancies in contributors' reported data. This yields a Nash equilibrium (NE) where the two lowest-cost contributors collect all data. We complement this with two hardness results: \emph{(i)} no nontrivial dominant-strategy incentive-compatible mechanism exists in this problem, and \emph{(ii)} no mechanism outperforms ours in a NE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16052v2</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keran Chen, Alexander Clinton, Kirthevasan Kandasamy</dc:creator>
    </item>
    <item>
      <title>Online Resource Sharing: Better Robust Guarantees via Randomized Strategies</title>
      <link>https://arxiv.org/abs/2505.13824</link>
      <description>arXiv:2505.13824v2 Announce Type: replace 
Abstract: We study the problem of fair online resource allocation via non-monetary mechanisms, where multiple agents repeatedly share a resource without monetary transfers. Previous work has shown that every agent can guarantee $1/2$ of their ideal utility (the highest achievable utility given their fair share of resources) robustly, i.e., under arbitrary behavior by the other agents. While this $1/2$-robustness guarantee has now been established under very different mechanisms, including pseudo-markets and dynamic max-min allocation, improving on it has appeared difficult.
  In this work, we obtain the first significant improvement on the robustness of online resource sharing. In more detail, we consider the widely-studied repeated first-price auction with artificial currencies. Our main contribution is to show that a simple randomized bidding strategy can guarantee each agent a $2 - \sqrt 2 \approx 0.59$ fraction of her ideal utility, irrespective of others' bids. Specifically, our strategy requires each agent with fair share $\alpha$ to use a uniformly distributed bid whenever her value is in the top $\alpha$-quantile of her value distribution. Our work almost closes the gap to the known $1 - 1/e \approx 0.63$ hardness for robust resource sharing; we also show that any static (i.e., budget independent) bidding policy cannot guarantee more than a $0.6$-fraction of the ideal utility, showing our technique is almost tight.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13824v2</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David X. Lin, Daniel Hall, Giannis Fikioris, Siddhartha Banerjee, \'Eva Tardos</dc:creator>
    </item>
    <item>
      <title>Transaction Fee Mechanism Design for Leaderless Blockchain Protocols</title>
      <link>https://arxiv.org/abs/2505.17885</link>
      <description>arXiv:2505.17885v2 Announce Type: replace 
Abstract: We initiate the study of transaction fee mechanism design for blockchain protocols in which multiple block producers contribute to the production of each block. Our contributions include:
  - We propose an extensive-form (multi-stage) game model to reason about the game theory of multi-proposer transaction fee mechanisms.
  - We define the strongly BPIC property to capture the idea that all block producers should be motivated to behave as intended: for every user bid profile, following the intended allocation rule is a Nash equilibrium for block producers that Pareto dominates all other Nash equilibria.
  - We propose the first-price auction with equal sharing (FPA-EQ) mechanism as an attractive solution to the multi-proposer transaction fee mechanism design problem. We prove that the mechanism is strongly BPIC and guarantees at least a 63.2% fraction of the maximum-possible expected welfare at equilibrium.
  - We prove that the compromises made by the FPA-EQ mechanism are qualitatively necessary: no strongly BPIC mechanism with non-trivial welfare guarantees can be DSIC, and no strongly BPIC mechanism can guarantee optimal welfare at equilibrium.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17885v2</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pranav Garimidi, Lioba Heimbach, Tim Roughgarden</dc:creator>
    </item>
    <item>
      <title>On the Limitations and Possibilities of Nash Regret Minimization in Zero-Sum Matrix Games under Noisy Feedback</title>
      <link>https://arxiv.org/abs/2306.13233</link>
      <description>arXiv:2306.13233v3 Announce Type: replace-cross 
Abstract: This paper studies a variant of two-player zero-sum matrix games, where, at each timestep, the row player selects row $i$, the column player selects column $j$, and the row player receives a noisy reward with expected value $A_{i,j}$, along with noisy feedback on the input matrix $A$. The row player's goal is to maximize their total reward against an adversarial column player. Nash regret, defined as the difference between the player's total reward and the game's Nash equilibrium value scaled by the time horizon $T$, is often used to evaluate algorithmic performance in zero-sum games.
  We begin by studying the limitations of existing algorithms for minimizing Nash regret. We show that standard algorithm--including Hedge, FTRL, and OMD--as well as the strategy of playing the Nash equilibrium of the empirical matrix--all incur $\Omega(\sqrt{T})$ Nash regret, even when the row player receives noisy feedback on the entire matrix $A$. Furthermore, we show that UCB for matrix games, a natural adaptation of the well-known bandit algorithm, also suffers $\Omega(\sqrt{T})$ Nash regret under bandit feedback. Notably, these lower bounds hold even in the simplest case of $2 \times 2$ matrix games, where the instance-dependent matrix parameters are constant.
  We next ask whether instance-dependent $\text{polylog}(T)$ Nash regret is achievable against adversarial opponents. We answer this affirmatively. In the full-information setting, we present the first algorithm for general $n \times m$ matrix games that achieves instance-dependent $\text{polylog}(T)$ Nash regret. In the bandit feedback setting, we design an algorithm with similar guarantees for the special case of $2 \times 2$ game--the same regime in which existing algorithms provably suffer $\Omega(\sqrt{T})$ regret despite the simplicity of the instance. Finally, we validate our theoretical results with empirical evidence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.13233v3</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arnab Maiti, Kevin Jamieson, Lillian J. Ratliff</dc:creator>
    </item>
    <item>
      <title>An Adversarial Analysis of Thompson Sampling for Full-information Online Learning: from Finite to Infinite Action Spaces</title>
      <link>https://arxiv.org/abs/2502.14790</link>
      <description>arXiv:2502.14790v4 Announce Type: replace-cross 
Abstract: We develop a form Thompson sampling for online learning under full feedback - also known as prediction with expert advice - where the learner's prior is defined over the space of an adversary's future actions, rather than the space of experts. We show regret decomposes into regret the learner expected a priori, plus a prior-robustness-type term we call excess regret. In the classical finite-expert setting, this recovers optimal rates. As an initial step towards practical online learning in settings with a potentially-uncountably-infinite number of experts, we show that Thompson sampling over the $d$-dimensional unit cube, using a certain Gaussian process prior widely-used in the Bayesian optimization literature, has a $\mathcal{O}\Big(\beta\sqrt{Td\log(1+\sqrt{d}\frac{\lambda}{\beta})}\Big)$ rate against a $\beta$-bounded $\lambda$-Lipschitz adversary.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14790v4</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Terenin, Jeffrey Negrea</dc:creator>
    </item>
  </channel>
</rss>
