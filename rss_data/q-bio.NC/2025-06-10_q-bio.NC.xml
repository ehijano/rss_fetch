<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 11 Jun 2025 01:35:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>AI Agent Behavioral Science</title>
      <link>https://arxiv.org/abs/2506.06366</link>
      <description>arXiv:2506.06366v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have enabled AI systems to behave in increasingly human-like ways, exhibiting planning, adaptation, and social dynamics across increasingly diverse, interactive, and open-ended scenarios. These behaviors are not solely the product of the models' internal architecture, but emerge from their integration into agentic systems that operate within situated contexts, where goals, feedback, and interactions shape behavior over time. This shift calls for a new scientific lens: AI Agent Behavioral Science. Rather than focusing only on internal mechanisms, this paradigm emphasizes the systematic observation of behavior, design of interventions to test hypotheses, and theory-guided interpretation of how AI agents act, adapt, and interact over time. We systematize a growing body of research across individual, multi-agent, and human-agent interaction settings, and further demonstrate how this perspective informs responsible AI by treating fairness, safety, interpretability, accountability, and privacy as behavioral properties. By unifying recent findings and laying out future directions, we position AI Agent Behavioral Science as a necessary complement to traditional approaches, providing essential tools for understanding, evaluating, and governing the real-world behavior of increasingly autonomous AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06366v1</guid>
      <category>q-bio.NC</category>
      <category>cs.CY</category>
      <category>cs.MA</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lin Chen, Yunke Zhang, Jie Feng, Haoye Chai, Honglin Zhang, Bingbing Fan, Yibo Ma, Shiyuan Zhang, Nian Li, Tianhui Liu, Nicholas Sukiennik, Keyu Zhao, Yu Li, Ziyi Liu, Fengli Xu, Yong Li</dc:creator>
    </item>
    <item>
      <title>A Neuronal Model at the Edge of Criticality: An Ising-Inspired Approach to Brain Dynamics</title>
      <link>https://arxiv.org/abs/2506.07027</link>
      <description>arXiv:2506.07027v1 Announce Type: new 
Abstract: We present a neuronal network model inspired by the Ising model, where each neuron is a binary spin ($s_i = \pm1$) interacting with its neighbors on a 2D lattice. Updates are asynchronous and follow Metropolis dynamics, with a temperature-like parameter $T$ introducing stochasticity.
  To incorporate physiological realism, each neuron includes fixed on/off durations, mimicking the refractory period found in real neurons. These counters prevent immediate reactivation, adding biologically grounded timing constraints to the model.
  As $T$ varies, the network transitions from asynchronous to synchronised activity. Near a critical point $T_c$, we observe hallmarks of criticality: heightened fluctuations, long-range correlations, and increased sensitivity. These features resemble patterns found in cortical recordings, supporting the hypothesis that the brain operates near criticality for optimal information processing.
  This simplified model demonstrates how basic spin interactions and physiological constraints can yield complex, emergent behavior, offering a useful tool for studying criticality in neural systems through statistical physics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07027v1</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.soft</category>
      <category>stat.CO</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sajedeh Sarmastani, Maliheh Ghodrat, Yousef Jamali</dc:creator>
    </item>
    <item>
      <title>Less is More: some Computational Principles based on Parcimony, and Limitations of Natural Intelligence</title>
      <link>https://arxiv.org/abs/2506.07060</link>
      <description>arXiv:2506.07060v1 Announce Type: new 
Abstract: Natural intelligence (NI) consistently achieves more with less. Infants learn language, develop abstract concepts, and acquire sensorimotor skills from sparse data, all within tight neural and energy limits. In contrast, today's AI relies on virtually unlimited computational power, energy, and data to reach high performance. This paper argues that constraints in NI are paradoxically catalysts for efficiency, adaptability, and creativity. We first show how limited neural bandwidth promotes concise codes that still capture complex patterns. Spiking neurons, hierarchical structures, and symbolic-like representations emerge naturally from bandwidth constraints, enabling robust generalization. Next, we discuss chaotic itinerancy, illustrating how the brain transits among transient attractors to flexibly retrieve memories and manage uncertainty. We then highlight reservoir computing, where random projections facilitate rapid generalization from small datasets. Drawing on developmental perspectives, we emphasize how intrinsic motivation, along with responsive social environments, drives infant language learning and discovery of meaning. Such active, embodied processes are largely absent in current AI. Finally, we suggest that adopting 'less is more' principles -- energy constraints, parsimonious architectures, and real-world interaction -- can foster the emergence of more efficient, interpretable, and biologically grounded artificial systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07060v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laura Cohen, Xavier Hinaut, Lilyana Petrova, Alexandre Pitti, Syd Reynal, Ichiro Tsuda</dc:creator>
    </item>
    <item>
      <title>Slow and Fast Neurons Cooperate in Contextual Working Memory through Timescale Diversity</title>
      <link>https://arxiv.org/abs/2506.07341</link>
      <description>arXiv:2506.07341v1 Announce Type: new 
Abstract: Neural systems process information across a broad range of intrinsic timescales, both within and across cortical areas. While such diversity is a hallmark of biological networks, its computational role in nonlinear information processing remains elusive. In this study, we examine how heterogeneity in intrinsic neural timescales within the frontal cortex - a region central to cognitive control - enhances performance in a context-dependent working memory task. We develop a recurrent neural network (RNN) composed of units with distinct time constants to model a delayed match-to-sample task with contextual cues. This task demands nonlinear integration of temporally dispersed inputs and flexible behavioral adaptation. Our analysis shows that task performance is optimized when fast and slow timescales are appropriately balanced. Intriguingly, slow neurons, despite weaker encoding of task-relevant inputs, play a causal role in sustaining memory and improving performance. In contrast, fast neurons exhibit strong but transient encoding of input signals. These results highlight a division of computational roles among neurons with different timescales: slow dynamics support stable internal states, while fast dynamics enable rapid signal encoding. Our findings provide a mechanistic account of how temporal heterogeneity contributes to nonlinear information processing in neural circuits, shedding light on the dynamic architecture underlying cognitive flexibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07341v1</guid>
      <category>q-bio.NC</category>
      <category>nlin.AO</category>
      <category>physics.bio-ph</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tomoki Kurikawa</dc:creator>
    </item>
    <item>
      <title>Dataset combining EEG, eye-tracking, and high-speed video for ocular activity analysis across BCI paradigms</title>
      <link>https://arxiv.org/abs/2506.07488</link>
      <description>arXiv:2506.07488v1 Announce Type: new 
Abstract: In Brain-Computer Interface (BCI) research, the detailed study of blinks is crucial. They can be considered as noise, affecting the efficiency and accuracy of decoding users' cognitive states and intentions, or as potential features, providing valuable insights into users' behavior and interaction patterns. We introduce a large dataset capturing electroencephalogram (EEG) signals, eye-tracking, high-speed camera recordings, as well as subjects' mental states and characteristics, to provide a multifactor analysis of eye-related movements. Four paradigms -- motor imagery, motor execution, steady-state visually evoked potentials, and P300 spellers -- are selected due to their capacity to evoke various sensory-motor responses and potential influence on ocular activity. This online-available dataset contains over 46 hours of data from 31 subjects across 63 sessions, totaling 2520 trials for each of the first three paradigms, and 5670 for P300. This multimodal and multi-paradigms dataset is expected to allow the development of algorithms capable of efficiently handling eye-induced artifacts and enhancing task-specific classification. Furthermore, it offers the opportunity to evaluate the cross-paradigm robustness involving the same participants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07488v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1038/s41597-025-04861-9</arxiv:DOI>
      <dc:creator>E. Guttmann-Flury, X. Sheng, X. Zhu</dc:creator>
    </item>
    <item>
      <title>Bio-Inspired Classification: Combining Information Theory and Spiking Neural Networks -- Influence of the Learning Rules</title>
      <link>https://arxiv.org/abs/2506.06750</link>
      <description>arXiv:2506.06750v1 Announce Type: cross 
Abstract: Training of Spiking Neural Networks (SNN) is challenging due to their unique properties, including temporal dynamics, non-differentiability of spike events, and sparse event-driven activations. In this paper, we widely consider the influence of the type of chosen learning algorithm, including bioinspired learning rules on the accuracy of classification. We proposed a bioinspired classifier based on the combination of SNN and Lempel-Ziv complexity (LZC). This approach synergizes the strengths of SNNs in temporal precision and biological realism with LZC's structural complexity analysis, facilitating efficient and interpretable classification of spatiotemporal neural data. It turned out that the classic backpropagation algorithm achieves excellent classification accuracy, but at extremely high computational cost, which makes it impractical for real-time applications. Biologically inspired learning algorithms such as tempotron and Spikprop provide increased computational efficiency while maintaining competitive classification performance, making them suitable for time-sensitive tasks. The results obtained indicate that the selection of the most appropriate learning algorithm depends on the trade-off between classification accuracy and computational cost as well as application constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06750v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zofia Rudnicka, Janusz Szczepanski, Agnieszka Pregowska</dc:creator>
    </item>
    <item>
      <title>NSD-Imagery: A benchmark dataset for extending fMRI vision decoding methods to mental imagery</title>
      <link>https://arxiv.org/abs/2506.06898</link>
      <description>arXiv:2506.06898v1 Announce Type: cross 
Abstract: We release NSD-Imagery, a benchmark dataset of human fMRI activity paired with mental images, to complement the existing Natural Scenes Dataset (NSD), a large-scale dataset of fMRI activity paired with seen images that enabled unprecedented improvements in fMRI-to-image reconstruction efforts. Recent models trained on NSD have been evaluated only on seen image reconstruction. Using NSD-Imagery, it is possible to assess how well these models perform on mental image reconstruction. This is a challenging generalization requirement because mental images are encoded in human brain activity with relatively lower signal-to-noise and spatial resolution; however, generalization from seen to mental imagery is critical for real-world applications in medical domains and brain-computer interfaces, where the desired information is always internally generated. We provide benchmarks for a suite of recent NSD-trained open-source visual decoding models (MindEye1, MindEye2, Brain Diffuser, iCNN, Takagi et al.) on NSD-Imagery, and show that the performance of decoding methods on mental images is largely decoupled from performance on vision reconstruction. We further demonstrate that architectural choices significantly impact cross-decoding performance: models employing simple linear decoding architectures and multimodal feature decoding generalize better to mental imagery, while complex architectures tend to overfit visual training data. Our findings indicate that mental imagery datasets are critical for the development of practical applications, and establish NSD-Imagery as a useful resource for better aligning visual decoding methods with this goal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06898v1</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Reese Kneeland, Paul S. Scotti, Ghislain St-Yves, Jesse Breedlove, Kendrick Kay, Thomas Naselaris</dc:creator>
    </item>
    <item>
      <title>Can Biologically Plausible Temporal Credit Assignment Rules Match BPTT for Neural Similarity? E-prop as an Example</title>
      <link>https://arxiv.org/abs/2506.06904</link>
      <description>arXiv:2506.06904v1 Announce Type: cross 
Abstract: Understanding how the brain learns may be informed by studying biologically plausible learning rules. These rules, often approximating gradient descent learning to respect biological constraints such as locality, must meet two critical criteria to be considered an appropriate brain model: (1) good neuroscience task performance and (2) alignment with neural recordings. While extensive research has assessed the first criterion, the second remains underexamined. Employing methods such as Procrustes analysis on well-known neuroscience datasets, this study demonstrates the existence of a biologically plausible learning rule -- namely e-prop, which is based on gradient truncation and has demonstrated versatility across a wide range of tasks -- that can achieve neural data similarity comparable to Backpropagation Through Time (BPTT) when matched for task accuracy. Our findings also reveal that model architecture and initial conditions can play a more significant role in determining neural similarity than the specific learning rule. Furthermore, we observe that BPTT-trained models and their biologically plausible counterparts exhibit similar dynamical properties at comparable accuracies. These results underscore the substantial progress made in developing biologically plausible learning rules, highlighting their potential to achieve both competitive task performance and neural data similarity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06904v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yuhan Helena Liu, Guangyu Robert Yang, Christopher J. Cueva</dc:creator>
    </item>
    <item>
      <title>Decoding Saccadic Eye Movements from Brain Signals Using an Endovascular Neural Interface</title>
      <link>https://arxiv.org/abs/2506.07481</link>
      <description>arXiv:2506.07481v1 Announce Type: cross 
Abstract: An Oculomotor Brain-Computer Interface (BCI) records neural activity from regions of the brain involved in planning eye movements and translates this activity into control commands. While previous successful oculomotor BCI studies primarily relied on invasive microelectrode implants in non-human primates, this study investigates the feasibility of an oculomotor BCI using a minimally invasive endovascular Stentrode device implanted near the supplementary motor area in a patient with amyotrophic lateral sclerosis (ALS). To achieve this, self-paced visually-guided and free-viewing saccade tasks were designed, in which the participant performed saccades in four directions (left, right, up, down), with simultaneous recording of endovascular EEG and eye gaze. The visually guided saccades were cued with visual stimuli, whereas the free-viewing saccades were self-directed without explicit cues. The results showed that while the neural responses of visually guided saccades overlapped with the cue-evoked potentials, the free-viewing saccades exhibited distinct saccade-related potentials that began shortly before eye movement, peaked approximately 50 ms after saccade onset, and persisted for around 200 ms. In the frequency domain, these responses appeared as a low-frequency synchronisation below 15 Hz. Classification of 'fixation vs. saccade' was robust, achieving mean area under the receiver operating characteristic curve (AUC) scores of 0.88 within sessions and 0.86 between sessions. In contrast, classifying saccade direction proved more challenging, yielding within-session AUC scores of 0.67 for four-class decoding and up to 0.75 for the best-performing binary comparisons (left vs. up and left vs. down). This proof-of-concept study demonstrates the feasibility of an endovascular oculomotor BCI in an ALS patient, establishing a foundation for future oculomotor BCI studies in human subjects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07481v1</guid>
      <category>eess.SP</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Suleman Rasheed, James Bennett, Peter E. Yoo, Anthony N. Burkitt, David B. Grayden</dc:creator>
    </item>
    <item>
      <title>ODIN: Open Data In Neurophysiology: Advancements, Solutions &amp; Challenges</title>
      <link>https://arxiv.org/abs/2407.00976</link>
      <description>arXiv:2407.00976v2 Announce Type: replace 
Abstract: Across the life sciences, an ongoing effort over the last 50 years has made data and methods more reproducible and transparent. This openness has led to transformative insights and vastly accelerated scientific progress. For example, structural biology and genomics have undertaken systematic collection and publication of protein sequences and structures over the past half-century, and these data have led to scientific breakthroughs that were unthinkable when data collection first began. We believe that neuroscience is poised to follow the same path, and that principles of open data and open science will transform our understanding of the nervous system in ways that are impossible to predict at the moment. To this end, new social structures along with active and open scientific communities are essential to facilitate and expand the still limited adoption of open science practices in our field. Unified by shared values of openness, we set out to organize a symposium for Open Data in Neuroscience (ODIN) to strengthen our community and facilitate transformative neuroscience research at large. In this report, we share what we learned during this first ODIN event. We also lay out plans for how to grow this movement, document emerging conversations, and propose a path toward a better and more transparent science of tomorrow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00976v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Colleen J. Gillon, Cody Baker, Ryan Ly, Edoardo Balzani, Bingni W. Brunton, Manuel Schottdorf, Satrajit Ghosh, Nima Dehghani</dc:creator>
    </item>
    <item>
      <title>ST-USleepNet: A Spatial-Temporal Coupling Prominence Network for Multi-Channel Sleep Staging</title>
      <link>https://arxiv.org/abs/2408.11884</link>
      <description>arXiv:2408.11884v3 Announce Type: replace 
Abstract: Sleep staging is critical to assess sleep quality and diagnose disorders. Despite advancements in artificial intelligence enabling automated sleep staging, significant challenges remain: (1) Simultaneously extracting prominent temporal and spatial sleep features from multi-channel raw signals, including characteristic sleep waveforms and salient spatial brain networks. (2) Capturing the spatial-temporal coupling patterns essential for accurate sleep staging. To address these challenges, we propose a novel framework named ST-USleepNet, comprising a spatial-temporal graph construction module (ST) and a U-shaped sleep network (USleepNet). The ST module converts raw signals into a spatial-temporal graph based on signal similarity, temporal, and spatial relationships to model spatial-temporal coupling patterns. The USleepNet employs a U-shaped structure for both the temporal and spatial streams, mirroring its original use in image segmentation to isolate significant targets. Applied to raw sleep signals and graph data from the ST module, USleepNet effectively segments these inputs, simultaneously extracting prominent temporal and spatial sleep features. Testing on three datasets demonstrates that ST-USleepNet outperforms existing baselines, and model visualizations confirm its efficacy in extracting prominent sleep features and temporal-spatial coupling patterns across various sleep stages. The code is available at https://github.com/Majy-Yuji/ST-USleepNet.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11884v3</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingying Ma, Qika Lin, Ziyu Jia, Mengling Feng</dc:creator>
    </item>
    <item>
      <title>MindLLM: A Subject-Agnostic and Versatile Model for fMRI-to-Text Decoding</title>
      <link>https://arxiv.org/abs/2502.15786</link>
      <description>arXiv:2502.15786v2 Announce Type: replace 
Abstract: Decoding functional magnetic resonance imaging (fMRI) signals into text has been a key challenge in the neuroscience community, with the potential to advance brain-computer interfaces and uncover deeper insights into brain mechanisms. However, existing approaches often struggle with suboptimal predictive performance, limited task variety, and poor generalization across subjects. In response to this, we propose MindLLM, a model designed for subject-agnostic and versatile fMRI-to-text decoding. MindLLM consists of an fMRI encoder and an off-the-shelf LLM. The fMRI encoder employs a neuroscience-informed attention mechanism, which is capable of accommodating subjects with varying input shapes and thus achieves high-performance subject-agnostic decoding. Moreover, we introduce Brain Instruction Tuning (BIT), a novel approach that enhances the model's ability to capture diverse semantic representations from fMRI signals, facilitating more versatile decoding. We evaluate MindLLM on comprehensive fMRI-to-text benchmarks. Results demonstrate that our model outperforms the baselines, improving downstream tasks by 12.0%, unseen subject generalization by 24.5%, and novel task adaptation by 25.0%. Furthermore, the attention patterns in MindLLM provide interpretable insights into its decision-making process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15786v2</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weikang Qiu, Zheng Huang, Haoyu Hu, Aosong Feng, Yujun Yan, Rex Ying</dc:creator>
    </item>
    <item>
      <title>R-FORCE: Robust Learning for Random Recurrent Neural Networks</title>
      <link>https://arxiv.org/abs/2003.11660</link>
      <description>arXiv:2003.11660v2 Announce Type: replace-cross 
Abstract: Random Recurrent Neural Networks (RRNN) are the simplest recurrent networks to model and extract features from sequential data. The simplicity however comes with a price; RRNN are known to be susceptible to diminishing/exploding gradient problem when trained with gradient-descent based optimization. To enhance robustness of RRNN, alternative training approaches have been proposed. Specifically, FORCE learning approach proposed a recursive least squares alternative to train RRNN and was shown to be applicable even for the challenging task of target-learning, where the network is tasked with generating dynamic patterns with no guiding input. While FORCE training indicates that solving target-learning is possible, it appears to be effective only in a specific regime of network dynamics (edge-of-chaos). We thereby investigate whether initialization of RRNN connectivity according to a tailored distribution can guarantee robust FORCE learning. We are able to generate such distribution by inference of four generating principles constraining the spectrum of the network Jacobian to remain in stability region. This initialization along with FORCE learning provides a robust training method, i.e., Robust-FORCE (R-FORCE). We validate R-FORCE performance on various target functions for a wide range of network configurations and compare with alternative methods. Our experiments indicate that R-FORCE facilitates significantly more stable and accurate target-learning for a wide class of RRNN. Such stability becomes critical in modeling multi-dimensional sequences as we demonstrate on modeling time-series of human body joints during physical movements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2003.11660v2</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Zheng, Eli Shlizerman</dc:creator>
    </item>
    <item>
      <title>Fast Two-photon Microscopy by Neuroimaging with Oblong Random Acquisition (NORA)</title>
      <link>https://arxiv.org/abs/2503.15487</link>
      <description>arXiv:2503.15487v2 Announce Type: replace-cross 
Abstract: Advances in neural imaging have enabled neuroscientists to study how large neural populations conspire to produce perception, behavior and cognition. Despite many advances in optical methods, there exists a fundamental tradeoff between imaging speed, field of view, and resolution that limits the scope of neural imaging, especially for the raster-scanning multi-photon imaging needed to image deeper into the brain. One approach to overcoming this trade-off is computational imaging: the co-development of optics designed to encode the target images into fewer measurements that are faster to acquire, with algorithms that compensate by inverting the optical coding to recover a larger or higher resolution image. We present here one such approach for raster-scanning two-photon imaging: Neuroimaging with Oblong Random Acquisition (NORA). NORA quickly acquires each frame in a microscopy video by subsampling only a fraction of the fast scanning lines, ignoring large portions of each frame. NORA mitigates the loss of information by 1) extending the point-spread function in the slow-scan direction to effectively integrate the fluorescence of several lines into a single set of measurements and 2) imaging different, randomly selected, lines at each frame. Rather than reconstruct the video frame-by-frame, NORA recovers full video sequences via nuclear-norm minimization on the pixels-by-time matrix, for which we prove theoretical guarantees on recovery. We simulated NORA imaging using the Neural Anatomy and Optical Microscopy (NAOMi) biophysical simulator, and used the simulations to demonstrate that NORA can accurately recover 400 um X 400 um fields of view at subsampling rates up to 20X, despite realistic noise and motion conditions. As NORA requires minimal changes to current microscopy systems, our results indicate that NORA can provide a promising avenue towards fast imaging of neural circuits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15487v2</guid>
      <category>eess.IV</category>
      <category>eess.SP</category>
      <category>physics.optics</category>
      <category>q-bio.NC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Esther Whang, Skyler Thomas, Ji Yi, Adam S. Charles</dc:creator>
    </item>
  </channel>
</rss>
