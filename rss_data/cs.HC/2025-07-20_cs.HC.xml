<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 21 Jul 2025 04:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Humans learn to prefer trustworthy AI over human partners</title>
      <link>https://arxiv.org/abs/2507.13524</link>
      <description>arXiv:2507.13524v1 Announce Type: new 
Abstract: Partner selection is crucial for cooperation and hinges on communication. As artificial agents, especially those powered by large language models (LLMs), become more autonomous, intelligent, and persuasive, they compete with humans for partnerships. Yet little is known about how humans select between human and AI partners and adapt under AI-induced competition pressure. We constructed a communication-based partner selection game and examined the dynamics in hybrid mini-societies of humans and bots powered by a state-of-the-art LLM. Through three experiments (N = 975), we found that bots, though more prosocial than humans and linguistically distinguishable, were not selected preferentially when their identity was hidden. Instead, humans misattributed bots' behaviour to humans and vice versa. Disclosing bots' identity induced a dual effect: it reduced bots' initial chances of being selected but allowed them to gradually outcompete humans by facilitating human learning about the behaviour of each partner type. These findings show how AI can reshape social interaction in mixed societies and inform the design of more effective and cooperative hybrid systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13524v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yaomin Jiang, Levin Brinkmann, Anne-Marie Nussberger, Ivan Soraperra, Jean-Fran\c{c}ois Bonnefon, Iyad Rahwan</dc:creator>
    </item>
    <item>
      <title>Human-Like Trajectories Generation via Receding Horizon Tracking Applied to the TickTacking Interface</title>
      <link>https://arxiv.org/abs/2507.13528</link>
      <description>arXiv:2507.13528v1 Announce Type: new 
Abstract: TickTacking is a rhythm-based interface that allows users to control a pointer in a two-dimensional space through dual-button tapping. This paper investigates the generation of human-like trajectories using a receding horizon approach applied to the TickTacking interface in a target-tracking task. By analyzing user-generated trajectories, we identify key human behavioral features and incorporate them in a controller that mimics these behaviors. The performance of this human-inspired controller is evaluated against a baseline optimal-control-based agent, demonstrating the importance of specific control features for achieving human-like interaction. These findings contribute to the broader goal of developing rhythm-based human-machine interfaces by offering design insights that enhance user performance, improve intuitiveness, and reduce interaction frustration</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13528v1</guid>
      <category>cs.HC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniele Masti, Stefano Menchetti, \c{C}a\u{g}r{\i} Erdem, Giorgio Gnecco, Davide Rocchesso</dc:creator>
    </item>
    <item>
      <title>In-Home Social Robots Design for Cognitive Stimulation Therapy in Dementia Care</title>
      <link>https://arxiv.org/abs/2507.13578</link>
      <description>arXiv:2507.13578v1 Announce Type: new 
Abstract: Individual cognitive stimulation therapy (iCST) is a non-pharmacological intervention for improving the cognition and quality of life of persons with dementia (PwDs); however, its effectiveness is limited by low adherence to delivery by their family members. In this work, we present the user-centered design and evaluation of a novel socially assistive robotic system to provide iCST therapy to PwDs in their homes for long-term use. We consulted with 16 dementia caregivers and professionals. Through these consultations, we gathered design guidelines and developed the prototype. The prototype was validated by testing it with three dementia professionals and five PwDs. The evaluation revealed PwDs enjoyed using the system and are willing to adopt its use over the long term. One shortcoming was the system's speech-to-text capabilities, where it frequently failed to understand the PwDs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13578v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emmanuel Akinrintoyo, Nicole Salomons</dc:creator>
    </item>
    <item>
      <title>From Firms to Computation: AI Governance and the Evolution of Institutions</title>
      <link>https://arxiv.org/abs/2507.13616</link>
      <description>arXiv:2507.13616v1 Announce Type: new 
Abstract: The integration of agential artificial intelligence into socioeconomic systems requires us to reexamine the evolutionary processes that describe changes in our economic institutions. This article synthesizes three frameworks: multi-level selection theory, Aoki's view of firms as computational processes, and Ostrom's design principles for robust institutions. We develop a framework where selection operates concurrently across organizational levels, firms implement distributed inference via game-theoretic architectures, and Ostrom-style rules evolve as alignment mechanisms that address AI-related risks. This synthesis yields a multi-level Price equation expressed over nested games, providing quantitative metrics for how selection and governance co-determine economic outcomes. We examine connections to Acemoglu's work on inclusive institutions, analyze how institutional structures shape AI deployment, and demonstrate the framework's explanatory power via case studies. We conclude by proposing a set of design principles that operationalize alignment between humans and AI across institutional layers, enabling scalable, adaptive, and inclusive governance of agential AI systems. We conclude with practical policy recommendations and further research to extend these principles into real-world implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13616v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <category>cs.IT</category>
      <category>cs.MA</category>
      <category>math.IT</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael S. Harre</dc:creator>
    </item>
    <item>
      <title>Managing level of detail through peripheral degradation: Effects on search performance with a head-mounted display</title>
      <link>https://arxiv.org/abs/2507.13660</link>
      <description>arXiv:2507.13660v1 Announce Type: new 
Abstract: Two user studies were performed to evaluate the effect of level-of-detail (LOD) degradation in the periphery of head-mounted displays on visual search performance. In the first study, spatial detail was degraded by reducing resolution. In the second study, detail was degraded in the color domain by using grayscale in the periphery. In each study, 10 subjects were given a complex search task that required users to indicate whether or not a target object was present among distracters. Subjects used several different displays varying in the amount of detail presented. Frame rate, object location, subject input method, and order of display use were all controlled. The primary dependent measures were search time on correctly performed trials and the percentage of all trials correctly performed. Results indicated that peripheral LOD degradation can be used to reduce color or spatial visual complexity by almost half in some search tasks with out significantly reducing performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13660v1</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/267135.267137</arxiv:DOI>
      <arxiv:journal_reference>ACM Transactions on Computer-Human Interaction (TOCHI) Volume 4 Issue 4 Pages 323-346. (1997)</arxiv:journal_reference>
      <dc:creator>Benjamin Watson, Neff Walker, Larry F Hodges, Aileen Worden</dc:creator>
    </item>
    <item>
      <title>Regression-Based Approach to Anxiety Estimation of Spider Phobics During Behavioural Avoidance Tasks</title>
      <link>https://arxiv.org/abs/2507.13795</link>
      <description>arXiv:2507.13795v1 Announce Type: new 
Abstract: Phobias significantly impact the quality of life of affected persons. Two methods of assessing anxiety responses are questionnaires and behavioural avoidance tests (BAT). While these can be used in a clinical environment they only record momentary insights into anxiety measures. In this study, we estimate the intensity of anxiety during these BATs, using physiological data collected from unobtrusive, wrist-worn sensors. Twenty-five participants performed four different BATs in a single session, while periodically being asked how anxious they currently are. Using heart rate, heart rate variability, electrodermal activity, and skin temperature, we trained regression models to predict anxiety ratings from three types of input data: (1) using only physiological signals, (2) adding computed features (e.g., min, max, range, variability), and (3) computed features combined with contextual task information. Adding contextual information increased the effectiveness of the model, leading to a root mean squared error (RMSE) of 0.197 and a mean absolute error (MAE) of 0.041. Overall, this study shows, that data obtained from wearables can continuously provide meaningful estimations of anxiety, which can assist in therapy planning and enable more personalised treatment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13795v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Florian Grensing, Vanessa Schm\"ucker, Anne Sophie Hildebrand, Tim Klucken, Maria Maleshkova</dc:creator>
    </item>
    <item>
      <title>Effects of Cognitive Distraction and Driving Environment Complexity on Adaptive Cruise Control Use and Its Impact on Driving Performance: A Simulator Study</title>
      <link>https://arxiv.org/abs/2507.13886</link>
      <description>arXiv:2507.13886v1 Announce Type: new 
Abstract: In this simulator study, we adopt a human-centered approach to explore whether and how drivers' cognitive state and driving environment complexity influence reliance on driving automation features. Besides, we examine whether such reliance affects driving performance. Participants operated a vehicle equipped with adaptive cruise control (ACC) in a simulator across six predefined driving scenarios varying in traffic conditions while either performing a cognitively demanding task (i.e., responding to mental calculations) or not. Throughout the experiment, participants had to respect speed limits and were free to activate or deactivate ACC. In complex driving environments, we found that the overall ACC engagement time was lower compared to less complex driving environments. We observed no significant effect of cognitive load on ACC use. Furthermore, while ACC use had no effect on the number of lane changes, it impacted the speed limits compliance and improved lateral control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13886v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3744333.3747822</arxiv:DOI>
      <dc:creator>Ana\"is Halin, Marc Van Droogenbroeck, Christel Devue</dc:creator>
    </item>
    <item>
      <title>Initiating and Replicating the Observations of Interactional Properties by User Studies Optimizing Applicative Prototypes</title>
      <link>https://arxiv.org/abs/2507.13923</link>
      <description>arXiv:2507.13923v1 Announce Type: new 
Abstract: The science of Human-Computer Interaction (HCI) is populated by isolated empirical findings, often tied to specific technologies, designs, and tasks. This paper proposes a formalization of user interaction observations (instead of user interfaces) and an associated revealing method (interaction loop diffraction). The resulting interactional properties that are studied in a calibrated manner, are well suited to replication across various conditions (prototypes, technologies, tasks, and user profiles). In particular, interactional properties can emerge and be replicated within the workflow of applicative cases, which in return benefit from the optimization of applicative prototypes. Applicative cases' publications will then contribute to demonstrating technology utility, along with providing empirical results that will lead future work to theory consolidation and theory building, and finally to a catalog and a science of relevant interactional properties. These properties will contribute to better user interactions, especially for the variety of ubiquitous user interfaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13923v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guillaume Rivi\`ere</dc:creator>
    </item>
    <item>
      <title>Democratizing Game Modding with GenAI: A Case Study of StarCharM, a Stardew Valley Character Maker</title>
      <link>https://arxiv.org/abs/2507.13951</link>
      <description>arXiv:2507.13951v1 Announce Type: new 
Abstract: Game modding offers unique and personalized gaming experiences, but the technical complexity of creating mods often limits participation to skilled users. We envision a future where every player can create personalized mods for their games. To explore this space, we designed StarCharM, a GenAI-based non-player character (NPC) creator for Stardew Valley. Our tool enables players to iteratively create new NPC mods, requiring minimal user input while allowing for fine-grained adjustments through user control. We conducted a user study with ten Stardew Valley players who had varied mod usage experiences to understand the impacts of StarCharM and provide insights into how GenAI tools may reshape modding, particularly in NPC creation. Participants expressed excitement in bringing their character ideas to life, although they noted challenges in generating rich content to fulfill complex visions. While they believed GenAI tools like StarCharM can foster a more diverse modding community, some voiced concerns about diminished originality and community engagement that may come with such technology. Our findings provided implications and guidelines for the future of GenAI-powered modding tools and co-creative modding practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13951v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3748612</arxiv:DOI>
      <dc:creator>Hamid Zand Miralvand, Mohammad Ronagh Nikghalb, Mohammad Darandeh, Abidullah Khan, Ian Arawjo, Jinghui Cheng</dc:creator>
    </item>
    <item>
      <title>Estimating Cognitive Effort from Functional Near-Infrared Spectroscopy (fNIRS) Signals using Machine Learning</title>
      <link>https://arxiv.org/abs/2507.13952</link>
      <description>arXiv:2507.13952v1 Announce Type: new 
Abstract: The estimation of cognitive effort could potentially help educators to modify material to enhance learning effectiveness and student engagement. Where cognitive load refers how much work the brain is doing while someone is learning or doing a task cognitive effort consider both load and behavioral performance. Cognitive effort can be captured by measuring oxygen flow and behavioral performance during a task. This study infers cognitive effort metrics using machine learning models based on oxygenated hemoglobin collected by using functional near-infrared spectroscopy from the prefrontal cortex during an educational gameplay. In our study, sixteen participants responded to sixteen questions in an in-house Unity-based educational game. The quiz was divided into two sessions, each session consisting of two task segments. We extracted temporal statistical and functional connectivity features from collected oxygenated hemoglobin and analyzed their correlation with quiz performance. We trained multiple machine learning models to predict quiz performance from oxygenated hemoglobin features and achieved accuracies ranging from 58\% to 67\% accuracy. These predictions were used to calculate cognitive effort via relative neural involvement and efficiency, which consider both brain activation and behavioral performance. Although quiz score predictions achieved moderate accuracy, the derived relative neural efficiency and involvement values remained robust. Since both metrics are based on the relative positions of standardized brain activation and performance scores, even small misclassifications in predicted scores preserved the overall cognitive effort trends observed during gameplay.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13952v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shayla Sharmin, Roghayeh Leila Barmaki</dc:creator>
    </item>
    <item>
      <title>Architecting Human-AI Cocreation for Technical Services -- Interaction Modes and Contingency Factors</title>
      <link>https://arxiv.org/abs/2507.14034</link>
      <description>arXiv:2507.14034v1 Announce Type: new 
Abstract: Agentic AI systems, powered by Large Language Models (LLMs), offer transformative potential for value co-creation in technical services. However, persistent challenges like hallucinations and operational brittleness limit their autonomous use, creating a critical need for robust frameworks to guide human-AI collaboration. Drawing on established Human-AI teaming research and analogies from fields like autonomous driving, this paper develops a structured taxonomy of human-agent interaction. Based on case study research within technical support platforms, we propose a six-mode taxonomy that organizes collaboration across a spectrum of AI autonomy. This spectrum is anchored by the Human-Out-of-the-Loop (HOOTL) model for full automation and the Human-Augmented Model (HAM) for passive AI assistance. Between these poles, the framework specifies four distinct intermediate structures. These include the Human-in-Command (HIC) model, where AI proposals re-quire mandatory human approval, and the Human-in-the-Process (HITP) model for structured work-flows with deterministic human tasks. The taxonomy further delineates the Human-in-the-Loop (HITL) model, which facilitates agent-initiated escalation upon uncertainty, and the Human-on-the-Loop (HOTL) model, which enables discretionary human oversight of an autonomous AI. The primary contribution of this work is a comprehensive framework that connects this taxonomy to key contingency factors -- such as task complexity, operational risk, and system reliability -- and their corresponding conceptual architectures. By providing a systematic method for selecting and designing an appropriate level of human oversight, our framework offers practitioners a crucial tool to navigate the trade-offs between automation and control, thereby fostering the development of safer, more effective, and context-aware technical service systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14034v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jochen Wulf, Jurg Meierhofer, Frank Hannich</dc:creator>
    </item>
    <item>
      <title>The Emotion-Memory Link: Do Memorability Annotations Matter for Intelligent Systems?</title>
      <link>https://arxiv.org/abs/2507.14084</link>
      <description>arXiv:2507.14084v1 Announce Type: new 
Abstract: Humans have a selective memory, remembering relevant episodes and forgetting the less relevant information. Possessing awareness of event memorability for a user could help intelligent systems in more accurate user modelling, especially for such applications as meeting support systems, memory augmentation, and meeting summarisation. Emotion recognition has been widely studied, since emotions are thought to signal moments of high personal relevance to users. The emotional experience of situations and their memorability have traditionally been considered to be closely tied to one another: moments that are experienced as highly emotional are considered to also be highly memorable. This relationship suggests that emotional annotations could serve as proxies for memorability. However, existing emotion recognition systems rely heavily on third-party annotations, which may not accurately represent the first-person experience of emotional relevance and memorability. This is why, in this study, we empirically examine the relationship between perceived group emotions (Pleasure-Arousal) and group memorability in the context of conversational interactions. Our investigation involves continuous time-based annotations of both emotions and memorability in dynamic, unstructured group settings, approximating conditions of real-world conversational AI applications such as online meeting support systems. Our results show that the observed relationship between affect and memorability annotations cannot be reliably distinguished from what might be expected under random chance. We discuss the implications of this surprising finding for the development and applications of Affective Computing technology. In addition, we contextualise our findings in broader discourses in the Affective Computing and point out important targets for future research efforts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14084v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maria Tsfasman, Ramin Ghorbani, Catholijn M. Jonker, Bernd Dudzik</dc:creator>
    </item>
    <item>
      <title>ERR@HRI 2.0 Challenge: Multimodal Detection of Errors and Failures in Human-Robot Conversations</title>
      <link>https://arxiv.org/abs/2507.13468</link>
      <description>arXiv:2507.13468v1 Announce Type: cross 
Abstract: The integration of large language models (LLMs) into conversational robots has made human-robot conversations more dynamic. Yet, LLM-powered conversational robots remain prone to errors, e.g., misunderstanding user intent, prematurely interrupting users, or failing to respond altogether. Detecting and addressing these failures is critical for preventing conversational breakdowns, avoiding task disruptions, and sustaining user trust. To tackle this problem, the ERR@HRI 2.0 Challenge provides a multimodal dataset of LLM-powered conversational robot failures during human-robot conversations and encourages researchers to benchmark machine learning models designed to detect robot failures. The dataset includes 16 hours of dyadic human-robot interactions, incorporating facial, speech, and head movement features. Each interaction is annotated with the presence or absence of robot errors from the system perspective, and perceived user intention to correct for a mismatch between robot behavior and user expectation. Participants are invited to form teams and develop machine learning models that detect these failures using multimodal data. Submissions will be evaluated using various performance metrics, including detection accuracy and false positive rate. This challenge represents another key step toward improving failure detection in human-robot interaction through social signal analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13468v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shiye Cao, Maia Stiber, Amama Mahmood, Maria Teresa Parreira, Wendy Ju, Micol Spitale, Hatice Gunes, Chien-Ming Huang</dc:creator>
    </item>
    <item>
      <title>Improving Low-Cost Teleoperation: Augmenting GELLO with Force</title>
      <link>https://arxiv.org/abs/2507.13602</link>
      <description>arXiv:2507.13602v1 Announce Type: cross 
Abstract: In this work we extend the low-cost GELLO teleoperation system, initially designed for joint position control, with additional force information. Our first extension is to implement force feedback, allowing users to feel resistance when interacting with the environment. Our second extension is to add force information into the data collection process and training of imitation learning models. We validate our additions by implementing these on a GELLO system with a Franka Panda arm as the follower robot, performing a user study, and comparing the performance of policies trained with and without force information on a range of simulated and real dexterous manipulation tasks. Qualitatively, users with robotics experience preferred our controller, and the addition of force inputs improved task success on the majority of tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13602v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/SII59315.2025.10871050</arxiv:DOI>
      <dc:creator>Shivakanth Sujit, Luca Nunziante, Dan Ogawa Lillrank, Rousslan Fernand Julien Dossa, Kai Arulkumaran</dc:creator>
    </item>
    <item>
      <title>DailyLLM: Context-Aware Activity Log Generation Using Multi-Modal Sensors and LLMs</title>
      <link>https://arxiv.org/abs/2507.13737</link>
      <description>arXiv:2507.13737v1 Announce Type: cross 
Abstract: Rich and context-aware activity logs facilitate user behavior analysis and health monitoring, making them a key research focus in ubiquitous computing. The remarkable semantic understanding and generation capabilities of Large Language Models (LLMs) have recently created new opportunities for activity log generation. However, existing methods continue to exhibit notable limitations in terms of accuracy, efficiency, and semantic richness. To address these challenges, we propose DailyLLM. To the best of our knowledge, this is the first log generation and summarization system that comprehensively integrates contextual activity information across four dimensions: location, motion, environment, and physiology, using only sensors commonly available on smartphones and smartwatches. To achieve this, DailyLLM introduces a lightweight LLM-based framework that integrates structured prompting with efficient feature extraction to enable high-level activity understanding. Extensive experiments demonstrate that DailyLLM outperforms state-of-the-art (SOTA) log generation methods and can be efficiently deployed on personal computers and Raspberry Pi. Utilizing only a 1.5B-parameter LLM model, DailyLLM achieves a 17% improvement in log generation BERTScore precision compared to the 70B-parameter SOTA baseline, while delivering nearly 10x faster inference speed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13737v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>cs.MM</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ye Tian, Xiaoyuan Ren, Zihao Wang, Onat Gungor, Xiaofan Yu, Tajana Rosing</dc:creator>
    </item>
    <item>
      <title>The Expressions of Depression and Anxiety in Chinese Psycho-counseling: Usage of First-person Singular Pronoun and Negative Emotional Words</title>
      <link>https://arxiv.org/abs/2507.13839</link>
      <description>arXiv:2507.13839v1 Announce Type: cross 
Abstract: This study explores the relationship between linguistic expressions and psychological states of depression and anxiety within Chinese psycho-counseling interactions, focusing specifically on the usage of first-person singular pronouns and negative emotional words. Utilizing a corpus derived from 735 online counseling sessions, the analysis employed a general linear mixed-effect model to assess linguistic patterns quantified by the Linguistic Inquiry and Word Count (LIWC) software. Results indicate a significant positive correlation between the frequency of negative emotional words and the severity of both depressive and anxious states among clients. However, contrary to prior findings predominantly derived from English-language contexts, the usage frequency of first-person singular pronouns did not vary significantly with the clients' psychological conditions. These outcomes are discussed within the framework of cultural distinctions between collectivist Chinese contexts and individualistic Western settings, as well as the interactive dynamics unique to psycho-counseling conversations. The findings highlight the nuanced influence of cultural and conversational contexts on language use in mental health communications, providing insights into psycholinguistic markers relevant to therapeutic practices in Chinese-speaking populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13839v1</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lizhi Ma, Tong Zhao, Shuai Zhang, Nirui Song, Hongliang He, Anqi Li, Ran Feng, Huachuan Qiu, Jingsong Ma, Zhenzhong Lan</dc:creator>
    </item>
    <item>
      <title>The Levers of Political Persuasion with Conversational AI</title>
      <link>https://arxiv.org/abs/2507.13919</link>
      <description>arXiv:2507.13919v1 Announce Type: cross 
Abstract: There are widespread fears that conversational AI could soon exert unprecedented influence over human beliefs. Here, in three large-scale experiments (N=76,977), we deployed 19 LLMs-including some post-trained explicitly for persuasion-to evaluate their persuasiveness on 707 political issues. We then checked the factual accuracy of 466,769 resulting LLM claims. Contrary to popular concerns, we show that the persuasive power of current and near-future AI is likely to stem more from post-training and prompting methods-which boosted persuasiveness by as much as 51% and 27% respectively-than from personalization or increasing model scale. We further show that these methods increased persuasion by exploiting LLMs' unique ability to rapidly access and strategically deploy information and that, strikingly, where they increased AI persuasiveness they also systematically decreased factual accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13919v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kobi Hackenburg, Ben M. Tappin, Luke Hewitt, Ed Saunders, Sid Black, Hause Lin, Catherine Fist, Helen Margetts, David G. Rand, Christopher Summerfield</dc:creator>
    </item>
    <item>
      <title>Eye-tracked Virtual Reality: A Comprehensive Survey on Methods and Privacy Challenges</title>
      <link>https://arxiv.org/abs/2305.14080</link>
      <description>arXiv:2305.14080v2 Announce Type: replace 
Abstract: The latest developments in computer hardware, sensor technologies, and artificial intelligence can make virtual reality (VR) and virtual spaces an important part of human everyday life. Eye tracking offers not only a hands-free way of interaction but also the possibility of a deeper understanding of human visual attention and cognitive processes in VR. Despite these possibilities, eye-tracking data also reveals users' privacy-sensitive attributes when combined with the information about the presented stimulus. To address all these possibilities and potential privacy issues, in this survey, we first cover major works in eye tracking, VR, and privacy areas between 2012 and 2022. While eye tracking in the VR part covers the complete pipeline of eye-tracking methodology from pupil detection and gaze estimation to offline use of the data and analyses, as for privacy and security, we focus on eye-based authentication as well as computational methods to preserve the privacy of individuals and their eye-tracking data in VR. Later, considering all of these, we draw three main directions for the research community by focusing on privacy challenges. In summary, this survey provides an extensive literature review of the utmost possibilities with eye tracking in VR and the privacy implications of those possibilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.14080v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.GR</category>
      <category>cs.LG</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Efe Bozkir, S\"uleyman \"Ozdel, Mengdi Wang, Brendan David-John, Hong Gao, Kevin Butler, Eakta Jain, Enkelejda Kasneci</dc:creator>
    </item>
    <item>
      <title>A Meaningful Human Control Perspective on User Perception of Partially Automated Driving Systems: A Case Study of Tesla Users</title>
      <link>https://arxiv.org/abs/2402.08080</link>
      <description>arXiv:2402.08080v2 Announce Type: replace 
Abstract: The use of partially automated driving systems raises concerns about potential responsibility issues, posing risk to the system safety, acceptance, and adoption of these technologies. The concept of meaningful human control has emerged in response to the responsibility gap problem, requiring the fulfillment of two conditions, tracking and tracing. While this concept has provided important philosophical and design insights on automated driving systems, there is currently little knowledge on how meaningful human control relates to subjective experiences of actual users of these systems. To address this gap, our study aimed to investigate the alignment between the degree of meaningful human control and drivers' perceptions of safety and trust in a real-world partially automated driving system. We utilized previously collected data from interviews with Tesla "Full Self-Driving" (FSD) Beta users, investigating the alignment between the user perception and how well the system was tracking the users' reasons. We found that tracking of users' reasons for driving tasks (such as safe maneuvers) correlated with perceived safety and trust, albeit with notable exceptions. Surprisingly, failure to track lane changing and braking reasons was not necessarily associated with negative perceptions of safety. However, the failure of the system to track expected maneuvers in dangerous situations always resulted in low trust and perceived lack of safety. Overall, our analyses highlight alignment points but also possible discrepancies between perceived safety and trust on the one hand, and meaningful human control on the other hand. Our results can help the developers of automated driving technology to design systems under meaningful human control and are perceived as safe and trustworthy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08080v2</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/IV55156.2024.10588431</arxiv:DOI>
      <dc:creator>Lucas Elbert Suryana, Sina Nordhoff, Simeon C. Calvert, Arkady Zgonnikov, Bart van Arem</dc:creator>
    </item>
    <item>
      <title>Visual Grounding Methods for Efficient Interaction with Desktop Graphical User Interfaces</title>
      <link>https://arxiv.org/abs/2407.01558</link>
      <description>arXiv:2407.01558v3 Announce Type: replace 
Abstract: Most visual grounding solutions primarily focus on realistic images. However, applications involving synthetic images, such as Graphical User Interfaces (GUIs), remain limited. This restricts the development of autonomous computer vision-powered artificial intelligence (AI) agents for automatic application interaction. Enabling AI to effectively understand and interact with GUIs is crucial to advancing automation in software testing, accessibility, and human-computer interaction. In this work, we explore Instruction Visual Grounding (IVG), a multi-modal approach to object identification within a GUI. More precisely, given a natural language instruction and a GUI screen, IVG locates the coordinates of the element on the screen where the instruction should be executed. We propose two main methods: (1) IVGocr, which combines a Large Language Model (LLM), an object detection model, and an Optical Character Recognition (OCR) module; and (2) IVGdirect, which uses a multimodal architecture for end-to-end grounding. For each method, we introduce a dedicated dataset. In addition, we propose the Central Point Validation (CPV) metric, a relaxed variant of the classical Central Proximity Score (CPS) metric. Our final test dataset is publicly released to support future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01558v3</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>El Hassane Ettifouri, Jessica L\'opez Espejel, Laura Minkova, Tassnim Dardouri, Walid Dahhane</dc:creator>
    </item>
    <item>
      <title>TR-LLM: Integrating Trajectory Data for Scene-Aware LLM-Based Human Action Prediction</title>
      <link>https://arxiv.org/abs/2410.03993</link>
      <description>arXiv:2410.03993v4 Announce Type: replace 
Abstract: Accurate prediction of human behavior is crucial for AI systems to effectively support real-world applications, such as autonomous robots anticipating and assisting with human tasks. Real-world scenarios frequently present challenges such as occlusions and incomplete scene observations, which can compromise predictive accuracy. Thus, traditional video-based methods often struggle due to limited temporal and spatial perspectives. Large Language Models (LLMs) offer a promising alternative. Having been trained on a large text corpus describing human behaviors, LLMs likely encode plausible sequences of human actions in a home environment. However, LLMs, trained primarily on text data, lack inherent spatial awareness and real-time environmental perception. They struggle with understanding physical constraints and spatial geometry. Therefore, to be effective in a real-world spatial scenario, we propose a multimodal prediction framework that enhances LLM-based action prediction by integrating physical constraints derived from human trajectories. Our experiments demonstrate that combining LLM predictions with trajectory data significantly improves overall prediction performance. This enhancement is particularly notable in situations where the LLM receives limited scene information, highlighting the complementary nature of linguistic knowledge and physical constraints in understanding and anticipating human behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03993v4</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kojiro Takeyama, Yimeng Liu, Misha Sra</dc:creator>
    </item>
    <item>
      <title>From Code to Compliance: Assessing ChatGPT's Utility in Designing an Accessible Webpage -- A Case Study</title>
      <link>https://arxiv.org/abs/2501.03572</link>
      <description>arXiv:2501.03572v2 Announce Type: replace 
Abstract: Web accessibility ensures that individuals with disabilities can access and interact with digital content without barriers, yet a significant majority of most used websites fail to meet accessibility standards. This study evaluates ChatGPT's (GPT-4o) ability to generate and improve web pages in line with Web Content Accessibility Guidelines (WCAG). While ChatGPT can effectively address accessibility issues when prompted, its default code often lacks compliance, reflecting limitations in its training data and prevailing inaccessible web practices. Automated and manual testing revealed strengths in resolving simple issues but challenges with complex tasks, requiring human oversight and additional iterations. Unlike prior studies, we incorporate manual evaluation, dynamic elements, and use the visual reasoning capability of ChatGPT along with the prompts to fix accessibility issues. Providing screenshots alongside prompts enhances the LLM's ability to address accessibility issues by allowing it to analyze surrounding components, such as determining appropriate contrast colors. We found that effective prompt engineering, such as providing concise, structured feedback and incorporating visual aids, significantly enhances ChatGPT's performance. These findings highlight the potential and limitations of large language models for accessible web development, offering practical guidance for developers to create more inclusive websites.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03572v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ammar Ahmed, Margarida Fresco, Fredrik Forsberg, Hallvard Grotli</dc:creator>
    </item>
    <item>
      <title>What Sensors See, What People Feel: Exploring Subjective Collaboration Perception in Mixed Reality</title>
      <link>https://arxiv.org/abs/2504.16373</link>
      <description>arXiv:2504.16373v2 Announce Type: replace 
Abstract: Mixed Reality (MR) enables rich, embodied collaboration, yet it's uncertain if sensor and system-logged behavioral signals capture how users experience that collaboration. This disconnect stems from a fundamental gap: behavioral signals are observable and continuous, while collaboration is interpreted subjectively, shaped by internal states like presence, cognitive availability, and social awareness. Our core insight is that sensor signals serve as observable manifestations of subjective experiences in MR collaboration, and they can be captured through sensor data such as shared gaze, speech, spatial movement, and other system-logged performance metrics. We propose the Sensor-to-Subjective (S2S) Mapping Framework, a conceptual model that links observable interaction patterns to users' subjective perceptions of collaboration and internal cognitive states through sensor-based indicators and task performance metrics. To validate this model, we conducted a study with 48 participants across 12 MR groups engaged in a collaborative image-sorting task. Our findings show a correlation between sensed behavior and perceived collaboration, particularly through shared attention and proximity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16373v2</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yasra Chandio, Diana Romero, Salma Elmalaki, Fatima Anwar</dc:creator>
    </item>
    <item>
      <title>The role of large language models in UI/UX design: A systematic literature review</title>
      <link>https://arxiv.org/abs/2507.04469</link>
      <description>arXiv:2507.04469v2 Announce Type: replace 
Abstract: This systematic literature review examines the role of large language models (LLMs) in UI/UX design, synthesizing findings from 38 peer-reviewed studies published between 2022 and 2025. We identify key LLMs in use, including GPT-4, Gemini, and PaLM, and map their integration across the design lifecycle, from ideation to evaluation. Common practices include prompt engineering, human-in-the-loop workflows, and multimodal input. While LLMs are reshaping design processes, challenges such as hallucination, prompt instability, and limited explainability persist. Our findings highlight LLMs as emerging collaborators in design, and we propose directions for the ethical, inclusive, and effective integration of these technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04469v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ammar Ahmed, Ali Shariq Imran</dc:creator>
    </item>
    <item>
      <title>Deconstructing Implicit Beliefs in Visual Data Journalism: Unstable Meanings Behind Data as Truth &amp; Design for Insight</title>
      <link>https://arxiv.org/abs/2507.12377</link>
      <description>arXiv:2507.12377v3 Announce Type: replace 
Abstract: We conduct a deconstructive reading of a qualitative interview study with 17 visual data journalists from newsrooms across the globe. We borrow a deconstruction approach from literary critique to explore the instability of meaning in language and reveal implicit beliefs in words and ideas. Through our analysis we surface two sets of opposing implicit beliefs in visual data journalism: objectivity/subjectivity and humanism/mechanism. We contextualize these beliefs through a genealogical analysis, which brings deconstruction theory into practice by providing a historic backdrop for these opposing perspectives. Our analysis shows that these beliefs held within visual data journalism are not self-enclosed but rather a product of external societal forces and paradigm shifts over time. Through this work, we demonstrate how thinking with critical theories such as deconstruction and genealogy can reframe "success" in visual data storytelling and diversify visualization research outcomes. These efforts push the ways in which we as researchers produce domain knowledge to examine the sociotechnical issues of today's values towards datafication and data visualization. All supplemental materials for this work are available at osf.io/5fr48.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12377v3</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ke Er Amy Zhang, Jodie Jenkinson, Laura Garrison</dc:creator>
    </item>
    <item>
      <title>Consistency of Responses and Continuations Generated by Large Language Models on Social Media</title>
      <link>https://arxiv.org/abs/2501.08102</link>
      <description>arXiv:2501.08102v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) demonstrate remarkable capabilities in text generation, yet their emotional consistency and semantic coherence in social media contexts remain insufficiently understood. This study investigates how LLMs handle emotional content and maintain semantic relationships through continuation and response tasks using two open-source models: Gemma and Llama. By analyzing climate change discussions from Twitter and Reddit, we examine emotional transitions, intensity patterns, and semantic similarity between human-authored and LLM-generated content. Our findings reveal that while both models maintain high semantic coherence, they exhibit distinct emotional patterns: Gemma shows a tendency toward negative emotion amplification, particularly anger, while maintaining certain positive emotions like optimism. Llama demonstrates superior emotional preservation across a broader spectrum of affects. Both models systematically generate responses with attenuated emotional intensity compared to human-authored content and show a bias toward positive emotions in response tasks. Additionally, both models maintain strong semantic similarity with original texts, though performance varies between continuation and response tasks. These findings provide insights into LLMs' emotional and semantic processing capabilities, with implications for their deployment in social media contexts and human-AI interaction design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08102v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenlu Fan, Yuqi Zhu, Chenyang Wang, Bin Wang, Wentao Xu</dc:creator>
    </item>
    <item>
      <title>AIvaluateXR: An Evaluation Framework for on-Device AI in XR with Benchmarking Results</title>
      <link>https://arxiv.org/abs/2502.15761</link>
      <description>arXiv:2502.15761v2 Announce Type: replace-cross 
Abstract: The deployment of large language models (LLMs) on extended reality (XR) devices has great potential to advance the field of human-AI interaction. In the case of direct, on-device model inference, selecting the appropriate model and device for specific tasks remains challenging. In this paper, we present AIvaluateXR, a comprehensive evaluation framework for benchmarking LLMs running on XR devices. To demonstrate the framework, we deploy 17 selected LLMs across four XR platforms: Magic Leap 2, Meta Quest 3, Vivo X100s Pro, and Apple Vision Pro, and conduct an extensive evaluation. Our experimental setup measures four key metrics: performance consistency, processing speed, memory usage, and battery consumption. For each of the 68 model-device pairs, we assess performance under varying string lengths, batch sizes, and thread counts, analyzing the trade-offs for real-time XR applications. We propose a unified evaluation method based on the 3D Pareto Optimality theory to select the optimal device-model pairs from quality and speed objectives. Additionally, we compare the efficiency of on-device LLMs with client-server and cloud-based setups, and evaluate their accuracy on two interactive tasks. We believe our findings offer valuable insight to guide future optimization efforts for LLM deployment on XR devices. Our evaluation method can be used as standard groundwork for further research and development in this emerging field. The source code and supplementary materials are available at: www.nanovis.org/AIvaluateXR.html</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15761v2</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.GR</category>
      <category>cs.HC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Dawar Khan, Xinyu Liu, Omar Mena, Donggang Jia, Alexandre Kouyoumdjian, Ivan Viola</dc:creator>
    </item>
    <item>
      <title>Strategic Reflectivism In Intelligent Systems</title>
      <link>https://arxiv.org/abs/2505.22987</link>
      <description>arXiv:2505.22987v2 Announce Type: replace-cross 
Abstract: By late 20th century, the rationality wars had launched debates about the nature and norms of intuitive and reflective thinking. Those debates drew from mid-20th century ideas such as bounded rationality, which challenged more idealized notions of rationality observed since the 19th century. Now that 21st century cognitive scientists are applying the resulting dual pro-cess theories to artificial intelligence, it is time to dust off some lessons from this history. So this paper synthesizes old ideas with recent results from experiments on humans and machines. The result is Strategic Reflec-tivism, the position that one key to intelligent systems (human or artificial) is pragmatic switching between intuitive and reflective inference to opti-mally fulfill competing goals. Strategic Reflectivism builds on American Pragmatism, transcends superficial indicators of reflective thinking such as model size or chains of thought, applies to both individual and collective intelligence systems (including human-AI teams), and becomes increasingly actionable as we learn more about the value of intuition and reflection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22987v2</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>econ.TH</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nick Byrd</dc:creator>
    </item>
    <item>
      <title>LearnLens: LLM-Enabled Personalised, Curriculum-Grounded Feedback with Educators in the Loop</title>
      <link>https://arxiv.org/abs/2507.04295</link>
      <description>arXiv:2507.04295v3 Announce Type: replace-cross 
Abstract: Effective feedback is essential for student learning but is time-intensive for teachers. We present LearnLens, a modular, LLM-based system that generates personalised, curriculum-aligned feedback in science education. LearnLens comprises three components: (1) an error-aware assessment module that captures nuanced reasoning errors; (2) a curriculum-grounded generation module that uses a structured, topic-linked memory chain rather than traditional similarity-based retrieval, improving relevance and reducing noise; and (3) an educator-in-the-loop interface for customisation and oversight. LearnLens addresses key challenges in existing systems, offering scalable, high-quality feedback that empowers both teachers and students.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04295v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Runcong Zhao, Artem Bobrov, Jiazheng Li, Yulan He</dc:creator>
    </item>
  </channel>
</rss>
