<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SE</link>
    <description>cs.SE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 21 Jan 2026 05:00:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Reinforcement Learning for Dynamic Workflow Optimization in CI/CD Pipelines</title>
      <link>https://arxiv.org/abs/2601.11647</link>
      <description>arXiv:2601.11647v1 Announce Type: new 
Abstract: Continuous Integration and Continuous Deployment (CI/CD) pipelines are central to modern software delivery, yet their static workflows often introduce inefficiencies as systems scale. This paper proposes a reinforcement learning (RL) based approach to dynamically optimize CI/CD pipeline workflows. The pipeline is modeled as a Markov Decision Process, and an RL agent is trained to make runtime decisions such as selecting full, partial, or no test execution in order to maximize throughput while minimizing testing overhead.
  A configurable CI/CD simulation environment is developed to evaluate the approach across build, test, and deploy stages. Experimental results show that the RL optimized pipeline achieves up to a 30 percent improvement in throughput and approximately a 25 percent reduction in test execution time compared to static baselines, while maintaining a defect miss rate below 5 percent. The agent learns to selectively skip or abbreviate tests for low risk commits, accelerating feedback cycles without significantly increasing failure risk.
  These results demonstrate the potential of reinforcement learning to enable adaptive and intelligent DevOps workflows, providing a practical pathway toward more efficient, resilient, and sustainable CI/CD automation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11647v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aniket Abhishek Soni, Milan Parikh, Rashi Nimesh Kumar Dhenia, Jubin Abhishek Soni, Ayush Raj Jha, Sneja Mitinbhai Shah</dc:creator>
    </item>
    <item>
      <title>Advances and Frontiers of LLM-based Issue Resolution in Software Engineering: A Comprehensive Survey</title>
      <link>https://arxiv.org/abs/2601.11655</link>
      <description>arXiv:2601.11655v1 Announce Type: new 
Abstract: Issue resolution, a complex Software Engineering (SWE) task integral to real-world development, has emerged as a compelling challenge for artificial intelligence. The establishment of benchmarks like SWE-bench revealed this task as profoundly difficult for large language models, thereby significantly accelerating the evolution of autonomous coding agents. This paper presents a systematic survey of this emerging domain. We begin by examining data construction pipelines, covering automated collection and synthesis approaches. We then provide a comprehensive analysis of methodologies, spanning training-free frameworks with their modular components to training-based techniques, including supervised fine-tuning and reinforcement learning. Subsequently, we discuss critical analyses of data quality and agent behavior, alongside practical applications. Finally, we identify key challenges and outline promising directions for future research. An open-source repository is maintained at https://github.com/DeepSoftwareAnalytics/Awesome-Issue-Resolution to serve as a dynamic resource in this field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11655v1</guid>
      <category>cs.SE</category>
      <category>cs.CL</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Caihua Li, Lianghong Guo, Yanlin Wang, Daya Guo, Wei Tao, Zhenyu Shan, Mingwei Liu, Jiachi Chen, Haoyu Song, Duyu Tang, Hongyu Zhang, Zibin Zheng</dc:creator>
    </item>
    <item>
      <title>The Llama 4 Herd: Architecture, Training, Evaluation, and Deployment Notes</title>
      <link>https://arxiv.org/abs/2601.11659</link>
      <description>arXiv:2601.11659v1 Announce Type: new 
Abstract: This document consolidates publicly reported technical details about Metas Llama 4 model family. It summarizes (i) released variants (Scout and Maverick) and the broader herd context including the previewed Behemoth teacher model, (ii) architectural characteristics beyond a high-level MoE description covering routed/shared-expert structure, early-fusion multimodality, and long-context design elements reported for Scout (iRoPE and length generalization strategies), (iii) training disclosures spanning pre-training, mid-training for long-context extension, and post-training methodology (lightweight SFT, online RL, and lightweight DPO) as described in release materials, (iv) developer-reported benchmark results for both base and instruction-tuned checkpoints, and (v) practical deployment constraints observed across major serving environments, including provider-specific context limits and quantization packaging. The manuscript also summarizes licensing obligations relevant to redistribution and derivative naming, and reviews publicly described safeguards and evaluation practices. The goal is to provide a compact technical reference for researchers and practitioners who need precise, source-backed facts about Llama 4.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11659v1</guid>
      <category>cs.SE</category>
      <category>cs.LG</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aaron Adcock, Aayushi Srivastava, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pande, Abhinav Pandey, Abhinav Sharma, Abhishek Kadian, Abhishek Kumawat, Adam Kelsey, Adam Stelle, Adeel Cheema, Adela Kabiljo, Adina Katz, Adithya Gangidi, Aditya Tayade, Adolfo Victoria, Adrian Samatan Alastuey, Adrien Conrath, Afroz Mohiuddin, Ahmed Sharif, Ahnaf Siddiqui, Ahuva Goldstand, Aijung Li, Aidan Boyd, Aidin Kazemi Daliri, Aisha Iqbal, Ajay Menon, Ajit Mathews, Akhil Mathur, Akshat Agarwal, Alan Schelten, Alana Shine, Alejandro Castillejo Mu\~noz, Aleksei Guliaev, Alex Radovic, Alex Song, Alex Vaughan, Alexander Simeonov, Alexandre Rezende, Alexandre Rezende, Alexei Baevski, Alexey Roubaud, Allen Ma, Alvin Lee, Alyssa Pereira, Aman Ahmed, Aman Shankar, Amanda Kallet, Amar Budhiraja, Ameya Khandekar, Amine Benhalloum, Amir Gershman, Amit Nagpal, Amit Zohar, Amr Sharaf, Anant Desai, Anastasia Razdaibiedina, Anca Agape, Andranik Kurghinyan, Andre Perunicic, Andrea Madotto, Andrei Darabanov, Andr\'es Alvarado, Andrew Brown, Andrew Cohen, Andrew Fang, Andrew Freeman, Andrew Gallagher, Andrew Gu, Andrew Prasetyo Jo, Andrew Ryan, Andrew Steffen, Andrew Wei, Andrey Rusakov, Andrii Golovei, Andy Shang, Angela Fan, Angela Fan, Angela Flewellen, Animesh Pathak, Anirudh Goyal, Ankit Ramchandani, Ankur Pai, Ankur Singh, Ankush Garg, Anlu Xing, Anna Cai, Anna Grosul, Anna Prochowska, Anna Sun, Annie Dong, Annie Franco, Anqi Hu, Anshul Chawla, Anthony Hartshorn, Antonia Sheng, Antony Thomas, Anuj Goyal, Anusha De, Anvit Bodiwala, Anvit Bodiwala, Aobo Yang, Aparajita Saraf, Apurva Samudra, Aran Mun, Arash Rahnama, Archi Mitra, Archie Sravankumar, Archit Gupta, Aria Haghighi, Ariel Stolerman, Arkabandhu Chowdhury, Arnab Choudhury, Artem Korenev, Arthur Guo, Arthur Hinsvark, Arun Mallya, Arvind Neelakantan, Arya Talebzadeh, Ashish Shah, Ashmitha Jeevaraj Shetty, Ashwin Bharambe, Asif Islam, Aston Zhang, Austen Gregerson, Avi Lewis, Aya Ibrahim, Ayaz Minhas, Ayelet Dahan, Ayelet Regev Dabah, Bangsheng Tang, Bar Ulman, Bardiya Sadeghi, Bartosz Jedrzejewski, Barys Skarabahaty, Beibei Zhu, Beibin Li, Ben Bharier, Benjamin Leonhardi, Benjamin Muller, Bennett Plessala, Bernie Huang, Beth Loyd, Bhargavi Paranjape, Bhavik Sheth, Bill Bonner, Bill Holland, Bill Wang, Bingzhe Liu, Binh Tang, Bo Liu, Bo Wu, Boduo Li, Bokai Yu, Bor-Chun Chen, Boris Araya, Boris Vidolov, Botao Chen, Boya Peng, Boyu Ni, Bradley Davis, Bram Wasti, Brandon Adams, Brandon Taylor, Brandon Wu, Brant Swidler, Brian Chiang, Brian Clerkin, Brian Fuller, Brooks Cutter, Bruno Novais, Bryan Gmyrek, Bysshe Easton, Cait Campos, Canaan Case, Carl Chengyan Fu, Carly Burton, Caro Diaz, Catherine Cole, Ce Liu, Cedric Fougerat, Cen Peng, Cen Peng, Cen Zhao, Changhan Wang, Changkyu Kim, Chantal Shaib, Chao Zhou, Charlotte Caucheteux, Chau Nguyen, Chawin Sitawarin, Chaya Nayak, Chelsea Asher, Chen Fan, Chen Zhu, Cheng Cheng, Cheng Zhang, Chenguang Zhu, Chengxiong Ruan, Chengzhu Yu, Chenheli Hua, Chenxi Whitehouse, Cheryl Holloway, Ching-Hsiang Chu, Ching-Yao Chuang, Chinmay Karande, Chirag Nagpal, Chlo\'e Bakalar, Chloe Bi, Chris Cai, Chris Marra, Chris McConnell, Chris Thi, Chris Tindal, Chris Waterson, Christian Deverall, Christian Fuegen, Christian Keller, Christine Cheng, Christine Jou, Christine Smith, Christine Wang, Christoph Feichtenhofer, Christophe Touret, Christopher Luc, Christy Sauper, Chuanhao Zhuge, Chun-Yi Sung, Chunqiang Tang, Chunyang Wu, Clara Siegel, Cody Heale, Cody Wilbourn, Colin White, Congying Xia, Corinne Wong, Cornel Rat, Cristian Canton Ferrer, Cyrille Habis, Cyrus Nikolaidis, D Lohachov, Da Ju, Dalton Flanagan, Damien Allonsius, Damon Civin, Dan Johnson, Daniel Bolya, Daniel Francisco, Daniel Fried, Daniel Hawthorne, Daniel Haziza, Daniel Ho, Daniel Kreymer, Daniel Li, Daniel Machlab, Daniel McKinnon, Daniel Obenshain, Daniel Rodriguez, Daniel Song, Daniel Tse, Danielle Pintz, Danny Livshits, Daryl James Rodrigo, Dat Huynh, Daulet Askarov, David Brandfonbrener, David Esiobu, David Kant, David Levin, David Renardy, David Soofian, David Stevens, David Xu, David Zhang, Deep Shah, Delia David, Demi Douglas, Denis Boyda, Desh Raj, Devamanyu Hazarika, Dheeraj Mekala, Dhruv Choudhary, Dhruv Mahajan, Di Jin, Didac Suris Coll-Vinent, Didem Foss, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, DiJia Su, Dilip Madathil, Dinesh Govindasamy, Dinesh Yeduguru, Dmitry Vengertsev, Dong He, Dong Li, Dong Wang, Dongzhuo Li, Duc Le, Dunant Hin, Dustin Holland, Duy Nguyen, Duy Nguyen, Ed Dowling, Eden Litt, Egor Lakomkin, Ehab AlBadawy, Ehsan K. Ardestani, Elad Eckstein, Elahe Dabir, Elaine Montgomery, Elina Lobanova, Elior Abramoviz, Eliot Hedeman, Elissa Li, Elizabeth Hilbert, Ellen Xiaoqing Tan, Elliot Yun, Elodie Stener, Emilian Stoimenov, Emilien Garreau, Emily Dinan, Emily Hahn, Emily Wood, Emma Li, Emmanuel Ademuwagun, Emrah Seker, Eric Alamillo, Eric Gan, Eric Han, Eric Huang, Eric Michael Smith, Eric-Tuan Le, Ernie Chang, Eryk Helenowski, Eslam Elnikety, Esteban Arcaute, Ethan Myers, Eugene Nho, Eugene Poliukhovych, Evan Dunbar, Evgeniy Litvinenko, Evrim Alt{\i}nta\c{s}, Eyal Hochman, Eyal Shtrauch, Fabian Mastenbroek, Faiza Zeb, Faizan Ahmad, Farhad Farahbakhshian, Fei Kou, Fei Sun, Feiyu Chen, Felix Chung, Feng Tian, Feng Xu, Filip Radenovic, Filippos Kokkinos, Francesco Barbieri, Francesco Caggioni, Francisco Esparza, Francisco Guzm\'an, Frank Kanayet, Frank Seide, Frank Zhang, Fred Lewis, Freda Huang, Fulton Wang, Gabriel Synnaeve, Gabriela Jacques-Silva, Gabriella Schwarz, Gaganjit Ghardhora, Gal Elfer, Garrett Dickson, Gaurav Chaurasia, Gautam Sewani, Geet Shingi, Gefei Zuo, Geonhwa Jeong, George Puthanpurackal, Georgia Swee, Gerard Moreno-Torres Bertran, Gil Keren, Gina Ling, Gjergji Stasa, Gobinda Saha, Gor Safran, Gordy French, Goutham Rajendran, Govind Thattai, Grace Cineas, Graeme Nail, Greg Fletcher, Gr\'egoire Mialon, Griffin Adams, Grigory Sizov, Guan Pang, Hady Elsahar, Hai Dang Tran, Hailey Nguyen, Haiping Wu, Hakan Inan, Hamid Eghbalzadeh, Han Fang, Han Zou, Hannah Doyle, Hannah Korevaar, Hannah Wang, Hannah Werbel, Hanwen Zha, Hany Morsy, Hao Ma, Haoci Zhang, Haonan Sun, Haozhu Wang, Hardik Shah, Haroun Habeeb, Harrison Rudolph, Harsh Gupta, Harsh Poddar, Harshil Parikh, Hejia Zhang, Heming Wang, Hengduo Li, Himanshu Sharma, Hoang Phi Nguyen, Hongbo Zhang, Honghao Qiu, Hongjiang Lv, Hongli Xu, Hongyuan Zhan, Hossein Hamooni, Howard Huang, Hu Xu, Hugo Lauren\c{c}on, Hugo Touvron, Hung Dinh, Hunter Goldman, Hussein Mehanna, Huy Nguyen, Hweimi Tsuo, Ian Graves, Ian Yu, Ibrahim Damlaj, Idan Cohen, Igor Tufanov, Ilan Goldenstein, Ilias Leontiadis, Iliyan Zarov, Imad Ahmed, Innocent Djiofack, Iosif Spulber, Irina-Elena Veliche, Isabella Ramos, Ishan Misra, Itai Gal, Ivan Evtimov, Ivan Evtimov, Ivan Obraztsov, Jack Wu, Jacqueline Romero Vertino, Jaemo Koo, Jaewon Lee, Jake Jung, Jake Weissman, James Beldock, James Crnkovich, James Grinage, James Hongyi Zeng, James Kohli, James Tian, Jamie Cahill, Jan Geffert, Jan Seidel, Jan Seidel, Janey Tracey, Jang Hyun Cho, Janice Wei, Jarrod Kahn, Jasmyn Howell, Jason Long Vu, Jason Park, Jason Yan, Jason Yip, Jay Li, Jay Mahadeokar, Jaya Bharath R Goluguri, Jayasi Mehar, Jean-Baptiste Gaya, Jeet Shah, Jeff Hanson, Jeff Marcus, Jeff Walsh, Jeff Yang, Jelmer van der Linde, Jemma Fan, Jennifer Chan, Jenny Zhen, Jenya Lee, Jeremy Fu, Jeremy Reizenstein, Jeremy Teboul, Jesse He, Jessica Zhong, Ji Hou, Ji Yang, Jia Ding, Jiabo Hu, Jiacheng Zhu, Jiadong Guo, Jialiang Wang, Jialin Ouyang, Jianfeng Chi, Jianyu Huang, Jianyun Zhao, Jiaowen Yang, Jiatong Zhou, Jiawei Zhao, Jiawen Liu, Jie Wang, Jie You, Jiecao Yu, Jillian Schwiep, Jilong Wu, Jing Huang, Jing Li, Jing Yu Koh, Jing Zhang, Jingxiang Chen, Jingyi Yang, Jingyue Shen, Jinho Hwang, Jinxi Guo, Jiwan Khatiwada, Joanna Bitton, Joe Li, Joe Quanaim, Joel Beales, Johan Schuijt, John Chang, John Quan, Johnnie Chan, Jon Shepard, Jona Harris, Jonah Rubin, Jonathan Janzen, Jonathan Kaldor, Jorge Lopez Silva, Jose Leitao, Joseph Greer, Joseph Moon, Joseph Rocca, Joseph Tighe, Josh Fromm, Joshua Deng, Joshua Fernandes, Joshua Saxe, Joyce Zheng, Juan Pino, Julien Prigent, Jun Chen, Junjiao Tian, Junjie Qi, Junjie Wang, Junteng Jia, Kade Baker, Kai Londenberg, Kai Wang, Kainan Peng, Kaiyan Peng, Kaiyue Yang, Kalyan Vasudev Alwala, Kam Hou Yu, Kanika Narang, Karan Chadha, Karan Sikka, Karen Zhang, Karina Schuberts, Karishma Mandyam, Karthik Abinav Sankararaman, Karthik Padthe, Karthik Prasad, Karthik Sivakumar, Kartikeya Upasani, Kate Plawiak, Kate Saenko, Kate\v{r}ina \v{Z}mol\'ikov\'a, Kathryn Stadler, Kathy Matosich, Katie Doulgass, Kaveh Hassani, Kay Ji, Ke Li, Kenneth Heafield, Kenny Yu, Keqian Li, Kevin Chih-Yao Ma, Kevin Hannan, Keyu Man, Kezhen Chen, Khalid El-Arini, Khrystyna Hutsulyak, Kieran Nash, Kiran Jagadeesh, Kody Bartelt, Konstantin Topaloglou-Mundy, Konstantinos Chatziioannou, Konstantinos Karanasos, Konstantinos Vougioukas, Kostas Tsiampouris, Kristen Hamill, Kristy Choi, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kun Huang, Kunal Bhalla, Kunal Chawla, Kunpeng Li, Kushal Lakhotia, Kyle Monk, Lakshya Garg, Lalit Chourey, Lars Hamre, Laura Gustafson, Lauren Deason, Laurence Rouesnel, Laurens van der Maaten, Lavender A, Lawrence Chen, Lawrence Jang, Leandro Silva, Leda Sari, Lee Hetherington, Lei Zhang, Leiyu Zhao, Lele Chen, Leo Chenghui Li, Leon Yang, Leon Zhan, Levi Corallo, Liang Tan, Licheng Yu, Lijuan Liu, Lilach Mor, Lincoln Lin, Linfeng Li, Lisa Titus, Liz Jenkins, Lovish Madaan, Lu Fang, Lu Yuan, Lucas Nava, Lucas Pasqualin, Lucas Switzer, Lucia Fang, Lucy Sun, Luka Tadic, Lukas Blecher, Lukas Landzaat, Luxin Zhang, Madhavi Rao, Madian Khabsa, Mahalia Miller, Mahendra Kariya, Mahesh Pasupuleti, Mahi Luthra, Manaal Faruqui, Manav Avlani, Manchen Wang, Mannat Singh, Manohar Paluri, Manoj Chakkaravarthy, Manoj Nair, Maquelle Tiffany, Marcin Pawlowski, Marcus Wu, Maria Lomeli, Mario Consuegra, Marion Boiteux, Marios Andreas Galanis, Marshall Chen, Martin Gleize, Maryam Fazel-Zarandi, Matan Hasson, Mathew Oldham, Mathieu Rita, Matt Dordal, Matt Setzler, Matt Staats, Matt Staats, Matt Wilde, Matthew Clark, Matthew Grange, Matthew Lennie, Matthew Schmohl, Max Raphael, Maxim Naumov, Maxim Samoylov, Maxime Lecanu, Maya Pavlova, Md Taha Bin Jawaid, Meghan Keneally, Melanie Kambadur, Meng Zhang, Mengchen Liu, Mengdi Lin, Mengjiao Wang, Mervyn Abraham, Miao Liu, Michael Au-Yeung, Michael Feldergraf, Michael Man, Michael Matheny, Michael Suo, Michael Tontchev, Michel Meyer, Michelle Ma, Mihir Patel, Mihir Sanjay Kale, Mik Vyatskov, Mikayla Alexander, Mike Andersland, Mike Clark, Mike Lewis, Mike Li, Mike Macey, Mike Macey, Mike Seltzer, Mikel Jimenez Fernandez, Mikhail Antonov, Mikhail Plekhanov, Milan Zhou, Min Si, Ming Qiao, Mingbo Ma, Mingjun Zhang, Mingyi Liang, Miquel Jubert Hermoso, Mirac Suzgun, Mirjam Skarica, Mitesh Kumar Singh, Mohammad Kabbani, Mohammad Rastegari, Mona Sarantakos, Monica Sim, Monika Gangapuram, Mor Moshe, Morrie Doulaty, Morvarid Metanat, Moya Chen, Mrinal Kumar, Munish Bansal, Murali Ramarao, Na Li, Nadav Azaria, Nahiyan Malik, Naman Goyal, Nancy Vargas Balderas, Nanshu Wang, Naoyuki Kanda, Natalia Gimelshein, Natalia Neverova, Nathan Aclander, Natt Sithiviraporn, Navneet Madhu Kumar, Ned Newton, Neeraj Bahl, Negar Ghorbani, Neil Patel, Neta-lee Golan, Nicholas Longenbaugh, Nick Egebo, Nikhil Johri, Nikhil Mehta, Nikhil Naik, Niko Moritz, Nikolay Bashlykov, Nikolay Bogoychev, Nikolay Pavlovich Laptev, Niladri Chatterji, Nile Jones, Nimish Shah, Ning Dong, Ning Li, Ning Li, Ning Zhang, Nishant Yadav, Noam Paz, Norman Cheng, Norman Cheng, Olaoluwa Adesanya, Oleg Repin, Oleksandr Maksymets, Omkar Salpekar, Omri Harosh, Onkar Pednekar, Onur \c{C}elebi, Oran Gafni, Oren Edinger, Osama Hanna, Owais Khan Mohammed, Ozlem Kalinli, Paden Tomasello, Pankaj Singh, Paola Quevedo, Parag Jain, Paria Rashidinejad, Parker Tooley, Parth Parekh, Parth Thakkar, Parvin Taheri, Pasan Hapuarachchi, Pascal Kesseli, Patrick Alrassy, Paulo de Rezende Pinatti, Pavan Balaji, Pawan Sisodiya, Pedro Jose Ferreira Moreira, Pedro Rittner, Pedro Valenzuela, Peize Sun, Peizhao Zhang, Peng-Jen Chen, Pengchao Wang, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Carras, Peter Ney, Peter Weng, Petru Dumea, Phil Hayes, Philip Woods, Pierre Andrews, Pierre M\'enard, Ping-Hao Wu, Pingchuan Liu, Piotr Dollar, Plamen Dzhelepov, Polina Zvyagina, Posten A, Prabhav Agrawal, Pradhapan Rajendran, Pradyot Prakash, Prajjwal Bhargava,  Pramono, Pranay Shah, Pranshu Dave, Prash Jain, Pratik Dubal, Praveen Gollakota, Praveen Krishnan, Pritish Yuvraj, Projjal Ghosh, Punit Singh Koura, Puxin Xu, Qi Qi, Qi Zhou, Qian Guan, Qian Sun, Qiang Liu, Qing He, Qinqing Zheng, Qirui Yang, Qizhen Guo, Quanzeng You, Quentin Carbonneaux, Quentin Carbonneaux, Quentin Duval, Quintin Fettes, Rachad Alao, Rachel Batish, Rachel Guo, Rachel Rodriguez, Radhika Bhargava, Rafael Asuncion, Raghotham Murthy, Rahul Dutta, Rahul Jha, Rahul Kindi, Rahul Mitra, Raj Ganapathy, Raj Shah, Rajarshi Das, Rajat Shrivastava, Rajesh Nishtala, Ramakant Shankar, Raman Shukhau, Ramon Calderer, Rangaprabhu Parthasarathy, Ranjan Subramanian, Raphael Bensadoun, Rares Bostan, Rashnil Chaturvedi, Ravi Agrawal, Ray Gao, Raymond Li, Rebecca Kogen, Ricardo Juan Palma Duran, Ricardo Silveira Cabral, Richard Lee, Richard Yuanzhe Pang, Riddhish Bhalodia, Riham Mansour, Rishabh Singh, Rishi Godugu, Ritun Patney, Rob Boyle, Robbie Goldfarb, Robert Caldwell, Robert Kuo, Roberta Raileanu, Robin Battey, Robin Sharma, Rochit Sapra, Rocky Wang, Rodolfo Granata, Rodrigo De Castro, Rodrigo Paim, Rohan Maheshwari, Rohan Varma, Rohit Girdhar, Rohit Patel, Roshan Sumbaly, Roy Sheaffer, Ruan Silva, Ruben Rodriguez Buchillon, Rui Hou, Ruiming Xie, Ruslan Mavlyutov, Ruslan Semenov, Rustam Dinov, Ruxiao Bao, Ryan Fox, Ryan Kilpatrick, Ryan Kwan, Ryan Lim, Ryan Smith, Saaketh Narayan, Sabrina Qiao, Sachin Mehta, Sachin Siby, Sagar Jain, Saghar Hosseini, Sagie Gur-Ari, Sahana Chennabasappa, Sahin Geyik, Sai Jayesh Bondu, Sai Mounika Chowdhary Nekkalapudi, Saif Hasan, Saisuke Okabayashi, Saketh Rambhatla, Salil Sawhney, Sam Dunster, Sam Zhao, Saman Keon, Samaneh Azadi, Sameet Sapra, Samuel Dooley, Samyak Datta, Sandeep Parab, Sang Michael Xie, Sanjay Singh, Sanyuan Chen, Sara Behn, Sara Khodeir, Sarah Shirazyan, Sargun Dhillon, Sarunya Pumma, Sasha Sidorov, Saskia Adaime, Saurabh Khanna, Sayem Wani, Scott Brenton, Sean Bell, Sean Kelly, Sean Koger, Sean Nunley, Sean Perry, Sebastian Caicedo, Sebastian Dahlgren, Sebastian Ruder, Seiji Yamamoto, Selam Mehretu, Selvan Sunitha Ravi, Sen Lyu, Senthil Chellapan, Serafeim Mellos, Sergey Edunov, Sergey Royt, Shaina Cohen, Shangfu Peng, Shannon Adams, Shaoliang Nie, Sharadh Ramaswamy, Sharan Narang, Shashank Pisupati, Shashi Gandham, Shaun Lim, Shaun Lindsay, Sheena Artrip, Shelly Sheynin, Shen Yan, Sheng Feng, Sheng Shen, Shengbao Zheng, Shenghao Lin, Shengjie Bi, Shengxin Cindy Zha, Shengye Wan, Shengyi Qian, Shengyong Cai, Shengzhi Shao, Shervin Shahidi, Shikai Li, Shimon Bernholtz, Shiqi Wang, Shishir G. Patil, Shiv Verma, Shiva Shankar P, Shiyang Chen, Sho Yaida, Shoubhik Debnath, Shreyas Siravara, Shruti Bhosale, Shuang Ma, Shun Zhang, Shuo Tang, Shuqiang Zhang, Shuyan Zhou, Sicong Che, Sidd Srinivisan, Siddharth Bhattacharya, Siddharth Patki, Sijia Chen, Sili Chen, Simon Vandenhende, Simone Merello, Sinong Wang, Sivan Barzily, Sixian Yi, Siyu Lin, SK Bong, Sky Yin, Sneha Agarwal, Sneha Agarwal, Soerian Lieve, Soji Sajuyigbe, Song Jiang, Songlin Li, Sonia Kim, Sopan Khosla, Soumi Maiti, Spencer Whitman, Sravya Popuri, Sreen Tallam, Srinivas Vaidyanathan, Srinivas Vaidyanathan, Sten Sootla, Stephane Collot, Stephanie Ding, Stephen Chen, Steven Cai, Suchin Gururangan, Sudarshan Govindaprasad, Sue Young, Suganthi Dewakar, Sujan Kumar Gonugondla, Sujeet Bhandari, Suman Gumudavelli, Suman Gumudavelli, Sumit Gupta, Summer Deng, Sungmin Cho, Suresh Ganapathy, Surjyendu Dhal, Susan Fedynak, Susana Contrera, Suyoun Kim, Sylvestre Rebuffi, Takshak Chahande, Tamar Herman, Tan Li, Tao Xu, Tara Fowler, Tarek Sheasha, Tarun Anand, Tarun Kalluri, Tarun Singh, Tatiana Shavrina, Ted Li, Teja Rao, Tejas Patil, Teng Li, Thach Bui, Thai Quach, Thamer Alharbash, Thanh Vinh Vo, Thawan Kooburat, Thilo Koehler, Thomas Georgiou, Thomas Scialom, Tian Ye, Tianhe Li, Tianjun Zhang, Tianyu Li, Tijmen Blankevoort, Timon Willi, Timothy Chou, Timothy Leung, TJ Lee, Todor Mihaylov, Tom Heatwole, Tong Xiao, Tony Cao, Tony Lee, Trang Le, Tristan Rice, Tsz Kei Serena Chan, Tuan Tran, Tudor Tiplea, Tyler Baumgartner, Uday Savagaonkar, Ujjwal Karn, Ulises Martinez Araiza, Umar Farooq, Uriel Cohen, Usman Sharif, Utkarsh Murarka, Van Phung, Varun Joginpalli, Varun Saravagi, Vasu Sharma, Vasudha Viswamurthy, Vedanuj Goswami, Vedika Seth, Venkat Ramesh, Venkat Ramesh, Vibhor Gupta, Victoria Montanez, Vidhya Natarajan, Vidya Sarma, Vignesh Ramanathan, Viktor Kerkez, Vinay Rao, Vincent Gonguet, Vincent Mauge, Virginie Do, Vish Vogeti, Vishrav Chaudhary, Viswesh Sankaran, V\'itor Albiero, Vivek Miglani, Vivek Pai, Vlad Cojanu, Vlad Shubin, Vlad Tiberiu Mihailescu, Vladan Petrovic, Vladimir Ivanov, Vladislav Vorotilov, Vrushali Bhutada, Wai I Ng, Wei Cheng, Wei Sun, Wei Tu, Wei Wei, Wei Zhou, Wei-Ning Hsu, Weiwei Chu, Weizhe Yuan, Wenchen Wang, Wenjun Zhao, Wenwen Jiang, Wenyin Fu, Wenzhe Jiang, Whitney Meers, Will Constable, Will Wang, William R. Wong, Xavier Martinet, Xi Victoria Lin, Xi Yan, Xi Yin, Xian Li, Xianfeng Rui, Xianjun Yang, Xiaocheng Tang, Xiaodong Wang, Xiaofang Wang, Xiaolan Wang, Xiaoliang Dai, Xiaoliang Peng, Xiaopeng Li, Xiaozhu Meng, Xibei Zhang, Xide Xia, Xin Jin, xinbo Gao, Xinfeng Xie, Xingyi Zhou, Xu Ma, Xuan Ju, Xuanyi Zhao, Xubo Liu, Xuchao Jia, Xuedong Zhang, Xuefei Cao, Xuewei Wang, Xuewei Wu, Xunnan Xu, Xutai Ma, Xuyang Wang, Yan Cui, Yang Chen, Yang Li, Yang Shu, Yang Xia, Yanjun Chen, Yanjun Zhou, Yash Mehta, Yash Patel, Yash Tekena, Yashesh Gaur, Yasmine Babaei, Yaxuan Zhou, Ye Hu, Ye Qi, Yejin Lee, Yeming Wen, Yen-Cheng Liu, Yexin Bruce Wu, Yi Pan, Yi Yang, Yi-Hui Lin, Yifan Wang, Yifan Wu, Yifan Yang, Yifei Huang, Yiftah Ben Aharon, Yilin Yang, Yiling You, Ying Xu, Ying Zhang, Yingquan Yuan, Yingru Liu, Yingyi Ma, Yining Yang, Yiting Lu, Yonatan Komornik, Yongjie Lin, Yoni Goyhman, Yossi Moran Mamo, Youngjin Nam, Yu Wang, Yu Lu, Yu Zhao, Yu-Ho Hsieh, Yu-Jung Lo, Yuandong Tian, Yuanhan Zhang, Yuanhao Xiong, Yuanshun Yao, Yuchen Hao, Yuchen Zhang, Yuchuan Li, Yue Cao, Yue Yu, Yue Zhao, Yuhan Guo, Yuhao Wang, Yuheng Huang, Yujie Lu, Yujun Shi, Yulun Wang, Yun He, Yun Wang, Yundi Qian, Yunfan Wang, Yunhao Tang, Yuning Mao, Yunlu Li, Yuqi Dai, Yuriy Hulovatyy, Yushi Hu, Yuxuan Sun, Zach Rait, Zach Wentz, Zacharie Delpierre Coudert, Zachary Collins, Zahra Hankir, Zecheng He, Zeeshan Ahmed, Zeeshan Ahmed, Zef RosnBrick, Zhan Shu, Zhanna Rohalska, Zhaoduo Wen, Zhe Liu, Zhe Liu, Zhen Qiao, Zhenggang Xu, Zhengwen Zhou, Zhengxing Chen, Zhenyu Tang, Zhichen Wu, Zhicheng Ouyang, Zhihong Lei, Zhipeng Hong, Zhiping Xiu, Zhiwei Zhao, Zhong Meng, Zhou Jin, Zhouhao Zeng, Zichang Liu, Zihang Meng, Zihuan Qiao, Zinnia Zheng, Zixi Qi, Ziyi Luo, Zoe Foulkes Birkhead, Zoey Sun, Zohar Achdut</dc:creator>
    </item>
    <item>
      <title>From Everything-is-a-File to Files-Are-All-You-Need: How Unix Philosophy Informs the Design of Agentic AI Systems</title>
      <link>https://arxiv.org/abs/2601.11672</link>
      <description>arXiv:2601.11672v1 Announce Type: new 
Abstract: A core abstraction in early Unix systems was the principle that 'everything is a file', enabling heterogeneous devices and kernel resources to be manipulated via uniform read/write interfaces. This paper explores how an analogous unification is emerging in contemporary agentic AI. We trace the evolution from Unix to DevOps, Infrastructure-as-Code, and finally autonomous software agents, highlighting how file-like abstractions and code-based specifications collapse diverse resources into consistent, composable interfaces. The resulting perspective suggests that adopting file- and code-centric interaction models may enable agentic systems that are more maintainable, auditable, and operationally robust.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11672v1</guid>
      <category>cs.SE</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Deepak Babu Piskala</dc:creator>
    </item>
    <item>
      <title>Semantic Caching and Intent-Driven Context Optimization for Multi-Agent Natural Language to Code Systems</title>
      <link>https://arxiv.org/abs/2601.11687</link>
      <description>arXiv:2601.11687v1 Announce Type: new 
Abstract: We present a production-optimized multi-agent system designed to translate natural language queries into executable Python code for structured data analytics. Unlike systems that rely on expensive frontier models, our approach achieves high accuracy and cost efficiency through three key innovations: (1) a semantic caching system with LLM-based equivalence detection and structured adaptation hints that provides cache hit rates of 67% on production queries; (2) a dual-threshold decision mechanism that separates exact-match retrieval from reference-guided generation; and (3) an intent-driven dynamic prompt assembly system that reduces token consumption by 40-60% through table-aware context filtering. The system has been deployed in production for enterprise inventory management, processing over 10,000 queries with an average latency of 8.2 seconds and 94.3% semantic accuracy. We describe the architecture, present empirical results from production deployment, and discuss practical considerations for deploying LLM-based analytics systems at scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11687v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Harmohit Singh</dc:creator>
    </item>
    <item>
      <title>SpecMap: Hierarchical LLM Agent for Datasheet-to-Code Traceability Link Recovery in Systems Engineering</title>
      <link>https://arxiv.org/abs/2601.11688</link>
      <description>arXiv:2601.11688v1 Announce Type: new 
Abstract: Establishing precise traceability between embedded systems datasheets and their corresponding code implementations remains a fundamental challenge in systems engineering, particularly for low-level software where manual mapping between specification documents and large code repositories is infeasible. Existing Traceability Link Recovery approaches primarily rely on lexical similarity and information retrieval techniques, which struggle to capture the semantic, structural, and symbol level relationships prevalent in embedded systems software. We present a hierarchical datasheet-to-code mapping methodology that employs large language models for semantic analysis while explicitly structuring the traceability process across multiple abstraction levels. Rather than performing direct specification-to-code matching, the proposed approach progressively narrows the search space through repository-level structure inference, file-level relevance estimation, and fine-grained symbollevel alignment. The method extends beyond function-centric mapping by explicitly covering macros, structs, constants, configuration parameters, and register definitions commonly found in systems-level C/C++ codebases. We evaluate the approach on multiple open-source embedded systems repositories using manually curated datasheet-to-code ground truth. Experimental results show substantial improvements over traditional information-retrieval-based baselines, achieving up to 73.3% file mapping accuracy. We significantly reduce computational overhead, lowering total LLM token consumption by 84% and end-to-end runtime by approximately 80%. This methodology supports automated analysis of large embedded software systems and enables downstream applications such as training data generation for systems-aware machine learning models, standards compliance verification, and large-scale specification coverage analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11688v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vedant Nipane, Pulkit Agrawal, Amit Singh</dc:creator>
    </item>
    <item>
      <title>Technical Lag as Latent Technical Debt: A Rapid Review</title>
      <link>https://arxiv.org/abs/2601.11693</link>
      <description>arXiv:2601.11693v1 Announce Type: new 
Abstract: Context: Technical lag accumulates when software systems fail to keep pace with technological advancements, leading to a deterioration in software quality. Objective: This paper aims to consolidate existing research on technical lag, clarify definitions, explore its detection and quantification methods, examine underlying causes and consequences, review current management practices, and lay out a vision as an indicator of passively accumulated technical debt. Method: We conducted a Rapid Review with snowballing to select the appropriate peer-reviewed studies. We leveraged the ACM Digital Library, IEEE Xplore, Scopus, and Springer as our primary source databases. Results: Technical lag accumulates passively, often unnoticed due to inadequate detection metrics and tools. It negatively impacts software quality through outdated dependencies, obsolete APIs, unsupported platforms, and aging infrastructure. Strategies to manage technical lag primarily involve automated dependency updates, continuous integration processes, and regular auditing. Conclusions: Enhancing and extending the current standardized metrics, detection methods, and empirical studies to use technical lag as an indication of accumulated latent debt can greatly improve the process of maintaining large codebases that are heavily dependent on external packages. We have identified the research gaps and outlined a future vision for researchers and practitioners to explore.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11693v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shane K. Panter, Nasir U. Eisty</dc:creator>
    </item>
    <item>
      <title>The Stability Trap: Evaluating the Reliability of LLM-Based Instruction Adherence Auditing</title>
      <link>https://arxiv.org/abs/2601.11783</link>
      <description>arXiv:2601.11783v1 Announce Type: new 
Abstract: The enterprise governance of Generative AI (GenAI) in regulated sectors, such as Human Resources (HR), demands scalable yet reproducible auditing mechanisms. While Large Language Model (LLM)-as-a-Judge approaches offer scalability, their reliability in evaluating adherence of different types of system instructions remains unverified. This study asks: To what extent does the instruction type of an Application Under Test (AUT) influence the stability of judge evaluations? To address this, we introduce the Scoped Instruction Decomposition Framework to classify AUT instructions into Objective and Subjective types, isolating the factors that drive judge instability. We applied this framework to two representative HR GenAI applications, evaluating the stability of four judge architectures over variable runs. Our results reveal a ``Stability Trap'' characterized by a divergence between Verdict Stability and Reasoning Stability. While judges achieved near-perfect verdict agreement ($&gt;99\%$) for both objective and subjective evaluations, their accompanying justification traces diverged significantly. Objective instructions requiring quantitative analysis, such as word counting, exhibited reasoning stability as low as $\approx19\%$, driven by variances in numeric justifications. Similarly, reasoning stability for subjective instructions varied widely ($35\%$--$83\%$) based on evidence granularity, with feature-specific checks failing to reproduce consistent rationale. Conversely, objective instructions focusing on discrete entity extraction achieved high reasoning stability ($&gt;90\%$). These findings demonstrate that high verdict stability can mask fragile reasoning. Thus, we suggest that auditors scope automated evaluation protocols strictly: delegate all deterministically verifiable logic to code, while reserving LLM judges for complex semantic evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11783v1</guid>
      <category>cs.SE</category>
      <category>cs.CL</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Murtuza N. Shergadwala</dc:creator>
    </item>
    <item>
      <title>Changes in Coding Behavior and Performance Since the Introduction of LLMs</title>
      <link>https://arxiv.org/abs/2601.11835</link>
      <description>arXiv:2601.11835v1 Announce Type: new 
Abstract: The widespread availability of large language models (LLMs) has changed how students engage with coding and problem-solving. While these tools may increase student productivity, they also make it more difficult for instructors to assess students' learning and effort. In this quasi-longitudinal study, we analyze five years of student source code submissions in a graduate-level cloud computing course, focusing on an assignment that remained unchanged and examining students' behavior during the period spanning five semesters before the release of ChatGPT and five semesters after.
  Student coding behavior has changed significantly since Fall 2022. The length of their final submissions increased. Between consecutive submissions, average edit distances increased while average score improvement decreased, suggesting that both student productivity and learning have decreased after ChatGPT's release. Additionally, there are statistically significant correlations between these behavioral changes and their overall performance. Although we cannot definitively attribute them to LLM misuse, they are consistent with our hypothesis that some students are over-reliant on LLMs, which is negatively affecting their learning outcomes. Our findings raise an alarm around the first generation of graduates in the age of LLMs, calling upon both educators and employers to reflect on their evaluation methods for genuine expertise and productivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11835v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3785022.3785075</arxiv:DOI>
      <dc:creator>Yufan Zhang, Jaromir Savelka, Seth Goldstein, Michael Conway</dc:creator>
    </item>
    <item>
      <title>Trace Validation of Unmodified Concurrent Systems with OmniLink</title>
      <link>https://arxiv.org/abs/2601.11836</link>
      <description>arXiv:2601.11836v1 Announce Type: new 
Abstract: Concurrent systems are notoriously difficult to validate: subtle bugs may only manifest under rare thread interleavings, and existing tools often require intrusive instrumentation or unrealistic execution models. We present OmniLink, a new methodology for validating concurrent implementations against high-level specifications in TLA+. Unlike prior TLA+ based approaches which use a technique called trace validation, OmniLink treats system events as black boxes with a timebox in which they occurred and a meaning in TLA+, solving for a logical total order of actions. Unlike prior approaches based on linearizability checking, which already solves for total orders of actions with timeboxes, OmniLink uses a flexible specification language, and offers a different linearizability checking method based on off-the-shelf model checking. OmniLink offers different features compared existing linearizability checking tools, and we show that it outperforms the state of the art on large scale validation tasks.
  Our evaluation validates WiredTiger, a state-of-the-art industrial database storage layer, as well as Balanced Augmented Tree (BAT), a state-of-the art lock-free data structure from the research community, and ConcurrentQueue, a popular lock-free queue featuring aggressive performance optimizations. We use OmniLink to improve WiredTiger's existing TLA+ model, as well as develop new TLA+ models that closely match the behavior of the modeled systems, including non-linearizable behaviors. OmniLink is able to find known bugs injected into the systems under test, as well as help discover two previously unknown bugs (1 in BAT, 1 in ConcurrentQueue), which we have confirmed with the authors of those systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11836v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Finn Hackett, Evan Wrench, Peter Macko, A. Jesse Jiryu Davis, Yuanhao Wei, Ivan Beschastnikh</dc:creator>
    </item>
    <item>
      <title>Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces</title>
      <link>https://arxiv.org/abs/2601.11868</link>
      <description>arXiv:2601.11868v1 Announce Type: new 
Abstract: AI agents may soon become capable of autonomously completing valuable, long-horizon tasks in diverse domains. Current benchmarks either do not measure real-world tasks, or are not sufficiently difficult to meaningfully measure frontier models. To this end, we present Terminal-Bench 2.0: a carefully curated hard benchmark composed of 89 tasks in computer terminal environments inspired by problems from real workflows. Each task features a unique environment, human-written solution, and comprehensive tests for verification. We show that frontier models and agents score less than 65\% on the benchmark and conduct an error analysis to identify areas for model and agent improvement. We publish the dataset and evaluation harness to assist developers and researchers in future work at https://www.tbench.ai/ .</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11868v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mike A. Merrill, Alexander G. Shaw, Nicholas Carlini, Boxuan Li, Harsh Raj, Ivan Bercovich, Lin Shi, Jeong Yeon Shin, Thomas Walshe, E. Kelly Buchanan, Junhong Shen, Guanghao Ye, Haowei Lin, Jason Poulos, Maoyu Wang, Marianna Nezhurina, Jenia Jitsev, Di Lu, Orfeas Menis Mastromichalakis, Zhiwei Xu, Zizhao Chen, Yue Liu, Robert Zhang, Leon Liangyu Chen, Anurag Kashyap, Jan-Lucas Uslu, Jeffrey Li, Jianbo Wu, Minghao Yan, Song Bian, Vedang Sharma, Ke Sun, Steven Dillmann, Akshay Anand, Andrew Lanpouthakoun, Bardia Koopah, Changran Hu, Etash Guha, Gabriel H. S. Dreiman, Jiacheng Zhu, Karl Krauth, Li Zhong, Niklas Muennighoff, Robert Amanfu, Shangyin Tan, Shreyas Pimpalgaonkar, Tushar Aggarwal, Xiangning Lin, Xin Lan, Xuandong Zhao, Yiqing Liang, Yuanli Wang, Zilong Wang, Changzhi Zhou, David Heineman, Hange Liu, Harsh Trivedi, John Yang, Junhong Lin, Manish Shetty, Michael Yang, Nabil Omi, Negin Raoof, Shanda Li, Terry Yue Zhuo, Wuwei Lin, Yiwei Dai, Yuxin Wang, Wenhao Chai, Shang Zhou, Dariush Wahdany, Ziyu She, Jiaming Hu, Zhikang Dong, Yuxuan Zhu, Sasha Cui, Ahson Saiyed, Arinbj\"orn Kolbeinsson, Jesse Hu, Christopher Michael Rytting, Ryan Marten, Yixin Wang, Alex Dimakis, Andy Konwinski, Ludwig Schmidt</dc:creator>
    </item>
    <item>
      <title>Harmonica: A Self-Adaptation Exemplar for Sustainable MLOps</title>
      <link>https://arxiv.org/abs/2601.11926</link>
      <description>arXiv:2601.11926v1 Announce Type: new 
Abstract: Machine learning enabled systems (MLS) often operate in settings where they regularly encounter uncertainties arising from changes in their surrounding environment. Without structured oversight, such changes can degrade model behavior, increase operational cost, and reduce the usefulness of deployed systems. Although Machine Learning Operations (MLOps) streamlines the lifecycle of ML models, it provides limited support for addressing runtime uncertainties that influence the longer term sustainability of MLS. To support continued viability, these systems need a mechanism that detects when execution drifts outside acceptable bounds and adjusts system behavior in response. Despite the growing interest in sustainable and self-adaptive MLS, there has been limited work towards exemplars that allow researchers to study these challenges in MLOps pipelines. This paper presents Harmonica, a self-adaptation exemplar built on the HarmonE approach, designed to enable the sustainable operation of such pipelines. Harmonica introduces structured adaptive control through MAPE-K loop, separating high-level adaptation policy from low-level tactic execution. It continuously monitors sustainability metrics, evaluates them against dynamic adaptation boundaries, and automatically triggers architectural tactics when thresholds are violated. We demonstrate the tool through case studies in time series regression and computer vision, examining its ability to improve system stability and reduce manual intervention. The results show that Harmonica offers a practical and reusable foundation for enabling adaptive behavior in MLS that rely on MLOps pipelines for sustained operation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11926v1</guid>
      <category>cs.SE</category>
      <category>cs.LG</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ananya Halgatti, Shaunak Biswas, Hiya Bhatt, Srinivasan Rakhunathan, Karthik Vaidhyanathan</dc:creator>
    </item>
    <item>
      <title>Enhancing Fuzz Testing Efficiency through Automated Fuzz Target Generation</title>
      <link>https://arxiv.org/abs/2601.11972</link>
      <description>arXiv:2601.11972v1 Announce Type: new 
Abstract: Fuzzing continues to be the most effective method for identifying security vulnerabilities in software. In the context of fuzz testing, the fuzzer supplies varied inputs to fuzz targets, which are designed to comprehensively exercise critical sections of the client code. Various studies have focused on optimizing and developing advanced fuzzers, such as AFL++, libFuzzer, Honggfuzz, syzkaller, ISP-Fuzzer, which have substantially enhanced vulnerability detection in widely used software and libraries. Nevertheless, achieving greater coverage necessitates improvements in both the quality and quantity of fuzz targets. In large-scale software projects and libraries -- characterized by numerous user defined functions and data types -- manual creation of fuzz targets is both labor-intensive and time-consuming. This challenge underscores the need for automated techniques not only to generate fuzz targets but also to streamline the execution and analysis of their results. In this paper, we introduce an approach to improving fuzz target generation through static analysis of library source code. The proposed method encompasses several key aspects: it analyzes source code structures to accurately construct function calls and generate fuzz targets; it maps fuzzer input data to the corresponding function parameters; it synthesizes compilation information for the fuzz targets; and it automatically collects and analyzes execution results. Our findings are demonstrated through the application of this approach to the generation of fuzz targets for C/C++ libraries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11972v1</guid>
      <category>cs.SE</category>
      <category>cs.CR</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1134/S0361768825700227</arxiv:DOI>
      <arxiv:journal_reference>Programming and Computer Software, vol. 51, no. 2, 2025</arxiv:journal_reference>
      <dc:creator>Chi Thien Tran</dc:creator>
    </item>
    <item>
      <title>From LLMs to Agents in Programming: The Impact of Providing an LLM with a Compiler</title>
      <link>https://arxiv.org/abs/2601.12146</link>
      <description>arXiv:2601.12146v1 Announce Type: new 
Abstract: Large Language Models have demonstrated a remarkable capability in natural language and program generation and software development. However, the source code generated by the LLMs does not always meet quality requirements and may fail to compile. Therefore, many studies evolve into agents that can reason about the problem before generating the source code for the solution. The goal of this paper is to study the degree to which such agents benefit from access to software development tools, in our case, a \texttt{gcc} compiler. We conduct a computational experiment on the RosettaCode dataset, on 699 programming tasks in C. We evaluate how the integration with a compiler shifts the role of the language model from a passive generator to an active agent capable of iteratively developing runnable programs based on feedback from the compiler. We evaluated 16 language models with sizes ranging from small (135 million) to medium (3 billion) and large (70 billion). Our results show that access to a compiler improved the compilation success by 5.3 to 79.4 percentage units in compilation without affecting the semantics of the generated program. Syntax errors dropped by 75\%, and errors related to undefined references dropped by 87\% for the tasks where the agents outperformed the baselines. We also observed that in some cases, smaller models with a compiler outperform larger models with a compiler. We conclude that it is essential for LLMs to have access to software engineering tools to enhance their performance and reduce the need for large models in software engineering, such as reducing our energy footprint.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12146v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Viktor Kjellberg, Miroslaw Staron, Farnaz Fotrousi</dc:creator>
    </item>
    <item>
      <title>Many Hands Make Light Work: An LLM-based Multi-Agent System for Detecting Malicious PyPI Packages</title>
      <link>https://arxiv.org/abs/2601.12148</link>
      <description>arXiv:2601.12148v1 Announce Type: new 
Abstract: Malicious code in open-source repositories such as PyPI poses a growing threat to software supply chains. Traditional rule-based tools often overlook the semantic patterns in source code that are crucial for identifying adversarial components. Large language models (LLMs) show promise for software analysis, yet their use in interpretable and modular security pipelines remains limited. This paper presents LAMPS, a multi-agent system that employs collaborative LLMs to detect malicious PyPI packages. The system consists of four role-specific agents for package retrieval, file extraction, classification, and verdict aggregation, coordinated through the CrewAI framework. A prototype combines a fine-tuned CodeBERT model for classification with LLaMA-3 agents for contextual reasoning. LAMPS has been evaluated on two complementary datasets: D1, a balanced collection of 6,000 setup.py files, and D2, a realistic multi-file dataset with 1,296 files and natural class imbalance. On D1, LAMPS achieves 97.7% accuracy, surpassing MPHunter--one of the state-of-the-art approaches. On D2, it reaches 99.5% accuracy and 99.5% balanced accuracy, outperforming RAG-based approaches and fine-tuned single-agent baselines. McNemar's test confirmed these improvements as highly significant. The results demonstrate the feasibility of distributed LLM reasoning for malicious code detection and highlight the benefits of modular multi-agent designs in software supply chain security.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12148v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Muhammad Umar Zeshan, Motunrayo Ibiyo, Claudio Di Sipio, Phuong T. Nguyen, Davide Di Ruscio</dc:creator>
    </item>
    <item>
      <title>Aletheia: What Makes RLVR For Code Verifiers Tick?</title>
      <link>https://arxiv.org/abs/2601.12186</link>
      <description>arXiv:2601.12186v1 Announce Type: new 
Abstract: Multi-domain thinking verifiers trained via Reinforcement Learning from Verifiable Rewards (RLVR) are a prominent fixture of the Large Language Model (LLM) post-training pipeline, owing to their ability to robustly rate and rerank model outputs. However, the adoption of such verifiers towards code generation has been comparatively sparse, with execution feedback constituting the dominant signal. Nonetheless, code verifiers remain valuable toward judging model outputs in scenarios where execution feedback is hard to obtain and are a potentially powerful addition to the code generation post-training toolbox. To this end, we create and open-source Aletheia, a controlled testbed that enables execution-grounded evaluation of code verifiers' robustness across disparate policy models and covariate shifts. We examine components of the RLVR-based verifier training recipe widely credited for its success: (1) intermediate thinking traces, (2) learning from negative samples, and (3) on-policy training. While experiments show the optimality of RLVR, we uncover important opportunities to simplify the recipe. Particularly, despite code verification exhibiting positive training- and inference-time scaling, on-policy learning stands out as the key component at small verifier sizes, and thinking-based training emerges as the most important component at larger scales.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12186v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vatsal Venkatkrishna, Indraneil Paul, Iryna Gurevych</dc:creator>
    </item>
    <item>
      <title>Environment-Aware Code Generation: How far are We?</title>
      <link>https://arxiv.org/abs/2601.12262</link>
      <description>arXiv:2601.12262v1 Announce Type: new 
Abstract: Recent progress in large language models (LLMs) has improved code generation, but most evaluations still test isolated, small-scale code (e.g., a single function) under default or unspecified software environments. As a result, it is unclear whether LLMs can reliably generate executable code tailored to a user's specific environment. We present the first systematic study of Environment-Aware Code Generation (EACG), where generated code must be functionally correct and directly executable under arbitrary software configurations. To enable realistic evaluation, we introduce VersiBCB, a benchmark that is multi-package, execution-verified, and deprecation-aware, capturing complex and evolving environments that prior datasets often overlook. Using VersiBCB, we investigate three complementary adaptation axes: data, parameters, and cache, and develop representative strategies for each. Our results show that current LLMs struggle with environment-specific code generation, while our adaptations improve environment compatibility and executability. These findings highlight key challenges and opportunities for deploying LLMs in practical software engineering workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12262v1</guid>
      <category>cs.SE</category>
      <category>cs.CL</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tongtong Wu, Rongyi Chen, Wenjie Du, Suyu Ma, Guilin Qi, Zhenchang Xing, Shahram Khadivi, Ramesh Periyathambi, Gholamreza Haffari</dc:creator>
    </item>
    <item>
      <title>Leveraging Mutation Analysis for LLM-based Repair of Quantum Programs</title>
      <link>https://arxiv.org/abs/2601.12273</link>
      <description>arXiv:2601.12273v1 Announce Type: new 
Abstract: In recent years, Automated Program Repair (APR) techniques specifically designed for quantum programs have been proposed. However, existing approaches often suffer from low repair success rates or poor understandability of the generated patches. In this study, we construct a framework in which a large language model (LLM) generates code repairs along with a natural language explanation of the applied repairs. To investigate how the contextual information included in prompts influences APR performance for quantum programs, we design four prompt configurations with different combinations of static information, dynamic information, and mutation analysis results. Mutation analysis evaluates how small changes to specific parts of a program affect its execution results and provides more detailed dynamic information than simple execution outputs such as stack traces. Our experimental results show that mutation analysis can provide valuable contextual information for LLM-based APR of quantum programs, improving repair success rates (achieving 94.4% in our experiment) and in some cases also improving the quality of generated explanations. Our findings point toward new directions for developing APR techniques for quantum programs that enhance both reliability and explainability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12273v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chihiro Yoshida, Yuta Ishimoto, Olivier Nourry, Masanari Kondo, Makoto Matsushita, Yasutaka Kamei, Yoshiki Higo</dc:creator>
    </item>
    <item>
      <title>Hybrid Concolic Testing with Large Language Models for Guided Path Exploration</title>
      <link>https://arxiv.org/abs/2601.12274</link>
      <description>arXiv:2601.12274v1 Announce Type: new 
Abstract: Concolic testing, a powerful hybrid software testing technique, has historically been plagued by fundamental limitations such as path explosion and the high cost of constraint solving, which hinder its practical application in large-scale, real-world software systems. This paper introduces a novel algorithmic framework that synergistically integrates concolic execution with Large Language Models (LLMs) to overcome these challenges. Our hybrid approach leverages the semantic reasoning capabilities of LLMs to guide path exploration, prioritize interesting execution paths, and assist in constraint solving. We formally define the system architecture and algorithms that constitute this new paradigm. Through a series of experiments on both synthetic and real-world Fintech applications, we demonstrate that our approach significantly outperforms traditional concolic testing, random testing, and genetic algorithm-based methods in terms of branch coverage, path coverage, and time-to-coverage. The results indicate that by combining the strengths of both concolic execution and LLMs, our method achieves a more efficient and effective exploration of the program state space, leading to improved bug detection capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12274v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mahdi Eslamimehr</dc:creator>
    </item>
    <item>
      <title>The Expert Validation Framework (EVF): Enabling Domain Expert Control in AI Engineering</title>
      <link>https://arxiv.org/abs/2601.12327</link>
      <description>arXiv:2601.12327v1 Announce Type: new 
Abstract: Generative AI (GenAI) systems promise to transform knowledge work by automating a range of tasks, yet their deployment in enterprise settings remains hindered by the lack of systematic quality assurance mechanisms. We present an Expert Validation Framework that places domain experts at the center of building software with GenAI components, enabling them to maintain authoritative control over system behavior through structured specification, testing, validation, and continuous monitoring processes. Our framework addresses the critical gap between AI capabilities and organizational trust by establishing a rigorous, expert-driven methodology for ensuring quality across diverse GenAI applications. Through a four-stage implementation process encompassing specification, system creation, validation, and production monitoring, the framework enables organizations to leverage GenAI capabilities while maintaining expert oversight and quality standards.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12327v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>CAIN2026: 5th International Conference on AI Engineering - Software Engineering for AI</arxiv:journal_reference>
      <dc:creator>Lucas Gren, Felix Dobslaw</dc:creator>
    </item>
    <item>
      <title>Discovering 100+ Compiler Defects in 72 Hours via LLM-Driven Semantic Logic Recomposition</title>
      <link>https://arxiv.org/abs/2601.12360</link>
      <description>arXiv:2601.12360v1 Announce Type: new 
Abstract: Compilers constitute the foundational root-of-trust in software supply chains; however, their immense complexity inevitably conceals critical defects. Recent research has attempted to leverage historical bugs to design new mutation operators or fine-tune models to increase program diversity for compiler fuzzing.We observe, however, that bugs manifest primarily based on the semantics of input programs rather than their syntax. Unfortunately, current approaches, whether relying on syntactic mutation or general Large Language Model (LLM) fine-tuning, struggle to preserve the specific semantics found in the logic of bug-triggering programs. Consequently, these critical semantic triggers are often lost, resulting in a limitation of the diversity of generated programs.
  To explicitly reuse such semantics, we propose FeatureFuzz, a compiler fuzzer that combines features to generate programs. We define a feature as a decoupled primitive that encapsulates a natural language description of a bug-prone invariant, such as an out-of-bounds array access, alongside a concrete code witness of its realization. FeatureFuzz operates via a three-stage workflow: it first extracts features from historical bug reports, synthesizes coherent groups of features, and finally instantiates these groups into valid programs for compiler fuzzing.
  We evaluated FeatureFuzz on GCC and LLVM. Over 24-hour campaigns, FeatureFuzz uncovered 167 unique crashes, which is 2.78x more than the second-best fuzzer. Furthermore, through a 72-hour fuzzing campaign, FeatureFuzz identified 106 bugs in GCC and LLVM, 76 of which have already been confirmed by compiler developers, validating the approach's ability to stress-test modern compilers effectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12360v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinabang He, Yuanwei Chen, Hao Wu, Jikang Zhang, Zicheng Wang, Ligeng Chen, Junjie Peng, Haiyang Wei, Yi Qian, Tiantai Zhang, Linzhang Wang, Bing Mao</dc:creator>
    </item>
    <item>
      <title>Evaluating Large Language Models for Time Series Anomaly Detection in Aerospace Software</title>
      <link>https://arxiv.org/abs/2601.12448</link>
      <description>arXiv:2601.12448v1 Announce Type: new 
Abstract: Time series anomaly detection (TSAD) is essential for ensuring the safety and reliability of aerospace software systems. Although large language models (LLMs) provide a promising training-free alternative to unsupervised approaches, their effectiveness in aerospace settings remains under-examined because of complex telemetry, misaligned evaluation metrics, and the absence of domain knowledge. To address this gap, we introduce ATSADBench, the first benchmark for aerospace TSAD. ATSADBench comprises nine tasks that combine three pattern-wise anomaly types, univariate and multivariate signals, and both in-loop and out-of-loop feedback scenarios, yielding 108,000 data points. Using this benchmark, we systematically evaluate state-of-the-art open-source LLMs under two paradigms: Direct, which labels anomalies within sliding windows, and Prediction-Based, which detects anomalies from prediction errors. To reflect operational needs, we reformulate evaluation at the window level and propose three user-oriented metrics: Alarm Accuracy (AA), Alarm Latency (AL), and Alarm Contiguity (AC), which quantify alarm correctness, timeliness, and credibility. We further examine two enhancement strategies, few-shot learning and retrieval-augmented generation (RAG), to inject domain knowledge. The evaluation results show that (1) LLMs perform well on univariate tasks but struggle with multivariate telemetry, (2) their AA and AC on multivariate tasks approach random guessing, (3) few-shot learning provides modest gains whereas RAG offers no significant improvement, and (4) in practice LLMs can detect true anomaly onsets yet sometimes raise false alarms, which few-shot prompting mitigates but RAG exacerbates. These findings offer guidance for future LLM-based TSAD in aerospace software.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12448v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Liu, Yixing Luo, Xiaofeng Li, Xiaogang Dong, Bin Gu, Zhi Jin</dc:creator>
    </item>
    <item>
      <title>Improved Bug Localization with AI Agents Leveraging Hypothesis and Dynamic Cognition</title>
      <link>https://arxiv.org/abs/2601.12522</link>
      <description>arXiv:2601.12522v1 Announce Type: new 
Abstract: Software bugs cost technology providers (e.g., AT&amp;T) billions annually and cause developers to spend roughly 50% of their time on bug resolution. Traditional methods for bug localization often analyze the suspiciousness of code components (e.g., methods, documents) in isolation, overlooking their connections with other components in the codebase. Recent advances in Large Language Models (LLMs) and agentic AI techniques have shown strong potential for code understanding, but still lack causal reasoning during code exploration and struggle to manage growing context effectively, limiting their capability. In this paper, we present a novel agentic technique for bug localization -- CogniGent -- that overcomes the limitations above by leveraging multiple AI agents capable of causal reasoning, call-graph-based root cause analysis and context engineering. It emulates developers-inspired debugging practices (a.k.a., dynamic cognitive debugging) and conducts hypothesis testing to support bug localization. We evaluate CogniGent on a curated dataset of 591 bug reports using three widely adopted performance metrics and compare it against six established baselines from the literature. Experimental results show that our technique consistently outperformed existing traditional and LLM-based techniques, achieving MAP improvements of 23.33-38.57% at the document and method levels. Similar gains were observed in MRR, with increases of 25.14-53.74% at both granularity levels. Statistical significance tests also confirm the superiority of our technique. By addressing the reasoning, dependency, and context limitations, CogniGent advances the state of bug localization, bridging human-like cognition with agentic automation for improved performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12522v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Asif Mohammed Samir, Mohammad Masudur Rahman</dc:creator>
    </item>
    <item>
      <title>Automated Tool Support for Category-Partition Testing: Design Decisions, UI and Examples of Use</title>
      <link>https://arxiv.org/abs/2601.12559</link>
      <description>arXiv:2601.12559v1 Announce Type: new 
Abstract: Category-Partition is a functional testing technique that is based on the idea that the input domain of the system under test can be divided into sub-domains, with the assumption that inputs that belong to the same sub-domain trigger a similar behaviour and that therefore it is sufficient to select one input from each sub-domain. Category-Partition proceeds in several steps, from the identification of so-called categories and choices, possibly constrained, which are subsequently used to form test frames, i.e., combinations of choices, and eventually test cases. This paper reports on an ongoing attempt to automate as many of those steps as possible, with graphical-user interface tool support. Specifically, the user interface allows the user to specify parameters as well as so-called environment variables, further specify categories and choices with optional constraints. Choices are provided with precise specifications with operations specific to their types (e.g., Boolean, Integer, Real, String). Then, the tool automates the construction of test frames, which are combinations of choices, according to alternative selection criteria, and the identification of input values for parameters and environment variables for these test frames, thereby producing test cases. The paper illustrates the capabilities of the tool with the use of nine different case studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12559v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yvan Labiche</dc:creator>
    </item>
    <item>
      <title>OpenAI for OpenAPI: Automated generation of REST API specification via LLMs</title>
      <link>https://arxiv.org/abs/2601.12735</link>
      <description>arXiv:2601.12735v1 Announce Type: new 
Abstract: REST APIs, based on the REpresentational State Transfer (REST) architecture, are the primary type of Web API. The OpenAPI Specification (OAS) serves as the de facto standard for describing REST APIs and is crucial for multiple software engineering tasks. However, developers face challenges in writing and maintaining OAS. Although static analysis shows potential for OAS generation, it is limited to specific programming languages and development frameworks. The powerful code understanding capabilities of LLMs offer new opportunities for OAS generation, yet they are constrained by context limitations and hallucinations. To address these challenges, we propose the OpenAI OpenAPI Project Scanner (OOPS), the first technology-agnostic LLM-based static analysis method for OAS generation, requiring fewer technology-specific rules and less human expert intervention. OOPS is implemented as an LLM agent workflow comprising two key steps: endpoint method extraction and OAS generation. By constructing an API dependency graph, it establishes necessary file associations to address LLMs' context limitations. Through multi-stage generation and self-refine, it mitigates both syntactic and semantic hallucinations during OAS generation. We evaluated OOPS on 12 real-world REST APIs spanning 5 programming languages and 8 development frameworks. Experimental results demonstrate that OOPS accurately generates high-quality OAS for REST APIs implemented with diverse technologies, achieving an average F1-score exceeding 98% for endpoint method inference, 97% for both request parameter and response inference, and 92% for parameter constraint inference. The input tokens average below 5.6K with a maximum of 16.2K, while the output tokens average below 0.9K with a maximum of 7.7K.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12735v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao Chen, Yunchun Li, Chen Chen, Fengxu Lin, Wei Li</dc:creator>
    </item>
    <item>
      <title>Teaching LLMs to Learn Tool Trialing and Execution through Environment Interaction</title>
      <link>https://arxiv.org/abs/2601.12762</link>
      <description>arXiv:2601.12762v1 Announce Type: new 
Abstract: Equipping Large Language Models (LLMs) with external tools enables them to solve complex real-world problems. However, the robustness of existing methods remains a critical challenge when confronting novel or evolving tools. Existing trajectory-centric paradigms primarily rely on memorizing static solution paths during training, which limits the ability of LLMs to generalize tool usage to newly introduced or previously unseen tools. In this paper, we propose ToolMaster, a framework that shifts tool use from imitating golden tool-calling trajectories to actively learning tool usage through interaction with the environment. To optimize LLMs for tool planning and invocation, ToolMaster adopts a trial-and-execution paradigm, which trains LLMs to first imitate teacher-generated trajectories containing explicit tool trials and self-correction, followed by reinforcement learning to coordinate the trial and execution phases jointly. This process enables agents to autonomously explore correct tool usage by actively interacting with environments and forming experiential knowledge that benefits tool execution. Experimental results demonstrate that ToolMaster significantly outperforms existing baselines in terms of generalization and robustness across unseen or unfamiliar tools. All code and data are available at https://github.com/NEUIR/ToolMaster.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12762v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xingjie Gao, Pengcheng Huang, Zhenghao Liu, Yukun Yan, Shuo Wang, Zulong Chen, Chen Qian, Ge Yu, Yu Gu</dc:creator>
    </item>
    <item>
      <title>Docker Does Not Guarantee Reproducibility</title>
      <link>https://arxiv.org/abs/2601.12811</link>
      <description>arXiv:2601.12811v1 Announce Type: new 
Abstract: The reproducibility of software environments is a critical concern in modern software engineering, with ramifications ranging from the effectiveness of collaboration workflows to software supply chain security and scientific reproducibility. Containerization technologies like Docker address this problem by encapsulating software environments into shareable filesystem snapshots known as images. While Docker is frequently cited in the literature as a tool that enables reproducibility in theory, the extent of its guarantees and limitations in practice remains under-explored.
  In this work, we address this gap through two complementary approaches. First, we conduct a systematic literature review to examine how Docker is framed in scientific discourse on reproducibility and to identify documented best practices for writing Dockerfiles enabling reproducible image building. Then, we perform a large-scale empirical study of 5298 Docker builds collected from GitHub workflows. By rebuilding these images and comparing the results with their historical counterparts, we assess the real reproducibility of Docker images and evaluate the effectiveness of the best practices identified in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12811v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julien Malka, Stefano Zacchiroli, Th\'eo Zimmermann</dc:creator>
    </item>
    <item>
      <title>Automatic Generation of Formal Specification and Verification Annotations Using LLMs and Test Oracles</title>
      <link>https://arxiv.org/abs/2601.12845</link>
      <description>arXiv:2601.12845v1 Announce Type: new 
Abstract: Recent verification tools aim to make formal verification more accessible to software engineers by automating most of the verification process. However, annotating conventional programs with the formal specification and verification constructs (preconditions, postconditions, loop invariants, auxiliary predicates and functions and proof helpers) required to prove their correctness still demands significant manual effort and expertise. This paper investigates how LLMs can automatically generate such annotations for programs written in Dafny, a verification-aware programming language, starting from conventional code accompanied by natural language specifications (in comments) and test code. In experiments on 110 Dafny programs, a multimodel approach combining Claude Opus 4.5 and GPT-5.2 generated correct annotations for 98.2% of the programs within at most 8 repair iterations, using verifier feedback. A logistic regression analysis shows that proof-helper annotations contribute disproportionately to problem difficulty for current LLMs. Assertions in the test cases served as static oracles to automatically validate the generated pre/postconditions. We also compare generated and manual solutions and present an extension for Visual Studio Code to incorporate automatic generation into the IDE, with encouraging usability feedback.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12845v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jo\~ao Pascoal Faria, Emanuel Trigo, Vinicius Honorato, Rui Abreu</dc:creator>
    </item>
    <item>
      <title>Efficient Code Analysis via Graph-Guided Large Language Models</title>
      <link>https://arxiv.org/abs/2601.12890</link>
      <description>arXiv:2601.12890v1 Announce Type: new 
Abstract: Malicious behavior is often hidden in small, easily overlooked code fragments, especially within large and complex codebases. The cross-file dependencies of these fragments make it difficult for even powerful large language models (LLMs) to detect them reliably. We propose a graph-centric attention acquisition pipeline that enhances LLMs' ability to localize malicious behavior. The approach parses a project into a code graph, uses an LLM to encode nodes with semantic and structural signals, and trains a Graph Neural Network (GNN) under sparse supervision. The GNN performs an initial detection, and through backtracking of its predictions, identifies key code sections that are most likely to contain malicious behavior. These influential regions are then used to guide the LLM's attention for in-depth analysis. This strategy significantly reduces interference from irrelevant context while maintaining low annotation costs. Extensive experiments show that the method consistently outperforms existing methods on multiple public and self-built datasets, highlighting its potential for practical deployment in software security scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12890v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hang Gao, Tao Peng, Baoquan Cui, Hong Huang, Fengge Wu, Junsuo Zhao, Jian Zhang</dc:creator>
    </item>
    <item>
      <title>A Benchmark for Language Models in Real-World System Building</title>
      <link>https://arxiv.org/abs/2601.12927</link>
      <description>arXiv:2601.12927v1 Announce Type: new 
Abstract: During migration across instruction set architectures (ISAs), software package build repair is a critical task for ensuring the reliability of software deployment and the stability of modern operating systems. While Large Language Models (LLMs) have shown promise in tackling this challenge, prior work has primarily focused on single instruction set architecture (ISA) and homogeneous programming languages. To address this limitation, we introduce a new benchmark designed for software package build repair across diverse architectures and languages. Comprising 268 real-world software package build failures, the benchmark provides a standardized evaluation pipeline. We evaluate six state-of-the-art LLMs on the benchmark, and the results show that cross-ISA software package repair remains difficult and requires further advances. By systematically exposing this challenge, the benchmark establishes a foundation for advancing future methods aimed at improving software portability and bridging architectural gaps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12927v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weilin Jin, Chenyu Zhao, Zeshun Huang, Chaoyun Zhang, Qingwei Lin, Chetan Bansal, Saravan Rajmohan, Shenglin Zhang, Yongqian Sun, Dan Pei, Yifan Wu, Tong Jia, Ying Li, Zhonghai Wu, Minghua Ma</dc:creator>
    </item>
    <item>
      <title>Beyond Accuracy: Characterizing Code Comprehension Capabilities in (Large) Language Models</title>
      <link>https://arxiv.org/abs/2601.12951</link>
      <description>arXiv:2601.12951v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly integrated into software engineering workflows, yet current benchmarks provide only coarse performance summaries that obscure the diverse capabilities and limitations of these models. This paper investigates whether LLMs' code-comprehension performance aligns with traditional human-centric software metrics or instead reflects distinct, non-human regularities. We introduce a diagnostic framework that reframes code understanding as a binary input-output consistency task, enabling the evaluation of classification and generative models. Using a large-scale dataset, we correlate model performance with traditional, human-centric complexity metrics, such as lexical size, control-flow complexity, and abstract syntax tree structure. Our analyses reveal minimal correlation between human-defined metrics and LLM success (AUROC 0.63), while shadow models achieve substantially higher predictive performance (AUROC 0.86), capturing complex, partially predictable patterns beyond traditional software measures. These findings suggest that LLM comprehension reflects model-specific regularities only partially accessible through either human-designed or learned features, emphasizing the need for benchmark methodologies that move beyond aggregate accuracy and toward instance-level diagnostics, while acknowledging fundamental limits in predicting correct outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12951v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Felix M\"achtle, Jan-Niclas Serr, Nils Loose, Thomas Eisenbarth</dc:creator>
    </item>
    <item>
      <title>ArchAgent: Scalable Legacy Software Architecture Recovery with LLMs</title>
      <link>https://arxiv.org/abs/2601.13007</link>
      <description>arXiv:2601.13007v1 Announce Type: new 
Abstract: Recovering accurate architecture from large-scale legacy software is hindered by architectural drift, missing relations, and the limited context of Large Language Models (LLMs). We present ArchAgent, a scalable agent-based framework that combines static analysis, adaptive code segmentation, and LLM-powered synthesis to reconstruct multiview, business-aligned architectures from cross-repository codebases. ArchAgent introduces scalable diagram generation with contextual pruning and integrates cross-repository data to identify business-critical modules. Evaluations of typical large-scale GitHub projects show significant improvements over existing benchmarks. An ablation study confirms that dependency context improves the accuracy of generated architectures of production-level repositories, and a real-world case study demonstrates effective recovery of critical business logics from legacy projects. The dataset is available at https://github.com/panrusheng/arch-eval-benchmark.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13007v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rusheng Pan, Bingcheng Mao, Tianyi Ma, Zhenhua Ling</dc:creator>
    </item>
    <item>
      <title>MeltRTL: Multi-Expert LLMs with Inference-time Intervention for RTL Code Generation</title>
      <link>https://arxiv.org/abs/2601.13015</link>
      <description>arXiv:2601.13015v1 Announce Type: new 
Abstract: The automated generation of hardware register-transfer level (RTL) code with large language models (LLMs) shows promise, yet current solutions struggle to produce syntactically and functionally correct code for complex digital designs. This paper introduces MeltRTL, a novel framework that integrates multi-expert attention with inference-time intervention (ITI) to significantly improve LLM-based RTL code generation accuracy without retraining the base model. MeltRTL introduces three key innovations: (1) A multi-expert attention architecture that dynamically routes design specifications to specialized expert networks, enabling targeted reasoning across various hardware categories; (2) An inference-time intervention mechanism that employs non-linear probes to detect and correct hardware-specific inaccuracies during generation; and (3) An efficient intervention framework that selectively operates on expert-specific attention heads with minimal computational overhead. We evaluate MeltRTL on the VerilogEval benchmark, achieving 96% synthesizability and 60% functional correctness, compared to the base LLM's 85.3% and 45.3%, respectively. These improvements are obtained entirely at inference time, with only 27% computational overhead and no model fine-tuning, making MeltRTL immediately deployable on existing pre-trained LLMs. Ablation studies further show the complementary benefits of multi-expert architecture and ITI, highlighting their synergistic effects when combined.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13015v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nowfel Mashnoor, Mohammad Akyash, Hadi Kamali, Kimia Azar</dc:creator>
    </item>
    <item>
      <title>RM -RF: Reward Model for Run-Free Unit Test Evaluation</title>
      <link>https://arxiv.org/abs/2601.13097</link>
      <description>arXiv:2601.13097v1 Announce Type: new 
Abstract: We present RM-RF, a lightweight reward model for run-free evaluation of automatically generated unit tests. Instead of repeatedly compiling and executing candidate tests, RM-RF predicts - from source and test code alone - three execution-derived signals: (1) whether the augmented test suite compiles and runs successfully, (2) whether the generated test cases increase code coverage, and (3) whether the generated test cases improve the mutation kill rate. To train and evaluate RM-RF we assemble a multilingual dataset (Java, Python, Go) of focal files, test files, and candidate test additions labeled by an execution-based pipeline, and we release an associated dataset and methodology for comparative evaluation. We tested multiple model families and tuning regimes (zero-shot, full fine-tuning, and PEFT via LoRA), achieving an average F1 of 0.69 across the three targets. Compared to conventional compile-and-run instruments, RM-RF provides substantially lower latency and infrastructure cost while delivering competitive predictive fidelity, enabling fast, scalable feedback for large-scale test generation and RL-based code optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13097v1</guid>
      <category>cs.SE</category>
      <category>cs.LG</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elena Bruches, Daniil Grebenkin, Mikhail Klementev, Vadim Alperovich, Roman Derunets, Dari Baturova, Georgy Mkrtchyan, Oleg Sedukhin, Ivan Bondarenko, Nikolay Bushkov, Stanislav Moiseev</dc:creator>
    </item>
    <item>
      <title>Guidelines to Prompt Large Language Models for Code Generation: An Empirical Characterization</title>
      <link>https://arxiv.org/abs/2601.13118</link>
      <description>arXiv:2601.13118v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are nowadays extensively used for various types of software engineering tasks, primarily code generation. Previous research has shown how suitable prompt engineering could help developers in improving their code generation prompts. However, so far, there do not exist specific guidelines driving developers towards writing suitable prompts for code generation. In this work, we derive and evaluate development-specific prompt optimization guidelines. First, we use an iterative, test-driven approach to automatically refine code generation prompts, and we analyze the outcome of this process to identify prompt improvement items that lead to test passes. We use such elements to elicit 10 guidelines for prompt improvement, related to better specifying I/O, pre-post conditions, providing examples, various types of details, or clarifying ambiguities. We conduct an assessment with 50 practitioners, who report their usage of the elicited prompt improvement patterns, as well as their perceived usefulness, which does not always correspond to the actual usage before knowing our guidelines. Our results lead to implications not only for practitioners and educators, but also for those aimed at creating better LLM-aided software development tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13118v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessandro Midolo, Alessandro Giagnorio, Fiorella Zampetti, Rosalia Tufano, Gabriele Bavota, Massimiliano Di Penta</dc:creator>
    </item>
    <item>
      <title>Earth Embeddings as Products: Taxonomy, Ecosystem, and Standardized Access</title>
      <link>https://arxiv.org/abs/2601.13134</link>
      <description>arXiv:2601.13134v1 Announce Type: new 
Abstract: Geospatial Foundation Models (GFMs) provide powerful representations, but high compute costs hinder their widespread use. Pre-computed embedding data products offer a practical "frozen" alternative, yet they currently exist in a fragmented ecosystem of incompatible formats and resolutions. This lack of standardization creates an engineering bottleneck that prevents meaningful model comparison and reproducibility. We formalize this landscape through a three-layer taxonomy: Data, Tools, and Value. We survey existing products to identify interoperability barriers. To bridge this gap, we extend TorchGeo with a unified API that standardizes the loading and querying of diverse embedding products. By treating embeddings as first-class geospatial datasets, we decouple downstream analysis from model-specific engineering, providing a roadmap for more transparent and accessible Earth observation workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13134v1</guid>
      <category>cs.SE</category>
      <category>cs.CV</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heng Fang, Adam J. Stewart, Isaac Corley, Xiao Xiang Zhu, Hossein Azizpour</dc:creator>
    </item>
    <item>
      <title>From Human to Machine Refactoring: Assessing GPT-4's Impact on Python Class Quality and Readability</title>
      <link>https://arxiv.org/abs/2601.13139</link>
      <description>arXiv:2601.13139v1 Announce Type: new 
Abstract: Refactoring is a software engineering practice that aims to improve code quality without altering program behavior. Although automated refactoring tools have been extensively studied, their practical applicability remains limited. Recent advances in Large Language Models (LLMs) have introduced new opportunities for automated code refactoring. The evaluation of such an LLM-driven approach, however, leaves unanswered questions about its effects on code quality. In this paper, we present a comprehensive empirical study on LLM-driven refactoring using GPT-4o, applied to 100 Python classes from the ClassEval benchmark. Unlike prior work, our study explores a wide range of class-level refactorings inspired by Fowler's catalog and evaluates their effects from three complementary perspectives: (i) behavioral correctness, verified through unit tests; (ii) code quality, assessed via Pylint, Flake8, and SonarCloud; and (iii) readability, measured using a state-of-the-art readability tool. Our findings show that GPT-4o generally produces behavior-preserving refactorings that reduce code smells and improve quality metrics, albeit at the cost of decreased readability. Our results provide new evidence on the capabilities and limitations of LLMs in automated software refactoring, highlighting directions for integrating LLMs into practical refactoring workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13139v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessandro Midolo, Emiliano Tramontana, Massimiliano Di Penta</dc:creator>
    </item>
    <item>
      <title>KOCO-BENCH: Can Large Language Models Leverage Domain Knowledge in Software Development?</title>
      <link>https://arxiv.org/abs/2601.13240</link>
      <description>arXiv:2601.13240v1 Announce Type: new 
Abstract: Large language models (LLMs) excel at general programming but struggle with domain-specific software development, necessitating domain specialization methods for LLMs to learn and utilize domain knowledge and data. However, existing domain-specific code benchmarks cannot evaluate the effectiveness of domain specialization methods, which focus on assessing what knowledge LLMs possess rather than how they acquire and apply new knowledge, lacking explicit knowledge corpora for developing domain specialization methods. To this end, we present KOCO-BENCH, a novel benchmark designed for evaluating domain specialization methods in real-world software development. KOCO-BENCH contains 6 emerging domains with 11 software frameworks and 25 projects, featuring curated knowledge corpora alongside multi-granularity evaluation tasks including domain code generation (from function-level to project-level with rigorous test suites) and domain knowledge understanding (via multiple-choice Q&amp;A). Unlike previous benchmarks that only provide test sets for direct evaluation, KOCO-BENCH requires acquiring and applying diverse domain knowledge (APIs, rules, constraints, etc.) from knowledge corpora to solve evaluation tasks. Our evaluations reveal that KOCO-BENCH poses significant challenges to state-of-the-art LLMs. Even with domain specialization methods (e.g., SFT, RAG, kNN-LM) applied, improvements remain marginal. Best-performing coding agent, Claude Code, achieves only 34.2%, highlighting the urgent need for more effective domain specialization methods. We release KOCO-BENCH, evaluation code, and baselines to advance further research at https://github.com/jiangxxxue/KOCO-bench.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13240v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xue Jiang, Jiaru Qian, Xianjie Shi, Chenjie Li, Hao Zhu, Ziyu Wang, Jielun Zhang, Zheyu Zhao, Kechi Zhang, Jia Li, Wenpin Jiao, Zhi Jin, Ge Li, Yihong Dong</dc:creator>
    </item>
    <item>
      <title>SEER: Spectral Entropy Encoding of Roles for Context-Aware Attention-Based Design Pattern Detection</title>
      <link>https://arxiv.org/abs/2601.13334</link>
      <description>arXiv:2601.13334v1 Announce Type: new 
Abstract: This paper presents SEER, an upgraded version of our prior method Context Is All You Need for detecting Gang of Four (GoF) design patterns from source code. The earlier approach modeled code as attention-ready sequences that blended lightweight structure with behavioral context; however, it lacked explicit role disambiguation within classes and treated call edges uniformly. SEER addresses these limitations with two principled additions: (i) a spectral-entropy role encoder that derives per-member role embeddings from the Laplacian spectrum of each class's interaction graph, and (ii) a time-weighted calling context that assigns empirically calibrated duration priors to method categories (e.g., constructors, getters/setters, static calls, virtual dispatch, cloning). Together, these components sharpen the model's notion of "who does what" and "how much it matters," while remaining portable across languages with minimal adaptation and fully compatible with Transformer-based sequence encoders. Importantly, SEER does not "force" a win by capacity or data; it nudges the classifier, steering attention toward role-consistent and temporally calibrated signals that matter most. We evaluate SEER on PyDesignNet (1,832 files, 35,000 sequences, 23 GoF patterns) and observe consistent gains over our previous system: macro-F1 increases from 92.47% to 93.20% and accuracy from 92.52% to 93.98%, with macro-precision 93.98% and macro-recall 92.52%. Beyond aggregate metrics, SEER reduces false positives by nearly 20%, a decisive improvement that strengthens its robustness and practical reliability. Moreover, SEER yields interpretable, symbol-level attributions aligned with canonical roles, exhibits robustness under small graph perturbations, and shows stable calibration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13334v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tarik Houichime, Younes El Amrani</dc:creator>
    </item>
    <item>
      <title>FlipFlop: A Static Analysis-based Energy Optimization Framework for GPU Kernels</title>
      <link>https://arxiv.org/abs/2601.13345</link>
      <description>arXiv:2601.13345v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) applications, such as Large Language Models, are primarily driven and executed by Graphics Processing Units (GPUs). These GPU programs (kernels) consume substantial amounts of energy, yet software developers often lack the hardware expertise and ad hoc knowledge required to optimize for power efficiency. We propose FlipFlop, a framework using static code analysis to predict energy consumption and recommend Pareto-optimal thread block configurations considering both power consumption and execution time. Our framework requires no runtime execution and analyzes PTX code, a low-level instruction set for CUDA-enabled GPUs. It is validated across a diverse set of GPUs and kernels, including multi-head attention, convolution, and matrix multiplication. FlipFlop achieves 83% accuracy in identifying locally optimal energy-efficient configurations, while also minimizing developer effort by reducing the optimization search space by 93.4%. For multi-head attention kernels, it yields up to 79% energy savings and 106% throughput gains relative to NVIDIA's occupancy heuristic. By integrating static analysis with real-time monitoring and providing explainable optimization guidance, FlipFlop empowers developers to create sustainable, high-performance GPU software which minimizes environmental and computational costs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13345v1</guid>
      <category>cs.SE</category>
      <category>cs.PF</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Saurabhsingh Rajput, Alexander Brandt, Vadim Elisseev, Tushar Sharma</dc:creator>
    </item>
    <item>
      <title>From Completion to Editing: Unlocking Context-Aware Code Infilling via Search-and-Replace Instruction Tuning</title>
      <link>https://arxiv.org/abs/2601.13384</link>
      <description>arXiv:2601.13384v1 Announce Type: new 
Abstract: The dominant Fill-in-the-Middle (FIM) paradigm for code completion is constrained by its rigid inability to correct contextual errors and reliance on unaligned, insecure Base models. While Chat LLMs offer safety and Agentic workflows provide flexibility, they suffer from performance degradation and prohibitive latency, respectively. To resolve this dilemma, we propose Search-and-Replace Infilling (SRI), a framework that internalizes the agentic verification-and-editing mechanism into a unified, single-pass inference process. By structurally grounding edits via an explicit search phase, SRI harmonizes completion tasks with the instruction-following priors of Chat LLMs, extending the paradigm from static infilling to dynamic context-aware editing. We synthesize a high-quality dataset, SRI-200K, and fine-tune the SRI-Coder series. Extensive evaluations demonstrate that with minimal data (20k samples), SRI-Coder enables Chat models to surpass the completion performance of their Base counterparts. Crucially, unlike FIM-style tuning, SRI preserves general coding competencies and maintains inference latency comparable to standard FIM. We empower the entire Qwen3-Coder series with SRI, encouraging the developer community to leverage this framework for advanced auto-completion and assisted development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13384v1</guid>
      <category>cs.SE</category>
      <category>cs.CL</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiajun Zhang, Zeyu Cui, Jiaxi Yang, Lei Zhang, Yuheng Jing, Zeyao Ma, Tianyi Bai, Zilei Wang, Qiang Liu, Liang Wang, Binyuan Hui, Junyang Lin</dc:creator>
    </item>
    <item>
      <title>A Tool for Automatically Cataloguing and Selecting Pre-Trained Models and Datasets for Software Engineering</title>
      <link>https://arxiv.org/abs/2601.13460</link>
      <description>arXiv:2601.13460v1 Announce Type: new 
Abstract: The rapid growth of machine learning assets has made it increasingly difficult for software engineers to identify models and datasets that match their specific needs. Browsing large registries, such as Hugging Face, is time-consuming, error-prone, and rarely tailored to Software Engineering (SE) tasks. We present MLAssetSelection, a web application that automatically extracts SE assets and supports four key functionalities: (i) a configurable leaderboard for ranking models across multiple benchmarks and metrics; (ii) requirements-based selection of models and datasets; (iii) real-time automated updates through scheduled jobs that keep asset information current; and (iv) user-centric features including login, personalized asset lists, and configurable alert notifications. A demonstration video is available at https://youtu.be/t6CJ6P9asV4.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13460v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexandra Gonz\'alez, Oscar Cerezo, Xavier Franch, Silverio Mart\'inez-Fern\'andez</dc:creator>
    </item>
    <item>
      <title>Governance Matters: Lessons from Restructuring the data.table OSS Project</title>
      <link>https://arxiv.org/abs/2601.13466</link>
      <description>arXiv:2601.13466v1 Announce Type: new 
Abstract: Open source software (OSS) forms the backbone of industrial data workflows and enterprise systems. However, many OSS projects face operational risks due to informal or centralized governance. This paper presents a practical case study of data.table, a high-performance R package widely adopted in production analytics pipelines, which underwent a community-led governance reform to address scalability and sustainability concerns. Before the reform, data.table faced a growing backlog of unresolved issues and open pull requests, unclear contributor pathways, and bottlenecks caused by reliance on a single core maintainer. In response, the community initiated a redesign of its governance structure. In this paper, we evaluated the impact of this transition through a mixed-methods approach, combining a contributor survey (n=17) with mining project repository data. Our results show that following the reform, the project experienced a 200% increase in new contributor recruitment, a drop in pull request resolution time from over 700 days to under a week, and a 3x increase in contributor retention. Community sentiment improved around transparency, onboarding, and project momentum, though concerns around fairness and conflict resolution remain. This case study provides practical guidance for maintainers, companies, and foundations seeking to enhance OSS governance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13466v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ICSME64153.2025.00067</arxiv:DOI>
      <dc:creator>Pedro Oliveira, Doris Amoakohene, Toby Hocking, Marco Gerosa, Igor Steinmacher</dc:creator>
    </item>
    <item>
      <title>AI IDEs or Autonomous Agents? Measuring the Impact of Coding Agents on Software Development</title>
      <link>https://arxiv.org/abs/2601.13597</link>
      <description>arXiv:2601.13597v1 Announce Type: new 
Abstract: Large language model (LLM)-based coding agents increasingly act as autonomous contributors that generate and merge pull requests, yet their real-world effects on software projects are unclear, especially relative to widely adopted IDE-based AI assistants. We present a longitudinal causal study of agent adoption in open-source repositories using staggered difference-in-differences with matched controls. Using the AIDev dataset, we define adoption as the first agent-generated pull request and analyze monthly repository-level outcomes spanning development velocity (commits, lines added) and software quality (static-analysis warnings, cognitive complexity, duplication, and comment density). Results show large, front-loaded velocity gains only when agents are the first observable AI tool in a project; repositories with prior AI IDE usage experience minimal or short-lived throughput benefits. In contrast, quality risks are persistent across settings, with static-analysis warnings and cognitive complexity rising roughly 18% and 35%, indicating sustained agent-induced complexity debt even when velocity advantages fade. These heterogeneous effects suggest diminishing returns to AI assistance and highlight the need for quality safeguards, provenance tracking, and selective deployment of autonomous agents. Our findings establish an empirical basis for understanding how agentic and IDE-based tools interact, and motivate research on balancing acceleration with maintainability in AI-integrated development workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13597v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shyam Agarwal, Hao He, Bogdan Vasilescu</dc:creator>
    </item>
    <item>
      <title>Why Does the LLM Stop Computing: An Empirical Study of User-Reported Failures in Open-Source LLMs</title>
      <link>https://arxiv.org/abs/2601.13655</link>
      <description>arXiv:2601.13655v1 Announce Type: new 
Abstract: The democratization of open-source Large Language Models (LLMs) allows users to fine-tune and deploy models on local infrastructure but exposes them to a First Mile deployment landscape. Unlike black-box API consumption, the reliability of user-managed orchestration remains a critical blind spot. To bridge this gap, we conduct the first large-scale empirical study of 705 real-world failures from the open-source DeepSeek, Llama, and Qwen ecosystems.
  Our analysis reveals a paradigm shift: white-box orchestration relocates the reliability bottleneck from model algorithmic defects to the systemic fragility of the deployment stack. We identify three key phenomena: (1) Diagnostic Divergence: runtime crashes distinctively signal infrastructure friction, whereas incorrect functionality serves as a signature for internal tokenizer defects. (2) Systemic Homogeneity: Root causes converge across divergent series, confirming reliability barriers are inherent to the shared ecosystem rather than specific architectures. (3) Lifecycle Escalation: Barriers escalate from intrinsic configuration struggles during fine-tuning to compounded environmental incompatibilities during inference. Supported by our publicly available dataset, these insights provide actionable guidance for enhancing the reliability of the LLM landscape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13655v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guangba Yu, Zirui Wang, Yujie Huang, Renyi Zhong, Yuedong Zhong, Yilun Wang, Michael R. Lyu</dc:creator>
    </item>
    <item>
      <title>CodeContests-O: Powering LLMs via Feedback-Driven Iterative Test Case Generation</title>
      <link>https://arxiv.org/abs/2601.13682</link>
      <description>arXiv:2601.13682v1 Announce Type: new 
Abstract: The rise of reasoning models necessitates large-scale verifiable data, for which programming tasks serve as an ideal source. However, while competitive programming platforms provide abundant problems and solutions, high-quality test cases for verification remain scarce. Existing approaches attempt to synthesize test cases using Large Language Models (LLMs), but rely solely on the model's intrinsic generation capabilities without external feedback, frequently resulting in insufficiently diverse cases. To address this limitation, we propose a $\textbf{Feedback-Driven Iterative Framework}$ for comprehensive test case construction. Specifically, our method leverages the LLM to generate initial test cases, executes them against known correct and incorrect solutions, and utilizes the failed results as feedback to guide the LLM in refining the test cases toward high fidelity and discriminability. We then apply this method to the CodeContests dataset to construct an optimized high-quality derivative, $\textbf{CodeContests-O}$. Evaluating against the entire pool of solutions ($1.1 \times 10^7$ in total), our dataset achieves an average True Positive Rate (TPR) of $89.37\%$ and True Negative Rate (TNR) of $90.89\%$, significantly outperforming the CodeContests and CodeContests+ by margins of $4.32\%$ and $9.37\%$, respectively. Furthermore, fine-tuning the Qwen2.5-7B model on CodeContests-O results in a $9.52\%$ improvement on LiveCodeBench (Pass@1). Experiments demonstrate the effectiveness of our framework and the quality of CodeContests-O. To support reproducibility and facilitate future research, we release the $\href{https://github.com/cai-jianfeng/CodeContests-O}{code}$ and $\href{https://huggingface.co/datasets/caijanfeng/CodeContests-O}{dataset}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13682v1</guid>
      <category>cs.SE</category>
      <category>cs.PL</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianfeng Cai, Jinhua Zhu, Ruopei Sun, Kangwen Zhao, Dongyun Xue, Mingxiao Feng, Wengang Zhou, Houqiang Li</dc:creator>
    </item>
    <item>
      <title>SWE-Tester: Training Open-Source LLMs for Issue Reproduction in Real-World Repositories</title>
      <link>https://arxiv.org/abs/2601.13713</link>
      <description>arXiv:2601.13713v1 Announce Type: new 
Abstract: Software testing is crucial for ensuring the correctness and reliability of software systems. Automated generation of issue reproduction tests from natural language issue descriptions enhances developer productivity by simplifying root cause analysis, promotes test-driven development -- "test first, write code later", and can be used for improving the effectiveness of automated issue resolution systems like coding agents. Existing methods proposed for this task predominantly rely on closed-source LLMs, with limited exploration of open models. To address this, we propose SWE-Tester -- a novel pipeline for training open-source LLMs to generate issue reproduction tests. First, we curate a high-quality training dataset of 41K instances from 2.6K open-source GitHub repositories and use it to train LLMs of varying sizes and families. The fine-tuned models achieve absolute improvements of up to 10\% in success rate and 21\% in change coverage on SWT-Bench Verified. Further analysis shows consistent improvements with increased inference-time compute, more data, and larger models. These results highlight the effectiveness of our framework for advancing open-source LLMs in this domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13713v1</guid>
      <category>cs.SE</category>
      <category>cs.LG</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aditya Bharat Soni, Rajat Ghosh, Vaishnavi Bhargava, Valerie Chen, Debojyoti Dutta</dc:creator>
    </item>
    <item>
      <title>Counterexample Classification against Signal Temporal Logic Specifications</title>
      <link>https://arxiv.org/abs/2601.13743</link>
      <description>arXiv:2601.13743v1 Announce Type: new 
Abstract: Signal Temporal Logic (STL) has been widely adopted as a specification language for specifying desirable behaviors of hybrid systems. By monitoring a given STL specification, we can detect the executions that violate it, which are often referred to as counterexamples. In practice, these counterexamples may arise from different causes and thus are relevant to different system defects. To effectively address this, we need a proper criterion for classifying these counterexamples, by which we can comprehend the possible violation patterns and the distributions of these counterexamples with respect to the patterns. In this paper, we propose a classification criterion by using parametric signal temporal logic (PSTL) to represent each class. Due to this formalism, identifying the classes of a counterexample requires finding proper parameter values of PSTL that enable a class to include the counterexample. To improve the efficiency of class identification, we further derive an inclusion relation between different classes, and then propose a binary search-like approach over it that significantly prunes the classes needed to query. We implement a prototype tool and experimentally evaluate its effectiveness on two widely-studied systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13743v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zhenya Zhang, Parv Kapoor, Jie An, Eunsuk Kang</dc:creator>
    </item>
    <item>
      <title>On Autopilot? An Empirical Study of Human-AI Teaming and Review Practices in Open Source</title>
      <link>https://arxiv.org/abs/2601.13754</link>
      <description>arXiv:2601.13754v1 Announce Type: new 
Abstract: Large Language Models (LLMs) increasingly automate software engineering tasks. While recent studies highlight the accelerated adoption of ``AI as a teammate'' in Open Source Software (OSS), developer interaction patterns remain under-explored. In this work, we investigated project-level guidelines and developers' interactions with AI-assisted pull requests (PRs) by expanding the AIDev dataset to include finer-grained contributor code ownership and a comparative baseline of human-created PRs. We found that over 67.5\% of AI-co-authored PRs originate from contributors without prior code ownership. Despite this, the majority of repositories lack guidelines for AI-coding agent usage. Notably, we observed a distinct interaction pattern: AI-co-authored PRs are merged significantly faster with minimal feedback. In contrast to human-created PRs where non-owner developers receive the most feedback, AI-co-authored PRs from non-owners receive the least, with approximately 80\% merged without any explicit review. Finally, we discuss implications for developers and researchers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13754v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoyu Gao, Peerachai Banyongrakkul, Hao Guan, Mansooreh Zahedi, Christoph Treude</dc:creator>
    </item>
    <item>
      <title>A Blockchain-Oriented Software Engineering Architecture for Carbon Credit Certification Systems</title>
      <link>https://arxiv.org/abs/2601.13772</link>
      <description>arXiv:2601.13772v1 Announce Type: new 
Abstract: Carbon credit systems have emerged as a policy tool to incentivize emission reductions and support the transition to clean energy. Reliable carbon-credit certification depends on mechanisms that connect actual, measured renewable-energy production to verifiable emission-reduction records. Although blockchain and IoT technologies have been applied to emission monitoring and trading, existing work offers limited support for certification processes, particularly for small and medium-scale renewable installations. This paper introduces a blockchain-based carbon-credit certification architecture, demonstrated through a 100 kWp photovoltaic case study, that integrates real-time IoT data collection, edge-level aggregation, and secure on-chain storage on a permissioned blockchain with smart contracts. Unlike approaches focused on trading mechanisms, the proposed system aligns with European legislation and voluntary carbon-market standards, clarifying the practical requirements and constraints that apply to photovoltaic operators. The resulting architecture provides a structured pathway for generating verifiable carbon-credit records and supporting third-party verification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13772v1</guid>
      <category>cs.SE</category>
      <category>cs.DC</category>
      <category>cs.SI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matteo Vaccargiu, Azmat Ullah, Pierluigi Gallo</dc:creator>
    </item>
    <item>
      <title>Multi-Location Software Model Completion</title>
      <link>https://arxiv.org/abs/2601.13894</link>
      <description>arXiv:2601.13894v1 Announce Type: new 
Abstract: In model-driven engineering and beyond, software models are key development artifacts. In practice, they often grow to substantial size and complexity, undergoing thousands of modifications over time due to evolution, refactoring, and maintenance. The rise of AI has sparked interest in how software modeling activities can be automated. Recently, LLM-based approaches for software model completion have been proposed, however, the state of the art supports only single-location model completion by predicting changes at a specific location. Going beyond, we aim to bridge the gap toward handling coordinated changes that span multiple locations across large, complex models. Specifically, we propose a novel global embedding-based next focus predictor, NextFocus, which is capable of multi-location model completion for the first time. The predictor consists of a neural network with an attention mechanism that is trained on historical software model evolution data. Starting from an existing change, it predicts further model elements to change, potentially spanning multiple parts of the model. We evaluate our approach on multi-location model changes that have actually been performed by developers in real-world projects. NextFocus achieves promising results for multi-location model completion, even when changes are heavily spread across the model. It achieves an average Precision@k score of 0.98 for $k \leq 10$, significantly outperforming the three baseline approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13894v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alisa Welter, Christof Tinnes, Sven Apel</dc:creator>
    </item>
    <item>
      <title>VulnResolver: A Hybrid Agent Framework for LLM-Based Automated Vulnerability Issue Resolution</title>
      <link>https://arxiv.org/abs/2601.13933</link>
      <description>arXiv:2601.13933v1 Announce Type: new 
Abstract: As software systems grow in complexity, security vulnerabilities have become increasingly prevalent, posing serious risks and economic costs. Although automated detection tools such as fuzzers have advanced considerably, effective resolution still often depends on human expertise. Existing automated vulnerability repair (AVR) methods rely heavily on manually provided annotations (e.g., fault locations or CWE labels), which are often difficult and time-consuming to obtain, while overlooking the rich, naturally embedded semantic context found in issue reports from developers.
  In this paper, we present VulnResolver, the first LLM-based hybrid agent framework for automated vulnerability issue resolution. VulnResolver unites the adaptability of autonomous agents with the stability of workflow-guided repair through two specialized agents. The Context Pre-Collection Agent (CPCAgent) adaptively explores the repository to gather dependency and contextual information, while the Safety Property Analysis Agent (SPAAgent) generates and validates the safety properties violated by vulnerabilities. Together, these agents produce structured analyses that enrich the original issue reports, enabling more accurate vulnerability localization and patch generation.
  Evaluations on the SEC-bench benchmark show that VulnResolver resolves 75% of issues on SEC-bench Lite, achieving the best resolution performance. On SEC-bench Full, VulnResolver also significantly outperforms the strongest baseline, the agent-based OpenHands, confirming its effectiveness. Overall, VulnResolver delivers an adaptive and security-aware framework that advances end-to-end automated vulnerability issue resolution through workflow stability and the specialized agents' capabilities in contextual reasoning and property-based analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13933v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingming Zhang, Xu Wang, Jian Zhang, Xiangxin Meng, Jiayi Zhang, Chunming Hu</dc:creator>
    </item>
    <item>
      <title>RepoGenesis: Benchmarking End-to-End Microservice Generation from Readme to Repository</title>
      <link>https://arxiv.org/abs/2601.13943</link>
      <description>arXiv:2601.13943v1 Announce Type: new 
Abstract: Large language models and agents have achieved remarkable progress in code generation. However, existing benchmarks focus on isolated function/class-level generation (e.g., ClassEval) or modifications to existing codebases (e.g., SWE-Bench), neglecting complete microservice repository generation that reflects real-world 0-to-1 development workflows. To bridge this gap, we introduce RepoGenesis, the first multilingual benchmark for repository-level end-to-end web microservice generation, comprising 106 repositories (60 Python, 46 Java) across 18 domains and 11 frameworks, with 1,258 API endpoints and 2,335 test cases verified through a "review-rebuttal" quality assurance process. We evaluate open-source agents (e.g., DeepCode) and commercial IDEs (e.g., Cursor) using Pass@1, API Coverage (AC), and Deployment Success Rate (DSR). Results reveal that despite high AC (up to 73.91%) and DSR (up to 100%), the best-performing system achieves only 23.67% Pass@1 on Python and 21.45% on Java, exposing deficiencies in architectural coherence, dependency management, and cross-file consistency. Notably, GenesisAgent-8B, fine-tuned on RepoGenesis (train), achieves performance comparable to GPT-5 mini, demonstrating the quality of RepoGenesis for advancing microservice generation. We release our benchmark at https://github.com/pzy2000/RepoGenesis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13943v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiyuan Peng, Xin Yin, Pu Zhao, Fangkai Yang, Lu Wang, Ran Jia, Xu Chen, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang</dc:creator>
    </item>
    <item>
      <title>Software Testing in the Quantum World</title>
      <link>https://arxiv.org/abs/2601.13996</link>
      <description>arXiv:2601.13996v1 Announce Type: new 
Abstract: Quantum computing offers significant speedups for simulating physical, chemical, and biological systems, and for optimization and machine learning. As quantum software grows in complexity, the classical simulation of quantum computers, which has long been essential for quality assurance, becomes infeasible. This shift requires new quality-assurance methods that operate directly on real quantum computers. This paper presents the key challenges in testing large-scale quantum software and offers software engineering perspectives for addressing them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13996v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rui Abreu, Shaukat Ali, Paolo Arcaini, Jose Campos, Michael Felderer, Claude Gravel, Fuyuki Ishikawa, Stefan Klikovits, Andriy Miranskyy, Mohammad Mousavi, Masaomi Yamaguchi, Lei Zhang, Jianjun Zhao, Anila Mjeda</dc:creator>
    </item>
    <item>
      <title>Analyzing the Availability of E-Mail Addresses for PyPI Libraries</title>
      <link>https://arxiv.org/abs/2601.14034</link>
      <description>arXiv:2601.14034v1 Announce Type: new 
Abstract: Open Source Software (OSS) libraries form the backbone of modern software systems, yet their long-term sustainability often depends on maintainers being reachable for support, coordination, and security reporting. In this paper, we empirically analyze the availability of contact information - specifically e-mail addresses - across 686,034 Python libraries on the Python Package Index (PyPI) and their associated GitHub repositories. We examine how and where maintainers provide this information, assess its validity, and explore coverage across individual libraries and their dependency chains. Our findings show that 81.6% of libraries include at least one valid e-mail address, with PyPI serving as the primary source (79.5%). When analyzing dependency chains, we observe that up to 97.8% of direct and 97.7% of transitive dependencies provide valid contact information. At the same time, we identify over 698,000 invalid entries, primarily due to missing fields. These results demonstrate strong maintainer reachability across the ecosystem, while highlighting opportunities for improvement - such as offering clearer guidance to maintainers during the packaging process and introducing opt-in validation mechanisms for existing e-mail addresses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14034v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexandros Tsakpinis, Alexander Pretschner</dc:creator>
    </item>
    <item>
      <title>Feature-Aware Test Generation for Deep Learning Models</title>
      <link>https://arxiv.org/abs/2601.14081</link>
      <description>arXiv:2601.14081v1 Announce Type: new 
Abstract: As deep learning models are widely used in software systems, test generation plays a crucial role in assessing the quality of such models before deployment. To date, the most advanced test generators rely on generative AI to synthesize inputs; however, these approaches remain limited in providing semantic insight into the causes of misbehaviours and in offering fine-grained semantic controllability over the generated inputs. In this paper, we introduce Detect, a feature-aware test generation framework for vision-based deep learning (DL) models that systematically generates inputs by perturbing disentangled semantic attributes within the latent space. Detect perturbs individual latent features in a controlled way and observes how these changes affect the model's output. Through this process, it identifies which features lead to behavior shifts and uses a vision-language model for semantic attribution. By distinguishing between task-relevant and irrelevant features, Detect applies feature-aware perturbations targeted for both generalization and robustness. Empirical results across image classification and detection tasks show that Detect generates high-quality test cases with fine-grained control, reveals distinct shortcut behaviors across model architectures (convolutional and transformer-based), and bugs that are not captured by accuracy metrics. Specifically, Detect outperforms a state-of-the-art test generator in decision boundary discovery and a leading spurious feature localization method in identifying robustness failures. Our findings show that fully fine-tuned convolutional models are prone to overfitting on localized cues, such as co-occurring visual traits, while weakly supervised transformers tend to rely on global features, such as environmental variances. These findings highlight the value of interpretable and feature-aware testing in improving DL model reliability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14081v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Xingcheng Chen, Oliver Weissl, Andrea Stocco</dc:creator>
    </item>
    <item>
      <title>Practitioner Views on Mobile App Accessibility: Practices and Challenges</title>
      <link>https://arxiv.org/abs/2601.14131</link>
      <description>arXiv:2601.14131v1 Announce Type: new 
Abstract: As mobile applications (apps) become ubiquitous in everyday life, it is crucial for developers to prioritize accessibility for users with diverse abilities. While previous research has identified widespread accessibility issues and raised awareness of developer challenges, there remains a lack of cross-platform, globally representative insights into how practitioners approach accessibility in practice. This paper presents findings from a mixed-methods survey of 110 mobile app developers across 43 countries, examining how platform ecosystems (iOS vs. Android) and developer experience shape accessibility practices. Results show that while developers recognize the importance of accessibility, they often rely on platform-specific guidelines and typically perform compliance testing late in the development process. Developers primarily implement text-focused features while struggling with API limitations and organizational constraints. Through systematic cross-platform comparison, we identify novel platform-specific barriers and demonstrate how accessibility practices differ across developer experience levels. Our findings offer new insights into the challenges of achieving accessibility in practice and provide actionable steps for various stakeholders to promote more consistent and inclusive app development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14131v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3744916.3787791</arxiv:DOI>
      <dc:creator>Amila Indika, Rick Kazman, Anthony Peruma</dc:creator>
    </item>
    <item>
      <title>Toward self-coding information systems</title>
      <link>https://arxiv.org/abs/2601.14132</link>
      <description>arXiv:2601.14132v1 Announce Type: new 
Abstract: In this extended abstract, we propose a novel research topic in the field of agentic AI, which we refer to as self-coding information systems. These systems will be able to dynamically adapt their structure or behavior by evaluating potential adaptation decisions, generate source code, test, and (re)deploy their source code autonomously, at runtime, reducing the time to market of new features. Here we motivate the topic, provide a formal definition of self-coding information systems, discuss some expected impacts of the new technology, and indicate potential research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14132v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rodrigo Falc\~ao, Frank Elberzhager, Karthik Vaidhyanathan</dc:creator>
    </item>
    <item>
      <title>An Empirical Study on Remote Code Execution in Machine Learning Model Hosting Ecosystems</title>
      <link>https://arxiv.org/abs/2601.14163</link>
      <description>arXiv:2601.14163v1 Announce Type: new 
Abstract: Model-sharing platforms, such as Hugging Face, ModelScope, and OpenCSG, have become central to modern machine learning development, enabling developers to share, load, and fine-tune pre-trained models with minimal effort. However, the flexibility of these ecosystems introduces a critical security concern: the execution of untrusted code during model loading (i.e., via trust_remote_code or trust_repo). In this work, we conduct the first large-scale empirical study of custom model loading practices across five major model-sharing platforms to assess their prevalence, associated risks, and developer perceptions. We first quantify the frequency with which models require custom code to function and identify those that execute arbitrary Python files during loading. We then apply three complementary static analysis tools: Bandit, CodeQL, and Semgrep, to detect security smells and potential vulnerabilities, categorizing our findings by CWE identifiers to provide a standardized risk taxonomy. We also use YARA to identify malicious patterns and payload signatures. In parallel, we systematically analyze the documentation, API design, and safety mechanisms of each platform to understand their mitigation strategies and enforcement levels. Finally, we conduct a qualitative analysis of over 600 developer discussions from GitHub, Hugging Face, and PyTorch Hub forums, as well as Stack Overflow, to capture community concerns and misconceptions regarding security and usability. Our findings reveal widespread reliance on unsafe defaults, uneven security enforcement across platforms, and persistent confusion among developers about the implications of executing remote code. We conclude with actionable recommendations for designing safer model-sharing infrastructures and striking a balance between usability and security in future AI ecosystems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14163v1</guid>
      <category>cs.SE</category>
      <category>cs.CR</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammed Latif Siddiq, Tanzim Hossain Romel, Natalie Sekerak, Beatrice Casey, Joanna C. S. Santos</dc:creator>
    </item>
    <item>
      <title>NOVAID: Natural-language Observability Visualization Assistant for ITOps Dashboard Widget Generation</title>
      <link>https://arxiv.org/abs/2601.11531</link>
      <description>arXiv:2601.11531v1 Announce Type: cross 
Abstract: Manual creation of IT monitoring dashboard widgets is slow, error-prone, and a barrier for both novice and expert users. We present NOVAID, an interactive chatbot that leverages Large Language Models (LLMs) to generate IT monitoring widgets directly from natural language queries. Unlike general natural language-to-visualization tools, NOVAID addresses IT operations-specific challenges: specialized widget types like SLO charts, dynamic API-driven data retrieval, and complex contextual filters. The system combines a domain-aware semantic parser, fuzzy entity matching, and schema completion to produce standardized widget JSON specifications. An interactive clarification loop ensures accuracy in underspecified queries. On a curated dataset of 271 realistic queries, NOVAID achieves promising accuracy (up to 94.10% in metric extraction) across multiple LLMs. A user study with IT engineers yielded a System Usability Scale score of 74.2 for NOVAID, indicating good usability. By bridging natural language intent with operational dashboards, NOVAID demonstrates clear potential and a path for deployment in enterprise ITOps monitoring platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11531v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Pratik Mishra, Caner G\"oz\"ub\"uy\"uk, Seema Nagar, Prateeti Mohapatra, Raya Wittich, Arthur de Magalhaes</dc:creator>
    </item>
    <item>
      <title>Enhancing Model Context Protocol (MCP) with Context-Aware Server Collaboration</title>
      <link>https://arxiv.org/abs/2601.11595</link>
      <description>arXiv:2601.11595v1 Announce Type: cross 
Abstract: The Model Context Protocol (MCP) has emerged as a widely used framework for enabling LLM-based agents to communicate with external tools and services. The most common implementation of MCP, proposed by Anthropic, heavily relies on a Large Language Model (LLM) to decompose tasks and issue instructions to servers, which act as stateless executors. In particular, the agents, models, and servers are stateless and do not have access to a global context. However, in tasks involving LLM-driven coordination, it is natural that a Shared Context Store (SCS) could improve the efficiency and coherence of multi-agent workflows by reducing redundancy and enabling knowledge transfer between servers. Thus, in this work, we design and assess the performance of a Context-Aware MCP (CA-MCP) that offloads execution logic to specialized MCP servers that read from and write to a shared context memory, allowing them to coordinate more autonomously in real time. In this design, context management serves as the central mechanism that maintains continuity across task executions by tracking intermediate states and shared variables, thereby enabling persistent collaboration among agents without repeated prompting. We present experiments showing that the CA-MCP can outperform the traditional MCP by reducing the number of LLM calls required for complex tasks and decreasing the frequency of response failures when task conditions are not satisfied, thereby improving overall efficiency and responsiveness. In particular, we conducted experiments on the TravelPlanner and REALM-Bench benchmark datasets and observed statistically significant results indicating the potential advantages of incorporating a shared context store via CA-MCP in LLM-driven multi-agent systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11595v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Meenakshi Amulya Jayanti, X. Y. Han</dc:creator>
    </item>
    <item>
      <title>A Survey on Mapping Digital Systems with Bill of Materials: Development, Practices, and Challenges</title>
      <link>https://arxiv.org/abs/2601.11678</link>
      <description>arXiv:2601.11678v1 Announce Type: cross 
Abstract: Modern digital ecosystems, spanning software, hardware, learning models, datasets, and cryptographic products, continue to grow in complexity, making it difficult for organizations to understand and manage component dependencies. Bills of Materials (BOMs) have emerged as a structured way to document product components, their interrelationships, and key metadata, improving visibility and security across digital supply chains. This survey provides the first comprehensive cross-domain review of BOM developments and practices. We start by examining the evolution of BOM frameworks in three stages (i.e., pre-development, initial, and accelerated) and summarizing their core principles, key stakeholders, and standardization efforts for hardware, software, artificial intelligence (AI) models, datasets, and cryptographic assets. We then review industry practices for generating BOM data, evaluating its quality, and securely sharing it. Next, we review practical downstream uses of BOM data, including dependency modeling, compliance verification, operational risk assessment, and vulnerability tracking. We also discuss academic efforts to address limitations in current BOM frameworks through refinements, extensions, or new models tailored to emerging domains such as data ecosystems and AI supply chains. Finally, we identify four key gaps that limit the usability and reliability of today's BOM frameworks, motivating future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11678v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuai Zhang, Minzhao Lyu, Hassan Habibi Gharakheili</dc:creator>
    </item>
    <item>
      <title>Attesting Model Lineage by Consisted Knowledge Evolution with Fine-Tuning Trajectory</title>
      <link>https://arxiv.org/abs/2601.11683</link>
      <description>arXiv:2601.11683v1 Announce Type: cross 
Abstract: The fine-tuning technique in deep learning gives rise to an emerging lineage relationship among models. This lineage provides a promising perspective for addressing security concerns such as unauthorized model redistribution and false claim of model provenance, which are particularly pressing in \textcolor{blue}{open-weight model} libraries where robust lineage verification mechanisms are often lacking. Existing approaches to model lineage detection primarily rely on static architectural similarities, which are insufficient to capture the dynamic evolution of knowledge that underlies true lineage relationships. Drawing inspiration from the genetic mechanism of human evolution, we tackle the problem of model lineage attestation by verifying the joint trajectory of knowledge evolution and parameter modification. To this end, we propose a novel model lineage attestation framework. In our framework, model editing is first leveraged to quantify parameter-level changes introduced by fine-tuning. Subsequently, we introduce a novel knowledge vectorization mechanism that refines the evolved knowledge within the edited models into compact representations by the assistance of probe samples. The probing strategies are adapted to different types of model families. These embeddings serve as the foundation for verifying the arithmetic consistency of knowledge relationships across models, thereby enabling robust attestation of model lineage. Extensive experimental evaluations demonstrate the effectiveness and resilience of our approach in a variety of adversarial scenarios in the real world. Our method consistently achieves reliable lineage verification across a broad spectrum of model types, including classifiers, diffusion models, and large language models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11683v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhuoyi Shang, Jiasen Li, Pengzhen Chen, Yanwei Liu, Xiaoyan Gu, Weiping Wang</dc:creator>
    </item>
    <item>
      <title>Imandra CodeLogician: Neuro-Symbolic Reasoning for Precise Analysis of Software Logic</title>
      <link>https://arxiv.org/abs/2601.11840</link>
      <description>arXiv:2601.11840v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have shown strong performance on code understanding tasks, yet they fundamentally lack the ability to perform precise, exhaustive mathematical reasoning about program behavior. Existing benchmarks either focus on mathematical proof automation, largely disconnected from real-world software, or on engineering tasks that do not require semantic rigor.
  We present CodeLogician, a neurosymbolic agent for precise analysis of software logic, integrated with ImandraX, an industrial automated reasoning engine deployed in financial markets and safety-critical systems. Unlike prior approaches that use formal methods primarily to validate LLM outputs, CodeLogician uses LLMs to construct explicit formal models of software systems, enabling automated reasoning to answer rich semantic questions beyond binary verification outcomes.
  To rigorously evaluate mathematical reasoning about software logic, we introduce code-logic-bench, a benchmark targeting the middle ground between theorem proving and software engineering benchmarks. It measures reasoning correctness about program state spaces, control flow, coverage constraints, and edge cases, with ground truth defined via formal modeling and region decomposition.
  Comparing LLM-only reasoning against LLMs augmented with CodeLogician, formal augmentation yields substantial improvements, closing a 41-47 percentage point gap in reasoning accuracy. These results demonstrate that neurosymbolic integration is essential for scaling program analysis toward rigorous, autonomous software understanding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11840v1</guid>
      <category>cs.AI</category>
      <category>cs.LO</category>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongyu Lin, Samer Abdallah, Makar Valentinov, Paul Brennan, Elijah Kagan, Christoph M. Wintersteiger, Denis Ignatovich, Grant Passmore</dc:creator>
    </item>
    <item>
      <title>DevBench: A Realistic, Developer-Informed Benchmark for Code Generation Models</title>
      <link>https://arxiv.org/abs/2601.11895</link>
      <description>arXiv:2601.11895v1 Announce Type: cross 
Abstract: DevBench is a telemetry-driven benchmark designed to evaluate Large Language Models (LLMs) on realistic code completion tasks. It includes 1,800 evaluation instances across six programming languages and six task categories derived from real developer telemetry, such as API usage and code purpose understanding. Unlike prior benchmarks, it emphasizes ecological validity, avoids training data contamination, and enables detailed diagnostics. The evaluation combines functional correctness, similarity-based metrics, and LLM-judge assessments focused on usefulness and contextual relevance. 9 state-of-the-art models were assessed, revealing differences in syntactic precision, semantic reasoning, and practical utility. Our benchmark provides actionable insights to guide model selection and improvement-detail that is often missing from other benchmarks but is essential for both practical deployment and targeted model development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11895v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pareesa Ameneh Golnari, Adarsh Kumarappan, Wen Wen, Xiaoyu Liu, Gabriel Ryan, Yuting Sun, Shengyu Fu, Elsie Nallipogu</dc:creator>
    </item>
    <item>
      <title>Towards Airborne Object Detection: A Deep Learning Analysis</title>
      <link>https://arxiv.org/abs/2601.11907</link>
      <description>arXiv:2601.11907v1 Announce Type: cross 
Abstract: The rapid proliferation of airborne platforms, including commercial aircraft, drones, and UAVs, has intensified the need for real-time, automated threat assessment systems. Current approaches depend heavily on manual monitoring, resulting in limited scalability and operational inefficiencies. This work introduces a dual-task model based on EfficientNetB4 capable of performing airborne object classification and threat-level prediction simultaneously. To address the scarcity of clean, balanced training data, we constructed the AODTA Dataset by aggregating and refining multiple public sources. We benchmarked our approach on both the AVD Dataset and the newly developed AODTA Dataset and further compared performance against a ResNet-50 baseline, which consistently underperformed EfficientNetB4. Our EfficientNetB4 model achieved 96% accuracy in object classification and 90% accuracy in threat-level prediction, underscoring its promise for applications in surveillance, defense, and airspace management. Although the title references detection, this study focuses specifically on classification and threat-level inference using pre-localized airborne object images provided by existing datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11907v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Prosenjit Chatterjee, ANK Zaman</dc:creator>
    </item>
    <item>
      <title>Big Data Workload Profiling for Energy-Aware Cloud Resource Management</title>
      <link>https://arxiv.org/abs/2601.11935</link>
      <description>arXiv:2601.11935v1 Announce Type: cross 
Abstract: Cloud data centers face increasing pressure to reduce operational energy consumption as big data workloads continue to grow in scale and complexity. This paper presents a workload aware and energy efficient scheduling framework that profiles CPU utilization, memory demand, and storage IO behavior to guide virtual machine placement decisions. By combining historical execution logs with real time telemetry, the proposed system predicts the energy and performance impact of candidate placements and enables adaptive consolidation while preserving service level agreement compliance. The framework is evaluated using representative Hadoop MapReduce, Spark MLlib, and ETL workloads deployed on a multi node cloud testbed. Experimental results demonstrate consistent energy savings of 15 to 20 percent compared to a baseline scheduler, with negligible performance degradation. These findings highlight workload profiling as a practical and scalable strategy for improving the sustainability of cloud based big data processing environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11935v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Milan Parikh, Aniket Abhishek Soni, Sneja Mitinbhai Shah, Ayush Raj Jha</dc:creator>
    </item>
    <item>
      <title>Are LLMs Ready for TOON? Benchmarking Structural Correctness-Sustainability Trade-offs in Novel Structured Output Formats</title>
      <link>https://arxiv.org/abs/2601.12014</link>
      <description>arXiv:2601.12014v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly required to generate structured, machine-readable outputs for downstream systems. While recent benchmarks have focused on evaluating the structural correctness of such outputs, the environmental impact of inference for different output formats has largely been overlooked. In this paper, we argue that structured output formats should be assessed not only in terms of correctness, but also with respect to their environmental efficiency. To this end, we introduce a sustainability-aware evaluation framework for structured generation that measures token usage, generation time, and estimated carbon emissions. Within this framework, we propose the Environment-Aware Generation Correctness Score (GCS_env), a unified metric that integrates structural correctness with carbon-aware efficiency. Using this framework, we systematically benchmark the novel TOON format against established representations (JSON, XML, YAML) across multiple LLMs spanning different architectures and parameter scales.
  Our results reveal a consistent trade-off: TOON yields markedly more compact outputs and lower emissions, but lower structural correctness when models lack native support. We show that increased model capacity reduces this gap and that environment-aware scoring can shift format rankings depending on deployment priorities. highlighting the need for sustainability-inclusive benchmarking and provides empirical evidence that compact representations such as TOON can offer practical advantages in large-scale, carbon-conscious LLM deployments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12014v1</guid>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Elio Masciari, Vincenzo Moscato, Enea Vincenzo Napolitano, Gian Marco Orlando, Marco Perillo, Diego Russo</dc:creator>
    </item>
    <item>
      <title>ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents</title>
      <link>https://arxiv.org/abs/2601.12294</link>
      <description>arXiv:2601.12294v1 Announce Type: cross 
Abstract: Reward-guided search methods have demonstrated strong potential in enhancing tool-using agents by effectively guiding sampling and exploration over complex action spaces. As a core design, those search methods utilize process reward models (PRMs) to provide step-level rewards, enabling more fine-grained monitoring. However, there is a lack of systematic and reliable evaluation benchmarks for PRMs in tool-using settings. In this paper, we introduce ToolPRMBench, a large-scale benchmark specifically designed to evaluate PRMs for tool-using agents. ToolPRMBench is built on top of several representative tool-using benchmarks and converts agent trajectories into step-level test cases. Each case contains the interaction history, a correct action, a plausible but incorrect alternative, and relevant tool metadata. We respectively utilize offline sampling to isolate local single-step errors and online sampling to capture realistic multi-step failures from full agent rollouts. A multi-LLM verification pipeline is proposed to reduce label noise and ensure data quality. We conduct extensive experiments across large language models, general PRMs, and tool-specialized PRMs on ToolPRMBench. The results reveal clear differences in PRM effectiveness and highlight the potential of specialized PRMs for tool-using. Code and data will be released at https://github.com/David-Li0406/ToolPRMBench.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12294v1</guid>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dawei Li, Yuguang Yao, Zhen Tan, Huan Liu, Ruocheng Guo</dc:creator>
    </item>
    <item>
      <title>Zero-Permission Manipulation: Can We Trust Large Multimodal Model Powered GUI Agents?</title>
      <link>https://arxiv.org/abs/2601.12349</link>
      <description>arXiv:2601.12349v1 Announce Type: cross 
Abstract: Large multimodal model powered GUI agents are emerging as high-privilege operators on mobile platforms, entrusted with perceiving screen content and injecting inputs. However, their design operates under the implicit assumption of Visual Atomicity: that the UI state remains invariant between observation and action. We demonstrate that this assumption is fundamentally invalid in Android, creating a critical attack surface.
  We present Action Rebinding, a novel attack that allows a seemingly-benign app with zero dangerous permissions to rebind an agent's execution. By exploiting the inevitable observation-to-action gap inherent in the agent's reasoning pipeline, the attacker triggers foreground transitions to rebind the agent's planned action toward the target app. We weaponize the agent's task-recovery logic and Android's UI state preservation to orchestrate programmable, multi-step attack chains. Furthermore, we introduce an Intent Alignment Strategy (IAS) that manipulates the agent's reasoning process to rationalize UI states, enabling it to bypass verification gates (e.g., confirmation dialogs) that would otherwise be rejected.
  We evaluate Action Rebinding Attacks on six widely-used Android GUI agents across 15 tasks. Our results demonstrate a 100% success rate for atomic action rebinding and the ability to reliably orchestrate multi-step attack chains. With IAS, the success rate in bypassing verification gates increases (from 0% to up to 100%). Notably, the attacker application requires no sensitive permissions and contains no privileged API calls, achieving a 0% detection rate across malware scanners (e.g., VirusTotal). Our findings reveal a fundamental architectural flaw in current agent-OS integration and provide critical insights for the secure design of future agent systems. To access experimental logs and demonstration videos, please contact yi_qian@smail.nju.edu.cn.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12349v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yi Qian, Kunwei Qian, Xingbang He, Ligeng Chen, Jikang Zhang, Tiantai Zhang, Haiyang Wei, Linzhang Wang, Hao Wu, Bing Mao</dc:creator>
    </item>
    <item>
      <title>The Cost of Convenience: Identifying, Analyzing, and Mitigating Predatory Loan Applications on Android</title>
      <link>https://arxiv.org/abs/2601.12634</link>
      <description>arXiv:2601.12634v1 Announce Type: cross 
Abstract: Digital lending applications, commonly referred to as loan apps, have become a primary channel for microcredit in emerging markets. However, many of these apps demand excessive permissions and misuse sensitive user data for coercive debt-recovery practices, including harassment, blackmail, and public shaming that affect both borrowers and their contacts.
  This paper presents the first cross-country measurement of loan app compliance against both national regulations and Google's Financial Services Policy. We analyze 434 apps drawn from official registries and app markets from Indonesia, Kenya, Nigeria, Pakistan, and the Philippines. To operationalize policy requirements at scale, we translate policy text into testable permission checks using LLM-assisted policy-to-permission mapping and combine this with static and dynamic analyses of loan apps' code and runtime behavior.
  Our findings reveal pervasive non-compliance among approved apps: 141 violate national regulatory policy and 147 violate Google policy. Dynamic analysis further shows that several apps transmit sensitive data (contacts, SMS, location, media) before user signup or registration, undermining informed consent and enabling downstream harassment of borrowers and third parties. Following our disclosures, Google removed 93 flagged apps from Google Play, representing over 300M cumulative installs.
  We advocate for adopting our methodology as a proactive compliance-monitoring tool and offer targeted recommendations for regulators, platforms, and developers to strengthen privacy protections. Overall, our results highlight the need for coordinated enforcement and robust technical safeguards to ensure that digital lending supports financial inclusion without compromising user privacy or safety.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12634v1</guid>
      <category>cs.CR</category>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3779208.3785263</arxiv:DOI>
      <dc:creator>Olawale Amos Akanji, Manuel Egele, Gianluca Stringhini</dc:creator>
    </item>
    <item>
      <title>Vision Language Models for Optimization-Driven Intent Processing in Autonomous Networks</title>
      <link>https://arxiv.org/abs/2601.12744</link>
      <description>arXiv:2601.12744v1 Announce Type: cross 
Abstract: Intent-Based Networking (IBN) allows operators to specify high-level network goals rather than low-level configurations. While recent work demonstrates that large language models can automate configuration tasks, a distinct class of intents requires generating optimization code to compute provably optimal solutions for traffic engineering, routing, and resource allocation. Current systems assume text-based intent expression, requiring operators to enumerate topologies and parameters in prose. Network practitioners naturally reason about structure through diagrams, yet whether Vision-Language Models (VLMs) can process annotated network sketches into correct optimization code remains unexplored. We present IntentOpt, a benchmark of 85 optimization problems across 17 categories, evaluating four VLMs (GPT-5-Mini, Claude-Haiku-4.5, Gemini-2.5-Flash, Llama-3.2-11B-Vision) under three prompting strategies on multimodal versus text-only inputs. Our evaluation shows that visual parameter extraction reduces execution success by 12-21 percentage points (pp), with GPT-5-Mini dropping from 93% to 72%. Program-of-thought prompting decreases performance by up to 13 pp, and open-source models lag behind closed-source ones, with Llama-3.2-11B-Vision reaching 18% compared to 75% for GPT-5-Mini. These results establish baseline capabilities and limitations of current VLMs for optimization code generation within an IBN system. We also demonstrate practical feasibility through a case study that deploys VLM-generated code to network testbed infrastructure using Model Context Protocol.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12744v1</guid>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tasnim Ahmed, Yifan Zhu, Salimur Choudhury</dc:creator>
    </item>
    <item>
      <title>The Energy-Throughput Trade-off in Lossless-Compressed Source Code Storage</title>
      <link>https://arxiv.org/abs/2601.13220</link>
      <description>arXiv:2601.13220v1 Announce Type: cross 
Abstract: Retrieving data from large-scale source code archives is vital for AI training, neural-based software analysis, and information retrieval, to cite a few. This paper studies and experiments with the design of a compressed key-value store for the indexing of large-scale source code datasets, evaluating its trade-off among three primary computational resources: (compressed) space occupancy, time, and energy efficiency. Extensive experiments on a national high-performance computing infrastructure demonstrate that different compression configurations yield distinct trade-offs, with high compression ratios and order-of-magnitude gains in retrieval throughput and energy efficiency. We also study data parallelism and show that, while it significantly improves speed, scaling energy efficiency is more difficult, reflecting the known non-energy-proportionality of modern hardware and challenging the assumption of a direct time-energy correlation. This work streamlines automation in energy-aware configuration tuning and standardized green benchmarking deployable in CI/CD pipelines, thus empowering system architects with a spectrum of Pareto-optimal energy-compression-throughput trade-offs and actionable guidelines for building sustainable, efficient storage backends for massive open-source code archival.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13220v1</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paolo Ferragina, Francesco Tosoni</dc:creator>
    </item>
    <item>
      <title>Eliciting Harmful Capabilities by Fine-Tuning On Safeguarded Outputs</title>
      <link>https://arxiv.org/abs/2601.13528</link>
      <description>arXiv:2601.13528v1 Announce Type: cross 
Abstract: Model developers implement safeguards in frontier models to prevent misuse, for example, by employing classifiers to filter dangerous outputs. In this work, we demonstrate that even robustly safeguarded models can be used to elicit harmful capabilities in open-source models through elicitation attacks. Our elicitation attacks consist of three stages: (i) constructing prompts in adjacent domains to a target harmful task that do not request dangerous information; (ii) obtaining responses to these prompts from safeguarded frontier models; (iii) fine-tuning open-source models on these prompt-output pairs. Since the requested prompts cannot be used to directly cause harm, they are not refused by frontier model safeguards. We evaluate these elicitation attacks within the domain of hazardous chemical synthesis and processing, and demonstrate that our attacks recover approximately 40% of the capability gap between the base open-source model and an unrestricted frontier model. We then show that the efficacy of elicitation attacks scales with the capability of the frontier model and the amount of generated fine-tuning data. Our work demonstrates the challenge of mitigating ecosystem level risks with output-level safeguards.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13528v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jackson Kaunismaa, Avery Griffin, John Hughes, Christina Q. Knight, Mrinank Sharma, Erik Jones</dc:creator>
    </item>
    <item>
      <title>Lessons from Formally Verified Deployed Software Systems (Extended version)</title>
      <link>https://arxiv.org/abs/2301.02206</link>
      <description>arXiv:2301.02206v4 Announce Type: replace 
Abstract: The technology of formal software verification has made spectacular advances, but how much does it actually benefit the development of practical software? Considerable disagreement remains about the practicality of building systems with mechanically-checked proofs of correctness. Is this prospect confined to a few expensive, life-critical projects, or can the idea be applied to a wide segment of the software industry? To help answer this question, the present survey examines a range of projects, in various application areas, that have produced formally verified systems and deployed them for actual use. It considers the technologies used, the form of verification applied, the results obtained, and the lessons that the software industry should draw regarding its ability to benefit from formal verification techniques and tools.
  Note: this version is the extended article, covering all the systems identified as relevant. A shorter version, covering only a selection, is also available (see https://doi.org/10.1145/3785652).</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.02206v4</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3785652</arxiv:DOI>
      <dc:creator>Li Huang, Sophie Ebersold, Alexander Kogtenkov, Bertrand Meyer, Yinling Liu</dc:creator>
    </item>
    <item>
      <title>Quantum Approximate Optimization Algorithm for Test Case Optimization</title>
      <link>https://arxiv.org/abs/2312.15547</link>
      <description>arXiv:2312.15547v2 Announce Type: replace 
Abstract: Test case optimization (TCO) reduces software testing cost while preserving its effectiveness, but solving TCO problems for large-scale and complex systems requires substantial computational resources. Quantum approximate optimization algorithms (QAOAs) are promising combinatorial optimization algorithms that rely on quantum computational resources, with the potential efficiency advantages over classical approaches. Several proof-of-concept applications of QAOAs for solving combinatorial problems, such as portfolio optimization, energy systems, and job scheduling, have been proposed. Given the lack of investigation into QAOA's application to TCO problems, and motivated by the computational challenges of TCO problems and the potential of QAOAs, we present IGDec-QAOA to formulate a TCO problem as a QAOA problem and solve it on both ideal and noisy quantum computer simulators, as well as on a real quantum computer. To solve bigger TCO problems that require many qubits, which are unavailable currently, we integrate a problem decomposition strategy with the QAOA. We performed an empirical evaluation with five TCO problems and four publicly available industrial datasets from ABB, Google, and Orona to compare various configurations of IGDec-QAOA, assess its decomposition strategy of handling large datasets, and compare its performance with classical algorithms (i.e., GA and Random Search). Based on the evaluation results achieved on an ideal simulator, we recommend the best configuration of our approach for TCO problems. We also demonstrate that it can reach the same effectiveness as GA and outperform GA in two out of five test case optimization problems. In addition, we observe that, on a noisy simulator, IGDec-QAOA achieved similar performance to that from an ideal simulator. Finally, we demonstrate the feasibility of IGDec-QAOA on a real quantum computer in the presence of noise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.15547v2</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TSE.2024.3479421</arxiv:DOI>
      <arxiv:journal_reference>in IEEE Transactions on Software Engineering, vol. 50, no. 12, pp. 3249-3264, Dec. 2024</arxiv:journal_reference>
      <dc:creator>Xinyi Wang, Shaukat Ali, Tao Yue, Paolo Arcaini</dc:creator>
    </item>
    <item>
      <title>Generator-Based Fuzzers with Type-Based Targeted Mutation</title>
      <link>https://arxiv.org/abs/2406.02034</link>
      <description>arXiv:2406.02034v4 Announce Type: replace 
Abstract: As with any fuzzer, directing Generator-Based Fuzzers (GBF) to reach particular code targets can increase the fuzzer's effectiveness. In previous work, coverage-guided fuzzers used a mix of static analysis, taint analysis, and constraint-solving approaches to address this problem. However, none of these techniques were particularly crafted for GBF where input generators are used to construct program inputs. The observation is that input generators carry information about the input structure that is naturally present through the typing composition of the program input.
  In this paper, we introduce a type-based mutation heuristic, along with constant string lookup, for Java GBF. Our key intuition is that if one can identify which sub-part (types) of the input will likely influence the branching decision, then focusing on mutating the choices of the generators constructing these types is likely to achieve the desired coverages. We used our technique to fuzz AWSLambda applications. Results compared to a baseline GBF tool show an almost 20\% average improvement in application coverage, and larger improvements when third-party code is included.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02034v4</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Soha Hussein, Stephen McCamant, Mike Whalen</dc:creator>
    </item>
    <item>
      <title>A Taxonomy of Real Faults in Hybrid Quantum-Classical Architectures</title>
      <link>https://arxiv.org/abs/2502.08739</link>
      <description>arXiv:2502.08739v3 Announce Type: replace 
Abstract: With the popularity of Hybrid Quantum-Classical architectures, particularly noisy intermediate-scale quantum (NISQ) architectures, comes the need for quality assurance methods tailored to their specific faults. In this study, we propose a taxonomy of faults in Hybrid Quantum-Classical architectures accompanied by a dataset of real faults in the identified categories. To achieve this, we empirically analysed open-source repositories for fixed faults. We analysed over 5000 closed issues on GitHub and pre-selected 529 of them based on rigorously defined inclusion criteria. We selected 133 faults that we labelled around symptoms and the origin of the faults. We cross-validated the classification and labels assigned to every fault between two of the authors. As a result, we introduced a taxonomy of real faults in Hybrid Quantum-Classical architectures. Subsequently, we validated the taxonomy through interviews conducted with eleven developers. The taxonomy was dynamically updated throughout the cross-validation and interview processes. The final version was validated and discussed through surveys conducted with an independent group of domain experts to ensure its relevance and to gain further insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08739v3</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3788677</arxiv:DOI>
      <dc:creator>Avner Bensoussan, Gunel Jahangirova, Mohammad Reza Mousavi</dc:creator>
    </item>
    <item>
      <title>StructEval: Benchmarking LLMs' Capabilities to Generate Structural Outputs</title>
      <link>https://arxiv.org/abs/2505.20139</link>
      <description>arXiv:2505.20139v2 Announce Type: replace 
Abstract: As Large Language Models (LLMs) become integral to software development workflows, their ability to generate structured outputs has become critically important. We introduce StructEval, a comprehensive benchmark for evaluating LLMs' capabilities in producing both non-renderable (JSON, YAML, CSV) and renderable (HTML, React, SVG) structured formats. Unlike prior benchmarks, StructEval systematically evaluates structural fidelity across diverse formats through two paradigms: 1) generation tasks, producing structured output from natural language prompts, and \textbf{2)} conversion tasks, translating between structured formats. Our benchmark encompasses 18 formats and 44 types of task, with novel metrics for format adherence and structural correctness. Results reveal significant performance gaps-even state-of-the-art models like o1-mini achieve only 75.58 average score, with open-source alternatives lagging approximately 10 points behind. We find generation tasks more challenging than conversion tasks, and producing correct visual content more difficult than generating text-only structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20139v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jialin Yang, Dongfu Jiang, Lipeng He, Sherman Siu, Yuxuan Zhang, Disen Liao, Zhuofeng Li, Huaye Zeng, Yiming Jia, Haozhe Wang, Benjamin Schneider, Chi Ruan, Wentao Ma, Zhiheng Lyu, Yifei Wang, Yi Lu, Quy Duc Do, Ziyan Jiang, Ping Nie, Wenhu Chen</dc:creator>
    </item>
    <item>
      <title>Integrating Symbolic Execution with LLMs for Automated Generation of Program Specifications</title>
      <link>https://arxiv.org/abs/2506.09550</link>
      <description>arXiv:2506.09550v4 Announce Type: replace 
Abstract: Automatically generating formal specifications including loop invariants, preconditions, and postconditions for legacy code is critical for program understanding, reuse and verification. However, the inherent complexity of control and data structures in programs makes this task particularly challenging. This paper presents a novel framework that integrates symbolic execution with large language models (LLMs) to automatically synthesize formally verified program specifications. Our method first employs symbolic execution to derive precise strongest postconditions for loop-free code segments. These symbolic execution results, along with automatically generated invariant templates, then guide the LLM to propose and iteratively refine loop invariants until a correct specification is obtained. The template-guided generation process robustly combines symbolic inference with LLM reasoning, significantly reducing hallucinations and syntactic errors by structurally constraining the LLM's output space. Furthermore, our approach can produce strong specifications without relying on externally provided verification goals, enabled by the rich semantic context supplied by symbolic execution, overcoming a key limitation of prior goal-dependent tools. Extensive evaluation shows that our tool SESpec outperforms the existing state-of-the-art tools across numerical and data-structure benchmarks, demonstrating both high precision and broad applicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09550v4</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fanpeng Yang, Xu Ma, Shuling Wang, Xiong Xu, Qinxiang Cao, Naijun Zhan, Xiaofeng Li, Bin Gu</dc:creator>
    </item>
    <item>
      <title>The CAISAR Platform: Extending the Reach of Machine Learning Specification and Verification</title>
      <link>https://arxiv.org/abs/2506.12084</link>
      <description>arXiv:2506.12084v2 Announce Type: replace 
Abstract: The formal specification and verification of machine learning programs saw remarkable progress in less than a decade, leading to a profusion of tools. However, diversity may lead to fragmentation, resulting in tools that are difficult to compare, except for very specific benchmarks. Furthermore, this progress is heavily geared towards the specification and verification of a certain class of property, that is, local robustness properties. But while provers are becoming more and more efficient at solving local robustness properties, even slightly more complex properties, involving multiple neural networks for example, cannot be expressed in the input languages of winners of the International Competition of Verification of Neural Networks VNN-Comp. In this tool paper, we present CAISAR, an open-source platform dedicated to machine learning specification and verification. We present its specification language, suitable for modelling complex properties on neural networks, support vector machines and boosted trees. We show on concrete use-cases how specifications written in this language are automatically translated to queries to state-of-the-art provers, notably by using automated graph editing techniques, making it possible to use their off-the-shelf versions. The artifact to reproduce the paper claims is available at the following DOI: https://doi.org/10.5281/zenodo.15209510</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12084v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.FL</category>
      <category>cs.NE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michele Alberti (LSL), Fran\c{c}ois Bobot (LSL), Julien Girard-Satabin (LSL), Alban Grastien (LSL), Aymeric Varasse (LSL), Zakaria Chihani (LSL)</dc:creator>
    </item>
    <item>
      <title>Challenges and Practices in Quantum Software Testing and Debugging: Insights from Practitioners</title>
      <link>https://arxiv.org/abs/2506.17306</link>
      <description>arXiv:2506.17306v2 Announce Type: replace 
Abstract: Quantum software engineering is an emerging discipline with distinct challenges, particularly in testing and debugging. As quantum computing transitions from theory to implementation, developers face issues not present in classical software development, such as probabilistic execution, limited observability, shallow abstractions, and low awareness of quantum-specific tools. To better understand current practices, we surveyed 26 quantum software developers from academia and industry and conducted follow-up interviews focused on testing, debugging, and recurring challenges. All participants reported engaging in testing, with unit testing (88%), regression testing (54%), and acceptance testing (54%) being the most common. However, only 31% reported using quantum-specific testing tools, relying instead on classical and manual methods. Debugging practices were similarly grounded in classical strategies, such as print statements, circuit visualizations, and simulators, which respondents noted do not scale well. The most frequently cited sources of bugs were classical in nature: library updates (81%), developer errors (69%), and compatibility issues (62%)-often worsened by limited abstraction in existing quantum SDKs. These findings highlight the urgent need for better-aligned testing and debugging tools integrated more seamlessly into the workflows of quantum developers. We present these results in detail and offer actionable recommendations grounded in the real-world needs of practitioners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17306v2</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jake Zappin, Trevor Stalnaker, Oscar Chaparro, Denys Poshyvanyk</dc:creator>
    </item>
    <item>
      <title>May the Feedback Be with You! Unlocking the Power of Feedback-Driven Deep Learning Framework Fuzzing via LLMs</title>
      <link>https://arxiv.org/abs/2506.17642</link>
      <description>arXiv:2506.17642v4 Announce Type: replace 
Abstract: Deep Learning (DL) frameworks have served as fundamental components in DL systems over the last decade. However, bugs in DL frameworks could lead to catastrophic consequences in critical scenarios. A simple yet effective way to find bugs in DL frameworks is fuzz testing (Fuzzing). Existing approaches focus on test generation, leaving execution results with high semantic value (e.g., coverage information, bug reports, and exception logs) in the wild, which can serve as multiple types of feedback. To fill this gap, we propose FUEL to effectively utilize the feedback information, which comprises two Large Language Models (LLMs): analysis LLM and generation LLM. Specifically, analysis LLM infers analysis summaries from feedback information, while the generation LLM creates tests guided by these summaries. Furthermore, based on multiple feedback guidance, we design two additional components: (i) a feedback-aware simulated annealing algorithm to select operators for test generation, enriching test diversity. (ii) a program self-repair strategy to automatically repair invalid tests, enhancing test validity. We evaluate FUEL on the two most popular DL frameworks, and experiment results show that FUEL can improve line code coverage of PyTorch and TensorFlow by 4.48% and 9.14% over four state-of-the-art baselines. By the time of submission, FUEL has detected 104 previously unknown bugs for PyTorch and TensorFlow, with 93 confirmed as new bugs, 53 already fixed. 14 vulnerabilities have been assigned CVE IDs, among which 7 are rated as high-severity with a CVSS score of "7.5 HIGH". Our artifact is available at https://github.com/NJU-iSE/FUEL</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17642v4</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shaoyu Yang, Chunrong Fang, Haifeng Lin, Xiang Chen, Jia Liu, Zhenyu Chen</dc:creator>
    </item>
    <item>
      <title>Assessing Small Language Models for Code Generation: An Empirical Study with Benchmarks</title>
      <link>https://arxiv.org/abs/2507.03160</link>
      <description>arXiv:2507.03160v4 Announce Type: replace 
Abstract: The recent advancements of Small Language Models (SLMs) have opened new possibilities for efficient code generation. SLMs offer lightweight and cost-effective alternatives to Large Language Models (LLMs), making them attractive for use in resource-constrained environments. However, empirical understanding of SLMs, particularly their capabilities, limitations, and performance trade-offs in code generation remains limited. This study presents a comprehensive empirical evaluation of 20 open-source SLMs ranging from 0.4B to 10B parameters on five diverse code-related benchmarks (HumanEval, MBPP, Mercury, HumanEvalPack, and CodeXGLUE). The models are assessed along three dimensions: i) functional correctness of generated code, ii) computational efficiency and iii) performance across multiple programming languages. The findings of this study reveal that several compact SLMs achieve competitive results while maintaining a balance between performance and efficiency, making them viable for deployment in resource-constrained environments. However, achieving further improvements in accuracy requires switching to larger models. These models generally outperform their smaller counterparts, but they require much more computational power. We observe that for 10% performance improvements, models can require nearly a 4x increase in VRAM consumption, highlighting a trade-off between effectiveness and scalability. Besides, the multilingual performance analysis reveals that SLMs tend to perform better in languages such as Python, Java, and PHP, while exhibiting relatively weaker performance in Go, C++, and Ruby. However, statistical analysis suggests these differences are not significant, indicating a generalizability of SLMs across programming languages. Based on the findings, this work provides insights into the design and selection of SLMs for real-world code generation tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03160v4</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Mahade Hasan, Muhammad Waseem, Kai-Kristian Kemell, Jussi Rasku, Juha Ala-Rantala, Pekka Abrahamsson</dc:creator>
    </item>
    <item>
      <title>CoRe: Benchmarking LLMs Code Reasoning Capabilities through Static Analysis Tasks</title>
      <link>https://arxiv.org/abs/2507.05269</link>
      <description>arXiv:2507.05269v3 Announce Type: replace 
Abstract: Large language models (LLMs) have been widely adopted across diverse domains of software engineering, such as code generation, program repair, and vulnerability detection. These applications require understanding beyond surface-level code patterns: value propagation, control flow, and interdependence between program elements. However, existing benchmarks primarily evaluate end-to-end outcomes, such as whether code is correctly repaired or generated, leaving the models' ability for program semantic reasoning underexplored. This work presents CORE, a high-quality, human-verified benchmark designed to evaluate LLMs on fundamental static analysis tasks. CORE includes 12,553 task instances spanning data dependency, control dependency, and information flow across programs written in C/C++, Java, and Python. To ensure semantic diversity and reasoning complexity, we propose a semantics-aware diverse sampling strategy that selects targets and task instances based on structural coverage and dependency depth. We evaluate 10 mainstream LLMs and show that, while they perform well at identifying dependencies, models still struggle with tasks that require deeper semantic understanding and multi-step reasoning. We further conduct qualitative analyses to uncover key challenges, such as complex control structures and backward dependency patterns, offering insights into improving LLMs' code reasoning capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05269v3</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Danning Xie, Mingwei Zheng, Xuwei Liu, Jiannan Wang, Chengpeng Wang, Lin Tan, Xiangyu Zhang</dc:creator>
    </item>
    <item>
      <title>Code2MCP: Transforming Code Repositories into MCP Services</title>
      <link>https://arxiv.org/abs/2509.05941</link>
      <description>arXiv:2509.05941v3 Announce Type: replace 
Abstract: The Model Context Protocol (MCP) aims to create a standard for how Large Language Models use tools. However, most current research focuses on selecting tools from an existing pool. A more fundamental, yet largely overlooked, problem is how to populate this pool by converting the vast number of existing software projects into MCP-compatible services. To bridge this gap, we introduce Code2MCP, an agent-based framework that automatically transforms a GitHub repository into a functional MCP service with minimal human intervention. Code2MCP employs a multi-agent workflow for code analysis, environment setup, tool function design, and service generation, enhanced by a self-correcting loop to ensure reliability. We demonstrate that Code2MCP successfully transforms open-source computing libraries in scientific fields such as bioinformatics, mathematics, and fluid dynamics that are not available in existing MCP servers. By providing a novel automated pathway to unlock GitHub, the world's largest code repository, for the MCP ecosystem, Code2MCP serves as a catalyst to significantly accelerate the protocol's adoption and practical application. The code is public at https://github.com/DEFENSE-SEU/Code2MCP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05941v3</guid>
      <category>cs.SE</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chaoqian Ouyang, Ling Yue, Shimin Di, Libin Zheng, Linan Yue, Shaowu Pan, Jian Yin, Min-Ling Zhang</dc:creator>
    </item>
    <item>
      <title>TransLibEval: Demystify Large Language Models' Capability in Third-party Library-targeted Code Translation</title>
      <link>https://arxiv.org/abs/2509.12087</link>
      <description>arXiv:2509.12087v2 Announce Type: replace 
Abstract: In recent years, Large Language Models (LLMs) have been widely studied in the code translation field on the method, class, and even repository levels. However, most of these benchmarks are limited in terms of Third-Party Library (TPL) categories and scales, making TPL-related errors hard to expose and hindering the development of targeted solutions. Considering the high dependence (over 90%) on TPLs in practical programming, demystifying and analyzing LLMs' code translation performance involving various TPLs becomes imperative. To address this gap, we construct TransLibEval, the first benchmark dedicated to library-centric code translation. It consists of 200 real-world tasks across Python, Java, and C++, each explicitly involving TPLs from diverse categories such as data processing, machine learning, and web development, with comprehensive dependency coverage and high-coverage test suites. We evaluate seven recent LLMs of commercial, general, and code-specialized families under six translation strategies of three categories: Direct, IR-guided, and Retrieval-augmented. Experimental results show a dramatic performance drop compared with library-free settings (average CA decline over 60%), while diverse strategies demonstrate heterogeneous advantages. Furthermore, we analyze 4,831 failed cases from GPT-4o, one of the State-of-the-Art (SOTA) LLMs, revealing numerous third-party reference errors that were obscured previously. These findings highlight the unique challenges of library-centric translation and provide practical guidance for improving TPL-aware code intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12087v2</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Pengyu Xue, Kunwu Zheng, Zhen Yang, Yifei Pei, Linhao Wu, Jiahui Dong, Xiapu Luo, Yan Xiao, Fei Liu, Yuxuan Zhang, Xiran Lyu, Xianhang Li, Xuanyu Zhu, Chengyi Wang</dc:creator>
    </item>
    <item>
      <title>SandCell: Sandboxing Rust Beyond Unsafe Code</title>
      <link>https://arxiv.org/abs/2509.24032</link>
      <description>arXiv:2509.24032v2 Announce Type: replace 
Abstract: Rust is a modern systems programming language that ensures memory safety by enforcing ownership and borrowing rules at compile time. While the unsafe keyword allows programmers to bypass these restrictions, it introduces significant risks. Various approaches for isolating unsafe code to protect safe Rust from vulnerabilities have been proposed, yet these methods provide only fixed isolation boundaries and do not accommodate expressive policies that require sandboxing both safe and unsafe code. This paper presents SandCell for flexible and lightweight isolation in Rust by leveraging existing syntactic boundaries. SandCell allows programmers to specify which components to sandbox with minimal annotation effort, enabling fine-grained control over isolation. The system also introduces novel techniques to minimize overhead when transferring data between sandboxes. Our evaluation demonstrates SandCell's effectiveness in preventing vulnerabilities across various Rust applications while maintaining reasonable performance overheads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24032v2</guid>
      <category>cs.SE</category>
      <category>cs.CR</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jialun Zhang, Merve Gulmez, Thomas Nyman, Gang Tan</dc:creator>
    </item>
    <item>
      <title>C2|Q&gt;: A Robust Framework for Bridging Classical and Quantum Software Development</title>
      <link>https://arxiv.org/abs/2510.02854</link>
      <description>arXiv:2510.02854v2 Announce Type: replace 
Abstract: Quantum Software Engineering (QSE) is emerging as a critical discipline to make quantum computing accessible to a broader developer community; however, most quantum development environments still require developers to engage with low-level details across the software stack - including problem encoding, circuit construction, algorithm configuration, hardware selection, and result interpretation - making them difficult for classical software engineers to use. To bridge this gap, we present C2|Q&gt;, a hardware-agnostic quantum software development framework that translates specific types of classical specifications into quantum-executable programs while preserving methodological rigor. The framework applies modular software engineering principles by classifying the workflow into three core modules: an encoder that classifies problems, produces Quantum-Compatible Formats (QCFs), and constructs quantum circuits, a deployment module that generates circuits and recommends hardware based on fidelity, runtime, and cost, and a decoder that interprets quantum outputs into classical solutions. This architecture supports systematic evaluation across simulators and Noisy Intermediate-Scale Quantum (NISQ) quantum devices, remaining scalable to new problem classes and algorithms. In evaluation, the encoder module achieved a 93.8% completion rate, the hardware recommendation module consistently selected the appropriate quantum devices for workloads scaling up to 56 qubits. These results indicate that C2|Q&gt; lowers the entry barrier to quantum software development by providing a reproducible, extensible toolchain that connects classical specifications to quantum execution. The open-source implementation of C2|Q&gt; is available at https://github.com/C2-Q/C2Q.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02854v2</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Boshuai Ye, Arif Ali Khan, Teemu Pihkakoski, Peng Liang, Muhammad Azeem Akbar, Matti Silveri, Lauri Malmi</dc:creator>
    </item>
    <item>
      <title>Saving SWE-Bench: A Benchmark Mutation Approach for Realistic Agent Evaluation</title>
      <link>https://arxiv.org/abs/2510.08996</link>
      <description>arXiv:2510.08996v3 Announce Type: replace 
Abstract: Current benchmarks for evaluating software engineering agents, such as SWE-Bench Verified, are predominantly derived from GitHub issues and fail to accurately reflect how developers interact with chat-based coding assistants in integrated development environments (IDEs). We posit that this mismatch leads to a systematic overestimation of agent's capabilities in real-world scenarios, especially bug fixing. We introduce a novel benchmarking framework that transforms existing formal benchmarks into realistic user queries through systematic analysis of developer interaction patterns with chat-based agents. Our methodology is flexible and can be easily extended to existing benchmarks. In this paper, we apply our testing framework to SWE-Bench Verified, the TypeScript subset of Multi-SWE-Bench and a private benchmark, SWE-Bench C# and transform formal GitHub issue descriptions into realistic user-style queries based on telemetry analysis of a popular chat-based agent interactions. Our findings reveal that existing benchmarks significantly overestimate agent capabilities for some models by &gt;50% over baseline performance for public benchmarks and ~10-16% for our internal benchmark. This work establishes a new paradigm for evaluating interactive chat-based software engineering agents through benchmark mutation techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08996v3</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Spandan Garg, Benjamin Steenhoek, Yufan Huang</dc:creator>
    </item>
    <item>
      <title>When Abstraction Breaks Physics: Rethinking Modular Design in Quantum Software</title>
      <link>https://arxiv.org/abs/2510.18557</link>
      <description>arXiv:2510.18557v2 Announce Type: replace 
Abstract: Abstraction is a fundamental principle in classical software engineering, which enables modularity, reusability, and scalability. However, quantum programs adhere to fundamentally different semantics, such as unitarity, entanglement, the no-cloning theorem, and the destructive nature of measurement, which introduce challenges to the safe use of classical abstraction mechanisms. This paper identifies a fundamental conflict in quantum software engineering: abstraction practices that are syntactically valid may violate the physical constraints of quantum computation. We present three classes of failure cases where naive abstraction breaks quantum semantics and propose a set of design principles for physically sound abstraction mechanisms. We further propose research directions, including quantum-specific type systems, effect annotations, and contract-based module design. Our goal is to initiate a systematic rethinking of abstraction in quantum software engineering, based on quantum semantics and considering engineering scalability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18557v2</guid>
      <category>cs.SE</category>
      <category>quant-ph</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianjun Zhao</dc:creator>
    </item>
    <item>
      <title>Empirical and Sustainability Aspects of Software Engineering Research in the Era of Large Language Models: A Reflection</title>
      <link>https://arxiv.org/abs/2510.26538</link>
      <description>arXiv:2510.26538v3 Announce Type: replace 
Abstract: Software Engineering (SE) research involving the use of Large Language Models (LLMs) has introduced several new challenges related to rigour in benchmarking, contamination, replicability, and sustainability. In this paper, we invite the research community to reflect on how these challenges are addressed in SE. Our results provide a structured overview of current LLM-based SE research at ICSE, highlighting both encouraging practices and persistent shortcomings. We conclude with recommendations to strengthen benchmarking rigour, improve replicability, and address the financial and environmental costs of LLM-based SE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26538v3</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Williams, Max Hort, Maria Kechagia, Aldeida Aleti, Justyna Petke, Federica Sarro</dc:creator>
    </item>
    <item>
      <title>Process-based Indicators of Vulnerability Re-Introducing Code Changes: An Exploratory Case Study</title>
      <link>https://arxiv.org/abs/2510.26676</link>
      <description>arXiv:2510.26676v2 Announce Type: replace 
Abstract: Software vulnerabilities often persist or re-emerge even after being fixed, revealing the complex interplay between code evolution and socio-technical factors. While source code metrics provide useful indicators of vulnerabilities, software engineering process metrics can uncover patterns that lead to their introduction. Yet few studies have explored whether process metrics can reveal risky development activities over time -- insights that are essential for anticipating and mitigating software vulnerabilities. This work highlights the critical role of process metrics along with code changes in understanding and mitigating vulnerability reintroduction. We move beyond file-level prediction and instead analyze security fixes at the commit level, focusing not only on whether a single fix introduces a vulnerability but also on the longer sequences of changes through which vulnerabilities evolve and re-emerge. Our approach emphasizes that reintroduction is rarely the result of one isolated action, but emerges from cumulative development activities and socio-technical conditions. To support this analysis, we conducted a case study on the ImageMagick project by correlating longitudinal process metrics such as bus factor, issue density, and issue spoilage with vulnerability reintroduction activities, encompassing 76 instances of reintroduced vulnerabilities. Our findings show that reintroductions often align with increased issue spoilage and fluctuating issue density, reflecting short-term inefficiencies in issue management and team responsiveness. These observations provide a foundation for broader studies that combine process and code metrics to predict risky fixes and strengthen software security.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26676v2</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samiha Shimmi, Nicholas M. Synovic, Mona Rahimi, George K. Thiruvathukal</dc:creator>
    </item>
    <item>
      <title>Mind the GAPS: Bridging the GAPS between Targeted Dynamic Analysis and Static Path Reconstruction in Android Apps</title>
      <link>https://arxiv.org/abs/2511.23213</link>
      <description>arXiv:2511.23213v2 Announce Type: replace 
Abstract: Dynamically executing specific target methods in Android applications remains a critical and unresolved challenge. Despite notable advancements in GUI testing, current tools are insufficient for reliably driving execution toward specific target methods.
  To address this challenge, we present GAPS (Graph-based Automated Path Synthesizer), the first system that leverages static, method-guided call graph reconstruction to guide the dynamic, interaction-driven execution of an Android app. GAPS performs a lightweight backward traversal of the call graph, guided by data-flow analysis, to reconstruct paths reaching the target methods. These paths are then translated into instructions that guide runtime app exploration.
  On the AndroTest benchmark, GAPS statically identifies paths towards 88.24% of the target methods, averaging just 4.27 seconds per app, and reaching 57.44% of them through dynamic analysis. This performance exceeds the state-of-the-art tools' one: the model-based GUI tester APE reaches only 12.82%, the hybrid tool GoalExplorer reaches 9.69%, and the LLM-based Guardian reaches 17.12%. Finally, we applied GAPS to the 50 most downloaded apps from the Google Play Store, achieving an average static analysis time of 278.9 seconds to reconstruct paths towards 62.03% of the target methods and reaching 59.86% of them through dynamic analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.23213v2</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Samuele Doria, Eleonora Losiouk</dc:creator>
    </item>
    <item>
      <title>Beyond Blind Spots: Analytic Hints for Mitigating LLM-Based Evaluation Pitfalls</title>
      <link>https://arxiv.org/abs/2512.16272</link>
      <description>arXiv:2512.16272v2 Announce Type: replace 
Abstract: Large Language Models are increasingly deployed as judges (LaaJ) in code generation pipelines. While attractive for scalability, LaaJs tend to overlook domain specific issues raising concerns about their reliability in critical evaluation tasks. To better understand these limitations in practice, we examine LaaJ behavior in a concrete industrial use case: legacy code modernization via COBOL code generation. In this setting, we find that even production deployed LaaJs can miss domain critical errors, revealing consistent blind spots in their evaluation capabilities.
  To better understand these blind spots, we analyze generated COBOL programs and associated LaaJs judgments, drawing on expert knowledge to construct a preliminary taxonomy. Based on this taxonomy, we develop a lightweight analytic checker tool that flags over 30 domain specific issues observed in practice. We use its outputs as analytic hints, dynamically injecting them into the judges prompt to encourage LaaJ to revisit aspects it may have overlooked.
  Experiments on a test set of 100 programs using four production level LaaJs show that LaaJ alone detects only about 45-63% of the errors present in the code (in all judges we tested), while the analytic checker alone lacks explanatory depth. When combined, the LaaJ+Hints configuration achieves up to 74% coverage (for the best performing judge and injection prompt) and produces qualitatively richer, more accurate explanations, demonstrating that analytic-LLM hybrids can substantially enhance evaluation reliability in deployed pipelines. We release the dataset and all used prompts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16272v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ora Nova Fandina, Eitan Farchi, Shmulik Froimovich, Raviv Gal, Wesam Ibraheem, Rami Katan, Alice Podolsky</dc:creator>
    </item>
    <item>
      <title>What You Trust Is Insecure: Demystifying How Developers (Mis)Use Trusted Execution Environments in Practice</title>
      <link>https://arxiv.org/abs/2512.17363</link>
      <description>arXiv:2512.17363v3 Announce Type: replace 
Abstract: Trusted Execution Environments (TEEs), such as Intel SGX and ARM TrustZone, provide isolated regions of CPU and memory for secure computation and are increasingly used to protect sensitive data and code across diverse application domains. However, little is known about how developers actually use TEEs in practice. This paper presents the first large-scale empirical study of real-world TEE applications. We collected and analyzed 241 open-source projects from GitHub that utilize the two most widely-adopted TEEs, Intel SGX and ARM TrustZone. By combining manual inspection with customized static analysis scripts, we examined their adoption contexts, usage patterns, and development practices across three phases. First, we categorized the projects into 8 application domains and identified trends in TEE adoption over time. We found that the dominant use case is IoT device security (30%), which contrasts sharply with prior academic focus on blockchain and cryptographic systems (7%), while AI model protection (12%) is rapidly emerging as a growing domain. Second, we analyzed how TEEs are integrated into software and observed that 32.4% of the projects reimplement cryptographic functionalities instead of using official SDK APIs, suggesting that current SDKs may have limited usability and portability to meet developers' practical needs. Third, we examined security practices through manual inspection and found that 25.3% (61 of 241) of the projects exhibit insecure coding behaviors when using TEEs, such as hardcoded secrets and missing input validation, which undermine their intended security guarantees. Our findings have important implications for improving the usability of TEE SDKs and supporting developers in trusted software development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17363v3</guid>
      <category>cs.SE</category>
      <category>cs.CR</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuqing Niu, Jieke Shi, Ruidong Han, Ye Liu, Chengyan Ma, Yunbo Lyu, David Lo</dc:creator>
    </item>
    <item>
      <title>Focus on What Matters: Fisher-Guided Adaptive Multimodal Fusion for Vulnerability Detection</title>
      <link>https://arxiv.org/abs/2601.02438</link>
      <description>arXiv:2601.02438v2 Announce Type: replace 
Abstract: Software vulnerability detection can be formulated as a binary classification problem that determines whether a given code snippet contains security defects. Existing multimodal methods typically fuse Natural Code Sequence (NCS) representations extracted by pretrained models with Code Property Graph (CPG) representations extracted by graph neural networks, under the implicit assumption that introducing an additional modality necessarily yields information gain. Through empirical analysis, we demonstrate the limitations of this assumption: pretrained models already encode substantial structural information implicitly, leading to strong overlap between the two modalities; moreover, graph encoders are generally less effective than pretrained language models in feature extraction. As a result, naive fusion not only struggles to obtain complementary signals but can also dilute effective discriminative cues due to noise propagation. To address these challenges, we propose a task-conditioned complementary fusion strategy that uses Fisher information to quantify task relevance, transforming cross-modal interaction from full-spectrum matching into selective fusion within a task-sensitive subspace. Our theoretical analysis shows that, under an isotropic perturbation assumption, this strategy significantly tightens the upper bound on the output error. Based on this insight, we design the TaCCS-DFA framework, which combines online low-rank Fisher subspace estimation with an adaptive gating mechanism to enable efficient task-oriented fusion. Experiments on the BigVul, Devign, and ReVeal benchmarks demonstrate that TaCCS-DFA delivers up to a 6.3-point gain in F1 score with only a 3.4% increase in inference latency, while maintaining low calibration error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.02438v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yun Bian, Yi Chen, HaiQuan Wang, ShiHao Li, Zhe Cui</dc:creator>
    </item>
    <item>
      <title>Self-Admitted Technical Debt in LLM Software: An Empirical Comparison with ML and Non-ML Software</title>
      <link>https://arxiv.org/abs/2601.06266</link>
      <description>arXiv:2601.06266v3 Announce Type: replace 
Abstract: Self-admitted technical debt (SATD), referring to comments flagged by developers that explicitly acknowledge suboptimal code or incomplete functionality, has received extensive attention in machine learning (ML) and traditional (Non-ML) software. However, little is known about how SATD manifests and evolves in contemporary Large Language Model (LLM)-based systems, whose architectures, workflows, and dependencies differ fundamentally from both traditional and pre-LLM ML software. In this paper, we conduct the first empirical study of SATD in the LLM era, replicating and extending prior work on ML technical debt to modern LLM-based systems. We compare SATD prevalence across LLM, ML, and non-ML repositories across a total of 477 repositories (159 per category). We perform survival analysis of SATD introduction and removal to understand the dynamics of technical debt across different development paradigms. Surprisingly, despite their architectural complexity, our results reveal that LLM repositories accumulate SATD at similar rates to ML systems (3.95% vs. 4.10%). However, we observe that LLM repositories remain debt-free 2.4x longer than ML repositories (a median of 492 days vs. 204 days), and then start to accumulate technical debt rapidly. Moreover, our qualitative analysis of 377 SATD instances reveals three new forms of technical debt unique to LLM-based development that have not been reported in prior research: Model-Stack Workaround Debt, Model Dependency Debt, and Performance Optimization Debt. Finally, by mapping SATD to stages of the LLM development pipeline, we observe that debt concentrates</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06266v3</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Niruthiha Selvanayagam, Taher A. Ghaleb, Manel Abdellatif</dc:creator>
    </item>
    <item>
      <title>LAUDE: LLM-Assisted Unit Test Generation and Debugging of Hardware DEsigns</title>
      <link>https://arxiv.org/abs/2601.08856</link>
      <description>arXiv:2601.08856v2 Announce Type: replace 
Abstract: Unit tests are critical in the hardware design lifecycle to ensure that component design modules are functionally correct and conform to the specification before they are integrated at the system level. Thus developing unit tests targeting various design features requires deep understanding of the design functionality and creativity. When one or more unit tests expose a design failure, the debugging engineer needs to diagnose, localize, and debug the failure to ensure design correctness, which is often a painstaking and intense process. In this work, we introduce LAUDE, a unified unit-test generation and debugging framework for hardware designs that cross-pollinates the semantic understanding of the design source code with the Chain-of-Thought (CoT) reasoning capabilities of foundational Large-Language Models (LLMs). LAUDE integrates prompt engineering and design execution information to enhance its unit test generation accuracy and code debuggability. We apply LAUDE with closed- and open-source LLMs to a large corpus of buggy hardware design codes derived from the VerilogEval dataset, where generated unit tests detected bugs in up to 100% and 93% of combinational and sequential designs and debugged up to 93% and 84% of combinational and sequential designs, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.08856v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Deeksha Nandal, Riccardo Revalor, Soham Dan, Debjit Pal</dc:creator>
    </item>
    <item>
      <title>Revisiting Software Engineering Education in the Era of Large Language Models: A Curriculum Adaptation and Academic Integrity Framework</title>
      <link>https://arxiv.org/abs/2601.08857</link>
      <description>arXiv:2601.08857v2 Announce Type: replace 
Abstract: The integration of Large Language Models (LLMs), such as ChatGPT and GitHub Copilot, into professional workflows is increasingly reshaping software engineering practices. These tools have lowered the cost of code generation, explanation, and testing, while introducing new forms of automation into routine development tasks. In contrast, most of the software engineering and computer engineering curricula remain closely aligned with pedagogical models that equate manual syntax production with technical competence. This growing misalignment raises concerns regarding assessment validity, learning outcomes, and the development of foundational skills. Adopting a conceptual research approach, this paper proposes a theoretical framework for analyzing how generative AI alters core software engineering competencies and introduces a pedagogical design model for LLM-integrated education. Attention is given to computer engineering programs in Turkey, where centralized regulation, large class sizes, and exam-oriented assessment practices amplify these challenges. The framework delineates how problem analysis, design, implementation, and testing increasingly shift from construction toward critique, validation, and human-AI stewardship. In addition, the paper argues that traditional plagiarism-centric integrity mechanisms are becoming insufficient, motivating a transition toward a process transparency model. While this work provides a structured proposal for curriculum adaptation, it remains a theoretical contribution; the paper concludes by outlining the need for longitudinal empirical studies to evaluate these interventions and their long-term impacts on learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.08857v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mustafa Degerli</dc:creator>
    </item>
    <item>
      <title>LLM-Based Agentic Systems for Software Engineering: Challenges and Opportunities</title>
      <link>https://arxiv.org/abs/2601.09822</link>
      <description>arXiv:2601.09822v2 Announce Type: replace 
Abstract: Despite recent advancements in Large Language Models (LLMs), complex Software Engineering (SE) tasks require more collaborative and specialized approaches. This concept paper systematically reviews the emerging paradigm of LLM-based multi-agent systems, examining their applications across the Software Development Life Cycle (SDLC), from requirements engineering and code generation to static code checking, testing, and debugging. We delve into a wide range of topics such as language model selection, SE evaluation benchmarks, state-of-the-art agentic frameworks and communication protocols. Furthermore, we identify key challenges and outline future research opportunities, with a focus on multi-agent orchestration, human-agent coordination, computational cost optimization, and effective data collection. This work aims to provide researchers and practitioners with valuable insights into the current forefront landscape of agentic systems within the software engineering domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09822v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yongjian Tang, Thomas Runkler</dc:creator>
    </item>
    <item>
      <title>Learning Randomized Reductions</title>
      <link>https://arxiv.org/abs/2412.18134</link>
      <description>arXiv:2412.18134v3 Announce Type: replace-cross 
Abstract: A self-corrector for a function $f$ takes a black-box oracle computing $f$ that is correct on most inputs and turns it into one that is correct on every input with high probability. Self-correctors exist for any function that is randomly self-reducible (RSR), where the value $f$ at a given point $x$ can be recovered by computing $f$ on random correlated points. While RSRs enable powerful self-correction capabilities and have applications in complexity theory and cryptography, their discovery has traditionally required manual derivation by experts. We present Bitween, a method and tool for automated learning of randomized self-reductions for mathematical functions. We make two key contributions: First, we demonstrate that our learning framework based on linear regression outperforms sophisticated methods including genetic algorithms, symbolic regression, and mixed-integer linear programming for discovering RSRs from correlated samples. Second, we introduce Agentic Bitween, a neuro-symbolic approach where large language models dynamically discover novel query functions for RSR property discovery, leveraging vanilla Bitween as a tool for inference and verification, moving beyond the fixed query functions ($x+r$, $x-r$, $x \cdot r$, $x$, $r$) previously used in the literature. On RSR-Bench, our benchmark suite of 80 scientific and machine learning functions, vanilla Bitween surpasses existing symbolic methods, while Agentic Bitween discovers new RSR properties using frontier models to uncover query functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18134v3</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ferhat Erata, Orr Paradise, Thanos Typaldos, Timos Antonopoulos, ThanhVu Nguyen, Shafi Goldwasser, Ruzica Piskac</dc:creator>
    </item>
    <item>
      <title>A Survey of Fuzzing Open-Source Operating Systems</title>
      <link>https://arxiv.org/abs/2502.13163</link>
      <description>arXiv:2502.13163v3 Announce Type: replace-cross 
Abstract: Vulnerabilities in open-source operating systems (OSs) pose substantial security risks to software systems, making their detection crucial. While fuzzing has been an effective vulnerability detection technique in various domains, OS fuzzing (OSF) faces unique challenges due to OS complexity and multi-layered interaction, and has not been comprehensively reviewed. Therefore, this work systematically surveys the state-of-the-art OSF techniques, categorizes them based on the general fuzzing process, and investigates challenges specific to kernel, file system, driver, and hypervisor fuzzing. Finally, future research directions for OSF are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13163v3</guid>
      <category>cs.OS</category>
      <category>cs.CR</category>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kun Hu, Qicai Chen, Wenzhuo Zhang, Zilong Lu, Bihuan Chen, You Lu, Haowen Jiang, Bingkun Sun, Xin Peng, Wenyun Zhao</dc:creator>
    </item>
    <item>
      <title>Combating Toxic Language: A Review of LLM-Based Strategies for Software Engineering</title>
      <link>https://arxiv.org/abs/2504.15439</link>
      <description>arXiv:2504.15439v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have become integral to Software Engineering (SE), increasingly used in development workflows. However, their widespread adoption raises concerns about the presence and propagation of toxic language - harmful or offensive content that can foster exclusionary environments. This paper provides a comprehensive review of recent research (2020-2024) on toxicity detection and mitigation, focusing on both SE-specific and general-purpose datasets. We examine annotation and pre-processing techniques, assess detection methodologies, and evaluate mitigation strategies, particularly those leveraging LLMs. Additionally, we conduct an ablation study demonstrating the effectiveness of LLM-based rewriting for reducing toxicity. This review is limited to studies published within the specified timeframe and within the domain of toxicity in LLMs and SE; therefore, certain emerging methods or datasets beyond this period may fall outside its purview. By synthesizing existing work and identifying open challenges, this review highlights key areas for future research to ensure the responsible deployment of LLMs in SE and beyond.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15439v2</guid>
      <category>cs.LG</category>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao Zhuo, Yicheng Yang, Kewen Peng</dc:creator>
    </item>
    <item>
      <title>Learner-Tailored Program Repair: A Solution Generator with Iterative Edit-Driven Retrieval Enhancement</title>
      <link>https://arxiv.org/abs/2601.08545</link>
      <description>arXiv:2601.08545v2 Announce Type: replace-cross 
Abstract: With the development of large language models (LLMs) in the field of programming, intelligent programming coaching systems have gained widespread attention. However, most research focuses on repairing the buggy code of programming learners without providing the underlying causes of the bugs. To address this gap, we introduce a novel task, namely LRP (Learner-Tailored Program Repair). We then propose a novel and effective framework, LSGEN (Learner-Tailored Solution Generator), to enhance program repair while offering the bug descriptions for the buggy code. In the first stage, we utilize a repair solution retrieval framework to construct a solution retrieval database and then employ an edit-driven code retrieval approach to retrieve valuable solutions, guiding LLMs in identifying and fixing the bugs in buggy code. In the second stage, we propose a solution-guided program repair method, which fixes the code and provides explanations under the guidance of retrieval solutions. Moreover, we propose an Iterative Retrieval Enhancement method that utilizes evaluation results of the generated code to iteratively optimize the retrieval direction and explore more suitable repair strategies, improving performance in practical programming coaching scenarios. The experimental results show that our approach outperforms a set of baselines by a large margin, validating the effectiveness of our framework for the newly proposed LPR task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.08545v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.SE</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhenlong Dai, Zhuoluo Zhao, Hengning Wang, Xiu Tang, Sai Wu, Chang Yao, Zhipeng Gao, Jingyuan Chen</dc:creator>
    </item>
  </channel>
</rss>
