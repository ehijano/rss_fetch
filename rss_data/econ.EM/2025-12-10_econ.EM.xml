<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 10 Dec 2025 05:03:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Branching Fixed Effects: A Proposal for Communicating Uncertainty</title>
      <link>https://arxiv.org/abs/2512.08101</link>
      <description>arXiv:2512.08101v1 Announce Type: new 
Abstract: Economists often rely on estimates of linear fixed effects models developed by other teams of researchers. Assessing the uncertainty in these estimates can be challenging. I propose a form of sample splitting for network data that breaks two-way fixed effects estimates into statistically independent branches, each of which provides an unbiased estimate of the parameters of interest. These branches facilitate uncertainty quantification, moment estimation, and shrinkage. Algorithms are developed for efficiently extracting branches from large datasets. I illustrate these techniques using a benchmark dataset from Veneto, Italy that has been widely used to study firm wage effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.08101v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patrick Kline</dc:creator>
    </item>
    <item>
      <title>Robust Counterfactuals in Centralized Schools Choice Systems: Addressing Gender Inequality in STEM Education</title>
      <link>https://arxiv.org/abs/2512.08115</link>
      <description>arXiv:2512.08115v1 Announce Type: new 
Abstract: Counterfactual analysis is central to education market design and provides a foundation for credible policy recommendations. We develop a novel methodology for counterfactual analysis in Gale-Shapley deferred-acceptance (DA) assignment mechanisms under a weaker set of assumptions than those typically imposed in existing empirical works. Instead of fully specifying utility functions or students' beliefs about admission probabilities, we rely on interpretable restrictions on behavior that yield an incomplete but flexible model of preferences. This framework addresses the challenge of partial identification by delivering sharp bounds on counterfactual stable matching outcomes, which we compute efficiently using a combination of algorithmic techniques and integer programming. We illustrate the methodology by evaluating policies aimed at increasing female enrollment in STEM fields in Chile.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.08115v1</guid>
      <category>econ.EM</category>
      <category>econ.TH</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lixiong Li, Isma\"el Mourifi\'e</dc:creator>
    </item>
    <item>
      <title>Automatic Debiased Machine Learning of Structural Parameters with General Conditional Moments</title>
      <link>https://arxiv.org/abs/2512.08423</link>
      <description>arXiv:2512.08423v1 Announce Type: new 
Abstract: This paper proposes a method to automatically construct or estimate Neyman-orthogonal moments in general models defined by a finite number of conditional moment restrictions (CMRs), with possibly different conditioning variables and endogenous regressors. CMRs are allowed to depend on non-parametric components, which might be flexibly modeled using Machine Learning tools, and non-linearly on finite-dimensional parameters. The key step in this construction is the estimation of Orthogonal Instrumental Variables (OR-IVs) -- "residualized" functions of the conditioning variables, which are then combined to obtain a debiased moment. We argue that computing OR-IVs necessarily requires solving potentially complicated functional equations, which depend on unknown terms. However, by imposing an approximate sparsity condition, our method finds the solutions to those equations using a Lasso-type program and can then be implemented straightforwardly. Based on this, we introduce a GMM estimator of finite-dimensional parameters (structural parameters) in a two-step framework. We derive theoretical guarantees for our construction of OR-IVs and show $\sqrt{n}$-consistency and asymptotic normality for the estimator of the structural parameters. Our Monte Carlo experiments and an empirical application on estimating firm-level production functions highlight the importance of relying on inference methods like the one proposed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.08423v1</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Facundo Arga\~naraz</dc:creator>
    </item>
    <item>
      <title>Minimax and Bayes Optimal Adaptive Experimental Design for Treatment Choice</title>
      <link>https://arxiv.org/abs/2512.08513</link>
      <description>arXiv:2512.08513v1 Announce Type: new 
Abstract: We consider an adaptive experiment for treatment choice and design a minimax and Bayes optimal adaptive experiment with respect to regret. Given binary treatments, the experimenter's goal is to choose the treatment with the highest expected outcome through an adaptive experiment, in order to maximize welfare. We consider adaptive experiments that consist of two phases, the treatment allocation phase and the treatment choice phase. The experiment starts with the treatment allocation phase, where the experimenter allocates treatments to experimental subjects to gather observations. During this phase, the experimenter can adaptively update the allocation probabilities using the observations obtained in the experiment. After the allocation phase, the experimenter proceeds to the treatment choice phase, where one of the treatments is selected as the best. For this adaptive experimental procedure, we propose an adaptive experiment that splits the treatment allocation phase into two stages, where we first estimate the standard deviations and then allocate each treatment proportionally to its standard deviation. We show that this experiment, often referred to as Neyman allocation, is minimax and Bayes optimal in the sense that its regret upper bounds exactly match the lower bounds that we derive. To show this optimality, we derive minimax and Bayes lower bounds for the regret using change-of-measure arguments. Then, we evaluate the corresponding upper bounds using the central limit theorem and large deviation bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.08513v1</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
    <item>
      <title>Difference-in-Differences with Interval Data</title>
      <link>https://arxiv.org/abs/2512.08759</link>
      <description>arXiv:2512.08759v1 Announce Type: new 
Abstract: Difference-in-differences (DID) is one of the most popular tools used to evaluate causal effects of policy interventions. This paper extends the DID methodology to accommodate interval outcomes, which are often encountered in empirical studies using survey or administrative data. We point out that a naive application or extension of the conventional parallel trends assumption may yield uninformative or counterintuitive results, and present a suitable identification strategy, called parallel shifts, which exhibits desirable properties. Practical attractiveness of the proposed method is illustrated by revisiting an influential minimum wage study by Card and Krueger (1994).</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.08759v1</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daisuke Kurisu, Yuta Okamoto, Taisuke Otsu</dc:creator>
    </item>
    <item>
      <title>Pattern Recognition of Ozone-Depleting Substance Exports in Global Trade Data</title>
      <link>https://arxiv.org/abs/2512.07864</link>
      <description>arXiv:2512.07864v1 Announce Type: cross 
Abstract: New methods are needed to monitor environmental treaties, like the Montreal Protocol, by reviewing large, complex customs datasets. This paper introduces a framework using unsupervised machine learning to systematically detect suspicious trade patterns and highlight activities for review. Our methodology, applied to 100,000 trade records, combines several ML techniques. Unsupervised Clustering (K-Means) discovers natural trade archetypes based on shipment value and weight. Anomaly Detection (Isolation Forest and IQR) identifies rare "mega-trades" and shipments with commercially unusual price-per-kilogram values. This is supplemented by Heuristic Flagging to find tactics like vague shipment descriptions. These layers are combined into a priority score, which successfully identified 1,351 price outliers and 1,288 high-priority shipments for customs review. A key finding is that high-priority commodities show a different and more valuable value-to-weight ratio than general goods. This was validated using Explainable AI (SHAP), which confirmed vague descriptions and high value as the most significant risk predictors. The model's sensitivity was validated by its detection of a massive spike in "mega-trades" in early 2021, correlating directly with the real-world regulatory impact of the US AIM Act. This work presents a repeatable unsupervised learning pipeline to turn raw trade data into prioritized, usable intelligence for regulatory groups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07864v1</guid>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammad Sukri Bin Ramli</dc:creator>
    </item>
    <item>
      <title>LLM-Generated Counterfactual Stress Scenarios for Portfolio Risk Simulation via Hybrid Prompt-RAG Pipeline</title>
      <link>https://arxiv.org/abs/2512.07867</link>
      <description>arXiv:2512.07867v1 Announce Type: cross 
Abstract: We develop a transparent and fully auditable LLM-based pipeline for macro-financial stress testing, combining structured prompting with optional retrieval of country fundamentals and news. The system generates machine-readable macroeconomic scenarios for the G7, which cover GDP growth, inflation, and policy rates, and are translated into portfolio losses through a factor-based mapping that enables Value-at-Risk and Expected Shortfall assessment relative to classical econometric baselines. Across models, countries, and retrieval settings, the LLMs produce coherent and country-specific stress narratives, yielding stable tail-risk amplification with limited sensitivity to retrieval choices. Comprehensive plausibility checks, scenario diagnostics, and ANOVA-based variance decomposition show that risk variation is driven primarily by portfolio composition and prompt design rather than by the retrieval mechanism. The pipeline incorporates snapshotting, deterministic modes, and hash-verified artifacts to ensure reproducibility and auditability. Overall, the results demonstrate that LLM-generated macro scenarios, when paired with transparent structure and rigorous validation, can provide a scalable and interpretable complement to traditional stress-testing frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07867v1</guid>
      <category>q-fin.RM</category>
      <category>cs.AI</category>
      <category>econ.EM</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masoud Soleimani</dc:creator>
    </item>
    <item>
      <title>Identifying Treatment and Spillover Effects Using Exposure Contrasts</title>
      <link>https://arxiv.org/abs/2403.08183</link>
      <description>arXiv:2403.08183v4 Announce Type: replace 
Abstract: To report spillover effects, a common practice is to regress outcomes on statistics summarizing neighbors' treatments. This paper studies nonparametric analogs of these estimands, which we refer to as exposure contrasts. We demonstrate that a contrast may have the opposite sign of the unit-level effects of interest even under unconfoundedness. We then provide interpretable conditions on interference and the assignment mechanism under which exposure contrasts can be represented as convex averages of the unit-level effects and therefore avoid sign reversals. These conditions encompass cluster-randomized trials, network experiments, and observational settings with peer effects in selection into treatment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08183v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael P. Leung</dc:creator>
    </item>
    <item>
      <title>A quantile-based nonadditive fixed effects model</title>
      <link>https://arxiv.org/abs/2405.03826</link>
      <description>arXiv:2405.03826v2 Announce Type: replace 
Abstract: I propose a quantile-based nonadditive fixed effects panel model to study heterogeneous causal effects. Similar to standard fixed effects (FE) model, my model allows arbitrary dependence between regressors and unobserved heterogeneity, but it generalizes the additive separability of standard FE to allow the unobserved heterogeneity to enter nonseparably. Similar to structural quantile models, my model's random coefficient vector depends on an unobserved, scalar ''rank'' variable, in which outcomes (excluding an additive noise term) are monotonic at a particular value of the regressor vector, which is much weaker than the conventional monotonicity assumption that must hold at all possible values. This rank is assumed to be stable over time, which is often more economically plausible than the panel quantile studies that assume individual rank is iid over time. It uncovers the heterogeneous causal effects as functions of the rank variable. I provide identification and estimation results, establishing uniform consistency and uniform asymptotic normality of the heterogeneous causal effect function estimator. Simulations show reasonable finite-sample performance and show my model complements fixed effects quantile regression. Finally, I illustrate the proposed methods by examining the causal effect of a country's oil wealth on its military defense spending.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03826v2</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xin Liu</dc:creator>
    </item>
    <item>
      <title>Endogenous Heteroskedasticity in Linear Models</title>
      <link>https://arxiv.org/abs/2412.02767</link>
      <description>arXiv:2412.02767v4 Announce Type: replace 
Abstract: Linear regressions with endogeneity are widely used to estimate causal effects. This paper studies a framework that involves two common practical issues: endogeneity of the regressors and heteroskedasticity that depends on endogenous regressors, i.e., endogenous heteroskedasticity. To address the inconsistency of the two-stage least squares estimator in this scenario, and recover the causal parameters of interest, we develop a framework for practical estimation and inference based on the control function approach allowing for discrete and continuous regressors. In particular, we suggest a simple two-step estimation procedure. We establish the limiting properties of the estimator, namely, consistency and asymptotic normality. In addition, we develop practical valid inference methods by proposing an estimator for the asymptotic variance-covariance matrix, and formally establishing its consistency. Monte Carlo simulations provide evidence on the finite-sample performance of the proposed methods and evaluate different implementation strategies. We revisit an empirical application on job training to illustrate the methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02767v4</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Javier Alejo, Antonio F. Galvao, Julian Martinez-Iriarte, Gabriel Montes-Rojas</dc:creator>
    </item>
    <item>
      <title>A Generalized Control Function Approach to Production Function Estimation</title>
      <link>https://arxiv.org/abs/2511.21578</link>
      <description>arXiv:2511.21578v3 Announce Type: replace 
Abstract: We develop a generalized control function approach to production function estimation. Our approach accommodates settings in which productivity evolves jointly with other unobservable factors such as latent demand shocks and the invertibility assumption underpinning the traditional proxy variable approach fails. We provide conditions under which the output elasticity of the variable input -- and hence the markup -- is nonparametrically point-identified. A Neyman orthogonal moment condition ensures oracle efficiency of our GMM estimator. A Monte Carlo exercise shows a large bias for the traditional approach that decreases rapidly and nearly vanishes for our generalized control function approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21578v3</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ulrich Doraszelski, Lixiong Li</dc:creator>
    </item>
  </channel>
</rss>
