<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.ET updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.ET</link>
    <description>cs.ET updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.ET" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 20 Mar 2025 01:52:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The Immersive Archive: Archival Strategies for the Sensorama &amp; Sutherland HMD</title>
      <link>https://arxiv.org/abs/2503.13809</link>
      <description>arXiv:2503.13809v1 Announce Type: new 
Abstract: The Immersive Archive is an initiative dedicated to preserve and restore the groundbreaking works from across Extended Reality (XR) history. Originating at the University of Southern California's Mobile and Environmental Media Lab, this archive is committed to developing and exhibiting simulations of influential XR devices that have shaped immersive media over time. This paper examines the challenges and strategies involved in archiving seminal XR technologies, with a focus on Morton Heilig's Sensorama and Ivan Sutherland's HeadMounted Display. As pioneering prototypes in virtual and augmented reality, these devices provide valuable insights into the evolution of immersive media, highlighting both technological innovation and sensory experimentation. Through collaborative archival efforts with institutions such as the HMH Moving Image Archive at University of Southern California and the Computer History Museum, this research integrates media archaeology with digital preservation techniques. Emphasis is placed on documentation practices, restoration of physical artifacts and developing simulations of these historic experiences for contemporary virtual reality platforms. Our interdisciplinary approach to archival methodologies, which captures the multisensory and interactive qualities of these pioneering devices, has been instrumental in developing a framework for future immersive media preservation initiatives. By preserving the immersive essence of these early experiences, we lay the groundwork for future generations to explore and learn from the origins of immersive media. Safeguarding this rich legacy is essential to ensure these visionary works continue to inspire and shape the future of media landscapes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13809v1</guid>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <category>cs.MM</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1109/AIxVR63409.2025.00059</arxiv:DOI>
      <arxiv:journal_reference>Proc. IEEE Conf. AI &amp; XR, 2025, pp. 307-312</arxiv:journal_reference>
      <dc:creator>Zeynep Abes, Nathan Fairchild, Spencer Lin, Michael Wahba, Katrina Xiao, Scott S. Fisher</dc:creator>
    </item>
    <item>
      <title>LLM-Empowered IoT for 6G Networks: Architecture, Challenges, and Solutions</title>
      <link>https://arxiv.org/abs/2503.13819</link>
      <description>arXiv:2503.13819v1 Announce Type: new 
Abstract: The Internet of Things (IoT) in the sixth generation (6G) era is envisioned to evolve towards intelligence, ubiquity, and self-optimization. Large language models (LLMs) have demonstrated remarkable generalization capabilities across diverse domains, including natural language processing (NLP), computer vision (CV), and beyond. In this article, we propose an LLM-empowered IoT architecture for 6G networks to achieve intelligent autonomy while supporting advanced IoT applications. LLMs are pushed to the edge of the 6G network to support the synergy of LLMs and IoT. LLM solutions are tailored to both IoT application requirements and IoT management needs, i.e., LLM for IoT. On the other hand, edge inference and edge fine-tuning are discussed to support the deployment of LLMs, i.e., LLM on IoT. Furthermore, we propose a memory-efficient split federated learning (SFL) framework for LLM fine-tuning on heterogeneous IoT devices that alleviates memory pressures on both IoT devices and the edge server while achieving comparable performance and convergence time. Finally, a case study is presented, followed by a discussion about open issues of LLM-empowered IoT for 6G networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13819v1</guid>
      <category>cs.ET</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaopei Chen, Wen Wu, Zuguang Li, Liang Li, Fei Ji</dc:creator>
    </item>
    <item>
      <title>Hardware Implementation of Ring Oscillator Networks Coupled by BEOL Integrated ReRAM for Associative Memory Tasks</title>
      <link>https://arxiv.org/abs/2503.14126</link>
      <description>arXiv:2503.14126v1 Announce Type: new 
Abstract: We demonstrate the first hardware implementation of an oscillatory neural network (ONN) utilizing resistive memory (ReRAM) for coupling elements. A ReRAM crossbar array chip, integrated into the Back End of Line (BEOL) of CMOS technology, is leveraged to establish dense coupling elements between oscillator neurons, allowing phase-encoded analog information to be processed in-memory. We also realize an ONN architecture design with the coupling ReRAM array. To validate the architecture experimentally, we present a conductive metal oxide (CMO)/HfOx ReRAM array chip integrated with a 2-by-2 ring oscillator-based network. The system successfully retrieves patterns through correct binary phase locking. This proof of concept underscores the potential of ReRAM technology for large-scale, integrated ONNs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14126v1</guid>
      <category>cs.ET</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wooseok Choi, Thomas van Bodegraven, Jelle Verest, Olivier Maher, Donato Francesco Falcone, Antonio La Porta, Daniel Jubin, Bert Jan Offrein, Siegfried Karg, Valeria Bragaglia, Aida Todri-Sanial</dc:creator>
    </item>
    <item>
      <title>5G-Enabled Teleoperated Driving: An Experimental Evaluation</title>
      <link>https://arxiv.org/abs/2503.14186</link>
      <description>arXiv:2503.14186v1 Announce Type: new 
Abstract: Teleoperated driving enables remote human intervention in autonomous vehicles, addressing challenges in complex driving environments. However, its effectiveness depends on ultra-low latency, high-reliability communication. This paper evaluates teleoperated driving over 5G networks, analyzing key performance metrics such as glass-to-glass (G2G) latency, RTT and steering command delay. Using a real-world testbed with a Kia Soul EV and a remote teleoperation platform, we assess the feasibility and limitations of 5G-enabled teleoperated driving. Our system achieved an average G2G latency of 202ms and an RTT of 47ms highlighting the G2G latency as the critical bottleneck. The steering control proved to be mostly accurate and responsive. Finally, this paper provides recommendations and outlines future work to improve future teleoperated driving deployments for safer and more reliable autonomous mobility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14186v1</guid>
      <category>cs.ET</category>
      <category>cs.NI</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mehdi Testouri, Gamal Elghazaly, Faisal Hawlader, Raphael Frank</dc:creator>
    </item>
    <item>
      <title>Data Encoding for VQC in Qiskit, A Comparison With Novel Hybrid Encoding</title>
      <link>https://arxiv.org/abs/2503.14062</link>
      <description>arXiv:2503.14062v1 Announce Type: cross 
Abstract: If quantum machine learning emulates the ways of classical machine learning, data encoding in a quantum neural network is imperative for many reasons. One of the key ones is the complexity attributed to the data size depending upon the features and types, which is the essence of machine learning. While the standard various encoding techniques exist for quantum computing, hybrid one is not among many, though it tends to offer some distinct advantages, viz. efficient qubits utilization and increased entanglement, which fits well for variation quantum classifier algorithm by manipulating the essential criteria of ZZFeatureMaps and RealAmplitudes. While Amplitude encoding can turn traits normalized into quantum amplitudes, encoding an angle by using Ry gates to encode feature values into rotation angles, and phase encoding by using Rz gates to encode extra feature information as phase is plausible to combine all together. By combining these three methods, this paper demonstrates that efficient qubit usage is ensured as Amplitude encoding reduces the required qubits, Angle encoding makes state freedom better and is used for expressive encoding, and Phase-based distinction. Finally, using classical optimizers, the hybrid encoding technique through VQC is fit in training and testing using a synthetic dataset, and results have been compared to the standard VQC encoding in qiskit machine learning ecosystems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14062v1</guid>
      <category>quant-ph</category>
      <category>cs.ET</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hillol Biswas</dc:creator>
    </item>
    <item>
      <title>Sensory-driven microinterventions for improved health and wellbeing</title>
      <link>https://arxiv.org/abs/2503.14102</link>
      <description>arXiv:2503.14102v1 Announce Type: cross 
Abstract: The five senses are gateways to our wellbeing and their decline is considered a significant public health challenge which is linked to multiple conditions that contribute significantly to morbidity and mortality. Modern technology, with its ubiquitous nature and fast data processing has the ability to leverage the power of the senses to transform our approach to day to day healthcare, with positive effects on our quality of life. Here, we introduce the idea of sensory-driven microinterventions for preventative, personalised healthcare. Microinterventions are targeted, timely, minimally invasive strategies that seamlessly integrate into our daily life. This idea harnesses human's sensory capabilities, leverages technological advances in sensory stimulation and real-time processing ability for sensing the senses. The collection of sensory data from our continuous interaction with technology - for example the tone of voice, gait movement, smart home behaviour - opens up a shift towards personalised technology-enabled, sensory-focused healthcare interventions, coupled with the potential of early detection and timely treatment of sensory deficits that can signal critical health insights, especially for neurodegenerative diseases such as Parkinson's disease.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14102v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Youssef Abdalla, Elia Gatti, Mine Orlu, Marianna Obrist</dc:creator>
    </item>
    <item>
      <title>Retrospective: A CORDIC Based Configurable Activation Function for NN Applications</title>
      <link>https://arxiv.org/abs/2503.14354</link>
      <description>arXiv:2503.14354v1 Announce Type: cross 
Abstract: A CORDIC-based configuration for the design of Activation Functions (AF) was previously suggested to accelerate ASIC hardware design for resource-constrained systems by providing functional reconfigurability. Since its introduction, this new approach for neural network acceleration has gained widespread popularity, influencing numerous designs for activation functions in both academic and commercial AI processors. In this retrospective analysis, we explore the foundational aspects of this initiative, summarize key developments over recent years, and introduce the DA-VINCI AF tailored for the evolving needs of AI applications. This new generation of dynamically configurable and precision-adjustable activation function cores promise greater adaptability for a range of activation functions in AI workloads, including Swish, SoftMax, SeLU, and GeLU, utilizing the Shift-and-Add CORDIC technique. The previously presented design has been optimized for MAC, Sigmoid, and Tanh functionalities and incorporated into ReLU AFs, culminating in an accumulative NEURIC compute unit. These enhancements position NEURIC as a fundamental component in the resource-efficient vector engine for the realization of AI accelerators that focus on DNNs, RNNs/LSTMs, and Transformers, achieving a quality of results (QoR) of 98.5%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14354v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.ET</category>
      <category>eess.IV</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Omkar Kokane, Gopal Raut, Salim Ullah, Mukul Lokhande, Adam Teman, Akash Kumar, Santosh Kumar Vishvakarma</dc:creator>
    </item>
    <item>
      <title>EnQode: Fast Amplitude Embedding for Quantum Machine Learning Using Classical Data</title>
      <link>https://arxiv.org/abs/2503.14473</link>
      <description>arXiv:2503.14473v1 Announce Type: cross 
Abstract: Amplitude embedding (AE) is essential in quantum machine learning (QML) for encoding classical data onto quantum circuits. However, conventional AE methods suffer from deep, variable-length circuits that introduce high output error due to extensive gate usage and variable error rates across samples, resulting in noise-driven inconsistencies that degrade model accuracy. We introduce EnQode, a fast AE technique based on symbolic representation that addresses these limitations by clustering dataset samples and solving for cluster mean states through a low-depth, machine-specific ansatz. Optimized to reduce physical gates and SWAP operations, EnQode ensures all samples face consistent, low noise levels by standardizing circuit depth and composition. With over 90% fidelity in data mapping, EnQode enables robust, high-performance QML on noisy intermediate-scale quantum (NISQ) devices. Our open-source solution provides a scalable and efficient alternative for integrating classical data with quantum models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14473v1</guid>
      <category>quant-ph</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jason Han, Nicholas S. DiBrita, Younghyun Cho, Hengrui Luo, Tirthak Patel</dc:creator>
    </item>
    <item>
      <title>MERCI: Multimodal Emotional and peRsonal Conversational Interactions Dataset</title>
      <link>https://arxiv.org/abs/2412.04908</link>
      <description>arXiv:2412.04908v2 Announce Type: replace-cross 
Abstract: The integration of conversational agents into our daily lives has become increasingly common, yet many of these agents cannot engage in deep interactions with humans. Despite this, there is a noticeable shortage of datasets that capture multimodal information from human-robot interaction dialogues. To address this gap, we have recorded a novel multimodal dataset (MERCI) that encompasses rich embodied interaction data. The process involved asking participants to complete a questionnaire and gathering their profiles on ten topics, such as hobbies and favorite music. Subsequently, we initiated conversations between the robot and the participants, leveraging GPT-4 to generate contextually appropriate responses based on the participant's profile and emotional state, as determined by facial expression recognition and sentiment analysis. Automatic and user evaluations were conducted to assess the overall quality of the collected data. The results of both evaluations indicated a high level of naturalness, engagement, fluency, consistency, and relevance in the conversation, as well as the robot's ability to provide empathetic responses. It is worth noting that the dataset is derived from genuine interactions with the robot, involving participants who provided personal information and conveyed actual emotions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04908v2</guid>
      <category>cs.HC</category>
      <category>cs.ET</category>
      <category>cs.RO</category>
      <pubDate>Wed, 19 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mohammed Althubyani, Zhijin Meng, Shengyuan Xie, Cha Seung, Imran Razzak, Eduardo B. Sandoval, Baki Kocaballi, Francisco Cruz</dc:creator>
    </item>
  </channel>
</rss>
