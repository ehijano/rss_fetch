<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 Oct 2025 04:00:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A hybrid combinatorial-continuous strategy for solving molecular distance geometry problems</title>
      <link>https://arxiv.org/abs/2510.19970</link>
      <description>arXiv:2510.19970v1 Announce Type: new 
Abstract: The Molecular Distance Geometry Problem (MDGP) is essential in structural biology, as it seeks to determine three-dimensional protein structures from partial interatomic distances. Its discretizable subclass (DMDGP) admits an exact combinatorial formulation that enables efficient exploration of the search space. However, in practical settings such as Nuclear Magnetic Resonance (NMR) spectroscopy, distances are available only within uncertainty bounds, leading to the interval variant (\emph{i}DMDGP). We propose a hybrid combinatorial--continuous framework for solving the \emph{i}DMDGP. The method combines an enumeration process derived from the DMDGP with a continuous refinement stage that minimizes a nonconvex stress function that penalizes deviations from admissible distance intervals. This integration supports a systematic exploration guided by discrete structure and local optimization. The formulation incorporates torsion-angle intervals and chirality constraints through a refined atom ordering that preserves protein-backbone geometry. Numerical experiments show that the approach efficiently reconstructs geometrically valid conformations even under wide distance bounds, whereas most existing studies assume narrow ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19970v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leonardo D. Secchin, Wagner da Rocha, Mariana da Rosa, Leo Liberti, Carlile Lavor</dc:creator>
    </item>
    <item>
      <title>Simultaneously Solving Infinitely Many LQ Mean Field Games In Hilbert Spaces: The Power of Neural Operators</title>
      <link>https://arxiv.org/abs/2510.20017</link>
      <description>arXiv:2510.20017v1 Announce Type: new 
Abstract: Traditional mean-field game (MFG) solvers operate on an instance-by-instance basis, which becomes infeasible when many related problems must be solved (e.g., for seeking a robust description of the solution under perturbations of the dynamics or utilities, or in settings involving continuum-parameterized agents.). We overcome this by training neural operators (NOs) to learn the rules-to-equilibrium map from the problem data (``rules'': dynamics and cost functionals) of LQ MFGs defined on separable Hilbert spaces to the corresponding equilibrium strategy. Our main result is a statistical guarantee: an NO trained on a small number of randomly sampled rules reliably solves unseen LQ MFG variants, even in infinite-dimensional settings. The number of NO parameters needed remains controlled under appropriate rule sampling during training.
  Our guarantee follows from three results: (i) local-Lipschitz estimates for the highly nonlinear rules-to-equilibrium map; (ii) a universal approximation theorem using NOs with a prespecified Lipschitz regularity (unlike traditional NO results where the NO's Lipschitz constant can diverge as the approximation error vanishes); and (iii) new sample-complexity bounds for $L$-Lipschitz learners in infinite dimensions, directly applicable as the Lipschitz constants of our approximating NOs are controlled in (ii).</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20017v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>q-fin.MF</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dena Firoozi, Anastasis Kratsios, Xuwei Yang</dc:creator>
    </item>
    <item>
      <title>Endogenous Aggregation of Multiple Data Envelopment Analysis Scores for Large Data Sets</title>
      <link>https://arxiv.org/abs/2510.20052</link>
      <description>arXiv:2510.20052v1 Announce Type: new 
Abstract: We propose an approach for dynamic efficiency evaluation across multiple organizational dimensions using data envelopment analysis (DEA). The method generates both dimension-specific and aggregate efficiency scores, incorporates desirable and undesirable outputs, and is suitable for large-scale problem settings. Two regularized DEA models are introduced: a slack-based measure (SBM) and a linearized version of a nonlinear goal programming model (GP-SBM). While SBM estimates an aggregate efficiency score and then distributes it across dimensions, GP-SBM first estimates dimension-level efficiencies and then derives an aggregate score. Both models utilize a regularization parameter to enhance discriminatory power while also directly integrating both desirable and undesirable outputs. We demonstrate the computational efficiency and validity of our approach on multiple datasets and apply it to a case study of twelve hospitals in Ontario, Canada, evaluating three theoretically grounded dimensions of organizational effectiveness over a 24-month period from January 2018 to December 2019: technical efficiency, clinical efficiency, and patient experience. Our numerical results show that SBM and GP-SBM better capture correlations among input/output variables and outperform conventional benchmarking methods that separately evaluate dimensions before aggregation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20052v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Hashem Omrani, Raha Imanirad, Adam Diamant, Utkarsh Verma, Amol Verma, Fahad Razak</dc:creator>
    </item>
    <item>
      <title>Maximum principle for optimal control of infinite horizon stochastic difference equations driven by fractional noises</title>
      <link>https://arxiv.org/abs/2510.20058</link>
      <description>arXiv:2510.20058v1 Announce Type: new 
Abstract: In this paper, infinite horizon stochastic difference equations and backward stochastic difference equations with fractional noises are studied. The main difficulty comes from fractional noises on infinite horizon. Motivated by discrete-time optimal control problem driven by fractional noises and on infinite horizon, the stochastic maximum principle for discrete-time control problem driven by fractional noises in infinite horizon is proved. As an application, an optimal investment problem is solved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20058v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuecai Han, Yuhang Li</dc:creator>
    </item>
    <item>
      <title>Modeling to Generate Alternatives for Robustness of Mixed Integer DC Optimal Power Flow</title>
      <link>https://arxiv.org/abs/2510.20089</link>
      <description>arXiv:2510.20089v1 Announce Type: new 
Abstract: Transmission system operators face a variety of discrete operational decisions, such as switching of branches and/or devices. Incorporating these decisions into optimal power flow (OPF) results in mixed-integer non-linear programming problems (MINLPs), which can't presently be solved at scale in the required time. Various linearizations of the OPF exist, most famously the DC-OPF, which can be leveraged to find integer decisions. However, these linearizations can yield very poor integer solutions in some edge cases, making them challenging to incorporate into control rooms. This paper introduces the use of modeling to generate alternatives (MGA) to find alternative solutions to the linearized problems, reducing the chance of finding no AC feasible solutions. We test this approach using 13 networks where the DC linearization results in infeasible integer decisions, and MGA finds a solution in all cases. The MGA search criteria selected drastically affects the number and quality of solutions found, so network specific search functions may be necessary.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20089v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Constance Crozier</dc:creator>
    </item>
    <item>
      <title>A Unified and Scalable Method for Optimization over Graphs of Convex Sets</title>
      <link>https://arxiv.org/abs/2510.20184</link>
      <description>arXiv:2510.20184v1 Announce Type: new 
Abstract: A Graph of Convex Sets (GCS) is a graph in which vertices are associated with convex programs and edges couple pairs of programs through additional convex costs and constraints. Any optimization problem over an ordinary weighted graph (e.g., the shortest-path, the traveling-salesman, and the minimum-spanning-tree problems) can be naturally generalized to a GCS, yielding a new class of problems at the interface of combinatorial and convex optimization with numerous applications. In this paper, we introduce a unified method for solving any such problem. Starting from an integer linear program that models an optimization problem over a weighted graph, our method automatically produces an efficient mixed-integer convex formulation of the corresponding GCS problem. This formulation is based on homogenization (perspective) transformations, and the resulting program is solved to global optimality using off-the-shelf branch-and-bound solvers. We implement this framework in GCSOPT, an open-source and easy-to-use Python library designed for fast prototyping. We illustrate the versatility and scalability of our approach through multiple numerical examples and comparisons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20184v1</guid>
      <category>math.OC</category>
      <category>cs.MS</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tobia Marcucci</dc:creator>
    </item>
    <item>
      <title>Radius of Robust Feasibility for Ground Coverage in Aerial Sensor Networks</title>
      <link>https://arxiv.org/abs/2510.20213</link>
      <description>arXiv:2510.20213v1 Announce Type: new 
Abstract: Sensors are vital for environmental monitoring, yet their effectiveness diminishes under spatial uncertainty. We propose a robust optimization framework for maximizing the coverage of aerial directional sensors under spatial uncertainty. Each sensor projects a truncated sector on the ground, parameterized by its altitude, field of view, and orientation. To address sensor displacement uncertainty, we introduce the radius of robust feasibility (RRF) as a measure of tolerance against positional perturbations. We formulate an exact expression for the RRF of aerial sensor networks and embed it into the coverage maximization model as a robustness constraint. Our approach guarantees that the optimized configuration remains feasible under bounded uncertainty. A distributed greedy algorithm based on Voronoi partitioning is used for orientation adjustment, ensuring scalable and adaptive deployment toward high-impact regions. Experimental results validate the effectiveness of our model in preserving robust coverage across complex terrain and varying uncertainty conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20213v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vanshika Datta, C. Nahak</dc:creator>
    </item>
    <item>
      <title>Optimization of Bregman Variational Learning Dynamics</title>
      <link>https://arxiv.org/abs/2510.20227</link>
      <description>arXiv:2510.20227v1 Announce Type: new 
Abstract: We develop a general optimization-theoretic framework for Bregman-Variational Learning Dynamics (BVLD), a new class of operator-based updates that unify Bayesian inference, mirror descent, and proximal learning under time-varying environments. Each update is formulated as a variational optimization problem combining a smooth convex loss f_t with a Bregman divergence D_psi. We prove that the induced operator is averaged, contractive, and exponentially stable in the Bregman geometry. Further, we establish Fejer monotonicity, drift-aware convergence, and continuous-time equivalence via an evolution variational inequality (EVI). Together, these results provide a rigorous analytical foundation for well-posed and stability-guaranteed operator dynamics in nonstationary optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20227v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinho Cha, Youngchul Kim, Jungmin Shin, Jaeyoung Cho, Seon Jin Kim, Junyeol Ryu</dc:creator>
    </item>
    <item>
      <title>Convergence Analysis of Noisy Distributed Gradient Descent for Non-convex Optimization -- Saddle Point Escape</title>
      <link>https://arxiv.org/abs/2510.20246</link>
      <description>arXiv:2510.20246v1 Announce Type: new 
Abstract: A variant of consensus based distributed gradient descent (\textbf{DGD}) is studied for finite sums of smooth but possibly non-convex functions. In particular, the local gradient term in the fixed step-size iteration of each agent is randomly perturbed to evade saddle points. Under regularity conditions, it is established that for sufficiently small step size and noise variance, each agent converges with high probability to a specified radius neighborhood of a common second-order stationary point, i.e., local minimizer. The rate of convergence is shown to be comparable to centralized first-order algorithms. Numerical experiments are presented to validate the efficacy of the proposed approach over standard \textbf{DGD} in a non-convex setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20246v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lei Qin, Michael Cantoni, Ye Pu</dc:creator>
    </item>
    <item>
      <title>GPU-Accelerated Primal Heuristics for Mixed Integer Programming</title>
      <link>https://arxiv.org/abs/2510.20499</link>
      <description>arXiv:2510.20499v1 Announce Type: new 
Abstract: We introduce a fusion of GPU accelerated primal heuristics for Mixed Integer Programming. Leveraging GPU acceleration enables exploration of larger search regions and faster iterations. A GPU-accelerated PDLP serves as an approximate LP solver, while a new probing cache facilitates rapid roundings and early infeasibility detection. Several state-of-the-art heuristics, including Feasibility Pump, Feasibility Jump, and Fix-and-Propagate, are further accelerated and enhanced. The combined approach of these GPU-driven algorithms yields significant improvements over existing methods, both in the number of feasible solutions and the quality of objectives by achieving 221 feasible solutions and 22% objective gap in the MIPLIB2017 benchmark on a presolved dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20499v1</guid>
      <category>math.OC</category>
      <category>cs.DC</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akif \c{C}\"ord\"uk, Piotr Sielski, Alice Boucher, Kumar Aatish</dc:creator>
    </item>
    <item>
      <title>Path-Based Conditions for the Identifiability of Non-additive Nonlinear Networks with Full Measurements</title>
      <link>https://arxiv.org/abs/2510.20537</link>
      <description>arXiv:2510.20537v1 Announce Type: new 
Abstract: We analyze the identifiability of nonlinear networks with node dynamics characterized by functions that are non-additive. We consider the full measurement case (all the nodes are measured) in the path-independent delay scenario where all the excitation signals of a specific node have the same delay in the output of a measured node. Based on the notion of a generic nonlinear matrix associated with the network, we introduce the concept of generic identifiability and characterize the space of functions that satisfies this property. For directed acyclic graphs (DAGs) characterized by analytic functions, we derive a sufficient condition for identifiability based on vertex-disjoint paths from excited nodes to the in-neighbors of each node in the network. Furthermore, when we consider the class of polynomial functions, by using well-known results on algebraic varieties, we prove that the vertex-disjoint path condition is also necessary. Finally, we show that this identifiability condition is not necessary for the additive nonlinear model. Some examples are added to illustrate the results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20537v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Renato Vizuete, Julien M. Hendrickx</dc:creator>
    </item>
    <item>
      <title>The Intermodal Railroad Blocking and Railcar Fleet-Management Planning Problem</title>
      <link>https://arxiv.org/abs/2510.20597</link>
      <description>arXiv:2510.20597v1 Announce Type: new 
Abstract: Rail is a cost-effective and relatively low-emission mode for transporting intermodal containers over long distances. This paper addresses tactical planning of intermodal railroad operations by introducing a new problem that simultaneously considers three consolidation processes and the management of a heterogeneous railcar fleet. We model the problem with a scheduled service network design with resource management (SSND-RM) formulation, expressed as an integer linear program. While such formulations are challenging to solve at scale, we demonstrate that our problem can be tackled with a general-purpose solver when provided with high-quality warm-start solutions. To this end, we design a construction heuristic inspired by a relax-and-fix procedure. We evaluate the methodology on realistic, large-scale instances from our industrial partner, the Canadian National Railway Company: a North American Class I railroad. The computational experiments show that the proposed approach efficiently solves practically relevant instances, and that solutions to the SSND-RM formulation yield substantially more accurate capacity estimations compared to those obtained from simpler baseline models. Managerial insights from our study highlight that ignoring railcar fleet management or container loading constraints can lead to a severe underestimation of required capacity, which may result in costly operational inefficiencies. Furthermore, our results show that the use of multi-platform railcars improves overall capacity utilization and benefits the network, even if they can locally lead to less efficient loading as measured by terminal-level slot utilization performance indicators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20597v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Julie Kienzle, Serge Bisaillon, Teodor Gabriel Crainic, Emma Frejinger</dc:creator>
    </item>
    <item>
      <title>Bilevel Programming Problems: A view through Set-valued Optimization</title>
      <link>https://arxiv.org/abs/2510.20631</link>
      <description>arXiv:2510.20631v1 Announce Type: new 
Abstract: Bilevel programming is one of the very active areas of research with many real-life applications in economics and engineering. Bilevel problems are hierarchical problems consisting of lower-level and upper-level problems, respectively. The leader or the decision-maker for the upper-level problem decides first, and then the follower or the lower-level decision-maker chooses his/her strategy. In the case of multiple lower-level solutions, the bilevel problems are not well defined, and there are many ways to handle such a situation. One standard way is to put restrictions on the lower level problems (like strict convexity) so that nonuniqueness does not arise. However, those restrictions are not viable in many situations. Therefore, there are two standard formulations, called pessimistic formulations and optimistic formulations of the upper-level problem. A set-valued formulation has been proposed and has been studied in the literature. However, the study is limited to the continuous set-up with the assumption of value attainment, and the general case has not been considered. In this paper, we focus on the general case and study the connection among various notions of solution. Our main findings suggest that the set-valued formulation may not hold any bigger advantage than the existing optimistic and pessimistic formulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20631v1</guid>
      <category>math.OC</category>
      <category>econ.TH</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s10479-025-06589-6</arxiv:DOI>
      <arxiv:journal_reference>Ann Oper Res (2025)</arxiv:journal_reference>
      <dc:creator>Kuntal Som, Thirumulanathan D, Joydeep Dutta</dc:creator>
    </item>
    <item>
      <title>Extreme Strong Branching for QCQPs</title>
      <link>https://arxiv.org/abs/2510.20650</link>
      <description>arXiv:2510.20650v1 Announce Type: new 
Abstract: For mixed-integer programs (MIPs), strong branching is a highly effective variable selection method to reduce the number of nodes in the branch-and-bound algorithm. Extending it to nonlinear problems is conceptually simple but practically limited. Branching on a binary variable fixes the variable to 0 or 1, whereas branching on a continuous variable requires an additional decision to choose a branching point. Previous extensions of strong branching predefine this point and then solve $2n$ relaxations where $n$ is the number of candidate variables to branch. We propose extreme strong branching, which evaluates multiple branching points per variable and jointly selects both the branching variable and point based on the objective value improvement. This approach resembles the success of strong branching for MIPs while additionally exploiting bound tightening as a byproduct. For certain types of quadratically constrained quadratic programs (QCQPs), computational experiments show that the extreme strong branching rule outperforms existing commercial solvers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20650v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Santanu S. Dey, Dahye Han, Yang Wang</dc:creator>
    </item>
    <item>
      <title>Isotropic Noise in Stochastic and Quantum Convex Optimization</title>
      <link>https://arxiv.org/abs/2510.20745</link>
      <description>arXiv:2510.20745v1 Announce Type: new 
Abstract: We consider the problem of minimizing a $d$-dimensional Lipschitz convex function using a stochastic gradient oracle. We introduce and motivate a setting where the noise of the stochastic gradient is isotropic in that it is bounded in every direction with high probability. We then develop an algorithm for this setting which improves upon prior results by a factor of $d$ in certain regimes, and as a corollary, achieves a new state-of-the-art complexity for sub-exponential noise. We give matching lower bounds (up to polylogarithmic factors) for both results. Additionally, we develop an efficient quantum isotropifier, a quantum algorithm which converts a variance-bounded quantum sampling oracle into one that outputs an unbiased estimate with isotropic error. Combining our results, we obtain improved dimension-dependent rates for quantum stochastic convex optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20745v1</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Annie Marsden, Liam O'Carroll, Aaron Sidford, Chenyi Zhang</dc:creator>
    </item>
    <item>
      <title>Balancing Gradient and Hessian Queries in Non-Convex Optimization</title>
      <link>https://arxiv.org/abs/2510.20786</link>
      <description>arXiv:2510.20786v1 Announce Type: new 
Abstract: We develop optimization methods which offer new trade-offs between the number of gradient and Hessian computations needed to compute the critical point of a non-convex function. We provide a method that for any twice-differentiable $f\colon \mathbb R^d \rightarrow \mathbb R$ with $L_2$-Lipschitz Hessian, input initial point with $\Delta$-bounded sub-optimality, and sufficiently small $\epsilon &gt; 0$, outputs an $\epsilon$-critical point, i.e., a point $x$ such that $\|\nabla f(x)\| \leq \epsilon$, using $\tilde{O}(L_2^{1/4} n_H^{-1/2}\Delta\epsilon^{-9/4})$ queries to a gradient oracle and $n_H$ queries to a Hessian oracle for any positive integer $n_H$. As a consequence, we obtain an improved gradient query complexity of $\tilde{O}(d^{1/3}L_2^{1/2}\Delta\epsilon^{-3/2})$ in the case of bounded dimension and of $\tilde{O}(L_2^{3/4}\Delta^{3/2}\epsilon^{-9/4})$ in the case where we are allowed only a \emph{single} Hessian query. We obtain these results through a more general algorithm which can handle approximate Hessian computations and recovers the state-of-the-art bound of computing an $\epsilon$-critical point with $O(L_1^{1/2}L_2^{1/4}\Delta\epsilon^{-7/4})$
  gradient queries provided that $f$ also has an $L_1$-Lipschitz gradient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20786v1</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Deeksha Adil, Brian Bullins, Aaron Sidford, Chenyi Zhang</dc:creator>
    </item>
    <item>
      <title>From Optimization to Prediction: Transformer-Based Path-Flow Estimation to the Traffic Assignment Problem</title>
      <link>https://arxiv.org/abs/2510.19889</link>
      <description>arXiv:2510.19889v1 Announce Type: cross 
Abstract: The traffic assignment problem is essential for traffic flow analysis, traditionally solved using mathematical programs under the Equilibrium principle. These methods become computationally prohibitive for large-scale networks due to non-linear growth in complexity with the number of OD pairs. This study introduces a novel data-driven approach using deep neural networks, specifically leveraging the Transformer architecture, to predict equilibrium path flows directly. By focusing on path-level traffic distribution, the proposed model captures intricate correlations between OD pairs, offering a more detailed and flexible analysis compared to traditional link-level approaches. The Transformer-based model drastically reduces computation time, while adapting to changes in demand and network structure without the need for recalculation. Numerical experiments are conducted on the Manhattan-like synthetic network, the Sioux Falls network, and the Eastern-Massachusetts network. The results demonstrate that the proposed model is orders of magnitude faster than conventional optimization. It efficiently estimates path-level traffic flows in multi-class networks, reducing computational costs and improving prediction accuracy by capturing detailed trip and flow information. The model also adapts flexibly to varying demand and network conditions, supporting traffic management and enabling rapid `what-if' analyses for enhanced transportation planning and policy-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19889v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mostafa Ameli, Van Anh Le, Sulthana Shams, Alexander Skabardonis</dc:creator>
    </item>
    <item>
      <title>Beyond the Ideal: Analyzing the Inexact Muon Update</title>
      <link>https://arxiv.org/abs/2510.19933</link>
      <description>arXiv:2510.19933v1 Announce Type: cross 
Abstract: The Muon optimizer has rapidly emerged as a powerful, geometry-aware alternative to AdamW, demonstrating strong performance in large-scale training of neural networks. However, a critical theory-practice disconnect exists: Muon's efficiency relies on fast, approximate orthogonalization, yet all prior theoretical work analyzes an idealized, computationally intractable version assuming exact SVD-based updates. This work moves beyond the ideal by providing the first analysis of the inexact orthogonalized update at Muon's core. We develop our analysis within the general framework of Linear Minimization Oracle (LMO)-based optimization, introducing a realistic additive error model to capture the inexactness of practical approximation schemes. Our analysis yields explicit bounds that quantify performance degradation as a function of the LMO inexactness/error. We reveal a fundamental coupling between this inexactness and the optimal step size and momentum: lower oracle precision requires a smaller step size but larger momentum parameter. These findings elevate the approximation procedure (e.g., the number of Newton-Schulz steps) from an implementation detail to a critical parameter that must be co-tuned with the learning schedule. NanoGPT experiments directly confirm the predicted coupling, with optimal learning rates clearly shifting as approximation precision changes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19933v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Egor Shulgin, Sultan AlRashed, Francesco Orabona, Peter Richt\'arik</dc:creator>
    </item>
    <item>
      <title>Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets</title>
      <link>https://arxiv.org/abs/2510.19950</link>
      <description>arXiv:2510.19950v1 Announce Type: cross 
Abstract: In financial applications, reinforcement learning (RL) agents are commonly trained on historical data, where their actions do not influence prices. However, during deployment, these agents trade in live markets where their own transactions can shift asset prices, a phenomenon known as market impact. This mismatch between training and deployment environments can significantly degrade performance. Traditional robust RL approaches address this model misspecification by optimizing the worst-case performance over a set of uncertainties, but typically rely on symmetric structures that fail to capture the directional nature of market impact. To address this issue, we develop a novel class of elliptic uncertainty sets. We establish both implicit and explicit closed-form solutions for the worst-case uncertainty under these sets, enabling efficient and tractable robust policy evaluation. Experiments on single-asset and multi-asset trading tasks demonstrate that our method achieves superior Sharpe ratio and remains robust under increasing trade volumes, offering a more faithful and scalable approach to RL in financial markets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19950v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shaocong Ma, Heng Huang</dc:creator>
    </item>
    <item>
      <title>On the Optimal Construction of Unbiased Gradient Estimators for Zeroth-Order Optimization</title>
      <link>https://arxiv.org/abs/2510.19953</link>
      <description>arXiv:2510.19953v1 Announce Type: cross 
Abstract: Zeroth-order optimization (ZOO) is an important framework for stochastic optimization when gradients are unavailable or expensive to compute. A potential limitation of existing ZOO methods is the bias inherent in most gradient estimators unless the perturbation stepsize vanishes. In this paper, we overcome this biasedness issue by proposing a novel family of unbiased gradient estimators based solely on function evaluations. By reformulating directional derivatives as a telescoping series and sampling from carefully designed distributions, we construct estimators that eliminate bias while maintaining favorable variance. We analyze their theoretical properties, derive optimal scaling distributions and perturbation stepsizes of four specific constructions, and prove that SGD using the proposed estimators achieves optimal complexity for smooth non-convex objectives. Experiments on synthetic tasks and language model fine-tuning confirm the superior accuracy and convergence of our approach compared to standard methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19953v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shaocong Ma, Heng Huang</dc:creator>
    </item>
    <item>
      <title>Revisiting Zeroth-Order Optimization: Minimum-Variance Two-Point Estimators and Directionally Aligned Perturbations</title>
      <link>https://arxiv.org/abs/2510.19975</link>
      <description>arXiv:2510.19975v1 Announce Type: cross 
Abstract: In this paper, we explore the two-point zeroth-order gradient estimator and identify the distribution of random perturbations that minimizes the estimator's asymptotic variance as the perturbation stepsize tends to zero. We formulate it as a constrained functional optimization problem over the space of perturbation distributions. Our findings reveal that such desired perturbations can align directionally with the true gradient, instead of maintaining a fixed length. While existing research has largely focused on fixed-length perturbations, the potential advantages of directional alignment have been overlooked. To address this gap, we delve into the theoretical and empirical properties of the directionally aligned perturbation (DAP) scheme, which adaptively offers higher accuracy along critical directions. Additionally, we provide a convergence analysis for stochastic gradient descent using $\delta$-unbiased random perturbations, extending existing complexity bounds to a wider range of perturbations. Through empirical evaluations on both synthetic problems and practical tasks, we demonstrate that DAPs outperform traditional methods under specific conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19975v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shaocong Ma, Heng Huang</dc:creator>
    </item>
    <item>
      <title>Approximate Model Predictive Control for Microgrid Energy Management via Imitation Learning</title>
      <link>https://arxiv.org/abs/2510.20040</link>
      <description>arXiv:2510.20040v1 Announce Type: cross 
Abstract: Efficient energy management is essential for reliable and sustainable microgrid operation amid increasing renewable integration. This paper proposes an imitation learning-based framework to approximate mixed-integer Economic Model Predictive Control (EMPC) for microgrid energy management. The proposed method trains a neural network to imitate expert EMPC control actions from offline trajectories, enabling fast, real-time decision making without solving optimization problems online. To enhance robustness and generalization, the learning process includes noise injection during training to mitigate distribution shift and explicitly incorporates forecast uncertainty in renewable generation and demand. Simulation results demonstrate that the learned policy achieves economic performance comparable to EMPC while only requiring $10\%$ of the computation time of optimization-based EMPC in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20040v1</guid>
      <category>eess.SY</category>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Changrui Liu, Shengling Shi, Anil Alan, Ganesh Kumar Venayagamoorthy, Bart De Schutter</dc:creator>
    </item>
    <item>
      <title>From Bundles to Backstepping: Geometric Control Barrier Functions for Safety-Critical Control on Manifolds</title>
      <link>https://arxiv.org/abs/2510.20202</link>
      <description>arXiv:2510.20202v1 Announce Type: cross 
Abstract: Control barrier functions (CBFs) have a well-established theory in Euclidean spaces, yet still lack general formulations and constructive synthesis tools for systems evolving on manifolds common in robotics and aerospace applications. In this paper, we develop a general theory of geometric CBFs on bundles and, for control-affine systems, recover the standard optimization-based CBF controllers and their smooth analogues. Then, by generalizing kinetic energy-based CBF backstepping to Riemannian manifolds, we provide a constructive CBF synthesis technique for geometric mechanical systems, as well as easily verifiable conditions under which it succeeds. Further, this technique utilizes mechanical structure to avoid computations on higher-order tangent bundles. We demonstrate its application to an underactuated satellite on SO(3).</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20202v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Massimiliano de Sa, Pio Ong, Aaron D. Ames</dc:creator>
    </item>
    <item>
      <title>Anderson-type acceleration method for Deep Neural Network optimization</title>
      <link>https://arxiv.org/abs/2510.20254</link>
      <description>arXiv:2510.20254v1 Announce Type: cross 
Abstract: In this paper we consider the neural network optimization for DNN. We develop Anderson-type acceleration method for the stochastic gradient decent method and it improves the network permanence very much. We demonstrate the applicability of the method for DNN and CNN. We discuss the application of the general class of the neural network design for computer tomography and inverse medium problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20254v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kazufumi Ito, Tiancheng Xue</dc:creator>
    </item>
    <item>
      <title>Energy Decay in Measure Time: HUM Observability, Product-Exponential Envelopes, and GCC Calibration</title>
      <link>https://arxiv.org/abs/2510.20371</link>
      <description>arXiv:2510.20371v1 Announce Type: cross 
Abstract: We prove that for impulsive exposure patterns there is no uniform exponential energy law in wall-clock time t, which explains why past t-based unifications of continuous damping with impulses fail. We therefore replace t by a measure-valued clock, sigma, that aggregates absolutely continuous exposure and atomic doses within a single Lyapunov ledger. On this ledger we prove an observability-dissipation principle in the sense of the Hilbert Uniqueness Method (HUM): there exists a structural constant c_sigma &gt; 0 such that the energy decays at least at a product-exponential rate with respect to sigma. When sigma = t, the statement reduces to classical exponential stabilization with the same constant. For the damped wave under the Geometric Control Condition (GCC), the constant is calibrated by the usual observability and geometric factors. The framework yields a monotonicity principle ("more sigma-mass implies faster decay") and unifies intermittent regimes where quiescent intervals are punctuated by impulses. As robustness, secondary to the main contribution, the same decay law persists under structure-compatible discretizations and along compact variational limits; a stochastic extension supplies expectation and pathwise envelopes via the compensator. The contribution is a qualitative dynamics backbone: observability implies sigma-exponential decay with sharp constants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20371v1</guid>
      <category>math.AP</category>
      <category>math.DS</category>
      <category>math.OC</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ben F. Tibola</dc:creator>
    </item>
    <item>
      <title>Optimizing Feature Ordering in Radar Charts for Multi-Profile Comparison</title>
      <link>https://arxiv.org/abs/2510.20738</link>
      <description>arXiv:2510.20738v1 Announce Type: cross 
Abstract: Radar charts are widely used to visualize multivariate data and compare multiple profiles across features. However, the visual clarity of radar charts can be severely compromised when feature values alternate drastically in magnitude around the circle, causing areas to collapse, which misrepresents relative differences. In the present work we introduce a permutation optimization strategy that reorders features to minimize polygon ``spikiness'' across multiple profiles simultaneously. The method is combinatorial (exhaustive search) for moderate numbers of features and uses a lexicographic minimax criterion that first considers overall smoothness (mean jump) and then the largest single jump as a tie-breaker. This preserves more global information and produces visually balanced arrangements. We discuss complexity, practical bounds, and relations to existing approaches that either change the visualization (e.g., OrigamiPlot) or learn orderings (e.g., Versatile Ordering Network). An example with two profiles and $p=6$ features (before/after ordering) illustrates the qualitative improvement.
  Keywords: data visualization, radar charts, combinatorial optimization, minimax optimization, feature ordering</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20738v1</guid>
      <category>cs.HC</category>
      <category>cs.DS</category>
      <category>cs.GR</category>
      <category>math.OC</category>
      <category>stat.OT</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Albert Dorador</dc:creator>
    </item>
    <item>
      <title>Extended Dijkstra algorithm and Moore-Bellman-Ford algorithm</title>
      <link>https://arxiv.org/abs/1708.04541</link>
      <description>arXiv:1708.04541v3 Announce Type: replace 
Abstract: Study the general single-source shortest path problem. Firstly, define a path function on a set of some path with same source on a graph, and develop a kind of general single-source shortest path problem (GSSSP) on the defined path function. Secondly, following respectively the approaches of the well known Dijkstra's algorithm and Moore-Bellman-Ford algorithm, design an extended Dijkstra's algorithm (EDA) and an extended Moore-Bellman-Ford algorithm (EMBFA) to solve the problem GSSSP under certain given conditions. Thirdly, introduce a few concepts, such as order-preserving in last road (OPLR) of path function, and so on. And under the assumption that the value of related path function for any path can be obtained in $M(n)$ time, prove respectively the algorithm EDA solving the problem GSSSP in $O(n^2)M(n)$ time and the algorithm EMBFA solving the problem GSSSP in $O(mn)M(n)$ time. Finally, some applications of the designed algorithms are shown with a few examples. What we done can improve both the researchers and the applications of the shortest path theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:1708.04541v3</guid>
      <category>math.OC</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cong-Dian Cheng</dc:creator>
    </item>
    <item>
      <title>Automated tight Lyapunov analysis for first-order methods</title>
      <link>https://arxiv.org/abs/2302.06713</link>
      <description>arXiv:2302.06713v3 Announce Type: replace 
Abstract: We present a methodology for establishing the existence of quadratic Lyapunov inequalities for a wide range of first-order methods used to solve convex optimization problems. In particular, we consider i) classes of optimization problems of finite-sum form with (possibly strongly) convex and possibly smooth functional components, ii) first-order methods that can be written as a linear system in state-space form in feedback interconnection with the subdifferentials of the functional components of the objective function, and iii) quadratic Lyapunov inequalities that can be used to draw convergence conclusions. We present a necessary and sufficient condition for the existence of a quadratic Lyapunov inequality within a predefined class of Lyapunov inequalities, which amounts to solving a small-sized semidefinite program. We showcase our methodology on several first-order methods that fit the framework. Most notably, our methodology allows us to significantly extend the region of parameter choices that allow for duality-gap convergence in the Chambolle-Pock method when the linear operator is the identity mapping.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.06713v3</guid>
      <category>math.OC</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s10107-024-02061-8</arxiv:DOI>
      <dc:creator>Manu Upadhyaya, Sebastian Banert, Adrien B. Taylor, Pontus Giselsson</dc:creator>
    </item>
    <item>
      <title>Dimension-Free Descriptions of Convex Sets</title>
      <link>https://arxiv.org/abs/2307.04230</link>
      <description>arXiv:2307.04230v3 Announce Type: replace 
Abstract: Convex sets arising in a variety of applications are well-defined for every relevant dimension. Examples include the simplex and the spectraplex that correspond to probability distributions and to quantum states; combinatorial polytopes and their relaxations such as the cut polytope and the elliptope in integer programming; and unit balls of regularizers such as the $\ell_p$ and Schatten norms in inverse problems. Moreover, these sets are often specified using conic descriptions that can be obviously instantiated in any dimension. We develop a systematic framework to study such dimension-free descriptions of convex sets. We show that dimension-free descriptions arise from a recently-identified phenomenon in algebraic topology called representation stability, which relates invariants across dimensions in a sequence of group representations. Our framework yields structural results for dimension-free descriptions pertaining to the relations between the sets they describe across dimensions, extendability of a single set in a given dimension to a freely-described sequence, and continuous limits of such sequences. We also develop a procedure to obtain parametric families of freely-described convex sets whose structure is adapted to a given application; illustrations are provided via examples that arise in the literature as well as new families that are derived using our procedure. We demonstrate the utility of our framework in two contexts. First, we develop an algorithm for a dimension-free analog of the convex regression problem, where a convex function is fit to input-output data; by searching over our parametric families, we can fit a function to low-dimensional inputs and extend it to any other dimension. Second, we prove that many sequences of symmetric conic programs can be solved in constant time, which unifies and strengthens several results in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.04230v3</guid>
      <category>math.OC</category>
      <category>math.RT</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eitan Levin, Venkat Chandrasekaran</dc:creator>
    </item>
    <item>
      <title>A partitioned optimization framework for structure-aware optimization</title>
      <link>https://arxiv.org/abs/2407.05046</link>
      <description>arXiv:2407.05046v3 Announce Type: replace 
Abstract: This work tackles a class of optimization problems where fixing some well-chosen combinations of the variables makes the problem substantially easier to solve. We consider that the variables space may be partitioned into subsets that fix these combinations to given values, so the restriction of the problem to any of the partition sets admits a tractable solution. Then, we exhibit a reformulation of the original problem that consists in searching for the partition set index that minimizes the objective value of the solution to the restricted problem. We name partitioned optimization framework (POf) the formalization of this class of problems and this reformulated problem. As we prove in this work, the POf allows solving the original problem by focusing on the reformulated problem: all solutions to the reformulated problem are partition indices for which the solution to the associated restricted problem is also a solution to the original problem. Second, we introduce a derivative-free partitioned optimization method (DFPOm) to efficiently solve problems that fit in the POf. We prove that the reformulated problem is nicely handled by a class of derivative-free optimization (DFO) algorithms named algorithms with a covering step. Then the DFPOm consists in solving the reformulated problem using such DFO algorithm with a covering step to obtain an optimal partition index, and to return the solution to the associated restricted problem as a solution to the initial problem. Finally, we illustrate the DFPOm on a class of problems called composite greybox problems, and we highlight the gain in numerical performance provided by the DFPOm on large-dimensional instances by comparing it to two popular DFO solvers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05046v3</guid>
      <category>math.OC</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charles Audet, Pierre-Yves Bouchet, Lo\"ic Bourdin</dc:creator>
    </item>
    <item>
      <title>Parametric Shape Optimization of Flagellated Micro-Swimmers Using Bayesian Techniques</title>
      <link>https://arxiv.org/abs/2409.11776</link>
      <description>arXiv:2409.11776v2 Announce Type: replace 
Abstract: Understanding and optimizing the design of helical micro-swimmers is crucial for advancing their application in various fields. This study presents an innovative approach combining Free-Form Deformation with Bayesian Optimization to enhance the shape of these swimmers. Our method facilitates the computation of generic swimmer shapes that achieve optimal average speed and efficiency. Applied to both monoflagellated and biflagellated swimmers, our optimization framework has led to the identification of new optimal shapes. These shapes are compared with biological counterparts, highlighting a diverse range of swimmers, including both pushers and pullers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11776v2</guid>
      <category>math.OC</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevFluids.10.034101</arxiv:DOI>
      <arxiv:journal_reference>Phys. Rev. Fluids 10, 034101 (2025)</arxiv:journal_reference>
      <dc:creator>Lucas Palazzolo (CRISAM), La\"etitia Giraldi (CRISAM), Mickael Binois (ACUMES, CRISAM, LJAD), Luca Berti (IRMA)</dc:creator>
    </item>
    <item>
      <title>A primal-dual algorithm for image reconstruction with input-convex neural network regularizers</title>
      <link>https://arxiv.org/abs/2410.12441</link>
      <description>arXiv:2410.12441v2 Announce Type: replace 
Abstract: We address the optimization problem in a data-driven variational reconstruction framework, where the regularizer is parameterized by an input-convex neural network (ICNN). While gradient-based methods are commonly used to solve such problems, they struggle to effectively handle non-smooth problems which often leads to slow convergence. Moreover, the nested structure of the neural network complicates the application of standard non-smooth optimization techniques, such as proximal algorithms. To overcome these challenges, we reformulate the problem and eliminate the network's nested structure. By relating this reformulation to epigraphical projections of the activation functions, we transform the problem into a convex optimization problem that can be efficiently solved using a primal-dual algorithm. We also prove that this reformulation is equivalent to the original variational problem. Through experiments on several imaging tasks, we show that the proposed approach not only outperforms subgradient methods and even accelerated methods in the smooth setting, but also facilitates the training of the regularizer itself.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12441v2</guid>
      <category>math.OC</category>
      <category>cs.CV</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthias J. Ehrhardt, Subhadip Mukherjee, Hok Shing Wong</dc:creator>
    </item>
    <item>
      <title>CoCoA Is ADMM: Unifying Two Paradigms in Distributed Optimization</title>
      <link>https://arxiv.org/abs/2502.00470</link>
      <description>arXiv:2502.00470v2 Announce Type: replace 
Abstract: We consider primal-dual algorithms for general empirical risk minimization problems in distributed settings, focusing on two prominent classes of algorithms. The first class is the communication-efficient distributed dual coordinate ascent (CoCoA), derived from the coordinate ascent method for solving the dual problem. The second class is the alternating direction method of multipliers (ADMM), including consensus ADMM, proximal ADMM, and linearized ADMM. We demonstrate that both classes of algorithms can be transformed into a unified update form that involves only primal and dual variables. This discovery reveals key connections between the two classes of algorithms: CoCoA can be interpreted as a special case of proximal ADMM for solving the dual problem, while consensus ADMM is equivalent to a proximal ADMM algorithm. This discovery provides insight into how we can easily enable the ADMM variants to outperform the CoCoA variants by adjusting the augmented Lagrangian parameter. We further explore linearized versions of ADMM and analyze the effects of tuning parameters on these ADMM variants in the distributed setting. Extensive simulation studies and real-world data analysis support our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00470v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Runxiong Wu, Dong Liu, Xueqin Wang, Andi Wang</dc:creator>
    </item>
    <item>
      <title>Equity-aware Design and Timing of Fare-free Transit Zoning under Demand Uncertainty</title>
      <link>https://arxiv.org/abs/2502.08731</link>
      <description>arXiv:2502.08731v2 Announce Type: replace 
Abstract: We propose the first analytical stochastic model for optimizing the configuration and implementation policies of fare-free transit. The model focuses on a transportation corridor with two transportation modes: automobiles and buses. The corridor is divided into two sections, an inner one with fare-free transit service and an outer one with fare-based transit service. Under the static version of the model, the optimized length and frequency of the fare-free transit zone can be determined by maximizing total social welfare. The findings indicate that implementing fare-free transit can increase transit ridership and reduce automobile use within the fare-free zone while social equity among the demand groups can be enhanced by lengthening the fare-free zone. Notably, the optimal zone length increases when both social welfare and equity are considered jointly, compared to only prioritizing social welfare. The dynamic model, framed within a market entry and exit real options approach, solves the fare policy switching problem, establishing optimal timing policies for activating or terminating fare-free service. The results from dynamic models reveal earlier implementation and extended durations of fare-free transit in the social welfare-aware regime, driven by lower thresholds compared to the social equity-aware regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08731v2</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qianwen Guo, Jiaqing Lu, Joseph Y. J. Chow, Paul Schonfeld</dc:creator>
    </item>
    <item>
      <title>Ergodicity and turnpike properties of linear-quadratic mean field control problems</title>
      <link>https://arxiv.org/abs/2502.08935</link>
      <description>arXiv:2502.08935v2 Announce Type: replace 
Abstract: We study the asymptotic behavior of solutions to linear-quadratic mean field stochastic optimal control problems. By formulating an ergodic control framework, we characterize the convergence between the finite time horizon control problem and its ergodic counterpart. Leveraging these convergence results, we establish the turnpike property for the optimal pairs, demonstrating that solutions to the finite time horizon control problem remain exponentially close to the ergodic equilibrium except near the temporal boundaries. This result reveals the intrinsic connection between long-term dynamics and their asymptotic behavior in mean field control systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08935v2</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Erhan Bayraktar, Jiamin Jian</dc:creator>
    </item>
    <item>
      <title>Discrete-time gradient flows for unbounded convex functions on Gromov hyperbolic spaces</title>
      <link>https://arxiv.org/abs/2503.19652</link>
      <description>arXiv:2503.19652v2 Announce Type: replace 
Abstract: In proper, geodesic Gromov hyperbolic spaces, we investigate discrete-time gradient flows via the proximal point algorithm for unbounded Lipschitz convex functions. Assuming that the target convex function has negative asymptotic slope along some ray (thus unbounded below), we first prove the uniqueness of such a negative direction in the boundary at infinity. Then, we show that the discrete-time gradient flow from an arbitrary initial point diverges to that unique direction of negative asymptotic slope. This is inspired by and generalizes results of Karlsson-Margulis and Hirai-Sakabe on nonpositively curved spaces and a result of Karlsson concerning semi-contractions on Gromov hyperbolic spaces. We also give an estimate of the rate of convergence based on a contraction property for the proximal (resolvent) operator established in our previous work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19652v2</guid>
      <category>math.OC</category>
      <category>math.MG</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shin-ichi Ohta</dc:creator>
    </item>
    <item>
      <title>Controlling the Flow: Stability and Convergence for Stochastic Gradient Descent with Decaying Regularization</title>
      <link>https://arxiv.org/abs/2505.11434</link>
      <description>arXiv:2505.11434v2 Announce Type: replace 
Abstract: The present article studies the minimization of convex, L-smooth functions defined on a separable real Hilbert space. We analyze regularized stochastic gradient descent (reg-SGD), a variant of stochastic gradient descent that uses a Tikhonov regularization with time-dependent, vanishing regularization parameter. We prove strong convergence of reg-SGD to the minimum-norm solution of the original problem without additional boundedness assumptions. Moreover, we quantify the rate of convergence and optimize the interplay between step-sizes and regularization decay. Our analysis reveals how vanishing Tikhonov regularization controls the flow of SGD and yields stable learning dynamics, offering new insights into the design of iterative algorithms for convex problems, including those that arise in ill-posed inverse problems. We validate our theoretical findings through numerical experiments on image reconstruction and ODE-based inverse problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11434v2</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sebastian Kassing, Simon Weissmann, Leif D\"oring</dc:creator>
    </item>
    <item>
      <title>Qualitative and Generalized Differentiation Properties of Optimal Value Functions with Applications to Duality</title>
      <link>https://arxiv.org/abs/2507.07377</link>
      <description>arXiv:2507.07377v3 Announce Type: replace 
Abstract: This paper investigates general and generalized differentiation properties of the optimal value function associated with perturbed optimization problems. Fundamental results on nearly convex sets and functions in infinite-dimensional spaces are then established. We proceed by analyzing general properties of the optimal value function, including its domain, epigraph, strict epigraph, near convexity, semicontinuity, and Lipschitz-type continuity in both convex and nonconvex settings. Subsequently, we derive calculus rules and representation formulas for the $\epsilon$-subdifferentials of the optimal value function and its Fenchel conjugate. We then develop a duality framework for constrained optimization problems with set-valued constraints using the Fenchel conjugate for set-valued mappings. This approach provides new perspectives on duality in generalized settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07377v3</guid>
      <category>math.OC</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>V. S. T. Long, B. S. Mordukhovich, N. M. Nam, L. White</dc:creator>
    </item>
    <item>
      <title>Distributional Adversarial Attacks and Training in Deep Hedging</title>
      <link>https://arxiv.org/abs/2508.14757</link>
      <description>arXiv:2508.14757v2 Announce Type: replace 
Abstract: In this paper, we study the robustness of classical deep hedging strategies under distributional shifts by leveraging the concept of adversarial attacks. We first demonstrate that standard deep hedging models are highly vulnerable to small perturbations in the input distribution, resulting in significant performance degradation. Motivated by this, we propose an adversarial training framework tailored to increase the robustness of deep hedging strategies. Our approach extends pointwise adversarial attacks to the distributional setting and introduces a computationally tractable reformulation of the adversarial optimization problem over a Wasserstein ball. This enables the efficient training of hedging strategies that are resilient to distributional perturbations. Through extensive numerical experiments, we show that adversarially trained deep hedging strategies consistently outperform their classical counterparts in terms of out-of-sample performance and resilience to model misspecification. Additional results indicate that the robust strategies maintain reliable performance on real market data and remain effective during periods of market change. Our findings establish a practical and effective framework for robust deep hedging under realistic market uncertainties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14757v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guangyi He, Tobias Sutter, Lukas Gonon</dc:creator>
    </item>
    <item>
      <title>Bellman Optimality of Average-Reward Robust Markov Decision Processes with a Constant Gain</title>
      <link>https://arxiv.org/abs/2509.14203</link>
      <description>arXiv:2509.14203v2 Announce Type: replace 
Abstract: Learning and optimal control under robust Markov decision processes (MDPs) have received increasing attention, yet most existing theory, algorithms, and applications focus on finite-horizon or discounted models. Long-run average-reward formulations, while natural in many operations research and management contexts, remain underexplored. This is primarily because the dynamic programming foundations are technically challenging and only partially understood, with several fundamental questions remaining open. This paper steps toward a general framework for average-reward robust MDPs by analyzing the constant-gain setting. We study the average-reward robust control problem with possible information asymmetries between the controller and an S-rectangular adversary. Our analysis centers on the constant-gain robust Bellman equation, examining both the existence of solutions and their relationship to the optimal average reward. Specifically, we identify when solutions to the robust Bellman equation characterize the optimal average reward and stationary policies, and we provide one-sided weak communication conditions ensuring solutions' existence. These findings expand the dynamic programming theory for average-reward robust MDPs and lay a foundation for robust dynamic decision making under long-run average criteria in operational environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14203v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shengbo Wang, Nian Si</dc:creator>
    </item>
    <item>
      <title>Overfitting in Adaptive Robust Optimization</title>
      <link>https://arxiv.org/abs/2509.16451</link>
      <description>arXiv:2509.16451v2 Announce Type: replace 
Abstract: Adaptive robust optimization (ARO) extends static robust optimization by allowing decisions to depend on the realized uncertainty - weakly dominating static solutions within the modeled uncertainty set. However, ARO makes previous constraints that were independent of uncertainty now dependent, making it vulnerable to additional infeasibilities when realizations fall outside the uncertainty set. This phenomenon of adaptive policies being brittle is analogous to overfitting in machine learning. To mitigate against this, we propose assigning constraint-specific uncertainty set sizes, with harder constraints given stronger probabilistic guarantees. Interpreted through the overfitting lens, this acts as regularization: tighter guarantees shrink adaptive coefficients to ensure stability, while looser ones preserve useful flexibility. This view motivates a principled approach to designing uncertainty sets that balances robustness and adaptivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16451v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Karl Zhu, Dimitris Bertsimas</dc:creator>
    </item>
    <item>
      <title>Recommend-to-Match with Random Supply Rejections: Formulation, Approximation, and Analysis</title>
      <link>https://arxiv.org/abs/2510.19122</link>
      <description>arXiv:2510.19122v2 Announce Type: replace 
Abstract: Matching demand with supply in crowd-sourcing logistics platforms must contend with uncertain worker participation. Motivated by this challenge, we study a two-stage ``recommend-to-match" problem under stochastic supplier rejections, where each demand is initially recommended to multiple potential suppliers prior to final matching decisions. We formulate a stochastic optimization model that explicitly captures uncertain supplier acceptance behavior. We show that an exact mixed-integer linear formulation is obtainable for the special case with homogeneous and independent acceptance responses, but the general problem does not admit an efficient formulation. Particularly, our analysis reveals that deterministic linear approximation methods can perform arbitrarily poorly in such settings. To overcome this limitation, we propose a new approximation approach based on mixed-integer exponential cone programming (MIECP) and establish its parametric performance guarantees. Extensive experiments on synthetic data and real-world freight data validate the effectiveness of our approach. Our MIECP-based solution achieves near-optimal matching performance while reducing computation time by over 90% compared to benchmark methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19122v2</guid>
      <category>math.OC</category>
      <category>cs.DM</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoyue Liu, Sheng Liu, Mingyao Qi</dc:creator>
    </item>
    <item>
      <title>Stability and performance of stochastic economic MPC - Stochastic characterization of the closed-loop asymptotics</title>
      <link>https://arxiv.org/abs/2510.19416</link>
      <description>arXiv:2510.19416v2 Announce Type: replace 
Abstract: Model Predictive Control (MPC) is well understood in the deterministic setting, yet rigorous stability and performance guarantees for stochastic MPC remain limited to the consideration of terminal constraints and penalties. In contrast, this work analyzes stochastic economic MPC with an expected cost criterion and establishes closed-loop guarantees without terminal conditions. Relying on stochastic dissipativity and turnpike properties, we construct closed-loop Lyapunov functions that ensure $P$-practical asymptotic stability of a particular optimal stationary process under different notions of stochastic convergence, such as in distribution or in the $p$-th mean. In addition, we derive tight near-optimal bounds for both averaged and non-averaged performance, thereby extending classical deterministic results to the stochastic domain. Finally, we show that the abstract stochastic MPC scheme requiring distributional knowledge shares the same closed-loop properties as a practically implementable algorithm based only on sampled state information, ensuring applicability of our findings. Our findings are illustrated by a numerical example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19416v2</guid>
      <category>math.OC</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonas Schie{\ss}l, Hannah Selder, Ruchuan Ou, Michael Heinrich Baumann, Timm Faulwasser, Lars Gr\"une</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Linear Functionals of Online SGD in High-dimensional Linear Regression</title>
      <link>https://arxiv.org/abs/2302.09727</link>
      <description>arXiv:2302.09727v4 Announce Type: replace-cross 
Abstract: Stochastic gradient descent (SGD) has emerged as the quintessential method in a data scientist's toolbox. Using SGD for high-stakes applications requires, however, careful quantification of the associated uncertainty. Towards that end, in this work, we establish a high-dimensional Central Limit Theorem (CLT) for linear functionals of online SGD iterates for overparametrized least-squares regression with non-isotropic Gaussian inputs. We first show that a bias-corrected CLT holds when the number of iterations of the online SGD, $t$, grows sub-linearly in the dimensionality, $d$. In order to use the developed result in practice, we further develop an online approach for estimating the variance term appearing in the CLT, and establish high-probability bounds for the developed online estimator. Together with the CLT result, this provides a fully online and data-driven way to numerically construct confidence intervals. This enables practical high-dimensional algorithmic inference with SGD and to the best of our knowledge, is the first such result.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.09727v4</guid>
      <category>math.ST</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bhavya Agrawalla, Krishnakumar Balasubramanian, Promit Ghosal</dc:creator>
    </item>
    <item>
      <title>Solving 0-1 Integer Programs with Unknown Knapsack Constraints Using Membership Oracles</title>
      <link>https://arxiv.org/abs/2405.14090</link>
      <description>arXiv:2405.14090v4 Announce Type: replace-cross 
Abstract: We consider solving a combinatorial optimization problem with unknown knapsack constraints using a membership oracle for each unknown constraint such that, given a solution, the oracle determines whether the constraint is satisfied or not with absolute certainty. The goal of the decision maker is to find the best possible solution subject to a budget on the number of oracle calls. Inspired by active learning for binary classification based on Support Vector Machines (SVMs), we devise a framework to solve the problem by learning and exploiting surrogate linear constraints. The framework includes training linear separators on the labeled points and selecting new points to be labeled, which is achieved by applying a sampling strategy and solving a 0-1 integer linear program. Following the active learning literature, a natural choice would be SVM as a linear classifier and the information-based sampling strategy known as simple margin, for each unknown constraint. We improve on both sides: we propose an alternative sampling strategy based on mixed-integer quadratic programming and a linear separation method inspired by an algorithm for convex optimization in the oracle model. We conduct experiments on classical problems and variants inspired by realistic applications to show how different linear separation methods and sampling strategies influence the quality of the results in terms of several metrics including objective value, dual bound and running time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14090v4</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rosario Messana, Rui Chen, Andrea Lodi, Alberto Ceselli</dc:creator>
    </item>
    <item>
      <title>Deep Learning for Continuous-time Stochastic Control with Jumps</title>
      <link>https://arxiv.org/abs/2505.15602</link>
      <description>arXiv:2505.15602v2 Announce Type: replace-cross 
Abstract: In this paper, we introduce a model-based deep-learning approach to solve finite-horizon continuous-time stochastic control problems with jumps. We iteratively train two neural networks: one to represent the optimal policy and the other to approximate the value function. Leveraging a continuous-time version of the dynamic programming principle, we derive two different training objectives based on the Hamilton-Jacobi-Bellman equation, ensuring that the networks capture the underlying stochastic dynamics. Empirical evaluations on different problems illustrate the accuracy and scalability of our approach, demonstrating its effectiveness in solving complex, high-dimensional stochastic control tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15602v2</guid>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <category>q-fin.PM</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patrick Cheridito, Jean-Loup Dupret, Donatien Hainaut</dc:creator>
    </item>
    <item>
      <title>KOALA++: Efficient Kalman-Based Optimization of Neural Networks with Gradient-Covariance Products</title>
      <link>https://arxiv.org/abs/2506.04432</link>
      <description>arXiv:2506.04432v2 Announce Type: replace-cross 
Abstract: We propose KOALA++, a scalable Kalman-based optimization algorithm that explicitly models structured gradient uncertainty in neural network training. Unlike second-order methods, which rely on expensive second order gradient calculation, our method directly estimates the parameter covariance matrix by recursively updating compact gradient covariance products. This design improves upon the original KOALA framework that assumed diagonal covariance by implicitly capturing richer uncertainty structure without storing the full covariance matrix and avoiding large matrix inversions. Across diverse tasks, including image classification and language modeling, KOALA++ achieves accuracy on par or better than state-of-the-art first- and second-order optimizers while maintaining the efficiency of first-order methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04432v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zixuan Xia, Aram Davtyan, Paolo Favaro</dc:creator>
    </item>
  </channel>
</rss>
