<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 23 Sep 2024 04:00:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Hyperbolic Brain Representations</title>
      <link>https://arxiv.org/abs/2409.12990</link>
      <description>arXiv:2409.12990v1 Announce Type: new 
Abstract: Artificial neural networks (ANN) were inspired by the architecture and functions of the human brain and have revolutionised the field of artificial intelligence (AI). Inspired by studies on the latent geometry of the brain we posit that an increase in the research and application of hyperbolic geometry in machine learning will lead to increased accuracy, improved feature space representations and more efficient models across a range of tasks. We look at the structure and functions of the human brain, highlighting the alignment between the brain's hierarchical nature and hyperbolic geometry. By examining the brain's complex network of neuron connections and its cognitive processes, we illustrate how hyperbolic geometry plays a pivotal role in human intelligence. Empirical evidence indicates that hyperbolic neural networks outperform Euclidean models for tasks including natural language processing, computer vision and complex network analysis, requiring fewer parameters and exhibiting better generalisation. Despite its nascent adoption, hyperbolic geometry holds promise for improving machine learning models and advancing the field toward AGI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12990v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Joseph, Nathan Francis, Meijke Balay</dc:creator>
    </item>
    <item>
      <title>Stimulus-to-Stimulus Learning in RNNs with Cortical Inductive Biases</title>
      <link>https://arxiv.org/abs/2409.13471</link>
      <description>arXiv:2409.13471v1 Announce Type: new 
Abstract: Animals learn to predict external contingencies from experience through a process of conditioning. A natural mechanism for conditioning is stimulus substitution, whereby the neuronal response to a stimulus with no prior behavioral significance becomes increasingly identical to that generated by a behaviorally significant stimulus it reliably predicts. We propose a recurrent neural network model of stimulus substitution which leverages two forms of inductive bias pervasive in the cortex: representational inductive bias in the form of mixed stimulus representations, and architectural inductive bias in the form of two-compartment pyramidal neurons that have been shown to serve as a fundamental unit of cortical associative learning. The properties of these neurons allow for a biologically plausible learning rule that implements stimulus substitution, utilizing only information available locally at the synapses. We show that the model generates a wide array of conditioning phenomena, and can learn large numbers of associations with an amount of training commensurate with animal experiments, without relying on parameter fine-tuning for each individual experimental task. In contrast, we show that commonly used Hebbian rules fail to learn generic stimulus-stimulus associations with mixed selectivity, and require task-specific parameter fine-tuning. Our framework highlights the importance of multi-compartment neuronal processing in the cortex, and showcases how it might confer cortical animals the evolutionary edge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13471v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pantelis Vafidis, Antonio Rangel</dc:creator>
    </item>
    <item>
      <title>A Spacetime Perspective on Dynamical Computation in Neural Information Processing Systems</title>
      <link>https://arxiv.org/abs/2409.13669</link>
      <description>arXiv:2409.13669v1 Announce Type: new 
Abstract: There is now substantial evidence for traveling waves and other structured spatiotemporal recurrent neural dynamics in cortical structures; but these observations have typically been difficult to reconcile with notions of topographically organized selectivity and feedforward receptive fields. We introduce a new 'spacetime' perspective on neural computation in which structured selectivity and dynamics are not contradictory but instead are complimentary. We show that spatiotemporal dynamics may be a mechanism by which natural neural systems encode approximate visual, temporal, and abstract symmetries of the world as conserved quantities, thereby enabling improved generalization and long-term working memory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13669v1</guid>
      <category>q-bio.NC</category>
      <category>cs.NE</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>T. Anderson Keller, Lyle Muller, Terrence J. Sejnowski, Max Welling</dc:creator>
    </item>
    <item>
      <title>Fluctuation-learning relationship in neural networks</title>
      <link>https://arxiv.org/abs/2409.13597</link>
      <description>arXiv:2409.13597v1 Announce Type: cross 
Abstract: Learning in neural systems occurs through change in synaptic connectivity that is driven by neural activity. Learning performance is influenced by both neural activity and the task to be learned. Experimental studies suggest a link between learning speed and variability in neural activity before learning. However, the theoretical basis of this relationship has remained unclear. In this work, using principles from the fluctuation-response relation in statistical physics, we derive two formulae that connect neural activity with learning speed. The first formula shows that learning speed is proportional to the variance of spontaneous neural activity and the neural response to input. The second formula, for small input, indicates that speed is proportional to the variances of spontaneous activity in both target and input directions. These formulae apply to various learning tasks governed by Hebbian or generalized learning rules. Numerical simulations confirm that these formulae are valid beyond their theoretical assumptions, even in cases where synaptic connectivity undergoes large changes. Our theory predicts that learning speed increases with the gain of neuronal activation functions and the number of pre-embedded memories, as both enhance the variance of spontaneous neural fluctuations. Additionally, the formulae reveal which input/output relationships are easier to learn, aligning with experimental data. Thus, our results provide a theoretical foundation for the quantitative relationship between pre-learning neural activity fluctuations and learning speed, offering insights into a range of empirical observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13597v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>nlin.AO</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tomoki Kurikawa, Kunihiko Kaneko</dc:creator>
    </item>
  </channel>
</rss>
