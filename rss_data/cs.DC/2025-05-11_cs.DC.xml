<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 12 May 2025 04:00:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Understanding Stragglers in Large Model Training Using What-if Analysis</title>
      <link>https://arxiv.org/abs/2505.05713</link>
      <description>arXiv:2505.05713v1 Announce Type: new 
Abstract: Large language model (LLM) training is one of the most demanding distributed computations today, often requiring thousands of GPUs with frequent synchronization across machines. Such a workload pattern makes it susceptible to stragglers, where the training can be stalled by few slow workers. At ByteDance we find stragglers are not trivially always caused by hardware failures, but can arise from multiple complex factors. This work aims to present a comprehensive study on the straggler issues in LLM training, using a five-month trace collected from our ByteDance LLM training cluster. The core methodology is what-if analysis that simulates the scenario without any stragglers and contrasts with the actual case. We use this method to study the following questions: (1) how often do stragglers affect training jobs, and what effect do they have on job performance; (2) do stragglers exhibit temporal or spatial patterns; and (3) what are the potential root causes for stragglers?</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05713v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinkun Lin, Ziheng Jiang, Zuquan Song, Sida Zhao, Menghan Yu, Zhanghan Wang, Chenyuan Wang, Zuocheng Shi, Xiang Shi, Wei Jia, Zherui Liu, Shuguang Wang, Haibin Lin, Xiu Liu, Aurojit Panda, Jinyang Li</dc:creator>
    </item>
    <item>
      <title>DawnPiper: A Memory-scablable Pipeline Parallel Training Framework</title>
      <link>https://arxiv.org/abs/2505.05856</link>
      <description>arXiv:2505.05856v1 Announce Type: new 
Abstract: Pipeline parallelism is a crucial paradigm for large-scale model training. However, imbalances in memory footprint across stages can lead to significant GPU memory wastage, limiting the model sizes that pipeline parallelism can effectively support. In this paper, we introduce DawnPiper, a memory-scalable pipeline parallel training framework. Firstly, we develop a DL compilation-based profiling method that transforms the model into a fine-grained computation graph. This refinement gives us a finer granularity of model partitioning and memory optimization while facilitating automatic code generation. Based on observed memory usage characteristics, we derive a performance-optimal theorem for pipeline parallel partitioning that substantially reduces the partition search space. Secondly, we propose a binary pipeline partitioning algorithm and utilize a cost-model based memory optimization approach to efficiently identify nearly optimal pipeline parallel strategy. DawnPiper achieves up to a 4x and 11x increase in trainable maximum batch size compared to vPipe and PipeDream, respectively, and provides up to a 1.5x performance speedup compared to vPipe.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05856v1</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuan Peng, Xuanhua Shi, Haolin Zhang, Yunfei Zhao, Xuehai Qian</dc:creator>
    </item>
    <item>
      <title>Taming Offload Overheads in a Massively Parallel Open-Source RISC-V MPSoC: Analysis and Optimization</title>
      <link>https://arxiv.org/abs/2505.05911</link>
      <description>arXiv:2505.05911v1 Announce Type: new 
Abstract: Heterogeneous multi-core architectures combine on a single chip a few large, general-purpose host cores, optimized for single-thread performance, with (many) clusters of small, specialized, energy-efficient accelerator cores for data-parallel processing. Offloading a computation to the many-core acceleration fabric implies synchronization and communication overheads which can hamper overall performance and efficiency, particularly for small and fine-grained parallel tasks. In this work, we present a detailed, cycle-accurate quantitative analysis of the offload overheads on Occamy, an open-source massively parallel RISC-V based heterogeneous MPSoC. We study how the overheads scale with the number of accelerator cores. We explore an approach to drastically reduce these overheads by co-designing the hardware and the offload routines. Notably, we demonstrate that by incorporating multicast capabilities into the Network-on-Chip of a large (200+ cores) accelerator fabric we can improve offloaded application runtimes by as much as 2.3x, restoring more than 70% of the ideally attainable speedups. Finally, we propose a quantitative model to estimate the runtime of selected applications accounting for the offload overheads, with an error consistently below 15%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05911v1</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TPDS.2025.3555718</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Parallel and Distributed Systems, vol. 36, no. 6, pp. 1193-1205, June 2025</arxiv:journal_reference>
      <dc:creator>Luca Colagrande, Luca Benini</dc:creator>
    </item>
    <item>
      <title>An Autonomy Loop for Dynamic HPC Job Time Limit Adjustment</title>
      <link>https://arxiv.org/abs/2505.05927</link>
      <description>arXiv:2505.05927v1 Announce Type: new 
Abstract: High Performance Computing (HPC) systems rely on fixed user-provided estimates of job time limits. These estimates are often inaccurate, resulting in inefficient resource use and the loss of unsaved work if a job times out shortly before reaching its next checkpoint. This work proposes a novel feedback-driven autonomy loop that dynamically adjusts HPC job time limits based on checkpoint progress reported by applications. Our approach monitors checkpoint intervals and queued jobs, enabling informed decisions to either early cancel a job after its last completed checkpoint or extend the time limit sufficiently to accommodate the next checkpoint. The objective is to minimize tail waste, that is, the computation that occurs between the last checkpoint and the termination of a job, which is not saved and hence wasted. Through experiments conducted on a subset of a production workload trace, we show a 95% reduction of tail waste, which equates to saving approximately 1.3% of the total CPU time that would otherwise be wasted. We propose various policies that combine early cancellation and time limit extension, achieving tail waste reduction while improving scheduling metrics such as weighted average job wait time. This work contributes an autonomy loop for improved scheduling in HPC environments, where system job schedulers and applications collaborate to significantly reduce resource waste and improve scheduling performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05927v1</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Jakobsche, Osman Seckin Simsek, Jim Brandt, Ann Gentile, Florina M. Ciorba</dc:creator>
    </item>
    <item>
      <title>Toward Heterogeneous, Distributed, and Energy-Efficient Computing with SYCL</title>
      <link>https://arxiv.org/abs/2505.06022</link>
      <description>arXiv:2505.06022v1 Announce Type: new 
Abstract: Programming modern high-performance computing systems is challenging due to the need to efficiently program GPUs and accelerators and to handle data movement between nodes. The C++ language has been continuously enhanced in recent years with features that greatly increase productivity. In particular, the C++-based SYCL standard provides a powerful programming model for heterogeneous systems that can target a wide range of devices, including multicore CPUs, GPUs, FPGAs, and accelerators, while providing high-level abstractions. This presentation introduces our research efforts to design a SYCL-based high-level programming interface that provides advanced techniques such as task distribution and energy optimization. The key insight is that SYCL semantics can be easily extended to provide advanced features for easy integration into existing SYCL programs. In particular, we will highlight two SYCL extensions that are designed to deal with workload distribution on accelerator clusters (Celerity) and with energy-efficient computing (SYnergy).</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06022v1</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Biagio Cosenza, Lorenzo Carpentieri, Kaijie Fan, Marco D'Antonio, Peter Thoman, Philip Salzmann</dc:creator>
    </item>
    <item>
      <title>Distributed Tensor Network Library for Quantum Computing Emulation</title>
      <link>https://arxiv.org/abs/2505.06119</link>
      <description>arXiv:2505.06119v1 Announce Type: new 
Abstract: Tensor networks offer an adaptable and efficient approach to emulation of quantum computers. Their usage relies on partitioning circuits into small tensors, which are contracted together to form the final result. While this approach intends to minimise the problem size, exceeding the locally available memory is sometimes unavoidable due to the exponential nature of quantum systems. Most HPC tensor network packages tackle this issue with a procedure called circuit slicing, which distributes the entire network onto multiple ranks, recombining it back when necessary. In this study, we present a novel alternative approach, where individual tensors are both broadcast and scattered to harness multiple levels of parallelism. The technique is abstracted behind a fixed distribution pattern, and actualised in a new portable tensor network library, QTNH, built on top of MPI and ScaLAPACK. We showcase its capabilities on ARCHER2, by emulating two well-known algorithms - the Quantum Fourier Transform and Random Circuit Sampling. This is accomplished by leveraging the implemented operations to realise various contraction strategies, including a unique distributed MPS tensor factorisation approach. We thus demonstrate that our library can be used to advance the accuracy of quantum emulation, while offering a simple and flexible interface to tensor distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06119v1</guid>
      <category>cs.DC</category>
      <category>quant-ph</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jakub Adamski, Oliver Thomson Brown</dc:creator>
    </item>
    <item>
      <title>Characterizing GPU Energy Usage in Exascale-Ready Portable Science Applications</title>
      <link>https://arxiv.org/abs/2505.05623</link>
      <description>arXiv:2505.05623v1 Announce Type: cross 
Abstract: We characterize the GPU energy usage of two widely adopted exascale-ready applications representing two classes of particle and mesh solvers: (i) QMCPACK, a quantum Monte Carlo package, and (ii) AMReX-Castro, an adaptive mesh astrophysical code. We analyze power, temperature, utilization, and energy traces from double-/single (mixed)-precision benchmarks on NVIDIA's A100 and H100 and AMD's MI250X GPUs using queries in NVML and rocm smi lib, respectively. We explore application-specific metrics to provide insights on energy vs. performance trade-offs. Our results suggest that mixed-precision energy savings range between 6-25% on QMCPACK and 45% on AMReX-Castro. Also there are still gaps in the AMD tooling on Frontier GPUs that need to be understood, while query resolutions on NVML have little variability between 1 ms and 1 s. Overall, application level knowledge is crucial to define energy-cost/science-benefit opportunities for the codesign of future supercomputer architectures in the post-Moore era.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05623v1</guid>
      <category>cs.PF</category>
      <category>cs.CE</category>
      <category>cs.DC</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William F. Godoy, Oscar Hernandez, Paul R. C. Kent, Maria Patrou, Kazi Asifuzzaman, Narasinga Rao Miniskar, Pedro Valero-Lara, Jeffrey S. Vetter, Matthew D. Sinclair, Jason Lowe-Power, Bobby R. Bruce</dc:creator>
    </item>
    <item>
      <title>All-to-All Communication with Mobile Edge Adversary: Almost Linearly More Faults, For Free</title>
      <link>https://arxiv.org/abs/2505.05735</link>
      <description>arXiv:2505.05735v1 Announce Type: cross 
Abstract: Resilient computation in all-to-all-communication models has attracted tremendous attention over the years. Most of these works assume the classical faulty model which restricts the total number of corrupted edges (or vertices) by some integer fault parameter $f$. A recent work by [Bodwin, Haeupler and Parter, SODA 2024] introduced a stronger notion of fault-tolerance, in the context of graph sparsification, which restricts the degree of the failing edge set $F$, rather than its cardinality. For a subset of faulty edges $F$, the faulty-degree $\mathrm{deg}(F)$ is the largest number of faults in $F$ incident to any given node.
  In this work, we study the communication aspects of this faulty model which allows us to handle almost linearly more edge faults (possibly quadratic), with no extra cost. Our end results are general compilers that take any Congested Clique algorithm and simulate it, in a round by round manner, in the presence of a $\alpha$-Byzantine mobile adversary that controls a $\alpha$-fraction of the edges incident to each node in the fully connected network. For every round $i$, the mobile adversary is allowed to select a distinct set of corrupted edges $F_i$ under the restriction that $\mathrm{deg}(F_i)\leq \alpha n$. In the non-adaptive setting, the $F_i$ sets are selected at the beginning of the simulation, while in the adaptive setting, these edges can be chosen based on the entire history of the protocol up to round $i$.
  We show general compilers for the non-adaptive, adaptive, and deterministic settings. A key component of our algorithms is a new resilient routing scheme which may be of independent interest. Our approach is based on a combination of techniques, including error-correcting-code, locally decodable codes, cover-free families, and sparse recovery sketches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05735v1</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Orr Fischer, Merav Parter</dc:creator>
    </item>
    <item>
      <title>Efficient Information Updates in Compute-First Networking via Reinforcement Learning with Joint AoI and VoI</title>
      <link>https://arxiv.org/abs/2505.06025</link>
      <description>arXiv:2505.06025v1 Announce Type: cross 
Abstract: Timely and efficient dissemination of service information is critical in compute-first networking systems, where user requests arrive dynamically and computing resources are constrained. In such systems, the access point (AP) plays a key role in forwarding user requests to a server based on its latest received service information. This paper considers a single-source, single-destination system and introduces an Age-and-Value-Aware (AVA) metric that jointly captures both the timeliness and the task relevance of service information. Unlike traditional freshness-based metrics, AVA explicitly incorporates variations in server-side service capacity and AP forwarding decisions, allowing more context-aware update evaluation. Building upon AVA, we propose a reinforcement learning-based update policy that learns to selectively transmit service information updates to the AP. It aims to maximize overall task success while minimizing unnecessary communications. Extensive simulations under diverse user request patterns and varying service capacities demonstrate that AVA reduces the update frequency by over 90% on average compared to baselines, with reductions reaching 98% in certain configurations. Crucially, this reduction is achieved without compromising the accuracy of task execution or the quality of decision making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06025v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianpeng Qi, Chao Liu, Chengxiang Xu, Rui Wang, Junyu Dong, Yanwei Yu</dc:creator>
    </item>
    <item>
      <title>Scheduled Jacobian Chaining</title>
      <link>https://arxiv.org/abs/2505.06056</link>
      <description>arXiv:2505.06056v1 Announce Type: cross 
Abstract: This paper addresses the efficient computation of Jacobian matrices for programs composed of sequential differentiable subprograms. By representing the overall Jacobian as a chain product of the Jacobians of these subprograms, we reduce the problem to optimizing the sequence of matrix multiplications, known as the Jacobian Matrix Chain Product problem. Solutions to this problem yield "optimal bracketings", which induce a precedence-constraint scheduling problem. We investigate the inherent parallelism in the solutions and develop a new dynamic programming algorithm as a heuristic that incorporates the scheduling. To assess its performance, we benchmark it against the global optimum, which is computed via a branch-and-bound algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06056v1</guid>
      <category>cs.DM</category>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon M\"artens, Uwe Naumann</dc:creator>
    </item>
    <item>
      <title>HashKitty: Distributed Password Analysis</title>
      <link>https://arxiv.org/abs/2505.06084</link>
      <description>arXiv:2505.06084v1 Announce Type: cross 
Abstract: This article documents the HashKitty platform, a distributed solution for password analysis based on the hashcat tool, designed to improve efficiency in both offensive and defensive security operations. The main objectives of this work are to utilise and characterise the hashcat tool, to develop a central platform that connects various computational nodes, to allow the use of nodes with different equipment and manufacturers, to distribute tasks among the nodes through a web platform, and to perform distributed password analysis. The results show that the presented solution achieves the proposed objectives, demonstrating effectiveness in workload distribution and password analysis using different types of nodes based on various operating systems and architectures. The architecture of HashKitty is based on a scalable and modular distributed architecture, composed of several components such as computational nodes, integration and control software, a web platform that implements our API, and database servers. In order to achieve a fast and organised development process for our application we used multiple frameworks, runtimes and libraries. For the communication between the computational nodes and the other software we made use of websockets so that we have real-time updates between them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06084v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pedro Antunes, Tom\'as Santos, Daniel Fuentes, Lu\'is Fraz\~ao</dc:creator>
    </item>
    <item>
      <title>On Optimal Batch Size in Coded Computing</title>
      <link>https://arxiv.org/abs/2505.06199</link>
      <description>arXiv:2505.06199v1 Announce Type: cross 
Abstract: We consider computing systems that partition jobs into tasks, add redundancy through coding, and assign the encoded tasks to different computing nodes for parallel execution. The expected execution time depends on the level of redundancy. The computing nodes execute large jobs in batches of tasks. We show that the expected execution time depends on the batch size as well. The optimal batch size that minimizes the execution time depends on the level of redundancy under a fixed number of parallel servers and other system parameters. Furthermore, we show how to (jointly) optimize the redundancy level and batch size to reduce the expected job completion time for two service-time distributions. The simulation presented helps us appreciate the claims.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06199v1</guid>
      <category>cs.IT</category>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <category>math.IT</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Swapnil Saha, Emina Soljanin, Philip Whiting</dc:creator>
    </item>
    <item>
      <title>Sublinear-Time Sampling of Spanning Trees in the Congested Clique</title>
      <link>https://arxiv.org/abs/2411.13334</link>
      <description>arXiv:2411.13334v2 Announce Type: replace 
Abstract: We present the first sublinear-in-$n$ round algorithm for sampling an approximately uniform spanning tree of an $n$-vertex graph in the CongestedClique model of distributed computing. In particular, our algorithm requires $\Tilde{O}(n^{0.657})$ rounds for sampling a spanning tree within total variation distance $1/n^c$, for arbitrary constant $c &gt; 0$, from the uniform distribution. More precisely, our algorithm requires $\Tilde{O}(n^{1/2 + \alpha})$ rounds, where $O(n^\alpha)$ is the running time of matrix multiplication in the CongestedClique model (currently $\alpha = 1 - 2/\omega = 0.157$, where $\omega$ is the sequential matrix multiplication time exponent). We can adapt our algorithm to give exact rather than approximate samples, but with a larger, though still $o(n)$, runtime of $\Tilde{O}(n^{2/3+\alpha}) = O(n^{.824})$.
  In a remarkable result, Aldous (SIDM 1990) and Broder (FOCS 1989) showed that the first visit edge to each vertex, excluding the start vertex, during a random walk forms a uniformly chosen spanning tree of the underlying graph. Our algorithm is a significant departure from known techniques, featuring a top-down walk filling approach paired with Schur complement graphs for walk shortcutting. To make this idea work in the CongestedClique model, we present a novel compressed random walk reconstruction algorithm, based on randomly sampling a weighted perfect matching.
  In addition, we show how to take somewhat shorter random walks even more efficiently in the CongestedClique model, obtaining an $O(\log^3 n)$-round algorithm for uniformly sampling spanning trees from graphs with $O(n\log n)$ cover times. These results are obtained by adding a load balancing component to the random walk algorithm of Bahmani, Chakrabarti and Xin (SIGMOD 2011) that uses the bottom-up ``doubling'' technique.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13334v2</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sriram V. Pemmaraju, Sourya Roy, Joshua Z. Sobel</dc:creator>
    </item>
    <item>
      <title>Medha: Efficiently Serving Multi-Million Context Length LLM Inference Requests Without Approximations</title>
      <link>https://arxiv.org/abs/2409.17264</link>
      <description>arXiv:2409.17264v3 Announce Type: replace-cross 
Abstract: As large language models (LLMs) handle increasingly longer contexts, serving long inference requests of millions of tokens presents unique challenges. We show that existing work for long context inference is largely based on techniques from long context training, and does not handle the high variability in input lengths during inference. This leads to inefficient resource utilization, server fragmentation, and head-of-line (HOL) blocking.
  We present Medha, an end-to-end system for efficient long-context LLM inference that addresses these challenges through fine-grained time sharing. Medha introduces three key innovations: (1) the mechanism of adaptive prefill chunking to help mitigate HOL blocking with preemption; (2) two new parallelism strategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token by pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower time-peroutput-token by distributing decoding across servers; and (3) a novel input-length aware least remaining slack scheduling to meet Service Level Objectives (SLOs).
  Medha enables exact inference scaling beyond 10 million tokens, maintaining high throughput and low latency across mixed-length workloads. Compared to state-of-the-art systems, Medha reduces server fragmentation, cuts median latency by up to 30x, and improves throughput by over 5x, delivering production-scale long-context inference without compromising performance on shorter requests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17264v3</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amey Agrawal, Haoran Qiu, Junda Chen, \'I\~nigo Goiri, Chaojie Zhang, Rayyan Shahid, Ramachandran Ramjee, Alexey Tumanov, Esha Choukse</dc:creator>
    </item>
    <item>
      <title>Multi-Draft Speculative Sampling: Canonical Decomposition and Theoretical Limits</title>
      <link>https://arxiv.org/abs/2410.18234</link>
      <description>arXiv:2410.18234v2 Announce Type: replace-cross 
Abstract: We consider multi-draft speculative sampling, where the proposal sequences are sampled independently from different draft models. At each step, a token-level draft selection scheme takes a list of valid tokens as input and produces an output token whose distribution matches that of the target model. Previous works have demonstrated that the optimal scheme (which maximizes the probability of accepting one of the input tokens) can be cast as a solution to a linear program. In this work we show that the optimal scheme can be decomposed into a two-step solution: in the first step an importance sampling (IS) type scheme is used to select one intermediate token; in the second step (single-draft) speculative sampling is applied to generate the output token. For the case of two identical draft models we further 1) establish a necessary and sufficient condition on the distributions of the target and draft models for the acceptance probability to equal one and 2) provide an explicit expression for the optimal acceptance probability. Our theoretical analysis also motives a new class of token-level selection schemes based on weighted importance sampling. Our experimental results demonstrate consistent improvements in the achievable block efficiency and token rates over baseline schemes in a number of scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18234v2</guid>
      <category>cs.CL</category>
      <category>cs.DC</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ashish Khisti, M. Reza Ebrahimi, Hassan Dbouk, Arash Behboodi, Roland Memisevic, Christos Louizos</dc:creator>
    </item>
    <item>
      <title>Round and Communication Efficient Graph Coloring</title>
      <link>https://arxiv.org/abs/2412.12589</link>
      <description>arXiv:2412.12589v2 Announce Type: replace-cross 
Abstract: In the context of communication complexity, we explore protocols for graph coloring, focusing on the vertex and edge coloring problems in $n$-vertex graphs $G$ with a maximum degree $\Delta$. We consider a scenario where the edges of $G$ are partitioned between two players.
  Our first contribution is a randomized protocol that efficiently finds a $(\Delta + 1)$-vertex coloring of $G$, utilizing $O(n)$ bits of communication in expectation and completing in $O(\log \log n \cdot \log \Delta)$ rounds in the worst case. This advancement represents a significant improvement over the work of Flin and Mittal [Distributed Computing 2025], who achieved the same communication cost but required $O(n)$ rounds in expectation, thereby making a significant reduction in the round complexity.
  Our second contribution is a deterministic protocol to compute a $(2\Delta - 1)$-edge coloring of $G$, which maintains the same $O(n)$ bits of communication and uses only $O(1)$ rounds. We complement the result with a tight $\Omega(n)$-bit lower bound on the communication complexity of the $(2\Delta-1)$-edge coloring problem, while a similar $\Omega(n)$ lower bound for the $(\Delta+1)$-vertex coloring problem has been established by Flin and Mittal [Distributed Computing 2025]. Our result implies a space lower bound of $\Omega(n)$ bits for $(2\Delta - 1)$-edge coloring in the $W$-streaming model, which is the first non-trivial space lower bound for edge coloring in the $W$-streaming model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12589v2</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yi-Jun Chang, Gopinath Mishra, Hung Thuan Nguyen, Farrel D Salim</dc:creator>
    </item>
    <item>
      <title>Improving the scalability of a high-order atmospheric dynamics solver based on the deal.II library</title>
      <link>https://arxiv.org/abs/2505.00384</link>
      <description>arXiv:2505.00384v2 Announce Type: replace-cross 
Abstract: We present recent advances on the massively parallel performance of a numerical scheme for atmosphere dynamics applications based on the deal.II library. The implicit-explicit discontinuous finite element scheme is based on a matrix-free approach, meaning that no global sparse matrix is built and only the action of the linear operators on a vector is actually implemented. Following a profiling analysis, we focus on the performance optimization of the numerical method and describe the impact of different preconditioning and solving techniques in this framework. Moreover, we show how the use of the latest version of the deal.II library and of suitable execution flags can improve the parallel performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00384v2</guid>
      <category>math.NA</category>
      <category>cs.DC</category>
      <category>cs.NA</category>
      <category>cs.PF</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giuseppe Orlando, Tommaso Benacchio, Luca Bonaventura</dc:creator>
    </item>
  </channel>
</rss>
