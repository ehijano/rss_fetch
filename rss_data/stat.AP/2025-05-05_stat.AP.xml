<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 06 May 2025 02:30:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Thinning-Stable Point Processes as a Model for Spatial Burstiness</title>
      <link>https://arxiv.org/abs/2505.00717</link>
      <description>arXiv:2505.00717v1 Announce Type: new 
Abstract: In modern telecommunications, spatial burstiness of data traffic
  poses challenges to traditional Poisson-based models. This paper
  describes application of thinning-stable point processes,
  which provide a more appropriate framework for modeling bursty
  spatial data. We discuss their properties, representation, inference
  methods, and applications, demonstrating the advantages over
  classical approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00717v1</guid>
      <category>stat.AP</category>
      <category>math.PR</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sergei Zuyev</dc:creator>
    </item>
    <item>
      <title>Multi-site modelling and reconstruction of past extreme skew surges along the French Atlantic coast</title>
      <link>https://arxiv.org/abs/2505.00835</link>
      <description>arXiv:2505.00835v1 Announce Type: new 
Abstract: Appropriate modelling of extreme skew surges is crucial, particularly for coastal risk management. Our study focuses on modelling extreme skew surges along the French Atlantic coast, with a particular emphasis on investigating the extremal dependence structure between stations. We employ the peak-over-threshold framework, where a multivariate extreme event is defined whenever at least one location records a large value, though not necessarily all stations simultaneously. A novel method for determining an appropriate level (threshold) above which observations can be classified as extreme is proposed. Two complementary approaches are explored. First, the multivariate generalized Pareto distribution is employed to model extremes, leveraging its properties to derive a generative model that predicts extreme skew surges at one station based on observed extremes at nearby stations. Second, a novel extreme regression framework is assessed for point predictions. This specific regression framework enables accurate point predictions using only the "angle" of input variables, i.e. input variables divided by their norms. The ultimate objective is to reconstruct historical skew surge time series at stations with limited data. This is achieved by integrating extreme skew surge data from stations with longer records, such as Brest and Saint-Nazaire, which provide over 150 years of observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00835v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan Huet, Philippe Naveau, Anne Sabourin</dc:creator>
    </item>
    <item>
      <title>Low-rank bilinear autoregressive models for three-way criminal activity tensors</title>
      <link>https://arxiv.org/abs/2505.01166</link>
      <description>arXiv:2505.01166v1 Announce Type: new 
Abstract: Criminal activity data are typically available via a three-way tensor encoding the reported frequencies of different crime categories across time and space. The challenges that arise in the design of interpretable, yet realistic, model-based representations of the complex dependencies within and across these three dimensions have led to an increasing adoption of black-box predictive strategies. Although this perspective has proved successful in producing accurate forecasts guiding targeted interventions, the lack of interpretable model-based characterizations of the dependence structures underlying criminal activity tensors prevents from inferring the cascading effects of these interventions across the different dimensions. We address this gap through the design of a low-rank bilinear autoregressive model which achieves comparable predictive performance to black-box strategies, while allowing interpretable inference on the dependence structures of criminal activity reports across crime categories, time, and space. This representation incorporates the time dimension via an autoregressive construction, accounting for spatial effects and dependencies among crime categories through a separable low-rank bilinear formulation. When applied to Chicago police reports, the proposed model showcases remarkable predictive performance and also reveals interpretable dependence structures unveiling fundamental crime dynamics. These results facilitate the design of more refined intervention policies informed by cascading effects of the policy itself.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01166v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gregor Zens, Carlos D\'iaz, Daniele Durante, Eleonora Patacchini</dc:creator>
    </item>
    <item>
      <title>Geoinformation dependencies in geographic space and beyond</title>
      <link>https://arxiv.org/abs/2505.01260</link>
      <description>arXiv:2505.01260v1 Announce Type: new 
Abstract: The use of geospatially dependent information, which has been stipulated as a law in geography, to model geographic patterns forms the cornerstone of geostatistics, and has been inherited in many data science based techniques as well, such as statistical learning algorithms. Still, we observe hesitations in interpreting geographic dependency scientifically as a property in geography, since interpretations of such dependency are subject to model choice with different hypotheses of trends and stationarity. Rather than questioning what can be considered as trends or why it is non-stationary, in this work, we share and consolidate a view that the properties of geographic dependency, being it trending or stationary, are essentially variations can be explained further by unobserved or unknown predictors, and not intrinsic to geographic space. Particularly, geoinformation dependency properties are in fact a projection of high dimensional feature space formed by all potential predictors into the lower dimension of geographic space, where geographic coordinates are equivalent to other predictors for modelling geographic patterns. This work brings together different aspects of geographic dependency, including similarity and heterogeneity, under a coherent framework, and aligns with the understanding of modelling in high dimensional feature space with different modelling concept including the classical geostatistics, Gaussian Process Regression and popular data science based spatial modelling techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01260v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jon Wang, Meng Lu</dc:creator>
    </item>
    <item>
      <title>Drilling into Erasmus learning mobility flows between countries 2014-2024</title>
      <link>https://arxiv.org/abs/2505.00889</link>
      <description>arXiv:2505.00889v1 Announce Type: cross 
Abstract: Analyzing the Erasmus mobility network, we illustrate typical problems and approaches in analyzing weighted networks. We propose alternative exploratory views on the network "Erasmus+ learning mobility flows since 2014". The network has 35 nodes (countries), is very dense, and the range of link weights (number of visits) is huge (from 1 to 217003). An increasing transformation is used to reduce the range. The traditional graph-based visualization is unreadable. To gain insight into the structure of a dense network, it can be reduced to a skeleton by removing less essential links and/or nodes. We have determined the 1-neighbors and 2-neighbors subnetworks. The 1-neighbors skeleton highlights Spain as the main attractor in the network. The 2-neighbors skeleton shows the dominant role of Spain, Germany, France, and Italy. The hubs and authorities, Pathfinder and Ps cores methods confirm these observations.
  Using the "right" order of the nodes in a matrix representation can reveal the network structure as block patterns in the displayed matrix. The clustering of network nodes based on corrected Salton dissimilarity again shows the dominant role of Spain, Germany, France, and Italy, but also two main clusters of the division into developed/less developed countries. The Balassa normalization (log(measured/expected) visits) matrix shows that most visits within the two main clusters are above expected, while most visits between them are below expected; within the clusters of Balkan countries, Baltic countries, {SK, CZ, HU}, {IS, DK, NO} visits are much above expected, etc.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00889v1</guid>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vladimir Batagelj</dc:creator>
    </item>
    <item>
      <title>Towards modelling lifetime default risk: Exploring different subtypes of recurrent event Cox-regression models</title>
      <link>https://arxiv.org/abs/2505.01044</link>
      <description>arXiv:2505.01044v1 Announce Type: cross 
Abstract: In the pursuit of modelling a loan's probability of default (PD) over its lifetime, repeat default events are often ignored when using Cox Proportional Hazard (PH) models. Excluding such events may produce biased and inaccurate PD-estimates, which can compromise financial buffers against future losses. Accordingly, we investigate a few subtypes of Cox-models that can incorporate recurrent default events. Using South African mortgage data, we explore both the Andersen-Gill (AG) and the Prentice-Williams-Peterson (PWP) spell-time models. These models are compared against a baseline that deliberately ignores recurrent events, called the time to first default (TFD) model. Models are evaluated using Harrell's c-statistic, adjusted Cox-Sell residuals, and a novel extension of time-dependent receiver operating characteristic (ROC) analysis. From these Cox-models, we demonstrate how to derive a portfolio-level term-structure of default risk, which is a series of marginal PD-estimates at each point of the average loan's lifetime. While the TFD- and PWP-models do not differ significantly across all diagnostics, the AG-model underperformed expectations. Depending on the prevalence of recurrent defaults, one may therefore safely ignore them when estimating lifetime default risk. Accordingly, our work enhances the current practice of using Cox-modelling in producing timeous and accurate PD-estimates under IFRS 9.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01044v1</guid>
      <category>q-fin.RM</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arno Botha, Tanja Verster, Bernard Scheepers</dc:creator>
    </item>
    <item>
      <title>Joint Modelling of Line and Point Data on Metric Graphs</title>
      <link>https://arxiv.org/abs/2505.01175</link>
      <description>arXiv:2505.01175v1 Announce Type: cross 
Abstract: Metric graphs are useful tools for describing spatial domains like road and river networks, where spatial dependence act along the network. We take advantage of recent developments for such Gaussian Random Fields (GRFs), and consider joint spatial modelling of observations with different spatial supports. Motivated by an application to traffic state modelling in Trondheim, Norway, we consider line-referenced data, which can be described by an integral of the GRF along a line segment on the metric graph, and point-referenced data. Through a simulation study inspired by the application, we investigate the number of replicates that are needed to estimate parameters and to predict unobserved locations. The former is assessed using bias and variability, and the latter is assessed through root mean square error (RMSE), continuous rank probability scores (CRPSs), and coverage. Joint modelling is contrasted with a simplified approach that treat line-referenced observations as point-referenced observations. The results suggest joint modelling leads to strong improvements. The application to Trondheim, Norway, combines point-referenced induction loop data and line-referenced public transportation data. To ensure positive speeds, we use a non-linear link function, which requires integrals of non-linear combinations of the linear predictor. This is made computationally feasible by a combination of the R packages inlabru and MetricGraph, and new code for processing geographical line data to work with existing graph representations and fmesher methods for dealing with line support in inlabru on objects from MetricGraph. We fit the model to two datasets where we expect different spatial dependency and compare the results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01175v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Karina Lilleborge, Sara Martino, Geir-Arne Fuglstad, Finn Lindgren, Rikke Ingebrigtsen</dc:creator>
    </item>
    <item>
      <title>Integration of Multi-Mode Preference into Home Energy Management System Using Deep Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2505.01332</link>
      <description>arXiv:2505.01332v1 Announce Type: cross 
Abstract: Home Energy Management Systems (HEMS) have emerged as a pivotal tool in the smart home ecosystem, aiming to enhance energy efficiency, reduce costs, and improve user comfort. By enabling intelligent control and optimization of household energy consumption, HEMS plays a significant role in bridging the gap between consumer needs and energy utility objectives. However, much of the existing literature construes consumer comfort as a mere deviation from the standard appliance settings. Such deviations are typically incorporated into optimization objectives via static weighting factors. These factors often overlook the dynamic nature of consumer behaviors and preferences. Addressing this oversight, our paper introduces a multi-mode Deep Reinforcement Learning-based HEMS (DRL-HEMS) framework, meticulously designed to optimize based on dynamic, consumer-defined preferences. Our primary goal is to augment consumer involvement in Demand Response (DR) programs by embedding dynamic multi-mode preferences tailored to individual appliances. In this study, we leverage a model-free, single-agent DRL algorithm to deliver a HEMS framework that is not only dynamic but also user-friendly. To validate its efficacy, we employed real-world data at 15-minute intervals, including metrics such as electricity price, ambient temperature, and appliances' power consumption. Our results show that the model performs exceptionally well in optimizing energy consumption within different preference modes. Furthermore, when compared to traditional algorithms based on Mixed-Integer Linear Programming (MILP), our model achieves nearly optimal performance while outperforming in computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01332v1</guid>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.AP</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mohammed Sumayli, Olugbenga Moses Anubi</dc:creator>
    </item>
    <item>
      <title>Demystifying and avoiding the OLS "weighting problem": Unmodeled heterogeneity and straightforward solutions</title>
      <link>https://arxiv.org/abs/2403.03299</link>
      <description>arXiv:2403.03299v4 Announce Type: replace-cross 
Abstract: Researchers frequently estimate treatment effects by regressing outcomes (Y) on treatment (D) and covariates (X). Even without unobserved confounding, the coefficient on D yields a conditional-variance-weighted average of strata-wise effects, not the average treatment effect. Scholars have proposed characterizing the severity of these weights, evaluating resulting biases, or changing investigators' target estimand to the conditional-variance-weighted effect. We aim to demystify these weights, clarifying how they arise, what they represent, and how to avoid them. Specifically, these weights reflect misspecification bias from unmodeled treatment-effect heterogeneity. Rather than diagnosing or tolerating them, we recommend avoiding the issue altogether, by relaxing the standard regression assumption of "single linearity" to one of "separate linearity" (of each potential outcome in the covariates), accommodating heterogeneity. Numerous methods--including regression imputation (g-computation), interacted regression, and mean balancing weights--satisfy this assumption. In many settings, the efficiency cost to avoiding this weighting problem altogether will be modest and worthwhile.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03299v4</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tanvi Shinkre, Chad Hazlett</dc:creator>
    </item>
    <item>
      <title>Bayesian Emulation of Grey-Box Multi-Model Ensembles Exploiting Known Interior Structure</title>
      <link>https://arxiv.org/abs/2406.08367</link>
      <description>arXiv:2406.08367v2 Announce Type: replace-cross 
Abstract: Computer models are widely used to study complex real world physical systems. However, there are major limitations to their direct use including: their complex structure; large numbers of inputs and outputs; and long evaluation times. Bayesian emulators are an effective means of addressing these challenges providing fast and efficient statistical approximation for computer model outputs. It is commonly assumed that computer models behave like a ``black-box'' function with no knowledge of the output prior to its evaluation. This ensures that emulators are generalisable but potentially limits their accuracy compared with exploiting such knowledge of constrained or structured output behaviour. We assume a ``grey-box'' computer model and develop a methodological toolkit for its analysis. This includes: multi-model ensemble subsampling to identifying a representative model subset to reduce computational expense; constructing a targeted Bayesian design for optimisation or decision support; a ``divide-and-conquer'' approach to emulating sums of outputs; structured emulators exploiting known constrained and structured behaviour of constituent outputs through splitting the parameter space and imposing truncations; emulation of sums of time series outputs; and emulation of multi-model ensemble outputs. Combining these methods establishes a hierarchical emulation framework which achieves greater physical interpretability and more accurate emulator predictions. This research is motivated by and applied to the commercially important TNO OLYMPUS Well Control Optimisation Challenge from the petroleum industry which we re-express as a decision support under uncertainty problem. We thus encourage users to examine their ``black-box'' simulators to achieve superior emulator accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08367v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan Owen, Ian Vernon</dc:creator>
    </item>
  </channel>
</rss>
