<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.med-ph updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.med-ph</link>
    <description>physics.med-ph updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.med-ph" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 28 Aug 2024 01:43:54 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Multi-Modality and Temporal Analysis of Cervical Cancer Treatment Response</title>
      <link>https://arxiv.org/abs/2408.13408</link>
      <description>arXiv:2408.13408v1 Announce Type: new 
Abstract: Cervical cancer presents a significant global health challenge, necessitating advanced diagnostic and prognostic approaches for effective treatment. This paper investigates the potential of employing multi-modal medical imaging at various treatment stages to enhance cervical cancer treatment outcomes prediction. We show that among Gray Level Co-occurrence Matrix (GLCM) features, contrast emerges as the most effective texture feature regarding prediction accuracy. Integration of multi-modal imaging and texture analysis offers a promising avenue for personalized and targeted interventions, as well as more effective management of cervical cancer. Moreover, there is potential to reduce the number of time measurements and modalities in future cervical cancer treatment. This research contributes to advancing the field of precision diagnostics by leveraging the information embedded in noninvasive medical images, contributing to improving prognostication and optimizing therapeutic strategies for individuals diagnosed with cervical cancer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13408v1</guid>
      <category>physics.med-ph</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haotian Feng, Emi Yoshida, Ke Sheng</dc:creator>
    </item>
    <item>
      <title>Cross-sectional imaging of speed-of-sound distribution using photoacoustic reversal beacons</title>
      <link>https://arxiv.org/abs/2408.13975</link>
      <description>arXiv:2408.13975v1 Announce Type: new 
Abstract: Photoacoustic tomography (PAT) enables non-invasive cross-sectional imaging of biological tissues, but it fails to map the spatial variation of speed-of-sound (SOS) within tissues. While SOS is intimately linked to density and elastic modulus of tissues, the imaging of SOS distri-bution serves as a complementary imaging modality to PAT. Moreover, an accurate SOS map can be leveraged to correct for PAT image degradation arising from acoustic heterogene-ities. Herein, we propose a novel approach for SOS reconstruction using only PAT imaging modality. Our method is based on photoacoustic reversal beacons (PRBs), which are small light-absorbing targets with strong photoacoustic contrast. We excite and scan a number of PRBs positioned at the periphery of the target, and the generated photoacoustic waves prop-agate through the target from various directions, thereby achieve spatial sampling of the internal SOS. We formulate a linear inverse model for pixel-wise SOS reconstruction and solve it with iterative optimization technique. We validate the feasibility of the proposed method through simulations, phantoms, and ex vivo biological tissue tests. Experimental results demonstrate that our approach can achieve accurate reconstruction of SOS distribu-tion. Leveraging the obtained SOS map, we further demonstrate significantly enhanced PAT image reconstruction with acoustic correction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13975v1</guid>
      <category>physics.med-ph</category>
      <category>eess.IV</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Wang, Danni Wang, Liting Zhong, Yi Zhou, Qing Wang, Wufan Chen, Li Qi</dc:creator>
    </item>
    <item>
      <title>Multi-watt long-wavelength infrared femtosecond lasers and resonant enamel ablation</title>
      <link>https://arxiv.org/abs/2408.13789</link>
      <description>arXiv:2408.13789v1 Announce Type: cross 
Abstract: High-power broadband tunable long-wavelength infrared (LWIR) femtosecond lasers operating at fingerprint wavelengths of 7-14 {\mu}m hold significant promise across a range of applications, including molecular hyperspectral imaging, strong-field light-matter interaction, and resonant tissue ablation. Here we present 6-12 {\mu}m broadband tunable parametric amplifier based on LiGaS2 or BaGa4S7, generating new record output power of 2.4 W at 7.5 {\mu}m, and 1.5 W at 9.5 {\mu}m, pumped by a simple and effective thin-square-rod Yb:YAG amplifier producing 110 W 274 fs output pulses. As a proof of concept, we showcase efficient resonant ablation and microstructure fabrication on enamel at the hydroxyapatite resonant wavelength of 9.5 {\mu}m, with a laser intensity two orders-of-magnitude lower than that required by non-resonant femtosecond lasers, which could foster more precision surgical applications with superior biosafety.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13789v1</guid>
      <category>physics.optics</category>
      <category>physics.med-ph</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xuemei Yang, Dunxiang Zhang, Weizhe Wang, Kan Tian, Linzhen He, Jinmiao Guo, Bo Hu, Tao Pu, Wenlong Li, Shiran Sun, Chunmei Ding, Han Wu, Kenkai Li, Yujie Peng, Jianshu Li, Yuxin Leng, Houkun Liang</dc:creator>
    </item>
    <item>
      <title>Personalized Topology-Informed 12-Lead ECG Electrode Localization from Incomplete Cardiac MRIs for Efficient Cardiac Digital Twins</title>
      <link>https://arxiv.org/abs/2408.13945</link>
      <description>arXiv:2408.13945v1 Announce Type: cross 
Abstract: Cardiac digital twins (CDTs) offer personalized \textit{in-silico} cardiac representations for the inference of multi-scale properties tied to cardiac mechanisms. The creation of CDTs requires precise information about the electrode position on the torso, especially for the personalized electrocardiogram (ECG) calibration. However, current studies commonly rely on additional acquisition of torso imaging and manual/semi-automatic methods for ECG electrode localization. In this study, we propose a novel and efficient topology-informed model to fully automatically extract personalized ECG electrode locations from 2D clinically standard cardiac MRIs. Specifically, we obtain the sparse torso contours from the cardiac MRIs and then localize the electrodes from the contours. Cardiac MRIs aim at imaging of the heart instead of the torso, leading to incomplete torso geometry within the imaging. To tackle the missing topology, we incorporate the electrodes as a subset of the keypoints, which can be explicitly aligned with the 3D torso topology. The experimental results demonstrate that the proposed model outperforms the time-consuming conventional method in terms of accuracy (Euclidean distance: $1.24 \pm 0.293$ cm vs. $1.48 \pm 0.362$ cm) and efficiency ($2$~s vs. $30$-$35$~min). We further demonstrate the effectiveness of using the detected electrodes for \textit{in-silico} ECG simulation, highlighting their potential for creating accurate and efficient CDT models. The code will be released publicly after the manuscript is accepted for publication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13945v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>physics.med-ph</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Lei Li, Hannah Smith, Yilin Lyu, Julia Camps, Blanca Rodriguez, Abhirup Banerjee, Vicente Grau</dc:creator>
    </item>
    <item>
      <title>DuDoCROP: Dual-Domain CLIP-Assisted Residual Optimization Perception Model for CT Metal Artifact Reduction</title>
      <link>https://arxiv.org/abs/2408.14342</link>
      <description>arXiv:2408.14342v1 Announce Type: cross 
Abstract: Metal artifacts in computed tomography (CT) imaging pose significant challenges to accurate clinical diagnosis. The presence of high-density metallic implants results in artifacts that deteriorate image quality, manifesting in the forms of streaking, blurring, or beam hardening effects, etc. Nowadays, various deep learning-based approaches, particularly generative models, have been proposed for metal artifact reduction (MAR). However, these methods have limited perception ability in the diverse morphologies of different metal implants with artifacts, which may generate spurious anatomical structures and exhibit inferior generalization capability. To address the issues, we leverage visual-language model (VLM) to identify these morphological features and introduce them into a dual-domain CLIP-assisted residual optimization perception model (DuDoCROP) for MAR. Specifically, a dual-domain CLIP (DuDoCLIP) is fine-tuned on the image domain and sinogram domain using contrastive learning to extract semantic descriptions from anatomical structures and metal artifacts. Subsequently, a diffusion model is guided by the embeddings of DuDoCLIP, thereby enabling the dual-domain prior generation. Additionally, we design prompt engineering for more precise image-text descriptions that can enhance the model's perception capability. Then, a downstream task is devised for the one-step residual optimization and integration of dual-domain priors, while incorporating raw data fidelity. Ultimately, a new perceptual indicator is proposed to validate the model's perception and generation performance. With the assistance of DuDoCLIP, our DuDoCROP exhibits at least 63.7% higher generalization capability compared to the baseline model. Numerical experiments demonstrate that the proposed method can generate more realistic image structures and outperform other SOTA approaches both qualitatively and quantitatively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14342v1</guid>
      <category>cs.CV</category>
      <category>physics.med-ph</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinrui Zhang, Ailong Cai, Lei Li, Bin Yan</dc:creator>
    </item>
    <item>
      <title>Exploring the redundancy of Radon transform using a set of partial derivative equations: Could we precisely reconstruct the image from a sparse-view projection without any image prior?</title>
      <link>https://arxiv.org/abs/2405.19200</link>
      <description>arXiv:2405.19200v2 Announce Type: replace 
Abstract: In this study, we proposed a universal n-th order partial differential equation (PDE) of 2-D Radon transform to disclose the relationship of Radon transform over a neighborhood of the integral line, named as local correlation equation (LCE). It is independent to the imaging object while in present CT theory, the relationship of Radon transform over neighboring integral line had been described depended on the imaging objection. Hence, the LCE is the first PDE to reveal the universal correlation property of Radon transform. The LCE can be applied to either of 2D CT projections or any 2-D profile of 3-D CT projections. The correlation also provides the redundancy property of Radon transform. In this regard, we carried out a preliminary study on sparse-view CT reconstruction by using a discrete first order LCE to interpolate missing projections in sparse-view sampling without knowing image prior. Meanwhile, we also proposed a unified reconstruction framework that combines a regularized iterative reconstruction with the LCE based interpolation method to handle the sparse-view CT problem with higher sparsity level. The conducted experiments have credibly validated the proposed LCE, projection interpolation method, and the unified reconstruction scheme. The result of this study suggests an attractive possibility that a sparse-view projection may contain enough information of the complete projection, by which projection completeness in CT scanning may not be necessity. This possibility would bring profound changes in CT geometry designs and reconstruction algorithms. Moreover, this study initiates an appealing research topic of exploring the redundancy property of Radon transform and investigating new CT theories based on the redundancy property, which will boost the further development of CT reconstructions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19200v2</guid>
      <category>physics.med-ph</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuanqin Mou, Jiayu Duan</dc:creator>
    </item>
    <item>
      <title>Cross-Age and Cross-Site Domain Shift Impacts on Deep Learning-Based White Matter Fiber Estimation in Newborn and Baby Brains</title>
      <link>https://arxiv.org/abs/2312.14773</link>
      <description>arXiv:2312.14773v2 Announce Type: replace-cross 
Abstract: Deep learning models have shown great promise in estimating tissue microstructure from limited diffusion magnetic resonance imaging data. However, these models face domain shift challenges when test and train data are from different scanners and protocols, or when the models are applied to data with inherent variations such as the developing brains of infants and children scanned at various ages. Several techniques have been proposed to address some of these challenges, such as data harmonization or domain adaptation in the adult brain. However, those techniques remain unexplored for the estimation of fiber orientation distribution functions in the rapidly developing brains of infants. In this work, we extensively investigate the age effect and domain shift within and across two different cohorts of 201 newborns and 165 babies using the Method of Moments and fine-tuning strategies. Our results show that reduced variations in the microstructural development of babies in comparison to newborns directly impact the deep learning models' cross-age performance. We also demonstrate that a small number of target domain samples can significantly mitigate domain shift problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14773v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>physics.med-ph</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ISBI56570.2024.10635347</arxiv:DOI>
      <dc:creator>Rizhong Lin, Ali Gholipour, Jean-Philippe Thiran, Davood Karimi, Hamza Kebiri, Meritxell Bach Cuadra</dc:creator>
    </item>
  </channel>
</rss>
