<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 11 Nov 2025 11:21:45 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Tensor Algebra Toolkit for Folded Mixture Models: Symmetry-Aware Moments, Orbit-Space Estimation, and Poly-LAN Rates</title>
      <link>https://arxiv.org/abs/2511.05608</link>
      <description>arXiv:2511.05608v1 Announce Type: new 
Abstract: We develop a symmetry-aware toolkit for finite mixtures whose components are only identifiable up to a finite \emph{folding} group action. The correct estimand is the multiset of parameter orbits in the quotient space, not an ordered list of raw parameters. We design invariant tensor summaries via the Reynolds projector, show that mixtures become convex combinations in a low-dimensional invariant feature space, and prove identifiability, stability, and asymptotic normality \emph{on the quotient}. Our loss is a Hausdorff distance on orbit multisets; we prove it coincides with a bottleneck assignment metric and is thus computable in polynomial time. We give finite-sample Hausdorff bounds, a two-step efficient GMM formulation, consistent selection of the number of components, robustness to contamination, and minimax lower bounds that certify Poly-LAN rates $n^{-1/D}$ when the first nonzero invariant curvature appears at order $D$. The framework is illustrated for the hyperoctahedral group (signed permutations) and dihedral symmetries in the plane.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05608v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Koustav Mallik</dc:creator>
    </item>
    <item>
      <title>SAT-sampling for statistical significance testing in sparse contingency tables</title>
      <link>https://arxiv.org/abs/2511.05709</link>
      <description>arXiv:2511.05709v1 Announce Type: new 
Abstract: Exact conditional tests for contingency tables require sampling from fibers with fixed margins. Classical Markov basis MCMC is general but often impractical: computing full Markov bases that connect all fibers of a given constraint matrix can be infeasible and the resulting chains may converge slowly, especially in sparse settings or in presence of structural zeros. We introduce a SAT-based alternative that encodes fibers as Boolean circuits which allows modern SAT samplers to generate tables randomly. We analyze the sampling bias that SAT samplers may introduce, provide diagnostics, and propose practical mitigation. We propose hybrid MCMC schemes that combine SAT proposals with local moves to ensure correct stationary distributions which do not necessarily require connectivity via local moves which is particularly beneficial in presence of structural zeros. Across benchmarks, including small and involved tables with many structural zeros where pure Markov-basis methods underperform, our methods deliver reliable conditional p-values and often outperform samplers that rely on precomputed Markov bases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05709v1</guid>
      <category>stat.ME</category>
      <category>math.CO</category>
      <category>stat.CO</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patrick Scharpfenecker, Tobias Windisch</dc:creator>
    </item>
    <item>
      <title>Nonparametric Block Bootstrap Kolmogorov-Smirnov Goodness-of-Fit Test</title>
      <link>https://arxiv.org/abs/2511.05733</link>
      <description>arXiv:2511.05733v1 Announce Type: new 
Abstract: The Kolmogorov--Smirnov (KS) test is a widely used statistical test that assesses the conformity of a sample to a specified distribution. Its efficacy, however, diminishes with serially dependent data and when parameters within the hypothesized distribution are unknown. For independent data, parametric and nonparametric bootstrap procedures are available to adjust for estimated parameters. For serially dependent stationary data, parametric bootstrap has been developed with a working serial dependence structure. A counterpart for the nonparametric bootstrap approach, which needs a bias correction, has not been studied. Addressing this gap, our study introduces a bias correction method employing a nonparametric block bootstrap, which approximates the distribution of the KS statistic in assessing the goodness-of-fit of the marginal distribution of a stationary series, accounting for unspecified serial dependence and unspecified parameters. We assess its effectiveness through simulations, scrutinizing both its size and power. The practicality of our method is further illustrated with an examination of stock returns from the S\&amp;P 500 index, showcasing its utility in real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05733v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mathew Chandy, Elizabeth Schifano, Jun Yan, Xianyang Zhang</dc:creator>
    </item>
    <item>
      <title>Conformalized Bayesian Inference, with Applications to Random Partition Models</title>
      <link>https://arxiv.org/abs/2511.05746</link>
      <description>arXiv:2511.05746v1 Announce Type: new 
Abstract: Bayesian posterior distributions naturally represent parameter uncertainty informed by data. However, when the parameter space is complex, as in many nonparametric settings where it is infinite dimensional or combinatorially large, standard summaries such as posterior means, credible intervals, or simple notions of multimodality are often unavailable, hindering interpretable posterior uncertainty quantification. We introduce Conformalized Bayesian Inference (CBI), a broadly applicable and computationally efficient framework for posterior inference on nonstandard parameter spaces. CBI yields a point estimate, a credible region with assumption-free posterior coverage guarantees, and a principled analysis of posterior multimodality, requiring only Monte Carlo samples from the posterior and a notion of discrepancy between parameters. The method builds a discrepancy-based kernel density score for each parameter value, yielding a maximum-a-posteriori-like point estimate and a credible region derived from conformal prediction principles. The key conceptual step underlying this construction is the reinterpretation of posterior inference as prediction on the parameter space. A final density-based clustering step identifies representative posterior modes. We investigate a number of theoretical and methodological properties of CBI and demonstrate its practicality, scalability, and versatility in simulated and real data clustering applications with Bayesian random partition models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05746v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicola Bariletto, Nhat Ho, Alessandro Rinaldo</dc:creator>
    </item>
    <item>
      <title>Bounding interventional queries from generalized incomplete contingency tables</title>
      <link>https://arxiv.org/abs/2511.05755</link>
      <description>arXiv:2511.05755v1 Announce Type: new 
Abstract: We introduce a method for evaluating interventional queries and Average Treatment Effects (ATEs) in the presence of generalized incomplete contingency tables (GICTs), contingency tables containing a full row of random (sampling) zeros, rendering some conditional probabilities undefined. Rather than discarding such entries or imputing missing values, we model the unknown probabilities as free parameters and derive symbolic expressions for the queries that incorporate them. By extremizing these expressions over all values consistent with basic probability constraints and the support of all variables, we obtain sharp bounds for the query of interest under weak assumptions of small missing frequencies. These bounds provide a formal quantification of the uncertainty induced by the generalized incompleteness of the contingency table and ensure that the true value of the query will always lie within them. The framework applies independently of the missingness mechanism and offers a conservative yet rigorous approach to causal inference under random data gaps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05755v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ivano Lodato, Aditya V. Iyer, Isaac Z. To</dc:creator>
    </item>
    <item>
      <title>Standard and comparative e-backtests for general risk measures</title>
      <link>https://arxiv.org/abs/2511.05840</link>
      <description>arXiv:2511.05840v1 Announce Type: new 
Abstract: Backtesting risk measures is a unique and important problem for financial regulators to evaluate risk forecasts reported by financial institutions. As a natural extension to standard (or traditional) backtests, comparative backtests are introduced to evaluate different forecasts against regulatory standard models. Based on recently developed concepts of e-values and e-processes, we focus on how standard and comparative backtests can be manipulated in financial regulation by constructing e-processes. We design a model-free (non-parametric) method for standard backtests of identifiable risk measures and comparative backtests of elicitable risk measures. Our e-backtests are applicable to a wide range of common risk measures including the mean, the variance, the Value-at-Risk, the Expected Shortfall, and the expectile. Our results are illustrated by ample simulation studies and real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05840v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zhanyi Jiao, Qiuqi Wang, Yimiao Zhao</dc:creator>
    </item>
    <item>
      <title>Scalable and Distributed Individualized Treatment Rules for Massive Datasets</title>
      <link>https://arxiv.org/abs/2511.05842</link>
      <description>arXiv:2511.05842v1 Announce Type: new 
Abstract: Synthesizing information from multiple data sources is crucial for constructing accurate individualized treatment rules (ITRs). However, privacy concerns often present significant barriers to the integrative analysis of such multi-source data. Classical meta-learning, which averages local estimates to derive the final ITR, is frequently suboptimal due to biases in these local estimates. To address these challenges, we propose a convolution-smoothed weighted support vector machine for learning the optimal ITR. The accompanying loss function is both convex and smooth, which allows us to develop an efficient multi-round distributed learning procedure for ITRs. Such distributed learning ensures optimal statistical performance with a fixed number of communication rounds, thereby minimizing coordination costs across data centers while preserving data privacy. Our method avoids pooling subject-level raw data and instead requires only sharing summary statistics. Additionally, we develop an efficient coordinate gradient descent algorithm, which guarantees at least linear convergence for the resulting optimization problem. Extensive simulations and an application to sepsis treatment across multiple intensive care units validate the effectiveness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05842v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nan Qiao, Wangcheng Li, Jingxiao Zhang, Canyi Chen</dc:creator>
    </item>
    <item>
      <title>Identification of Emotionally Stressful Periods Through Tracking Changes in Statistical Features of mHealth Data</title>
      <link>https://arxiv.org/abs/2511.05887</link>
      <description>arXiv:2511.05887v1 Announce Type: new 
Abstract: Identifying the onset of emotional stress in older patients with mood disorders and chronic pain is crucial in mental health studies. To this end, studying the associations between passively sensed variables that measure human behaviors and self-reported stress levels collected from mobile devices is emerging. Existing algorithms rely on conventional change point detection (CPD) methods due to the nonstationary nature of the data. They also require explicit modeling of the associations between variables and output only discrete time points, which can lead to misinterpretation of stress onset timings. This is problematic when distributional shifts are complex, dependencies between variables are difficult to capture, and changes occur asynchronously across series with weak signals. In this study, we propose an algorithm that detects hotspots, defined as collections of time intervals during which statistical features of passive sensing variables and stress indicators shift, highlighting periods that require investigation. We first extend the moving sum (MOSUM) scheme to detect simultaneous changes both within and across series, and then define hotspots in two ways: using distance-based test statistics and confidence intervals. The proposed method tracks local changes in combined distributional features, enabling it to capture all types of simultaneous and asynchronous change. It does not require a specific functional relationship between series, and the results are expressed as intervals rather than as individual time points. We conduct simulations under varying signal strengths with mixed and asynchronous distributional shifts, where the proposed method outperforms benchmarks. Results on hotspot identification indicate that the two definitions are complementary. We further apply our method to ALACRITY Phase I data, analyzing hotspots from patients' stress levels and activity measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05887v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Younghoon Kim, Sumanta Basu, Samprit Banerjee</dc:creator>
    </item>
    <item>
      <title>Estimating Treatment Effects with Missings Not At Random in the Estimand Framework using Causal Inference</title>
      <link>https://arxiv.org/abs/2511.05939</link>
      <description>arXiv:2511.05939v1 Announce Type: new 
Abstract: The analysis of randomized trials is often complicated by the occurrence of intercurrent events and missing values. Even though there are different strategies to address missing values it is still common to require missing values imputation. In the present article we explore the estimation of treatment effects in RCTs from a causal inference perspective under different missing data mechanisms with a particular emphasis on missings not at random (MNAR). By modelling the missingness process with directed acylcic graphs and patient-specific potential response variables, we present a new approach to obtain an unbiased estimation of treatment effects without needing to impute missing values. Additionally, we provide a formal that the average conditional log-odds ratio is a robust measure even under MNAR missing values if adjusted by sufficient confounders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05939v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>A. Ruiz de Villa, Ll. Badiella</dc:creator>
    </item>
    <item>
      <title>Minimum bounding polytropes for estimation of max-linear Bayesian networks</title>
      <link>https://arxiv.org/abs/2511.05962</link>
      <description>arXiv:2511.05962v1 Announce Type: new 
Abstract: Max-linear Bayesian networks are recursive max-linear structural equation models represented by an edge weighted directed acyclic graph (DAG). The identifiability and estimation of max-linear Bayesian networks is an intricate issue as Gissibl, Kl\"uppelberg, and Lauritzen have shown. As such, a max-linear Bayesian network is generally unidentifiable and standard likelihood theory cannot be applied. We can associate tropical polyhedra to max-linear Bayesian networks. Using this, we investigate the minimum-ratio estimator proposed by Gissibl, Kl\"uppelberg, and Lauritzen and give insight on the structure of minimal best-case samples for parameter recovery which we describe in terms of set covers of certain triangulations. We also combine previous work on estimating max-linear models from Tran, Buck, and Kl\"uppelberg to apply our geometric approach to the structural inference of max-linear models. This is tested extensively on simulated data and on real world data set, the NHANES report for 2015--2016 and the upper Danube network data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05962v1</guid>
      <category>stat.ME</category>
      <category>math.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kamillo Ferry</dc:creator>
    </item>
    <item>
      <title>A Riemannian Framework for Linear and Quadratic Discriminant Analysis on the Tangent Space of Shapes</title>
      <link>https://arxiv.org/abs/2511.06027</link>
      <description>arXiv:2511.06027v1 Announce Type: new 
Abstract: We present a Riemannian framework for linear and quadratic discriminant classification on the tangent plane of the shape space of curves. The shape space is infinite dimensional and is constructed out of square root velocity functions of curves. We introduce the idea of mean and covariance of shape-valued random variables and samples from a tangent space to the pre-shape space (invariant to translation and scaling) and then extend it to the full shape space (rotational invariance). The shape observations from the population are approximated by coefficients of a Fourier basis of the tangent space. The algorithms for linear and quadratic discriminant analysis are then defined using reduced dimensional features obtained by projecting the original shape observations on to the truncated Fourier basis. We show classification results on synthetic data and shapes of cortical sulci, corpus callosum curves, as well as facial midline curve profiles from patients with fetal alcohol syndrome (FAS).</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06027v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Susovan Pal, Roger P. Woods, Suchit Panjiyar, Elizabeth Sowell, Katherine L. Narr, Shantanu H. Joshi</dc:creator>
    </item>
    <item>
      <title>Unifiedly Efficient Inference on All-Dimensional Targets for Large-Scale GLMs</title>
      <link>https://arxiv.org/abs/2511.06070</link>
      <description>arXiv:2511.06070v1 Announce Type: new 
Abstract: The scalability of Generalized Linear Models (GLMs) for large-scale, high-dimensional data often forces a trade-off between computational feasibility and statistical accuracy, particularly for inference on pre-specified parameters. While subsampling methods mitigate computational costs, existing estimators are typically constrained by a suboptimal $r^{-1/2}$ convergence rate, where $r$ is the subsample size. This paper introduces a unified framework that systematically breaks this barrier, enabling efficient and precise inference regardless of the dimension of the target parameters. To overcome the accuracy loss and enhance computational efficiency, we propose three estimators tailored to different scenarios. For low-dimensional targets, we propose a de-variance subsampling (DVS) estimator that achieves a sharply improved convergence rate of $\max\{r^{-1}, n^{-1/2}\}$, permitting valid inference even with very small subsamples. As $r$ grows, a multi-step refinement of our estimator is proven to be asymptotically normal and semiparametric efficient when $r/\sqrt{n} \to \infty$, matching the performance of the full-sample estimator-a property confirmed by its Bahadur representation. Critically, we provide an improved principle to high-dimensional targets, developing a novel decorrelated score function that facilitates simultaneous inference for a diverging number of pre-specified parameters. Comprehensive numerical experiments demonstrate that our framework delivers a superior balance of computational efficiency and statistical accuracy across both low- and high-dimensional inferential tasks in large-scale GLM, thereby realizing the promise of unifiedly efficient inference for large-scale GLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06070v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bo Fu, Dandan Jiang</dc:creator>
    </item>
    <item>
      <title>Counterfactual Forecasting For Panel Data</title>
      <link>https://arxiv.org/abs/2511.06189</link>
      <description>arXiv:2511.06189v1 Announce Type: new 
Abstract: We address the challenge of forecasting counterfactual outcomes in a panel data with missing entries and temporally dependent latent factors -- a common scenario in causal inference, where estimating unobserved potential outcomes ahead of time is essential. We propose Forecasting Counterfactuals under Stochastic Dynamics (FOCUS), a method that extends traditional matrix completion methods by leveraging time series dynamics of the factors, thereby enhancing the prediction accuracy of future counterfactuals. Building upon a PCA estimator, our method accommodates both stochastic and deterministic components within the factors, and provides a flexible framework for various applications. In case of stationary autoregressive factors and under standard conditions, we derive error bounds and establish asymptotic normality of our estimator. Empirical evaluations demonstrate that our method outperforms existing benchmarks when the latent factors have an autoregressive component. We illustrate FOCUS results on HeartSteps, a mobile health study, illustrating its effectiveness in forecasting step counts for users receiving activity prompts, thereby leveraging temporal patterns in user behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06189v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Navonil Deb, Raaz Dwivedi, Sumanta Basu</dc:creator>
    </item>
    <item>
      <title>A sensitivity analysis for the average derivative effect</title>
      <link>https://arxiv.org/abs/2511.06243</link>
      <description>arXiv:2511.06243v1 Announce Type: new 
Abstract: In observational studies, exposures are often continuous rather than binary or discrete. At the same time, sensitivity analysis is an important tool that can help determine the robustness of a causal conclusion to a certain level of unmeasured confounding, which can never be ruled out in an observational study. Sensitivity analysis approaches for continuous exposures have now been proposed for several causal estimands. In this article, we focus on the average derivative effect (ADE). We obtain closed-form bounds for the ADE under a sensitivity model that constrains the odds ratio (at any two dose levels) between the latent and observed generalized propensity score. We propose flexible, efficient estimators for the bounds, as well as point-wise and simultaneous (over the sensitivity parameter) confidence intervals. We examine the finite sample performance of the methods through simulations and illustrate the methods on a study assessing the effect of parental income on educational attainment and a study assessing the price elasticity of petrol.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06243v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeffrey Zhang</dc:creator>
    </item>
    <item>
      <title>Breaking the Winner's Curse with Bayesian Hybrid Shrinkage</title>
      <link>https://arxiv.org/abs/2511.06318</link>
      <description>arXiv:2511.06318v1 Announce Type: new 
Abstract: A 'Winner's Curse' arises in large-scale online experimentation platforms when the same experiments are used to both select treatments and evaluate their effects. In these settings, classical difference-in-means estimators of treatment effects are upwardly biased and conventional confidence intervals are rendered invalid. The bias scales with the magnitude of sampling variability and the selection threshold, and inversely with the treatment's true effect size. We propose a new Bayesian approach that incorporates experiment-specific 'local shrinkage' factors that mitigate sensitivity to the choice of prior and improve robustness to assumption violations. We demonstrate how the associated posterior distribution can be estimated without numerical integration techniques, making it a practical choice for at-scale deployment. Through simulation, we evaluate the performance of our approach under various scenarios and find that it performs well even when assumptions about the sampling and selection processes are violated. In an empirical evaluation, our approach demonstrated superior performance over alternative methods, providing more accurate estimates with well-calibrated uncertainty quantification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06318v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Richard Mudd, Rina Friedberg, Ilya Gorbachev, Houssam Nassif, Abbas Zaidi</dc:creator>
    </item>
    <item>
      <title>Gaussian Graphical Models for Partially Observed Multivariate Functional Data</title>
      <link>https://arxiv.org/abs/2511.06445</link>
      <description>arXiv:2511.06445v1 Announce Type: new 
Abstract: In many applications, the variables that characterize a stochastic system are measured along a second dimension, such as time. This results in multivariate functional data and the interest is in describing the statistical dependences among these variables. It is often the case that the functional data are only partially observed. This creates additional challenges to statistical inference, since the functional principal component scores, which capture all the information from these data, cannot be computed. Under an assumption of Gaussianity and of partial separability of the covariance operator, we develop an EM-type algorithm for penalized inference of a functional graphical model from multivariate functional data which are only partially observed. A simulation study and an illustration on German electricity market data show the potential of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06445v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marco Borriero, Luigi Augugliaro, Gianluca Sottile, Veronica Vinciotti</dc:creator>
    </item>
    <item>
      <title>Confidence Intervals Based on the Modified Chi-Squared Distribution and its Applications in Medicine</title>
      <link>https://arxiv.org/abs/2511.06476</link>
      <description>arXiv:2511.06476v1 Announce Type: new 
Abstract: Small sample sizes in clinical studies arises from factors such as reduced costs, limited subject availability, and the rarity of studied conditions. This creates challenges for accurately calculating confidence intervals (CIs) using the normal distribution approximation. In this paper, we employ a quadratic-form based statistic, from which we derive more accurate confidence intervals, particularly for data with small sample sizes or proportions. Based on the study, we suggest reasonable values of sample sizes and proportions for the application of the quadratic method. Consequently, this method enhances the reliability of statistical inferences. We illustrate this method with real medical data from clinical trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06476v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mulan Wu, Mengyu Xu, Dongyun Kim</dc:creator>
    </item>
    <item>
      <title>Collapsing Categories for Regression with Mixed Predictors</title>
      <link>https://arxiv.org/abs/2511.06542</link>
      <description>arXiv:2511.06542v1 Announce Type: new 
Abstract: Categorical predictors are omnipresent in everyday regression practice: in fact, most regression data involve some categorical predictors, and this tendency is increasing in modern applications with more complex structures and larger data sizes. However, including too many categories in a regression model would seriously hamper accuracy, as the information in the data is fragmented by the multitude of categories. In this paper, we introduce a systematic method to reduce the complexity of categorical predictors by adaptively collapsing categories in regressions, so as to enhance the performance of regression estimation. Our method is based on the {\em pairwise vector fused LASSO}, which automatically fuses the categories that bear a similar regression relation with the response. We develop our method under a wide class of regression models defined by a general loss function, which includes linear models and generalized linear models as special cases. We rigorously established the category collapsing consistency of our method, developed an Inexact Proximal Gradient Descent algorithm to implement it, and proved the feasibility and convergence of our algorithm. Through simulations and an application to Spotify music data, we demonstrate that our method can effectively reduce categorical complexity while improving prediction performance, making it a powerful tool for regression with mixed predictors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06542v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chaegeun Song, Zhong Zheng, Bing Li, Lingzhou Xue</dc:creator>
    </item>
    <item>
      <title>A Simple and Effective Random Forest Modelling for Nonlinear Time Series Data</title>
      <link>https://arxiv.org/abs/2511.06544</link>
      <description>arXiv:2511.06544v1 Announce Type: new 
Abstract: In this paper, we propose Random Forests by Random Weights (RF-RW), a theoretically grounded and practically effective alternative RF modelling for nonlinear time series data, where existing RF-based approaches struggle to adequately capture temporal dependence. RF-RW reconciles the strengths of classic RF with the temporal dependence inherent in time series forecasting. Specifically, it avoids the bootstrap resampling procedure, therefore preserves the serial dependence structure, whilst incorporates independent random weights to reduce correlations among trees. We establish non-asymptotic concentration bounds and asymptotic uniform consistency guarantees, for both fixed- and high-dimensional feature spaces, which extend beyond existing theoretical analyses of RF. Extensive simulation studies demonstrate that RF-RW outperforms existing RF-based approaches and other benchmarks such as SVM and LSTM. It also achieves the lowest error among competitors in our real-data example of predicting UK COVID-19 daily cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06544v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shihao Zhang, Zudi Lu, Chao Zheng</dc:creator>
    </item>
    <item>
      <title>Causal Inference for Network Data with Endogenous Peer Effect: A Targeted Minimum Loss Estimation Approach</title>
      <link>https://arxiv.org/abs/2511.06652</link>
      <description>arXiv:2511.06652v1 Announce Type: new 
Abstract: We study estimation of the average treatment effect (ATE) from a single network in observational settings with interference. The weak cross-unit dependence is modeled via an endogenous peer-effect (spatial autoregressive) term that induces distance-decaying spillover effects, relaxing the common finite-order interference assumption. We propose a targeted minimum loss estimation (TMLE) procedure that removes plug-in bias from an initial estimator. The targeting step yields an adjustment direction that incorporates the network autoregressive structure and assigns heterogeneous, network-dependent weights to units. We find that the asymptotic leading term related to the covariates $\mathbf{X}_i$ can be formulated into a $V$-statistic whose order diverges with the network degrees. A novel limit theory is developed to establish the asymptotic normality under such complex network dependent scenarios. We show that our method can achieve smaller asymptotic variance than existing methods when $\mathbf{X}_i$ is i.i.d. generated and estimated with empirical distribution, and provide theoretical guarantee for estimating the variance. Extensive numerical studies and a live-streaming data analysis are presented to illustrate the advantages of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06652v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yong Wu, Shuyuan Wu, Xinwei Sun, Xuening Zhu</dc:creator>
    </item>
    <item>
      <title>Integral-Operator-Based Spectral Algorithms for Goodness-of-Fit Tests</title>
      <link>https://arxiv.org/abs/2511.06718</link>
      <description>arXiv:2511.06718v1 Announce Type: new 
Abstract: The widespread adoption of the \emph{maximum mean discrepancy} (MMD) in goodness-of-fit testing has spurred extensive research on its statistical performance. However, recent studies indicate that the inherent structure of MMD may constrain its ability to distinguish between distributions, leaving room for improvement. Regularization techniques have the potential to overcome this limitation by refining the discrepancy measure. In this paper, we introduce a family of regularized kernel-based discrepancy measures constructed via spectral filtering. Our framework can be regarded as a natural generalization of prior studies, removing restrictive assumptions on both kernel functions and filter functions, thereby broadening the methodological scope and the theoretical inclusiveness. We establish non-asymptotic guarantees showing that the resulting tests achieve valid Type~I error control and enhanced power performance. Numerical experiments are conducted to demonstrate the broader generality and competitive performance of the proposed tests compared with existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06718v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shiwei Sang, Shao-Bo Lin, Xuehu Zhu</dc:creator>
    </item>
    <item>
      <title>Design and Analysis Considerations for Causal Inference under Two-Phase Sampling in Observational Studies</title>
      <link>https://arxiv.org/abs/2511.06808</link>
      <description>arXiv:2511.06808v1 Announce Type: new 
Abstract: Two-phase sampling is a simple and cost-effective estimation strategy in survey sampling and is widely used in practice. Because the phase-2 sampling probability typically depends on low-cost variables collected at phase 1, naive estimation based solely on the phase-2 sample generally results in biased inference. This issue arises even when estimating causal parameters such as the average treatment effect (ATE), and there has been growing interest in recent years in the proper estimation of such parameters under complex sampling designs (e.g., Nattino et al., 2025).
  In this paper, we derive the semiparametric efficiency bound for a broad class of weighted average treatment effects (WATE), which includes the ATE, the average treatment effect on the treated (ATT), and the average treatment effect on the overlapped population (ATO), under two-phase sampling. In addition to straightforward weighting estimators based on the sampling probabilities, we also propose estimators that can attain strictly higher efficiency under suitable conditions. In particular, under outcome-dependent sampling, we show that substantial efficiency gains can be achieved by appropriately incorporating phase-1 information. We further conduct extensive simulation studies, varying the choice of phase-1 variables and sampling schemes, to characterize when and to what extent leveraging phase-1 information leads to efficiency gains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06808v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kazuharu Harada, Masataka Taguri</dc:creator>
    </item>
    <item>
      <title>Bayesian design and analysis of two-arm cluster randomised trials using assurance: extension to binary outcomes and comparison of MCMC and INLA</title>
      <link>https://arxiv.org/abs/2511.06912</link>
      <description>arXiv:2511.06912v1 Announce Type: new 
Abstract: The paper considers two different designs; a two-arm superiority cluster randomised controlled trial (RCT) with a continuous outcome, and a twoarm superiority cluster RCT with a binary outcome. From a Bayesian perspective, for the analysis of the trial we use a (generalised) linear mixed effects model. We summarise the inference for the treatment effect for a cluster RCT based on the posterior distribution. Based on this inference we use assurance to choose the sample size. We consider and compare two different methods for the inference: Markov Chain Monte Carlo (MCMC) and Integrated Nested Laplace Approximations (INLA), and consider their implications for the assurance. We consider the Specialist Pre-hospital redirection for ischemic stroke thrombectomy (SPEEDY) trial, an RCT which has co-primary outcomes of thrombectomy rate and time to thrombectomy, as a case study for the developed Bayesian RCT designs. We demonstrate our novel approach to the sample size calculation using assurance on the SPEEDY trial, based on the results of a formal prior elicitation exercise with two clinical experts. The paper considers a range of different scenarios for cluster RCTs to evaluate INLA and MCMC, to determine when each inference scheme should be used, balancing the computational cost in terms of speed and accuracy. We make recommendations for when each should be used.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06912v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdullah Aloufi, Kevin Wilson, Nina Wilson, Lisa Shaw, Christopher Price</dc:creator>
    </item>
    <item>
      <title>Approximate Bayesian inference for cumulative probit regression models</title>
      <link>https://arxiv.org/abs/2511.06967</link>
      <description>arXiv:2511.06967v1 Announce Type: new 
Abstract: Ordinal categorical data are routinely encountered in a wide range of practical applications. When the primary goal is to construct a regression model for ordinal outcomes, cumulative link models represent one of the most popular choices to link the cumulative probabilities of the response with a set of covariates through a parsimonious linear predictor, shared across response categories. When the number of observations grows, standard sampling algorithms for Bayesian inference scale poorly, making posterior computation increasingly challenging in large datasets. In this article, we propose three scalable algorithms for approximating the posterior distribution of the regression coefficients in cumulative probit models relying on Variational Bayes and Expectation Propagation. We compare the proposed approaches with inference based on Markov Chain Monte Carlo, demonstrating superior computational performance and remarkable accuracy; finally, we illustrate the utility of the proposed algorithms on a challenging case study to investigate the structure of a criminal network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06967v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emanuele Aliverti</dc:creator>
    </item>
    <item>
      <title>Applying the Polynomial Maximization Method to Estimate ARIMA Models with Asymmetric Non-Gaussian Innovations</title>
      <link>https://arxiv.org/abs/2511.07059</link>
      <description>arXiv:2511.07059v1 Announce Type: new 
Abstract: Classical estimators for ARIMA parameters (MLE, CSS, OLS) assume Gaussian innovations, an assumption frequently violated in financial and economic data exhibiting asymmetric distributions with heavy tails. We develop and validate the second-order polynomial maximization method (PMM2) for estimating ARIMA$(p,d,q)$ models with non-Gaussian innovations. PMM2 is a semiparametric technique that exploits higher-order moments and cumulants without requiring full distributional specification.
  Monte Carlo experiments (128,000 simulations) across sample sizes $N \in \{100, 200, 500, 1000\}$ and four innovation distributions demonstrate that PMM2 substantially outperforms classical methods for asymmetric innovations. For ARIMA(1,1,0) with $N=500$, relative efficiency reaches 1.58--1.90 for Gamma, lognormal, and $\chi^2(3)$ innovations (37--47\% variance reduction). Under Gaussian innovations PMM2 matches OLS efficiency, avoiding the precision loss typical of robust estimators.
  The method delivers major gains for moderate asymmetry ($|\gamma_3| \geq 0.5$) and $N \geq 200$, with computational costs comparable to MLE. PMM2 provides an effective alternative for time series with asymmetric innovations typical of financial markets, macroeconomic indicators, and industrial measurements. Future extensions include seasonal SARIMA models, GARCH integration, and automatic order selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07059v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Serhii Zabolotnii</dc:creator>
    </item>
    <item>
      <title>A general approach to construct powerful tests for intersections of one-sided null-hypotheses based on influence functions</title>
      <link>https://arxiv.org/abs/2511.07096</link>
      <description>arXiv:2511.07096v1 Announce Type: new 
Abstract: Testing intersections of null-hypotheses is an integral part of closed testing procedures for assessing multiple null-hypotheses under family-wise type 1 error control. Popular intersection tests such as the minimum p-value test are based on marginal p-values and are typically evaluated conservatively by disregarding simultaneous behavior of the marginal p-values. We consider a general purpose Wald type test for testing intersections of one-sided null-hypotheses. The test is constructed on the basis of the simultaneous asymptotic behavior of the p values. The simultaneous asymptotic behavior is derived via influence functions of estimators using the so-called stacking approach. In particular, this approach does not require added assumptions on simultaneous behavior to be valid. The resulting test is shown to have attractive power properties and thus forms the basis of a powerful closed testing procedure for testing multiple one-sided hypotheses under family-wise type 1 error control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07096v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christian Bressen Pipper, Andreas Nordland, Klaus K\"ahler Holst</dc:creator>
    </item>
    <item>
      <title>Registration-Free Monitoring of Unstructured Point Cloud Data via Intrinsic Geometrical Properties</title>
      <link>https://arxiv.org/abs/2511.05623</link>
      <description>arXiv:2511.05623v1 Announce Type: cross 
Abstract: Modern sensing technologies have enabled the collection of unstructured point cloud data (PCD) of varying sizes, which are used to monitor the geometric accuracy of 3D objects. PCD are widely applied in advanced manufacturing processes, including additive, subtractive, and hybrid manufacturing. To ensure the consistency of analysis and avoid false alarms, preprocessing steps such as registration and mesh reconstruction are commonly applied prior to monitoring. However, these steps are error-prone, time-consuming and may introduce artifacts, potentially affecting monitoring outcomes. In this paper, we present a novel registration-free approach for monitoring PCD of complex shapes, eliminating the need for both registration and mesh reconstruction. Our proposal consists of two alternative feature learning methods and a common monitoring scheme. Feature learning methods leverage intrinsic geometric properties of the shape, captured via the Laplacian and geodesic distances. In the monitoring scheme, thresholding techniques are used to further select intrinsic features most indicative of potential out-of-control conditions. Numerical experiments and case studies highlight the effectiveness of the proposed approach in identifying different types of defects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05623v1</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mariafrancesca Patalano, Giovanna Capizzi, Kamran Paynabar</dc:creator>
    </item>
    <item>
      <title>Topologically Invariant Permutation Test</title>
      <link>https://arxiv.org/abs/2511.06153</link>
      <description>arXiv:2511.06153v1 Announce Type: cross 
Abstract: Functional brain networks exhibit topological structures that reflect neural organization; however, statistical comparison of these networks is challenging for several reasons. This paper introduces a topologically invariant permutation test for detecting topological inequivalence. Under topological equivalence, topological features can be permuted separately between groups without distorting individual network structures. The test statistic uses $2$-Wasserstein distances on persistent diagrams, computed in closed form. To reduce variability in brain connectivities while preserving topology, heat kernel expansion on the Hodge Laplacian is applied with bandwidth $t$ controlling diffusion intensity. Theoretical results guarantee variance reduction through optimal Hilbert space projection. Simulations across diverse network topologies show superior performance compared to conventional two-sample tests and alternative metrics. Applied to resting-state fMRI data from the Multimodal Treatment of ADHD study, the method detects significant topological differences between cannabis users and non-users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06153v1</guid>
      <category>q-bio.NC</category>
      <category>math.AT</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sixtus Dakurah</dc:creator>
    </item>
    <item>
      <title>A unified approach to spatial domain detection and cell-type deconvolution in spot-based spatial transcriptomics</title>
      <link>https://arxiv.org/abs/2511.06204</link>
      <description>arXiv:2511.06204v1 Announce Type: cross 
Abstract: Many popular technologies for generating spatially resolved transcriptomic (SRT) data measure gene expression at the resolution of a "spot", i.e., a small tissue region 55 microns in diameter. Each spot can contain many cells of different types. In typical analyses, researchers are interested in using these data to identify discrete spatial domains in the tissue. In this paper, we propose a new method, DUET, that simultaneously identifies discrete spatial domains and estimates each spot's cell-type proportion. This allows the identified spatial domains to be characterized in terms of the cell type proportions, which affords interpretability and biological insight. DUET utilizes a constrained version of model-based convex clustering, and as such, can accommodate Poisson, negative binomial, normal, and other types of expression data. Through simulation studies and multiple applications, we show that our method can achieve better clustering and deconvolution performance than existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06204v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hyun Jung Koo, Aaron J. Molstad</dc:creator>
    </item>
    <item>
      <title>Bayesian spatio--temporal disaggregation modeling using a diffusion-SPDE approach: a case study of Aerosol Optical Depth in India</title>
      <link>https://arxiv.org/abs/2511.06276</link>
      <description>arXiv:2511.06276v1 Announce Type: cross 
Abstract: Accurate estimation of Aerosol Optical Depth (AOD) is crucial for understanding climate change and its impacts on public health, as aerosols are a measure of air quality conditions. AOD is usually retrieved from satellite imagery at coarse spatial and temporal resolutions. However, producing high-resolution AOD estimates in both space and time can better support evidence-based policies and interventions. We propose a spatio-temporal disaggregation model that assumes a latent spatio--temporal continuous Gaussian process observed through aggregated measurements. The model links discrete observations to the continuous domain and accommodates covariates to improve explanatory power and interpretability. The approach employs Gaussian processes with separable or non-separable covariance structures derived from a diffusion-based spatio-temporal stochastic partial differential equation (SPDE). Bayesian inference is conducted using the INLA-SPDE framework for computational efficiency. Simulation studies and an application to nowcasting AOD at 550 nm in India demonstrate the model's effectiveness, improving spatial resolution from 0.75{\deg} to 0.25{\deg} and temporal resolution from 3 hours to 1 hour.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06276v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fernando Rodriguez Avellaneda, Paula Moraga</dc:creator>
    </item>
    <item>
      <title>Non-Negative Stiefel Approximating Flow: Orthogonalish Matrix Optimization for Interpretable Embeddings</title>
      <link>https://arxiv.org/abs/2511.06425</link>
      <description>arXiv:2511.06425v1 Announce Type: cross 
Abstract: Interpretable representation learning is a central challenge in modern machine learning, particularly in high-dimensional settings such as neuroimaging, genomics, and text analysis. Current methods often struggle to balance the competing demands of interpretability and model flexibility, limiting their effectiveness in extracting meaningful insights from complex data. We introduce Non-negative Stiefel Approximating Flow (NSA-Flow), a general-purpose matrix estimation framework that unifies ideas from sparse matrix factorization, orthogonalization, and constrained manifold learning. NSA-Flow enforces structured sparsity through a continuous balance between reconstruction fidelity and column-wise decorrelation, parameterized by a single tunable weight. The method operates as a smooth flow near the Stiefel manifold with proximal updates for non-negativity and adaptive gradient control, yielding representations that are simultaneously sparse, stable, and interpretable. Unlike classical regularization schemes, NSA-Flow provides an intuitive geometric mechanism for manipulating sparsity at the level of global structure while simplifying latent features. We demonstrate that the NSA-Flow objective can be optimized smoothly and integrates seamlessly with existing pipelines for dimensionality reduction while improving interpretability and generalization in both simulated and real biomedical data. Empirical validation on the Golub leukemia dataset and in Alzheimer's disease demonstrate that the NSA-Flow constraints can maintain or improve performance over related methods with little additional methodological effort. NSA-Flow offers a scalable, general-purpose tool for interpretable ML, applicable across data science domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06425v1</guid>
      <category>stat.ML</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Brian B. Avants (Department of Radiology,Medical Imaging University of Virginia, Charlottesville, VA), Nicholas J. Tustison (Department of Radiology,Medical Imaging University of Virginia, Charlottesville, VA), James R Stone (Department of Radiology,Medical Imaging University of Virginia, Charlottesville, VA)</dc:creator>
    </item>
    <item>
      <title>Boundary Discontinuity Designs: Theory and Practice</title>
      <link>https://arxiv.org/abs/2511.06474</link>
      <description>arXiv:2511.06474v1 Announce Type: cross 
Abstract: We review the literature on boundary discontinuity (BD) designs, a powerful non-experimental research methodology that identifies causal effects by exploiting a thresholding treatment assignment rule based on a bivariate score and a boundary curve. This methodology generalizes standard regression discontinuity designs based on a univariate score and scalar cutoff, and has specific challenges and features related to its multi-dimensional nature. We synthesize the empirical literature by systematically reviewing over $80$ empirical papers, tracing the method's application from its formative uses to its implementation in modern research. In addition to the empirical survey, we overview the latest methodological results on identification, estimation and inference for the analysis of BD designs, and offer recommendations for practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06474v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Rocio Titiunik, Ruiqi Rae Yu</dc:creator>
    </item>
    <item>
      <title>A Causal Formulation of Spike-Wave Duality</title>
      <link>https://arxiv.org/abs/2511.06602</link>
      <description>arXiv:2511.06602v1 Announce Type: cross 
Abstract: Understanding the relationship between brain activity and behavior is a central goal of neuroscience. Despite significant advances, a fundamental dichotomy persists: neural activity manifests as both discrete spikes of individual neurons and collective waves of populations. Both neural codes correlate with behavior, yet correlation alone cannot determine whether waves exert a causal influence or merely reflect spiking dynamics without causal efficacy. According to the Causal Hierarchy Theorem, no amount of observational data--however extensive--can settle this question; causal conclusions require explicit structural assumptions or careful experiment designs that directly correspond to the causal effect of interest. We develop a formal framework that makes this limitation precise and constructive. Formalizing epiphenomenality via the invariance of interventional distributions in Structural Causal Models (SCMs), we derive a certificate of sufficiency from Pearl's do-calculus that specifies when variables can be removed from the model without loss of causal explainability and clarifies how interventions should be interpreted under different causal structures of spike-wave duality. The purpose of this work is not to resolve the spike-wave debate, but to reformulate it. We shift the problem from asking which signal matters most to asking under what conditions any signal can be shown to matter at all. This reframing distinguishes prediction from explanation and offers neuroscience a principled route for deciding when waves belong to mechanism and when they constitute a byproduct of underlying coordination</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06602v1</guid>
      <category>q-bio.NC</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kasra Jalaldoust, Erfan Zabeh</dc:creator>
    </item>
    <item>
      <title>Smoothing Out Sticking Points: Sampling from Discrete-Continuous Mixtures with Dynamical Monte Carlo by Mapping Discrete Mass into a Latent Universe</title>
      <link>https://arxiv.org/abs/2511.07340</link>
      <description>arXiv:2511.07340v1 Announce Type: cross 
Abstract: Combining a continuous "slab" density with discrete "spike" mass at zero, spike-and-slab priors provide important tools for inducing sparsity and carrying out variable selection in Bayesian models. However, the presence of discrete mass makes posterior inference challenging. "Sticky" extensions to piecewise-deterministic Markov process samplers have shown promising performance, where sampling from the spike is achieved by the process sticking there for an exponentially distributed duration. As it turns out, the sampler remains valid when the exponential sticking time is replaced with its expectation. We justify this by mapping the spike to a continuous density over a latent universe, allowing the sampler to be reinterpreted as traversing this universe while being stuck in the original space. This perspective opens up an array of possibilities to carry out posterior computation under spike-and-slab type priors. Notably, it enables us to construct sticky samplers using other dynamics-based paradigms such as Hamiltonian Monte Carlo, and, in fact, original sticky process can be established as a partial position-momentum refreshment limit of our Hamiltonian sticky sampler. Further, our theoretical and empirical findings suggest these alternatives to be at least as efficient as the original sticky approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07340v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew Chin, Akihiko Nishimura</dc:creator>
    </item>
    <item>
      <title>Wasserstein-Cram\'er-Rao Theory of Unbiased Estimation</title>
      <link>https://arxiv.org/abs/2511.07414</link>
      <description>arXiv:2511.07414v1 Announce Type: cross 
Abstract: The quantity of interest in the classical Cram\'er-Rao theory of unbiased estimation (e.g., the Cram\'er-Rao lower bound, its exact attainment for exponential families, and asymptotic efficiency of maximum likelihood estimation) is the variance, which represents the instability of an estimator when its value is compared to the value for an independently-sampled data set from the same distribution. In this paper we are interested in a quantity which represents the instability of an estimator when its value is compared to the value for an infinitesimal additive perturbation of the original data set; we refer to this as the "sensitivity" of an estimator. The resulting theory of sensitivity is based on the Wasserstein geometry in the same way that the classical theory of variance is based on the Fisher-Rao (equivalently, Hellinger) geometry, and this insight allows us to determine a collection of results which are analogous to the classical case: a Wasserstein-Cram\'er-Rao lower bound for the sensitivity of any unbiased estimator, a characterization of models in which there exist unbiased estimators achieving the lower bound exactly, and some concrete results that show that the Wasserstein projection estimator achieves the lower bound asymptotically. We use these results to treat many statistical examples, sometimes revealing new optimality properties for existing estimators and other times revealing entirely new estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07414v1</guid>
      <category>math.ST</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicol\'as Garc\'ia Trillos, Adam Quinn Jaffe, Bodhisattva Sen</dc:creator>
    </item>
    <item>
      <title>Mixed Semi-Supervised Generalized-Linear-Regression with Applications to Deep-Learning and Interpolators</title>
      <link>https://arxiv.org/abs/2302.09526</link>
      <description>arXiv:2302.09526v5 Announce Type: replace 
Abstract: We present a methodology for using unlabeled data to design semi-supervised learning (SSL) methods that improve the predictive performance of supervised learning for regression tasks. The main idea is to design different mechanisms for integrating the unlabeled data, and include in each of them a mixing parameter $\alpha$, controlling the weight given to the unlabeled data. Focusing on Generalized Linear Models (GLM) and linear interpolators classes of models, we analyze the characteristics of different mixing mechanisms, and prove that it is consistently beneficial to integrate the unlabeled data with some nonzero mixing ratio $\alpha&gt;0$, in terms of predictive performance. Moreover, we provide a rigorous framework to estimate the best mixing ratio where mixed-SSL delivers the best predictive performance, while using the labeled and unlabeled data on hand. The effectiveness of our methodology in delivering substantial improvement compared to the standard supervised models, in a variety of settings, is demonstrated empirically through extensive simulation, providing empirical support for our theoretical analysis. We also demonstrate the applicability of our methodology (with some heuristic modifications) to improve more complex models, such as deep neural networks, in real-world regression tasks</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.09526v5</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oren Yuval, Saharon Rosset</dc:creator>
    </item>
    <item>
      <title>Disentangling Structural Breaks in Factor Models for Macroeconomic Data</title>
      <link>https://arxiv.org/abs/2303.00178</link>
      <description>arXiv:2303.00178v3 Announce Type: replace 
Abstract: We develop a projection-based decomposition to disentangle structural breaks in the factor variance and factor loadings. Our approach yields test statistics that can be compared against standard distributions commonly used in the structural break literature. Because standard methods for estimating factor models in macroeconomics normalize the factor variance, they do not distinguish between breaks of the factor variance and factor loadings. Applying our procedure to U.S. macroeconomic data, we find that the Great Moderation is more naturally accommodated as a break in the factor variance as opposed to a break in the factor loadings, in contrast to extant procedures which do not tell the two apart and thus interpret the Great Moderation as a structural break in the factor loadings. Through our projection-based decomposition, we estimate that the Great Moderation is associated with an over 70\% reduction in the total factor variance, highlighting the relevance of disentangling breaks in the factor structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.00178v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/07350015.2025.2583205</arxiv:DOI>
      <dc:creator>Bonsoo Koo, Benjamin Wong, Ze-Yu Zhong</dc:creator>
    </item>
    <item>
      <title>Low quality exposure and point processes with a view to the first phase of a pandemic</title>
      <link>https://arxiv.org/abs/2308.09918</link>
      <description>arXiv:2308.09918v2 Announce Type: replace 
Abstract: In the early days of a pandemic there is no time for complicated data collection. One needs a simple cross-country benchmark approach based on robust data that is easy to understand and easy to collect. The recent pandemic has shown us what early available pandemic data might look like, because statistical data was published every day in standard news outlets in many countries. This paper provides new methodology for the analysis of data where exposure is only vaguely understood and where the very definition of exposure might change over time. The exposure of poor quality is used to analyse and forecast events. Our example of such exposure is daily infections during a pandemic and the events are number of new infected patients in hospitals every day. Examples are given with French Covid-19 data on hospitalized patients and numbers of infected.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.09918v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mar\'ia Luz G\'amiz, Enno Mammen, Mar\'ia Dolores Mart\'inez-Miranda, Jens Perch Nielsen</dc:creator>
    </item>
    <item>
      <title>Monitoring a developing pandemic with available data</title>
      <link>https://arxiv.org/abs/2308.09919</link>
      <description>arXiv:2308.09919v2 Announce Type: replace 
Abstract: This paper addresses statistical modelling and forecasting of key indicators describing the severity of a developing pandemic, using routinely reported daily counts of infections, hospitalizations, deaths (both in and out of hospital), and recoveries. These observed counts constitute what we term ``available data''. Because such data are typically incomplete or inconsistently reported, we address several novel missing data challenges arising in this context and propose statistically rigorous solutions that enable inference based solely on the available information. The model is formulated dynamically, explicitly incorporating calendar effects to capture systematic temporal variations in the progression of the pandemic. The proposed framework is illustrated using data from France collected during the COVID-19 pandemic. Our approach also establishes a new benchmark for integrating prior information from domain experts directly into the modelling process, thereby enabling a potential new division of labour between statistical estimation and epidemiological knowledge from external experts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.09919v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mar\'ia Luz G\'amiz, Enno Mammen, Mar\'ia Dolores Mart\'inez-Miranda, Jens Perch Nielsen, Michael Scholz, Germ\'an Ernesto Silva-G\'omez</dc:creator>
    </item>
    <item>
      <title>Selective inference after convex clustering with $\ell_1$ penalization</title>
      <link>https://arxiv.org/abs/2309.01492</link>
      <description>arXiv:2309.01492v2 Announce Type: replace 
Abstract: Classical inference methods notoriously fail when applied to data-driven test hypotheses or inference targets. Instead, dedicated methodologies are required to obtain statistical guarantees for these selective inference problems. Selective inference is particularly relevant post-clustering, typically when testing a difference in mean between two clusters. In this paper, we address convex clustering with $\ell_1$ penalization, by leveraging related selective inference tools for regression, based on Gaussian vectors conditioned to polyhedral sets. In the one-dimensional case, we prove a polyhedral characterization of obtaining given clusters, than enables us to suggest a test procedure with statistical guarantees. This characterization also allows us to provide a computationally efficient regularization path algorithm. Then, we extend the above test procedure and guarantees to multi-dimensional clustering with $\ell_1$ penalization, and also to more general multi-dimensional clusterings that aggregate one-dimensional ones. With various numerical experiments, we validate our statistical guarantees and we demonstrate the power of our methods to detect differences in mean between clusters. Our methods are implemented in the R package poclin.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.01492v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1051/ps/2025004</arxiv:DOI>
      <arxiv:journal_reference>ESAIM: PS, 29 (2025) 204-242</arxiv:journal_reference>
      <dc:creator>Fran\c{c}ois Bachoc, Cathy Maugis-Rabusseau, Pierre Neuvial</dc:creator>
    </item>
    <item>
      <title>Regularized Estimation of Sparse Spectral Precision Matrices</title>
      <link>https://arxiv.org/abs/2401.11128</link>
      <description>arXiv:2401.11128v3 Announce Type: replace 
Abstract: Estimation of a sparse spectral precision matrix, the inverse of a spectral density matrix, is a canonical problem in frequency-domain analysis of high-dimensional time series (HDTS), with applications in neurosciences and environmental sciences. Existing estimators use off-the-shelf optimizers for complex variables that limit scalability, uniform (non-adaptive) penalization that is not tailored to handle heterogeneity across time series components, and lack a formal non-asymptotic theory that systematically analyzes approximation and estimation errors in high-dimension. In this work, develop fast pathwise coordinate descent (CD) algorithms and non-asymptotic theory for a complex graphical lasso (CGLASSO) and an adaptive version CAGLASSO, that adapts penalization to the underlying scale of variability. For fast algorithms, we devise a realification procedure based on ring isomorphism, a notion from abstract algebra, that can be used for other high-dimensional optimization problems over complex variables. Our non-asymptotic analysis shows that consistency is possible in high-dimension under suitable sparsity assumptions. A key step is to separately bound the approximation and estimation error arising from treating the finite-sample discrete Fourier Transforms (DFTs) as i.i.d. complex-valued data, an issue well-addressed in classical time series but relatively less explored in HDTS literature. We demonstrate the performance of our proposed estimators in several simulated data sets and a real data application from neuroscience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.11128v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Navonil Deb, Amy Kuceyeski, Sumanta Basu</dc:creator>
    </item>
    <item>
      <title>Sample-Efficient "Clustering and Conquer" Procedures for Parallel Large-Scale Ranking and Selection</title>
      <link>https://arxiv.org/abs/2402.02196</link>
      <description>arXiv:2402.02196v5 Announce Type: replace 
Abstract: This work aims to improve the sample efficiency of parallel large-scale ranking and selection (R&amp;S) problems by leveraging correlation information. We modify the commonly used "divide and conquer" framework in parallel computing by adding a correlation-based clustering step, transforming it into "clustering and conquer". Analytical results under a symmetric benchmark scenario show that this seemingly simple modification yields an $\mathcal{O}(p)$ reduction in sample complexity for a widely used class of sample-optimal R&amp;S procedures. Our approach enjoys two key advantages: 1) it does not require highly accurate correlation estimation or precise clustering, and 2) it allows for seamless integration with various existing R&amp;S procedures, while achieving optimal sample complexity. Theoretically, we develop a novel gradient analysis framework to analyze sample efficiency and guide the design of large-scale R&amp;S procedures. We also introduce a new parallel clustering algorithm tailored for large-scale scenarios. Finally, in large-scale AI applications such as neural architecture search, our methods demonstrate superior performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02196v5</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zishi Zhang, Yijie Peng</dc:creator>
    </item>
    <item>
      <title>Towards Robust Matched Observational Studies with General Treatment Types: Consistency, Efficiency, and Adaptivity</title>
      <link>https://arxiv.org/abs/2403.14152</link>
      <description>arXiv:2403.14152v3 Announce Type: replace 
Abstract: To ensure reliable causal conclusions from observational (i.e., non-randomized) studies, researchers routinely conduct sensitivity analysis to assess robustness to hidden bias due to unmeasured confounding. In matched observational studies (one of the most widely used observational study designs), two foundational concepts, design sensitivity and Bahadur-Rosenbaum efficiency, are used to quantify the robustness of test statistics and study designs in sensitivity analyses. Unfortunately, these measures of robustness are not developed for non-binary treatments (e.g., continuous or ordinal treatments) and consequently, prevailing recommendations about robust tests may be misleading. In this work, we provide a unified framework to quantify robustness of test statistics and study designs that are agnostic to treatment types. We first present a negative result about a popular, ad-hoc approach based on dichotomizing the treatment variable. Next, we introduce a universal, nearly sufficient sensitivity parameter that is agnostic to the underlying treatment type. We then generalize and derive all-in-one formulas for design sensitivity and Bahadur-Rosenbaum efficiency that can be used for any treatment type. We also propose a general data-adaptive approach to combine candidate test statistics to enhance robustness against unmeasured confounding. Extensive simulation studies and a data application illustrate our proposed framework. For practice, our results yield new, previously undiscovered insights about the robustness of tests and study designs in matched observational studies, especially when investigators are faced with non-binary treatment.sed sensitivity analysis for the binary treatment case, built on the generalized Rosenbaum sensitivity bounds and large-scale mixed integer programming.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14152v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siyu Heng, Elaine K. Chiu, Hyunseung Kang</dc:creator>
    </item>
    <item>
      <title>NIRVAR: Network Informed Restricted Vector Autoregression</title>
      <link>https://arxiv.org/abs/2407.13314</link>
      <description>arXiv:2407.13314v4 Announce Type: replace 
Abstract: High-dimensional panels of time series often arise in finance and macroeconomics, where co-movements within groups of panel components occur. Extracting these groupings from the data provides a coarse-grained description of the complex system in question and can inform subsequent prediction tasks. We develop a novel methodology to model such a panel as a restricted vector autoregressive process, where the coefficient matrix is the weighted adjacency matrix of a stochastic block model. This network time series model, which we call the Network Informed Restricted Vector Autoregression (NIRVAR) model, yields a coefficient matrix that has a sparse block-diagonal structure. We propose an estimation procedure that embeds each panel component in a low-dimensional latent space and clusters the embedded points to recover the blocks of the coefficient matrix. Crucially, the method allows for network-based time series modelling when the underlying network is unobserved. We derive the bias, consistency and asymptotic normality of the NIRVAR estimator. Simulation studies suggest that the NIRVAR estimated embedded points are Gaussian distributed around the ground truth latent positions. On three applications to finance, macroeconomics, and transportation systems, NIRVAR outperforms competing models in terms of prediction and provides interpretable results regarding group recovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13314v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brendan Martin, Francesco Sanna Passino, Mihai Cucuringu, Alessandra Luati</dc:creator>
    </item>
    <item>
      <title>Going With the Flow: Normalizing Flows for Gaussian Process Regression under Hierarchical Shrinkage Priors</title>
      <link>https://arxiv.org/abs/2501.13173</link>
      <description>arXiv:2501.13173v2 Announce Type: replace 
Abstract: Gaussian Process Regression (GPR) is a powerful tool for nonparametric regression, but its application in a fully Bayesian fashion in high-dimensional settings is hindered by two primary challenges: the difficulty of variable selection and the computational burden, which is particularly acute in fully Bayesian inference. This paper introduces a novel methodology that combines hierarchical global-local shrinkage priors with normalizing flows to address these challenges. The hierarchical triple gamma prior offers a principled framework for inducing sparsity in high-dimensional GPR, effectively excluding irrelevant covariates while preserving interpretability and flexibility. Normalizing flows are employed within a variational inference framework to approximate the posterior distribution of parameters, capturing complex dependencies while ensuring computational scalability. Simulation studies demonstrate the efficacy of the proposed approach, outperforming traditional maximum likelihood estimation and mean-field variational methods, particularly in high-sparsity and high-dimensional settings. This is also borne out in an application to binding affinity ($\text{pIC}_{50}$) measurements for small molecules targeting $\beta$-secretase-1 (BACE-1). The results highlight the robustness and flexibility of hierarchical shrinkage priors and the computational efficiency of normalizing flows for Bayesian GPR. This work provides a scalable and interpretable solution for high-dimensional nonparametric regression, with implications for sparse modeling and posterior approximation in broader Bayesian contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13173v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peter Knaus</dc:creator>
    </item>
    <item>
      <title>A framework for joint assessment of a terminal event and a score existing only in the absence of the terminal event</title>
      <link>https://arxiv.org/abs/2502.03942</link>
      <description>arXiv:2502.03942v5 Announce Type: replace 
Abstract: Analysis of data from randomized controlled trials in vulnerable populations requires special attention when assessing treatment effect by a score measuring, e.g., disease stage or activity together with onset of prevalent terminal events. In reality, it is impossible to disentangle a disease score from the terminal event, since the score is not clinically meaningful after this event. In this work, we propose to assess treatment interventions simultaneously on the terminal event and the disease score in the absence of a terminal event. Our proposal is based on a natural data-generating mechanism, respecting that a disease score does not exist beyond the terminal event. We use modern semi-parametric statistical methods to provide robust and efficient estimation of the risk of terminal event and expected disease score conditional on no terminal event at a pre-specified landmark time. We also use the simultaneous asymptotic behaviour of our estimators to develop a powerful closed testing procedure for confirmatory assessment of treatment effect on both onset of terminal event and level of disease score in the absence of a terminal event. A simulation study mimicking a large-scale outcome trial in chronic kidney patients as well as an analysis of that trial is provided to assess performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03942v5</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Klaus K\"ahler Holst, Andreas Nordland, Julie Funch Furberg, Lars Holm Damgaard, Christian Bressen Pipper</dc:creator>
    </item>
    <item>
      <title>On a fast consistent selection of nested models with possibly unnormalized probability densities</title>
      <link>https://arxiv.org/abs/2503.06331</link>
      <description>arXiv:2503.06331v3 Announce Type: replace 
Abstract: Models with unnormalized probability density functions are ubiquitous in statistics, artificial intelligence and many other fields. However, they face significant challenges in model selection if the normalizing constants are intractable. Existing methods to address this issue often incur high computational costs, either due to numerical approximations of normalizing constants or evaluation of bias corrections in information criteria. In this paper, we propose a novel and fast selection criterion, MIC, for nested models of possibly dependent data, allowing direct data sampling from a possibly unnormalized probability density function. With a suitable multiplying factor depending only on the sample size and the model complexity, MIC gives a consistent selection under mild regularity conditions and is computationally efficient. Extensive simulation studies and real-data applications demonstrate the efficacy of MIC in the selection of nested models with unnormalized probability densities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06331v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rong Bian, Kung-Sik Chan, Bing Cheng, Howell Tong</dc:creator>
    </item>
    <item>
      <title>Meta-analytic-predictive priors based on a single study</title>
      <link>https://arxiv.org/abs/2505.15502</link>
      <description>arXiv:2505.15502v2 Announce Type: replace 
Abstract: Meta-analytic-predictive (MAP) priors have been proposed as a generic approach to deriving informative prior distributions, where external empirical data are processed to learn about certain parameter distributions. The use of MAP priors is also closely related to shrinkage estimation (also sometimes referred to as dynamic borrowing). A potentially odd situation arises when the external data consist only of a single study. Conceptually this is not a problem, it only implies that certain prior assumptions gain in importance and need to be specified with particular care. We outline this important, not uncommon special case and demonstrate its implementation and interpretation based on the normal-normal hierarchical model. The approach is illustrated using example applications in clinical medicine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15502v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian R\"over, Tim Friede</dc:creator>
    </item>
    <item>
      <title>The Causal-Noncausal Tail Processes</title>
      <link>https://arxiv.org/abs/2506.04046</link>
      <description>arXiv:2506.04046v3 Announce Type: replace 
Abstract: This paper considers one-dimensional mixed causal/noncausal autoregressive (MAR) processes with heavy tail, usually introduced to model trajectories with patterns including asymmetric peaks and throughs, speculative bubbles, flash crashes, or jumps. We especially focus on the extremal behaviour of these processes when at a given date the process is above a large threshold and emphasize the roles of pure causal and noncausal components of the tail process. We provide the dynamic of the tail process and explain how it can be updated during the life of a speculative bubble. In particular we discuss the prediction of the turning point(s) and introduce pure residual plots as a diagnostic for the bubble episodes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04046v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian Gouri\'eroux, Yang Lu, Christian-Yann Robert</dc:creator>
    </item>
    <item>
      <title>An easily verifiable dispersion order for discrete distributions</title>
      <link>https://arxiv.org/abs/2506.23677</link>
      <description>arXiv:2506.23677v2 Announce Type: replace 
Abstract: Dispersion is a fundamental concept in statistics, yet standard approaches - especially via stochastic orders - face limitations in the discrete setting. In particular, the classical dispersive order, well-established for continuous distributions, becomes overly restrictive for discrete random variables due to support inclusion requirements. To address this, we propose a novel weak dispersive order for discrete distributions. This order retains desirable properties while relaxing structural constraints, thereby broadening applicability. We further introduce a class of variability measures based on probability concentration, offering robust and interpretable alternatives that conform to classical axioms. Empirical illustrations highlight the practical relevance of this framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23677v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andreas Eberl, Bernhard Klar, Alfonso Su\'arez-Llorens</dc:creator>
    </item>
    <item>
      <title>Factor-Driven Network Informed Restricted Vector Autoregression</title>
      <link>https://arxiv.org/abs/2508.02198</link>
      <description>arXiv:2508.02198v2 Announce Type: replace 
Abstract: High-dimensional financial time series often exhibit complex dependence relations driven by both common market structures and latent connections among assets. To capture these characteristics, this paper proposes Factor-Driven Network Informed Restricted Vector Autoregression (FNIRVAR), a model for the common and idiosyncratic components of high-dimensional time series with an underlying unobserved network structure. The common component is modelled by a static factor model, which allows for strong cross-sectional dependence, whilst a network vector autoregressive process captures the residual co-movements due to the idiosyncratic component. An assortative stochastic block model underlies the network VAR, leading to groups of highly co-moving variables in the idiosyncratic component. For estimation, a two-step procedure is proposed, whereby the static factors are estimated via principal component analysis, followed by estimation of the network VAR parameters. The method is demonstrated in financial applications to daily returns, intraday returns, and FRED-MD macroeconomic variables. In all cases, the proposed method outperforms a static factor model, as well as a static factor plus LASSO-estimated sparse VAR model, in terms of forecasting and financial performance metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02198v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brendan Martin, Mihai Cucuringu, Alessandra Luati, Francesco Sanna Passino</dc:creator>
    </item>
    <item>
      <title>On the Practical Use of Blaschke Decomposition in Nonstationary Signal Analysis</title>
      <link>https://arxiv.org/abs/2508.10861</link>
      <description>arXiv:2508.10861v2 Announce Type: replace 
Abstract: The Blaschke decomposition-based algorithm, {\em Phase Dynamics Unwinding} (PDU), possesses several attractive theoretical properties, including fast convergence, effective decomposition, and multiscale analysis. However, its application to real-world signal decomposition tasks encounters notable challenges. In this work, we propose two techniques, divide-and-conquer via tapering and cumulative summation (cumsum), to handle complex trends and amplitude modulations and the mode-mixing caused by winding. The resulting method, termed {\em windowed PDU}, enhances PDU's performance in practical decomposition tasks. We validate our approach through both simulated and real-world signals, demonstrating its effectiveness across diverse scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10861v2</guid>
      <category>stat.ME</category>
      <category>math.CV</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ronald R. Coifman, Hau-Tieng Wu</dc:creator>
    </item>
    <item>
      <title>A simulation-free extrapolation method for misspecified models with errors-in-variables in epidemiological studies</title>
      <link>https://arxiv.org/abs/2509.06118</link>
      <description>arXiv:2509.06118v2 Announce Type: replace 
Abstract: In epidemiological studies, it is common to analyze disease risk by categorizing continuous variables, such as calorie and nutrient intake, for interpretability. When the original continuous variable is contaminated with measurement errors, ignoring this issue and performing regular statistical analysis leads to severely biased point estimates and invalid confidence intervals. Although the errors-in-variables problem is a well-known critical issue in many areas, most existing methods addressing measurement errors either do not account for model misspecification or make strong parametric assumptions. We introduce SIMFEX, a simulation-free extrapolation method, which provides valid and robust statistical inference across a range of models and imposes no distributional assumptions on the observed data. Through extensive numerical studies, we show that SIMFEX can provide consistent point estimation and valid confidence intervals under various regression models. Using Food Frequency Questionnaire in UK Biobank data, we show that ignoring measurement errors underestimates the impact of high fat intake on BMI and obesity by at least 30% and 60%, respectively, compared with the results of correcting for measurement errors using SIMFEX.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06118v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huali Zhao, Tianying Wang</dc:creator>
    </item>
    <item>
      <title>The $\alpha$--regression for compositional data: a unified framework for standard, spatially-lagged, spatial autoregressive and geographically-weighted regression models</title>
      <link>https://arxiv.org/abs/2510.12663</link>
      <description>arXiv:2510.12663v2 Announce Type: replace 
Abstract: Compositional data-vectors of non-negative components summing to unity-frequently arise in scientific applications where covariates influence the relative proportions of components, yet traditional regression approaches ace challenges regarding the unit-sum constraint and zero values. This paper revisits the $\alpha$--regression framework, which uses a flexible power transformation parameterized by $\alpha$ to interpolate between raw data analysis and log-ratio methods, naturally handling zeros without imputation while allowing data-driven transformation selection. We formulate $\alpha$--regression as a non-linear least squares problem, provide efficient estimation via the Levenberg-Marquardt algorithm, and derive marginal effects for interpretation. The framework is extended to spatial settings through two models: the $\alpha$--spatially lagged X regression model, which incorporates spatial spillover effects via spatially lagged covariates with decomposition into direct and indirect effects, the $\alpha$--spatially autoregressive regression model and the geographically weighted $\alpha$--regression, which allows coefficients to vary spatially for capturing local relationships. Applications to two real data sets illustrate the performance of the models and showcase that spatial extensions capture the spatial dependence and improve the predictive performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12663v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michail Tsagris</dc:creator>
    </item>
    <item>
      <title>Bayesian nonparametric modeling of multivariate count data with an unknown number of traits</title>
      <link>https://arxiv.org/abs/2510.24526</link>
      <description>arXiv:2510.24526v2 Announce Type: replace 
Abstract: Feature and trait allocation models are fundamental objects in Bayesian nonparametrics and play a prominent role in several applications. Existing approaches, however, typically assume full exchangeability of the data, which may be restrictive in settings characterized by heterogeneous but related groups. In this paper, we introduce a general and tractable class of Bayesian nonparametric priors for partially exchangeable trait allocation models, relying on completely random vectors. We provide a comprehensive theoretical analysis, including closed-form expressions for marginal and posterior distributions, and illustrate the tractability of our framework in the cases of binary and Poisson-distributed traits. A distinctive aspect of our approach is that the number of traits is a random quantity, thereby allowing us to model and estimate unobserved traits. Building on these results, we also develop a novel mixture model that infers the group partition structure from the data, effectively clustering trait allocations. This extension generalizes Bayesian nonparametric latent class models and avoids the systematic overclustering that arises when the number of traits is assumed to be fixed. We demonstrate the practical usefulness of our methodology through an application to the `Ndrangheta criminal network from the Operazione Infinito investigation, where our model provides insights into the organization of illicit activities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24526v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Ghilotti, Federico Camerlenghi, Tommaso Rigon, Michele Guindani</dc:creator>
    </item>
    <item>
      <title>A Pragmatic Framework for Bayesian Utility Magnitude-Based Decisions</title>
      <link>https://arxiv.org/abs/2511.03932</link>
      <description>arXiv:2511.03932v2 Announce Type: replace 
Abstract: This article presents a pragmatic framework for making formal, utility-based decisions from statistical inferences. The method calculates an expected utility score for an intervention by combining Bayesian posterior probabilities of different effect magnitudes with points representing their practical value. A key innovation is a unified, non-arbitrary points scale (1-9 for small to extremely large) derived from a principle linking tangible outcomes across different effect types. This tangible scale enables a principled "trade-off" method for including values for loss aversion, side effects, and implementation cost. The framework produces a single, definitive expected utility score, and the initial decision is made by comparing the magnitude of this single score to a user-defined smallest important net benefit, a direct and intuitive comparison made possible by the scale's tangible nature. This expected utility decision is interpreted alongside clinical magnitude-based decision probabilities or credible interval coverage to assess evidence strength. Inclusion of a standard deviation representing individual responses to an intervention (or differences between settings with meta-analytic data) allows characterization of differences between individuals (or settings) in the utility score expressed as proportions expected to experience benefit, a negligible effect, and harm. These proportions provide context for the final decision about implementation. Users must perform sensitivity analyses to investigate the effects of systematic bias and of the subjective inputs on the final decision. This framework, implemented in an accessible spreadsheet, has not been empirically validated. It represents a tool in development, designed for practical decision-making from available statistical evidence and structured thinking about values of outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03932v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Will G. Hopkins</dc:creator>
    </item>
    <item>
      <title>Geometric Decomposition of Statistical Inference through Gradient Flow and Co-Monotonicity Measures</title>
      <link>https://arxiv.org/abs/2511.04599</link>
      <description>arXiv:2511.04599v2 Announce Type: replace 
Abstract: Understanding feature-outcome associations in high-dimensional data remains
  challenging when relationships vary across subpopulations, yet standard
  methods assuming global associations miss context-dependent patterns, reducing
  statistical power and interpretability. We develop a geometric decomposition
  framework offering two strategies for partitioning inference problems into
  regional analyses on data-derived Riemannian graphs. Gradient flow
  decomposition uses path-monotonicity-validated discrete Morse theory to
  partition samples into gradient flow cells where outcomes exhibit monotonic
  behavior. Co-monotonicity decomposition leverages association structure:
  vertex-level coefficients measuring directional concordance between outcome
  and features, or between feature pairs, define embeddings of samples into
  association space. These embeddings induce Riemannian k-NN graphs on which
  biclustering identifies co-monotonicity cells (coherent regions) and feature
  modules. This extends naturally to multi-modal integration across multiple
  feature sets. Both strategies apply independently or jointly, with Bayesian
  posterior sampling providing credible intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04599v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pawel Gajer, Jacques Ravel</dc:creator>
    </item>
    <item>
      <title>The upper-crossing/solution (US) algorithm for root-finding with strongly stable convergence</title>
      <link>https://arxiv.org/abs/2212.00797</link>
      <description>arXiv:2212.00797v2 Announce Type: replace-cross 
Abstract: In this paper, we propose a new and broadly applicable root-finding method, called as the upper-crossing/solution (US) algorithm, which belongs to the category of non-bracketing (or open domain) methods. The US algorithm is a general principle for iteratively seeking the unique root $\theta^{*}$ of a non-linear equation $g(\theta)=0$ and its each iteration consists of two steps: an upper-crossing step (U-step) and a solution step (S-step), where the U-step finds an upper-crossing function or a $U$-function $U(\theta|\theta^{(t)})$ [whose form depends on $\theta^{(t)}$ being the $t$-th iteration of $\theta^{*}$] based on a new notion of so-called changing direction inequality, and the S-step solves the simple $U$-equation $U(\theta|\theta^{(t)}) =0$ to obtain its explicit solution $\theta^{(t+1)}$. The US algorithm holds two major advantages: (i) It strongly stably converges to the root $\theta^{*}$; and (ii) it does not depend on any initial values, in contrast to Newton's method. The key step for applying the US algorithm is to construct one simple $U$-function $U(\theta|\theta^{(t)})$ such that an explicit solution to the $U$-equation $U(\theta|\theta^{(t)}) =0$ is available. Based on the first-, second- and third-derivative of $g(\theta)$, three methods are given for constructing such $U$-functions. We show various applications of the US algorithm in such as calculating quantile in continuous distributions, calculating exact $p$-values for skew null distributions, and finding maximum likelihood estimates of parameters in a class of continuous/discrete distributions. The analysis of the convergence rate of the US algorithm and some numerical experiments are also provided. Especially, because of the property of strongly stable convergence, the US algorithm could be one of the powerful tools for solving an equation with multiple roots.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.00797v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xun-Jian Li, Hua Zhou, Kenneth Lange, Guo-Liang Tian</dc:creator>
    </item>
    <item>
      <title>Simple Estimation of Semiparametric Models with Measurement Errors</title>
      <link>https://arxiv.org/abs/2306.14311</link>
      <description>arXiv:2306.14311v4 Announce Type: replace-cross 
Abstract: We develop a practical way of addressing the Errors-In-Variables (EIV) problem in the Generalized Method of Moments (GMM) framework. We focus on the settings in which the variability of the EIV is a fraction of that of the mismeasured variables, which is typical for empirical applications. For any initial set of moment conditions our approach provides a ``corrected'' set of moment conditions that are robust to the EIV. We show that the GMM estimator based on these moments is root-n-consistent, with the standard tests and confidence intervals providing valid inference. This is true even when the EIV are so large that naive estimators (that ignore the EIV problem) are heavily biased with their confidence intervals having 0% coverage. Our approach involves no nonparametric estimation, which is especially important for applications with many covariates and settings with multivariate EIV. In particular, the approach makes it easy to use instrumental variables to address EIV in nonlinear models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.14311v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kirill S. Evdokimov, Andrei Zeleneev</dc:creator>
    </item>
    <item>
      <title>Contextual Linear Optimization with Partial Feedback</title>
      <link>https://arxiv.org/abs/2405.16564</link>
      <description>arXiv:2405.16564v3 Announce Type: replace-cross 
Abstract: Contextual linear optimization (CLO) uses predictive contextual features to reduce uncertainty in random cost coefficients in the objective and thereby improve decision-making performance. A canonical example is the stochastic shortest path problem with random edge costs (e.g., travel time) and contextual features (e.g., lagged traffic, weather). While existing work on CLO assumes fully observed cost coefficient vectors, in many applications the decision maker observes only partial feedback corresponding to each chosen decision in the history. In this paper, we study both a bandit-feedback setting (e.g., only the overall travel time of each historical path is observed) and a semi-bandit-feedback setting (e.g., travel times of the individual segments on each chosen path are additionally observed). We propose a unified class of offline learning algorithms for CLO with different types of feedback, following a powerful induced empirical risk minimization (IERM) framework that integrates estimation and optimization. We provide a novel fast-rate regret bound for IERM that allows for misspecified model classes and flexible choices of estimation methods. To solve the partial-feedback IERM, we also tailor computationally tractable surrogate losses. A byproduct of our theory of independent interest is the fast-rate regret bound for IERM with full feedback and a misspecified policy class. We compare the performance of different methods numerically using stochastic shortest path examples on simulated and real data and provide practical insights from the empirical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16564v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yichun Hu, Nathan Kallus, Xiaojie Mao, Yanchen Wu</dc:creator>
    </item>
    <item>
      <title>Variable Selection for Multi-Source Count Data with Controlled False Discovery Rate</title>
      <link>https://arxiv.org/abs/2411.18986</link>
      <description>arXiv:2411.18986v2 Announce Type: replace-cross 
Abstract: The rapid generation of complex, highly skewed, and zero-inflated multi-source count data poses significant challenges for variable selection, particularly in biomedical domains like tumor development and metabolic dysregulation. To address this, we propose a new variable selection method, Zero-Inflated Poisson-Gamma Simultaneous Knockoff (ZIPG-SK), specifically designed for multi-source count data. Our method leverages a gaussian copula based on the Zero-Inflated Poisson-Gamma (ZIPG) distribution to construct knockoffs that properly account for the properties of count data, including high skewness and zero inflation, while effectively incorporating covariate information. This framework enables the detection of common features across multi-source datasets with guaranteed false discovery rate (FDR) control. Furthermore, we enhance the power of the method by incorporating e-value aggregation, which effectively mitigates the inherent randomness in knockoff generation. Through extensive simulations, we demonstrate that ZIPG-SK significantly outperforms existing methods, achieving superior power across various scenarios. We validate the utility of our method on real-world colorectal cancer (CRC) and type 2 diabetes (T2D) datasets, identifying key variables whose characteristics align with established findings and simultaneously provide new mechanistic insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18986v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shan Tang, Shanjun Mao, Shourong Ma, Falong Tan</dc:creator>
    </item>
    <item>
      <title>Hierarchical models for small area estimation using zero-inflated forest inventory variables: comparison and implementation</title>
      <link>https://arxiv.org/abs/2503.22103</link>
      <description>arXiv:2503.22103v3 Announce Type: replace-cross 
Abstract: National Forest Inventory (NFI) data are typically limited to sparse networks of sample locations due to cost constraints. While design-based estimators provide reliable forest parameter estimates for large areas, there is increasing interest in model-based small area estimation (SAE) methods to improve precision for smaller spatial, temporal, or biophysical domains. SAE methods can be broadly categorized into area- and unit-level models, with unit-level models offering greater flexibility, making them the focus of this study. Ensuring valid inference requires satisfying model distributional assumptions, which is particularly challenging for NFI variables that exhibit positive support and zero-inflation, such as forest biomass, carbon, and volume. Here, we evaluate nine candidate estimators, including two-stage unit-level hierarchical Bayesian models, single-stage Bayesian models, and two-stage frequentist models, for estimating forest biomass at the county level in Nevada and Washington, United States. Estimator performance is assessed using repeated sampling from simulated populations and unit-level cross-validation with FIA data. Results show that small area estimators incorporating a two-stage approach to account for zero-inflation, county-specific random intercepts and residual variances, and spatial random effects yield the most accurate and well-calibrated county-level estimates, with spatial effects providing the greatest benefits when spatial autocorrelation is present in the underlying population.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22103v3</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Grayson W. White, Andrew O. Finley, Josh K. Yamamoto, Jennifer L. Green, Tracey S. Frescino, David. W. MacFarlane, Hans-Erik Andersen, Grant M. Domke</dc:creator>
    </item>
    <item>
      <title>Causal Discovery in Dynamic Fading Wireless Networks</title>
      <link>https://arxiv.org/abs/2506.03163</link>
      <description>arXiv:2506.03163v2 Announce Type: replace-cross 
Abstract: Dynamic causal discovery in wireless networks is essential due to evolving interference, fading, and mobility, which complicate traditional static causal models. This paper addresses causal inference challenges in dynamic fading wireless environments by proposing a sequential regression-based algorithm with a novel application of the NOTEARS acyclicity constraint, enabling efficient online updates. We derive theoretical lower and upper bounds on the detection delay required to identify structural changes, explicitly quantifying their dependence on network size, noise variance, and fading severity. Monte Carlo simulations validate these theoretical results, demonstrating linear increases in detection delay with network size, quadratic growth with noise variance, and inverse-square dependence on the magnitude of structural changes. Our findings provide rigorous theoretical insights and practical guidelines for designing robust online causal inference mechanisms to maintain network reliability under nonstationary wireless conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03163v2</guid>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oluwaseyi Giwa</dc:creator>
    </item>
    <item>
      <title>Differentially Private Distribution Release of Gaussian Mixture Models via KL-Divergence Minimization</title>
      <link>https://arxiv.org/abs/2506.03467</link>
      <description>arXiv:2506.03467v2 Announce Type: replace-cross 
Abstract: Gaussian Mixture Models (GMMs) are widely used statistical models for representing multi-modal data distributions, with numerous applications in data mining, pattern recognition, data simulation, and machine learning. However, recent research has shown that releasing GMM parameters poses significant privacy risks, potentially exposing sensitive information about the underlying data. In this paper, we address the challenge of releasing GMM parameters while ensuring differential privacy (DP) guarantees. Specifically, we focus on the privacy protection of mixture weights, component means, and covariance matrices. We propose to use Kullback-Leibler (KL) divergence as a utility metric to assess the accuracy of the released GMM, as it captures the joint impact of noise perturbation on all the model parameters. To achieve privacy, we introduce a DP mechanism that adds carefully calibrated random perturbations to the GMM parameters. Through theoretical analysis, we quantify the effects of privacy budget allocation and perturbation statistics on the DP guarantee, and derive a tractable expression for evaluating KL divergence. We formulate and solve an optimization problem to minimize the KL divergence between the released and original models, subject to a given $(\epsilon, \delta)$-DP constraint. Extensive experiments on both synthetic and real-world datasets demonstrate that our approach achieves strong privacy guarantees while maintaining high utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03467v2</guid>
      <category>cs.IT</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hang Liu, Anna Scaglione, Sean Peisert</dc:creator>
    </item>
    <item>
      <title>Preference-Based Dynamic Ranking Structure Recognition</title>
      <link>https://arxiv.org/abs/2509.24493</link>
      <description>arXiv:2509.24493v2 Announce Type: replace-cross 
Abstract: Preference-based data often appear complex and noisy but may conceal underlying homogeneous structures. This paper introduces a novel framework of ranking structure recognition for preference-based data. We first develop an approach to identify dynamic ranking groups by incorporating temporal penalties into a spectral estimation for the celebrated Bradley-Terry model. To detect structural changes, we introduce an innovative objective function and present a practicable algorithm based on dynamic programming. Theoretically, we establish the consistency of ranking group recognition by exploiting properties of a random `design matrix' induced by a reversible Markov chain. We also tailor a group inverse technique to quantify the uncertainty in item ability estimates. Additionally, we prove the consistency of structure change recognition, ensuring the robustness of the proposed framework. Experiments on both synthetic and real-world datasets demonstrate the practical utility and interpretability of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24493v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nan Lu, Jian Shi, Xin-Yu Tian</dc:creator>
    </item>
    <item>
      <title>The causal structure of galactic astrophysics</title>
      <link>https://arxiv.org/abs/2510.01112</link>
      <description>arXiv:2510.01112v2 Announce Type: replace-cross 
Abstract: Data-driven astrophysics currently relies on the detection and characterisation of correlations between objects' properties, which are then used to test physical theories that make predictions for them. This process fails to utilise information in the data that forms a crucial part of the theories' predictions, namely which variables are directly correlated (as opposed to accidentally correlated through others), the directions of these determinations, and the presence or absence of confounders that correlate variables in the dataset but are themselves absent from it. We propose to recover this information through causal discovery, a well-developed methodology for inferring the causal structure of datasets that is however almost entirely unknown to astrophysics. We develop a causal discovery algorithm suitable for large astrophysical datasets and illustrate it on $\sim$5$\times10^5$ low-redshift galaxies from the Nasa Sloan Atlas, demonstrating its ability to distinguish physical mechanisms that are degenerate on the basis of correlations alone.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01112v2</guid>
      <category>astro-ph.GA</category>
      <category>astro-ph.CO</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harry Desmond, Joseph Ramsey</dc:creator>
    </item>
    <item>
      <title>Choosing What to Learn: Experimental Design when Combining Experimental with Observational Evidence</title>
      <link>https://arxiv.org/abs/2510.23434</link>
      <description>arXiv:2510.23434v2 Announce Type: replace-cross 
Abstract: Experiments deliver credible but often localized effects, tied to specific sites, populations, or mechanisms. When such estimates are insufficient to extrapolate effects for broader policy questions, such as external validity and general-equilibrium (GE) effects, researchers combine trials with external evidence from reduced-form or structural observational estimates, or prior experiments. We develop a unified framework for designing experiments in this setting: the researcher selects which parameters (or moments) to identify experimentally from a feasible set (e.g., which treatment arms and/or individuals to include in the experiment), allocates sample size, and specifies how to weight experimental and observational estimators. Because observational inputs may be biased in ways unknown ex ante, we develop a minimax proportional regret objective that evaluates any candidate design relative to an oracle that knows the bias and jointly chooses the design and estimator. This yields a transparent bias-variance trade-off that requires no prespecified bias bound and depends only on information about the precision of the estimators and the estimand's sensitivity to the underlying parameters. We illustrate the framework by (i) designing small-scale cash transfer experiments aimed at estimating GE effects and (ii) optimizing site selection for microfinance interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23434v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Aristotelis Epanomeritakis, Davide Viviano</dc:creator>
    </item>
    <item>
      <title>Beyond the Trade-off Curve: Multivariate and Advanced Risk-Utility Maps for Evaluating Anonymized and Synthetic Data</title>
      <link>https://arxiv.org/abs/2510.23500</link>
      <description>arXiv:2510.23500v2 Announce Type: replace-cross 
Abstract: Anonymizing microdata requires balancing the reduction of disclosure risk with the preservation of data utility. Traditional evaluations often rely on single measures or two-dimensional risk-utility (R-U) maps, but real-world assessments involve multiple, often correlated, indicators of both risk and utility. Pairwise comparisons of these measures can be inefficient and incomplete. We therefore systematically compare six visualization approaches for simultaneous evaluation of multiple risk and utility measures: heatmaps, dot plots, composite scatterplots, parallel coordinate plots, radial profile charts, and PCA-based biplots. We introduce blockwise PCA for composite scatterplots and joint PCA for biplots that simultaneously reveal method performance and measure interrelationships. Through systematic identification of Pareto-optimal methods in all approaches, we demonstrate how multivariate visualization supports a more informed selection of anonymization methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23500v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oscar Thees, Roman M\"uller, Matthias Templ</dc:creator>
    </item>
  </channel>
</rss>
