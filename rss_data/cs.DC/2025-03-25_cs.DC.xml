<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 26 Mar 2025 02:15:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Serinv: A Scalable Library for the Selected Inversion of Block-Tridiagonal with Arrowhead Matrices</title>
      <link>https://arxiv.org/abs/2503.17528</link>
      <description>arXiv:2503.17528v1 Announce Type: new 
Abstract: The inversion of structured sparse matrices is a key but computationally and memory-intensive operation in many scientific applications. There are cases, however, where only particular entries of the full inverse are required. This has motivated the development of so-called selected-inversion algorithms, capable of computing only specific elements of the full inverse. Currently, most of them are either shared-memory codes or limited to CPU implementations. Here, we introduce Serinv, a scalable library providing distributed, GPU-based algorithms for the selected inversion and Cholesky decomposition of positive-definite, block-tridiagonal arrowhead matrices. This matrix class is highly relevant in statistical climate modeling and materials science applications. The performance of Serinv is demonstrated on synthetic and real datasets from statistical air temperature prediction models. In our numerical tests, Serinv achieves 32.3% strong and 47.2% weak scaling efficiency and up to two orders of magnitude speedup over the sparse direct solvers PARDISO and MUMPS on 16 GPUs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17528v1</guid>
      <category>cs.DC</category>
      <category>cs.NA</category>
      <category>cs.PF</category>
      <category>math.NA</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vincent Maillou, Lisa Gaedke-Merzhaeuser, Alexandros Nikolaos Ziogas, Olaf Schenk, Mathieu Luisier</dc:creator>
    </item>
    <item>
      <title>Time- and Space-Optimal Silent Self-Stabilizing Exact Majority in Population Protocols</title>
      <link>https://arxiv.org/abs/2503.17652</link>
      <description>arXiv:2503.17652v1 Announce Type: new 
Abstract: We address the self-stabilizing exact majority problem in the population protocol model, introduced by Angluin, Aspnes, Diamadi, Fischer, and Peralta (2004). In this model, there are $n$ state machines, called agents, which form a network. At each time step, only two agents interact with each other, and update their states. In the self-stabilizing exact majority problem, each agent has a fixed opinion, $\mathtt{A}$ or $\mathtt{B}$, and stabilizes to a safe configuration in which all agents output the majority opinion from any initial configuration.
  In this paper, we show the impossibility of solving the self-stabilizing exact majority problem without knowledge of $n$ in any protocol. We propose a silent self-stabilizing exact majority protocol, which stabilizes within $O(n)$ parallel time in expectation and within $O(n \log n)$ parallel time with high probability, using $O(n)$ states, with knowledge of $n$. Here, a silent protocol means that, after stabilization, the state of each agent does not change. We establish lower bounds, proving that any silent protocol requires $\Omega(n)$ states, $\Omega(n)$ parallel time in expectation, and $\Omega(n \log n)$ parallel time with high probability to reach a safe configuration. Thus, the proposed protocol is time- and space-optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17652v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haruki Kanaya, Ryota Eguchi, Taisho Sasada, Fukuhito Ooshita, Michiko Inoue</dc:creator>
    </item>
    <item>
      <title>Using a Market Economy to Provision Compute Resources Across Planet-wide Clusters</title>
      <link>https://arxiv.org/abs/2503.17691</link>
      <description>arXiv:2503.17691v1 Announce Type: new 
Abstract: We present a practical, market-based solution to the resource provisioning problem in a set of heterogeneous resource clusters. We focus on provisioning rather than immediate scheduling decisions to allow users to change long-term job specifications based on market feedback. Users enter bids to purchase quotas, or bundles of resources for long-term use. These requests are mapped into a simulated clock auction which determines uniform, fair resource prices that balance supply and demand. The reserve prices for resources sold by the operator in this auction are set based on current utilization, thus guiding the users as they set their bids towards under-utilized resources. By running these auctions at regular time intervals, prices fluctuate like those in a real-world economy and provide motivation for users to engineer systems that can best take advantage of available resources.
  These ideas were implemented in an experimental resource market at Google. Our preliminary results demonstrate an efficient transition of users from more congested resource pools to less congested resources. The disparate engineering costs for users to reconfigure their jobs to run on less expensive resource pools was evidenced by the large price premiums some users were willing to pay for more expensive resources. The final resource allocations illustrated how this framework can lead to significant, beneficial changes in user behavior, reducing the excessive shortages and surpluses of more traditional allocation methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17691v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/IPDPS.2009.5160966</arxiv:DOI>
      <arxiv:journal_reference>IPDPS '09: Proceedings of the 2009 IEEE International Symposium on Parallel &amp; Distributed Processing, Rome, Italy, 2009, pp. 1-8</arxiv:journal_reference>
      <dc:creator>Murray Stokely, Jim Winget, Ed Keyes, Carrie Grimes, Benjamin Yolken</dc:creator>
    </item>
    <item>
      <title>PipeBoost: Resilient Pipelined Architecture for Fast Serverless LLM Scaling</title>
      <link>https://arxiv.org/abs/2503.17707</link>
      <description>arXiv:2503.17707v1 Announce Type: new 
Abstract: This paper presents PipeBoost, a low-latency LLM serving system for multi-GPU (serverless) clusters, which can rapidly launch inference services in response to bursty requests without preemptively over-provisioning GPUs. Many LLM inference tasks rely on the same base model (e.g., LoRA). To leverage this, PipeBoost introduces fault-tolerant pipeline parallelism across both model loading and inference stages. This approach maximizes aggregate PCIe bandwidth and parallel computation across GPUs, enabling faster generation of the first token. PipeBoost also introduces recovery techniques that enable uninterrupted inference services by utilizing the shared advantages of multiple GPUs. Experimental results show that, compared to state-of-the-art low-latency LLM serving systems, PipeBoost reduces inference latency by 31% to 49.8%. For certain models (e.g., OPT-1.3B), PipeBoost achieves cold-start latencies in the range of a few hundred microseconds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17707v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chongpeng Liu, Xiaojian Liao, Hancheng Liu, Limin Xiao, Jianxin Li</dc:creator>
    </item>
    <item>
      <title>Neutron particle transport 3D method of characteristic Multi GPU platform Parallel Computing</title>
      <link>https://arxiv.org/abs/2503.17743</link>
      <description>arXiv:2503.17743v1 Announce Type: new 
Abstract: Three-dimensional neutron transport calculations using the Method of Characteristics (MOC) are highly regarded for their exceptional computational efficiency, precision, and stability. Nevertheless, when dealing with extensive-scale computations, the computational demands are substantial, leading to prolonged computation times. To address this challenge while considering GPU memory limitations, this study transplants the real-time generation and characteristic line computation techniques onto the GPU platform. Empirical evidence emphasizes that the GPU-optimized approach maintains a heightened level of precision in computation results and produces a significant acceleration effect. Furthermore, to fully harness the computational capabilities of GPUs, a dual approach involving characteristic line preloading and load balancing mechanisms is adopted, further enhancing computational efficiency. The resulting increase in computational efficiency, compared to traditional methods, reaches an impressive 300 to 400-fold improvement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17743v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Faguo Zhou, Shunde Li, Rong Xue, Lingkun Bu, Ningming Nie, Peng Shi, Jue Wang, Yun Hu, Zongguo Wang, Yangang Wang, Qinmeng Yang, Miao Yu</dc:creator>
    </item>
    <item>
      <title>CRDT-Based Game State Synchronization in Peer-to-Peer VR</title>
      <link>https://arxiv.org/abs/2503.17826</link>
      <description>arXiv:2503.17826v1 Announce Type: new 
Abstract: Virtual presence demands ultra-low latency, a factor that centralized architectures, by their nature, cannot minimize. Local peer-to-peer architectures offer a compelling alternative, but also pose unique challenges in terms of network infrastructure. This paper introduces a prototype leveraging Conflict-Free Replicated Data Types (CRDTs) to enable real-time collaboration in a shared virtual environment. Using this prototype, we investigate latency, synchronization, and the challenges of decentralized coordination in dynamic non-Byzantine contexts. We aim to question prevailing assumptions about decentralized architectures and explore the practical potential of P2P in advancing virtual presence. This work challenges the constraints of mediated networks and highlights the potential of decentralized architectures to redefine collaboration and interaction in digital spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17826v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Abel Dantas, Carlos Baquero</dc:creator>
    </item>
    <item>
      <title>WLB-LLM: Workload-Balanced 4D Parallelism for Large Language Model Training</title>
      <link>https://arxiv.org/abs/2503.17924</link>
      <description>arXiv:2503.17924v1 Announce Type: new 
Abstract: In this work, we present WLB-LLM, a workLoad-balanced 4D parallelism for large language model training. We first thoroughly analyze the workload imbalance issue in LLM training and identify two primary sources of imbalance at the pipeline parallelism and context parallelism levels. Then, to address the imbalance issue, at the pipeline parallelism level, WLB-LLM incorporates a workload-aware variable-length document packing method to balance the computation and communication workload across micro-batches. Additionally, at the context parallelism level, WLB-LLM introduces a novel fine-grained per-document sharding strategy, ensuring each worker within a context parallelism group has an identical workload. Comprehensive experiments under different model scales demonstrate that WLB-LLM significantly mitigates the workload imbalance during 4D parallelism LLM training and achieves an average speedup of 1.23x when applying WLB-LLM in our internal LLM training framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17924v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zheng Wang, Anna Cai, Xinfeng Xie, Zaifeng Pan, Yue Guan, Weiwei Chu, Jie Wang, Shikai Li, Jianyu Huang, Chris Cai, Yuchen Hao, Yufei Ding</dc:creator>
    </item>
    <item>
      <title>Reliable Replication Protocols on SmartNICs</title>
      <link>https://arxiv.org/abs/2503.18093</link>
      <description>arXiv:2503.18093v1 Announce Type: new 
Abstract: Today's datacenter applications rely on datastores that are required to provide high availability, consistency, and performance. To achieve high availability, these datastores replicate data across several nodes. Such replication is managed through a reliable protocol designed to keep the replicas consistent using a consistency model, even in the presence of faults. For several applications, strong consistency models are favored over weaker consistency models, as the former guarantee a more intuitive behavior for clients. Furthermore, to meet the demands of high online traffic, datastores must offer high throughput and low latency.
  However, delivering both strong consistency and high performance simultaneously can be challenging. Reliable replication protocols typically require multiple rounds of communication over the network stack, which introduces latency and increases the load on network resources. Moreover, these protocols consume considerable CPU resources, which impacts the overall performance of applications, especially in high-throughput environments.
  In this work, we aim to design a hardware-accelerated system for replication protocols to address these challenges. We approach offloading the replication protocol onto SmartNICs, which are specialized network interface cards that can be programmed to implement custom logic directly on the NIC. By doing so, we aim to enhance performance while preserving strong consistency, all while saving valuable CPU cycles that can be used for applications' logic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18093v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>M. R. Siavash Katebzadeh, Antonios Katsarakis, Boris Grot</dc:creator>
    </item>
    <item>
      <title>INDIGO: Page Migration for Hardware Memory Disaggregation Across a Network</title>
      <link>https://arxiv.org/abs/2503.18140</link>
      <description>arXiv:2503.18140v1 Announce Type: new 
Abstract: Hardware memory disaggregation (HMD) is an emerging technology that enables access to remote memory, thereby creating expansive memory pools and reducing memory underutilization in datacenters. However, a significant challenge arises when accessing remote memory over a network: increased contention that can lead to severe application performance degradation. To reduce the performance penalty of using remote memory, the operating system uses page migration to promote frequently accessed pages closer to the processor. However, previously proposed page migration mechanisms do not achieve the best performance in HMD systems because of obliviousness to variable page transfer costs that occur due to network contention. To address these limitations, we present INDIGO: a network-aware page migration framework that uses novel page telemetry and a learning-based approach for network adaptation. We implemented INDIGO in the Linux kernel and evaluated it with common cloud and HPC applications on a real disaggregated memory system prototype. Our evaluation shows that INDIGO offers up to 50-70% improvement in application performance compared to other state-of-the-art page migration policies and reduces network traffic up to 2x.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18140v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Archit Patke, Christian Pinto, Saurabh Jha, Haoran Qiu, Zbigniew Kalbarczyk, Ravishankar Iyer</dc:creator>
    </item>
    <item>
      <title>Accelerating Sparse MTTKRP for Small Tensor Decomposition on GPU</title>
      <link>https://arxiv.org/abs/2503.18198</link>
      <description>arXiv:2503.18198v1 Announce Type: new 
Abstract: Sparse Matricized Tensor Times Khatri-Rao Product (spMTTKRP) is the bottleneck kernel of sparse tensor decomposition. In tensor decomposition, spMTTKRP is performed iteratively along all the modes of an input tensor. In this work, we propose a mode-specific tensor layout on GPU that uses multiple tensor copies, where each copy is optimized for a specific mode. The proposed tensor layout increases the data locality of external memory accesses and eliminates the intermediate values communicated between the GPU thread blocks and the GPU global memory. We also propose a tensor partitioning scheme to optimally distribute the total computations among GPU streaming multiprocessors based on the sparsity and the dimensions of the input tensor. Our approach achieves a geometric mean speedup of 2.4x, 7.9x, and 8.9x in total execution time compared with the state-of-the-art GPU baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18198v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sasindu Wijeratne, Rajgopal Kannan, Viktor Prasanna</dc:creator>
    </item>
    <item>
      <title>Predictive Performance of Photonic SRAM-based In-Memory Computing for Tensor Decomposition</title>
      <link>https://arxiv.org/abs/2503.18206</link>
      <description>arXiv:2503.18206v1 Announce Type: new 
Abstract: Photonics-based in-memory computing systems have demonstrated a significant speedup over traditional transistor-based systems because of their ultra-fast operating frequencies and high data bandwidths. Photonic static random access memory (pSRAM) is a crucial component for achieving the objective of ultra-fast photonic in-memory computing systems. In this work, we model and evaluate the performance of a novel photonic SRAM array architecture in development. Additionally, we examine hyperspectral operation through wavelength division multiplexing (WDM) to enhance the throughput of the pSRAM array. We map Matricized Tensor Times Khatri-Rao Product (MTTKRP), a computational kernel commonly used in tensor decomposition, to the proposed pSRAM array architecture. We also develop a predictive performance model to estimate the sustained performance of different configurations of the pSRAM array. Using the predictive performance model, we demonstrate that the pSRAM array achieves 17 PetaOps while performing MTTKRP in a practical hardware configuration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18206v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sasindu Wijeratne, Sugeet Sunder, Md Abdullah-Al Kaiser, Akhilesh Jaiswal, Clynn Mathew, Ajey P. Jacob, Viktor Prasanna</dc:creator>
    </item>
    <item>
      <title>Risk Management for Distributed Arbitrage Systems: Integrating Artificial Intelligence</title>
      <link>https://arxiv.org/abs/2503.18265</link>
      <description>arXiv:2503.18265v1 Announce Type: new 
Abstract: Effective risk management solutions become absolutely crucial when financial markets embrace distributed technology and decentralized financing (DeFi). This study offers a thorough survey and comparative analysis of the integration of artificial intelligence (AI) in risk management for distributed arbitrage systems. We examine several modern caching techniques namely in memory caching, distributed caching, and proxy caching and their functions in enhancing performance in decentralized settings. Through literature review we examine the utilization of AI techniques for alleviating risks related to market volatility, liquidity challenges, operational failures, regulatory compliance, and security threats. This comparison research evaluates various case studies from prominent DeFi technologies, emphasizing critical performance metrics like latency reduction, load balancing, and system resilience. Additionally, we examine the problems and trade offs associated with these technologies, emphasizing their effects on consistency, scalability, and fault tolerance. By meticulously analyzing real world applications, specifically centering on the Aave platform as our principal case study, we illustrate how the purposeful amalgamation of AI with contemporary caching methodologies has revolutionized risk management in distributed arbitrage systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18265v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Akaash Vishal Hazarika, Mahak Shah, Swapnil Patil, Pradyumna Shukla</dc:creator>
    </item>
    <item>
      <title>Jenga: Effective Memory Management for Serving LLM with Heterogeneity</title>
      <link>https://arxiv.org/abs/2503.18292</link>
      <description>arXiv:2503.18292v1 Announce Type: new 
Abstract: Large language models (LLMs) are widely used but expensive to run, especially as inference workloads grow. To lower costs, maximizing the request batch size by managing GPU memory efficiently is crucial. While PagedAttention has recently been proposed to improve the efficiency of memory management, we find that the growing heterogeneity in the embeddings dimensions, attention, and access patterns of modern LLM architectures introduces new challenges for memory allocation.
  In this paper, we present Jenga, a novel memory allocation framework for heterogeneous embeddings in LLMs. Jenga tackles two key challenges: (1) minimizing memory fragmentation when managing embeddings of different sizes, and (2) enabling flexible caching and eviction policies tailored to the specific token-dependency patterns of various layers. Jenga employs a two-level memory allocator, leveraging the least common multiple (LCM) of embedding sizes to optimize memory usage and providing APIs to express layer-specific caching logic to enhance memory reuse.
  We implemente Jenga on vLLM, a state-of-the-art LLM inference engine, and evaluate it with diverse LLMs, datasets, and GPU configurations. Evaluations show that Jenga improves GPU memory utilization by up to 79.6%, and increases serving throughput by up to 4.92x (1.80x on average).</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18292v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chen Zhang, Kuntai Du, Shu Liu, Woosuk Kwon, Xiangxi Mo, Yufeng Wang, Xiaoxuan Liu, Kaichao You, Zhuohan Li, Mingsheng Long, Jidong Zhai, Joseph Gonzalez, Ion Stoica</dc:creator>
    </item>
    <item>
      <title>ED-DAO: Energy Donation Algorithms based on Decentralized Autonomous Organization</title>
      <link>https://arxiv.org/abs/2503.18424</link>
      <description>arXiv:2503.18424v1 Announce Type: new 
Abstract: Energy is a fundamental component of modern life, driving nearly all aspects of daily activities. As such, the inability to access energy when needed is a significant issue that requires innovative solutions. In this paper, we propose ED-DAO, a novel fully transparent and community-driven decentralized autonomous organization (DAO) designed to facilitate energy donations. We analyze the energy donation process by exploring various approaches and categorizing them based on both the source of donated energy and funding origins. We propose a novel Hybrid Energy Donation (HED) algorithm, which enables contributions from both external and internal donors. External donations are payments sourced from entities such as charities and organizations, where energy is sourced from the utility grid and prosumers. Internal donations, on the other hand, come from peer contributors with surplus energy. HED prioritizes donations in the following sequence: peer-sourced energy (P2D), utilitygrid-sourced energy (UG2D), and direct energy donations by peers (P2PD). By merging these donation approaches, the HED algorithm increases the volume of donated energy, providing a more effective means to address energy poverty. Experiments were conducted on a dataset to evaluate the effectiveness of the proposed method. The results showed that HED increased the total donated energy by at least 0.43% (64 megawatts) compared to the other algorithms (UG2D, P2D, and P2PD).</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18424v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdulrezzak Zekiye, Ouns Bouachir, \"Oznur \"Ozkasap, Moayad Aloqaily</dc:creator>
    </item>
    <item>
      <title>AES-SpMM: Balancing Accuracy and Speed by Adaptive Edge Sampling Strategy to Accelerate SpMM in GNNs</title>
      <link>https://arxiv.org/abs/2503.18427</link>
      <description>arXiv:2503.18427v1 Announce Type: new 
Abstract: Coordinating the design of sampling and sparse-dense matrix multiplication (SpMM) is crucial for accelerating graph neural networks (GNNs). However, due to irrational sampling strategies, existing methods face a trade-off between accuracy and speed. Moreover, as computational optimizations progress, data loading has gradually become the primary bottleneck in GNN inference. To address these issues, we propose AES-SpMM, an adaptive edge sampling SpMM kernel. It considers the relationship between the number of non-zero elements in each matrix row and the shared memory width. The edge sampling scheme is adaptively selected according to the different situations of each row. AES-SpMM reduces the graph size through adaptive edge sampling to fit the GPU's shared memory, lowering the computational cost and enhancing data locality, thus balancing the accuracy and speed of GNN inference. Additionally, we introduce a quantization-based AES-SpMM, which applies quantization and dequantization to feature data in GNNs. This approach significantly reduces data loading time while keeping accuracy loss negligible. We evaluated AES-SpMM with common GNN models and datasets. The results show that AES-SpMM outperforms both the cuSPARSE SpMM kernel and GE-SpMM by up to 25.87 times and 23.01 times, respectively, with less than 1% accuracy loss. Compared to ES-SpMM, it reduces accuracy loss by 3.4% on average , achieving a 1.31 times speedup. Compared to AES-SpMM, quantization-based AES-SpMM has a maximum accuracy loss of 0.3% and feature data loading time overhead is reduced by 50.91%-70.51%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18427v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingchen Song, Yaobin Wang, Yi Luo, Huan Wu, Pingping Tang</dc:creator>
    </item>
    <item>
      <title>Monte Cimone v2: Down the Road of RISC-V High-Performance Computers</title>
      <link>https://arxiv.org/abs/2503.18543</link>
      <description>arXiv:2503.18543v2 Announce Type: new 
Abstract: Many RISC-V platforms and SoCs have been announced in recent years targeting the HPC sector, but only a few of them are commercially available and engineered to fit the HPC requirements. The Monte Cimone project targeted assessing their capabilities and maturity, aiming to make RISC-V a competitive choice when building a datacenter. Nowadays, RV SoCs with vector extension, form factor and memory capacity suitable for HPC applications are available in the market, but it is unclear how compilers and open-source libraries can take advantage of its performance. In this paper, we describe the performance assessment of the upgrade of the Monte Cimone (MCv2) cluster with the Sophgo SG2042 processor's HPC operations. The upgrade increases the attained node's performance by 127x on HPL DP FLOP/s and 69x on Stream Memory Bandwidth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18543v2</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emanuele Venieri, Simone Manoni, Giacomo Madella, Federico Ficarelli, Daniele Gregori, Daniele Cesarini, Luca Benini, Andrea Bartolini</dc:creator>
    </item>
    <item>
      <title>Efficient Distributed Algorithms for Shape Reduction via Reconfigurable Circuits</title>
      <link>https://arxiv.org/abs/2503.18663</link>
      <description>arXiv:2503.18663v2 Announce Type: new 
Abstract: In this paper, we study the problem of efficiently reducing geometric shapes into other such shapes in a distributed setting through size-changing operations. We develop distributed algorithms using the reconfigurable circuit model to enable fast node-to-node communication. Our study considers two graph update models: the connectivity model and the adjacency model. Let $n$ denote the number of nodes and $k$ the number of turning points in the initial shape. In the connectivity model, we show that the system of nodes can reduce itself from any tree to a single node using only shrinking operations in $O(k \log n)$ rounds w.h.p. and any tree to its minimal (incompressible) form in $O(\log n)$ rounds with additional knowledge or $O(k \log n)$ without, w.h.p. We also give an algorithm to transform any tree to any topologically equivalent tree in $O(k \log n+\log^2 n)$ rounds w.h.p. if both shrinking and growth operations are available to the nodes. On the negative side, we show that one cannot hope for $o(\log^2 n)$-round transformations for all shapes of $O(\log n)$ turning points: for all reasonable values of $k$, there exists a pair of geometrically equivalent paths of $k$ turning points each, such that $\Omega(k\log n)$ rounds are required to reduce one to the other. In the adjacency model, we show that the system can reduce itself from any connected shape to a single node using only shrinking in $O(\log n)$ rounds w.h.p.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18663v2</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nada Almalki, Siddharth Gupta, Othon Michail, Andreas Padalkin</dc:creator>
    </item>
    <item>
      <title>A Generative Caching System for Large Language Models</title>
      <link>https://arxiv.org/abs/2503.17603</link>
      <description>arXiv:2503.17603v1 Announce Type: cross 
Abstract: Caching has the potential to be of significant benefit for accessing large language models (LLMs) due to their high latencies which typically range from a small number of seconds to well over a minute. Furthermore, many LLMs charge money for queries; caching thus has a clear monetary benefit. This paper presents a new caching system for improving user experiences with LLMs. In addition to reducing both latencies and monetary costs for accessing LLMs, our system also provides important features that go beyond the performance benefits typically associated with caches. A key feature we provide is generative caching, wherein multiple cached responses can be synthesized to provide answers to queries which have never been seen before. Our generative caches function as repositories of valuable information which can be mined and analyzed. We also improve upon past semantic caching techniques by tailoring the caching algorithms to optimally balance cost and latency reduction with the quality of responses provided. Performance tests indicate that our caches are considerably faster than GPTcache.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17603v1</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arun Iyengar, Ashish Kundu, Ramana Kompella, Sai Nandan Mamidi</dc:creator>
    </item>
    <item>
      <title>Sense4FL: Vehicular Crowdsensing Enhanced Federated Learning for Autonomous Driving</title>
      <link>https://arxiv.org/abs/2503.17697</link>
      <description>arXiv:2503.17697v1 Announce Type: cross 
Abstract: To accommodate constantly changing road conditions, real-time model training is essential for autonomous driving (AD). Federated learning (FL) serves as a promising paradigm to enable autonomous vehicles to train models collaboratively with their onboard computing resources. However, existing vehicle selection schemes for FL all assume predetermined and location-independent vehicles' datasets, neglecting the fact that vehicles collect training data along their routes, thereby resulting in suboptimal vehicle selection. To improve the perception quality in AD for a region, we propose Sense4FL, a vehicular crowdsensing-enhanced FL framework featuring trajectory-dependent vehicular training data collection. To this end, we first derive the convergence bound of FL by considering the impact of both vehicles' uncertain trajectories and uploading probabilities, from which we discover that minimizing the training loss is equivalent to minimizing a weighted sum of local and global earth mover's distance (EMD) between vehicles' collected data distribution and global data distribution. Based on this observation, we formulate the trajectory-dependent vehicle selection and data collection problem for FL in AD. Given that the problem is NP-hard, we develop an efficient algorithm to find the solution with an approximation guarantee. Extensive simulation results have demonstrated the effectiveness of our approach in improving object detection performance compared with existing benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17697v1</guid>
      <category>cs.RO</category>
      <category>cs.DC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yanan Ma, Senkang Hu, Zhengru Fang, Yun Ji, Yiqin Deng, Yuguang Fang</dc:creator>
    </item>
    <item>
      <title>Bridging Emotions and Architecture: Sentiment Analysis in Modern Distributed Systems</title>
      <link>https://arxiv.org/abs/2503.18260</link>
      <description>arXiv:2503.18260v1 Announce Type: cross 
Abstract: Sentiment analysis is a field within NLP that has gained importance because it is applied in various areas such as; social media surveillance, customer feedback evaluation and market research. At the same time, distributed systems allow for effective processing of large amounts of data. Therefore, this paper examines how sentiment analysis converges with distributed systems by concentrating on different approaches, challenges and future investigations. Furthermore, we do an extensive experiment where we train sentiment analysis models using both single node configuration and distributed architecture to bring out the benefits and shortcomings of each method in terms of performance and accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18260v1</guid>
      <category>cs.CL</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Mahak Shah, Akaash Vishal Hazarika, Meetu Malhotra, Sachin C. Patil, Joshit Mohanty</dc:creator>
    </item>
    <item>
      <title>\~Optimal Fault-Tolerant Labeling for Reachability and Approximate Distances in Directed Planar Graphs</title>
      <link>https://arxiv.org/abs/2503.18474</link>
      <description>arXiv:2503.18474v1 Announce Type: cross 
Abstract: We present a labeling scheme that assigns labels of size $\tilde O(1)$ to the vertices of a directed weighted planar graph $G$, such that for any fixed $\varepsilon&gt;0$ from the labels of any three vertices $s$, $t$ and $f$ one can determine in $\tilde O(1)$ time a $(1+\varepsilon)$-approximation of the $s$-to-$t$ distance in the graph $G\setminus\{f\}$. For approximate distance queries, prior to our work, no efficient solution existed, not even in the centralized oracle setting. Even for the easier case of reachability, $\tilde O(1)$ queries were known only with a centralized oracle of size $\tilde O(n)$ [SODA 21].</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18474v1</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Itai Boneh, Shiri Chechik, Shay Golan, Shay Mozes, Oren Weimann</dc:creator>
    </item>
    <item>
      <title>Efficient Computation in Congested Anonymous Dynamic Networks</title>
      <link>https://arxiv.org/abs/2301.07849</link>
      <description>arXiv:2301.07849v5 Announce Type: replace 
Abstract: An anonymous dynamic network is a network of indistinguishable processes whose communication links may appear or disappear unpredictably over time. Previous research has shown that deterministically computing an arbitrary function of a multiset of input values given to these processes takes only a linear number of communication rounds (Di Luna-Viglietta, FOCS 2022).
  However, fast algorithms for anonymous dynamic networks rely on the construction and transmission of large data structures called "history trees", whose size is polynomial in the number of processes. This approach is unfeasible if the network is congested, and only messages of logarithmic size can be sent through its links. Observe that sending a large message piece by piece over several rounds is not in itself a solution, due to the anonymity of the processes combined with the dynamic nature of the network. Moreover, it is known that certain basic tasks such as all-to-all token dissemination (by means of single-token forwarding) require $\Omega(n^2/\log n)$ rounds in congested networks (Dutta et al., SODA 2013).
  In this work, we develop a series of practical and efficient techniques that make it possible to use history trees in congested anonymous dynamic networks. Among other applications, we show how to compute arbitrary functions in such networks in $O(n^3)$ communication rounds, greatly improving upon previous state-of-the-art algorithms for congested networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.07849v5</guid>
      <category>cs.DC</category>
      <category>cs.DM</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giuseppe A. Di Luna, Giovanni Viglietta</dc:creator>
    </item>
    <item>
      <title>RTop-K: Ultra-Fast Row-Wise Top-K Selection for Neural Network Acceleration on GPUs</title>
      <link>https://arxiv.org/abs/2409.00822</link>
      <description>arXiv:2409.00822v3 Announce Type: replace 
Abstract: Top-k selection algorithms are fundamental in a wide range of applications, including high-performance computing, information retrieval, big data processing, and neural network model training. In this paper, we present RTop-K, a highly efficient parallel row-wise top-k selection algorithm specifically designed for GPUs. RTop-K leverages a binary search-based approach to optimize row-wise top-k selection, providing a scalable and accelerated solution. We conduct a detailed analysis of early stopping in our algorithm, showing that it effectively maintains the testing accuracy of neural network models while substantially improving performance. Our GPU implementation of RTop-K demonstrates superior performance over state-of-the-art row-wise top-k GPU implementations, achieving an average speed-up of up to 11.49$\times$ with early stopping and 7.29$\times$ without early stopping. Moreover, RTop-K accelerates the overall training workflow of MaxK-GNNs, delivering speed-ups ranging from 11.97% to 33.29% across different models and datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00822v3</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xi Xie, Yuebo Luo, Hongwu Peng, Caiwen Ding</dc:creator>
    </item>
    <item>
      <title>Debunking the CUDA Myth Towards GPU-based AI Systems</title>
      <link>https://arxiv.org/abs/2501.00210</link>
      <description>arXiv:2501.00210v2 Announce Type: replace 
Abstract: This paper presents a comprehensive evaluation of Intel Gaudi NPUs as an alternative to NVIDIA GPUs, which is currently the de facto standard in AI system design. First, we create a suite of microbenchmarks to compare Intel Gaudi-2 with NVIDIA A100, showing that Gaudi-2 achieves competitive performance not only in primitive AI compute, memory, and communication operations but also in executing several important AI workloads end-to-end. We then assess Gaudi NPU's programmability by discussing several software-level optimization strategies to employ for implementing critical FBGEMM operators and vLLM, evaluating their efficiency against GPU-optimized counterparts. Results indicate that Gaudi-2 achieves energy efficiency comparable to A100, though there are notable areas for improvement in terms of software maturity. Overall, we conclude that, with effective integration into high-level AI frameworks, Gaudi NPUs could challenge NVIDIA GPU's dominance in the AI server market, though further improvements are necessary to fully compete with NVIDIA's robust software ecosystem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00210v2</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yunjae Lee, Juntaek Lim, Jehyeon Bang, Eunyeong Cho, Huijong Jeong, Taesu Kim, Hyungjun Kim, Joonhyung Lee, Jinseop Im, Ranggi Hwang, Se Jung Kwon, Dongsoo Lee, Minsoo Rhu</dc:creator>
    </item>
    <item>
      <title>Characterizing GPU Resilience and Impact on AI/HPC Systems</title>
      <link>https://arxiv.org/abs/2503.11901</link>
      <description>arXiv:2503.11901v2 Announce Type: replace 
Abstract: In this study, we characterize GPU failures in Delta, the current large-scale AI system with over 600 petaflops of peak compute throughput. The system comprises GPU and non-GPU nodes with modern AI accelerators, such as NVIDIA A40, A100, and H100 GPUs. The study uses two and a half years of data on GPU errors. We evaluate the resilience of GPU hardware components to determine the vulnerability of different GPU components to failure and their impact on the GPU and node availability. We measure the key propagation paths in GPU hardware, GPU interconnect (NVLink), and GPU memory. Finally, we evaluate the impact of the observed GPU errors on user jobs. Our key findings are: (i) Contrary to common beliefs, GPU memory is over 30x more reliable than GPU hardware in terms of MTBE (mean time between errors). (ii) The newly introduced GSP (GPU System Processor) is the most vulnerable GPU hardware component. (iii) NVLink errors did not always lead to user job failure, and we attribute it to the underlying error detection and retry mechanisms employed. (iv) We show multiple examples of hardware errors originating from one of the key GPU hardware components, leading to application failure. (v) We project the impact of GPU node availability on larger scales with emulation and find that significant overprovisioning between 5-20% would be necessary to handle GPU failures. If GPU availability were improved to 99.9%, the overprovisioning would be reduced by 4x.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11901v2</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shengkun Cui, Archit Patke, Ziheng Chen, Aditya Ranjan, Hung Nguyen, Phuong Cao, Saurabh Jha, Brett Bode, Gregory Bauer, Chandra Narayanaswami, Daby Sow, Catello Di Martino, Zbigniew T. Kalbarczyk, Ravishankar K. Iyer</dc:creator>
    </item>
    <item>
      <title>Emulating Full Participation: An Effective and Fair Client Selection Strategy for Federated Learning</title>
      <link>https://arxiv.org/abs/2405.13584</link>
      <description>arXiv:2405.13584v2 Announce Type: replace-cross 
Abstract: In federated learning, client selection is a critical problem that significantly impacts both model performance and fairness. Prior studies typically treat these two objectives separately, or balance them using simple weighting schemes. However, we observe that commonly used metrics for model performance and fairness often conflict with each other, and a straightforward weighted combination is insufficient to capture their complex interactions. To address this, we first propose two guiding principles that directly tackle the inherent conflict between the two metrics while reinforcing each other. Based on these principles, we formulate the client selection problem as a long-term optimization task, leveraging the Lyapunov function and the submodular nature of the problem to solve it effectively. Experiments show that the proposed method improves both model performance and fairness, guiding the system to converge comparably to full client participation. This improvement can be attributed to the fact that both model performance and fairness benefit from the diversity of the selected clients' data distributions. Our approach adaptively enhances this diversity by selecting clients based on their data distributions, thereby improving both model performance and fairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13584v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qingming Li, Juzheng Miao, Puning Zhao, Li Zhou, H. Vicky Zhao, Shouling Ji, Bowen Zhou, Furui Liu</dc:creator>
    </item>
    <item>
      <title>Approximating Spanning Centrality with Random Bouquets</title>
      <link>https://arxiv.org/abs/2410.14056</link>
      <description>arXiv:2410.14056v2 Announce Type: replace-cross 
Abstract: Spanning Centrality is a measure used in network analysis to determine the importance of an edge in a graph based on its contribution to the connectivity of the entire network. Specifically, it quantifies how critical an edge is in terms of the number of spanning trees that include that edge. The current state-of-the-art for All Edges Spanning Centrality~(AESC), which computes the exact centrality values for all the edges, has a time complexity of $\mathcal{O}(mn^{3/2})$ for $n$ vertices and $m$ edges. This makes the computation infeasible even for moderately sized graphs. Instead, there exist approximation algorithms which process a large number of random walks to estimate edge centralities. However, even the approximation algorithms can be computationally overwhelming, especially if the approximation error bound is small. In this work, we propose a novel, hash-based sampling method and a vectorized algorithm which greatly improves the execution time by clustering random walks into {\it Bouquets}. On synthetic random walk benchmarks, {\it Bouquets} performs $7.8\times$ faster compared to naive, traditional random-walk generation. We also show that the proposed technique is scalable by employing it within a state-of-the-art AESC approximation algorithm, {\sc TGT+}. The experiments show that using Bouquets yields more than $100\times$ speed-up via parallelization with 16 threads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14056v2</guid>
      <category>cs.SI</category>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>G\"okhan G\"okt\"urk, Kamer Kaya</dc:creator>
    </item>
    <item>
      <title>Parallel $k$-Core Decomposition: Theory and Practice</title>
      <link>https://arxiv.org/abs/2502.08042</link>
      <description>arXiv:2502.08042v2 Announce Type: replace-cross 
Abstract: This paper proposes efficient solutions for $k$-core decomposition with high parallelism. The problem of $k$-core decomposition is fundamental in graph analysis and has applications across various domains. However, existing algorithms face significant challenges in achieving work-efficiency in theory and/or high parallelism in practice, and suffer from various performance bottlenecks.
  We present a simple, work-efficient parallel framework for $k$-core decomposition that is easy to implement and adaptable to various strategies for improving work-efficiency. We introduce two techniques to enhance parallelism: a sampling scheme to reduce contention on high-degree vertices, and vertical granularity control (VGC) to mitigate scheduling overhead for low-degree vertices. Furthermore, we design a hierarchical bucket structure to optimize performance for graphs with high coreness values.
  We evaluate our algorithm on a diverse set of real-world and synthetic graphs. Compared to state-of-the-art parallel algorithms, including ParK, PKC, and Julienne, our approach demonstrates superior performance on 23 out of 25 graphs when tested on a 96-core machine. Our algorithm shows speedups of up to 315$\times$ over ParK, 33.4$\times$ over PKC, and 52.5$\times$ over Julienne.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08042v2</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Youzhe Liu, Xiaojun Dong, Yan Gu, Yihan Sun</dc:creator>
    </item>
    <item>
      <title>Towards Seamless Hierarchical Federated Learning under Intermittent Client Participation: A Stagewise Decision-Making Methodology</title>
      <link>https://arxiv.org/abs/2502.09303</link>
      <description>arXiv:2502.09303v2 Announce Type: replace-cross 
Abstract: Federated Learning (FL) offers a pioneering distributed learning paradigm that enables devices/clients to build a shared global model. This global model is obtained through frequent model transmissions between clients and a central server, which may cause high latency, energy consumption, and congestion over backhaul links. To overcome these drawbacks, Hierarchical Federated Learning (HFL) has emerged, which organizes clients into multiple clusters and utilizes edge nodes (e.g., edge servers) for intermediate model aggregations between clients and the central server. Current research on HFL mainly focus on enhancing model accuracy, latency, and energy consumption in scenarios with a stable/fixed set of clients. However, addressing the dynamic availability of clients -- a critical aspect of real-world scenarios -- remains underexplored. This study delves into optimizing client selection and client-to-edge associations in HFL under intermittent client participation so as to minimize overall system costs (i.e., delay and energy), while achieving fast model convergence. We unveil that achieving this goal involves solving a complex NP-hard problem. To tackle this, we propose a stagewise methodology that splits the solution into two stages, referred to as Plan A and Plan B. Plan A focuses on identifying long-term clients with high chance of participation in subsequent model training rounds. Plan B serves as a backup, selecting alternative clients when long-term clients are unavailable during model training rounds. This stagewise methodology offers a fresh perspective on client selection that can enhance both HFL and conventional FL via enabling low-overhead decision-making processes. Through evaluations on MNIST and CIFAR-10 datasets, we show that our methodology outperforms existing benchmarks in terms of model accuracy and system costs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09303v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minghong Wu, Minghui Liwang, Yuhan Su, Li Li, Seyyedali Hosseinalipour, Xianbin Wang, Huaiyu Dai, Zhenzhen Jiao</dc:creator>
    </item>
    <item>
      <title>ILVES: Accurate and efficient bond length and angle constraints in molecular dynamics</title>
      <link>https://arxiv.org/abs/2503.13075</link>
      <description>arXiv:2503.13075v2 Announce Type: replace-cross 
Abstract: Force field-based molecular dynamics simulations are customarily carried out by constraining internal degrees of freedom. The de facto state-of-the-art algorithms for this purpose, SHAKE, LINCS and P-LINCS, converge slowly, impeding high-accuracy calculations and limiting the realism of simulations. Furthermore, LINCS and P-LINCS cannot handle general angular constraints, which restricts increasing the time step.
  In this paper, we introduce ILVES, a set of parallel algorithms that converge so rapidly that it is now practical to solve bond length and associated angular constraint equations as accurately as the hardware will allow. We have integrated our work into Gromacs and our analysis demonstrates that, in most cases, our software is superior to the state-of-the-art. We anticipate that ILVES will allow for an increase in the time step, thus accelerating contemporary calculations by a factor of at least 2. This will allow the scientific community to increase the range of phenomena that can therefore be simulated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13075v2</guid>
      <category>physics.chem-ph</category>
      <category>cs.DC</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lori\'en L\'opez-Villellas, Carl Christian Kjelgaard Mikkelsen, Juan Jos\'e Galano-Frutos, Santiago Marco-Sola, Jes\'us Alastruey-Bened\'e, Pablo Ib\'a\~nez, Miquel Moret\'o, Maria Cristina De Rosa, Pablo Garc\'ia-Risue\~no</dc:creator>
    </item>
  </channel>
</rss>
