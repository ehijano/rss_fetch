<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 17 Dec 2025 05:00:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Finding $b$-colorings Using Feedback Edges</title>
      <link>https://arxiv.org/abs/2512.14390</link>
      <description>arXiv:2512.14390v1 Announce Type: new 
Abstract: A $b$-coloring of a graph is a proper vertex coloring such that each color class contains a vertex that sees all other colors in its neighborhood. The $b$-coloring problem, in which the task is to decide whether a graph admits a $b$-coloring with $k$ colors, is NP-complete in general but polytime solvable on trees. Moreover, it is known that $b$-coloring is in XP but W[$t$]-hard for all $t \in \mathbb{N}$ when parameterized by tree-width. In fact, only very few parameters, such as the vertex cover number, were known to admit an FPT algorithm for $b$-coloring. In this paper, we consider a more restrictive parameter measuring similarity to trees than tree-width, namely the feedback edge number, and show that $b$-coloring is fixed-parameter tractable under this parameterization. Our algorithm combines standard techniques used in parameterized algorithmics with the problem-specific ideas used in the polytime algorithm for trees. In addition, we present an FPT algorithm for $b$-coloring parameterized by distance to co-cluster, which is a parameter measuring similarity to complete multipartite graphs. Finally, we make several observations based on known results, including that $b$-coloring is W[$1$]-hard when parameterized by tree-depth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14390v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jakub Balab\'an</dc:creator>
    </item>
    <item>
      <title>Cost-Free Neutrality for the River Method</title>
      <link>https://arxiv.org/abs/2512.14409</link>
      <description>arXiv:2512.14409v1 Announce Type: new 
Abstract: Recently, the River Method was introduced as novel refinement of the Split Cycle voting rule.
  The decision-making process of River is closely related to the well established Ranked Pairs Method.
  Both methods consider a margin graph computed from the voters' preferences and eliminate majority cycles in that graph to choose a winner.
  As ties can occur in the margin graph, a tiebreaker is required along with the preferences.
  While such a tiebreaker makes the computation efficient, it compromises the fundamental property of neutrality: the voting rule should not favor alternatives in advance.
  One way to reintroduce neutrality is to use Parallel-Universe Tiebreaking (PUT), where each alternative is a winner if it wins according to any possible tiebreaker.
  Unfortunately, computing the winners selected by Ranked Pairs with PUT is NP-complete.
  Given the similarity of River to Ranked Pairs, one might expect River to suffer from the same complexity.
  Surprisingly, we show the opposite:
  We present a polynomial-time algorithm for computing River winners with PUT, highlighting significant structural advantages of River over Ranked Pairs.
  Our Fused-Universe (FUN) algorithm simulates River for every possible tiebreaking in one pass.
  From the resulting FUN diagram one can then directly read off both the set of winners and, for each winner, a certificate that explains how this alternative dominates the others.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14409v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michelle D\"oring, Jannes Malanowski, Stefan Neubert</dc:creator>
    </item>
    <item>
      <title>An Improved Approximation Algorithm for Maximum Weight 3-Path Packing</title>
      <link>https://arxiv.org/abs/2512.14457</link>
      <description>arXiv:2512.14457v1 Announce Type: new 
Abstract: Given a complete graph with $n$ vertices and non-negative edge weights, where $n$ is divisible by 3, the maximum weight 3-path packing problem is to find a set of $n/3$ vertex-disjoint 3-paths such that the total weight of the 3-paths in the packing is maximized. This problem is closely related to the classic maximum weight matching problem. In this paper, we propose a $10/17$-approximation algorithm, improving the best-known $7/12$-approximation algorithm (ESA 2015). Our result is obtained by making a trade-off among three algorithms. The first is based on the maximum weight matching of size $n/2$, the second is based on the maximum weight matching of size $n/3$, and the last is based on an approximation algorithm for star packing. Our first algorithm is the same as the previous $7/12$-approximation algorithm, but we propose a new analysis method -- a charging method -- for this problem, which is not only essential to analyze our second algorithm but also may be extended to analyze algorithms for some related problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14457v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingyang Zhao, Mingyu Xiao</dc:creator>
    </item>
    <item>
      <title>Towards Lazy B-Trees</title>
      <link>https://arxiv.org/abs/2507.00277</link>
      <description>arXiv:2507.00277v2 Announce Type: replace 
Abstract: Lazy search trees (Sandlund &amp; Wild FOCS 2020, Sandlund &amp; Zhang SODA 2022) are sorted dictionaries whose update and query performance smoothly interpolates between that of efficient priority queues and binary search trees - automatically, depending on actual use; no adjustments are necessary to the data structure to realize the cost savings. In this paper, we design lazy B-trees, a variant of lazy search trees suitable for external memory that generalizes the speedup of B-trees over binary search trees wrt. input/output operations to the same smooth interpolation regime.
  A key technical difficulty to overcome is the lack of a (fully satisfactory) external variant of biased search trees, on which lazy search trees crucially rely. We give a construction for a subset of performance guarantees sufficient to realize external-memory lazy search trees, which we deem of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00277v2</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4230/LIPIcs.MFCS.2025.87</arxiv:DOI>
      <dc:creator>Casper Moldrup Rysgaard, Sebastian Wild</dc:creator>
    </item>
    <item>
      <title>Fast-Convergent Proximity Graphs for Approximate Nearest Neighbor Search</title>
      <link>https://arxiv.org/abs/2510.05975</link>
      <description>arXiv:2510.05975v4 Announce Type: replace 
Abstract: Approximate nearest neighbor (ANN) search in high-dimensional metric spaces is a fundamental problem with many applications. Over the past decade, proximity graph (PG)-based indexes have demonstrated superior empirical performance over alternatives. However, these methods often lack theoretical guarantees regarding the quality of query results, especially in the worst-case scenarios. In this paper, we introduce the {\alpha}-convergent graph ({\alpha}-CG), a new PG structure that employs a carefully designed edge pruning rule. This rule eliminates candidate neighbors for each data point p by applying the shifted-scaled triangle inequalities among p, its existing out-neighbors, and new candidates. If the distance between the query point q and its exact nearest neighbor v* is at most {\tau} for some constant {\tau} &gt; 0, our {\alpha}-CG finds the exact nearest neighbor in poly-logarithmic time, assuming bounded intrinsic dimensionality for the dataset; otherwise, it can find an ANN in the same time. To enhance scalability, we develop the {\alpha}-convergent neighborhood graph ({\alpha}-CNG), a practical variant that applies the pruning rule locally within each point's neighbors. We also introduce optimizations to reduce the index construction time. Experimental results show that our {\alpha}-CNG outperforms existing PGs on real-world datasets. For most datasets, {\alpha}-CNG can reduce the number of distance computations and search steps by over 15% and 45%, respectively, when compared with the best-performing baseline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05975v4</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Binhong Li, Xiao Yan, Shangqi Lu</dc:creator>
    </item>
    <item>
      <title>Almost-Optimal Approximation Algorithms for Global Minimum Cut in Directed Graphs</title>
      <link>https://arxiv.org/abs/2512.09080</link>
      <description>arXiv:2512.09080v2 Announce Type: replace 
Abstract: We develop new $(1+\epsilon)$-approximation algorithms for finding the global minimum edge-cut in a directed edge-weighted graph, and for finding the global minimum vertex-cut in a directed vertex-weighted graph. Our algorithms are randomized, and have a running time of $O\left(m^{1+o(1)}/\epsilon\right)$ on any $m$-edge $n$-vertex input graph, assuming all edge/vertex weights are polynomially-bounded. In particular, for any constant $\epsilon&gt;0$, our algorithms have an almost-optimal running time of $O\left(m^{1+o(1)}\right)$. The fastest previously-known running time for this setting, due to (Cen et al., FOCS 2021), is $\tilde{O}\left(\min\left\{n^2/\epsilon^2,m^{1+o(1)}\sqrt{n}\right\}\right)$ for Minimum Edge-Cut, and $\tilde{O}\left(n^2/\epsilon^2\right)$ for Minimum Vertex-Cut. Our results further extend to the rooted variants of the Minimum Edge-Cut and Minimum Vertex-Cut problems, where the algorithm is additionally given a root vertex $r$, and the goal is to find a minimum-weight cut separating any vertex from the root $r$. In terms of techniques, we build upon and extend a framework that was recently introduced by (Chuzhoy et al., SODA 2026) for solving the Minimum Vertex-Cut problem in unweighted directed graphs. Additionally, in order to obtain our result for the Global Minimum Vertex-Cut problem, we develop a novel black-box reduction from this problem to its rooted variant. Prior to our work, such reductions were only known for more restricted settings, such as when all vertex-weights are unit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.09080v2</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ron Mosenzon</dc:creator>
    </item>
    <item>
      <title>Efficient Local and Tabu Search Strategies for Large-Scale Quadratic Integer Programming</title>
      <link>https://arxiv.org/abs/2409.14176</link>
      <description>arXiv:2409.14176v3 Announce Type: replace-cross 
Abstract: This study investigates the area of general quadratic integer programming (QIP), encompassing both unconstrained (UQIP) and constrained (CQIP) variants. These NP-hard problems have far-reaching applications, yet the non-convex cases have received limited attention in the literature. To address this gap, we introduce a closed-form formula for single-variable changes, establishing novel necessary and sufficient conditions for 1-Opt local improvement in UQIP and CQIP. We develop a simple local and sophisticated tabu search with an oscillation strategy tailored for large-scale problems. Experimental results on instances with up to 8000 variables demonstrate the efficiency of these strategies, producing high-quality solutions within a short time. Our approaches significantly outperform the Gurobi 11.0.2 solver.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14176v3</guid>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haibo Wang, Bahram Alidaee</dc:creator>
    </item>
    <item>
      <title>QCircuitBench: A Large-Scale Dataset for Benchmarking Quantum Algorithm Design</title>
      <link>https://arxiv.org/abs/2410.07961</link>
      <description>arXiv:2410.07961v3 Announce Type: replace-cross 
Abstract: Quantum computing is an emerging field recognized for the significant speedup it offers over classical computing through quantum algorithms. However, designing and implementing quantum algorithms pose challenges due to the complex nature of quantum mechanics and the necessity for precise control over quantum states. Despite the significant advancements in AI, there has been a lack of datasets specifically tailored for this purpose. In this work, we introduce QCircuitBench, the first benchmark dataset designed to evaluate AI's capability in designing and implementing quantum algorithms using quantum programming languages. Unlike using AI for writing traditional codes, this task is fundamentally more complicated due to highly flexible design space. Our key contributions include: 1. A general framework which formulates the key features of quantum algorithm design for Large Language Models. 2. Implementations for quantum algorithms from basic primitives to advanced applications, spanning 3 task suites, 25 algorithms, and 120,290 data points. 3. Automatic validation and verification functions, allowing for iterative evaluation and interactive reasoning without human inspection. 4. Promising potential as a training dataset through preliminary fine-tuning results. We observed several interesting experimental phenomena: LLMs tend to exhibit consistent error patterns, and fine-tuning does not always outperform few-shot learning. In all, QCircuitBench is a comprehensive benchmark for LLM-driven quantum algorithm design, and it reveals limitations of LLMs in this domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07961v3</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rui Yang, Ziruo Wang, Yuntian Gu, Tianyi Chen, Yitao Liang, Tongyang Li</dc:creator>
    </item>
    <item>
      <title>Near-Optimal Algorithms for Omniprediction</title>
      <link>https://arxiv.org/abs/2501.17205</link>
      <description>arXiv:2501.17205v3 Announce Type: replace-cross 
Abstract: Omnipredictors are simple prediction functions that encode loss-minimizing predictions with respect to a hypothesis class $H$, simultaneously for every loss function within a class of losses $L$. In this work, we give near-optimal learning algorithms for omniprediction, in both the online and offline settings. To begin, we give an oracle-efficient online learning algorithm that acheives $(L,H)$-omniprediction with $\tilde O (\sqrt{T \log |H|})$ regret for any class of Lipschitz loss functions $L \subseteq L_\mathrm{Lip}$. Quite surprisingly, this regret bound matches the optimal regret for \emph{minimization of a single loss function} (up to a $\sqrt{\log(T)}$ factor). Given this online algorithm, we develop an online-to-offline conversion that achieves near-optimal complexity across a number of measures. In particular, for all bounded loss functions within the class of Bounded Variation losses $L_\mathrm{BV}$ (which include all convex, all Lipschitz, and all proper losses) and any (possibly-infinite) $H$, we obtain an offline learning algorithm that, leveraging an (offline) ERM oracle and $m$ samples from $D$, returns an efficient $(L_{\mathrm{BV}},H,\epsilon(m))$-omnipredictor for $\varepsilon(m)$ scaling near-linearly in the Rademacher complexity of a class derived from $H$ by taking convex combinations of a fixed number of elements of $\mathrm{Th} \circ H$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17205v3</guid>
      <category>stat.ML</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Princewill Okoroafor, Robert Kleinberg, Michael P. Kim</dc:creator>
    </item>
  </channel>
</rss>
