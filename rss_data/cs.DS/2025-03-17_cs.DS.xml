<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 17 Mar 2025 04:00:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Matrix Scaling: a New Heuristic for the Feedback Vertex Set Problem</title>
      <link>https://arxiv.org/abs/2503.10780</link>
      <description>arXiv:2503.10780v1 Announce Type: new 
Abstract: For a digraph $G$, a set $F\subseteq V(G)$ is said to be a feedback vertex set (FVS) if $G-F$ is acyclic. The problem of finding a smallest FVS is NP-hard. We present a matrix scaling technique for finding feedback vertex sets in un-weighted directed graphs that runs in $O(|F|\log(|V|)|V|^{2})$ time. Our technique is empirically shown to produce smaller feedback vertex sets than other known heuristics and in a shorter amount of time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10780v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>James M. Shook, Isabel Beichl</dc:creator>
    </item>
    <item>
      <title>A $(2+\varepsilon)$-Approximation Algorithm for Metric $k$-Median</title>
      <link>https://arxiv.org/abs/2503.10972</link>
      <description>arXiv:2503.10972v1 Announce Type: new 
Abstract: In the classical NP-hard metric $k$-median problem, we are given a set of $n$ clients and centers with metric distances between them, along with an integer parameter $k\geq 1$. The objective is to select a subset of $k$ open centers that minimizes the total distance from each client to its closest open center.
  In their seminal work, Jain, Mahdian, Markakis, Saberi, and Vazirani presented the Greedy algorithm for facility location, which implies a $2$-approximation algorithm for $k$-median that opens $k$ centers in expectation. Since then, substantial research has aimed at narrowing the gap between their algorithm and the best achievable approximation by an algorithm guaranteed to open exactly $k$ centers. During the last decade, all improvements have been achieved by leveraging their algorithm or a small improvement thereof, followed by a second step called bi-point rounding, which inherently increases the approximation guarantee.
  Our main result closes this gap: for any $\epsilon &gt;0$, we present a $(2+\epsilon)$-approximation algorithm for $k$-median, improving the previous best-known approximation factor of $2.613$. Our approach builds on a combination of two algorithms. First, we present a non-trivial modification of the Greedy algorithm that operates with $O(\log n/\epsilon^2)$ adaptive phases. Through a novel walk-between-solutions approach, this enables us to construct a $(2+\epsilon)$-approximation algorithm for $k$-median that consistently opens at most $k + O(\log n{/\epsilon^2})$ centers. Second, we develop a novel $(2+\epsilon)$-approximation algorithm tailored for stable instances, where removing any center from an optimal solution increases the cost by at least an $\Omega(\epsilon^3/\log n)$ fraction. Achieving this involves a sampling approach inspired by the $k$-means++ algorithm and a reduction to submodular optimization subject to a partition matroid.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10972v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vincent Cohen-Addad, Fabrizio Grandoni, Euiwoong Lee, Chris Schwiegelshohn, Ola Svensson</dc:creator>
    </item>
    <item>
      <title>Approximating the Total Variation Distance between Gaussians</title>
      <link>https://arxiv.org/abs/2503.11099</link>
      <description>arXiv:2503.11099v1 Announce Type: new 
Abstract: The total variation distance is a metric of central importance in statistics and probability theory. However, somewhat surprisingly, questions about computing it algorithmically appear not to have been systematically studied until very recently. In this paper, we contribute to this line of work by studying this question in the important special case of multivariate Gaussians. More formally, we consider the problem of approximating the total variation distance between two multivariate Gaussians to within an $\epsilon$-relative error. Previous works achieved a fixed constant relative error approximation via closed-form formulas. In this work, we give algorithms that given any two $n$-dimensional Gaussians $D_1,D_2$, and any error bound $\epsilon &gt; 0$, approximate the total variation distance $D := d_{TV}(D_1,D_2)$ to $\epsilon$-relative accuracy in $\text{poly}(n,\frac{1}{\epsilon},\log \frac{1}{D})$ operations. The main technical tool in our work is a reduction that helps us extend the recent progress on computing the TV-distance between discrete random variables to our continuous setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11099v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arnab Bhattacharyya, Weiming Feng, Piyush Srivastava</dc:creator>
    </item>
    <item>
      <title>Discrete Effort Distribution via Regrettable Greedy Algorithm</title>
      <link>https://arxiv.org/abs/2503.11107</link>
      <description>arXiv:2503.11107v1 Announce Type: new 
Abstract: This paper addresses resource allocation problem with a separable objective function under a single linear constraint, formulated as maximizing $\sum_{j=1}^{n}R_j(x_j)$ subject to $\sum_{j=1}^{n}x_j=k$ and $x_j\in\{0,\dots,m\}$. While classical dynamic programming approach solves this problem in $O(n^2m^2)$ time, we propose a regrettable greedy algorithm that achieves $O(n\log n)$ time when $m=O(1)$. The algorithm significantly outperforms traditional dynamic programming for small $m$. Our algorithm actually solves the problem for all $k~(0\leq k\leq nm)$ in the mentioned time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11107v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Song Cao, Taikun Zhu, Kai Jin</dc:creator>
    </item>
    <item>
      <title>Sum-of-Max Chain Partition of a Tree</title>
      <link>https://arxiv.org/abs/2503.11526</link>
      <description>arXiv:2503.11526v1 Announce Type: new 
Abstract: Path partition problems on trees have found various applications. In this paper, we present an $O(n \log n)$ time algorithm for solving the following variant of path partition problem: given a rooted tree of $n$ nodes $1, \ldots, n$, where vertex $i$ is associated with a weight $w_i$ and a cost $s_i$, partition the tree into several disjoint chains $C_1,\ldots,C_k$, so that the weight of each chain is no more than a threshold $w_0$ and the sum of the largest $s_i$ in each chain is minimized. We also generalize the algorithm to the case where the cost of a chain is determined by the $s_i$ of the vertex with the highest rank in the chain, which can be determined by an arbitrary total order defined on all nodes instead of the value of $s_i$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11526v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruixi Luo, Taikun Zhu, Kai Jin</dc:creator>
    </item>
    <item>
      <title>Finding a Fair Scoring Function for Top-$k$ Selection: Hardness, Algorithms, and Experiments</title>
      <link>https://arxiv.org/abs/2503.11575</link>
      <description>arXiv:2503.11575v1 Announce Type: cross 
Abstract: Selecting a subset of the $k$ "best" items from a dataset of $n$ items, based on a scoring function, is a key task in decision-making. Given the widespread use of automated decision-making software nowadays, it is important that the outcome of this process, called top-$k$ selection, is fair. Here we consider the problem of identifying a linear scoring function for top-$k$ selection that is fair. The function computes a score for each item as a weighted sum of its (numerical) attribute values. Additionally, the function must ensure that the subset selected is a faithful representative of the entire dataset for a minority or historically disadvantaged group. Existing algorithms do not scale effectively on large, high-dimensional datasets. Our theoretical analysis shows that in more than two dimensions, no algorithm is likely to achieve good scalability with respect to dataset size (i.e., a run time of $O(n\cdot \text{polylog}(n))$), and the computational complexity is likely to increase rapidly with dimensionality. However, there are exceptions for small values of $k$ and for this case we provide significantly faster algorithms. We also provide efficient practical variants of these algorithms. Our implementations of these take advantage of modern hardware (e.g., exploiting parallelism). For large values of $k$, we give an alternative algorithm that, while theoretically worse, performs better in practice. Experimental results on real-world datasets demonstrate the efficiency of our proposed algorithms, which achieve speed-ups of up to several orders of magnitude compared to the state of the art (SoTA).</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11575v1</guid>
      <category>cs.DB</category>
      <category>cs.CC</category>
      <category>cs.CY</category>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Guangya Cai</dc:creator>
    </item>
    <item>
      <title>Simple Realizability of Abstract Topological Graphs</title>
      <link>https://arxiv.org/abs/2409.20108</link>
      <description>arXiv:2409.20108v2 Announce Type: replace 
Abstract: An abstract topological graph (AT-graph) is a pair $A=(G,\mathcal{X})$, where $G=(V,E)$ is a graph and $\mathcal{X} \subseteq {E \choose 2}$ is a set of pairs of edges of $G$. A realization of $A$ is a drawing $\Gamma_A$ of $G$ in the plane such that any two edges $e_1,e_2$ of $G$ cross in $\Gamma_A$ if and only if $(e_1,e_2) \in \mathcal{X}$; $\Gamma_A$ is simple if any two edges intersect at most once (either at a common endpoint or at a proper crossing). The AT-graph Realizability (ATR) problem asks whether an input AT-graph admits a realization. The version of this problem that requires a simple realization is called Simple AT-graph Realizability (SATR). It is a classical result that both ATR and SATR are NP-complete.
  In this paper, we study the SATR problem from a new structural perspective. More precisely, we consider the size $\mathrm{\lambda}(A)$ of the largest connected component of the crossing graph of any realization of $A$, i.e., the graph ${\cal C}(A) = (E, \mathcal{X})$. This parameter represents a natural way to measure the level of interplay among edge crossings. First, we prove that SATR is NP-complete when $\mathrm{\lambda}(A) \geq 6$. On the positive side, we give an optimal linear-time algorithm that solves SATR when $\mathrm{\lambda}(A) \leq 3$ and returns a simple realization if one exists. Our algorithm is based on several ingredients, in particular the reduction to a new embedding problem subject to constraints that require certain pairs of edges to alternate (in the rotation system), and a sequence of transformations that exploit the interplay between alternation constraints and the SPQR-tree and PQ-tree data structures to eventually arrive at a simpler embedding problem that can be solved with standard techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20108v2</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giordano Da Lozzo, Walter Didimo, Fabrizio Montecchiani, Miriam M\"unch, Maurizio Patrignani, Ignaz Rutter</dc:creator>
    </item>
    <item>
      <title>One Attack to Rule Them All: Tight Quadratic Bounds for Adaptive Queries on Cardinality Sketches</title>
      <link>https://arxiv.org/abs/2411.06370</link>
      <description>arXiv:2411.06370v2 Announce Type: replace 
Abstract: Cardinality sketches are compact data structures for representing sets or vectors. These sketches are space-efficient, typically requiring only logarithmic storage in the input size, and enable approximation of cardinality (or the number of nonzero entries). A crucial property in applications is \emph{composability}, meaning that the sketch of a union of sets can be computed from individual sketches. Existing designs provide strong statistical guarantees, ensuring that a randomly sampled sketching map remains robust for an exponential number of queries in terms of the sketch size $k$. However, these guarantees degrade to quadratic in $k$ when queries are \emph{adaptive}, meaning they depend on previous responses.
  Prior works on statistical queries (Steinke and Ullman, 2015) and specific MinHash cardinality sketches (Ahmadian and Cohen, 2024) established that this is tight in that they can be compromised using a quadratic number of adaptive queries. In this work, we develop a universal attack framework that applies to broad classes of cardinality sketches. We show that any union-composable sketching map can be compromised with $\tilde{O}(k^4)$ adaptive queries and this improves to a tight bound of $\tilde{O}(k^2)$ for monotone maps (including MinHash, statistical queries, and Boolean linear maps). Similarly, any linear sketching map over the reals $\mathbb{R}$ and finite fields $\mathbb{F}_p$ can be compromised using $\tilde{O}(k^2)$ adaptive queries, which is optimal and strengthens some of the recent results by~\citet{GribelyukLWYZ:FOCS2024}, who established a weaker polynomial bound.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06370v2</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Edith Cohen, Jelani Nelson, Tam\'as Sarl\'os, Mihir Singhal, Uri Stemmer</dc:creator>
    </item>
    <item>
      <title>Combinatorial Optimization via LLM-driven Iterated Fine-tuning</title>
      <link>https://arxiv.org/abs/2503.06917</link>
      <description>arXiv:2503.06917v2 Announce Type: replace-cross 
Abstract: We present a novel way to integrate flexible, context-dependent constraints into combinatorial optimization by leveraging Large Language Models (LLMs) alongside traditional algorithms. Although LLMs excel at interpreting nuanced, locally specified requirements, they struggle with enforcing global combinatorial feasibility. To bridge this gap, we propose an iterated fine-tuning framework where algorithmic feedback progressively refines the LLM's output distribution. Interpreting this as simulated annealing, we introduce a formal model based on a "coarse learnability" assumption, providing sample complexity bounds for convergence. Empirical evaluations on scheduling, graph connectivity, and clustering tasks demonstrate that our framework balances the flexibility of locally expressed constraints with rigorous global optimization more effectively compared to baseline sampling methods. Our results highlight a promising direction for hybrid AI-driven combinatorial reasoning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06917v2</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pranjal Awasthi, Sreenivas Gollapudi, Ravi Kumar, Kamesh Munagala</dc:creator>
    </item>
  </channel>
</rss>
