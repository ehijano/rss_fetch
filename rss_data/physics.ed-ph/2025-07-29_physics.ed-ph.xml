<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.ed-ph updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.ed-ph</link>
    <description>physics.ed-ph updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.ed-ph" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Jul 2025 01:27:52 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Network analysis of students' drawn representations of an introductory lab</title>
      <link>https://arxiv.org/abs/2505.02681</link>
      <description>arXiv:2505.02681v2 Announce Type: replace 
Abstract: Introductory physics labs can be designed and studied using the Communities of Practice framework, where a group of members pursues a common set of goals by learning and implementing a set of practices. Studies of student preferences and behaviors in lab show how students might perceive the introductory lab community differently, as they occupy different positions within the community and engage differently with its practices. Such differences can be particularly sharp along the dimensions of gender, college generation, and race. In this proof-of-concept study, we demonstrate how we can explore students' perceptions of the introductory lab community of practice using a drawing-based survey and network analysis. The survey collects students' expressions of their perspectives and explanations of the goals, members, and practices of the lab. We catalog the distinct elements from these drawings and categorize them as related to goals, members, and practices. With this quantitative count of drawing elements, network analysis gives an overview of how the elements in these drawings relate to each other while preserving the openness of student expression. We are particularly interested in the frequency of each element's depiction across the network and its betweenness centrality within the network. We collected N = 74 student drawings from a studio-format introductory physics for life sciences sequence. Our network analysis reveals a wide diversity of drawing elements with a focus on centralized hands-on practices and community members and a sparsity of goals across the students' perspectives. We demonstrate how some elements show differences with large effect sizes between identity groups based on gender, college generation, and racial background. We compare these differences to those found in observational studies and discuss future uses of the drawing survey and network analysis approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02681v2</guid>
      <category>physics.ed-ph</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>W. Brian Lane, Charlotte Dries, Gabriella Khazal, Tiffany Snow</dc:creator>
    </item>
    <item>
      <title>Scaling Physical Reasoning with the PHYSICS Dataset</title>
      <link>https://arxiv.org/abs/2506.00022</link>
      <description>arXiv:2506.00022v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have achieved remarkable progress on advanced reasoning tasks such as mathematics and coding competitions. Meanwhile, physics, despite being both reasoning-intensive and essential to real-world understanding, received limited academic and industrial attention. This paper introduces PHYSICS, a dataset containing 16,568 high-quality physics problems spanning subjects and difficulty levels, to facilitate this issue. Specifically, PHYSICS is curated with exercises from over 100 textbooks through a carefully designed pipeline for quality control. It covers five major physics domains: Mechanics, Electromagnetism, Thermodynamics, Optics, and Modern Physics. It also spans a wide range of difficulty levels, from high school to graduate-level physics courses. To utilize the data for improving and evaluating the model's physical reasoning capabilities, we split the dataset into training and test sets, and provide reasoning paths generated by powerful reasoning models for the training data to facilitate model training. In addition, for the evaluation part, we find that existing evaluation frameworks exhibit biases in aspects such as units, simplification, and precision in physics domain. To balance efficiency and accuracy, we introduce a Rule+Model evaluation framework tailored to physics problems. Our evaluations on current state-of-the-art open-source and proprietary models highlight the limitations of current models in handling physics-related tasks. We hope that our dataset and evaluation methodology will jointly advance the development of LLMs in the field of physics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00022v3</guid>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>physics.ed-ph</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shenghe Zheng, Qianjia Cheng, Junchi Yao, Mengsong Wu, Haonan He, Ning Ding, Yu Cheng, Shuyue Hu, Lei Bai, Dongzhan Zhou, Ganqu Cui, Peng Ye</dc:creator>
    </item>
  </channel>
</rss>
