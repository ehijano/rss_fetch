<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 15 Jan 2026 02:36:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Language markers of emotion flexibility predict depression and anxiety treatment outcomes</title>
      <link>https://arxiv.org/abs/2601.07961</link>
      <description>arXiv:2601.07961v1 Announce Type: new 
Abstract: Predicting treatment non-response for anxiety and depression is challenging, in part because of sparse symptom assessments in real-world care. We examined whether passively captured, fine-grained emotions serve as linguistic markers of treatment outcomes by analyzing 12 weeks of de-identified teletherapy transcripts from 12,043 U.S. patients with moderate-to-severe anxiety and depression symptoms. A transformer-based small language model extracted patients' emotions at the talk-turn level; a state-space model (VISTA) clustered subgroups based on emotion dynamics over time and produced temporal networks. Two groups emerged: an improving group (n=8,230) and a non-response group (n=3813) showing increased odds of symptom deterioration, and lower likelihood of clinically significant improvement. Temporal networks indicated that sadness and fear exerted most influence on emotion dynamics in non-responders, whereas improving patients showed balanced joy, sadness, and neutral expressions. Findings suggest that linguistic markers of emotional inflexibility can serve as scalable, interpretable, and theoretically grounded indicators for treatment risk stratification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07961v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin Brindle, George Bonanno, Thomas Derrick Hull, Nicolas Charon, Matteo Malgaroli</dc:creator>
    </item>
    <item>
      <title>A parsimonious tail compliant multiscale statistical model for aggregated rainfall</title>
      <link>https://arxiv.org/abs/2601.08350</link>
      <description>arXiv:2601.08350v1 Announce Type: new 
Abstract: Modeling rainfall intensity distributions across aggregation scales (from sub-hourly to weekly) is essential for hydrological risk analysis and IDF curves. Aggregation naturally imposes mathematical constraints: return levels must be ordered by time scale, as daily accumulations necessarily exceed sub-daily ones. From a statistical perspective, each aggregation step should ideally not require additional parameters, yet parsimonious models describing the full distribution remain scarce, as most literature focuses on seasonal block maxima.
  In this study, we propose a parsimonious framework to model all rainfall intensities (low to large) across scales. We utilize the Extended Generalized Pareto Distribution (EGPD), which aligns with extreme value theory for both tails while remaining flexible for the bulk of the distribution. We establish a general result on the behavior of EGPD variables under various aggregation procedures.
  To overcome the difficulty of direct likelihood inference, we link the EGPD class to Poisson compound sums. This allows the use of the Panjer algorithm for efficient composite likelihood evaluation. Our approach ensures that return levels do not cross across scales and enables estimation for return periods below annual or seasonal levels.
  We demonstrate the method using sub-hourly series from six French stations with diverse climates. Only eight parameters are needed per station to capture scales from six minutes to three days. IDF curves above and below the annual scale are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.08350v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pierre Ailliot, Carlo Gaetan, Philippe Naveau</dc:creator>
    </item>
    <item>
      <title>Reliability Modeling of Single-Sided Aluminized Polyimide Films during Storage Considering Stress-Induced Degradation Mechanism Transition</title>
      <link>https://arxiv.org/abs/2601.08655</link>
      <description>arXiv:2601.08655v1 Announce Type: new 
Abstract: Single-sided aluminized polyimide films (SAPF) are widely used in thermal management of aerospace systems. Although the reliability of SAPF in space environments has been thoroughly studied, its reliability in ground environments during storage is always ignored, potentially leading to system failure. This paper aims to investigate the reliability of SAPF in storage environments, focusing on the effects of temperature and relative humidity. Firstly, the relationship between the performance degradation of SAPF and aluminum corrosion is identified. Next, considering the presence of two distinct stages in the influence of temperature on aluminum corrosion, a novel degradation model accounting for the degradation mechanism transition is developed. Additionally, a parameter analysis method is proposed for determining SAPF degradation mechanism based on experimental data. Then, a statistical analysis method incorporating an improved rime optimization algorithm is employed for parameter estimation, and the reliability model is established. Experimental results demonstrate that the proposed method effectively identifies two distinct stages in the impact of temperature on SAPF performance degradation. Furthermore, the proposed degradation model outperforms traditional degradation models with unchanged degradation mechanism in terms of degradation prediction accuracy, extrapolation capability and robustness, indicating its suitability for describing the degradation pattern of SAPFs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.08655v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TR.2025.3648418</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Reliability, 2026. https://ieeexplore.ieee.org/document/11342365</arxiv:journal_reference>
      <dc:creator>Shi-Shun Chen, Dong-Hua Niu, Wen-Bin Chen, Jia-Yun Song, Ya-Fei Zhang, Xiao-Yang Li, Enrico Zio</dc:creator>
    </item>
    <item>
      <title>A Symmetric Random Scan Collapsed Gibbs Sampler for Fully Bayesian Variable Selection with Spike-and-Slab Priors</title>
      <link>https://arxiv.org/abs/2601.07864</link>
      <description>arXiv:2601.07864v1 Announce Type: cross 
Abstract: We introduce a symmetric random scan Gibbs sampler for scalable Bayesian variable selection that eliminates storage of the full cross-product matrix by computing required quantities on-the-fly. Data-informed proposal weights, constructed from marginal correlations, concentrate sampling effort on promising candidates while a uniform mixing component ensures theoretical validity. We provide explicit guidance for selecting tuning parameters based on the ratio of signal to null correlations, ensuring adequate posterior exploration. The posterior-mean-size selection rule provides an adaptive alternative to the median probability model that automatically calibrates to the effective signal density without requiring an arbitrary threshold. In simulations with one hundred thousand predictors, the method achieves sensitivity of 1.000 and precision above 0.76. Application to a genomic dataset studying riboflavin production in Bacillus subtilis identifies six genes, all validated by previous studies using alternative methods. The underlying model combines a Dirac spike-and-slab prior with Laplace-type shrinkage: the Dirac spike enforces exact sparsity by setting inactive coefficients to precisely zero, while the Laplace-type slab provides adaptive regularization for active coefficients through a local-global scale mixture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07864v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mengta Chung</dc:creator>
    </item>
    <item>
      <title>Joint Modeling of Two Stochastic Processes, with Application to Learning Hospitalization Dynamics from Wastewater Viral Concentrations</title>
      <link>https://arxiv.org/abs/2601.07977</link>
      <description>arXiv:2601.07977v1 Announce Type: cross 
Abstract: In the post-pandemic era of COVID-19, hospitalization remains a primary public health concern and wastewater surveillance has become an important tool for monitoring its dynamics at the level of community. However, there is usually no sufficient information to know the infection process that results in both wastewater viral signals and hospital admissions. That key challenge has motived a statistical framework proposed in this paper. We formulate the connection of overtime wastewater viral signals and hospitalization counts through a latent process of infection at the level of individual subject. We provide a strategy for accommodating aggregated data, a typical form of surveillance data. Moreover, we ease the conventional procedure of the statistical learning with the joint modeling using available information on the infection process, which can be under-reporting. A simulation study demonstrates that the proposed approach yields stable inference under different degrees of under-ascertainment. The COVID-19 surveillance data from Ottawa, Canada shows that the framework recovers coherent temporal patterns in infection prevalence and variant-specific hospitalization risk under several reporting assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07977v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>K. Ken Peng, Charmaine B. Dean, Robert Delatolla, X. Joan Hu, Elizabeth Renouf</dc:creator>
    </item>
    <item>
      <title>Modeling Event Dynamics by Self-Exciting Processes with Random Memory</title>
      <link>https://arxiv.org/abs/2601.07980</link>
      <description>arXiv:2601.07980v1 Announce Type: cross 
Abstract: Event history data from sports competitions have recently drawn increasing attention in sports analytics to generate data-driven strategies. Such data often exhibit self-excitation in the event occurrence and dependence within event clusters. The conventional event models based on gap times may struggle to capture those features. In particular, while consecutive events may occur within a short timeframe, the self-excitation effect caused by previous events is often transient and continues for a period of uncertain time. This paper introduces an extended Hawkes process model with random self-excitation duration to formulate the dynamics of event occurrence. We present examples of the proposed model and procedures for estimating the associated model parameters. We employ the collection of the corner kicks in the games of the 2019 regular season of the Chinese Super League to motivate and illustrate the modeling and its usefulness. We also design algorithms for simulating the event process under proposed models. The proposed approach can be adapted with little modification in many other research fields such as Criminology and Infectious Disease.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07980v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>K. Ken Peng, X. Joan Hu, Tim B. Swartz</dc:creator>
    </item>
    <item>
      <title>Learning a Stochastic Differential Equation Model of Tropical Cyclone Intensification from Reanalysis and Observational Data</title>
      <link>https://arxiv.org/abs/2601.08116</link>
      <description>arXiv:2601.08116v1 Announce Type: cross 
Abstract: Tropical cyclones are dangerous natural hazards, but their hazard is challenging to quantify directly from historical datasets due to limited dataset size and quality. Models of cyclone intensification fill this data gap by simulating huge ensembles of synthetic hurricanes based on estimates of the storm's large scale environment. Both physics-based and statistical/ML intensification models have been developed to tackle this problem, but an open question is: can a physically reasonable and simple physics-style differential equation model of intensification be learned from data? In this paper, we answer this question in the affirmative by presenting a 10-term cubic stochastic differential equation model of Tropical Cyclone intensification. The model depends on a well-vetted suite of engineered environmental features known to drive intensification and is trained using a high quality dataset of hurricane intensity (IBTrACS) with estimates of the cyclone's large scale environment from a data-assimilated simulation (ERA5 reanalysis), restricted to the Northern Hemisphere. The model generates synthetic intensity series which capture many aspects of historical intensification statistics and hazard estimates in the Northern Hemisphere. Our results show promise that interpretable, physics style models of complex earth system dynamics can be learned using automated system identification techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.08116v1</guid>
      <category>cs.LG</category>
      <category>math.DS</category>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kenneth Gee, Sai Ravela</dc:creator>
    </item>
    <item>
      <title>Stable Filtering for Efficient Dimensionality Reduction of Streaming Manifold Data</title>
      <link>https://arxiv.org/abs/2601.08685</link>
      <description>arXiv:2601.08685v1 Announce Type: cross 
Abstract: Many areas in science and engineering now have access to technologies that enable the rapid collection of overwhelming data volumes. While these datasets are vital for understanding phenomena from physical to biological and social systems, the sheer magnitude of the data makes even simple storage, transmission, and basic processing highly challenging. To enable efficient and accurate execution of these data processing tasks, we require new dimensionality reduction tools that 1) do not need expensive, time-consuming training, and 2) preserve the underlying geometry of the data that has the information required to understand the measured system. Specifically, the geometry to be preserved is that induced by the fact that in many applications, streaming high-dimensional data evolves on a low-dimensional attractor manifold. Importantly, we may not know the exact structure of this manifold a priori. To solve these challenges, we present randomized filtering (RF), which leverages a specific instantiation of randomized dimensionality reduction to provably preserve non-linear manifold structure in the embedded space while remaining data-independent and computationally efficient. In this work we build on the rich theoretical promise of randomized dimensionality reduction to develop RF as a real, practical approach. We introduce novel methods, analysis, and experimental verification to illuminate the practicality of RF in diverse scientific applications, including several simulated and real-data examples that showcase the tangible benefits of RF.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.08685v1</guid>
      <category>eess.SP</category>
      <category>q-bio.NC</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nicholas P. Bertrand, Eva Yezerets, Han Lun Yap, Adam S. Charles, Christopher J. Rozell</dc:creator>
    </item>
    <item>
      <title>A Langevin sampler for quantum tomography</title>
      <link>https://arxiv.org/abs/2601.08775</link>
      <description>arXiv:2601.08775v1 Announce Type: cross 
Abstract: Quantum tomography involves obtaining a full classical description of a prepared quantum state from experimental results. We propose a Langevin sampler for quantum tomography, that relies on a new formulation of Bayesian quantum tomography exploiting the Burer-Monteiro factorization of Hermitian positive-semidefinite matrices. If the rank of the target density matrix is known, this formulation allows us to define a posterior distribution that is only supported on matrices whose rank is upper-bounded by the rank of the target density matrix. Conversely, if the target rank is unknown, any upper bound on the rank can be used by our algorithm, and the rank of the resulting posterior mean estimator is further reduced by the use of a low-rank promoting prior density. This prior density is a complex extension of the one proposed in (Annales de l'Institut Henri Poincare Probability and Statistics, 56(2):1465-1483, 2020). We derive a PAC-Bayesian bound on our proposed estimator that matches the best bounds available in the literature, and we show numerically that it leads to strong scalability improvements compared to existing techniques when the rank of the density matrix is known to be small.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.08775v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tameem Adel, Abhishek Agarwal, St\'ephane Chr\'etien, Estelle Massart, Danila Mokeev, Ivan Rungger, Andrew Thompson</dc:creator>
    </item>
    <item>
      <title>The Impact of Renewable Energy Communities in the Italian Day-Ahead Electricity Market: A Scenario Analysis</title>
      <link>https://arxiv.org/abs/2510.13517</link>
      <description>arXiv:2510.13517v2 Announce Type: replace 
Abstract: This paper evaluates the economic impact of Renewable Energy Communities (RECs) on the Italian wholesale power market. Combining a bottom-up engineering approach with a short-run economic impact assessment, the study begins by mapping existing and emerging RECs in Italy. We identify key characteristics of RECs, such as average installed capacity, institutional profiles of members, types of renewable systems used, and transmission across Italy's electricity market zones. This mapping yields representative REC configurations, which are employed within a bottom-up engineering model to generate energy injection and self-consumption profiles for different REC prosumer and producer categories (residential, public, small and medium enterprise, non-profit organization, and standalone installation), considering the different levels of solar irradiance in Italy based on latitude. These zonal results, aggregated on an hourly basis, inform the implementation of the synthetic counterfactual approach, which develops alternative scenarios (e.g., 5 GW target for REC-driven capacity set by Italian policy for 2027) to assess the impact of REC-driven injection and self-consumption on the Italian day-ahead power market. The findings suggest that REC deployment can increase equilibrium quantities during daylight in most of the time, while decreasing equilibrium quantities mostly during the cold months, as electrified heating drives greater self-consumption and offsets lower grid injections. Both positive and negative effects on equilibrium quantities suggest that REC deployment also has a potential to reduce wholesale electricity prices. Moreover, by reducing grid exchanges through higher self-consumption, REC proliferation can alleviate pressure on the distribution system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13517v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maksym Koltunov, Filippo Beltrami, Luigi Grossi, Nicola Blasuttigh</dc:creator>
    </item>
    <item>
      <title>A two-step approach to production frontier estimation and the Matsuoka's distribution</title>
      <link>https://arxiv.org/abs/2311.06086</link>
      <description>arXiv:2311.06086v3 Announce Type: replace-cross 
Abstract: In this work, we introduce a deterministic frontier model in which efficiency is governed by the Matsuoka distribution, a parsimonious one-parameter specification on $(0,1)$ designed to reflect patterns typically observed in efficiency data. Based on this formulation, we develop a two-step semiparametric estimation procedure: a nonparametric smoothing for the regression component, followed by a feasible method of moments estimation for the efficiency parameter with plug-in reconstruction of the frontier. Theoretical results establish convergence rates, asymptotic normality, and an oracle property for the parametric estimator of the efficiency parameter. A Monte Carlo study demonstrates that the procedure performs consistently with the theoretical results and improves upon a fully nonparametric alternative. Applying the method to Brazilian temporary crops with land and agrochemicals as inputs, we find that both regions exhibit isoquants close to the constant elasticity substitution form, but differ in the relative productivity of inputs. Most notably, statistical tests provide evidence that the South is relatively more efficient than the Center-West, highlighting the empirical relevance of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.06086v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Danilo Hiroshi Matsuoka, Guilherme Pumi, Hudson da Silva Torrent, Marcio valk</dc:creator>
    </item>
    <item>
      <title>Benchmarking multi-step methods for the dynamic prediction of survival with numerous longitudinal predictors</title>
      <link>https://arxiv.org/abs/2403.14336</link>
      <description>arXiv:2403.14336v3 Announce Type: replace-cross 
Abstract: In recent years, the growing availability of biomedical datasets featuring numerous longitudinal covariates has motivated the development of several multi-step methods for the dynamic prediction of survival outcomes. These methods employ either mixed-effects models or multivariate functional principal component analysis to model and summarize the longitudinal covariates' evolution over time. Then, they use Cox models or random survival forests to predict survival probabilities, using as covariates both baseline variables and the summaries of the longitudinal variables obtained in the previous modelling step. Because these multi-step methods are still quite new, to date little is known about their applicability, limitations, and predictive performance when applied to real-world data. To gain a better understanding of these aspects, we performed a benchmarking of these multi-step methods (and two simpler prediction approaches) using three datasets that differ in sample size, number of longitudinal covariates and length of follow-up. We discuss the different modelling choices made by these methods, and some adjustments that one may need to do in order to be able to apply them to real-world data. Furthermore, we compare their predictive performance using multiple performance measures and landmark times, assess their computing time, and discuss their strengths and limitations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14336v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1515/ijb-2025-0049</arxiv:DOI>
      <arxiv:journal_reference>The International Journal of Biostatistics (2025)</arxiv:journal_reference>
      <dc:creator>Mirko Signorelli, Sophie Retif</dc:creator>
    </item>
    <item>
      <title>Rotation of the Globular Cluster Population of the Dark Matter Deficient Galaxy NGC 1052-DF4: Implication for the Total Mass</title>
      <link>https://arxiv.org/abs/2405.10462</link>
      <description>arXiv:2405.10462v2 Announce Type: replace-cross 
Abstract: We explore the globular cluster population of NGC 1052-DF4, a dark matter deficient galaxy, using Bayesian inference to search for the presence of rotation. The existence of such a rotating component is relevant to the estimation of the mass of the galaxy, and therefore the question of whether NGC 1052-DF4 is truly deficient of dark matter, similar to NGC 1052-DF2 another galaxy in the same group. The rotational characteristics of seven globular clusters in NGC 1052-DF4 were investigated, finding that a non-rotating kinematic model has a higher Bayesian evidence than a rotating model, by a factor of approximately 2.5. In addition, we find that under the assumption of rotation, its amplitude must be small. This distinct lack of rotation strengthens the case that, based on its intrinsic velocity dispersion, NGC 1052-DF4 is a truly dark matter deficient galaxy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10462v2</guid>
      <category>astro-ph.IM</category>
      <category>astro-ph.GA</category>
      <category>stat.AP</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Publications of the Astronomical Society of Australia , Volume 41 , 2024 , e073</arxiv:journal_reference>
      <dc:creator>Yuan Li (Cher), Brendon J. Brewer, Geraint F. Lewis</dc:creator>
    </item>
    <item>
      <title>Revisiting The Cosmological Time Dilation of Distant Quasars: Influence of Source Properties and Evolution</title>
      <link>https://arxiv.org/abs/2501.04171</link>
      <description>arXiv:2501.04171v2 Announce Type: replace-cross 
Abstract: After decades of searching, cosmological time dilation was recently identified in the timescale of variability seen in distant quasars. Here, we expand on the previous analysis to disentangle this cosmological signal from the influence of the properties of the source population, specifically the quasar bolometric luminosity and the rest-frame emission wavelength at which the variability was observed. Furthermore, we consider the potential influence of the evolution of the quasar population over cosmic time. We find that a significant intrinsic scatter of 0.288 +- 0.021 dex in the variability timescales, which was not considered in the previous analysis, is favoured by the data. This slightly increases the uncertainty in the results. However, the expected cosmological dependence of the variability timescales is confirmed to be robust to changes in the underlying assumptions. We find that the variability timescales increase smoothly with both wavelength and bolometric luminosity, and that black hole mass has no effect on the variability timescale once rest wavelength and bolometric luminosity are accounted for. Moreover, if the standard cosmological model is correct, governed by relativistic expansion, we also find very little cosmological evolution in the intrinsic variability timescales of distant quasars.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04171v2</guid>
      <category>astro-ph.CO</category>
      <category>astro-ph.IM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Monthly Notices of the Royal Astronomical Society, Volume 537, Issue 2, February 2025, Pages 809-816</arxiv:journal_reference>
      <dc:creator>Brendon J. Brewer (Cher), Geraint F. Lewis (Cher), Yuan Li (Cher)</dc:creator>
    </item>
    <item>
      <title>Quantization Error Propagation: Revisiting Layer-Wise Post-Training Quantization</title>
      <link>https://arxiv.org/abs/2504.09629</link>
      <description>arXiv:2504.09629v3 Announce Type: replace-cross 
Abstract: Layer-wise PTQ is a promising technique for compressing large language models (LLMs), due to its simplicity and effectiveness without requiring retraining. However, recent progress in this area is saturating, underscoring the need to revisit its core limitations and explore further improvements. We address this challenge by identifying a key limitation of existing layer-wise PTQ methods: the growth of quantization errors across layers significantly degrades performance, particularly in low-bit regimes. To address this fundamental issue, we propose Quantization Error Propagation (QEP), a general, lightweight, and scalable framework that enhances layer-wise PTQ by explicitly propagating quantization errors and compensating for accumulated errors. QEP also offers a tunable propagation mechanism that prevents overfitting and controls computational overhead, enabling the framework to adapt to various architectures and resource budgets. Extensive experiments on several LLMs demonstrate that QEP-enhanced layer-wise PTQ achieves substantially higher accuracy than existing methods. Notably, the gains are most pronounced in the extremely low-bit quantization regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09629v3</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yamato Arai, Yuma Ichikawa</dc:creator>
    </item>
    <item>
      <title>Joint Reproduction Number and Spatial Connectivity Structure Estimation via Graph Sparsity-Promoting Penalized Functional</title>
      <link>https://arxiv.org/abs/2509.20034</link>
      <description>arXiv:2509.20034v2 Announce Type: replace-cross 
Abstract: During an epidemic outbreak, decision makers crucially need accurate and robust tools to monitor the pathogen propagation. The effective reproduction number, defined as the expected number of secondary infections stemming from one contaminated individual, is a state-of-the-art indicator quantifying the epidemic intensity. Numerous estimators have been developed to precisely track the reproduction number temporal evolution. Yet, COVID-19 pandemic surveillance raised unprecedented challenges due to the poor quality of worldwide reported infection counts. When monitoring the epidemic in different territories simultaneously, leveraging the spatial structure of data significantly enhances both the accuracy and robustness of reproduction number estimates. However, this requires a good estimate of the spatial structure. To tackle this major limitation, the present work proposes a joint estimator of the reproduction number and connectivity structure. The procedure is assessed through intensive numerical simulations on carefully designed synthetic data and illustrated on real COVID-19 spatiotemporal infection counts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20034v2</guid>
      <category>eess.SP</category>
      <category>math.OC</category>
      <category>stat.AP</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Etienne Lasalle, Barbara Pascal</dc:creator>
    </item>
    <item>
      <title>Clarifying identification and estimation of treatment effects in the Sequential Parallel Comparison Design</title>
      <link>https://arxiv.org/abs/2511.19677</link>
      <description>arXiv:2511.19677v2 Announce Type: replace-cross 
Abstract: Sequential parallel comparison design (SPCD) clinical trials aim to adjust active treatment effect estimates for placebo response to minimize the impact of placebo responders on the estimates. This is potentially accomplished using a two stage design by measuring treatment effects among all participants during the first stage, then classifying some placebo arm participants as placebo non-responders who will be re-randomized in the second stage. In this paper, we use causal inference tools to clarify under what assumptions treatment effects can be identified in SPCD trials and what effects the conventional estimators target at each stage of the SPCD trial. We further illustrate the highly influential impact of placebo response misclassification on the second stage estimate. We conclude that the conventional SPCD estimators do not target meaningful treatment effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19677v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Stockton, Michele Santacatterina, Soutrik Mandal, Charles M. Cleland, Erinn M. Hade, Nicholas Illenberger, Sharon Meropol, Andrea B. Troxel, Eva Petkova, Chang Yu, Thaddeus Tarpey</dc:creator>
    </item>
    <item>
      <title>Joint Progression Modeling (JPM): A Probabilistic Framework for Mixed-Pathology Progression</title>
      <link>https://arxiv.org/abs/2512.03475</link>
      <description>arXiv:2512.03475v2 Announce Type: replace-cross 
Abstract: Event-based models (EBMs) infer disease progression from cross-sectional data, and standard EBMs assume a single underlying disease per individual. In contrast, mixed pathologies are common in neurodegeneration. We introduce the Joint Progression Model (JPM), a probabilistic framework that treats single-disease trajectories as partial rankings and builds a prior over joint progressions. We study several JPM variants (Pairwise, Bradley-Terry, Plackett-Luce, and Mallows) and analyze three properties: (i) calibration -- whether lower model energy predicts smaller distance to the ground truth ordering; (ii) separation -- the degree to which sampled rankings are distinguishable from random permutations; and (iii) sharpness -- the stability of sampled aggregate rankings. All variants are calibrated, and all achieve near-perfect separation; sharpness varies by variant and is well-predicted by simple features of the input partial rankings (number and length of rankings, conflict, and overlap). In synthetic experiments, JPM improves ordering accuracy by roughly 21 percent over a strong EBM baseline (SA-EBM) that treats the joint disease as a single condition. Finally, using NACC, we find that the Mallows variant of JPM and the baseline model (SA-EBM) have results that are more consistent with prior literature on the possible disease progression of the mixed pathology of AD and VaD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03475v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hongtao Hao, Joseph L. Austerweil</dc:creator>
    </item>
    <item>
      <title>Rotational Kinematics in the Globular Cluster System of M31: Insights from Bayesian Inference</title>
      <link>https://arxiv.org/abs/2601.05380</link>
      <description>arXiv:2601.05380v2 Announce Type: replace-cross 
Abstract: As ancient stellar systems, globular clusters (GCs) offer valuable insights into the dynamical histories of large galaxies. Previous studies of GC populations in the inner and outer regions of the Andromeda Galaxy (M31) have revealed intriguing subpopulations with distinct kinematic properties. Here, we build upon earlier studies by employing Bayesian modelling to investigate the kinematics of the combined inner and outer GC populations of M31. Given the heterogeneous nature of the data, we examine subpopulations defined by GCs' metallicity and by associations with substructure, in order to characterise possible relationships between the inner and outer GC populations. We find that lower-metallicity GCs and those linked to substructures exhibit a common, more rapid rotation, whose alignment is distinct from that of higher-metallicity and non-substructure GCs. Furthermore, the higher-metallicity GCs rotate in alignment with Andromeda's stellar disk. These pronounced kinematic differences reinforce the idea that different subgroups of GCs were accreted to M31 at distinct epochs, shedding light on the complex assembly history of the galaxy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05380v2</guid>
      <category>astro-ph.GA</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.33232/001c.155259</arxiv:DOI>
      <dc:creator>Yuan Li (Cher), Brendon J. Brewer, Geraint F. Lewis, Dougal Mackey</dc:creator>
    </item>
    <item>
      <title>Buffered AUC maximization for scoring systems via mixed-integer optimization</title>
      <link>https://arxiv.org/abs/2601.05544</link>
      <description>arXiv:2601.05544v2 Announce Type: replace-cross 
Abstract: A scoring system is a linear classifier composed of a small number of explanatory variables, each assigned a small integer coefficient. This system is highly interpretable and allows predictions to be made with simple manual calculations without the need for a calculator. Several previous studies have used mixed-integer optimization (MIO) techniques to develop scoring systems for binary classification; however, they have not focused on directly maximizing AUC (i.e., area under the receiver operating characteristic curve), even though AUC is recognized as an essential evaluation metric for scoring systems. Our goal herein is to establish an effective MIO framework for constructing scoring systems that directly maximize the buffered AUC (bAUC) as the tightest concave lower bound on AUC. Our optimization model is formulated as a mixed-integer linear optimization (MILO) problem that maximizes bAUC subject to a group sparsity constraint for limiting the number of questions in the scoring system. Computational experiments using publicly available real-world datasets demonstrate that our MILO method can build scoring systems with superior AUC values compared to the baseline methods based on regularization and stepwise regression. This research contributes to the advancement of MIO techniques for developing highly interpretable classification models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05544v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.AP</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Moe Shiina, Shunnosuke Ikeda, Yuichi Takano</dc:creator>
    </item>
  </channel>
</rss>
