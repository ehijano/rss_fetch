<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.QM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.QM</link>
    <description>q-bio.QM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.QM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 13 Feb 2026 05:00:44 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>How Well Do Large-Scale Chemical Language Models Transfer to Downstream Tasks?</title>
      <link>https://arxiv.org/abs/2602.11618</link>
      <description>arXiv:2602.11618v1 Announce Type: cross 
Abstract: Chemical Language Models (CLMs) pre-trained on large scale molecular data are widely used for molecular property prediction. However, the common belief that increasing training resources such as model size, dataset size, and training compute improves both pretraining loss and downstream task performance has not been systematically validated in the chemical domain. In this work, we evaluate this assumption by pretraining CLMs while scaling training resources and measuring transfer performance across diverse molecular property prediction (MPP) tasks. We find that while pretraining loss consistently decreases with increased training resources, downstream task performance shows limited improvement. Moreover, alternative metrics based on the Hessian or loss landscape also fail to estimate downstream performance in CLMs. We further identify conditions under which downstream performance saturates or degrades despite continued improvements in pretraining metrics, and analyze the underlying task dependent failure modes through parameter space visualizations. These results expose a gap between pretraining based evaluation and downstream performance, and emphasize the need for model selection and evaluation strategies that explicitly account for downstream task characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.11618v1</guid>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuya Sagawa, Ryosuke Kojima</dc:creator>
    </item>
    <item>
      <title>Protein Circuit Tracing via Cross-layer Transcoders</title>
      <link>https://arxiv.org/abs/2602.12026</link>
      <description>arXiv:2602.12026v1 Announce Type: cross 
Abstract: Protein language models (pLMs) have emerged as powerful predictors of protein structure and function. However, the computational circuits underlying their predictions remain poorly understood. Recent mechanistic interpretability methods decompose pLM representations into interpretable features, but they treat each layer independently and thus fail to capture cross-layer computation, limiting their ability to approximate the full model. We introduce ProtoMech, a framework for discovering computational circuits in pLMs using cross-layer transcoders that learn sparse latent representations jointly across layers to capture the model's full computational circuitry. Applied to the pLM ESM2, ProtoMech recovers 82-89% of the original performance on protein family classification and function prediction tasks. ProtoMech then identifies compressed circuits that use &lt;1% of the latent space while retaining up to 79% of model accuracy, revealing correspondence with structural and functional motifs, including binding, signaling, and stability. Steering along these circuits enables high-fitness protein design, surpassing baseline methods in more than 70% of cases. These results establish ProtoMech as a principled framework for protein circuit tracing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12026v1</guid>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Darin Tsui, Kunal Talreja, Daniel Saeedi, Amirali Aghazadeh</dc:creator>
    </item>
    <item>
      <title>Notation-level confounding: When inconsistent molecular notations mislead chemical language models</title>
      <link>https://arxiv.org/abs/2505.07139</link>
      <description>arXiv:2505.07139v5 Announce Type: replace 
Abstract: Chemical language models (CLMs) are increasingly used for molecular design and property prediction. Because these models learn from textual encodings of molecules, differences in how such encodings are generated may affect their behavior. In cheminformatics, the term canonical SMILES implies a single standardized notation, yet different toolkits define distinct canonicalization rules, yielding multiple canonical strings for the same molecule. To examine how this variability arises and why it matters, we surveyed 264 CLM papers in PubMed and found that about half did not specify their canonicalization procedure, limiting transparency and reproducibility. Using a molecular translation framework, we show that when multiple valid notations are mixed or left undocumented, inconsistent notations distort latent representations and, in some benchmarks, can spuriously inflate predictive accuracy, a phenomenon we term notation-level confounding. These findings demonstrate how subtle differences in SMILES generation can mislead CLMs and highlight the importance of explicitly reporting preprocessing tools and settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07139v5</guid>
      <category>q-bio.QM</category>
      <pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yosuke Kikuchi, Yasuhiro Yoshikai, Shumpei Nemoto, Ayako Furuhama, Takashi Yamada, Hiroyuki Kusuhara, Tadahaya Mizuno</dc:creator>
    </item>
    <item>
      <title>BrainSymphony: A parameter-efficient multimodal foundation model for brain dynamics with limited data</title>
      <link>https://arxiv.org/abs/2506.18314</link>
      <description>arXiv:2506.18314v2 Announce Type: replace 
Abstract: Foundation models are transforming neuroscience but are often prohibitively large, data-hungry, and difficult to deploy. Here, we introduce BrainSymphony, a lightweight and parameter-efficient foundation model with plug-and-play integration of fMRI time series and diffusion-derived structural connectivity, allowing unimodal or multimodal training and deployment without architectural changes while requiring substantially less data compared to the state-of-the-art. The model processes fMRI time series through parallel spatial and temporal transformer streams, distilled into compact embeddings by a Perceiver module, while a novel signed graph transformer encodes anatomical connectivity from diffusion MRI. These complementary representations are then combined through an adaptive fusion mechanism. Despite its compact design, BrainSymphony consistently outperforms larger models on benchmarks spanning prediction, classification, and unsupervised network discovery. Highlighting the model's generalizability and interpretability, attention maps reveal drug-induced context-dependent reorganization of cortical hierarchies in an independent psilocybin neuroimaging dataset. BrainSymphony delivers accessible, interpretable, and clinically meaningful results and demonstrates that architecturally informed, multimodal models can surpass much larger counterparts and advance applications of AI in neuroscience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18314v2</guid>
      <category>q-bio.QM</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Moein Khajehnejad, Forough Habibollahi, Devon Stoliker, Adeel Razi</dc:creator>
    </item>
    <item>
      <title>TF-DWGNet: A Directed Weighted Graph Neural Network with Tensor Fusion for Multi-Omics Cancer Subtype Classification</title>
      <link>https://arxiv.org/abs/2509.16301</link>
      <description>arXiv:2509.16301v2 Announce Type: replace 
Abstract: Integration and analysis of multi-omics data provide valuable insights for improving cancer subtype classification. However, such data are inherently heterogeneous, high-dimensional, and exhibit complex intra- and inter-modality dependencies. Graph neural networks (GNNs) offer a principled framework for modeling these structures, but existing approaches often rely on prior knowledge or predefined similarity networks that produce undirected or unweighted graphs and fail to capture task-specific directionality and interaction strength. Interpretability at both the modality and feature levels also remains limited. To address these challenges, we propose TF-DWGNet, a novel Graph Neural Network framework that combines tree-based Directed Weighted graph construction with Tensor Fusion for multiclass cancer subtype classification. TF-DWGNet introduces two key innovations: (i) a supervised tree-based strategy that constructs directed, weighted graphs tailored to each omics modality, and (ii) a tensor fusion mechanism that captures unimodal, bimodal, and trimodal interactions using low-rank decomposition for computational efficiency. Experiments on three real-world cancer datasets demonstrate that TF-DWGNet consistently outperforms state-of-the-art baselines across multiple metrics and statistical tests. In addition, the model provides biologically meaningful insights through modality-level contribution scores and ranked feature importance. These results highlight that TF-DWGNet is an effective and interpretable solution for multi-omics integration in cancer research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16301v2</guid>
      <category>q-bio.QM</category>
      <category>cs.LG</category>
      <pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tiantian Yang, Zhiqian Chen</dc:creator>
    </item>
    <item>
      <title>Mixture of Inverse Gaussians for Hemodynamic Transport (MIGHT) in Multiple-Input Multiple-Output Vascular Networks</title>
      <link>https://arxiv.org/abs/2510.11743</link>
      <description>arXiv:2510.11743v2 Announce Type: replace 
Abstract: Synthetic molecular communication (MC) in the cardiovascular system is a key enabler for many envisioned medical applications inside the human body, such as targeted drug delivery, early disease detection, and continuous health monitoring. The design of synthetic MC systems for such applications requires suitable models for the signaling molecule propagation through complex vessel networks (VNs). Existing theoretical models offer limited analytical tractability and lack closed-form solutions, making the analysis of realistic large-scale VNs either infeasible or not insightful. To overcome these limitations, in this paper, we propose a novel closed-form physical model, termed mixture of inverse Gaussians for hemodynamic transport (MIGHT), for the advection-diffusion-driven transport of signaling molecules through complex VNs. The model represents the received molecule flux as a weighted sum of inverse Gaussian distributions, parameterized by the physical properties of the underlying VN. We show that MIGHT is capable of accurately representing the transport dynamics of signaling molecules in large-scale VNs ranging from simple single-input single-output (SISO) to complex multiple-input multiple-output (MIMO) network topologies. The accuracy of the proposed model is validated by comparison to the results from an existing convolution-based model and numerical finite-element simulations, with all finite-element simulation data available on Zenodo. Furthermore, we investigate three applications of the model, namely the reduction of SISO-VNs to obtain simplified representations preserving the essential transport dynamics, the identification and analysis of network regions that are most important for molecule transport in MIMO-VNs comprising multiple transmitters and multiple receivers, and the estimation of representative SISO-VNs that can reproduce the received signal of an unknown SISO-VN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11743v2</guid>
      <category>q-bio.QM</category>
      <category>cs.ET</category>
      <pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Timo Jakumeit, Bastian Heinlein, Nunzio Tuccitto, Robert Schober, Sebastian Lotter, Maximilian Sch\"afer</dc:creator>
    </item>
    <item>
      <title>An open-source computational framework for immersed fluid-structure interaction modeling using FEBio and MFEM</title>
      <link>https://arxiv.org/abs/2601.08266</link>
      <description>arXiv:2601.08266v2 Announce Type: replace 
Abstract: Fluid-structure interaction (FSI) simulation of biological systems presents significant computational challenges, particularly for applications involving large structural deformations and contact mechanics, such as heart valve dynamics. Traditional ALE methods encounter fundamental difficulties with such problems due to mesh distortion, motivating immersed techniques. This work presents a novel open-source immersed FSI framework that strategically couples two mature finite element libraries: MFEM, a GPU-ready and scalable library with state-of-the-art parallel performance developed at LLNL, and FEBio, a nonlinear finite element solver with sophisticated solid mechanics capabilities designed for biomechanics applications developed at the University of Utah and Columbia University. This coupling creates a unique synergy wherein the fluid solver leverages MFEM's distributed-memory parallelization and pathway to GPU acceleration, while the immersed solid exploits FEBio's comprehensive suite of hyperelastic and viscoelastic constitutive models and advanced solid mechanics modeling targeted for biomechanics applications. FSI coupling is achieved using a fictitious domain methodology with variational multiscale stabilization for enhanced accuracy on under-resolved grids expected with unfitted meshes used in immersed FSI. A fully implicit, monolithic scheme provides robust coupling for strongly coupled FSI characteristic of cardiovascular applications. The framework's modular architecture facilitates straightforward extension to additional physics and element technologies. Several test problems are considered to demonstrate the capabilities of the proposed framework, including a 3D semilunar heart valve simulation. This platform addresses a critical need for open-source immersed FSI software combining advanced biomechanics modeling with high-performance computing infrastructure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.08266v2</guid>
      <category>q-bio.QM</category>
      <category>physics.comp-ph</category>
      <pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan T. Black, Steve A. Maas, Wensi Wu, Jalaj Maheshwari, Tzanio Kolev, Jeffrey A. Weiss, Matthew A. Jolley</dc:creator>
    </item>
    <item>
      <title>Geometric Stability: The Missing Axis of Representations</title>
      <link>https://arxiv.org/abs/2601.09173</link>
      <description>arXiv:2601.09173v3 Announce Type: replace-cross 
Abstract: Analysis of learned representations has a blind spot: it focuses on $similarity$, measuring how closely embeddings align with external references, but similarity reveals only what is represented, not whether that structure is robust. We introduce $geometric$ $stability$, a distinct dimension that quantifies how reliably representational geometry holds under perturbation, and present $Shesha$, a framework for measuring it. Across 2,463 configurations in seven domains, we show that stability and similarity are empirically uncorrelated ($\rho \approx 0.01$) and mechanistically distinct: similarity metrics collapse after removing the top principal components, while stability retains sensitivity to fine-grained manifold structure. This distinction yields actionable insights: for safety monitoring, stability acts as a functional geometric canary, detecting structural drift nearly 2$\times$ more sensitively than CKA while filtering out the non-functional noise that triggers false alarms in rigid distance metrics; for controllability, supervised stability predicts linear steerability ($\rho = 0.89$-$0.96$); for model selection, stability dissociates from transferability, revealing a geometric tax that transfer optimization incurs. Beyond machine learning, stability predicts CRISPR perturbation coherence and neural-behavioral coupling. By quantifying $how$ $reliably$ systems maintain structure, geometric stability provides a necessary complement to similarity for auditing representations across biological and computational systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09173v3</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>q-bio.QM</category>
      <category>stat.ML</category>
      <pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Prashant C. Raju</dc:creator>
    </item>
    <item>
      <title>Central Dogma Transformer II: An AI Microscope for Understanding Cellular Regulatory Mechanisms</title>
      <link>https://arxiv.org/abs/2602.08751</link>
      <description>arXiv:2602.08751v2 Announce Type: replace-cross 
Abstract: Current biological AI models lack interpretability -- their internal representations do not correspond to
  biological relationships that researchers can examine. Here we present CDT-II, an "AI microscope" whose
  attention maps are directly interpretable as regulatory structure. By mirroring the central dogma in its
  architecture, CDT-II ensures that each attention mechanism corresponds to a specific biological relationship:
  DNA self-attention for genomic relationships, RNA self-attention for gene co-regulation, and DNA-to-RNA
  cross-attention for transcriptional control. Using only genomic embeddings and raw per-cell expression, CDT-II
  enables experimental biologists to observe regulatory networks in their own data. Applied to K562 CRISPRi
  data, CDT-II predicts perturbation effects (per-gene mean $r = 0.84$) and recovers the GFI1B regulatory
  network without supervision (6.6-fold enrichment, $P = 3.5 \times 10^{-17}$). Systematic comparison against
  ENCODE K562 regulatory annotations reveals that cross-attention autonomously focuses on known regulatory
  elements -- DNase hypersensitive sites ($201\times$ enrichment), CTCF binding sites ($28\times$), and histone
  marks -- across all five held-out genes. Two distinct attention mechanisms independently identify an
  overlapping RNA processing module (80% gene overlap; RNA binding enrichment $P = 1 \times 10^{-16}$). CDT-II
  establishes mechanism-oriented AI as an alternative to task-oriented approaches, revealing regulatory
  structure rather than merely optimizing predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.08751v2</guid>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 13 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nobuyuki Ota</dc:creator>
    </item>
  </channel>
</rss>
