<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 31 Jan 2025 05:00:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Noise-Adaptive Conformal Classification with Marginal Coverage</title>
      <link>https://arxiv.org/abs/2501.18060</link>
      <description>arXiv:2501.18060v1 Announce Type: new 
Abstract: Conformal inference provides a rigorous statistical framework for uncertainty quantification in machine learning, enabling well-calibrated prediction sets with precise coverage guarantees for any classification model. However, its reliance on the idealized assumption of perfect data exchangeability limits its effectiveness in the presence of real-world complications, such as low-quality labels -- a widespread issue in modern large-scale data sets. This work tackles this open problem by introducing an adaptive conformal inference method capable of efficiently handling deviations from exchangeability caused by random label noise, leading to informative prediction sets with tight marginal coverage guarantees even in those challenging scenarios. We validate our method through extensive numerical experiments demonstrating its effectiveness on synthetic and real data sets, including CIFAR-10H and BigEarthNet.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18060v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Teresa Bortolotti, Y. X. Rachel Wang, Xin Tong, Alessandra Menafoglio, Simone Vantini, Matteo Sesia</dc:creator>
    </item>
    <item>
      <title>An optimal dynamic treatment regime estimator for indefinite-horizon survival outcomes</title>
      <link>https://arxiv.org/abs/2501.18070</link>
      <description>arXiv:2501.18070v1 Announce Type: new 
Abstract: We propose a new method in indefinite-horizon settings for estimating optimal dynamic treatment regimes for time-to-event outcomes. This method allows patients to have different numbers of treatment stages and is constructed using generalized survival random forests to maximize mean survival time. We use summarized history and data pooling, preventing data from growing in dimension as a patient's decision points increase. The algorithm operates through model re-fitting, resulting in a single model optimized for all patients and all stages. We derive theoretical properties of the estimator such as consistency of the estimator and value function and characterize the number of refitting iterations needed. We also conduct a simulation study of patients with a flexible number of treatment stages to examine finite-sample performance of the estimator. Finally, we illustrate use of the algorithm using administrative insurance claims data for pediatric Crohn's disease patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18070v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jane She, Matthew Egberg, Michael R. Kosorok</dc:creator>
    </item>
    <item>
      <title>Logistic regression models: practical induced prior specification</title>
      <link>https://arxiv.org/abs/2501.18106</link>
      <description>arXiv:2501.18106v1 Announce Type: new 
Abstract: Bayesian inference for statistical models with a hierarchical structure is often characterized by specification of priors for parameters at different levels of the hierarchy. When higher level parameters are functions of the lower level parameters, specifying a prior on the lower level parameters leads to induced priors on the higher level parameters. However, what are deemed uninformative priors for lower level parameters can induce strikingly non-vague priors for higher level parameters. Depending on the sample size and specific model parameterization, these priors can then have unintended effects on the posterior distribution of the higher level parameters.
  Here we focus on Bayesian inference for the Bernoulli distribution parameter $\theta$ which is modeled as a function of covariates via a logistic regression, where the coefficients are the lower level parameters for which priors are specified. A specific area of interest and application is the modeling of survival probabilities in capture-recapture data and occupancy and detection probabilities in presence-absence data. In particular we propose alternative priors for the coefficients that yield specific induced priors for $\theta$. We address three induced prior cases. The simplest is when the induced prior for $\theta$ is Uniform(0,1). The second case is when the induced prior for $\theta$ is an arbitrary Beta($\alpha$, $\beta$) distribution. The third case is one where the intercept in the logistic model is to be treated distinct from the partial slope coefficients; e.g., $E[\theta]$ equals a specified value on (0,1) when all covariates equal 0. Simulation studies were carried out to evaluate performance of these priors and the methods were applied to a real presence/absence data set and occupancy modelling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18106v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ken B. Newman, Cristiano Villa, Ruth King</dc:creator>
    </item>
    <item>
      <title>A spectral clustering-type algorithm for the consistent estimation of the Hurst distribution in moderately high dimensions</title>
      <link>https://arxiv.org/abs/2501.18115</link>
      <description>arXiv:2501.18115v1 Announce Type: new 
Abstract: Scale invariance (fractality) is a prominent feature of the large-scale behavior of many stochastic systems. In this work, we construct an algorithm for the statistical identification of the Hurst distribution (in particular, the scaling exponents) undergirding a high-dimensional fractal system. The algorithm is based on wavelet random matrices, modified spectral clustering and a model selection step for picking the value of the clustering precision hyperparameter. In a moderately high-dimensional regime where the dimension, the sample size and the scale go to infinity, we show that the algorithm consistently estimates the Hurst distribution. Monte Carlo simulations show that the proposed methodology is efficient for realistic sample sizes and outperforms another popular clustering method based on mixed-Gaussian modeling. We apply the algorithm in the analysis of real-world macroeconomic time series to unveil evidence for cointegration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18115v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patrice Abry, Gustavo Didier, Oliver Orejola, Herwig Wendt</dc:creator>
    </item>
    <item>
      <title>Nonlocal prior mixture-based Bayesian wavelet regression</title>
      <link>https://arxiv.org/abs/2501.18134</link>
      <description>arXiv:2501.18134v1 Announce Type: new 
Abstract: We propose a novel Bayesian wavelet regression approach using a three-component spike-and-slab prior for wavelet coefficients, combining a point mass at zero, a moment (MOM) prior, and an inverse moment (IMOM) prior. This flexible prior supports small and large coefficients differently, offering advantages for highly dispersed data where wavelet coefficients span multiple scales. The IMOM prior's heavy tails capture large coefficients, while the MOM prior is better suited for smaller non-zero coefficients. Further, our method introduces innovative hyperparameter specifications for mixture probabilities and scaling parameters, including generalized logit, hyperbolic secant, and generalized normal decay for probabilities, and double exponential decay for scaling. Hyperparameters are estimated via an empirical Bayes approach, enabling posterior inference tailored to the data. Extensive simulations demonstrate significant performance gains over two-component wavelet methods. Applications to electroencephalography and noisy audio data illustrate the method's utility in capturing complex signal characteristics. We implement our method in an R package NLPwavelet.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18134v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nilotpal Sanyal</dc:creator>
    </item>
    <item>
      <title>Conformal novelty detection for replicate point patterns with FDR or FWER control</title>
      <link>https://arxiv.org/abs/2501.18195</link>
      <description>arXiv:2501.18195v1 Announce Type: new 
Abstract: Monte Carlo tests are widely used for computing valid p-values without requiring known distributions of test statistics. When performing multiple Monte Carlo tests, it is essential to maintain control of the type I error. Some techniques for multiplicity control pose requirements on the joint distribution of the p-values, for instance independence, which can be computationally intensive to achieve using na\"ive multiple Monte Carlo testing. We highlight in this work that multiple Monte Carlo testing is an instance of conformal novelty detection. Leveraging this insight enables a more efficient multiple Monte Carlo testing procedure, avoiding excessive simulations while still ensuring exact control over the false discovery rate or the family-wise error rate. We call this approach conformal multiple Monte Carlo testing. The performance is investigated in the context of global envelope tests for point pattern data through a simulation study and an application to a sweat gland data set. Results reveal that with a fixed number of simulations under the null hypothesis, our proposed method yields substantial improvements in power of the testing procedure as compared to the na\"ive multiple Monte Carlo testing procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18195v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christophe A. N. Biscio, Adrien Mazoyer, Martin V. Vejling</dc:creator>
    </item>
    <item>
      <title>Network Weighted Functional Regression: a method for modeling dependencies between functional data in a network</title>
      <link>https://arxiv.org/abs/2501.18221</link>
      <description>arXiv:2501.18221v1 Announce Type: new 
Abstract: This paper focuses on predicting continuous signals in a sensor lab network, particularly studying microclimate changes. We propose two novel concepts: Network Functional Data (NFD), which represents time series signals as functions on network nodes, and the Network Weighted Functional Regression (NWFR) model, which analyzes relationships between functional responses and predictors in a weighted network. Additionally, we introduce a functional conformal method to provide prediction bands with guaranteed coverage probabilities, independent of data distribution.
  Our statistical analysis on simulated and real-world data demonstrates that incorporating network structure enhances regression accuracy and improves the reliability of conformal prediction regions. These findings advance the analysis of complex network-structured data, offering a more precise and efficient approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18221v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Diana, Elvira Romano, Antonio Irpino</dc:creator>
    </item>
    <item>
      <title>Functional-Ordinal Canonical Correlation Analysis With Application to Data from Optical Sensors</title>
      <link>https://arxiv.org/abs/2501.18317</link>
      <description>arXiv:2501.18317v1 Announce Type: new 
Abstract: We address the problem of predicting a target ordinal variable based on observable features consisting of functional profiles. This problem is crucial, especially in decision-making driven by sensor systems, when the goal is to assess an ordinal variable such as the degree of deterioration, quality level, or risk stage of a process, starting from functional data observed via sensors. We purposely introduce a novel approach called functional-ordinal Canonical Correlation Analysis (foCCA), which is based on a functional data analysis approach. FoCCA allows for dimensionality reduction of observable features while maximizing their ability to differentiate between consecutive levels of an ordinal target variable. Unlike existing methods for supervised learning from functional data, foCCA fully incorporates the ordinal nature of the target variable. This enables the model to capture and represent the relative dissimilarities between consecutive levels of the ordinal target, while also explaining these differences through the functional features. Extensive simulations demonstrate that foCCA outperforms current state-of-the-art methods in terms of prediction accuracy in the reduced feature space. A case study involving the prediction of antigen concentration levels from optical biosensor signals further confirms the superior performance of foCCA, showcasing both improved predictive power and enhanced interpretability compared to competing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18317v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giulia Patan\`e, Federica Nicolussi, Alexander Krauth, G\"unter Gauglitz, Bianca Maria Colosimo, Luca Dede', Alessandra Menafoglio</dc:creator>
    </item>
    <item>
      <title>Univariate-Guided Sparse Regression</title>
      <link>https://arxiv.org/abs/2501.18360</link>
      <description>arXiv:2501.18360v1 Announce Type: new 
Abstract: In this paper, we introduce ``UniLasso'' -- a novel statistical method for regression. This two-stage approach preserves the signs of the univariate coefficients and leverages their magnitude. Both of these properties are attractive for stability and interpretation of the model. Through comprehensive simulations and applications to real-world datasets, we demonstrate that UniLasso outperforms Lasso in various settings, particularly in terms of sparsity and model interpretability. We prove asymptotic support recovery and mean-squared error consistency under a set of conditions different from the well-known irrepresentability conditions for the Lasso. Extensions to generalized linear models (GLMs) and Cox regression are also discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18360v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sourav Chatterjee, Trevor Hastie, Robert Tibshirani</dc:creator>
    </item>
    <item>
      <title>A tutorial on conducting sample size and power calculations for detecting treatment effect heterogeneity in cluster randomized trials</title>
      <link>https://arxiv.org/abs/2501.18383</link>
      <description>arXiv:2501.18383v1 Announce Type: new 
Abstract: Cluster-randomized trials (CRTs) are a well-established class of designs for evaluating large-scale, community-based research questions. An essential task in planning these trials is determining the required number of clusters and cluster sizes to achieve sufficient statistical power for detecting a clinically relevant effect size. Compared to methods for evaluating the average treatment effect (ATE) for the entire study population, there is more recent development of sample size methods for testing the heterogeneity of treatment effects (HTEs), i.e., modification of treatment effects by subpopulation characteristics, in CRTs. For confirmatory analyses of HTEs in CRTs, effect modifiers must be pre-specified, and ideally, accompanied by sample size or power calculations to ensure the trial has adequate power for the planned analyses. Power analysis for HTE analyses is more complex than for ATEs due to the additional design parameters that must be specified. Power and sample size formulas for HTE analyses have been separately derived under several cluster-randomized designs, including single and multi-period parallel designs, crossover designs, and stepped-wedge designs, as well as under continuous and binary outcomes. This tutorial provides a consolidated reference guide for these methods and enhances their accessibility through the development of an online R Shiny calculator. We further discuss key considerations for researchers conducting sample size and power calculations for testing pre-specified HTE hypotheses in CRTs, including the essential role of advance estimates of intracluster correlation coefficients for both outcomes and covariates on power. The sample size methodology and calculator functionality are demonstrated through real CRT examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18383v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mary Ryan Baumann, Monica Taljaard, Patrick J. Heagerty, Michael O. Harhay, Guangyu Tong, Rui Wang, Fan Li</dc:creator>
    </item>
    <item>
      <title>Log-Gaussian Cox Processes on General Metric Graphs</title>
      <link>https://arxiv.org/abs/2501.18558</link>
      <description>arXiv:2501.18558v1 Announce Type: new 
Abstract: The modeling of spatial point processes has advanced considerably, yet extending these models to non-Euclidean domains, such as road networks, remains a challenging problem. We propose a novel framework for log-Gaussian Cox processes on general compact metric graphs by leveraging the Gaussian Whittle-Mat\'ern fields, which are solutions to fractional-order stochastic differential equations on metric graphs. To achieve computationally efficient likelihood-based inference, we introduce a numerical approximation of the likelihood that eliminates the need to approximate the Gaussian process. This method, coupled with the exact evaluation of finite-dimensional distributions for Whittle-Mat\'ern fields with integer smoothness, ensures scalability and theoretical rigour, with derived convergence rates for posterior distributions. The framework is implemented in the open-source MetricGraph R package, which integrates seamlessly with R-INLA to support fully Bayesian inference. We demonstrate the applicability and scalability of this approach through an analysis of road accident data from Al-Ahsa, Saudi Arabia, consisting of over 150,000 road segments. By identifying high-risk road segments using exceedance probabilities and excursion sets, our framework provides localized insights into accident hotspots and offers a powerful tool for modeling spatial point processes directly on complex networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18558v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Bolin, Damilya Saduakhas, Alexandre B. Simas</dc:creator>
    </item>
    <item>
      <title>Prediction-Powered Inference with Imputed Covariates and Nonuniform Sampling</title>
      <link>https://arxiv.org/abs/2501.18577</link>
      <description>arXiv:2501.18577v1 Announce Type: new 
Abstract: Machine learning models are increasingly used to produce predictions that serve as input data in subsequent statistical analyses. For example, computer vision predictions of economic and environmental indicators based on satellite imagery are used in downstream regressions; similarly, language models are widely used to approximate human ratings and opinions in social science research. However, failure to properly account for errors in the machine learning predictions renders standard statistical procedures invalid. Prior work uses what we call the Predict-Then-Debias estimator to give valid confidence intervals when machine learning algorithms impute missing variables, assuming a small complete sample from the population of interest. We expand the scope by introducing bootstrap confidence intervals that apply when the complete data is a nonuniform (i.e., weighted, stratified, or clustered) sample and to settings where an arbitrary subset of features is imputed. Importantly, the method can be applied to many settings without requiring additional calculations. We prove that these confidence intervals are valid under no assumptions on the quality of the machine learning model and are no wider than the intervals obtained by methods that do not use machine learning predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18577v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dan M. Kluger, Kerri Lu, Tijana Zrnic, Sherrie Wang, Stephen Bates</dc:creator>
    </item>
    <item>
      <title>Statistical Tools for Frequency Response Functions from Posture Control Experiments: Estimation of Probability of a Sample and Comparison Between Groups of Unpaired Samples</title>
      <link>https://arxiv.org/abs/2501.17891</link>
      <description>arXiv:2501.17891v1 Announce Type: cross 
Abstract: The frequency response function (FRF) is an established way to describe the outcome of experiments in posture control literature. The FRF is an empirical transfer function between an input stimulus and the induced body segment sway profile, represented as a vector of complex values associated with a vector of frequencies. Having obtained an FRF from a trial with a subject, it can be useful to quantify the likelihood it belongs to a certain population, e.g., to diagnose a condition or to evaluate the human likeliness of a humanoid robot or a wearable device. In this work, a recently proposed method for FRF statistics based on confidence bands computed with bootstrap will be summarized, and, on its basis, possible ways to quantify the likelihood of FRFs belonging to a given set will be proposed. Furthermore, a statistical test to compare groups of unpaired samples is presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17891v1</guid>
      <category>eess.SP</category>
      <category>q-bio.NC</category>
      <category>stat.ME</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vittorio Lippi</dc:creator>
    </item>
    <item>
      <title>The No-Underrun Sampler: A Locally-Adaptive, Gradient-Free MCMC Method</title>
      <link>https://arxiv.org/abs/2501.18548</link>
      <description>arXiv:2501.18548v1 Announce Type: cross 
Abstract: In this work, we introduce the No-Underrun Sampler (NURS): a locally-adaptive, gradient-free Markov chain Monte Carlo method that combines elements of Hit-and-Run and the No-U-Turn Sampler. NURS dynamically adapts to the local geometry of the target distribution without requiring gradient evaluations, making it especially suitable for applications where gradients are unavailable or costly. We establish key theoretical properties, including reversibility, formal connections to Hit-and-Run and Random Walk Metropolis, Wasserstein contraction comparable to Hit-and-Run in Gaussian targets, and bounds on the total variation distance between the transition kernels of Hit-and-Run and NURS. Finally, we demonstrate - through empirical experiments supported by theoretical insights - that NURS can effectively sample Neal's funnel, a challenging multi-scale distribution from Bayesian hierarchical inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18548v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nawaf Bou-Rabee, Bob Carpenter, Sifan Liu, Stefan Oberd\"orster</dc:creator>
    </item>
    <item>
      <title>Dynamic treatment effects: high-dimensional inference under model misspecification</title>
      <link>https://arxiv.org/abs/2111.06818</link>
      <description>arXiv:2111.06818v3 Announce Type: replace 
Abstract: Estimating dynamic treatment effects is crucial across various disciplines, providing insights into the time-dependent causal impact of interventions. However, this estimation poses challenges due to time-varying confounding, leading to potentially biased estimates. Furthermore, accurately specifying the growing number of treatment assignments and outcome models with multiple exposures appears increasingly challenging to accomplish. Double robustness, which permits model misspecification, holds great value in addressing these challenges. This paper introduces a novel "sequential model doubly robust" estimator. We develop novel moment-targeting estimates to account for confounding effects and establish that root-$N$ inference can be achieved as long as at least one nuisance model is correctly specified at each exposure time, despite the presence of high-dimensional covariates. Although the nuisance estimates themselves do not achieve root-$N$ rates, the carefully designed loss functions in our framework ensure final root-$N$ inference for the causal parameter of interest. Unlike off-the-shelf high-dimensional methods, which fail to deliver robust inference under model misspecification even within the doubly robust framework, our newly developed loss functions address this limitation effectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.06818v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuqian Zhang, Weijie Ji, Jelena Bradic</dc:creator>
    </item>
    <item>
      <title>Statistical inference for radial generalized Pareto distributions and return sets in geometric extremes</title>
      <link>https://arxiv.org/abs/2310.06130</link>
      <description>arXiv:2310.06130v3 Announce Type: replace 
Abstract: We use a functional analogue of the quantile function for probability measures on $\mathbb{R}^d$ to characterize a novel limit Poisson point process for radially recentred and rescaled random vectors under a radial-directional decomposition. This limit process yields new multivariate distributions, including \textit{radial generalised Pareto distributions}, exhibiting stability for extrapolation to extremal sets along any direction. We show that the normalising functions leading to the limit Poisson point process correspond to a novel class of sets visited with fixed probability, with geometric properties determined by the conditional distribution of the radius given the direction and the Radon-Nikodym derivative of the directional probability distribution relative to reference spherical measures. This leads to return sets, defined by the complement of these probability sets and expressed by their return period. We identify an important member, the \textit{isotropic return set}, where all directions of exceedances outside the set are equally likely. Building on the limit Poisson point process likelihood, we develop parsimonious statistical models leveraging links between limit distribution parameters, with novel diagnostics for assessing convergence to the limiting distribution. These models enable Bayesian inference for return sets with arbitrarily large return periods and probabilities of unobserved extreme events, incorporating directional information from observations outside probability sets. The framework supports efficient computations in dimensions d=2 and d=3. We demonstrate the utility of the methods through simulations and case studies involving hydrological and oceanographic data, showcasing potential for robust and interpretable analysis of multivariate extremes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.06130v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ioannis Papastathopoulos, Lambert de Monte, Ryan Campbell, Haavard Rue</dc:creator>
    </item>
    <item>
      <title>Is control of type I error rate needed in Bayesian clinical trial designs?</title>
      <link>https://arxiv.org/abs/2312.15222</link>
      <description>arXiv:2312.15222v3 Announce Type: replace 
Abstract: Practical employment of Bayesian trial designs is still rare. Even if accepted in principle, the regulators have commonly required that such designs be calibrated according to an upper bound for the frequentist type I error rate. This represents an internally inconsistent hybrid methodology, where important advantages from following the Bayesian principles are lost. In particular, all preplanned interim looks have an inflating multiplicity effect on type I error rate. To present an alternative approach, we consider the prototype case of a 2-arm superiority trial with dichotomous outcomes. The design is adaptive, using error control based on sequentially updated posterior probabilities, to conclude efficacy of the experimental treatment or futility of the trial. As gatekeepers for a proposed design, the regulators have the main responsibility in determining the parameters of the control of false positives, whereas the trial sponsors and investigators will have a natural role in specifying the criteria for stopping the trial due to futility. It is suggested that the traditional frequentist operating characteristics in the design, type I and type II error rates, be replaced, respectively, by Bayesian criteria called False Discovery Probability (FDP) and False Futility Probability (FFP), both terms corresponding directly to their probability interpretations. Importantly, the sequential error control during the data analysis based on posterior probabilities will satisfy these numerical criteria automatically, without need of preliminary computations before the trial is started. The method contains the option of applying a decision rule for terminating the trial early if the predicted costs from continuing would exceed the corresponding gains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.15222v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elja Arjas, Dario Gasbarra</dc:creator>
    </item>
    <item>
      <title>Degrees-of-freedom penalized piecewise regression</title>
      <link>https://arxiv.org/abs/2312.16512</link>
      <description>arXiv:2312.16512v2 Announce Type: replace 
Abstract: Many popular piecewise regression models rely on minimizing a cost function on the model fit with a linear penalty on the number of segments. However, this penalty does not take into account varying complexities of the model functions on the segments potentially leading to overfitting when models with varying complexities, such as polynomials of different degrees, are used. In this work, we enhance on this approach by instead using a penalty on the sum of the degrees of freedom over all segments, called degrees-of-freedom penalized piecewise regression (DofPPR). We show that the solutions of the resulting minimization problem are unique for almost all input data in a least squares setting. We develop a fast algorithm which does not only compute a minimizer but also determines an optimal hyperparameter -- in the sense of rolling cross validation with the one standard error rule -- exactly. This eliminates manual hyperparameter selection. Our method supports optional user parameters for incorporating domain knowledge. We provide an open-source Python/Rust code for the piecewise polynomial least squares case which can be extended to further models. We demonstrate the practical utility through a simulation study and by applications to real data. A constrained variant of the proposed method gives state-of-the-art results in the Turing benchmark for unsupervised changepoint detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.16512v2</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stefan Volz, Martin Storath, Andreas Weinmann</dc:creator>
    </item>
    <item>
      <title>Conformal Prediction for Ensembles: Improving Efficiency via Score-Based Aggregation</title>
      <link>https://arxiv.org/abs/2405.16246</link>
      <description>arXiv:2405.16246v2 Announce Type: replace 
Abstract: Distribution-free uncertainty estimation for ensemble methods is increasingly desirable due to the widening deployment of multi-modal black-box predictive models. Conformal prediction is one approach that avoids such distributional assumptions. Methods for conformal aggregation have in turn been proposed for ensembled prediction, where the prediction regions of individual models are merged as to retain coverage guarantees while minimizing conservatism. Merging the prediction regions directly, however, sacrifices structures present in the conformal scores that can further reduce conservatism. We, therefore, propose a novel framework that extends the standard scalar formulation of a score function to a multivariate score that produces more efficient prediction regions. We then demonstrate that such a framework can be efficiently leveraged in both classification and predict-then-optimize regression settings downstream and empirically show the advantage over alternate conformal aggregation methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16246v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eduardo Ochoa Rivera, Yash Patel, Ambuj Tewari</dc:creator>
    </item>
    <item>
      <title>Semiparametric Localized Principal Stratification Analysis with Continuous Strata</title>
      <link>https://arxiv.org/abs/2406.13478</link>
      <description>arXiv:2406.13478v2 Announce Type: replace 
Abstract: Principal stratification is essential for revealing causal mechanisms involving post-treatment intermediate variables, in real-world applications like surrogate marker evaluation. Principal stratification analysis with continuous intermediate variables is increasingly common but challenging due to the infinite principal strata and the nonidentifiability and nonregularity of principal causal effects. Inspired by recent research, we resolve these challenges by first using a flexible copula-based principal score model to identify principal causal effect under weak principal ignorability. We then target the local functional substitute of principal causal effect, which is statistically regular and can accurately approximate principal causal effect with vanishing bandwidth. We simplify the full efficient influence function of the local functional substitute by considering its oracle-scenario alternative. This leads to a computationally efficient and straightforward estimator for the local functional substitute and principal causal effect with vanishing bandwidth. We prove the double robustness of our proposed estimator, and derive its asymptotic normality for inferential purposes. With a vanishing bandwidth, our method attains minimax optimality for the nonparametric estimation of the principal causal effect. With a fixed bandwidth, it achieves semiparametric efficiency in estimating its local functional substitute. We demonstrate the strong performance of our proposed estimator through simulations and apply it to surrogate analysis of short-term CD4 count in ACTG 175.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13478v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yichi Zhang, Shu Yang</dc:creator>
    </item>
    <item>
      <title>Cross Sectional Regression with Cluster Dependence: Inference based on Averaging</title>
      <link>https://arxiv.org/abs/2408.13514</link>
      <description>arXiv:2408.13514v2 Announce Type: replace 
Abstract: We re-investigate the asymptotic properties of the traditional OLS (pooled) estimator, $\hat{\beta} _P$, in the context of cluster dependence. The present study considers various scenarios under various restrictions on the cluster sizes and number of clusters. It is shown that $\hat{\beta}_P$ could be inconsistent in many realistic situations. We propose a simple estimator, $\hat{\beta}_A$ based on data averaging. The asymptotic properties of $\hat{\beta}_A$ are studied. It is shown that $\hat{\beta}_A$ is consistent even when $\hat{\beta}_P$ is inconsistent. It is further shown that the proposed estimator $\hat{\beta}_A$ is more efficient than $\hat{\beta}_P$ in many practical scenarios. As a consequence of averaging, we show that $\hat{\beta}_A$ retains consistency, asymptotic normality under classical measurement error problem circumventing the use of Instrumental Variables (IV). A detailed simulation study shows the efficacy of $\hat{\beta}_A$. It is also seen that $\hat{\beta}_A$ yields better goodness of fit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13514v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Subhodeep Dey, Gopal K. Basak, Samarjit Das</dc:creator>
    </item>
    <item>
      <title>Conformal Robust Control of Linear Systems</title>
      <link>https://arxiv.org/abs/2405.16250</link>
      <description>arXiv:2405.16250v2 Announce Type: replace-cross 
Abstract: End-to-end engineering design pipelines, in which designs are evaluated using concurrently defined optimal controllers, are becoming increasingly common in practice. To discover designs that perform well even under the misspecification of system dynamics, such end-to-end pipelines have now begun evaluating designs with a robust control objective in place of the nominal optimal control setup. Current approaches of specifying such robust control subproblems, however, rely on hand specification of perturbations anticipated to be present upon deployment or margin methods that ignore problem structure, resulting in a lack of theoretical guarantees and overly conservative empirical performance. We, instead, propose a novel methodology for LQR systems that leverages conformal prediction to specify such uncertainty regions in a data-driven fashion. Such regions have distribution-free coverage guarantees on the true system dynamics, in turn allowing for a probabilistic characterization of the regret of the resulting robust controller. We then demonstrate that such a controller can be efficiently produced via a novel policy gradient method that has convergence guarantees. We finally demonstrate the superior empirical performance of our method over alternate robust control specifications, such as $H_{\infty}$ and LQR with multiplicative noise, across a collection of engineering control systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16250v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>stat.ME</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yash Patel, Sahana Rayan, Ambuj Tewari</dc:creator>
    </item>
  </channel>
</rss>
