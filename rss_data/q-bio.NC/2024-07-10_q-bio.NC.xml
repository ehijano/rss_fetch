<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 10 Jul 2024 04:00:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 10 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Higher-Order Spatial Information for Self-Supervised Place Cell Learning</title>
      <link>https://arxiv.org/abs/2407.06195</link>
      <description>arXiv:2407.06195v1 Announce Type: new 
Abstract: Mammals navigate novel environments and exhibit resilience to sparse environmental sensory cues via place and grid cells, which encode position in space. While the efficiency of grid cell coding has been extensively studied, the computational role of place cells is less well understood. This gap arises partially because spatial information measures have, until now, been limited to single place cells. We derive and implement a higher-order spatial information measure, allowing for the study of the emergence of multiple place cells in a self-supervised manner. We show that emergent place cells have many desirable features, including high-accuracy spatial decoding. This is the first work in which higher-order spatial information measures that depend solely on place cells' firing rates have been derived and which focuses on the emergence of multiple place cells via self-supervised learning. By quantifying the spatial information of multiple place cells, we enhance our understanding of place cell formation and capabilities in recurrent neural networks, thereby improving the potential navigation capabilities of artificial systems in novel environments without objective location information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06195v1</guid>
      <category>q-bio.NC</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>math.IT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jared Deighton, Wyatt Mackey, Ioannis Schizas, David L. Boothe Jr., Vasileios Maroulas</dc:creator>
    </item>
    <item>
      <title>Neural population dynamics in songbird RA and HVC during learned motor-vocal behavior</title>
      <link>https://arxiv.org/abs/2407.06244</link>
      <description>arXiv:2407.06244v1 Announce Type: new 
Abstract: Complex, learned motor behaviors involve the coordination of large-scale neural activity across multiple brain regions, but our understanding of the population-level dynamics within different regions tied to the same behavior remains limited. Here, we investigate the neural population dynamics underlying learned vocal production in awake-singing songbirds. We use Neuropixels probes to record the simultaneous extracellular activity of populations of neurons in two regions of the vocal motor pathway. In line with observations made in non-human primates during limb-based motor tasks, we show that the population-level activity in both the premotor nucleus HVC and the motor nucleus RA is organized on low-dimensional neural manifolds upon which coordinated neural activity is well described by temporally structured trajectories during singing behavior. Both the HVC and RA latent trajectories provide relevant information to predict vocal sequence transitions between song syllables. However, the dynamics of these latent trajectories differ between regions. Our state-space models suggest a unique and continuous-over-time correspondence between the latent space of RA and vocal output, whereas the corresponding relationship for HVC exhibits a higher degree of neural variability. We then demonstrate that comparable high-fidelity reconstruction of continuous vocal outputs can be achieved from HVC and RA neural latents and spiking activity. Unlike those that use spiking activity, however, decoding models using neural latents generalize to novel sub-populations in each region, consistent with the existence of preserved manifolds that confine vocal-motor activity in HVC and RA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06244v1</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pablo Tostado-Marcos, Ezequiel M. Arneodo, Lauren Ostrowski, Daril E. Brown II, Xavier A. Perez, Adam Kadwory, Lauren L. Stanwicks, Abdullah Alothmanm, Timothy Q. Gentner, Vikash Gilja</dc:creator>
    </item>
    <item>
      <title>Beyond acoustics -- capacity limitations of linguistic levels</title>
      <link>https://arxiv.org/abs/2407.06596</link>
      <description>arXiv:2407.06596v1 Announce Type: new 
Abstract: Speech is a multiplexed signal displaying levels of complexity, organizational principles and perceptual units of analysis at distinct timescales. This critical acoustic signal for human communication is thus characterized at distinct representational and temporal scales, related to distinct linguistic features, from acoustic to supra-lexical. This chapter presents an overview of experimental work devoted to the characterization of the speech signal at different timescales, beyond its acoustic properties. The functional relevance of these different levels of analysis for speech processing is discussed. We advocate that studying speech perception through the prism of multi-time scale representations effectively integrates work from various research areas into a coherent picture and contributes significantly to increase our knowledge on the topic. Finally, we discuss how these experimental results fit with neural data and current dynamical models of speech perception.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06596v1</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J\'er\'emy Giroud (INS), Benjamin Morillon (INS)</dc:creator>
    </item>
    <item>
      <title>Shifts in Brain Dynamics and Drivers of Consciousness State Transitions</title>
      <link>https://arxiv.org/abs/2407.06928</link>
      <description>arXiv:2407.06928v1 Announce Type: new 
Abstract: Understanding the neural mechanisms underlying the transitions between different states of consciousness is a fundamental challenge in neuroscience. Thus, we investigate the underlying drivers of changes during the resting-state dynamics of the human brain, as captured by functional magnetic resonance imaging (fMRI) across varying levels of consciousness (awake, light sedation, deep sedation, and recovery). We deploy a model-based approach relying on linear time-invariant (LTI) dynamical systems under unknown inputs (UI). Our findings reveal distinct changes in the spectral profile of brain dynamics - particularly regarding the stability and frequency of the system's oscillatory modes during transitions between consciousness states. These models further enable us to identify external drivers influencing large-scale brain activity during naturalistic auditory stimulation. Our findings suggest that these identified inputs delineate how stimulus-induced co-activity propagation differs across consciousness states. Notably, our approach showcases the effectiveness of LTI models under UI in capturing large-scale brain dynamic changes and drivers in complex paradigms, such as naturalistic stimulation, which are not conducive to conventional general linear model analysis. Importantly, our findings shed light on how brain-wide dynamics and drivers evolve as the brain transitions towards conscious states, holding promise for developing more accurate biomarkers of consciousness recovery in disorders of consciousness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06928v1</guid>
      <category>q-bio.NC</category>
      <category>q-bio.QM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Joseph Bodenheimer, Paul Bogdan, S\'ergio Pequito, Arian Ashourvan</dc:creator>
    </item>
    <item>
      <title>Differentiable Optimization of Similarity Scores Between Models and Brains</title>
      <link>https://arxiv.org/abs/2407.07059</link>
      <description>arXiv:2407.07059v1 Announce Type: new 
Abstract: What metrics should guide the development of more realistic models of the brain? One proposal is to quantify the similarity between models and brains using methods such as linear regression, Centered Kernel Alignment (CKA), and angular Procrustes distance. To better understand the limitations of these similarity measures we analyze neural activity recorded in five experiments on nonhuman primates, and optimize synthetic datasets to become more similar to these neural recordings. How similar can these synthetic datasets be to neural activity while failing to encode task relevant variables? We find that some measures like linear regression and CKA, differ from angular Procrustes, and yield high similarity scores even when task relevant variables cannot be linearly decoded from the synthetic datasets. Synthetic datasets optimized to maximize similarity scores initially learn the first principal component of the target dataset, but angular Procrustes captures higher variance dimensions much earlier than methods like linear regression and CKA. We show in both theory and simulations how these scores change when different principal components are perturbed. And finally, we jointly optimize multiple similarity scores to find their allowed ranges, and show that a high angular Procrustes similarity, for example, implies a high CKA score, but not the converse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07059v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan Cloos, Moufan Li, Markus Siegel, Scott L. Brincat, Earl K. Miller, Guangyu Robert Yang, Christopher J. Cueva</dc:creator>
    </item>
    <item>
      <title>Resting state fMRI-based temporal coherence mapping</title>
      <link>https://arxiv.org/abs/2109.00146</link>
      <description>arXiv:2109.00146v2 Announce Type: replace 
Abstract: Long-range temporal coherence (LRTC) is quite common to dynamic systems and is fundamental to the system function. LRTC in the brain has been shown to be important to cognition. Assessing LRTC may provide critical information for understanding the potential underpinnings of brain organization, function, and cognition. To facilitate this overarching goal, we provide a method, which is named temporal coherence mapping (TCM), to explicitly quantify LRTC using resting state fMRI. TCM is based on correlation analysis of the transit states of the phase space reconstructed by temporal embedding. A few TCM properties were collected to measure LRTC, including the averaged correlation, anti-correlation, the ratio of correlation and anticorrelation, the mean coherent and incoherent duration, and the ratio between the coherent and incoherent time. TCM was first evaluated with simulations and then with the large Human Connectome Project data. Evaluation results showed that TCM metrics can successfully differentiate signals with different temporal coherence regardless of the parameters used to reconstruct the phase space. In human brain, TCM metrics except the ratio of the coherent/incoherent time showed high test-retest reproducibility; TCM metrics are related to age, sex, and total cognitive scores. In summary, TCM provides a first-of-its-kind tool to assess LRTC and the imbalance between coherence and incoherence; TCM properties are physiologically and cognitively meaningful.</description>
      <guid isPermaLink="false">oai:arXiv.org:2109.00146v2</guid>
      <category>q-bio.NC</category>
      <category>q-bio.QM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ze Wang</dc:creator>
    </item>
    <item>
      <title>A Concept-Value Network as a Brain Model</title>
      <link>https://arxiv.org/abs/1904.04579</link>
      <description>arXiv:1904.04579v4 Announce Type: replace-cross 
Abstract: This paper suggests a statistical framework for describing the relations between the physical and conceptual entities of a brain-like model. Features and concept instances are put into context, where the paper suggests that features may be the electrical wiring, although chemical connections are also possible. With this idea, the actual length of the connection is important, because it is related to firing rates and neuron synchronization, but the signal type is less important. The paper then suggests that concepts are neuron groups that link feature sets and concept instances are determined by chemical signals from those groups. Therefore, features become the static horizontal framework of the neural system and concepts are vertically interconnected combinations of these. With regards to functionality, the neuron is then considered to be functional and the more horizontal memory structures can be glial. This would also suggest that features can be distributed entities and not concentrated to a single area.</description>
      <guid isPermaLink="false">oai:arXiv.org:1904.04579v4</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kieran Greer</dc:creator>
    </item>
    <item>
      <title>Human Brain Exhibits Distinct Patterns When Listening to Fake Versus Real Audio: Preliminary Evidence</title>
      <link>https://arxiv.org/abs/2402.14982</link>
      <description>arXiv:2402.14982v3 Announce Type: replace-cross 
Abstract: In this paper we study the variations in human brain activity when listening to real and fake audio. Our preliminary results suggest that the representations learned by a state-of-the-art deepfake audio detection algorithm, do not exhibit clear distinct patterns between real and fake audio. In contrast, human brain activity, as measured by EEG, displays distinct patterns when individuals are exposed to fake versus real audio. This preliminary evidence enables future research directions in areas such as deepfake audio detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.14982v3</guid>
      <category>cs.SD</category>
      <category>cs.LG</category>
      <category>eess.AS</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mahsa Salehi, Kalin Stefanov, Ehsan Shareghi</dc:creator>
    </item>
  </channel>
</rss>
