<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SE</link>
    <description>cs.SE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 07 Jan 2025 03:20:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Test Input Validation for Vision-based DL Systems: An Active Learning Approach</title>
      <link>https://arxiv.org/abs/2501.01606</link>
      <description>arXiv:2501.01606v1 Announce Type: new 
Abstract: Testing deep learning (DL) systems requires extensive and diverse, yet valid, test inputs. While synthetic test input generation methods, such as metamorphic testing, are widely used for DL testing, they risk introducing invalid inputs that do not accurately reflect real-world scenarios. Invalid test inputs can lead to misleading results. Hence, there is a need for automated validation of test inputs to ensure effective assessment of DL systems. In this paper, we propose a test input validation approach for vision-based DL systems. Our approach uses active learning to balance the trade-off between accuracy and the manual effort required for test input validation. Further, by employing multiple image-comparison metrics, it achieves better results in classifying valid and invalid test inputs compared to methods that rely on single metrics. We evaluate our approach using an industrial and a public-domain dataset. Our evaluation shows that our multi-metric, active learning-based approach produces several optimal accuracy-effort trade-offs, including those deemed practical and desirable by our industry partner. Furthermore, provided with the same level of manual effort, our approach is significantly more accurate than two state-of-the-art test input validation methods, achieving an average accuracy of 97%. Specifically, the use of multiple metrics, rather than a single metric, results in an average improvement of at least 5.4% in overall accuracy compared to the state-of-the-art baselines. Incorporating an active learning loop for test input validation yields an additional 7.5% improvement in average accuracy, bringing the overall average improvement of our approach to at least 12.9% compared to the baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01606v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Delaram Ghobari, Mohammad Hossein Amini, Dai Quoc Tran, Seunghee Park, Shiva Nejati, Mehrdad Sabetzadeh</dc:creator>
    </item>
    <item>
      <title>How Toxic Can You Get? Search-based Toxicity Testing for Large Language Models</title>
      <link>https://arxiv.org/abs/2501.01741</link>
      <description>arXiv:2501.01741v1 Announce Type: new 
Abstract: Language is a deep-rooted means of perpetration of stereotypes and discrimination. Large Language Models (LLMs), now a pervasive technology in our everyday lives, can cause extensive harm when prone to generating toxic responses. The standard way to address this issue is to align the LLM, which, however, dampens the issue without constituting a definitive solution. Therefore, testing LLM even after alignment efforts remains crucial for detecting any residual deviations with respect to ethical standards. We present EvoTox, an automated testing framework for LLMs' inclination to toxicity, providing a way to quantitatively assess how much LLMs can be pushed towards toxic responses even in the presence of alignment. The framework adopts an iterative evolution strategy that exploits the interplay between two LLMs, the System Under Test (SUT) and the Prompt Generator steering SUT responses toward higher toxicity. The toxicity level is assessed by an automated oracle based on an existing toxicity classifier. We conduct a quantitative and qualitative empirical evaluation using four state-of-the-art LLMs as evaluation subjects having increasing complexity (7-13 billion parameters). Our quantitative evaluation assesses the cost-effectiveness of four alternative versions of EvoTox against existing baseline methods, based on random search, curated datasets of toxic prompts, and adversarial attacks. Our qualitative assessment engages human evaluators to rate the fluency of the generated prompts and the perceived toxicity of the responses collected during the testing sessions. Results indicate that the effectiveness, in terms of detected toxicity level, is significantly higher than the selected baseline methods (effect size up to 1.0 against random search and up to 0.99 against adversarial attacks). Furthermore, EvoTox yields a limited cost overhead (from 22% to 35% on average).</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01741v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simone Corbo, Luca Bancale, Valeria De Gennaro, Livia Lestingi, Vincenzo Scotti, Matteo Camilli</dc:creator>
    </item>
    <item>
      <title>Leveraging Sustainable Systematic Literature Reviews</title>
      <link>https://arxiv.org/abs/2501.01819</link>
      <description>arXiv:2501.01819v1 Announce Type: new 
Abstract: Systematic Literature Reviews (SLRs) are a widely employed research method in software engineering. However, there are several problems with SLRs, including the enormous time and effort to conduct them and the lack of obvious impacts of SLR results on software engineering practices and industry projects. To address these problems, the concepts of \textit{sustainability} and \textit{sustainable SLR} have been proposed, aiming to raise awareness among researchers about the importance of dealing with SLR problems in a consistent way; however, practical and concrete actions are still lacking. This paper presents concrete directions towards sustainable SLRs. We first identified 18 ``green drivers'' (GD) that could directly impact SLR sustainability, and we distilled 25 sustainability indicators (SI) associated with the GD to assess SLRs regarding their sustainability. A preliminary evaluation was conducted on the ten top-cited SLRs in software engineering published over the last decade. From this analysis, we synthesized our insights into 12 leverage points for sustainability. Our results indicate that even in high-quality reviews, there are threats to sustainability, such as: flaws in the search process, lack of essential details in the documentation, weak collaboration with stakeholders, poor knowledge management, lack of use of supporting tools, and a dearth of practical insights for software engineering practitioners. The good news is that moving towards sustainable SLRs only requires some simple actions, which can pave the way for a profound change in the software engineering community's mindset about how to create and sustain SLRs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01819v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Vinicius dos Santos, Rick Kazman, Elisa Yumi Nakagawa</dc:creator>
    </item>
    <item>
      <title>Latent Mutants: A large-scale study on the Interplay between mutation testing and software evolution</title>
      <link>https://arxiv.org/abs/2501.01873</link>
      <description>arXiv:2501.01873v1 Announce Type: new 
Abstract: In this paper we apply mutation testing in an in-time fashion, i.e., across multiple project releases. Thus, we investigate how the mutants of the current version behave in the future versions of the programs. We study the characteristics of what we call latent mutants, i.e., the mutants that are live in one version and killed in later revisions, and explore whether they are predictable with these properties. We examine 131,308 mutants generated by Pitest on 13 open-source projects. Around 11.2% of these mutants are live, and 3.5% of them are latent, manifesting in 104 days on average. Using the mutation operators and change-related features we successfully demonstrate that these latent mutants are identifiable, predicting them with an accuracy of 86% and a balanced accuracy of 67% using a simple random forest classifier.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01873v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeongju Sohn, Ezekiel Soremekun, Michail Papadakis</dc:creator>
    </item>
    <item>
      <title>Accuracy Can Lie: On the Impact of Surrogate Model in Configuration Tuning</title>
      <link>https://arxiv.org/abs/2501.01876</link>
      <description>arXiv:2501.01876v1 Announce Type: new 
Abstract: To ease the expensive measurements during configuration tuning, it is natural to build a surrogate model as the replacement of the system, and thereby the configuration performance can be cheaply evaluated. Yet, a stereotype therein is that the higher the model accuracy, the better the tuning result would be. This "accuracy is all" belief drives our research community to build more and more accurate models and criticize a tuner for the inaccuracy of the model used. However, this practice raises some previously unaddressed questions, e.g., Do those somewhat small accuracy improvements reported in existing work really matter much to the tuners? What role does model accuracy play in the impact of tuning quality? To answer those related questions, we conduct one of the largest-scale empirical studies to date-running over the period of 13 months 24*7-that covers 10 models, 17 tuners, and 29 systems from the existing works while under four different commonly used metrics, leading to 13,612 cases of investigation. Surprisingly, our key findings reveal that the accuracy can lie: there are a considerable number of cases where higher accuracy actually leads to no improvement in the tuning outcomes (up to 58% cases under certain setting), or even worse, it can degrade the tuning quality (up to 24% cases under certain setting). We also discover that the chosen models in most proposed tuners are sub-optimal and that the required % of accuracy change to significantly improve tuning quality varies according to the range of model accuracy. Deriving from the fitness landscape analysis, we provide in-depth discussions of the rationale behind, offering several lessons learned as well as insights for future opportunities. Most importantly, this work poses a clear message to the community: we should take one step back from the natural "accuracy is all" belief for model-based configuration tuning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01876v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TSE.2025.3525955</arxiv:DOI>
      <dc:creator>Pengzhou Chen, Jingzhi Gong, Tao Chen</dc:creator>
    </item>
    <item>
      <title>Teaching Mining Software Repositories</title>
      <link>https://arxiv.org/abs/2501.01903</link>
      <description>arXiv:2501.01903v1 Announce Type: new 
Abstract: Mining Software Repositories (MSR) has become a popular research area recently. MSR analyzes different sources of data, such as version control systems, code repositories, defect tracking systems, archived communication, deployment logs, and so on, to uncover interesting and actionable insights from the data for improved software development, maintenance, and evolution. This chapter provides an overview of MSR and how to conduct an MSR study, including setting up a study, formulating research goals and questions, identifying repositories, extracting and cleaning the data, performing data analysis and synthesis, and discussing MSR study limitations. Furthermore, the chapter discusses MSR as part of a mixed method study, how to mine data ethically, and gives an overview of recent trends in MSR as well as reflects on the future. As a teaching aid, the chapter provides tips for educators, exercises for students at all levels, and a list of repositories that can be used as a starting point for an MSR study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01903v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zadia Codabux, Fatemeh Fard, Roberto Verdecchia, Fabio Palomba, Dario Di Nucci, Gilberto Recupito</dc:creator>
    </item>
    <item>
      <title>Innovative Approaches to Teaching Quantum Computer Programming and Quantum Software Engineering</title>
      <link>https://arxiv.org/abs/2501.01446</link>
      <description>arXiv:2501.01446v1 Announce Type: cross 
Abstract: Quantum computing is an emerging field that promises to revolutionize various domains, such as simulation optimization, data processing, and more, by leveraging the principles of quantum mechanics. This paper outlines innovative pedagogical strategies developed by university lecturers in Finland and Spain for teaching quantum computer programming and quantum software engineering. Our curriculum integrates essential tools and methodologies such as containerization with Docker, Qiskit, PennyLane, and Ocean SDK to provide a comprehensive learning experience. The approach consists of several steps, from introducing the fundamentals of quantum mechanics to hands-on labs focusing on practical use cases. We believe quantum computer programming is an important topic and one that is hard to teach, so having a teaching agenda and guidelines for teaching can be of great help.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01446v1</guid>
      <category>physics.ed-ph</category>
      <category>cs.SE</category>
      <category>quant-ph</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Majid Haghparast, Enrique Moguel, Jose Garcia-Alonso, Tommi Mikkonen, Juan Manuel Murillo</dc:creator>
    </item>
    <item>
      <title>FairSense: Long-Term Fairness Analysis of ML-Enabled Systems</title>
      <link>https://arxiv.org/abs/2501.01665</link>
      <description>arXiv:2501.01665v1 Announce Type: cross 
Abstract: Algorithmic fairness of machine learning (ML) models has raised significant concern in the recent years. Many testing, verification, and bias mitigation techniques have been proposed to identify and reduce fairness issues in ML models. The existing methods are model-centric and designed to detect fairness issues under static settings. However, many ML-enabled systems operate in a dynamic environment where the predictive decisions made by the system impact the environment, which in turn affects future decision-making. Such a self-reinforcing feedback loop can cause fairness violations in the long term, even if the immediate outcomes are fair. In this paper, we propose a simulation-based framework called FairSense to detect and analyze long-term unfairness in ML-enabled systems. Given a fairness requirement, FairSense performs Monte-Carlo simulation to enumerate evolution traces for each system configuration. Then, FairSense performs sensitivity analysis on the space of possible configurations to understand the impact of design options and environmental factors on the long-term fairness of the system. We demonstrate FairSense's potential utility through three real-world case studies: Loan lending, opioids risk scoring, and predictive policing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01665v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>cs.SE</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Yining She, Sumon Biswas, Christian K\"astner, Eunsuk Kang</dc:creator>
    </item>
    <item>
      <title>Towards a Maturity Model for Systematic Literature Review Process</title>
      <link>https://arxiv.org/abs/2206.11936</link>
      <description>arXiv:2206.11936v2 Announce Type: replace 
Abstract: Systematic literature reviews (SLR) have been increasingly conducted in software engineering and they provide significant benefits in terms of summarizing the state of the research. The process of conducting SLR is complex, involving several activities and consuming considerable effort and time from researchers. Researchers often skip or poorly conduct essential activities, which introduce threats to validity, resulting in lower-quality SLR. But researchers are often unaware of what they could do to mature their SLR process, thus improving the SLR quality. The main goal of this paper is to introduce a maturity model for the SLR process named MM4SLR. To this end, we were inspired by well-known models like CMMI (Capability Maturity Model Integration). We first identified 39 key practices for SLR from the literature and grouped them into nine goals that were further grouped into five process areas. We then organized the process areas into five maturity levels which compose our model. Our proof of concept, applying the MM4SLR to four published SLR showed that the MM4SLR is suitable for appraising SLR and can identify important flaws in SLR quality. MM4SLR can therefore support researchers in creating their SLR processes and selecting practices that could be adopted to mature their processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.11936v2</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Vinicius dos Santos, Rick Kazman, Rafael Capilla, Elisa Yumi Nakagawa</dc:creator>
    </item>
    <item>
      <title>A Reference Architecture for Governance of Cloud Native Applications</title>
      <link>https://arxiv.org/abs/2302.11617</link>
      <description>arXiv:2302.11617v2 Announce Type: replace 
Abstract: The evolution of cloud computing has given rise to Cloud Native Applications (CNAs), presenting new challenges in governance, particularly when faced with strict compliance requirements. This work explores the unique characteristics of CNAs and their impact on governance. We introduce a comprehensive reference architecture designed to streamline governance across CNAs along with a sample implementation, offering insights for both single and multi-cloud environments. Our architecture seamlessly integrates governance within the CNA framework, adhering to a "battery-included" philosophy. Tailored for both expansive and compact CNA deployments across various industries, this design enables cloud practitioners to prioritize product development by alleviating the complexities associated with governance. In addition, it provides a building block for academic exploration of generic CNA frameworks, highlighting their relevance in the evolving cloud computing landscape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.11617v2</guid>
      <category>cs.SE</category>
      <category>cs.DC</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>William Pourmajidi, Lei Zhang, John Steinbacher, Tony Erwin, Andriy Miranskyy</dc:creator>
    </item>
    <item>
      <title>Do Developers Adopt Green Architectural Tactics for ML-Enabled Systems? A Mining Software Repository Study</title>
      <link>https://arxiv.org/abs/2410.06708</link>
      <description>arXiv:2410.06708v2 Announce Type: replace 
Abstract: As machine learning (ML) and artificial intelligence (AI) technologies become more widespread, concerns about their environmental impact are increasing due to the resource-intensive nature of training and inference processes. Green AI advocates for reducing computational demands while still maintaining accuracy. Although various strategies for creating sustainable ML systems have been identified, their real-world implementation is still underexplored. This paper addresses this gap by studying 168 open-source ML projects on GitHub. It employs a novel large language model (LLM)-based mining mechanism to identify and analyze green strategies. The findings reveal the adoption of established tactics that offer significant environmental benefits. This provides practical insights for developers and paves the way for future automation of sustainable practices in ML systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06708v2</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Vincenzo De Martino, Silverio Mart\'inez-Fern\'andez, Fabio Palomba</dc:creator>
    </item>
    <item>
      <title>Evaluating the Ability of Large Language Models to Generate Verifiable Specifications in VeriFast</title>
      <link>https://arxiv.org/abs/2411.02318</link>
      <description>arXiv:2411.02318v3 Announce Type: replace 
Abstract: Static verification is a powerful method for enhancing software quality, but it demands significant human labor and resources. This is particularly true of static verifiers that reason about heap manipulating programs using an ownership logic. LLMs have shown promise in a number of software engineering activities, including code generation, test generation, proof generation for theorem provers, and specification generation for static verifiers. However, prior work has not explored how well LLMs can perform specification generation for specifications based in an ownership logic, such as separation logic. To address this gap, this paper explores OpenAI's GPT-4o model's effectiveness in generating specifications on C programs that are verifiable with VeriFast, a separation logic based static verifier. Our experiment employs three different types of user inputs as well as basic and Chain-of-Thought (CoT) prompting to assess GPT's capabilities. Our results indicate that the specifications generated by GPT-4o preserve functional behavior, but struggle to be verifiable. When the specifications are verifiable they contain redundancies. Future directions are discussed to improve the performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02318v3</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wen Fan, Marilyn Rego, Xin Hu, Sanya Dod, Zhaorui Ni, Danning Xie, Jenna DiVincenzo, Lin Tan</dc:creator>
    </item>
    <item>
      <title>RCAEval: A Benchmark for Root Cause Analysis of Microservice Systems with Telemetry Data</title>
      <link>https://arxiv.org/abs/2412.17015</link>
      <description>arXiv:2412.17015v2 Announce Type: replace 
Abstract: Root cause analysis (RCA) for microservice systems has gained significant attention in recent years. However, there is still no standard benchmark that includes large-scale datasets and supports comprehensive evaluation environments. In this paper, we introduce RCAEval, an open-source benchmark that provides datasets and an evaluation environment for RCA in microservice systems. First, we introduce three comprehensive datasets comprising 735 failure cases collected from three microservice systems, covering various fault types observed in real-world failures. Second, we present a comprehensive evaluation framework that includes fifteen reproducible baselines covering a wide range of RCA approaches, with the ability to evaluate both coarse-grained and fine-grained RCA. RCAEval is designed to support both researchers and practitioners. We hope that this ready-to-use benchmark will enable researchers and practitioners to conduct extensive analysis and pave the way for robust new solutions for RCA of microservice systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17015v2</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luan Pham, Hongyu Zhang, Huong Ha, Flora Salim, Xiuzhen Zhang</dc:creator>
    </item>
    <item>
      <title>DELA: A Novel Approach for Detecting Errors Induced by Large Atomic Condition Numbers</title>
      <link>https://arxiv.org/abs/2412.20804</link>
      <description>arXiv:2412.20804v2 Announce Type: replace 
Abstract: Numerical programs form the foundation of modern science and engineering, providing essential solutions to complex mathematical problems. Therefore, errors in numerical results would lead to harmful consequences, especially in safety-critical applications. Since only a few inputs may lead to substantial errors for numerical programs, it is essential to determine whether a given input could result in a significant error. Existing researchers tend to use the results of high-precision programs to assess whether there is a substantial error, which introduces three main challenges: difficulty of implementation, existence of potential faults in the detection of numerical errors, and long execution time.
  To address these limitations, we propose a novel approach named DELA. Our approach is based on the observation that most numerical errors stem from large condition numbers in atomic operations (such as subtraction), which then propagate and accumulate. DELA injects small perturbations into the results of individual atomic operations within the program and compares the outcomes of the original program with the perturbed version to detect errors. We evaluate DELA with datasets from ATOMU and HSED, as well as data from a complex linear system-solving program. Experimental results demonstrate that we can detect all the significant errors that were reported by prior research. DELA shows strong alignment with high-precision programs of ATOMU and HSED, with average Pearson and Spearman correlations of 0.86 and 0.61. Additionally, DELA effectively detects significant errors in complex programs, achieving correlation scores of 0.9763 and 0.8993. More importantly, in experiments with ATOMU and HSED, DELA's perturbed programs run within only 0.13% of the time needed by high-precision versions; while for the linear system-solving programs, DELA is 73.46 times faster than the high-precision programs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20804v2</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Youshuai Tan, Zhanwei Zhang, Jinfu Chen, Zishuo Ding, Jifeng Xuan, Weiyi Shang</dc:creator>
    </item>
    <item>
      <title>Language Models for Code Optimization: Survey, Challenges and Future Directions</title>
      <link>https://arxiv.org/abs/2501.01277</link>
      <description>arXiv:2501.01277v2 Announce Type: replace 
Abstract: Language models (LMs) built upon deep neural networks (DNNs) have recently demonstrated breakthrough effectiveness in software engineering tasks such as code generation, completion, and repair. This has paved the way for the emergence of LM-based code optimization techniques, which are crucial for enhancing the performance of existing programs, such as accelerating program execution time. However, a comprehensive survey dedicated to this specific application has been lacking. To fill this gap, we present a systematic literature review of over 50 primary studies, identifying emerging trends and addressing 11 specialized questions. Our findings reveal five critical open challenges, such as balancing model complexity with practical usability, cross-language/performance generalizability, and building trust in AI-driven solutions. Furthermore, we provide eight future research directions to facilitate more efficient, robust, and reliable LM-based code optimization. Thereby, this study aims to provide actionable insights and foundational references for both researchers and practitioners in this rapidly evolving field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01277v2</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingzhi Gong, Vardan Voskanyan, Paul Brookes, Fan Wu, Wei Jie, Jie Xu, Rafail Giavrimis, Mike Basios, Leslie Kanthan, Zheng Wang</dc:creator>
    </item>
  </channel>
</rss>
