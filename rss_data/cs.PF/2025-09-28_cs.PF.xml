<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PF updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PF</link>
    <description>cs.PF updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PF" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 29 Sep 2025 04:01:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Size-Aware Dispatching to Fluid Queues</title>
      <link>https://arxiv.org/abs/2509.21693</link>
      <description>arXiv:2509.21693v1 Announce Type: new 
Abstract: We develop a fluid-flow model for routing problems, where fluid consists of different size particles and the task is to route the incoming fluid to $n$ parallel servers using the size information in order to minimize the mean latency. The problem corresponds to the dispatching problem of (discrete) jobs arriving according to a stochastic process. In the fluid model the problem reduces to finding an optimal path to empty the system in $n$-dimensional space. We use the calculus of variation to characterize the structure of optimal policies. Numerical examples shed further light on the fluid routing problem and the optimal control of large distributed service systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21693v1</guid>
      <category>cs.PF</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Runhan Xie, Esa Hyyti\"a, Rhonda Righter</dc:creator>
    </item>
    <item>
      <title>SAHM: State-Aware Heterogeneous Multicore for Single-Thread Performance</title>
      <link>https://arxiv.org/abs/2509.22405</link>
      <description>arXiv:2509.22405v1 Announce Type: new 
Abstract: Improving single-thread performance remains a critical challenge in modern processor design, as conventional approaches such as deeper speculation, wider pipelines, and complex out-of-order execution face diminishing returns. This work introduces SAHM-State-Aware Heterogeneous Multicore-a novel architecture that targets performance gains by exploiting fine-grained, time-varying behavioral diversity in single-threaded workloads. Through empirical characterization of performance counter data, we define 16 distinct behavioral states representing different microarchitectural demands. Rather than over-provisioning a monolithic core with all optimizations, SAHM uses a set of specialized cores tailored to specific states and migrates threads at runtime based on detected behavior. This design enables composable microarchitectural enhancements without incurring prohibitive area, power, or complexity costs.
  We evaluate SAHM in both single-threaded and multiprogrammed scenarios, demonstrating its ability to maintain core utilization while improving overall performance through intelligent state-driven scheduling. Experimental results show opportunity for 17% speed up in realistic scenarios. These speed ups are robust against high-cost migration, decreasing by less than 1%. Overall, state-aware core specialization is a new path forward for enhancing single-thread performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22405v1</guid>
      <category>cs.PF</category>
      <category>cs.AR</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shayne Wadle, Karthikeyan Sankaralingam</dc:creator>
    </item>
    <item>
      <title>Redesigning GROMACS Halo Exchange: Improving Strong Scaling with GPU-initiated NVSHMEM</title>
      <link>https://arxiv.org/abs/2509.21527</link>
      <description>arXiv:2509.21527v1 Announce Type: cross 
Abstract: Improving time-to-solution in molecular dynamics simulations often requires strong scaling due to fixed-sized problems. GROMACS is highly latency-sensitive, with peak iteration rates in the sub-millisecond, making scalability on heterogeneous supercomputers challenging. MPI's CPU-centric nature introduces additional latencies on GPU-resident applications' critical path, hindering GPU utilization and scalability. To address these limitations, we present an NVSHMEM-based GPU kernel-initiated redesign of the GROMACS domain decomposition halo-exchange algorithm. Highly tuned GPU kernels fuse data packing and communication, leveraging hardware latency-hiding for fine-grained overlap. We employ kernel fusion across overlapped data forwarding communication phases and utilize the asynchronous copy engine over NVLink to optimize latency and bandwidth. Our GPU-resident formulation greatly increases communication-computation overlap, improving GROMACS strong scaling performance across NVLink by up to 1.5x (intra-node) and 2x (multi-node), and up to 1.3x multi-node over NVLink+InfiniBand. This demonstrates the profound benefits of GPU-initiated communication for strong-scaling a broad range of latency-sensitive applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21527v1</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <category>physics.comp-ph</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3731599.3767508</arxiv:DOI>
      <dc:creator>Mahesh Doijade, Andrey Alekseenko, Ania Brown, Alan Gray, Szil\'ard P\'all</dc:creator>
    </item>
    <item>
      <title>PreLoRA: Hybrid Pre-training of Vision Transformers with Full Training and Low-Rank Adapters</title>
      <link>https://arxiv.org/abs/2509.21619</link>
      <description>arXiv:2509.21619v1 Announce Type: cross 
Abstract: Training large models ranging from millions to billions of parameters is highly resource-intensive, requiring significant time, compute, and memory. It is observed that most of the learning (higher change in weights) takes place in the earlier stage of the training loop. These changes stabilize as training continues, enabling them to be captured by matrices of a low intrinsic rank. Therefore, we propose an approach to identify such states of partial convergence and dynamically switch from full parameter training to Low-Rank Adaptation (LoRA) on the ViT-Large model. We introduce a flexible approach that leverages user-defined hyperparameters to determine the switching point and assign a rank specific to each module layer based on its level of convergence. Experimental results show that this approach preserves model accuracy while reducing the number of trainable parameters to 10% of its original size, resulting in a 3x improvement in throughput, and a 1.5x reduction in average training time per epoch while also reducing GPU memory consumption by 20%</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21619v1</guid>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Krishu K Thapa, Reet Barik, Krishna Teja Chitty-Venkata, Murali Emani, Venkatram Vishwanath</dc:creator>
    </item>
    <item>
      <title>Less is More: Faster Maximum Clique Search by Work-Avoidance</title>
      <link>https://arxiv.org/abs/2509.22245</link>
      <description>arXiv:2509.22245v1 Announce Type: cross 
Abstract: The maximum clique (MC) problem is a challenging graph mining problem which, due to its NP-hard nature, can take a substantial amount of execution time. The MC problem is dominated by set intersection operations similar to Maximal Clique Enumeration, however it differs in requiring to find only a clique of maximum size. As such, key to the problem is to demonstrate efficiently that a particular part of the search space does not contain a maximum clique, allowing to skip over major parts of the search space. We present a number of techniques to optimize MC search in light of leaving major parts of the search space unvisited, including (i) an efficient, lazily constructed graph representation; (ii) filtering prior to initiating a detailed search; (iii) efficient early-exit intersection algorithms; (iv) exploiting algorithmic choice. These techniques result in a speedup of up to 38.9x compared to PMC, which is the most comparable algorithm, and a speedup up to 11x over MC-BRB.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22245v1</guid>
      <category>cs.DS</category>
      <category>cs.PF</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/IPDPS64566.2025.00025</arxiv:DOI>
      <arxiv:journal_reference>2025 IEEE International Parallel and Distributed Processing Symposium (IPDPS), Milano, Italy, 2025, pp. 187-198</arxiv:journal_reference>
      <dc:creator>Hans Vandierendonck</dc:creator>
    </item>
    <item>
      <title>QECO: A QoE-Oriented Computation Offloading Algorithm based on Deep Reinforcement Learning for Mobile Edge Computing</title>
      <link>https://arxiv.org/abs/2311.02525</link>
      <description>arXiv:2311.02525v3 Announce Type: replace-cross 
Abstract: In the realm of mobile edge computing (MEC), efficient computation task offloading plays a pivotal role in ensuring a seamless quality of experience (QoE) for users. Maintaining a high QoE is paramount in today's interconnected world, where users demand reliable services. This challenge stands as one of the most primary key factors contributing to handling dynamic and uncertain mobile environments. In this study, we delve into computation offloading in MEC systems, where strict task processing deadlines and energy constraints can adversely affect the system performance. We formulate the computation task offloading problem as a Markov decision process (MDP) to maximize the long-term QoE of each user individually. We propose a distributed QoE-oriented computation offloading (QECO) algorithm based on deep reinforcement learning (DRL) that empowers mobile devices to make their offloading decisions without requiring knowledge of decisions made by other devices. Through numerical studies, we evaluate the performance of QECO. Simulation results reveal that compared to the state-of-the-art existing works, QECO increases the number of completed tasks by up to 14.4%, while simultaneously reducing task delay and energy consumption by 9.2% and 6.3%, respectively. Together, these improvements result in a significant average QoE enhancement of 37.1%. This substantial improvement is achieved by accurately accounting for user dynamics and edge server workloads when making intelligent offloading decisions. This highlights QECO's effectiveness in enhancing users' experience in MEC systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.02525v3</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TNSE.2025.3556809</arxiv:DOI>
      <arxiv:journal_reference>IEEE Trans. Netw. Sci. Eng., vol. 12, no. 4, pp. 3118-3130, 2025</arxiv:journal_reference>
      <dc:creator>Iman Rahmaty, Hamed Shah-Mansouri, Ali Movaghar</dc:creator>
    </item>
    <item>
      <title>SuperCoder: Assembly Program Superoptimization with Large Language Models</title>
      <link>https://arxiv.org/abs/2505.11480</link>
      <description>arXiv:2505.11480v2 Announce Type: replace-cross 
Abstract: Superoptimization is the task of transforming a program into a faster one while preserving its input-output behavior. In this work, we investigate whether large language models (LLMs) can serve as superoptimizers, generating assembly programs that outperform code already optimized by industry-standard compilers. We construct the first large-scale benchmark for this problem, consisting of 8,072 real-world assembly programs averaging 130 lines, in contrast to prior datasets restricted to 2-15 straight-line, loop-free programs. We evaluate 23 LLMs on this benchmark and find that the strongest baseline, Claude-opus-4, achieves a 51.5% test-passing rate and a 1.43x average speedup over gcc -O3. To further enhance performance, we fine-tune models with reinforcement learning, optimizing a reward function that integrates correctness and performance speedup. Starting from Qwen2.5-Coder-7B-Instruct (61.4% correctness, 1.10x speedup), the fine-tuned model SuperCoder attains 95.0% correctness and 1.46x average speedup. Our results demonstrate, for the first time, that LLMs can be applied as superoptimizers for assembly programs, establishing a foundation for future research in program performance optimization beyond compiler heuristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11480v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.PF</category>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anjiang Wei, Tarun Suresh, Huanmi Tan, Yinglun Xu, Gagandeep Singh, Ke Wang, Alex Aiken</dc:creator>
    </item>
  </channel>
</rss>
