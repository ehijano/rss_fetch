<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 17 Jan 2025 05:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A simpler QPTAS for scheduling jobs with precedence constraints</title>
      <link>https://arxiv.org/abs/2501.09091</link>
      <description>arXiv:2501.09091v1 Announce Type: new 
Abstract: We study the classical scheduling problem of minimizing the makespan
  of a set of unit size jobs with
  precedence constraints on parallel identical machines. Research on the problem dates back to the
  landmark paper by Graham from 1966 who showed that the simple List
  Scheduling algorithm is a $(2-\frac{1}{m})$-approximation. Interestingly,
  it is open whether the problem is NP-hard if $m=3$ which is one of
  the few remaining open problems in the seminal book by Garey and Johnson.
  Recently, quite some progress has been made for the setting that $m$
  is a constant. In a break-through paper, Levey and Rothvoss presented
  a $(1+\epsilon)$-approximation with a running time of $n^{(\log n)^{O((m^{2}/\epsilon^{2})\log\log n)}}$[STOC
  2016, SICOMP 2019] and this running time was improved to quasi-polynomial
  by Garg[ICALP 2018] and to even $n^{O_{m,\epsilon}(\log^{3}\log n)}$
  by Li[SODA 2021]. These results use techniques like LP-hierarchies,
  conditioning on certain well-selected jobs, and abstractions like
  (partial) dyadic systems and virtually valid schedules.
  In this paper, we present a QPTAS for the problem which is arguably
  simpler than the previous algorithms. We just guess the positions
  of certain jobs in the optimal solution, recurse on a set of guessed
  subintervals, and fill in the remaining jobs with greedy routines.
  We believe that also our analysis is more accessible, in particular since we do not
  use (LP-)hierarchies or abstractions of the problem like the ones above, but we guess properties
  of the optimal solution directly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09091v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Syamantak Das, Andreas Wiese</dc:creator>
    </item>
    <item>
      <title>Scheduling Coflows for Minimizing the Maximum Completion Time in Heterogeneous Parallel Networks</title>
      <link>https://arxiv.org/abs/2501.09293</link>
      <description>arXiv:2501.09293v1 Announce Type: new 
Abstract: Coflow represents a network abstraction that models communication patterns within data centers. The scheduling of coflows is a prevalent issue in large data center environments and is classified as an $\mathcal{NP}$-hard problem. This paper focuses on the scheduling of coflows in heterogeneous parallel networks, defined by architectures featuring multiple network cores operating concurrently. We introduce two pseudo-polynomial-time algorithms and two polynomial-time approximation algorithms to minimize the maximum completion time (makespan) in heterogeneous parallel networks. We propose a randomized algorithm that offers an expected approximation ratio of 1.5. Building upon this foundation, we provide a deterministic algorithm that utilizes derandomization techniques, which offers a performance guarantee of $1.5 + \frac{1}{2 \cdot LB}$, where $LB$ is the lower bound of the makespan for each instance. To address time complexity concerns, we implement an exponential partitioning of time intervals and present a randomized algorithm with an expected approximation ratio of $1.5 + \epsilon$ in polynomial time where $\epsilon&gt;0$. Additionally, we develop a deterministic algorithm with a performance guarantee expressed as $\max\left\{1.5+\epsilon, 1.5+\frac{1}{2 \cdot LB}\right\}$ within polynomial time. These advancements markedly enhance the best-known approximation ratio of $2+\epsilon$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09293v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chi-Yeh Chen</dc:creator>
    </item>
    <item>
      <title>Testing Noise Assumptions of Learning Algorithms</title>
      <link>https://arxiv.org/abs/2501.09189</link>
      <description>arXiv:2501.09189v1 Announce Type: cross 
Abstract: We pose a fundamental question in computational learning theory: can we efficiently test whether a training set satisfies the assumptions of a given noise model? This question has remained unaddressed despite decades of research on learning in the presence of noise. In this work, we show that this task is tractable and present the first efficient algorithm to test various noise assumptions on the training data.
  To model this question, we extend the recently proposed testable learning framework of Rubinfeld and Vasilyan (2023) and require a learner to run an associated test that satisfies the following two conditions: (1) whenever the test accepts, the learner outputs a classifier along with a certificate of optimality, and (2) the test must pass for any dataset drawn according to a specified modeling assumption on both the marginal distribution and the noise model. We then consider the problem of learning halfspaces over Gaussian marginals with Massart noise (where each label can be flipped with probability less than $1/2$ depending on the input features), and give a fully-polynomial time testable learning algorithm.
  We also show a separation between the classical setting of learning in the presence of structured noise and testable learning. In fact, for the simple case of random classification noise (where each label is flipped with fixed probability $\eta = 1/2$), we show that testable learning requires super-polynomial time while classical learning is trivial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09189v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Surbhi Goel, Adam R. Klivans, Konstantinos Stavropoulos, Arsen Vasilyan</dc:creator>
    </item>
    <item>
      <title>A Near-optimal Algorithm for Learning Margin Halfspaces with Massart Noise</title>
      <link>https://arxiv.org/abs/2501.09691</link>
      <description>arXiv:2501.09691v1 Announce Type: cross 
Abstract: We study the problem of PAC learning $\gamma$-margin halfspaces in the presence of Massart noise. Without computational considerations, the sample complexity of this learning problem is known to be $\widetilde{\Theta}(1/(\gamma^2 \epsilon))$. Prior computationally efficient algorithms for the problem incur sample complexity $\tilde{O}(1/(\gamma^4 \epsilon^3))$ and achieve 0-1 error of $\eta+\epsilon$, where $\eta&lt;1/2$ is the upper bound on the noise rate. Recent work gave evidence of an information-computation tradeoff, suggesting that a quadratic dependence on $1/\epsilon$ is required for computationally efficient algorithms. Our main result is a computationally efficient learner with sample complexity $\widetilde{\Theta}(1/(\gamma^2 \epsilon^2))$, nearly matching this lower bound. In addition, our algorithm is simple and practical, relying on online SGD on a carefully selected sequence of convex losses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09691v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilias Diakonikolas, Nikos Zarifis</dc:creator>
    </item>
    <item>
      <title>Efficient PAC Learning of Halfspaces with Constant Malicious Noise Rate</title>
      <link>https://arxiv.org/abs/2410.01186</link>
      <description>arXiv:2410.01186v3 Announce Type: replace-cross 
Abstract: Understanding noise tolerance of machine learning algorithms is a central quest in learning theory. In this work, we study the problem of computationally efficient PAC learning of halfspaces in the presence of malicious noise, where an adversary can corrupt both instances and labels of training samples. The best-known noise tolerance either depends on a target error rate under distributional assumptions or on a margin parameter under large-margin conditions. In this work, we show that when both types of conditions are satisfied, it is possible to achieve constant noise tolerance by minimizing a reweighted hinge loss. Our key ingredients include: 1) an efficient algorithm that finds weights to control the gradient deterioration from corrupted samples, and 2) a new analysis on the robustness of the hinge loss equipped with such weights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01186v3</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jie Shen</dc:creator>
    </item>
  </channel>
</rss>
