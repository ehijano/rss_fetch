<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 08 May 2024 04:00:41 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 08 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>An Autoregressive Model for Time Series of Random Objects</title>
      <link>https://arxiv.org/abs/2405.03778</link>
      <description>arXiv:2405.03778v1 Announce Type: new 
Abstract: Random variables in metric spaces indexed by time and observed at equally spaced time points are receiving increased attention due to their broad applicability. However, the absence of inherent structure in metric spaces has resulted in a literature that is predominantly non-parametric and model-free. To address this gap in models for time series of random objects, we introduce an adaptation of the classical linear autoregressive model tailored for data lying in a Hadamard space. The parameters of interest in this model are the Fr\'echet mean and a concentration parameter, both of which we prove can be consistently estimated from data. Additionally, we propose a test statistic and establish its asymptotic normality, thereby enabling hypothesis testing for the absence of serial dependence. Finally, we introduce a bootstrap procedure to obtain critical values for the test statistic under the null hypothesis. Theoretical results of our method, including the convergence of the estimators as well as the size and power of the test, are illustrated through simulations, and the utility of the model is demonstrated by an analysis of a time series of consumer inflation expectations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03778v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthieu Bult\'e, Helle S{\o}rensen</dc:creator>
    </item>
    <item>
      <title>Statistical inference for a stochastic generalized logistic differential equation</title>
      <link>https://arxiv.org/abs/2405.03815</link>
      <description>arXiv:2405.03815v1 Announce Type: new 
Abstract: This research aims to estimate three parameters in a stochastic generalized logistic differential equation. We assume the intrinsic growth rate and shape parameters are constant but unknown. To estimate these two parameters, we use the maximum likelihood method and establish that the estimators for these two parameters are strongly consistent. We estimate the diffusion parameter by using the quadratic variation processes. To test our results, we evaluate two data scenarios, complete and incomplete, with fixed values assigned to the three parameters. In the incomplete data scenario, we apply an Expectation Maximization algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03815v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fernando Baltazar-Larios, Francisco Delgado-Vences, Saul Diaz-Infante, Eduardo Lince Gomez</dc:creator>
    </item>
    <item>
      <title>Covariance-free Multifidelity Control Variates Importance Sampling for Reliability Analysis of Rare Events</title>
      <link>https://arxiv.org/abs/2405.03834</link>
      <description>arXiv:2405.03834v1 Announce Type: new 
Abstract: Multifidelity modeling has been steadily gaining attention as a tool to address the problem of exorbitant model evaluation costs that makes the estimation of failure probabilities a significant computational challenge for complex real-world problems, particularly when failure is a rare event. To implement multifidelity modeling, estimators that efficiently combine information from multiple models/sources are necessary. In past works, the variance reduction techniques of Control Variates (CV) and Importance Sampling (IS) have been leveraged for this task. In this paper, we present the CVIS framework; a creative take on a coupled Control Variates and Importance Sampling estimator for bifidelity reliability analysis. The framework addresses some of the practical challenges of the CV method by using an estimator for the control variate mean and side-stepping the need to estimate the covariance between the original estimator and the control variate through a clever choice for the tuning constant. The task of selecting an efficient IS distribution is also considered, with a view towards maximally leveraging the bifidelity structure and maintaining expressivity. Additionally, a diagnostic is provided that indicates both the efficiency of the algorithm as well as the relative predictive quality of the models utilized. Finally, the behavior and performance of the framework is explored through analytical and numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03834v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Promit Chakroborty (Dept. of Civil,Systems Engg, Johns Hopkins University), Somayajulu L. N. Dhulipala (Idaho National Laboratory), Michael D. Shields (Dept. of Civil,Systems Engg, Johns Hopkins University)</dc:creator>
    </item>
    <item>
      <title>Bayesian Multilevel Compositional Data Analysis: Introduction, Evaluation, and Application</title>
      <link>https://arxiv.org/abs/2405.03985</link>
      <description>arXiv:2405.03985v1 Announce Type: new 
Abstract: Multilevel compositional data commonly occur in various fields, particularly in intensive, longitudinal studies using ecological momentary assessments. Examples include data repeatedly measured over time that are non-negative and sum to a constant value, such as sleep-wake movement behaviours in a 24-hour day. This article presents a novel methodology for analysing multilevel compositional data using a Bayesian inference approach. This method can be used to investigate how reallocation of time between sleep-wake movement behaviours may be associated with other phenomena (e.g., emotions, cognitions) at a daily level. We explain the theoretical details of the data and the models, and outline the steps necessary to implement this method. We introduce the R package multilevelcoda to facilitate the application of this method and illustrate using a real data example. An extensive parameter recovery simulation study verified the robust performance of the method. Across all simulation conditions investigated in the simulation study, the model had minimal convergence issues (convergence rate &gt; 99%) and achieved excellent quality of parameter estimates and inference, with an average bias of 0.00 (range -0.09, 0.05) and coverage of 0.95 (range 0.93, 0.97). We conclude the article with recommendations on the use of the Bayesian compositional multilevel modelling approach, and hope to promote wider application of this method to answer robust questions using the increasingly available data from intensive, longitudinal studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03985v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Flora Le, Tyman E. Stanford, Dorothea Dumuid, Joshua F. Wiley</dc:creator>
    </item>
    <item>
      <title>A generalized ordinal quasi-symmetry model and its separability for analyzing multi-way tables</title>
      <link>https://arxiv.org/abs/2405.04193</link>
      <description>arXiv:2405.04193v1 Announce Type: new 
Abstract: This paper addresses the challenge of modeling multi-way contingency tables for matched set data with ordinal categories. Although the complete symmetry and marginal homogeneity models are well established, they may not always provide a satisfactory fit to the data. To address this issue, we propose a generalized ordinal quasi-symmetry model that offers increased flexibility when the complete symmetry model fails to capture the underlying structure. We investigate the properties of this new model and provide an information-theoretic interpretation, elucidating its relationship to the ordinal quasi-symmetry model. Moreover, we revisit Agresti's findings and present a new necessary and sufficient condition for the complete symmetry model, proving that the proposed model and the marginal moment equality model are separable hypotheses. The separability of the proposed model and marginal moment equality model is a significant development in the analysis of multi-way contingency tables. It enables researchers to examine the symmetry structure in the data with greater precision, providing a more thorough understanding of the underlying patterns. This powerful framework equips researchers with the necessary tools to explore the complexities of ordinal variable relationships in matched set data, paving the way for new discoveries and insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04193v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hisaya Okahara, Kouji Tahata</dc:creator>
    </item>
    <item>
      <title>NEST: Neural Estimation by Sequential Testing</title>
      <link>https://arxiv.org/abs/2405.04226</link>
      <description>arXiv:2405.04226v1 Announce Type: new 
Abstract: Adaptive psychophysical procedures aim to increase the efficiency and reliability of measurements. With increasing stimulus and experiment complexity in the last decade, estimating multi-dimensional psychometric functions has become a challenging task for adaptive procedures. If the experimenter has limited information about the underlying psychometric function, it is not possible to use parametric techniques developed for the multi-dimensional stimulus space. Although there are non-parametric approaches that use Gaussian process methods and specific hand-crafted acquisition functions, their performance is sensitive to proper selection of the kernel function, which is not always straightforward. In this work, we use a neural network as the psychometric function estimator and introduce a novel acquisition function for stimulus selection. We thoroughly benchmark our technique both using simulations and by conducting psychovisual experiments under realistic conditions. We show that our method outperforms the state of the art without the need to select a kernel function and significantly reduces the experiment duration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04226v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sjoerd Bruin, Ji\v{r}\'i Kosinka, Cara Tursun</dc:creator>
    </item>
    <item>
      <title>Homogeneity of multinomial populations when data are classified into a large number of groups</title>
      <link>https://arxiv.org/abs/2405.04238</link>
      <description>arXiv:2405.04238v1 Announce Type: new 
Abstract: Suppose that we are interested in the comparison of two independent categorical variables. Suppose also that the population is divided into subpopulations or groups. Notice that the distribution of the target variable may vary across subpopulations, moreover, it may happen that the two independent variables have the same distribution in the whole population, but their distributions could differ in some groups. So, instead of testing the homogeneity of the two categorical variables, one may be interested in simultaneously testing the homogeneity in all groups. A novel procedure is proposed for carrying out such a testing problem. The test statistic is shown to be asymptotically normal, avoiding the use of complicated resampling methods to get $p$-values. Here by asymptotic we mean when the number of groups increases; the sample sizes of the data from each group can either stay bounded or grow with the number of groups. The finite sample performance of the proposal is empirically evaluated through an extensive simulation study. The usefulness of the proposal is illustrated by three data sets coming from diverse experimental fields such as education, the COVID-19 pandemic and digital elevation models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04238v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>M. V. Alba-Fern\'andez, M. D. Jim\'enez--Gamero, F. J. Ariza-L\'opez</dc:creator>
    </item>
    <item>
      <title>Distributed variable screening for generalized linear models</title>
      <link>https://arxiv.org/abs/2405.04254</link>
      <description>arXiv:2405.04254v1 Announce Type: new 
Abstract: In this article, we develop a distributed variable screening method for generalized linear models. This method is designed to handle situations where both the sample size and the number of covariates are large. Specifically, the proposed method selects relevant covariates by using a sparsity-restricted surrogate likelihood estimator. It takes into account the joint effects of the covariates rather than just the marginal effect, and this characteristic enhances the reliability of the screening results. We establish the sure screening property of the proposed method, which ensures that with a high probability, the true model is included in the selected model. Simulation studies are conducted to evaluate the finite sample performance of the proposed method, and an application to a real dataset showcases its practical utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04254v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianbo Diao, Lianqiang Qu, Bo Li, Liuquan Sun</dc:creator>
    </item>
    <item>
      <title>Transportability of Principal Causal Effects</title>
      <link>https://arxiv.org/abs/2405.04419</link>
      <description>arXiv:2405.04419v1 Announce Type: new 
Abstract: Recent research in causal inference has made important progress in addressing challenges to the external validity of trial findings. Such methods weight trial participant data to more closely resemble the distribution of effect-modifying covariates in a well-defined target population. In the presence of participant non-adherence to study medication, these methods effectively transport an intention-to-treat effect that averages over heterogeneous compliance behaviors. In this paper, we develop a principal stratification framework to identify causal effects conditioning on both on compliance behavior and membership in the target population. We also develop non-parametric efficiency theory for and construct efficient estimators of such "transported" principal causal effects and characterize their finite-sample performance in simulation experiments. While this work focuses on treatment non-adherence, the framework is applicable to a broad class of estimands that target effects in clinically-relevant, possibly latent subsets of a target population.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04419v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Justin M. Clark, Kollin W. Rott, James S. Hodges, Jared D. Huling</dc:creator>
    </item>
    <item>
      <title>Causal Inference in the Multiverse of Hazard</title>
      <link>https://arxiv.org/abs/2405.04446</link>
      <description>arXiv:2405.04446v1 Announce Type: new 
Abstract: Hazard serves as a pivotal estimand in both practical applications and methodological frameworks. However, its causal interpretation poses notable challenges, including inherent selection biases and ill-defined populations to be compared between different treatment groups. In response, we propose a novel definition of counterfactual hazard within the framework of possible worlds. Instead of conditioning on prior survival status as a conditional probability, our new definition involves intervening in the prior status, treating it as a marginal probability. Using single-world intervention graphs, we demonstrate that the proposed counterfactual hazard is a type of controlled direct effect. Conceptually, intervening in survival status at each time point generates a new possible world, where the proposed hazards across time points represent risks in these hypothetical scenarios, forming a "multiverse of hazard." The cumulative and average counterfactual hazards correspond to the sum and average of risks across this multiverse, respectively, with the actual world's risk lying between the two. This conceptual shift reframes hazards in the actual world as a collection of risks across possible worlds, marking a significant advancement in the causal interpretation of hazards.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04446v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>En-Yu Lai, Yen-Tsung Huang</dc:creator>
    </item>
    <item>
      <title>Bayesian Copula Density Estimation Using Bernstein Yett-Uniform Priors</title>
      <link>https://arxiv.org/abs/2405.04475</link>
      <description>arXiv:2405.04475v1 Announce Type: new 
Abstract: Probability density estimation is a central task in statistics. Copula-based models provide a great deal of flexibility in modelling multivariate distributions, allowing for the specifications of models for the marginal distributions separately from the dependence structure (copula) that links them to form a joint distribution. Choosing a class of copula models is not a trivial task and its misspecification can lead to wrong conclusions. We introduce a novel class of random Bernstein copula functions, and studied its support and the behavior of its posterior distribution. The proposal is based on a particular class of random grid-uniform copulas, referred to as yett-uniform copulas. Alternative Markov chain Monte Carlo algorithms for exploring the posterior distribution under the proposed model are also studied. The methodology is illustrated by means of simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04475v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nicol\'as Kuschinski, Richard Warr, Alejandro Jara</dc:creator>
    </item>
    <item>
      <title>Stochastic Gradient MCMC for Massive Geostatistical Data</title>
      <link>https://arxiv.org/abs/2405.04531</link>
      <description>arXiv:2405.04531v1 Announce Type: new 
Abstract: Gaussian processes (GPs) are commonly used for prediction and inference for spatial data analyses. However, since estimation and prediction tasks have cubic time and quadratic memory complexity in number of locations, GPs are difficult to scale to large spatial datasets. The Vecchia approximation induces sparsity in the dependence structure and is one of several methods proposed to scale GP inference. Our work adds to the substantial research in this area by developing a stochastic gradient Markov chain Monte Carlo (SGMCMC) framework for efficient computation in GPs. At each step, the algorithm subsamples a minibatch of locations and subsequently updates process parameters through a Vecchia-approximated GP likelihood. Since the Vecchia-approximated GP has a time complexity that is linear in the number of locations, this results in scalable estimation in GPs. Through simulation studies, we demonstrate that SGMCMC is competitive with state-of-the-art scalable GP algorithms in terms of computational time and parameter estimation. An application of our method is also provided using the Argo dataset of ocean temperature measurements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04531v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohamed A. Abba, Brian J. Reich, Reetam Majumder, Brandon Feng</dc:creator>
    </item>
    <item>
      <title>Spatial Transfer Learning with Simple MLP</title>
      <link>https://arxiv.org/abs/2405.03720</link>
      <description>arXiv:2405.03720v1 Announce Type: cross 
Abstract: First step to investigate the potential of transfer learning applied to the field of spatial statistics</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03720v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hongjian Yang</dc:creator>
    </item>
    <item>
      <title>Generative adversarial learning with optimal input dimension and its adaptive generator architecture</title>
      <link>https://arxiv.org/abs/2405.03723</link>
      <description>arXiv:2405.03723v1 Announce Type: cross 
Abstract: We investigate the impact of the input dimension on the generalization error in generative adversarial networks (GANs). In particular, we first provide both theoretical and practical evidence to validate the existence of an optimal input dimension (OID) that minimizes the generalization error. Then, to identify the OID, we introduce a novel framework called generalized GANs (G-GANs), which includes existing GANs as a special case. By incorporating the group penalty and the architecture penalty developed in the paper, G-GANs have several intriguing features. First, our framework offers adaptive dimensionality reduction from the initial dimension to a dimension necessary for generating the target distribution. Second, this reduction in dimensionality also shrinks the required size of the generator network architecture, which is automatically identified by the proposed architecture penalty. Both reductions in dimensionality and the generator network significantly improve the stability and the accuracy of the estimation and prediction. Theoretical support for the consistent selection of the input dimension and the generator network is provided. Third, the proposed algorithm involves an end-to-end training process, and the algorithm allows for dynamic adjustments between the input dimension and the generator network during training, further enhancing the overall performance of G-GANs. Extensive experiments conducted with simulated and benchmark data demonstrate the superior performance of G-GANs. In particular, compared to that of off-the-shelf methods, G-GANs achieves an average improvement of 45.68% in the CT slice dataset, 43.22% in the MNIST dataset and 46.94% in the FashionMNIST dataset in terms of the maximum mean discrepancy or Frechet inception distance. Moreover, the features generated based on the input dimensions identified by G-GANs align with visually significant features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03723v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiyao Tan, Ling Zhou, Huazhen Lin</dc:creator>
    </item>
    <item>
      <title>A Primer on the Analysis of Randomized Experiments and a Survey of some Recent Advances</title>
      <link>https://arxiv.org/abs/2405.03910</link>
      <description>arXiv:2405.03910v1 Announce Type: cross 
Abstract: The past two decades have witnessed a surge of new research in the analysis of randomized experiments. The emergence of this literature may seem surprising given the widespread use and long history of experiments as the "gold standard" in program evaluation, but this body of work has revealed many subtle aspects of randomized experiments that may have been previously unappreciated. This article provides an overview of some of these topics, primarily focused on stratification, regression adjustment, and cluster randomization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03910v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuehao Bai, Azeem M. Shaikh, Max Tabord-Meehan</dc:creator>
    </item>
    <item>
      <title>Scalable Vertical Federated Learning via Data Augmentation and Amortized Inference</title>
      <link>https://arxiv.org/abs/2405.04043</link>
      <description>arXiv:2405.04043v1 Announce Type: cross 
Abstract: Vertical federated learning (VFL) has emerged as a paradigm for collaborative model estimation across multiple clients, each holding a distinct set of covariates. This paper introduces the first comprehensive framework for fitting Bayesian models in the VFL setting. We propose a novel approach that leverages data augmentation techniques to transform VFL problems into a form compatible with existing Bayesian federated learning algorithms. We present an innovative model formulation for specific VFL scenarios where the joint likelihood factorizes into a product of client-specific likelihoods. To mitigate the dimensionality challenge posed by data augmentation, which scales with the number of observations and clients, we develop a factorized amortized variational approximation that achieves scalability independent of the number of observations. We showcase the efficacy of our framework through extensive numerical experiments on logistic regression, multilevel regression, and a novel hierarchical Bayesian split neural net model. Our work paves the way for privacy-preserving, decentralized Bayesian inference in vertically partitioned data scenarios, opening up new avenues for research and applications in various domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04043v1</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Conor Hassan, Matthew Sutton, Antonietta Mira, Kerrie Mengersen</dc:creator>
    </item>
    <item>
      <title>Anisotropic local constant smoothing for change-point regression function estimation</title>
      <link>https://arxiv.org/abs/2012.00180</link>
      <description>arXiv:2012.00180v2 Announce Type: replace 
Abstract: Understanding forest fire spread in any region of Canada is critical to promoting forest health, and protecting human life and infrastructure. Quantifying fire spread from noisy images, where regions of a fire are separated by change-point boundaries, is critical to faithfully estimating fire spread rates. In this research, we develop a statistically consistent smooth estimator that allows us to denoise fire spread imagery from micro-fire experiments. We develop an anisotropic smoothing method for change-point data that uses estimates of the underlying data generating process to inform smoothing. We show that the anisotropic local constant regression estimator is consistent with convergence rate $O\left(n^{-1/{(q+2)}}\right)$. We demonstrate its effectiveness on simulated one- and two-dimensional change-point data and fire spread imagery from micro-fire experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2012.00180v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>John R. J. Thompson, W. John Braun</dc:creator>
    </item>
    <item>
      <title>ReBoot: Distributed statistical learning via refitting bootstrap samples</title>
      <link>https://arxiv.org/abs/2207.09098</link>
      <description>arXiv:2207.09098v3 Announce Type: replace 
Abstract: In this paper, we propose a one-shot distributed learning algorithm via refitting bootstrap samples, which we refer to as ReBoot. ReBoot refits a new model to mini-batches of bootstrap samples that are continuously drawn from each of the locally fitted models. It requires only one round of communication of model parameters without much memory. Theoretically, we analyze the statistical error rate of ReBoot for generalized linear models (GLM) and noisy phase retrieval, which represent convex and non-convex problems, respectively. In both cases, ReBoot provably achieves the full-sample statistical rate. In particular, we show that the systematic bias of ReBoot, the error that is independent of the number of subsamples (i.e., the number of sites), is $O(n ^ {-2})$ in GLM, where $n$ is the subsample size (the sample size of each local site). This rate is sharper than that of model parameter averaging and its variants, implying the higher tolerance of ReBoot with respect to data splits to maintain the full-sample rate. Our simulation study demonstrates the statistical advantage of ReBoot over competing methods. Finally, we propose FedReBoot, an iterative version of ReBoot, to aggregate convolutional neural networks for image classification. FedReBoot exhibits substantial superiority over Federated Averaging (FedAvg) within early rounds of communication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.09098v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yumeng Wang, Ziwei Zhu, Xuming He</dc:creator>
    </item>
    <item>
      <title>The transcoding sampler for stick-breaking inferences on Dirichlet process mixtures</title>
      <link>https://arxiv.org/abs/2304.02563</link>
      <description>arXiv:2304.02563v2 Announce Type: replace 
Abstract: Dirichlet process mixture models suffer from slow mixing of the MCMC posterior chain produced by stick-breaking Gibbs samplers, as opposed to collapsed Gibbs samplers based on the Polya urn representation which have shorter integrated autocorrelation time (IAT).
  We study how cluster membership information is encoded under the two aforementioned samplers, and we introduce the transcoding algorithm to switch between encodings. We also develop the transcoding sampler, which consists of undertaking posterior partition inference with any high-efficiency sampler, such as collapsed Gibbs, and to subsequently transcode it to the stick-breaking representation via the transcoding algorithm, thereby allowing inference on all stick-breaking parameters of interest while retaining the shorter IAT of the high-efficiency sampler.
  The transcoding sampler is substantially simpler to implement than the slice sampler, it can inherit the shorter IAT of collapsed Gibbs samplers and it can also achieve zero IAT when paired with a posterior partition sampler that is i.i.d., such as the sequential importance sampler.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.02563v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carlo Vicentini</dc:creator>
    </item>
    <item>
      <title>Bayesian Quantile Regression with Subset Selection: A Posterior Summarization Perspective</title>
      <link>https://arxiv.org/abs/2311.02043</link>
      <description>arXiv:2311.02043v2 Announce Type: replace 
Abstract: Quantile regression is a powerful tool for inferring how covariates affect specific percentiles of the response distribution. Existing methods either estimate conditional quantiles separately for each quantile of interest or estimate the entire conditional distribution using semi- or non-parametric models. The former often produce inadequate models for real data and do not share information across quantiles, while the latter are characterized by complex and constrained models that can be difficult to interpret and computationally inefficient. Further, neither approach is well-suited for quantile-specific subset selection. Instead, we pose the fundamental problems of linear quantile estimation, uncertainty quantification, and subset selection from a Bayesian decision analysis perspective. For any Bayesian regression model, we derive optimal and interpretable linear estimates and uncertainty quantification for each model-based conditional quantile. Our approach introduces a quantile-focused squared error loss, which enables efficient, closed-form computing and maintains a close relationship with Wasserstein-based density estimation. In an extensive simulation study, our methods demonstrate substantial gains in quantile estimation accuracy, variable selection, and inference over frequentist and Bayesian competitors. We apply these tools to identify the quantile-specific impacts of social and environmental stressors on educational outcomes for a large cohort of children in North Carolina.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.02043v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joseph Feldman, Daniel Kowal</dc:creator>
    </item>
    <item>
      <title>Recursive identification with regularization and on-line hyperparameters estimation</title>
      <link>https://arxiv.org/abs/2401.00097</link>
      <description>arXiv:2401.00097v2 Announce Type: replace 
Abstract: This paper presents a regularized recursive identification algorithm with simultaneous on-line estimation of both the model parameters and the algorithms hyperparameters. A new kernel is proposed to facilitate the algorithm development. The performance of this novel scheme is compared with that of the recursive least squares algorithm in simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.00097v2</guid>
      <category>stat.ME</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bernard Vau, Tudor-Bogdan Airimitoaie</dc:creator>
    </item>
    <item>
      <title>Random Effect Restricted Mean Survival Time Model</title>
      <link>https://arxiv.org/abs/2401.02048</link>
      <description>arXiv:2401.02048v2 Announce Type: replace 
Abstract: The restricted mean survival time (RMST) model has been garnering attention as a way to provide a clinically intuitive measure: the mean survival time. RMST models, which use methods based on pseudo time-to-event values and inverse probability censoring weighting, can adjust covariates. However, no approach has yet been introduced that considers random effects for clusters. In this paper, we propose a new random-effect RMST. We present two methods of analysis that consider variable effects by i) using a generalized mixed model with pseudo-values and ii) integrating the estimated results from the inverse probability censoring weighting estimating equations for each cluster. We evaluate our proposed methods through computer simulations. In addition, we analyze the effect of a mother's age at birth on under-five deaths in India using states as clusters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.02048v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keisuke Hanada, Masahiro Kojima</dc:creator>
    </item>
    <item>
      <title>Reasoning with fuzzy and uncertain evidence using epistemic random fuzzy sets: general framework and practical models</title>
      <link>https://arxiv.org/abs/2202.08081</link>
      <description>arXiv:2202.08081v4 Announce Type: replace-cross 
Abstract: We introduce a general theory of epistemic random fuzzy sets for reasoning with fuzzy or crisp evidence. This framework generalizes both the Dempster-Shafer theory of belief functions, and possibility theory. Independent epistemic random fuzzy sets are combined by the generalized product-intersection rule, which extends both Dempster's rule for combining belief functions, and the product conjunctive combination of possibility distributions. We introduce Gaussian random fuzzy numbers and their multi-dimensional extensions, Gaussian random fuzzy vectors, as practical models for quantifying uncertainty about scalar or vector quantities. Closed-form expressions for the combination, projection and vacuous extension of Gaussian random fuzzy numbers and vectors are derived.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.08081v4</guid>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.fss.2022.06.004</arxiv:DOI>
      <arxiv:journal_reference>Fuzzy Sets and Systems, Vol. 453, Pages 1-36, 2023</arxiv:journal_reference>
      <dc:creator>Thierry Denoeux</dc:creator>
    </item>
    <item>
      <title>Optimal empirical Bayes estimation for the Poisson model via minimum-distance methods</title>
      <link>https://arxiv.org/abs/2209.01328</link>
      <description>arXiv:2209.01328v2 Announce Type: replace-cross 
Abstract: The Robbins estimator is the most iconic and widely used procedure in the empirical Bayes literature for the Poisson model. On one hand, this method has been recently shown to be minimax optimal in terms of the regret (excess risk over the Bayesian oracle that knows the true prior) for various nonparametric classes of priors. On the other hand, it has been long recognized in practice that Robbins estimator lacks the desired smoothness and monotonicity of Bayes estimators and can be easily derailed by those data points that were rarely observed before. Based on the minimum-distance distance method, we propose a suite of empirical Bayes estimators, including the classical nonparametric maximum likelihood, that outperform the Robbins method in a variety of synthetic and real data sets and retain its optimality in terms of minimax regret.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.01328v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soham Jana, Yury Polyanskiy, Yihong Wu</dc:creator>
    </item>
    <item>
      <title>The Projected Covariance Measure for assumption-lean variable significance testing</title>
      <link>https://arxiv.org/abs/2211.02039</link>
      <description>arXiv:2211.02039v4 Announce Type: replace-cross 
Abstract: Testing the significance of a variable or group of variables $X$ for predicting a response $Y$, given additional covariates $Z$, is a ubiquitous task in statistics. A simple but common approach is to specify a linear model, and then test whether the regression coefficient for $X$ is non-zero. However, when the model is misspecified, the test may have poor power, for example when $X$ is involved in complex interactions, or lead to many false rejections. In this work we study the problem of testing the model-free null of conditional mean independence, i.e. that the conditional mean of $Y$ given $X$ and $Z$ does not depend on $X$. We propose a simple and general framework that can leverage flexible nonparametric or machine learning methods, such as additive models or random forests, to yield both robust error control and high power. The procedure involves using these methods to perform regressions, first to estimate a form of projection of $Y$ on $X$ and $Z$ using one half of the data, and then to estimate the expected conditional covariance between this projection and $Y$ on the remaining half of the data. While the approach is general, we show that a version of our procedure using spline regression achieves what we show is the minimax optimal rate in this nonparametric testing problem. Numerical experiments demonstrate the effectiveness of our approach both in terms of maintaining Type I error control, and power, compared to several existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.02039v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anton Rask Lundborg, Ilmun Kim, Rajen D. Shah, Richard J. Samworth</dc:creator>
    </item>
    <item>
      <title>A group testing based exploration of age-varying factors in chlamydia infections among Iowa residents</title>
      <link>https://arxiv.org/abs/2404.01469</link>
      <description>arXiv:2404.01469v2 Announce Type: replace-cross 
Abstract: Group testing, a method that screens subjects in pooled samples rather than individually, has been employed as a cost-effective strategy for chlamydia screening among Iowa residents. In efforts to deepen our understanding of chlamydia epidemiology in Iowa, several group testing regression models have been proposed. Different than previous approaches, we expand upon the varying coefficient model to capture potential age-varying associations with chlamydia infection risk. In general, our model operates within a Bayesian framework, allowing regression associations to vary with a covariate of key interest. We employ a stochastic search variable selection process for regularization in estimation. Additionally, our model can integrate random effects to consider potential geographical factors and estimate unknown assay accuracy probabilities. The performance of our model is assessed through comprehensive simulation studies. Upon application to the Iowa group testing dataset, we reveal a significant age-varying racial disparity in chlamydia infections. We believe this discovery has the potential to inform the enhancement of interventions and prevention strategies, leading to more effective chlamydia control and management, thereby promoting health equity across all populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01469v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yizeng Li, Dewei Wang, Joshua M. Tebbs</dc:creator>
    </item>
  </channel>
</rss>
