<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 11 Mar 2024 04:00:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 11 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Modified wavelet variation for the Hermite processes</title>
      <link>https://arxiv.org/abs/2403.05140</link>
      <description>arXiv:2403.05140v1 Announce Type: new 
Abstract: We define an asymptotically normal wavelet-based strongly consistent estimator for the Hurst parameter of any Hermite processes. This estimator is obtained by considering a modified wavelet variation in which coefficients are wisely chosen to be, up to negligeable remainders, independent. We use Stein-Malliavin calculus to prove that this wavelet variation satisfies a multidimensional Central Limit Theorem, with an explicit bound for the Wasserstein distance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05140v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Laurent Loosveldt, Ciprian A. Tudor</dc:creator>
    </item>
    <item>
      <title>Errors due to departure from independence in multivariate Weibull distributions</title>
      <link>https://arxiv.org/abs/2403.05251</link>
      <description>arXiv:2403.05251v1 Announce Type: new 
Abstract: We do the error analysis in reliability measures due to the assumption of independence amongst the component lifetimes. In reliability theory, we come across different n-component structures like series, parallel, and k-out-of-n systems. A n component series system works only if all the n components work. While studying the reliability measures of a n-component series system, we mostly assume that all the components have independent lifetimes. Such an assumption eases mathematical complexity while analyzing the data and hence is very common. But in reality, the lifetimes of the components are very much interdependent. Such an assumption of independence hence leads to inaccurate analysis of data. In multiple situations like studying a complex system with many components, we turn to assuming independence keeping some room for error. However, if we have some knowledge of the behaviour of errors or some estimate on the error bound, we could decide if we assume independence and prefer mathematical simplicity (if we know the error is within our allowed limit), or keep the mathematical complexity and get accurate results without assuming independence. We aim to find the relative errors in the reliability measures for a n-component series system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05251v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subarna Bhattacharjee, Aninda K. Nanda, Subhasree Patra</dc:creator>
    </item>
    <item>
      <title>Revised BDS Test</title>
      <link>https://arxiv.org/abs/2403.05411</link>
      <description>arXiv:2403.05411v1 Announce Type: new 
Abstract: In this paper, we focus on the BDS test, which is a nonparametric test of independence. Specifically, the null hypothesis $H_{0}$ of it is that $\{u_{t}\}$ is i.i.d. (independent and identically distributed), where $\{u_{t}\}$ is a random sequence. The BDS test is widely used in economics and finance, but it has a weakness that cannot be ignored: over-rejecting $H_{0}$ even if the length $T$ of $\{u_{t}\}$ is as large as $(100,2000)$. To improve the over-rejection problem of BDS test, considering that the correlation integral is the foundation of BDS test, we not only accurately describe the expectation of the correlation integral under $H_{0}$, but also calculate all terms of the asymptotic variance of the correlation integral whose order is $O(T^{-1})$ and $O(T^{-2})$, which is essential to improve the finite sample performance of BDS test. Based on this, we propose a revised BDS (RBDS) test and prove its asymptotic normality under $H_{0}$. The RBDS test not only inherits all the advantages of the BDS test, but also effectively corrects the over-rejection problem of the BDS test, which can be fully confirmed by the simulation results we presented. Moreover, based on the simulation results, we find that similar to BDS test, RBDS test would also be affected by the parameter estimations of the ARCH-type model, resulting in size distortion, but this phenomenon can be alleviated by the logarithmic transformation preprocessing of the estimate residuals of the model. Besides, through some actual datasets that have been demonstrated to fit well with ARCH-type models, we also compared the performance of BDS test and RBDS test in evaluating the goodness-of-fit of the model in empirical problem, and the results reflect that, under the same condition, the performance of the RBDS test is more encouraging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05411v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenya Luo, Zhidong Bai, Jiang Hu, Chen Wang</dc:creator>
    </item>
    <item>
      <title>On varimax asymptotics in network models and spectral methods for dimensionality reduction</title>
      <link>https://arxiv.org/abs/2403.05461</link>
      <description>arXiv:2403.05461v1 Announce Type: new 
Abstract: Varimax factor rotations, while popular among practitioners in psychology and statistics since being introduced by H. Kaiser, have historically been viewed with skepticism and suspicion by some theoreticians and mathematical statisticians. Now, work by K. Rohe and M. Zeng provides new, fundamental insight: varimax rotations provably perform statistical estimation in certain classes of latent variable models when paired with spectral-based matrix truncations for dimensionality reduction. We build on this newfound understanding of varimax rotations by developing further connections to network analysis and spectral methods rooted in entrywise matrix perturbation analysis. Concretely, this paper establishes the asymptotic multivariate normality of vectors in varimax-transformed Euclidean point clouds that represent low-dimensional node embeddings in certain latent space random graph models. We address related concepts including network sparsity, data denoising, and the role of matrix rank in latent variable parameterizations. Collectively, these findings, at the confluence of classical and contemporary multivariate analysis, reinforce methodology and inference procedures grounded in matrix factorization-based techniques. Numerical examples illustrate our findings and supplement our discussion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05461v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Cape</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference for High-dimensional Time Series by Latent Process Modeling</title>
      <link>https://arxiv.org/abs/2403.04915</link>
      <description>arXiv:2403.04915v1 Announce Type: cross 
Abstract: Time series data arising in many applications nowadays are high-dimensional. A large number of parameters describe features of these time series. We propose a novel approach to modeling a high-dimensional time series through several independent univariate time series, which are then orthogonally rotated and sparsely linearly transformed. With this approach, any specified intrinsic relations among component time series given by a graphical structure can be maintained at all time snapshots. We call the resulting process an Orthogonally-rotated Univariate Time series (OUT). Key structural properties of time series such as stationarity and causality can be easily accommodated in the OUT model. For Bayesian inference, we put suitable prior distributions on the spectral densities of the independent latent times series, the orthogonal rotation matrix, and the common precision matrix of the component times series at every time point. A likelihood is constructed using the Whittle approximation for univariate latent time series. An efficient Markov Chain Monte Carlo (MCMC) algorithm is developed for posterior computation. We study the convergence of the pseudo-posterior distribution based on the Whittle likelihood for the model's parameters upon developing a new general posterior convergence theorem for pseudo-posteriors. We find that the posterior contraction rate for independent observations essentially prevails in the OUT model under very mild conditions on the temporal dependence described in terms of the smoothness of the corresponding spectral densities. Through a simulation study, we compare the accuracy of estimating the parameters and identifying the graphical structure with other approaches. We apply the proposed methodology to analyze a dataset on different industrial components of the US gross domestic product between 2010 and 2019 and predict future observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04915v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arkaprava Roy, Anindya Roy, Subhashis Ghosal</dc:creator>
    </item>
    <item>
      <title>Information Theory in a Darwinian Evolution Population Dynamics Model</title>
      <link>https://arxiv.org/abs/2403.05044</link>
      <description>arXiv:2403.05044v1 Announce Type: cross 
Abstract: Using information theory, we propose an estimation method for traits parameters in a Darwinian evolution model for species with on trait or multiple traits. We use the Fisher's information to obtain the errors on the estimation for one species with one or multiple traits. We perform simulations to illustrate the method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05044v1</guid>
      <category>q-bio.PE</category>
      <category>cs.IT</category>
      <category>math.DS</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eddy Kwessi</dc:creator>
    </item>
    <item>
      <title>An Efficient Quasi-Random Sampling for Copulas</title>
      <link>https://arxiv.org/abs/2403.05281</link>
      <description>arXiv:2403.05281v1 Announce Type: cross 
Abstract: This paper examines an efficient method for quasi-random sampling of copulas in Monte Carlo computations. Traditional methods, like conditional distribution methods (CDM), have limitations when dealing with high-dimensional or implicit copulas, which refer to those that cannot be accurately represented by existing parametric copulas. Instead, this paper proposes the use of generative models, such as Generative Adversarial Networks (GANs), to generate quasi-random samples for any copula. GANs are a type of implicit generative models used to learn the distribution of complex data, thus facilitating easy sampling. In our study, GANs are employed to learn the mapping from a uniform distribution to copulas. Once this mapping is learned, obtaining quasi-random samples from the copula only requires inputting quasi-random samples from the uniform distribution. This approach offers a more flexible method for any copula. Additionally, we provide theoretical analysis of quasi-Monte Carlo estimators based on quasi-random samples of copulas. Through simulated and practical applications, particularly in the field of risk management, we validate the proposed method and demonstrate its superiority over various existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05281v1</guid>
      <category>stat.ML</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sumin Wang, Chenxian Huang, Yongdao Zhou, Min-Qian Liu</dc:creator>
    </item>
    <item>
      <title>Simulating conditioned diffusions on manifolds</title>
      <link>https://arxiv.org/abs/2403.05409</link>
      <description>arXiv:2403.05409v1 Announce Type: cross 
Abstract: To date, most methods for simulating conditioned diffusions are limited to the Euclidean setting. The conditioned process can be constructed using a change of measure known as Doob's $h$-transform. The specific type of conditioning depends on a function $h$ which is typically unknown in closed form. To resolve this, we extend the notion of guided processes to a manifold $M$, where one replaces $h$ by a function based on the heat kernel on $M$. We consider the case of a Brownian motion with drift, constructed using the frame bundle of $M$, conditioned to hit a point $x_T$ at time $T$. We prove equivalence of the laws of the conditioned process and the guided process with a tractable Radon-Nikodym derivative. Subsequently, we show how one can obtain guided processes on any manifold $N$ that is diffeomorphic to $M$ without assuming knowledge of the heat kernel on $N$.
  We illustrate our results with numerical simulations and an example of parameter estimation where a diffusion process on the torus is observed discretely in time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05409v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marc Corstanje, Frank van der Meulen, Moritz Schauer, Stefan Sommer</dc:creator>
    </item>
    <item>
      <title>Linear Model Estimators and Consistency under an Infill Asymptotic Domain</title>
      <link>https://arxiv.org/abs/2403.05503</link>
      <description>arXiv:2403.05503v1 Announce Type: cross 
Abstract: Functional data present as functions or curves possessing a spatial or temporal component. These components by nature have a fixed observational domain. Consequently, any asymptotic investigation requires modelling the increased correlation among observations as density increases due to this fixed domain constraint. One such appropriate stochastic process is the Ornstein-Uhlenbeck process. Utilizing this spatial autoregressive process, we demonstrate that parameter estimators for a simple linear regression model display inconsistency in an infill asymptotic domain. Such results are contrary to those expected under the customary increasing domain asymptotics. Although none of these estimator variances approach zero, they do display a pattern of diminishing return regarding decreasing estimator variance as sample size increases. This may prove invaluable to a practitioner as this indicates perhaps an optimal sample size to cease data collection. This in turn reduces time and data collection cost because little information is gained in sampling beyond a certain sample size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05503v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cory W. Natoli, Edward D. White, Beau A. Nunnally, Alex J. Gutman, Raymond R. Hill</dc:creator>
    </item>
    <item>
      <title>Automatic Debiased Machine Learning via Riesz Regression</title>
      <link>https://arxiv.org/abs/2104.14737</link>
      <description>arXiv:2104.14737v2 Announce Type: replace 
Abstract: A variety of interesting parameters may depend on high dimensional regressions. Machine learning can be used to estimate such parameters. However estimators based on machine learners can be severely biased by regularization and/or model selection. Debiased machine learning uses Neyman orthogonal estimating equations to reduce such biases. Debiased machine learning generally requires estimation of unknown Riesz representers. A primary innovation of this paper is to provide Riesz regression estimators of Riesz representers that depend on the parameter of interest, rather than explicit formulae, and that can employ any machine learner, including neural nets and random forests. End-to-end algorithms emerge where the researcher chooses the parameter of interest and the machine learner and the debiasing follows automatically. Another innovation here is debiased machine learners of parameters depending on generalized regressions, including high-dimensional generalized linear models. An empirical example of automatic debiased machine learning using neural nets is given. We find in Monte Carlo examples that automatic debiasing sometimes performs better than debiasing via inverse propensity scores and never worse. Finite sample mean square error bounds for Riesz regression estimators and asymptotic theory are also given.</description>
      <guid isPermaLink="false">oai:arXiv.org:2104.14737v2</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Victor Chernozhukov, Whitney K. Newey, Victor Quintas-Martinez, Vasilis Syrgkanis</dc:creator>
    </item>
    <item>
      <title>Analysis of two-component Gibbs samplers using the theory of two projections</title>
      <link>https://arxiv.org/abs/2201.12500</link>
      <description>arXiv:2201.12500v4 Announce Type: replace 
Abstract: The theory of two projections is utilized to study two-component Gibbs samplers. Through this theory, previously intractable problems regarding the asymptotic variances of two-component Gibbs samplers are reduced to elementary matrix algebra exercises. It is found that in terms of asymptotic variance, the two-component random-scan Gibbs sampler is never much worse, and could be considerably better than its deterministic-scan counterpart, provided that the selection probability is appropriately chosen. This is especially the case when there is a large discrepancy in computation cost between the two components. The result contrasts with the known fact that the deterministic-scan version has a faster convergence rate, which can also be derived from the method herein. On the other hand, a modified version of the deterministic-scan sampler that accounts for computation cost can outperform the random-scan version.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.12500v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qian Qin</dc:creator>
    </item>
    <item>
      <title>Likelihood-free hypothesis testing</title>
      <link>https://arxiv.org/abs/2211.01126</link>
      <description>arXiv:2211.01126v5 Announce Type: replace 
Abstract: Consider the problem of binary hypothesis testing. Given $Z$ coming from either $\mathbb P^{\otimes m}$ or $\mathbb Q^{\otimes m}$, to decide between the two with small probability of error it is sufficient, and in many cases necessary, to have $m\asymp1/\epsilon^2$, where $\epsilon$ measures the separation between $\mathbb P$ and $\mathbb Q$ in total variation ($\mathsf{TV}$). Achieving this, however, requires complete knowledge of the distributions and can be done, for example, using the Neyman-Pearson test. In this paper we consider a variation of the problem which we call likelihood-free hypothesis testing, where access to $\mathbb P$ and $\mathbb Q$ is given through $n$ i.i.d. observations from each. In the case when $\mathbb P$ and $\mathbb Q$ are assumed to belong to a non-parametric family, we demonstrate the existence of a fundamental trade-off between $n$ and $m$ given by $nm\asymp n_\sf{GoF}^2(\epsilon)$, where $n_\sf{GoF}(\epsilon)$ is the minimax sample complexity of testing between the hypotheses $H_0:\, \mathbb P=\mathbb Q$ vs $H_1:\, \mathsf{TV}(\mathbb P,\mathbb Q)\geq\epsilon$. We show this for three families of distributions, in addition to the family of all discrete distributions for which we obtain a more complicated trade-off exhibiting an additional phase-transition. Our results demonstrate the possibility of testing without fully estimating $\mathbb P$ and $\mathbb Q$, provided $m \gg 1/\epsilon^2$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.01126v5</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patrik R\'obert Gerber, Yury Polyanskiy</dc:creator>
    </item>
    <item>
      <title>A Simple Bootstrap for Chatterjee's Rank Correlation</title>
      <link>https://arxiv.org/abs/2308.01027</link>
      <description>arXiv:2308.01027v2 Announce Type: replace 
Abstract: We prove that an $m$ out of $n$ bootstrap procedure for Chatterjee's rank correlation is consistent whenever asymptotic normality of Chatterjee's rank correlation can be established. In particular, we prove that $m$ out of $n$ bootstrap works for continuous as well as for discrete data with independent coordinates; furthermore, simulations indicate that it also performs well for discrete data with dependent coordinates, and that it outperforms alternative estimation methods. Consistency of the bootstrap is proved in the Kolmogorov as well as in the Wasserstein distance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.01027v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Holger Dette, Marius Kroll</dc:creator>
    </item>
    <item>
      <title>An Optimal Transport Approach to Estimating Causal Effects via Nonlinear Difference-in-Differences</title>
      <link>https://arxiv.org/abs/2108.05858</link>
      <description>arXiv:2108.05858v2 Announce Type: replace-cross 
Abstract: We propose a nonlinear difference-in-differences method to estimate multivariate counterfactual distributions in classical treatment and control study designs with observational data. Our approach sheds a new light on existing approaches like the changes-in-changes and the classical semiparametric difference-in-differences estimator and generalizes them to settings with multivariate heterogeneity in the outcomes. The main benefit of this extension is that it allows for arbitrary dependence and heterogeneity in the joint outcomes. We demonstrate its utility both on synthetic and real data. In particular, we revisit the classical Card \&amp; Krueger dataset, examining the effect of a minimum wage increase on employment in fast food restaurants; a reanalysis with our method reveals that restaurants tend to substitute full-time with part-time labor after a minimum wage increase at a faster pace. A previous version of this work was entitled "An optimal transport approach to causal inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2108.05858v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>William Torous, Florian Gunsilius, Philippe Rigollet</dc:creator>
    </item>
    <item>
      <title>Settling the Sample Complexity of Model-Based Offline Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2204.05275</link>
      <description>arXiv:2204.05275v4 Announce Type: replace-cross 
Abstract: This paper is concerned with offline reinforcement learning (RL), which learns using pre-collected data without further exploration. Effective offline RL would be able to accommodate distribution shift and limited data coverage. However, prior algorithms or analyses either suffer from suboptimal sample complexities or incur high burn-in cost to reach sample optimality, thus posing an impediment to efficient offline RL in sample-starved applications.
  We demonstrate that the model-based (or "plug-in") approach achieves minimax-optimal sample complexity without burn-in cost for tabular Markov decision processes (MDPs). Concretely, consider a finite-horizon (resp. $\gamma$-discounted infinite-horizon) MDP with $S$ states and horizon $H$ (resp. effective horizon $\frac{1}{1-\gamma}$), and suppose the distribution shift of data is reflected by some single-policy clipped concentrability coefficient $C^{\star}_{\text{clipped}}$. We prove that model-based offline RL yields $\varepsilon$-accuracy with a sample complexity of \[ \begin{cases} \frac{H^{4}SC_{\text{clipped}}^{\star}}{\varepsilon^{2}} &amp; (\text{finite-horizon MDPs}) \frac{SC_{\text{clipped}}^{\star}}{(1-\gamma)^{3}\varepsilon^{2}} &amp; (\text{infinite-horizon MDPs}) \end{cases} \] up to log factor, which is minimax optimal for the entire $\varepsilon$-range. The proposed algorithms are "pessimistic" variants of value iteration with Bernstein-style penalties, and do not require sophisticated variance reduction. Our analysis framework is established upon delicate leave-one-out decoupling arguments in conjunction with careful self-bounding techniques tailored to MDPs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.05275v4</guid>
      <category>stat.ML</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Annals of Statistics, vol. 52, no. 1, pp. 233-260, 2024</arxiv:journal_reference>
      <dc:creator>Gen Li, Laixi Shi, Yuxin Chen, Yuejie Chi, Yuting Wei</dc:creator>
    </item>
    <item>
      <title>Functional Linear Regression of Cumulative Distribution Functions</title>
      <link>https://arxiv.org/abs/2205.14545</link>
      <description>arXiv:2205.14545v3 Announce Type: replace-cross 
Abstract: The estimation of cumulative distribution functions (CDF) is an important learning task with a great variety of downstream applications, such as risk assessments in predictions and decision making. In this paper, we study functional regression of contextual CDFs where each data point is sampled from a linear combination of context dependent CDF basis functions. We propose functional ridge-regression-based estimation methods that estimate CDFs accurately everywhere. In particular, given $n$ samples with $d$ basis functions, we show estimation error upper bounds of $\widetilde O(\sqrt{d/n})$ for fixed design, random design, and adversarial context cases. We also derive matching information theoretic lower bounds, establishing minimax optimality for CDF functional regression. Furthermore, we remove the burn-in time in the random design setting using an alternative penalized estimator. Then, we consider agnostic settings where there is a mismatch in the data generation process. We characterize the error of the proposed estimators in terms of the mismatched error, and show that the estimators are well-behaved under model mismatch. Moreover, to complete our study, we formalize infinite dimensional models where the parameter space is an infinite dimensional Hilbert space, and establish a self-normalized estimation error upper bound for this setting. Notably, the upper bound reduces to the $\widetilde O(\sqrt{d/n})$ bound when the parameter space is constrained to be $d$-dimensional. Our comprehensive numerical experiments validate the efficacy of our estimation methods in both synthetic and practical settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.14545v3</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qian Zhang, Anuran Makur, Kamyar Azizzadenesheli</dc:creator>
    </item>
    <item>
      <title>Near-Optimal Non-Parametric Sequential Tests and Confidence Sequences with Possibly Dependent Observations</title>
      <link>https://arxiv.org/abs/2212.14411</link>
      <description>arXiv:2212.14411v4 Announce Type: replace-cross 
Abstract: Sequential testing, always-valid $p$-values, and confidence sequences promise flexible statistical inference and on-the-fly decision making. However, unlike fixed-$n$ inference based on asymptotic normality, existing sequential tests either make parametric assumptions and end up under-covering/over-rejecting when these fail or use non-parametric but conservative concentration inequalities and end up over-covering/under-rejecting. To circumvent these issues, we sidestep exact at-least-$\alpha$ coverage and focus on asymptotic calibration and asymptotic optimality. That is, we seek sequential tests whose probability of \emph{ever} rejecting a true hypothesis approaches $\alpha$ and whose expected time to reject a false hypothesis approaches a lower bound on all such asymptotically calibrated tests, both "approaches" occurring under an appropriate limit. We permit observations to be both non-parametric and dependent and focus on testing whether the observations form a martingale difference sequence. We propose the universal sequential probability ratio test (uSPRT), a slight modification to the normal-mixture sequential probability ratio test, where we add a burn-in period and adjust thresholds accordingly. We show that even in this very general setting, the uSPRT is asymptotically optimal under mild generic conditions. We apply the results to stabilized estimating equations to test means, treatment effects, {\etc} Our results also provide corresponding guarantees for the implied confidence sequences. Numerical simulations verify our guarantees and the benefits of the uSPRT over alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.14411v4</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aurelien Bibaut, Nathan Kallus, Michael Lindon</dc:creator>
    </item>
    <item>
      <title>Estimating the Convex Hull of the Image of a Set with Smooth Boundary: Error Bounds and Applications</title>
      <link>https://arxiv.org/abs/2302.13970</link>
      <description>arXiv:2302.13970v3 Announce Type: replace-cross 
Abstract: We study the problem of estimating the convex hull of the image $f(X)\subset\mathbb{R}^n$ of a compact set $X\subset\mathbb{R}^m$ with smooth boundary through a smooth function $f:\mathbb{R}^m\to\mathbb{R}^n$. Assuming that $f$ is a submersion, we derive a new bound on the Hausdorff distance between the convex hull of $f(X)$ and the convex hull of the images $f(x_i)$ of $M$ sampled inputs $x_i$ on the boundary of $X$. When applied to the problem of geometric inference from a random sample, our results give error bounds that are tighter and more general than in previous work. We present applications to the problems of robust optimization, of reachability analysis of dynamical systems, and of robust trajectory optimization under bounded uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.13970v3</guid>
      <category>math.OC</category>
      <category>cs.CG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.DG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Lew, Riccardo Bonalli, Lucas Janson, Marco Pavone</dc:creator>
    </item>
    <item>
      <title>Efficient and Multiply Robust Risk Estimation under General Forms of Dataset Shift</title>
      <link>https://arxiv.org/abs/2306.16406</link>
      <description>arXiv:2306.16406v3 Announce Type: replace-cross 
Abstract: Statistical machine learning methods often face the challenge of limited data available from the population of interest. One remedy is to leverage data from auxiliary source populations, which share some conditional distributions or are linked in other ways with the target domain. Techniques leveraging such \emph{dataset shift} conditions are known as \emph{domain adaptation} or \emph{transfer learning}. Despite extensive literature on dataset shift, limited works address how to efficiently use the auxiliary populations to improve the accuracy of risk evaluation for a given machine learning task in the target population.
  In this paper, we study the general problem of efficiently estimating target population risk under various dataset shift conditions, leveraging semiparametric efficiency theory. We consider a general class of dataset shift conditions, which includes three popular conditions -- covariate, label and concept shift -- as special cases. We allow for partially non-overlapping support between the source and target populations. We develop efficient and multiply robust estimators along with a straightforward specification test of these dataset shift conditions. We also derive efficiency bounds for two other dataset shift conditions, posterior drift and location-scale shift. Simulation studies support the efficiency gains due to leveraging plausible dataset shift conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16406v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongxiang Qiu, Eric Tchetgen Tchetgen, Edgar Dobriban</dc:creator>
    </item>
    <item>
      <title>Sparse Data-Driven Random Projection in Regression for High-Dimensional Data</title>
      <link>https://arxiv.org/abs/2312.00130</link>
      <description>arXiv:2312.00130v2 Announce Type: replace-cross 
Abstract: We examine the linear regression problem in a challenging high-dimensional setting with correlated predictors where the vector of coefficients can vary from sparse to dense. In this setting, we propose a combination of probabilistic variable screening with random projection tools as a viable approach. More specifically, we introduce a new data-driven random projection tailored to the problem at hand and derive a theoretical bound on the gain in expected prediction error over conventional random projections. The variables to enter the projection are screened by accounting for predictor correlation. To reduce the dependence on fine-tuning choices, we aggregate over an ensemble of linear models. A thresholding parameter is introduced to obtain a higher degree of sparsity. Both this parameter and the number of models in the ensemble can be chosen by cross-validation. In extensive simulations, we compare the proposed method with other random projection tools and with classical sparse and dense methods and show that it is competitive in terms of prediction across a variety of scenarios with different sparsity and predictor covariance settings. We also show that the method with cross-validation is able to rank the variables satisfactorily. Finally, we showcase the method on two real data applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.00130v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roman Parzer, Peter Filzmoser, Laura Vana-G\"ur</dc:creator>
    </item>
  </channel>
</rss>
