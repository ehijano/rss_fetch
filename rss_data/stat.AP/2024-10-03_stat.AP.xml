<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 03 Oct 2024 07:17:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Data-Driven Modeling of Seasonal Dengue Dynamics in Bangladesh: A Bayesian-Stochastic Approach</title>
      <link>https://arxiv.org/abs/2410.00947</link>
      <description>arXiv:2410.00947v1 Announce Type: new 
Abstract: Bangladesh's worsening dengue crisis, fueled by its tropical climate, poor waste management infrastructure, rapid urbanization, and dense population, has led to increasingly deadly outbreaks, posing a significant public health threat. To address this, we propose a nonlinear, time-nonhomogeneous SEIR model incorporating seasonality through a novel transmission rate function. The model parameters are estimated using Bayesian inference with the Metropolis-Hastings algorithm in a Markov Chain Monte Carlo (MCMC) framework, calibrated with real-life dengue data from Bangladesh. To account for stochasticity and better assess outbreak probabilities, we extend the model to a time-nonhomogeneous continuous-time Markov chain (CTMC) framework. Our model provides new insights that can guide policymakers and offer a robust mathematical framework to better combat this crisis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00947v1</guid>
      <category>stat.AP</category>
      <category>q-bio.PE</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mahmudul Bari Hridoy (Texas Tech University), S M Mustaquim (The University of Texas at El Paso)</dc:creator>
    </item>
    <item>
      <title>Development of a Statistical Predictive Model for Daily Water Table Depth and Important Variables Selection for Inference</title>
      <link>https://arxiv.org/abs/2410.01001</link>
      <description>arXiv:2410.01001v1 Announce Type: new 
Abstract: Accurately predicting water table dynamics is vital for sustaining groundwater resources that support ecological functions and anthropogenic activities. This study evaluates a statistical model (BigVAR) that handles three major flexibilities: (a) prediction under a sparsity assumption in coefficients, (b) consideration of a time series autoregression framework, and (c) allowance for lags in both dependent and independent variables for estimating water table depth using daily hydroclimatic data from the USDA Forest Service Santee Experimental Forest (SC) and a site in NC. Data from 2006--2019 (SC) and 1988--2008 (NC) were used, with key predictors including soil and air temperature, precipitation, wind, and radiation. For WS80, RMSE during the dormant season was 10.09 cm, with a daily testing phase RMSE of 14.94 cm. The model achieved an R^2 of 0.93 for 2019 (a dry year) and 0.96 for 2016 (a wet year). Solar radiation, rainfall, and wind direction were among the most influential variables. This predictive model aids in managing wetland hydrology and supports decision-making for forest managers and hydrologists.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01001v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alokesh Manna, Sushant Mehan, Devendra M. Amatya</dc:creator>
    </item>
    <item>
      <title>The Effects of Air Pollution on Health: A Study of Los Angeles County</title>
      <link>https://arxiv.org/abs/2410.01151</link>
      <description>arXiv:2410.01151v1 Announce Type: new 
Abstract: This study aims to develop and implement a Poisson regression model with measurement error using a Bayesian framework, with model fitting performed in Stan. The focus is on examining the relationship between air pollution exposure and health outcomes, such as respiratory and cardiovascular disease counts, while accounting for inaccuracies in pollution measurements. Air pollution data is often subject to measurement error due to imperfect monitoring or averaging, which, if ignored, can lead to biased estimates and incorrect conclusions. The Poisson regression will model count data, where the response variable, such as disease counts, follows a Poisson distribution. Covariates including pollution levels, demographic factors, and meteorological conditions will be incorporated to control for confounders. To address measurement error in the exposure data, a Bayesian hierarchical model will be used, where observed pollution levels are treated as noisy measurements of the true underlying exposure. Priors will be specified for both the regression coefficients and the measurement error parameters, and posterior distributions will be estimated via Markov Chain Monte Carlo (MCMC) sampling. This approach ensures that both the count nature of the response and the uncertainty in exposure measurements are properly accounted for, leading to more accurate estimates of the health risks associated with air pollution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01151v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanfei Qu</dc:creator>
    </item>
    <item>
      <title>Diverse Expected Improvement (DEI): Diverse Bayesian Optimization of Expensive Computer Simulators</title>
      <link>https://arxiv.org/abs/2410.01196</link>
      <description>arXiv:2410.01196v1 Announce Type: new 
Abstract: The optimization of expensive black-box simulators arises in a myriad of modern scientific and engineering applications. Bayesian optimization provides an appealing solution, by leveraging a fitted surrogate model to guide the selection of subsequent simulator evaluations. In practice, however, the objective is often not to obtain a single good solution, but rather a ''basket'' of good solutions from which users can choose for downstream decision-making. This need arises in our motivating application for real-time control of internal combustion engines for flight propulsion, where a diverse set of control strategies is essential for stable flight control. There has been little work on this front for Bayesian optimization. We thus propose a new Diverse Expected Improvement (DEI) method that searches for diverse ''$\epsilon$-optimal'' solutions: locally-optimal solutions within a tolerance level $\epsilon &gt; 0$ from a global optimum. We show that DEI yields a closed-form acquisition function under a Gaussian process surrogate model, which facilitates efficient sequential queries via automatic differentiation. This closed form further reveals a novel exploration-exploitation-diversity trade-off, which incorporates the desired diversity property within the well-known exploration-exploitation trade-off. We demonstrate the improvement of DEI over existing methods in a suite of numerical experiments, then explore the DEI in two applications on rover trajectory optimization and engine control for flight propulsion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01196v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John Joshua Miller, Simon Mak, Benny Sun, Sai Ranjeet Narayanan, Suo Yang, Zongxuan Sun, Kenneth S. Kim, Chol-Bum Mike Kweon</dc:creator>
    </item>
    <item>
      <title>A Blockwise Mixed Membership Model for Multivariate Longitudinal Data: Discovering Clinical Heterogeneity and Identifying Parkinson's Disease Subtypes</title>
      <link>https://arxiv.org/abs/2410.01235</link>
      <description>arXiv:2410.01235v1 Announce Type: new 
Abstract: Current diagnosis and prognosis for Parkinson's disease (PD) face formidable challenges due to the heterogeneous nature of the disease course, including that (i) the impairment severity varies hugely between patients, (ii) whether a symptom occur independently or co-occurs with related symptoms differs significantly, and (iii) repeated symptom measurements exhibit substantial temporal dependence. To tackle these challenges, we propose a novel blockwise mixed membership model (BM3) to systematically unveil between-patient, between-symptom, and between-time clinical heterogeneity within PD. The key idea behind BM3 is to partition multivariate longitudinal measurements into distinct blocks, enabling measurements within each block to share a common latent membership while allowing latent memberships to vary across blocks. Consequently, the heterogeneous PD-related measurements across time are divided into clinically homogeneous blocks consisting of correlated symptoms and consecutive time. From the analysis of Parkinson's Progression Markers Initiative data (n=1,531), we discover three typical disease profiles (stages), four symptom groups (i.e., autonomic function, tremor, left-side and right-side motor function), and two periods, advancing the comprehension of PD heterogeneity. Moreover, we identify several clinically meaningful PD subtypes by summarizing the blockwise latent memberships, paving the way for developing more precise and targeted therapies to benefit patients. Our findings are validated using external variables, successfully reproduced in validation datasets, and compared with existing methods. Theoretical results of model identifiability further ensures the reliability and reproducibility of latent structure discovery in PD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01235v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kai Kang, Yuqi Gu</dc:creator>
    </item>
    <item>
      <title>Considerations for the planning, conduct and reporting of clinical trials with interim analyses</title>
      <link>https://arxiv.org/abs/2410.01478</link>
      <description>arXiv:2410.01478v1 Announce Type: new 
Abstract: Interim analyses are prevalent in clinical trials. Although methodology is well established, there are aspects of how to operationalize and interpret interim analyses which remain unclear to many stakeholders. In this paper, a team of statisticians from the pharmaceutical industry, academia, and regulatory agencies provide a multi-stakeholder perspective on the key concepts behind interim analyses and considerations on terminology. We illustrate our proposals using a hypothetical clinical trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01478v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elina Asikanius, Benjamin Hofner, Lisa V. Hampson, Gernot Wassmer, Christopher Jennison, Tobias Mielke, Cornelia Ursula Kunz, Kaspar Rufibach</dc:creator>
    </item>
    <item>
      <title>Uncertainty quantification in neutron and gamma time correlation measurements</title>
      <link>https://arxiv.org/abs/2410.01522</link>
      <description>arXiv:2410.01522v1 Announce Type: new 
Abstract: Neutron noise analysis is a predominant technique for fissile matter identification with passive methods. Quantifying the uncertainties associated with the estimated nuclear parameters is crucial for decision-making. A conservative uncertainty quantification procedure is possible by solving a Bayesian inverse problem with the help of statistical surrogate models but generally leads to large uncertainties due to the surrogate models' errors. In this work, we develop two methods for robust uncertainty quantification in neutron and gamma noise analysis based on the resolution of Bayesian inverse problems. We show that the uncertainties can be reduced by including information on gamma correlations. The investigation of a joint analysis of the neutron and gamma observations is also conducted with the help of active learning strategies to fine-tune surrogate models. We test our methods on a model of the SILENE reactor core, using simulated and real-world measurements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01522v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul Lartaud, Philippe Humbert, Josselin Garnier</dc:creator>
    </item>
    <item>
      <title>Addressing Spatial Confounding in geostatistical regression models: An R-INLA approach</title>
      <link>https://arxiv.org/abs/2410.01530</link>
      <description>arXiv:2410.01530v1 Announce Type: new 
Abstract: 1 - Spatial confounding is a phenomenon that has been studied extensively in recent years in the statistical literature to describe and mitigate apparent inconsistencies between the results obtained by regression models with and without random spatial effects. While the most common solutions target almost exclusively areal data or geostatistical data modelling by splines, we aim to extend some resolution methods in the context of geostatistical data modelling by Gaussian Markov Random Fields (GMRF) using R-INLA methodology.
  2 - First, we present three approaches for alleviating spatial confounding: Restricted Spatial Regression (RSR), Spatial+, and its recent simplified version, called here Spatial+ 2.0. We show how each can be implemented from geostatistical data in a GMRF framework using R-inlabru.
  3 - Next, a simulation study that reproduces a spatial confounding phenomenon is carried out to assess the coherence of the extensions with the expectations of these methods. Finally, we apply the expanded methods to a case study, linking cadmium (Cd) concentration in terrestrial mosses to Cd concentration in air.
  4 - Our findings support the feasibility of our extended approach of spatial confounding resolution methods to geostatistical data using R-INLA in keeping with the previous contexts, although certain precautions and limitations must be considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01530v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J\'er\'emy Lamouroux, Aliz\'ee Geffroy, S\'ebastien Leblond, Caroline Meyer, Isabelle Albert</dc:creator>
    </item>
    <item>
      <title>Learning and teaching biological data science in the Bioconductor community</title>
      <link>https://arxiv.org/abs/2410.01351</link>
      <description>arXiv:2410.01351v1 Announce Type: cross 
Abstract: Modern biological research is increasingly data-intensive, leading to a growing demand for effective training in biological data science. In this article, we provide an overview of key resources and best practices available within the Bioconductor project - an open-source software community focused on omics data analysis. This guide serves as a valuable reference for both learners and educators in the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01351v1</guid>
      <category>cs.CY</category>
      <category>q-bio.OT</category>
      <category>stat.AP</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jenny Drnevich, Frederick J. Tan, Fabricio Almeida-Silva, Robert Castelo, Aedin C. Culhane, Sean Davis, Maria A. Doyle, Susan Holmes, Leo Lahti, Alexandru Mahmoud, Kozo Nishida, Marcel Ramos, Kevin Rue-Albrecht, David J. H. Shih, Laurent Gatto, Charlotte Soneson</dc:creator>
    </item>
    <item>
      <title>Exploring Learning Rate Selection in Generalised Bayesian Inference using Posterior Predictive Checks</title>
      <link>https://arxiv.org/abs/2410.01475</link>
      <description>arXiv:2410.01475v1 Announce Type: cross 
Abstract: Generalised Bayesian Inference (GBI) attempts to address model misspecification in a standard Bayesian setup by tempering the likelihood. The likelihood is raised to a fractional power, called the learning rate, which reduces its importance in the posterior and has been established as a method to address certain kinds of model misspecification. Posterior Predictive Checks (PPC) attempt to detect model misspecification by locating a diagnostic, computed on the observed data, within the posterior predictive distribution of the diagnostic. This can be used to construct a hypothesis test where a small $p$-value indicates potential misfit. The recent Embedded Diachronic Sense Change (EDiSC) model suffers from misspecification and benefits from likelihood tempering. Using EDiSC as a case study, this exploratory work examines whether PPC could be used in a novel way to set the learning rate in a GBI setup. Specifically, the learning rate selected is the lowest value for which a hypothesis test using the log likelihood diagnostic is not rejected at the 10% level. The experimental results are promising, though not definitive, and indicate the need for further research along the lines suggested here.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01475v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Schyan Zafar, Geoff K. Nicholls</dc:creator>
    </item>
    <item>
      <title>Condensation phenomena of ions in an electrostatic logarithmic trap</title>
      <link>https://arxiv.org/abs/2410.01582</link>
      <description>arXiv:2410.01582v1 Announce Type: cross 
Abstract: The effects of an electrostatic logarithmic trap (ELT) on an ionic gas confined in a cylindric chamber are studied in detail, with special reference to the effects of the ion-ion Coulombic interactions and the resulting low-temperature thermodynamics. The collapse of the ions in radially localized states, about the axial cathode, is shown to cause an abrupt (but not critical) transition from non degeneration to strong degeneration, at a special temperature $T_c$. This transition could actually involve both Bosons and Fermions and is not to be confused with a Bose-Einstein condensation (BEC), which is excluded in principle. However, while for Bosons the resulting effects on the pressure are observable in the ultra high vacuum (UHV) regime, the Fermions' density should fall well below UHV, for the pressure change to be observable. This is because the ion-ion \emph{exchange} interactions increase the kinetic energy along the axial cathode, which makes the Fermi level and the non degeneration threshold temperature increase accordingly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01582v1</guid>
      <category>physics.app-ph</category>
      <category>cond-mat.quant-gas</category>
      <category>quant-ph</category>
      <category>stat.AP</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Loris Ferrari</dc:creator>
    </item>
    <item>
      <title>On metric choice in dimension reduction for Fr\'echet regression</title>
      <link>https://arxiv.org/abs/2410.01783</link>
      <description>arXiv:2410.01783v1 Announce Type: cross 
Abstract: Fr\'echet regression is becoming a mainstay in modern data analysis for analyzing non-traditional data types belonging to general metric spaces. This novel regression method utilizes the pairwise distances between the random objects, which makes the choice of metric crucial in the estimation. In this paper, the effect of metric choice on the estimation of the dimension reduction subspace for the regression between random responses and Euclidean predictors is investigated. Extensive numerical studies illustrate how different metrics affect the central and central mean space estimates for regression involving responses belonging to some popular metric spaces versus Euclidean predictors. An analysis of the distributions of glycaemia based on continuous glucose monitoring data demonstrate how metric choice can influence findings in real applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01783v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdul-Nasah Soale, Congli Ma, Siyu Chen, Obed Koomson</dc:creator>
    </item>
    <item>
      <title>Multi-time small-area estimation of oil and gas production capacity by Bayesian multilevel modeling</title>
      <link>https://arxiv.org/abs/2408.11167</link>
      <description>arXiv:2408.11167v3 Announce Type: replace 
Abstract: This paper presents a Bayesian multilevel modeling approach for estimating well-level oil and gas production capacities across small geographic areas over multiple time periods. Focusing on a basin, which is a geologically and economically distinct drilling region, we model the production level of wells grouped by area and time, using priors as regulators of inferences. Our model accounts for area-level and time-level variations as well as well-level variations, incorporating lateral length, water usage, and sand usage. The Maidenhead Coordinate System is used to define uniform (small) geographic areas, many of which contain only a small number of wells in a given time period. The Bayesian small-area model is first built and checked, using data from the Eagle Ford region, covering the years 2014 to 2019. The model is expanded to accommodate temporal dynamics by introducing time-effect components, allowing for the analysis of production trends over times. We explore the impact of technological advancements by modeling water-sand intensity as a proxy for production efficiency. The Bayesian multilevel modeling provides robust and flexible tools for understanding oil and gas production at area and time levels, offering valuable insights for energy production prediction and management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11167v3</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hiroaki Minato</dc:creator>
    </item>
    <item>
      <title>Graph-constrained Analysis for Multivariate Functional Data</title>
      <link>https://arxiv.org/abs/2209.06294</link>
      <description>arXiv:2209.06294v4 Announce Type: replace-cross 
Abstract: Functional Gaussian graphical models (GGM) used for analyzing multivariate functional data customarily estimate an unknown graphical model representing the conditional relationships between the functional variables. However, in many applications of multivariate functional data, the graph is known and existing functional GGM methods cannot preserve a given graphical constraint. In this manuscript, we demonstrate how to conduct multivariate functional analysis that exactly conforms to a given inter-variable graph. We first show the equivalence between partially separable functional GGM and graphical Gaussian processes (GP), proposed originally for constructing optimal covariance functions for multivariate spatial data that retain the conditional independence relations in a given graphical model. The theoretical connection help design a new algorithm that leverages Dempster's covariance selection to calculate the maximum likelihood estimate of the covariance function for multivariate functional data under graphical constraints. We also show that the finite term truncation of functional GGM basis expansion used in practice is equivalent to a low-rank graphical GP, which is known to oversmooth marginal distributions. To remedy this, we extend our algorithm to better preserve marginal distributions while still respecting the graph and retaining computational scalability. The insights obtained from the new results presented in this manuscript will help practitioners better understand the relationship between these graphical models and in deciding on the appropriate method for their specific multivariate data analysis task. The benefits of the proposed algorithms are illustrated using empirical experiments and an application to functional modeling of neuroimaging data using the connectivity graph among regions of the brain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.06294v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Debangan Dey, Sudipto Banerjee, Martin Lindquist, Abhirup Datta</dc:creator>
    </item>
    <item>
      <title>Nested Instrumental Variables Design: Switcher Average Treatment Effect, Identification, Efficient Estimation and Generalizability</title>
      <link>https://arxiv.org/abs/2405.07102</link>
      <description>arXiv:2405.07102v2 Announce Type: replace-cross 
Abstract: Instrumental variables (IV) are a commonly used tool to estimate causal effects from non-randomized data. An archetype of an IV is a randomized trial with non-compliance where the randomized treatment assignment serves as an IV for the non-ignorable treatment received. Under a monotonicity assumption, a valid IV non-parametrically identifies the average treatment effect among a non-identified, latent complier subgroup, whose generalizability is often under debate. In many studies, there could exist multiple versions of an IV, for instance, different nudges to take the same treatment in different study sites in a multicentre clinical trial. These different versions of an IV may result in different compliance rates and offer a unique opportunity to study IV estimates' generalizability. In this article, we introduce a novel nested IV assumption and study identification of the average treatment effect among two latent subgroups: always-compliers and switchers, who are defined based on the joint potential treatment received under two versions of a binary IV. We derive the efficient influence function for the SWitcher Average Treatment Effect (SWATE) and propose efficient estimators. We then propose formal statistical tests of the generalizability of IV estimates under the nested IV framework. We apply the proposed method to the Prostate, Lung, Colorectal and Ovarian (PLCO) Cancer Screening Trial and study the causal effect of colorectal cancer screening and its generalizability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07102v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rui Wang, Ying-Qi Zhao, Oliver Dukes, Bo Zhang</dc:creator>
    </item>
    <item>
      <title>Categorization of 33 computational methods to detect spatially variable genes from spatially resolved transcriptomics data</title>
      <link>https://arxiv.org/abs/2405.18779</link>
      <description>arXiv:2405.18779v3 Announce Type: replace-cross 
Abstract: In the analysis of spatially resolved transcriptomics data, detecting spatially variable genes (SVGs) is crucial. Numerous computational methods exist, but varying SVG definitions and methodologies lead to incomparable results. We review \rv{33} state-of-the-art methods, categorizing SVGs into three types: overall, cell-type-specific, and spatial-domain-marker SVGs. Our review explains the intuitions underlying these methods, summarizes their applications, and categorizes the hypothesis tests they use in the trade-off between generality and specificity for SVG detection. We discuss challenges in SVG detection and propose future directions for improvement. Our review offers insights for method developers and users, advocating for category-specific benchmarking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18779v3</guid>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Guanao Yan, Shuo Harper Hua, Jingyi Jessica Li</dc:creator>
    </item>
    <item>
      <title>Tackling GenAI Copyright Issues: Originality Estimation and Genericization</title>
      <link>https://arxiv.org/abs/2406.03341</link>
      <description>arXiv:2406.03341v5 Announce Type: replace-cross 
Abstract: The rapid progress of generative AI technology has sparked significant copyright concerns, leading to numerous lawsuits filed against AI developers. While various techniques for mitigating copyright issues have been studied, significant risks remain. Here, we propose a genericization method that modifies the outputs of a generative model to make them more generic and less likely to infringe copyright. To achieve this, we introduce a metric for quantifying the level of originality of data in a manner that is consistent with the legal framework. This metric can be estimated by drawing samples from a generative model, which is then used for the genericization process. As a practical implementation, we introduce PREGen, which combines our genericization method with an existing mitigation technique. Experiments demonstrate that our genericization method successfully modifies the output of a text-to-image generative model so that it produces more generic, copyright-compliant images. Compared to the existing method, PREGen reduces the likelihood of generating copyrighted characters by more than half when the names of copyrighted characters are used as the prompt, dramatically improving the performance. Additionally, while generative models can produce copyrighted characters even when their names are not directly mentioned in the prompt, PREGen almost entirely prevents the generation of such characters in these cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03341v5</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hiroaki Chiba-Okabe, Weijie J. Su</dc:creator>
    </item>
    <item>
      <title>Tractable Ridge Regression for Paired Comparisons</title>
      <link>https://arxiv.org/abs/2406.09597</link>
      <description>arXiv:2406.09597v2 Announce Type: replace-cross 
Abstract: Paired comparison models, such as Bradley-Terry and Thurstone-Mosteller, are commonly used to estimate relative strengths of pairwise compared items in tournament-style data. We discuss estimation of paired comparison models with a ridge penalty. A new approach is derived which combines empirical Bayes and composite likelihoods without any need to re-fit the model, as a convenient alternative to cross-validation of the ridge tuning parameter. Simulation studies demonstrate much better predictive accuracy of the new approach relative to ordinary maximum likelihood. A widely used alternative, the application of a standard bias-reducing penalty, is also found to improve appreciably the performance of maximum likelihood; but the ridge penalty, with tuning as developed here, yields greater accuracy still. The methodology is illustrated through application to 28 seasons of English Premier League football.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09597v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cristiano Varin, David Firth</dc:creator>
    </item>
  </channel>
</rss>
