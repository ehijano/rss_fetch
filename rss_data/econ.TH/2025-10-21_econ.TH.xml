<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.TH</link>
    <description>econ.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 21 Oct 2025 04:01:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Rethinking Arrow--Debreu: A New Framework for Exchange, Time, and Uncertainty</title>
      <link>https://arxiv.org/abs/2510.16003</link>
      <description>arXiv:2510.16003v1 Announce Type: new 
Abstract: This paper revisits the Arrow-Debreu general equilibrium framework through the lens of effective trade, emphasizing the distinction between theoretical and realizable market interactions. We develop the Effective Trade Model (ETM), where transactions arise from bilateral feasibility rather than aggregate supply and demand desires. Within this framework, we establish the main properties of the price-demand correspondence and prove the existence of Nash equilibria, incorporating production, money, and network topology. The analysis extends to time, uncertainty, and open economies, revealing how loanable funds and exchange rates emerge endogenously. Our results show that equilibrium is shaped by transaction constraints, subjective pricing, and decentralized negotiation, rather than by universal market-clearing conditions, and thereby call into question the foundations of welfare theory. Anticipation is modeled via the conditional mode, capturing bounded rationality and information limitations in contrast to the rational expectations hypothesis. The ETM thus offers a behaviorally and structurally grounded alternative to classical general equilibrium, bridging microfoundations, monetary dynamics, and temporal consistency within a unified framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16003v1</guid>
      <category>econ.TH</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nizar Riane</dc:creator>
    </item>
    <item>
      <title>Collective Experimentation with Correlated Payoffs</title>
      <link>https://arxiv.org/abs/2510.16608</link>
      <description>arXiv:2510.16608v1 Announce Type: new 
Abstract: This paper studies an exponential bandit model in which a group of agents collectively decide whether to undertake a risky action $R$. This action is implemented if the fraction of agents voting for it exceeds a predetermined threshold $k$. Building on Strulovici (2008), which assumes the agents' payoffs are independent, we explore the case in which the agents' payoffs are correlated. During experimentation, each agent learns individually whether she benefits from $R$; in this way, she also gains information about its overall desirability. Furthermore, each agent is able to learn indirectly from the others, because in making her decisions, she conditions on being pivotal (i.e., she assumes her vote will determine the collective outcome). We show that, when the number of agents is large, increasing the threshold $k$ for implementing $R$ leads to increased experimentation. However, information regarding the overall desirability of $R$ is effectively aggregated only if $k$ is sufficiently low.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16608v1</guid>
      <category>econ.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kailin Chen</dc:creator>
    </item>
    <item>
      <title>Preference Measurement Error, Concentration in Recommendation Systems, and Persuasion</title>
      <link>https://arxiv.org/abs/2510.16972</link>
      <description>arXiv:2510.16972v1 Announce Type: new 
Abstract: Algorithmic recommendation based on noisy preference measurement is prevalent in recommendation systems. This paper discusses the consequences of such recommendation on market concentration and inequality. Binary types denoting a statistical majority and minority are noisily revealed through a statistical experiment. The achievable utilities and recommendation shares for the two groups can be analyzed as a Bayesian Persuasion problem. While under arbitrary noise structures, effects on concentration compared to a full-information market are ambiguous, under symmetric noise, concentration increases and consumer welfare becomes more unequal. We define symmetric statistical experiments and analyze persuasion under a restriction to such experiments, which may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16972v1</guid>
      <category>econ.TH</category>
      <category>cs.CY</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Andreas Haupt</dc:creator>
    </item>
    <item>
      <title>Strategic hiding and exploration in networks</title>
      <link>https://arxiv.org/abs/2510.16994</link>
      <description>arXiv:2510.16994v1 Announce Type: new 
Abstract: We propose and study a model of strategic network design and exploration where the hider, subject to a budget constraint restricting the number of links, chooses a connected network and the location of an object. Meanwhile, the seeker, not observing the network and the location of the object, chooses a network exploration strategy starting at a fixed node in the network. The network exploration follows the expanding search paradigm of Alpern and Lidbetter (2013). We obtain a Nash equilibrium and characterize equilibrium payoffs in the case of linking budget allowing for trees only. We also give an upper bound on the expected number of steps needed to find the hider for the case where the linking budget allows for at most one cycle in the network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16994v1</guid>
      <category>econ.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francis Bloch, Bhaskar Dutta, Marcin Dziubi\'nski</dc:creator>
    </item>
    <item>
      <title>When and what to learn in a changing world</title>
      <link>https://arxiv.org/abs/2510.17757</link>
      <description>arXiv:2510.17757v1 Announce Type: new 
Abstract: A decision-maker periodically acquires information about a changing state, controlling both the timing and content of updates. I characterize optimal policies using a decomposition of the dynamic problem into optimal stopping and static information acquisition. Eventually, information acquisition either stops or follows a simple cycle in which updates occur at regular intervals to restore prescribed levels of relative certainty. This enables precise analysis of long run dynamics across environments. As fixed costs of information vanish, belief changes become lumpy: it is optimal to either wait or acquire information so as to exactly confirm the current belief until rare news prompts a sudden change. The long run solution admits a closed-form characterization in terms of the "virtual flow payoff". I highlight an illustrative application to portfolio diversification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17757v1</guid>
      <category>econ.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>C\'esar Barilla</dc:creator>
    </item>
    <item>
      <title>The Burden of Interactive Alignment with Inconsistent Preferences</title>
      <link>https://arxiv.org/abs/2510.16368</link>
      <description>arXiv:2510.16368v1 Announce Type: cross 
Abstract: From media platforms to chatbots, algorithms shape how people interact, learn, and discover information. Such interactions between users and an algorithm often unfold over multiple steps, during which strategic users can guide the algorithm to better align with their true interests by selectively engaging with content. However, users frequently exhibit inconsistent preferences: they may spend considerable time on content that offers little long-term value, inadvertently signaling that such content is desirable. Focusing on the user side, this raises a key question: what does it take for such users to align the algorithm with their true interests?
  To investigate these dynamics, we model the user's decision process as split between a rational system 2 that decides whether to engage and an impulsive system 1 that determines how long engagement lasts. We then study a multi-leader, single-follower extensive Stackelberg game, where users, specifically system 2, lead by committing to engagement strategies and the algorithm best-responds based on observed interactions. We define the burden of alignment as the minimum horizon over which users must optimize to effectively steer the algorithm. We show that a critical horizon exists: users who are sufficiently foresighted can achieve alignment, while those who are not are instead aligned to the algorithm's objective. This critical horizon can be long, imposing a substantial burden. However, even a small, costly signal (e.g., an extra click) can significantly reduce it. Overall, our framework explains how users with inconsistent preferences can align an engagement-driven algorithm with their interests in a Stackelberg equilibrium, highlighting both the challenges and potential remedies for achieving alignment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16368v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>econ.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ali Shirali</dc:creator>
    </item>
    <item>
      <title>Dominated Actions in Imperfect-Information Games</title>
      <link>https://arxiv.org/abs/2504.09716</link>
      <description>arXiv:2504.09716v5 Announce Type: replace-cross 
Abstract: Dominance is a fundamental concept in game theory. In strategic-form games dominated strategies can be identified in polynomial time. As a consequence, iterative removal of dominated strategies can be performed efficiently as a preprocessing step for reducing the size of a game before computing a Nash equilibrium. For imperfect-information games in extensive form, we could convert the game to strategic form and then iteratively remove dominated strategies in the same way; however, this conversion may cause an exponential blowup in game size. In this paper we define and study the concept of dominated actions in imperfect-information games. Our main result is a polynomial-time algorithm for determining whether an action is dominated (strictly or weakly) by any mixed strategy in n-player games, which can be extended to an algorithm for iteratively removing dominated actions. This allows us to efficiently reduce the size of the game tree as a preprocessing step for Nash equilibrium computation. We explore the role of dominated actions empirically in the "All In or Fold" No-Limit Texas Hold'em poker variant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09716v5</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <category>econ.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sam Ganzfried</dc:creator>
    </item>
    <item>
      <title>Statistical Decision Theory with Counterfactual Loss</title>
      <link>https://arxiv.org/abs/2505.08908</link>
      <description>arXiv:2505.08908v2 Announce Type: replace-cross 
Abstract: Many researchers have applied classical statistical decision theory to evaluate treatment choices and learn optimal policies. However, because this framework is based solely on realized outcomes under chosen decisions and ignores counterfactual outcomes, it cannot assess the quality of a decision relative to feasible alternatives. For example, in bail decisions, a judge must consider not only crime prevention but also the avoidance of unnecessary burdens on arrestees. To address this limitation, we generalize standard decision theory by incorporating counterfactual losses, allowing decisions to be evaluated using all potential outcomes. The central challenge in this counterfactual statistical decision framework is identification: since only one potential outcome is observed for each unit, the associated counterfactual risk is generally not identifiable. We prove that, under the assumption of strong ignorability, the counterfactual risk is identifiable if and only if the counterfactual loss function is additive in the potential outcomes. Moreover, we demonstrate that additive counterfactual losses can yield treatment recommendations, which differ from those based on standard loss functions when the decision problem involves more than two treatment options. One interpretation of this result is that additive counterfactual losses can capture the accuracy and difficulty of a decision, whereas standard losses account for accuracy alone. Finally, we formulate a symbolic linear inverse program that, given a counterfactual loss, determines whether its risk is identifiable, without requiring data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08908v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>econ.TH</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Benedikt Koch, Kosuke Imai</dc:creator>
    </item>
  </channel>
</rss>
