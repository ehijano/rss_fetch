<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.RM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.RM</link>
    <description>q-fin.RM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.RM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 04 Jun 2024 04:15:55 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 04 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Project Risk Management from the bottom-up: Activity Risk Index</title>
      <link>https://arxiv.org/abs/2406.00078</link>
      <description>arXiv:2406.00078v1 Announce Type: new 
Abstract: Project managers need to manage risks throughout the project lifecycle and, thus, need to know how changes in activity durations influence project duration and risk. We propose a new indicator (the Activity Risk Index, ARI) that measures the contribution of each activity to the total project risk while it is underway. In particular, the indicator informs us about what activities contribute the most to the project's uncertainty so that project managers can pay closer attention to the performance of these activities. The main difference between our indicator and other activity sensitivity metrics in the literature (e.g. cruciality, criticality, significance, or schedule sensitivity indices) is that our indicator is based on the Schedule Risk Baseline concept instead of on cost or schedule baselines. The new metric not only provides information at the beginning of the project, but also while it is underway. Furthermore, the ARI is the only one to offer a normalized result: if we add its value for each activity, the total sum is 100%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00078v1</guid>
      <category>q-fin.RM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s10100-020-00703-8</arxiv:DOI>
      <arxiv:journal_reference>Cent Eur J Oper Res 29, 1375-1396 (2021)</arxiv:journal_reference>
      <dc:creator>Fernando Acebes, Javier Pajares, Jose M Gonzalez-Varona, Adolfo Lopez-Paredes</dc:creator>
    </item>
    <item>
      <title>On the project risk baseline: integrating aleatory uncertainty into project scheduling</title>
      <link>https://arxiv.org/abs/2406.00077</link>
      <description>arXiv:2406.00077v1 Announce Type: cross 
Abstract: Obtaining a viable schedule baseline that meets all project constraints is one of the main issues for project managers. The literature on this topic focuses mainly on methods to obtain schedules that meet resource restrictions and, more recently, financial limitations. The methods provide different viable schedules for the same project, and the solutions with the shortest duration are considered the best-known schedule for that project. However, no tools currently select which schedule best performs in project risk terms. To bridge this gap, this paper aims to propose a method for selecting the project schedule with the highest probability of meeting the deadline of several alternative schedules with the same duration. To do so, we propose integrating aleatory uncertainty into project scheduling by quantifying the risk of several execution alternatives for the same project. The proposed method, tested with a well-known repository for schedule benchmarking, can be applied to any project type to help managers to select the project schedules from several alternatives with the same duration, but the lowest risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00077v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>q-fin.RM</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.cie.2021.107537</arxiv:DOI>
      <arxiv:journal_reference>Computers &amp; Industrial Engineering, 160(2021), 107537. 2021</arxiv:journal_reference>
      <dc:creator>Fernando Acebes, David Poza, Jose M Gonzalez-Varona, Javier Pajares, Adolfo Lopez-Paredes</dc:creator>
    </item>
    <item>
      <title>Distributional Refinement Network: Distributional Forecasting via Deep Learning</title>
      <link>https://arxiv.org/abs/2406.00998</link>
      <description>arXiv:2406.00998v1 Announce Type: cross 
Abstract: A key task in actuarial modelling involves modelling the distributional properties of losses. Classic (distributional) regression approaches like Generalized Linear Models (GLMs; Nelder and Wedderburn, 1972) are commonly used, but challenges remain in developing models that can (i) allow covariates to flexibly impact different aspects of the conditional distribution, (ii) integrate developments in machine learning and AI to maximise the predictive power while considering (i), and, (iii) maintain a level of interpretability in the model to enhance trust in the model and its outputs, which is often compromised in efforts pursuing (i) and (ii). We tackle this problem by proposing a Distributional Refinement Network (DRN), which combines an inherently interpretable baseline model (such as GLMs) with a flexible neural network-a modified Deep Distribution Regression (DDR; Li et al., 2019) method. Inspired by the Combined Actuarial Neural Network (CANN; Schelldorfer and W{\''u}thrich, 2019), our approach flexibly refines the entire baseline distribution. As a result, the DRN captures varying effects of features across all quantiles, improving predictive performance while maintaining adequate interpretability. Using both synthetic and real-world data, we demonstrate the DRN's superior distributional forecasting capacity. The DRN has the potential to be a powerful distributional regression model in actuarial science and beyond.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00998v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>q-fin.RM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Avanzi, Eric Dong, Patrick J. Laub, Bernard Wong</dc:creator>
    </item>
    <item>
      <title>Ensemble distributional forecasting for insurance loss reserving</title>
      <link>https://arxiv.org/abs/2206.08541</link>
      <description>arXiv:2206.08541v5 Announce Type: replace-cross 
Abstract: Loss reserving generally focuses on identifying a single model that can generate superior predictive performance. However, different loss reserving models specialise in capturing different aspects of loss data. This is recognised in practice in the sense that results from different models are often considered, and sometimes combined. For instance, actuaries may take a weighted average of the prediction outcomes from various loss reserving models, often based on subjective assessments.
  In this paper, we propose a systematic framework to objectively combine (i.e. ensemble) multiple _stochastic_ loss reserving models such that the strengths offered by different models can be utilised effectively. Our framework contains two main innovations compared to existing literature and practice. Firstly, our criteria model combination considers the full distributional properties of the ensemble and not just the central estimate - which is of particular importance in the reserving context. Secondly, our framework is that it is tailored for the features inherent to reserving data. These include, for instance, accident, development, calendar, and claim maturity effects. Crucially, the relative importance and scarcity of data across accident periods renders the problem distinct from the traditional ensembling techniques in statistical learning.
  Our framework is illustrated with a complex synthetic dataset. In the results, the optimised ensemble outperforms both (i) traditional model selection strategies, and (ii) an equally weighted ensemble. In particular, the improvement occurs not only with central estimates but also relevant quantiles, such as the 75th percentile of reserves (typically of interest to both insurers and regulators). The framework developed in this paper can be implemented thanks to an R package, `ADLP`, which is available from CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.08541v5</guid>
      <category>stat.ME</category>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Avanzi, Yanfeng Li, Bernard Wong, Alan Xian</dc:creator>
    </item>
  </channel>
</rss>
