<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 25 Mar 2024 04:00:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 25 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Time Series Clustering Using DBSCAN</title>
      <link>https://arxiv.org/abs/2403.14798</link>
      <description>arXiv:2403.14798v1 Announce Type: new 
Abstract: Economic policy and research rely on the correct evaluation of the billions of high-frequency data points that we collect every day. Consistent clustering algorithms, like DBSCAN, allow us to make sense of the data in a useful way. However, while there is a large literature on the consistency of various clustering algorithms for high-dimensional static clustering, the literature on multivariate time series clustering still largely relies on heuristics or restrictive assumptions. The aim of this paper is to prove a notion of consistency of DBSCAN for the task of clustering multivariate time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14798v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicholas Waltz</dc:creator>
    </item>
    <item>
      <title>The German Tank Problem with Multiple Factories</title>
      <link>https://arxiv.org/abs/2403.14881</link>
      <description>arXiv:2403.14881v1 Announce Type: new 
Abstract: During the Second World War, estimates of the number of tanks deployed by Germany were critically needed. The Allies adopted two methods to estimate this information: espionage and statistical analysis. The latter approach was far more successful and is as follows: assuming that the tanks are sequentially numbered starting from 1, if we observe $k$ serial numbers from an unknown total of $N$ tanks, with the highest observed number being $M$, then the best linear unbiased estimator for $N$ is $M(1+1/k)-1$. This is now known as the German Tank Problem. Suppose one wishes to estimate the productivity of a rival by inspecting captured or destroyed tanks, each with a unique serial number. In many situations, the original German Tank Problem is insufficient, since typically there are $l&gt;1$ factories, and tanks produced by different factories may have serial numbers in disjoint ranges that are often far separated, let alone sequentially numbered starting from 1. We wish to estimate the total tank production across all of the factories. We construct an efficient procedure to estimate the total productivity and prove that our procedure effectively estimates $N$ when $\log l/\log k$ is sufficiently small, and is robust against both large and small gaps between factories. In the final section, we show that given information about the gaps, we can make a far better estimator that is also effective when we have a small number of samples. When the number of samples is small compared to the number of gaps, the Mean Squared Error of this new estimator is several orders of magnitude smaller than the one that assumes no information. This quantifies the importance of hiding such information if one wishes to conceal their productivity from a rival.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14881v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Steven J. Miller, Kishan Sharma, Andrew K. Yang</dc:creator>
    </item>
    <item>
      <title>Double Cross-fit Doubly Robust Estimators: Beyond Series Regression</title>
      <link>https://arxiv.org/abs/2403.15175</link>
      <description>arXiv:2403.15175v1 Announce Type: new 
Abstract: Doubly robust estimators with cross-fitting have gained popularity in causal inference due to their favorable structure-agnostic error guarantees. However, when additional structure, such as H\"{o}lder smoothness, is available then more accurate "double cross-fit doubly robust" (DCDR) estimators can be constructed by splitting the training data and undersmoothing nuisance function estimators on independent samples. We study a DCDR estimator of the Expected Conditional Covariance, a functional of interest in causal inference and conditional independence testing, and derive a series of increasingly powerful results with progressively stronger assumptions. We first provide a structure-agnostic error analysis for the DCDR estimator with no assumptions on the nuisance functions or their estimators. Then, assuming the nuisance functions are H\"{o}lder smooth, but without assuming knowledge of the true smoothness level or the covariate density, we establish that DCDR estimators with several linear smoothers are semiparametric efficient under minimal conditions and achieve fast convergence rates in the non-$\sqrt{n}$ regime. When the covariate density and smoothnesses are known, we propose a minimax rate-optimal DCDR estimator based on undersmoothed kernel regression. Moreover, we show an undersmoothed DCDR estimator satisfies a slower-than-$\sqrt{n}$ central limit theorem, and that inference is possible even in the non-$\sqrt{n}$ regime. Finally, we support our theoretical results with simulations, providing intuition for double cross-fitting and undersmoothing, demonstrating where our estimator achieves semiparametric efficiency while the usual "single cross-fit" estimator fails, and illustrating asymptotic normality for the undersmoothed DCDR estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15175v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alec McClean, Sivaraman Balakrishnan, Edward H. Kennedy, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>A Wasserstein perspective of Vanilla GANs</title>
      <link>https://arxiv.org/abs/2403.15312</link>
      <description>arXiv:2403.15312v1 Announce Type: new 
Abstract: The empirical success of Generative Adversarial Networks (GANs) caused an increasing interest in theoretical research. The statistical literature is mainly focused on Wasserstein GANs and generalizations thereof, which especially allow for good dimension reduction properties. Statistical results for Vanilla GANs, the original optimization problem, are still rather limited and require assumptions such as smooth activation functions and equal dimensions of the latent space and the ambient space. To bridge this gap, we draw a connection from Vanilla GANs to the Wasserstein distance. By doing so, existing results for Wasserstein GANs can be extended to Vanilla GANs. In particular, we obtain an oracle inequality for Vanilla GANs in Wasserstein distance. The assumptions of this oracle inequality are designed to be satisfied by network architectures commonly used in practice, such as feedforward ReLU networks. By providing a quantitative result for the approximation of a Lipschitz function by a feedforward ReLU network with bounded H\"older norm, we conclude a rate of convergence for Vanilla GANs as well as Wasserstein GANs as estimators of the unknown probability distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15312v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lea Kunkel, Mathias Trabs</dc:creator>
    </item>
    <item>
      <title>Active Learning for Regression based on Wasserstein distance and GroupSort Neural Networks</title>
      <link>https://arxiv.org/abs/2403.15108</link>
      <description>arXiv:2403.15108v1 Announce Type: cross 
Abstract: This paper addresses a new active learning strategy for regression problems. The presented Wasserstein active regression model is based on the principles of distribution-matching to measure the representativeness of the labeled dataset. The Wasserstein distance is computed using GroupSort Neural Networks. The use of such networks provides theoretical foundations giving a way to quantify errors with explicit bounds for their size and depth. This solution is combined with another uncertainty-based approach that is more outlier-tolerant to complete the query strategy. Finally, this method is compared with other classical and recent solutions. The study empirically shows the pertinence of such a representativity-uncertainty approach, which provides good estimation all along the query procedure. Moreover, the Wasserstein active regression often achieves more precise estimations and tends to improve accuracy faster than other models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15108v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Benjamin Bobbia, Matthias Picard</dc:creator>
    </item>
    <item>
      <title>Sharp adaptive and pathwise stable similarity testing for scalar ergodic diffusions</title>
      <link>https://arxiv.org/abs/2203.13776</link>
      <description>arXiv:2203.13776v2 Announce Type: replace 
Abstract: Within the nonparametric diffusion model, we develop a multiple test to infer about similarity of an unknown drift $b$ to some reference drift $b_0$: At prescribed significance, we simultaneously identify those regions where violation from similiarity occurs, without a priori knowledge of their number, size and location. This test is shown to be minimax-optimal and adaptive. At the same time, the procedure is robust under small deviation from Brownian motion as the driving noise process. A detailed investigation for fractional driving noise, which is neither a semimartingale nor a Markov process, is provided for Hurst indices close to the Brownian motion case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.13776v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Johannes Brutsche, Angelika Rohde</dc:creator>
    </item>
    <item>
      <title>Hidden Clique Inference in Random Ising Model II: the planted Sherrington-Kirkpatrick model</title>
      <link>https://arxiv.org/abs/2309.14192</link>
      <description>arXiv:2309.14192v2 Announce Type: replace 
Abstract: We study the problem of testing and recovering $k$-clique Ferromagnetic mean shift in the planted Sherrington-Kirkpatrick model (i.e., a type of spin glass model) with $n$ spins. The planted SK model -- a stylized mixture of an uncountable number of Ising models -- allows us to study the fundamental limits of correlation analysis for dependent random variables under misspecification. Our paper makes three major contributions: (i) We identify the phase diagrams of the testing problem by providing minimax optimal rates for multiple different parameter regimes. We also provide minimax optimal rates for exact recovery in the high/critical and low temperature regimes. (ii) We prove a universality result implying that all the obtained rates still hold with non-Gaussian couplings. (iii) To achieve the major results, we also establish a family of novel concentration bounds and central limiting theorems for the averaging statistics in the local and global phases of the planted SK model. These technical results shed new insights into the planted spin glass models. The pSK model also exhibits close connections with a binary variant of the single spike Gaussian sparse principle component analysis model by replacing the background identity precision matrix with a Wigner random matrix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.14192v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yihan He, Han Liu, Jianqing Fan</dc:creator>
    </item>
    <item>
      <title>Hidden Clique Inference in Random Ising Model I: the planted random field Curie-Weiss model</title>
      <link>https://arxiv.org/abs/2310.00667</link>
      <description>arXiv:2310.00667v2 Announce Type: replace 
Abstract: We study the problem of testing and recovering the hidden $k$-clique Ferromagnetic correlation in the planted Random Field Curie-Weiss model (a.k.a. the pRFCW model). The pRFCW model is a random effect Ising model that exhibits richer phase diagrams both statistically and physically than the standard Curie-Weiss model. Using an alternative characterization of parameter regimes as 'temperatures' and the mean values as 'outer magnetic fields,' we establish the minimax optimal detection rates and recovery rates. The results consist of $7$ distinctive phases for testing and $3$ phases for exact recovery. Our results also imply that the randomness of the outer magnetic field contributes to countable possible convergence rates, which are not observed in the fixed field model. As a byproduct of the proof techniques, we provide two new mathematical results: (1) A family of tail bounds for the average magnetization of the Random Field Curie-Weiss model (a.k.a. the RFCW model) across all temperatures and arbitrary outer fields. (2) A sharp estimate of the information divergence between RFCW models. These play pivotal roles in establishing the major theoretical results in this paper. Additionally, we show that the mathematical structure involved in the pRFCW hidden clique inference problem resembles a 'sparse PCA-like' problem for discrete data. The richer statistical phases than the long-studied Gaussian counterpart shed new light on the theoretical insight of sparse PCA for discrete data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.00667v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yihan He, Han Liu, Jianqing Fan</dc:creator>
    </item>
    <item>
      <title>The numeraire e-variable</title>
      <link>https://arxiv.org/abs/2402.18810</link>
      <description>arXiv:2402.18810v2 Announce Type: replace 
Abstract: We consider testing a composite null hypothesis $\mathcal{P}$ against a point alternative $\mathsf{Q}$. This paper establishes a powerful and general result: under no conditions whatsoever on $\mathcal{P}$ or $\mathsf{Q}$, there exists a special e-variable $X^*$ that we call the numeraire. It is strictly positive and for every $\mathsf{P} \in \mathcal{P}$, $\mathbb{E}_\mathsf{P}[X^*] \le 1$ (the e-variable property), while for every other e-variable $X$, we have $\mathbb{E}_\mathsf{Q}[X/X^*] \le 1$ (the numeraire property). In particular, this implies $\mathbb{E}_\mathsf{Q}[\log(X/X^*)] \le 0$ (log-optimality). $X^*$ also identifies a particular sub-probability measure $\mathsf{P}^*$ via the density $d \mathsf{P}^*/d \mathsf{Q} = 1/X^*$. As a result, $X^*$ can be seen as a generalized likelihood ratio of $\mathsf{Q}$ against $\mathcal{P}$. We show that $\mathsf{P}^*$ coincides with the reverse information projection (RIPr) when additional assumptions are made that are required for the latter to exist. Thus $\mathsf{P}^*$ is a natural definition of the RIPr in the absence of any assumptions on $\mathcal{P}$ or $\mathsf{Q}$. In addition to the abstract theory, we provide several tools for finding the numeraire in concrete cases. We discuss several nonparametric examples where we can indeed identify the numeraire, despite not having a reference measure. We end with a more general optimality theory that goes beyond the ubiquitous logarithmic utility. We focus on certain power utilities, leading to reverse R\'enyi projections in place of the RIPr, which also always exist.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.18810v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Larsson, Aaditya Ramdas, Johannes Ruf</dc:creator>
    </item>
    <item>
      <title>CARE: Large Precision Matrix Estimation for Compositional Data</title>
      <link>https://arxiv.org/abs/2309.06985</link>
      <description>arXiv:2309.06985v2 Announce Type: replace-cross 
Abstract: High-dimensional compositional data are prevalent in many applications. The simplex constraint poses intrinsic challenges to inferring the conditional dependence relationships among the components forming a composition, as encoded by a large precision matrix. We introduce a precise specification of the compositional precision matrix and relate it to its basis counterpart, which is shown to be asymptotically identifiable under suitable sparsity assumptions. By exploiting this connection, we propose a composition adaptive regularized estimation (CARE) method for estimating the sparse basis precision matrix. We derive rates of convergence for the estimator and provide theoretical guarantees on support recovery and data-driven parameter tuning. Our theory reveals an intriguing trade-off between identification and estimation, thereby highlighting the blessing of dimensionality in compositional data analysis. In particular, in sufficiently high dimensions, the CARE estimator achieves minimax optimality and performs as well as if the basis were observed. We further discuss how our framework can be extended to handle data containing zeros, including sampling zeros and structural zeros. The advantages of CARE over existing methods are illustrated by simulation studies and an application to inferring microbial ecological networks in the human gut.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.06985v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shucong Zhang, Huiyuan Wang, Wei Lin</dc:creator>
    </item>
    <item>
      <title>Statistical inference for a service system with non-stationary arrivals and unobserved balking</title>
      <link>https://arxiv.org/abs/2311.16884</link>
      <description>arXiv:2311.16884v2 Announce Type: replace-cross 
Abstract: We study a multi-server queueing system with a periodic arrival rate and customers whose joining decision is based on their patience and a delay proxy. Specifically, each customer has a patience level sampled from a common distribution. Upon arrival, they receive an estimate of their delay before joining service and then join the system only if this delay is not more than their patience, otherwise they balk. The main objective is to estimate the parameters pertaining to the arrival rate and patience distribution. Here the complication factor is that this inference should be performed based on the observed process only, i.e., balking customers remain unobserved. We set up a likelihood function of the state dependent effective arrival process (i.e., corresponding to the customers who join), establish strong consistency of the MLE, and derive the asymptotic distribution of the estimation error. Due to the intrinsic non-stationarity of the Poisson arrival process, the proof techniques used in previous work become inapplicable. The novelty of the proving mechanism in this paper lies in the procedure of constructing i.i.d. objects from dependent samples by decomposing the sample path into i.i.d. regeneration cycles. The feasibility of the MLE-approach is discussed via a sequence of numerical experiments, for multiple choices of functions which provide delay estimates. In particular, it is observed that the arrival rate is best estimated at high service capacities, and the patience distribution is best estimated at lower service capacities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.16884v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shreehari Anand Bodas, Michel Mandjes, Liron Ravner</dc:creator>
    </item>
    <item>
      <title>Statistical Agnostic Regression: a machine learning method to validate regression models</title>
      <link>https://arxiv.org/abs/2402.15213</link>
      <description>arXiv:2402.15213v2 Announce Type: replace-cross 
Abstract: Regression analysis is a central topic in statistical modeling, aiming to estimate the relationships between a dependent variable, commonly referred to as the response variable, and one or more independent variables, i.e., explanatory variables. Linear regression is by far the most popular method for performing this task in several fields of research, such as prediction, forecasting, or causal inference. Beyond various classical methods to solve linear regression problems, such as Ordinary Least Squares, Ridge, or Lasso regressions - which are often the foundation for more advanced machine learning (ML) techniques - the latter have been successfully applied in this scenario without a formal definition of statistical significance. At most, permutation or classical analyses based on empirical measures (e.g., residuals or accuracy) have been conducted to reflect the greater ability of ML estimations for detection. In this paper, we introduce a method, named Statistical Agnostic Regression (SAR), for evaluating the statistical significance of an ML-based linear regression based on concentration inequalities of the actual risk using the analysis of the worst case. To achieve this goal, similar to the classification problem, we define a threshold to establish that there is sufficient evidence with a probability of at least 1-eta to conclude that there is a linear relationship in the population between the explanatory (feature) and the response (label) variables. Simulations in only two dimensions demonstrate the ability of the proposed agnostic test to provide a similar analysis of variance given by the classical $F$ test for the slope parameter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.15213v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan M Gorriz, J. Ramirez, F. Segovia, F. J. Martinez-Murcia, C. Jim\'enez-Mesa, J. Suckling</dc:creator>
    </item>
  </channel>
</rss>
