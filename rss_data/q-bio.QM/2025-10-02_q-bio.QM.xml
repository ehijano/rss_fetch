<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.QM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.QM</link>
    <description>q-bio.QM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.QM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 03 Oct 2025 04:00:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>To Remember, To Adapt, To Preempt: A Stable Continual Test-Time Adaptation Framework for Remote Physiological Measurement in Dynamic Domain Shifts</title>
      <link>https://arxiv.org/abs/2510.01282</link>
      <description>arXiv:2510.01282v1 Announce Type: new 
Abstract: Remote photoplethysmography (rPPG) aims to extract non-contact physiological signals from facial videos and has shown great potential. However, existing rPPG approaches struggle to bridge the gap between source and target domains. Recent test-time adaptation (TTA) solutions typically optimize rPPG model for the incoming test videos using self-training loss under an unrealistic assumption that the target domain remains stationary. However, time-varying factors like weather and lighting in dynamic environments often cause continual domain shifts. The erroneous gradients accumulation from these shifts may corrupt the model's key parameters for physiological information, leading to catastrophic forgetting. Therefore, We propose a physiology-related parameters freezing strategy to retain such knowledge. It isolates physiology-related and domain-related parameters by assessing the model's uncertainty to current domain and freezes the physiology-related parameters during adaptation to prevent catastrophic forgetting. Moreover, the dynamic domain shifts with various non-physiological characteristics may lead to conflicting optimization objectives during TTA, which is manifested as the over-adapted model losing its adaptability to future domains. To fix over-adaptation, we propose a preemptive gradient modification strategy. It preemptively adapts to future domains and uses the acquired gradients to modify current adaptation, thereby preserving the model's adaptability. In summary, we propose a stable continual test-time adaptation (CTTA) framework for rPPG measurement, called \textbf{PhysRAP}, which \textbf{R}emembers the past, \textbf{A}dapts to the present, and \textbf{P}reempts the future. Extensive experiments show its state-of-the-art performance, especially in domain shifts. The code is available at https://github.com/xjtucsy/PhysRAP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01282v1</guid>
      <category>q-bio.QM</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuyang Chu, Jingang Shi, Xu Cheng, Haoyu Chen, Xin Liu, Jian Xu, Guoying Zhao</dc:creator>
    </item>
    <item>
      <title>Evaluating New AI Cell Foundation Models on Challenging Kidney Pathology Cases Unaddressed by Previous Foundation Models</title>
      <link>https://arxiv.org/abs/2510.01287</link>
      <description>arXiv:2510.01287v1 Announce Type: new 
Abstract: Accurate cell nuclei segmentation is critical for downstream tasks in kidney pathology and remains a major challenge due to the morphological diversity and imaging variability of renal tissues. While our prior work has evaluated early-generation AI cell foundation models in this domain, the effectiveness of recent cell foundation models remains unclear. In this study, we benchmark advanced AI cell foundation models (2025), including CellViT++ variants and Cellpose-SAM, against three widely used cell foundation models developed prior to 2024, using a diverse large-scale set of kidney image patches within a human-in-the-loop rating framework. We further performed fusion-based ensemble evaluation and model agreement analysis to assess the segmentation capabilities of the different models. Our results show that CellViT++ [Virchow] yields the highest standalone performance with 40.3% of predictions rated as "Good" on a curated set of 2,091 challenging samples, outperforming all prior models. In addition, our fused model achieves 62.2% "Good" predictions and only 0.4% "Bad", substantially reducing segmentation errors. Notably, the fusion model (2025) successfully resolved the majority of challenging cases that remained unaddressed in our previous study. These findings demonstrate the potential of AI cell foundation model development in renal pathology and provide a curated dataset of challenging samples to support future kidney-specific model refinement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01287v1</guid>
      <category>q-bio.QM</category>
      <category>cs.AI</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Runchen Wang, Junlin Guo, Siqi Lu, Ruining Deng, Zhengyi Lu, Yanfan Zhu, Yuechen Yang, Chongyu Qu, Yu Wang, Shilin Zhao, Catie Chang, Mitchell Wilkes, Mengmeng Yin, Haichun Yang, Yuankai Huo</dc:creator>
    </item>
    <item>
      <title>MorphGen: Controllable and Morphologically Plausible Generative Cell-Imaging</title>
      <link>https://arxiv.org/abs/2510.01298</link>
      <description>arXiv:2510.01298v1 Announce Type: new 
Abstract: Simulating in silico cellular responses to interventions is a promising direction to accelerate high-content image-based assays, critical for advancing drug discovery and gene editing. To support this, we introduce MorphGen, a state-of-the-art diffusion-based generative model for fluorescent microscopy that enables controllable generation across multiple cell types and perturbations. To capture biologically meaningful patterns consistent with known cellular morphologies, MorphGen is trained with an alignment loss to match its representations to the phenotypic embeddings of OpenPhenom, a state-of-the-art biological foundation model. Unlike prior approaches that compress multichannel stains into RGB images -- thus sacrificing organelle-specific detail -- MorphGen generates the complete set of fluorescent channels jointly, preserving per-organelle structures and enabling a fine-grained morphological analysis that is essential for biological interpretation. We demonstrate biological consistency with real images via CellProfiler features, and MorphGen attains an FID score over $35\%$ lower than the prior state-of-the-art MorphoDiff, which only generates RGB images for a single cell type. Code is available at https://github.com/czi-ai/MorphGen.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01298v1</guid>
      <category>q-bio.QM</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Berker Demirel, Marco Fumero, Theofanis Karaletsos, Francesco Locatello</dc:creator>
    </item>
    <item>
      <title>BioVERSE: Representation Alignment of Biomedical Modalities to LLMs for Multi-Modal Reasoning</title>
      <link>https://arxiv.org/abs/2510.01428</link>
      <description>arXiv:2510.01428v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) and biomedical foundation models (BioFMs) have achieved strong results in biological text reasoning, molecular modeling, and single-cell analysis, yet they remain siloed in disjoint embedding spaces, limiting cross-modal reasoning. We present BIOVERSE (Biomedical Vector Embedding Realignment for Semantic Engagement), a two-stage approach that adapts pretrained BioFMs as modality encoders and aligns them with LLMs through lightweight, modality-specific projection layers. The approach first aligns each modality to a shared LLM space through independently trained projections, allowing them to interoperate naturally, and then applies standard instruction tuning with multi-modal data to bring them together for downstream reasoning. By unifying raw biomedical data with knowledge embedded in LLMs, the approach enables zero-shot annotation, cross-modal question answering, and interactive, explainable dialogue. Across tasks spanning cell-type annotation, molecular description, and protein function reasoning, compact BIOVERSE configurations surpass larger LLM baselines while enabling richer, generative outputs than existing BioFMs, establishing a foundation for principled multi-modal biomedical reasoning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01428v1</guid>
      <category>q-bio.QM</category>
      <category>cs.AI</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ching-Huei Tsou, Michal Ozery-Flato, Ella Barkan, Diwakar Mahajan, Ben Shapira</dc:creator>
    </item>
    <item>
      <title>Pharmacophore-Guided Generative Design of Novel Drug-Like Molecules</title>
      <link>https://arxiv.org/abs/2510.01480</link>
      <description>arXiv:2510.01480v1 Announce Type: new 
Abstract: The integration of artificial intelligence (AI) in early-stage drug discovery offers unprecedented opportunities for exploring chemical space and accelerating hit-to-lead optimization. However, docking optimization in generative approaches is computationally expensive and may lead to inaccurate results. Here, we present a novel generative framework that balances pharmacophore similarity to reference compounds with structural diversity from active molecules. The framework allows users to provide custom reference sets, including FDA-approved drugs or clinical candidates, and guides the \textit{de novo} generation of potential therapeutics. We demonstrate its applicability through a case study targeting estrogen receptor modulators and antagonists for breast cancer. The generated compounds maintain high pharmacophoric fidelity to known active molecules while introducing substantial structural novelty, suggesting strong potential for functional innovation and patentability. Comprehensive evaluation of the generated molecules against common drug-like properties confirms the robustness and pharmaceutical relevance of the approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01480v1</guid>
      <category>q-bio.QM</category>
      <category>cs.AI</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ekaterina Podplutova, Anastasia Vepreva, Olga A. Konovalova, Vladimir Vinogradov, Dmitrii O. Shkil, Andrei Dmitrenko</dc:creator>
    </item>
    <item>
      <title>First passage times to T cell activation</title>
      <link>https://arxiv.org/abs/2510.01694</link>
      <description>arXiv:2510.01694v1 Announce Type: new 
Abstract: Effective recognition of foreign antigens by the adaptive immune system relies on T cells being activated by antigen-presenting cells (APCs) in lymph nodes. Here, diffusing T cells may encounter cognate APCs that present matching antigen fragments or non-cognate ones that do not; they are also subject to degradation. We develop a stochastic model in which T cell-APCs interact via a sequence of recognition steps, represented as a multistage Markov chain. T cells are successfully activated only if the terminal state associated with a cognate APC is reached. We compute the probability of successful activation in the presence of interfering non-cognate APCs, T cell degradation, and lymph node exit, and analyze the mean first-passage time to activation. We also incorporate a kinetic proofreading mechanism that enables state resetting, and show how this enhances specificity toward cognate APCs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01694v1</guid>
      <category>q-bio.QM</category>
      <category>q-bio.CB</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tony Wong, Ikchang Cho, Maria R. D'Orsogna, Tom Chou</dc:creator>
    </item>
    <item>
      <title>A Multicentric Dataset for Training and Benchmarking Breast Cancer Segmentation in H&amp;E Slides</title>
      <link>https://arxiv.org/abs/2510.02037</link>
      <description>arXiv:2510.02037v1 Announce Type: new 
Abstract: Automated semantic segmentation of whole-slide images (WSIs) stained with hematoxylin and eosin (H&amp;E) is essential for large-scale artificial intelligence-based biomarker analysis in breast cancer. However, existing public datasets for breast cancer segmentation lack the morphological diversity needed to support model generalizability and robust biomarker validation across heterogeneous patient cohorts. We introduce BrEast cancEr hisTopathoLogy sEgmentation (BEETLE), a dataset for multiclass semantic segmentation of H&amp;E-stained breast cancer WSIs. It consists of 587 biopsies and resections from three collaborating clinical centers and two public datasets, digitized using seven scanners, and covers all molecular subtypes and histological grades. Using diverse annotation strategies, we collected annotations across four classes - invasive epithelium, non-invasive epithelium, necrosis, and other - with particular focus on morphologies underrepresented in existing datasets, such as ductal carcinoma in situ and dispersed lobular tumor cells. The dataset's diversity and relevance to the rapidly growing field of automated biomarker quantification in breast cancer ensure its high potential for reuse. Finally, we provide a well-curated, multicentric external evaluation set to enable standardized benchmarking of breast cancer segmentation models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02037v1</guid>
      <category>q-bio.QM</category>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Carlijn Lems, Leslie Tessier, John-Melle Bokhorst, Mart van Rijthoven, Witali Aswolinskiy, Matteo Pozzi, Natalie Klubickova, Suzanne Dintzis, Michela Campora, Maschenka Balkenhol, Peter Bult, Joey Spronck, Thomas Detone, Mattia Barbareschi, Enrico Munari, Giuseppe Bogina, Jelle Wesseling, Esther H. Lips, Francesco Ciompi, Fr\'ed\'erique Meeuwsen, Jeroen van der Laak</dc:creator>
    </item>
    <item>
      <title>BioinfoMCP: A Unified Platform Enabling MCP Interfaces in Agentic Bioinformatics</title>
      <link>https://arxiv.org/abs/2510.02139</link>
      <description>arXiv:2510.02139v1 Announce Type: new 
Abstract: Bioinformatics tools are essential for complex computational biology tasks, yet their integration with emerging AI-agent frameworks is hindered by incompatible interfaces, heterogeneous input-output formats, and inconsistent parameter conventions. The Model Context Protocol (MCP) provides a standardized framework for tool-AI communication, but manually converting hundreds of existing and rapidly growing specialized bioinformatics tools into MCP-compliant servers is labor-intensive and unsustainable. Here, we present BioinfoMCP, a unified platform comprising two components: BioinfoMCP Converter, which automatically generates robust MCP servers from tool documentation using large language models, and BioinfoMCP Benchmark, which systematically validates the reliability and versatility of converted tools across diverse computational tasks. We present a platform of 38 MCP-converted bioinformatics tools, extensively validated to show that 94.7% successfully executed complex workflows across three widely used AI-agent platforms. By removing technical barriers to AI automation, BioinfoMCP enables natural-language interaction with sophisticated bioinformatics analyses without requiring extensive programming expertise, offering a scalable path to intelligent, interoperable computational biology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02139v1</guid>
      <category>q-bio.QM</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Florensia Widjaja, Zhangtianyi Chen, Juexiao Zhou</dc:creator>
    </item>
    <item>
      <title>Bayesian Re-Analysis of the Phylogenetic Topology of Early SARS-CoV-2 Case Sequences</title>
      <link>https://arxiv.org/abs/2510.01484</link>
      <description>arXiv:2510.01484v1 Announce Type: cross 
Abstract: A much-cited 2022 paper by Pekar et al. claimed that Bayesian analysis of the molecular phylogeny of early SARS-CoV-2 cases indicated that it was more likely that two successful introductions to humans had occurred than that just one had. Here I show that after correcting a fundamental error in Bayesian reasoning the results in that paper give larger likelihood for a single introduction than for two.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01484v1</guid>
      <category>q-bio.PE</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael B. Weissman</dc:creator>
    </item>
    <item>
      <title>Median2Median: Zero-shot Suppression of Structured Noise in Images</title>
      <link>https://arxiv.org/abs/2510.01666</link>
      <description>arXiv:2510.01666v1 Announce Type: cross 
Abstract: Image denoising is a fundamental problem in computer vision and medical imaging. However, real-world images are often degraded by structured noise with strong anisotropic correlations that existing methods struggle to remove. Most data-driven approaches rely on large datasets with high-quality labels and still suffer from limited generalizability, whereas existing zero-shot methods avoid this limitation but remain effective only for independent and identically distributed (i.i.d.) noise. To address this gap, we propose Median2Median (M2M), a zero-shot denoising framework designed for structured noise. M2M introduces a novel sampling strategy that generates pseudo-independent sub-image pairs from a single noisy input. This strategy leverages directional interpolation and generalized median filtering to adaptively exclude values distorted by structured artifacts. To further enlarge the effective sampling space and eliminate systematic bias, a randomized assignment strategy is employed, ensuring that the sampled sub-image pairs are suitable for Noise2Noise training. In our realistic simulation studies, M2M performs on par with state-of-the-art zero-shot methods under i.i.d. noise, while consistently outperforming them under correlated noise. These findings establish M2M as an efficient, data-free solution for structured noise suppression and mark the first step toward effective zero-shot denoising beyond the strict i.i.d. assumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01666v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>q-bio.QM</category>
      <category>stat.ML</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianxu Wang, Ge Wang</dc:creator>
    </item>
    <item>
      <title>A cautionary tale of model misspecification and identifiability</title>
      <link>https://arxiv.org/abs/2507.04894</link>
      <description>arXiv:2507.04894v2 Announce Type: replace-cross 
Abstract: Mathematical models are routinely applied to interpret biological data, with common goals that include both prediction and parameter estimation. A challenge in mathematical biology, in particular, is that models are often complex and non-identifiable, while data are limited. Rectifying identifiability through simplification can seemingly yield more precise parameter estimates, albeit, as we explore in this perspective, at the potentially catastrophic cost of introducing model misspecification and poor accuracy. We demonstrate how uncertainty in model structure can be propagated through to uncertainty in parameter estimates using a semi-parametric Gaussian process approach that delineates parameters of interest from uncertainty in model terms. Specifically, we study generalised logistic growth with an unknown crowding function, and a spatially resolved process described by a partial differential equation with a time-dependent diffusivity parameter. Allowing for structural model uncertainty yields more robust and accurate parameter estimates, and a better quantification of remaining uncertainty. We conclude our perspective by discussing the connections between identifiability and model misspecification, and alternative approaches to dealing with model misspecification in mathematical biology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04894v2</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander P Browning, Jennifer A Flegg, Ryan J Murphy</dc:creator>
    </item>
    <item>
      <title>Neural Diffusion Processes for Physically Interpretable Survival Prediction</title>
      <link>https://arxiv.org/abs/2510.00733</link>
      <description>arXiv:2510.00733v2 Announce Type: replace-cross 
Abstract: We introduce DeepFHT, a survival-analysis framework that couples deep neural networks with first hitting time (FHT) distributions from stochastic process theory. Time to event is represented as the first passage of a latent diffusion process to an absorbing boundary. A neural network maps input variables to physically meaningful parameters including initial condition, drift, and diffusion, within a chosen FHT process such as Brownian motion, both with drift and driftless. This yields closed-form survival and hazard functions and captures time-varying risk without assuming proportional-hazards.
  We compare DeepFHT with Cox survival model using synthetic and real-world datasets. The method achieves predictive accuracy on par with state-of-the-art approaches, while maintaining a physics-based interpretable parameterization that elucidates the relation between input features and risk. This combination of stochastic process theory and deep learning provides a principled avenue for modeling survival phenomena in complex systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00733v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessio Cristofoletto, Cesare Rollo, Giovanni Birolo, Piero Fariselli</dc:creator>
    </item>
  </channel>
</rss>
