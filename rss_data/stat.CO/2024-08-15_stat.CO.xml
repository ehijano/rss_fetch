<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 16 Aug 2024 04:00:38 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 16 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>$statcheck$ is flawed by design and no valid spell checker for statistical results</title>
      <link>https://arxiv.org/abs/2408.07948</link>
      <description>arXiv:2408.07948v1 Announce Type: new 
Abstract: The R package $statcheck$ is designed to extract statistical test results from text and check the consistency of the reported test statistics and corresponding p-values. Recently, it has also been featured as a spell checker for statistical results, aimed at improving reporting accuracy in scientific publications. In this study, I perform a check on $statcheck$ using a non-exhaustive list of 187 simple text strings with arbitrary statistical test results. These strings represent a wide range of textual representations of results including correctly manageable results, non-targeted test statistics, variable reporting styles, and common typos. Since $statcheck$'s detection heuristic is tied to a specific set of statistical test results that strictly adhere to the American Psychological Association (APA) reporting guidelines, it is unable to detect and check any reported result that even slightly deviates from this narrow style. In practice, $statcheck$ is unlikely to detect many statistical test results reported in the literature. I conclude that the capabilities and usefulness of the $statcheck$ software are very limited and that it should not be used to detect irregularities in results nor as a spell checker for statistical results. Future developments should aim to incorporate more flexible algorithms capable of handling a broader variety of reporting styles, such as those provided by $JATSdecoder$ and Large Language Models, which show promise in overcoming these limitations but they cannot replace the critical eye of a knowledgeable reader.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07948v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ingmar B\"oschen</dc:creator>
    </item>
    <item>
      <title>Incorporating Local Step-Size Adaptivity into the No-U-Turn Sampler using Gibbs Self Tuning</title>
      <link>https://arxiv.org/abs/2408.08259</link>
      <description>arXiv:2408.08259v1 Announce Type: cross 
Abstract: Adapting the step size locally in the no-U-turn sampler (NUTS) is challenging because the step-size and path-length tuning parameters are interdependent. The determination of an optimal path length requires a predefined step size, while the ideal step size must account for errors along the selected path. Ensuring reversibility further complicates this tuning problem. In this paper, we present a method for locally adapting the step size in NUTS that is an instance of the Gibbs self-tuning (GIST) framework. Our approach guarantees reversibility with an acceptance probability that depends exclusively on the conditional distribution of the step size. We validate our step-size-adaptive NUTS method on Neal's funnel density and a high-dimensional normal distribution, demonstrating its effectiveness in challenging scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08259v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nawaf Bou-Rabee, Bob Carpenter, Tore Selland Kleppe, Milo Marsden</dc:creator>
    </item>
    <item>
      <title>Efficiently resolving rotational ambiguity in Bayesian matrix sampling with matching</title>
      <link>https://arxiv.org/abs/2107.13783</link>
      <description>arXiv:2107.13783v2 Announce Type: replace 
Abstract: A wide class of Bayesian models involve unidentifiable random matrices that display rotational ambiguity, with the Gaussian factor model being a typical example. A rich variety of Markov chain Monte Carlo (MCMC) algorithms have been proposed for sampling the parameters of these models. However, without identifiability constraints, reliable posterior summaries of the parameters cannot be obtained directly from the MCMC output. As an alternative, we propose a computationally efficient post-processing algorithm that allows inference on non-identifiable parameters. We first orthogonalize the posterior samples using Varimax and then tackle label and sign switching with a greedy matching algorithm. We compare the performance and computational complexity with other methods using a simulation study and chemical exposures data. The algorithm implementation is available in the infinitefactor R package on CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2107.13783v2</guid>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Evan Poworoznek, Niccolo Anceschi, Federico Ferrari, David Dunson</dc:creator>
    </item>
    <item>
      <title>Wasserstein Gaussianization and Efficient Variational Bayes for Robust Bayesian Synthetic Likelihood</title>
      <link>https://arxiv.org/abs/2305.14746</link>
      <description>arXiv:2305.14746v2 Announce Type: replace 
Abstract: The Bayesian Synthetic Likelihood (BSL) method is a widely-used tool for likelihood-free Bayesian inference. This method assumes that some summary statistics are normally distributed, which can be incorrect in many applications. We propose a transformation, called the Wasserstein Gaussianization transformation, that uses a Wasserstein gradient flow to approximately transform the distribution of the summary statistics into a Gaussian distribution. BSL also implicitly requires compatibility between simulated summary statistics under the working model and the observed summary statistics. A robust BSL variant which achieves this has been developed in the recent literature. We combine the Wasserstein Gaussianization transformation with robust BSL, and an efficient Variational Bayes procedure for posterior approximation, to develop a highly efficient and reliable approximate Bayesian inference method for likelihood-free problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.14746v2</guid>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nhat-Minh Nguyen, Minh-Ngoc Tran, Christopher Drovandi, David Nott</dc:creator>
    </item>
  </channel>
</rss>
