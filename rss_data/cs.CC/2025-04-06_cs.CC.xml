<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 07 Apr 2025 04:00:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Lower Bound on Conservative Elementary Object Systems Coverability</title>
      <link>https://arxiv.org/abs/2504.03591</link>
      <description>arXiv:2504.03591v1 Announce Type: new 
Abstract: Elementary Object Systems (EOS) are a form of Petri Net (PN) where tokens carry internal PN. This model has been recently proposed for analysis of robustness of Multi Agent Systems. While EOS reachability is known to be undecidable, the decidability of coverability of its conservative fragment (where the type of internal PN cannot be completely deleted and, thus, is conserved) was proved a decade ago, no study charted its complexity. Here, we take a first step in this direction, by showing how to encode $\nu$PNs, a well studied form of PN enriched with data, into conservative EOS (cEOS). This yields a non-Primitive Recursive, $F_{\omega2}$ lower-bound on cEOS coverability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03591v1</guid>
      <category>cs.CC</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francesco Di Cosmo (Free University of Bozen-Bolzano), Soumodev Mal (Chennai Mathematical Institute), Tephilla Prince (IIT Dharwad)</dc:creator>
    </item>
    <item>
      <title>Vanishing of Schubert coefficients is in ${\sf AM}\cap {\sf coAM}$ assuming the GRH</title>
      <link>https://arxiv.org/abs/2504.03004</link>
      <description>arXiv:2504.03004v1 Announce Type: cross 
Abstract: The Schubert vanishing problem is a central decision problem in algebraic combinatorics and Schubert calculus, with applications to representation theory and enumerative algebraic geometry. The problem has been studied for over 50 years in different settings, with much progress given in the last two decades.
  We prove that the Schubert vanishing problem is in ${\sf AM}$ assuming the Generalized Riemann Hypothesis (GRH). This complements our earlier result in arXiv:2412.02064, that the problem is in ${\sf coAM}$ assuming the GRH. In particular, this implies that the Schubert vanishing problem is unlikely to be ${\sf coNP}$-hard, as we previously conjectured in arXiv:2412.02064.
  The proof is of independent interest as we formalize and expand the notion of a lifted formulation partly inspired by algebraic computations of Schubert problems, and extended formulations of linear programs. We use a result by Mahajan--Vinay to show that the determinant has a lifted formulation of polynomial size. We combine this with Purbhoo's algebraic criterion to derive the result.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03004v1</guid>
      <category>math.CO</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>math.AG</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Igor Pak, Colleen Robichaux</dc:creator>
    </item>
    <item>
      <title>On the consistency of stronger lower bounds for NEXP</title>
      <link>https://arxiv.org/abs/2504.03320</link>
      <description>arXiv:2504.03320v1 Announce Type: cross 
Abstract: It was recently shown by Atserias, Buss and Mueller that the standard complexity-theoretic conjecture NEXP not in P / poly is consistent with the relatively strong bounded arithmetic theory V^0_2, which can prove a substantial part of complexity theory. We observe that their approach can be extended to show that the stronger conjectures NEXP not in EXP / poly and NEXP not in coNEXP are consistent with a stronger theory, which includes every true universal number-sort sentence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03320v1</guid>
      <category>cs.LO</category>
      <category>cs.CC</category>
      <category>math.LO</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Neil Thapen</dc:creator>
    </item>
    <item>
      <title>Undefinability of Approximation of 2-to-2 Games</title>
      <link>https://arxiv.org/abs/2504.03523</link>
      <description>arXiv:2504.03523v1 Announce Type: cross 
Abstract: Recent work by Atserias and Dawar (J. Log. Comp 2019) and Tucker-Foltz (LMCS 2024) has established undefinability results in fixed-point logic with counting (FPC) corresponding to many classical complexity results from the hardness of approximation. In this line of work, NP-hardness results are turned into unconditional FPC undefinability results. We extend this work by showing the FPC undefinability of any constant factor approximation of weighted 2-to-2 games, based on the NP-hardness results of Khot, Minzer and Safra. Our result shows that the completely satisfiable 2-to-2 games are not FPC-separable from those that are not epsilon-satisfiable, for arbitrarily small epsilon. The perfect completeness of our inseparability is an improvement on the complexity result, as the NP-hardness of such a separation is still only conjectured. This perfect completeness enables us to show the FPC undefinability of other problems whose NP-hardness is conjectured. In particular, we are able to show that no FPC formula can separate the 3-colourable graphs from those that are not t-colourable, for any constant t.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03523v1</guid>
      <category>cs.LO</category>
      <category>cs.CC</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Anuj Dawar, B\'alint Moln\'ar</dc:creator>
    </item>
    <item>
      <title>Constant Rate Isometric Embeddings of Hamming Metric into Edit Metric</title>
      <link>https://arxiv.org/abs/2504.03605</link>
      <description>arXiv:2504.03605v1 Announce Type: cross 
Abstract: A function $\varphi: \{0,1\}^n \to \{0,1\}^N$ is called an isometric embedding of the $n$-dimensional Hamming metric space to the $N$-dimensional edit metric space if, for all $x, y \in \{0,1\}^n$, the Hamming distance between $x$ and $y$ is equal to the edit distance between $\varphi(x)$ and $\varphi(y)$. The rate of such an embedding is defined as the ratio $n/N$. It is well known in the literature how to construct isometric embeddings with a rate of $\Omega(\frac{1}{\log n})$. However, achieving even near-isometric embeddings with a positive constant rate has remained elusive until now.
  In this paper, we present an isometric embedding with a rate of 1/8 by discovering connections to synchronization strings, which were studied in the context of insertion-deletion codes (Haeupler-Shahrasbi [JACM'21]). At a technical level, we introduce a framework for obtaining high-rate isometric embeddings using a novel object called misaligners. As an immediate consequence of our constant rate isometric embedding, we improve known conditional lower bounds for various optimization problems in the edit metric, but now with optimal dependency on the dimension.
  We complement our results by showing that no isometric embedding $\varphi:\{0, 1\}^n \to \{0, 1\}^N$ can have rate greater than 15/32 for all positive integers $n$. En route to proving this upper bound, we uncover fundamental structural properties necessary for every Hamming-to-edit isometric embedding. We also prove similar upper and lower bounds for embeddings over larger alphabets.
  Finally, we consider embeddings $\varphi:\Sigma_{\text{in}}^n\to \Sigma_{\text{out}}^N$ between different input and output alphabets, where the rate is given by $\frac{n\log|\Sigma_{\text{in}}|}{N\log|\Sigma_{\text{out}}|}$. In this setting, we show that the rate can be made arbitrarily close to 1.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03605v1</guid>
      <category>cs.DM</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>math.CO</category>
      <category>math.IT</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sudatta Bhattacharya, Sanjana Dey, Elazar Goldenberg, Mursalin Habib, Bernhard Haeupler, Karthik C. S., Michal Kouck\'y</dc:creator>
    </item>
    <item>
      <title>Quantum Search with In-Place Queries</title>
      <link>https://arxiv.org/abs/2504.03620</link>
      <description>arXiv:2504.03620v1 Announce Type: cross 
Abstract: Quantum query complexity is typically characterized in terms of XOR queries |x,y&gt; to |x,y+f(x)&gt; or phase queries, which ensure that even queries to non-invertible functions are unitary. When querying a permutation, another natural model is unitary: in-place queries |x&gt; to |f(x)&gt;.
  Some problems are known to require exponentially fewer in-place queries than XOR queries, but no separation has been shown in the opposite direction. A candidate for such a separation was the problem of inverting a permutation over N elements. This task, equivalent to unstructured search in the context of permutations, is solvable with $O(\sqrt{N})$ XOR queries but was conjectured to require $\Omega(N)$ in-place queries.
  We refute this conjecture by designing a quantum algorithm for Permutation Inversion using $O(\sqrt{N})$ in-place queries. Our algorithm achieves the same speedup as Grover's algorithm despite the inability to efficiently uncompute queries or perform straightforward oracle-controlled reflections.
  Nonetheless, we show that there are indeed problems which require fewer XOR queries than in-place queries. We introduce a subspace-conversion problem called Function Erasure that requires 1 XOR query and $\Theta(\sqrt{N})$ in-place queries. Then, we build on a recent extension of the quantum adversary method to characterize exact conditions for a decision problem to exhibit such a separation, and we propose a candidate problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03620v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Blake Holman, Ronak Ramachandran, Justin Yirka</dc:creator>
    </item>
    <item>
      <title>Constructibility, computational complexity and P versus NP</title>
      <link>https://arxiv.org/abs/2406.16843</link>
      <description>arXiv:2406.16843v4 Announce Type: replace 
Abstract: A decision problem called the IPL problem is defined, and it is argued for the validity of an associated thesis called the IPL thesis. This thesis states that for some instances of the IPL problem, while an algorithm for verifying correct solutions to the problem in polynomial time is explicitly constructible, the IPL problem itself is algorithmically unsolvable in the sense that no explicitly constructed algorithm can be verified as solving the problem. Thus the IPL thesis implies that under a constructive interpretation of algorithmic complexity classes, which is arguably their only meaningful interpretation, NP is not contained in any complexity class consisting of algorithmically solvable problems. In particular, the thesis implies a solution to the P versus NP problem: P is not equal to NP. It also implies that NP is not contained in EXPTIME, seemingly contradicting a well known result. However, classical proofs of the opposite result for EXPTIME tacitly use an assumption concerning constructibility which, assuming the IPL thesis, does not hold for the IPL problem under a constructive interpretation of NP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16843v4</guid>
      <category>cs.CC</category>
      <category>cs.LO</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arne Hole</dc:creator>
    </item>
    <item>
      <title>Conditional Complexity Hardness: Monotone Circuit Size, Matrix Rigidity, and Tensor Rank Under NSETH and Beyond</title>
      <link>https://arxiv.org/abs/2411.02936</link>
      <description>arXiv:2411.02936v2 Announce Type: replace 
Abstract: Proving complexity lower bounds remains a challenging task: we only know how to prove conditional uniform lower bounds and nonuniform lower bounds in restricted circuit models. Williams (STOC 2010) showed how to derive nonuniform lower bounds from uniform upper bounds: by designing a fast algorithm for checking satisfiability of circuits, one gets a lower bound for this circuit class. Since then, a number of results of this kind have been proved. For example, Jahanjou et al. (ICALP 2015) and Carmosino et al. (ITCS 2016) proved that if NSETH fails, then $\text{E}^{\text{NP}}$ has series-parallel circuit size $\omega(n)$.
  One can also derive nonuniform lower bounds from nondeterministic uniform lower bounds. Recent examples include lower bounds on tensor rank, arithmetic circuit size, $\text{ETHR} \circ \text{ETHR}$ circuit size under assumptions that various problems (like TSP, MAX-3-SAT, SAT, Set Cover) cannot be solved faster than in $2^n$ time. In this paper, we continue developing this line of research and show how uniform nondeterministic lower bounds can be used to construct generators of various types of combinatorial objects: Boolean functions of high circuit size, matrices of high rigidity, and tensors of high rank. Specifically, we prove the following. If $k$-SAT cannot be solved in input-oblivious co-nondeterministic time $O(2^{(1/2+\varepsilon)n})$, then there exists a monotone Boolean function family in coNP of monotone circuit size $2^{\Omega(n / \log n)}$. This implies win-win circuit lower bounds: either $\text{E}^{\text{NP}}$ requires series-parallel circuits of size $\omega(n)$ or coNP requires monotone circuits of size $2^{\Omega(n / \log n)}$. If MAX-3-SAT cannot be solved in co-nondeterministic time $O(2^{(1 - \varepsilon)n})$, then there exist small families of matrices with high rigidity as well as small families of three-dimensional tensors of high rank.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02936v2</guid>
      <category>cs.CC</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikolai Chukhin, Alexander S. Kulikov, Ivan Mihajlin, Arina Smirnova</dc:creator>
    </item>
    <item>
      <title>A Characterization of Basic Feasible Functionals Through Higher-Order Rewriting and Tuple Interpretations</title>
      <link>https://arxiv.org/abs/2401.12385</link>
      <description>arXiv:2401.12385v4 Announce Type: replace-cross 
Abstract: The class of type-two basic feasible functionals ($\mathtt{BFF}_2$) is the analogue of $\mathtt{FP}$ (polynomial time functions) for type-2 functionals, that is, functionals that can take (first-order) functions as arguments. $\mathtt{BFF}_2$ can be defined through Oracle Turing machines with running time bounded by second-order polynomials. On the other hand, higher-order term rewriting provides an elegant formalism for expressing higher-order computation. We address the problem of characterizing $\mathtt{BFF}_2$ by higher-order term rewriting. Various kinds of interpretations for first-order term rewriting have been introduced in the literature for proving termination and characterizing first-order complexity classes. In this paper, we consider a recently introduced notion of cost-size interpretations for higher-order term rewriting and see second order rewriting as ways of computing type-2 functionals. We then prove that the class of functionals represented by higher-order terms admitting polynomially bounded cost-size interpretations exactly corresponds to $\mathtt{BFF}_2$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.12385v4</guid>
      <category>cs.LO</category>
      <category>cs.CC</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Patrick Baillot, Ugo Dal Lago, Cynthia Kop, Deivid Vale</dc:creator>
    </item>
    <item>
      <title>Structures preserved by primitive actions of $S_\omega$</title>
      <link>https://arxiv.org/abs/2501.03789</link>
      <description>arXiv:2501.03789v2 Announce Type: replace-cross 
Abstract: We present a dichotomy for structures $A$ that are preserved by primitive actions of $S_{\omega} = \text{Sym}({\mathbb N})$: either such a structure interprets all finite structures primitively positively, or it is of a very simple form and in particular has a binary polymorphism $f$ and an automorphism $\alpha$ satisfying $f(x,y) = \alpha(f(y,x))$. It is a consequence of our results that the constraint satisfaction problem for $A$ is in P or NP-complete.
  To prove our result, we study the first-order reducts of the Johnson graph $J(k)$, for $k \geq 2$, whose automorphism group $G$ equals the action of $S_{\omega}$ on the set $V$ of $k$-element subsets of $\mathbb N$. We use the fact that $J(k)$ has a finitely bounded homogeneous Ramsey expansion and that $G$ is a maximal closed subgroup of $\text{Sym}(V)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03789v2</guid>
      <category>math.LO</category>
      <category>cs.CC</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Manuel Bodirsky, Bertalan Bodor</dc:creator>
    </item>
  </channel>
</rss>
