<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 16 Aug 2024 01:25:49 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 15 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Local linear smoothing for regression surfaces on the simplex using Dirichlet kernels</title>
      <link>https://arxiv.org/abs/2408.07209</link>
      <description>arXiv:2408.07209v1 Announce Type: new 
Abstract: This paper introduces a local linear smoother for regression surfaces on the simplex. The estimator solves a least-squares regression problem weighted by a locally adaptive Dirichlet kernel, ensuring excellent boundary properties. Asymptotic results for the bias, variance, mean squared error, and mean integrated squared error are derived, generalizing the univariate results of Chen (2002). A simulation study shows that the proposed local linear estimator with Dirichlet kernel outperforms its only direct competitor in the literature, the Nadaraya-Watson estimator with Dirichlet kernel due to Bouzebda, Nezzal, and Elhattab (2024).</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07209v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christian Genest, Fr\'ed\'eric Ouimet</dc:creator>
    </item>
    <item>
      <title>Limit Theorems for Weakly Dependent Non-stationary Random Field Arrays and Asymptotic Inference of Dynamic Spatio-temporal Models</title>
      <link>https://arxiv.org/abs/2408.07429</link>
      <description>arXiv:2408.07429v1 Announce Type: new 
Abstract: We obtain the law of large numbers (LLN) and the central limit theorem (CLT) for weakly dependent non-stationary arrays of random fields with asymptotically unbounded moments. The weak dependence condition for arrays of random fields is proved to be inherited through transformation and infinite shift. This paves a way to prove the consistency and asymptotic normality of maximum likelihood estimation for dynamic spatio-temporal models (i.e. so-called ultra high-dimensional time series models) when the sample size and/or dimension go to infinity. Especially the asymptotic properties of estimation for network autoregression are obtained under reasonable regularity conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07429v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yue Pan, Jiazhu Pan</dc:creator>
    </item>
    <item>
      <title>Uniform Consistency of Generalized Fr\'echet Means</title>
      <link>https://arxiv.org/abs/2408.07534</link>
      <description>arXiv:2408.07534v1 Announce Type: new 
Abstract: We study a generalization of the Fr\'echet mean on metric spaces, which we call $\phi$-means. Our generalization is indexed by a convex function $\phi$. We find necessary and sufficient conditions for $\phi$-means to be finite and provide a tight bound for the diameter of the intrinsic mean set. We also provide sufficient conditions under which all the $\phi$-means coincide in a single point. Then, we prove the consistency of the sample $\phi$-mean to its population analogue. We also find conditions under which classes of $\phi$-means converge uniformly, providing a Glivenko-Cantelli result. Finally, we illustrate applications of our results and provide algorithms for the computation of $\phi$-means.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07534v1</guid>
      <category>math.ST</category>
      <category>math.MG</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Aveni, Sayan Mukherjee</dc:creator>
    </item>
    <item>
      <title>Posterior Covariance Structures in Gaussian Processes</title>
      <link>https://arxiv.org/abs/2408.07379</link>
      <description>arXiv:2408.07379v1 Announce Type: cross 
Abstract: In this paper, we present a comprehensive analysis of the posterior covariance field in Gaussian processes, with applications to the posterior covariance matrix. The analysis is based on the Gaussian prior covariance but the approach also applies to other covariance kernels. Our geometric analysis reveals how the Gaussian kernel's bandwidth parameter and the spatial distribution of the observations influence the posterior covariance as well as the corresponding covariance matrix, enabling straightforward identification of areas with high or low covariance in magnitude. Drawing inspiration from the a posteriori error estimation techniques in adaptive finite element methods, we also propose several estimators to efficiently measure the absolute posterior covariance field, which can be used for efficient covariance matrix approximation and preconditioning. We conduct a wide range of experiments to illustrate our theoretical findings and their practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07379v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Difeng Cai, Edmond Chow, Yuanzhe Xi</dc:creator>
    </item>
    <item>
      <title>A General Framework for Constraint-based Causal Learning</title>
      <link>https://arxiv.org/abs/2408.07575</link>
      <description>arXiv:2408.07575v1 Announce Type: cross 
Abstract: By representing any constraint-based causal learning algorithm via a placeholder property, we decompose the correctness condition into a part relating the distribution and the true causal graph, and a part that depends solely on the distribution. This provides a general framework to obtain correctness conditions for causal learning, and has the following implications. We provide exact correctness conditions for the PC algorithm, which are then related to correctness conditions of some other existing causal discovery algorithms. We show that the sparsest Markov representation condition is the weakest correctness condition resulting from existing notions of minimality for maximal ancestral graphs and directed acyclic graphs. We also reason that additional knowledge than just Pearl-minimality is necessary for causal learning beyond faithfulness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07575v1</guid>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Kai Z. Teh, Kayvan Sadeghi, Terry Soo</dc:creator>
    </item>
    <item>
      <title>Adversarially robust clustering with optimality guarantees</title>
      <link>https://arxiv.org/abs/2306.09977</link>
      <description>arXiv:2306.09977v2 Announce Type: replace 
Abstract: We consider the problem of clustering data points coming from sub-Gaussian mixtures. Existing methods that provably achieve the optimal mislabeling error, such as the Lloyd algorithm, are usually vulnerable to outliers. In contrast, clustering methods seemingly robust to adversarial perturbations are not known to satisfy the optimal statistical guarantees. We propose a simple robust algorithm based on the coordinatewise median that obtains the optimal mislabeling rate even when we allow adversarial outliers to be present. Our algorithm achieves the optimal error rate in constant iterations when a weak initialization condition is satisfied. In the absence of outliers, in fixed dimensions, our theoretical guarantees are similar to that of the Lloyd algorithm. Extensive experiments on various simulated and public datasets are conducted to support the theoretical guarantees of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.09977v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soham Jana, Kun Yang, Sanjeev Kulkarni</dc:creator>
    </item>
    <item>
      <title>Quantiles on global non-positive curvature spaces</title>
      <link>https://arxiv.org/abs/2312.10870</link>
      <description>arXiv:2312.10870v3 Announce Type: replace 
Abstract: This paper develops a notion of geometric quantiles on Hadamard spaces, also known as global non-positive curvature spaces. After providing some definitions and basic properties, including scaled isometry equivariance and a necessary condition on the gradient of the quantile loss function at quantiles on Hadamard manifolds, we investigate asymptotic properties of sample quantiles on Hadamard manifolds, such as strong consistency and joint asymptotic normality. We provide a detailed description of how to compute quantiles using a gradient descent algorithm in hyperbolic space and, in particular, an explicit formula for the gradient of the quantile loss function, along with experiments using simulated and real single-cell RNA sequencing data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.10870v3</guid>
      <category>math.ST</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ha-Young Shin, Hee-Seok Oh</dc:creator>
    </item>
    <item>
      <title>Learning Coefficients in Semi-Regular Models</title>
      <link>https://arxiv.org/abs/2406.02646</link>
      <description>arXiv:2406.02646v4 Announce Type: replace 
Abstract: Recent advances have clarified theoretical learning accuracy in Bayesian inference, revealing that the asymptotic behavior of metrics such as generalization loss and free energy, assessing predictive accuracy, is dictated by a rational number unique to each statistical model, termed the learning coefficient (real log canonical threshold) . For models meeting regularity conditions, their learning coefficients are known. However, for singular models not meeting these conditions, exact values of learning coefficients are provided for specific models like reduced-rank regression, but a broadly applicable calculation method for these learning coefficients in singular models remains elusive. The problem of determining learning coefficients relates to finding normal crossings of Kullback-Leibler divergence in algebraic geometry. In this context, it is crucial to perform appropriate coordinate transformations and blow-ups. This paper introduces an approach that utilizes properties of the log-likelihood ratio function for constructing specific variable transformations and blow-ups to uniformly calculate the real log canonical threshold. It was found that linear independence in the differential structure of the log-likelihood ratio function significantly influences the real log canonical threshold. This approach has not been considered in previous research. In this approach, the paper presents cases and methods for calculating the exact values of learning coefficients in statistical models that satisfy a simple condition next to the regularity conditions (semi-regular models), offering examples of learning coefficients for two-parameter semi-regular models and mixture distribution models with a constant mixing ratio.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02646v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuki Kurumadani</dc:creator>
    </item>
    <item>
      <title>A General Framework for Treatment Effect Estimation in Semi-Supervised and High Dimensional Settings</title>
      <link>https://arxiv.org/abs/2201.00468</link>
      <description>arXiv:2201.00468v3 Announce Type: replace-cross 
Abstract: In this article, we aim to provide a general and complete understanding of semi-supervised (SS) causal inference for treatment effects. Specifically, we consider two such estimands: (a) the average treatment effect and (b) the quantile treatment effect, as prototype cases, in an SS setting, characterized by two available data sets: (i) a labeled data set of size $n$, providing observations for a response and a set of high dimensional covariates, as well as a binary treatment indicator; and (ii) an unlabeled data set of size $N$, much larger than $n$, but without the response observed. Using these two data sets, we develop a family of SS estimators which are ensured to be: (1) more robust and (2) more efficient than their supervised counterparts based on the labeled data set only. Beyond the 'standard' double robustness results (in terms of consistency) that can be achieved by supervised methods as well, we further establish root-n consistency and asymptotic normality of our SS estimators whenever the propensity score in the model is correctly specified, without requiring specific forms of the nuisance functions involved. Such an improvement of robustness arises from the use of the massive unlabeled data, so it is generally not attainable in a purely supervised setting. In addition, our estimators are shown to be semi-parametrically efficient as long as all the nuisance functions are correctly specified. Moreover, as an illustration of the nuisance estimators, we consider inverse-probability-weighting type kernel smoothing estimators involving unknown covariate transformation mechanisms, and establish in high dimensional scenarios novel results on their uniform convergence rates, which should be of independent interest. Numerical results on both simulated and real data validate the advantage of our methods over their supervised counterparts with respect to both robustness and efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.00468v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abhishek Chakrabortty, Guorong Dai</dc:creator>
    </item>
    <item>
      <title>Semi-Supervised Quantile Estimation: Robust and Efficient Inference in High Dimensional Settings</title>
      <link>https://arxiv.org/abs/2201.10208</link>
      <description>arXiv:2201.10208v2 Announce Type: replace-cross 
Abstract: We consider quantile estimation in a semi-supervised setting, characterized by two available data sets: (i) a small or moderate sized labeled data set containing observations for a response and a set of possibly high dimensional covariates, and (ii) a much larger unlabeled data set where only the covariates are observed. We propose a family of semi-supervised estimators for the response quantile(s) based on the two data sets, to improve the estimation accuracy compared to the supervised estimator, i.e., the sample quantile from the labeled data. These estimators use a flexible imputation strategy applied to the estimating equation along with a debiasing step that allows for full robustness against misspecification of the imputation model. Further, a one-step update strategy is adopted to enable easy implementation of our method and handle the complexity from the non-linear nature of the quantile estimating equation. Under mild assumptions, our estimators are fully robust to the choice of the nuisance imputation model, in the sense of always maintaining root-n consistency and asymptotic normality, while having improved efficiency relative to the supervised estimator. They also attain semi-parametric optimality if the relation between the response and the covariates is correctly specified via the imputation model. As an illustration of estimating the nuisance imputation function, we consider kernel smoothing type estimators on lower dimensional and possibly estimated transformations of the high dimensional covariates, and we establish novel results on their uniform convergence rates in high dimensions, involving responses indexed by a function class and usage of dimension reduction techniques. These results may be of independent interest. Numerical results on both simulated and real data confirm our semi-supervised approach's improved performance, in terms of both estimation and inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.10208v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abhishek Chakrabortty, Guorong Dai, Raymond J. Carroll</dc:creator>
    </item>
  </channel>
</rss>
