<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cond-mat.dis-nn updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cond-mat.dis-nn</link>
    <description>cond-mat.dis-nn updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cond-mat.dis-nn" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 02 May 2024 04:01:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 02 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Scaling and renormalization in high-dimensional regression</title>
      <link>https://arxiv.org/abs/2405.00592</link>
      <description>arXiv:2405.00592v1 Announce Type: cross 
Abstract: This paper presents a succinct derivation of the training and generalization performance of a variety of high-dimensional ridge regression models using the basic tools of random matrix theory and free probability. We provide an introduction and review of recent results on these topics, aimed at readers with backgrounds in physics and deep learning. Analytic formulas for the training and generalization errors are obtained in a few lines of algebra directly from the properties of the $S$-transform of free probability. This allows for a straightforward identification of the sources of power-law scaling in model performance. We compute the generalization error of a broad class of random feature models. We find that in all models, the $S$-transform corresponds to the train-test generalization gap, and yields an analogue of the generalized-cross-validation estimator. Using these techniques, we derive fine-grained bias-variance decompositions for a very general class of random feature models with structured covariates. These novel results allow us to discover a scaling regime for random feature models where the variance due to the features limits performance in the overparameterized setting. We also demonstrate how anisotropic weight structure in random feature models can limit performance and lead to nontrivial exponents for finite-width corrections in the overparameterized setting. Our results extend and provide a unifying perspective on earlier models of neural scaling laws.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00592v1</guid>
      <category>stat.ML</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander B. Atanasov, Jacob A. Zavatone-Veth, Cengiz Pehlevan</dc:creator>
    </item>
    <item>
      <title>From Empirical Observations to Universality: Dynamics of Deep Learning with Inputs Built on Gaussian mixture</title>
      <link>https://arxiv.org/abs/2405.00642</link>
      <description>arXiv:2405.00642v1 Announce Type: cross 
Abstract: This study broadens the scope of theoretical frameworks in deep learning by delving into the dynamics of neural networks with inputs that demonstrate the structural characteristics to Gaussian Mixture (GM). We analyzed how the dynamics of neural networks under GM-structured inputs diverge from the predictions of conventional theories based on simple Gaussian structures. A revelation of our work is the observed convergence of neural network dynamics towards conventional theory even with standardized GM inputs, highlighting an unexpected universality. We found that standardization, especially in conjunction with certain nonlinear functions, plays a critical role in this phenomena. Consequently, despite the complex and varied nature of GM distributions, we demonstrate that neural networks exhibit asymptotic behaviors in line with predictions under simple Gaussian frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00642v1</guid>
      <category>stat.ML</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jaeyong Bae, Hawoong Jeong</dc:creator>
    </item>
    <item>
      <title>Quantum cryptographic protocols with dual messaging system via 2D alternate quantum walks and genuine single particle entangled states</title>
      <link>https://arxiv.org/abs/2405.00663</link>
      <description>arXiv:2405.00663v1 Announce Type: cross 
Abstract: Single-particle entangled states (SPES) can offer a more secure way of encoding and processing quantum information than their multi-particle counterparts. The SPES generated via a 2D alternate quantum-walk setup from initially separable states can be either 3-way or 2-way entangled. This letter shows that the generated genuine three-way and nonlocal two-way SPES can be used as cryptographic keys to securely encode two distinct messages simultaneously. We detail the message encryption-decryption steps and show the resilience of the 3-way and 2-way SPES-based cryptographic protocols against eavesdropper attacks like intercept-and-resend and man-in-the-middle. We also detail how these protocols can be experimentally realized using single photons, with the three degrees of freedom being OAM, path, and polarization. These have unparalleled security for quantum communication tasks. The ability to simultaneously encode two distinct messages using the generated SPES showcases the versatility and efficiency of the proposed cryptographic protocol. This capability could significantly improve the throughput of quantum communication systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00663v1</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.CR</category>
      <category>math.QA</category>
      <category>physics.optics</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dinesh Kumar Panda, Colin Benjamin</dc:creator>
    </item>
    <item>
      <title>Scalar susceptibility of a diluted classical XY model</title>
      <link>https://arxiv.org/abs/2311.07457</link>
      <description>arXiv:2311.07457v2 Announce Type: replace 
Abstract: We analyze the amplitude fluctuations in a diluted 3D classical XY model near the magnetic phase transition, motivated by the unusual localization properties of the amplitude (Higgs) mode recently found at the disordered superfluid-Mott glass quantum phase transition. We calculate the amplitude correlation function and the corresponding scalar susceptibility by means of Monte Carlo simulations. In contrast to the quantum case, in which the scalar susceptibility was found to violate naive scaling, we find that the scalar susceptibility of the classical system fulfills naive scaling (employing the clean critical exponents, as expected from the Harris criterion) as the temperature is varied across the phase transition for several dilutions. We discuss possible reasons for this discrepancy as well as the generality of our findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.07457v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>cond-mat.str-el</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevB.109.184202</arxiv:DOI>
      <arxiv:journal_reference>Phys. Rev. B 109, 184202 (2024)</arxiv:journal_reference>
      <dc:creator>Reece Beattie-Hauser, Thomas Vojta</dc:creator>
    </item>
    <item>
      <title>Fidelity and criticality in the nonreciprocal Aubry-Andr{\'e}-Harper model</title>
      <link>https://arxiv.org/abs/2404.16704</link>
      <description>arXiv:2404.16704v2 Announce Type: replace 
Abstract: We study the critical behaviors of the ground and first excited states in the one-dimensional nonreciprocal Aubry-Andr{\'e}-Harper model using both the self-normal and biorthogonal fidelity susceptibilities. We demonstrate that fidelity susceptibilities serve as a probe for the phase transition in the nonreciprocal AAH model. For ground states, characterized by real eigenenergies across the entire regime, both fidelity susceptibilities near the critical points scale as $N^{2}$, akin to the Hermitian AAH model. However, for the first-excited states, where $\mathcal{PT}$ transitions occur, the fidelity susceptibilities exhibit distinct scaling laws, contingent upon whether the lattice consists of even or odd sites. For even lattices, both the self-normal and and biorthogonal fidelity susceptibilities near the critical points continue to scale as $N^{2}$. In contrast, for odd lattices, the biorthogonal fidelity susceptibilities diverge, while the self-normal fidelity susceptibilities exhibit linear behavior, indicating a novel scaling law.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16704v2</guid>
      <category>cond-mat.dis-nn</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chen-Chang Zeng, Zhen Cai, Guang-Heng Wang, Gaoyong Sun</dc:creator>
    </item>
    <item>
      <title>Qubit dephasing by spectrally diffusing quantum two-level systems</title>
      <link>https://arxiv.org/abs/2306.15264</link>
      <description>arXiv:2306.15264v2 Announce Type: replace-cross 
Abstract: We investigate the pure dephasing of a Josephson qubit due to the spectral diffusion of two-level systems that are close to resonance with the qubit. We identify the parameter regime in which this pure dephasing rate can be of the order of the energy relaxation rate and, thus, the relation $T_2 = 2 T_1$ is violated for the qubit. This regime is reached if the dynamics of the thermal TLSs responsible for the spectral diffusion is sufficiently slower than the energy relaxation of the qubit. By adding periodic bias modulating the qubit frequency or TLS excitation energies we show that this pure dephasing mechanism can be mitigated, allowing enhancement of superconducting qubits coherence time. Mitigating pure dephasing, even if it is subdominant, is of special significance in view of recent suggestions for converting the dominant relaxation process ($T_1$) into erasure errors, leaving pure dephasing as the bottleneck for efficient quantum computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15264v2</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevApplied.21.044055</arxiv:DOI>
      <arxiv:journal_reference>Phys. Rev. Applied 21, 044055 (2024)</arxiv:journal_reference>
      <dc:creator>Shlomi Matityahu, Alexander Shnirman, Moshe Schechter</dc:creator>
    </item>
    <item>
      <title>Electronic Structure Prediction of Multi-million Atom Systems Through Uncertainty Quantification Enabled Transfer Learning</title>
      <link>https://arxiv.org/abs/2308.13096</link>
      <description>arXiv:2308.13096v3 Announce Type: replace-cross 
Abstract: The ground state electron density -- obtainable using Kohn-Sham Density Functional Theory (KS-DFT) simulations -- contains a wealth of material information, making its prediction via machine learning (ML) models attractive. However, the computational expense of KS-DFT scales cubically with system size which tends to stymie training data generation, making it difficult to develop quantifiably accurate ML models that are applicable across many scales and system configurations. Here, we address this fundamental challenge by employing transfer learning to leverage the multi-scale nature of the training data, while comprehensively sampling system configurations using thermalization. Our ML models are less reliant on heuristics, and being based on Bayesian neural networks, enable uncertainty quantification. We show that our models incur significantly lower data generation costs while allowing confident -- and when verifiable, accurate -- predictions for a wide variety of bulk systems well beyond training, including systems with defects, different alloy compositions, and at unprecedented, multi-million-atom scales. Moreover, such predictions can be carried out using only modest computational resources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.13096v3</guid>
      <category>cond-mat.mtrl-sci</category>
      <category>cond-mat.dis-nn</category>
      <category>physics.comp-ph</category>
      <category>quant-ph</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shashank Pathrudkar, Ponkrshnan Thiagarajan, Shivang Agarwal, Amartya S. Banerjee, Susanta Ghosh</dc:creator>
    </item>
  </channel>
</rss>
