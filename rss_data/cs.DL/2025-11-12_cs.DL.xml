<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DL</link>
    <description>cs.DL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 12 Nov 2025 05:01:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>crate2bib: Citing Rust crates made easy</title>
      <link>https://arxiv.org/abs/2511.07468</link>
      <description>arXiv:2511.07468v1 Announce Type: new 
Abstract: crate2bib is a collection of tools designed to convert Rust crates hosted on crates.io into bibliography entries. It queries the server, extracts metadata from the given crate and also searches for possible CITATION.cff files within the repository that hosts the code of the crate in interest. From this information, it formats the provided information such as name, version, authors and generates entries for all available candidates. With this approach, crates can be cited easily and existing citations for published crates can be found. The tool can be used as a webapp, python package, command-line utility or Rust crate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07468v1</guid>
      <category>cs.DL</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas Pleyer</dc:creator>
    </item>
    <item>
      <title>A national study of postdoctoral research fellows in South Africa</title>
      <link>https://arxiv.org/abs/2511.07474</link>
      <description>arXiv:2511.07474v1 Announce Type: new 
Abstract: This report provides the first comprehensive analysis of postdoctoral research fellows (postdocs) in South African public universities. It combines an analysis of existing data with the analysis of primary data collected in the form of a survey of institutions on the postdocs they host, a bibliometric study of the research output of postdocs, and an individual survey of postdocs. The number of postdocs has been increasing steadily from 2016 to 2022 and varies across universities, with larger research-intensive universities hosting more postdocs. In terms of demographics, the proportion of black African postdocs has increased; the proportion of female postdocs has remained lower than that of males; there is an increasing proportion of older postdocs; and more than 60 percent of postdocs are foreign-born. The bibliometric analysis of the publication output of postdocs shows that it increased substantially from 2005 to 2022. Some main results of the individual survey are that a postdoc position is taken primarily to enhance prospects for employment in a permanent academic position. However, securing such positions is reported as challenging, which is supported by results that one in every four postdocs has held multiple consecutive postdoc positions, and postdocs in general, but especially non-South Africans, perceive the job market as poor. Postdocs plan to leave South Africa primarily to seek better job opportunities, but also due to immigration rules or visa issues, which constitute major challenges for non-South Africans. Most postdocs desire to contribute to teaching and supervision but often lack the opportunity to do so. Dissatisfaction stems mostly from low levels of remuneration, difficulties created by the precarious nature of their positions and a lack of support for training and career development in their hosting institutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07474v1</guid>
      <category>cs.DL</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Heidi Prozesky, Francois van Schalkwyk, Johann Mouton</dc:creator>
    </item>
    <item>
      <title>Have we reached the beginning of the end for review papers?</title>
      <link>https://arxiv.org/abs/2511.07490</link>
      <description>arXiv:2511.07490v1 Announce Type: new 
Abstract: Review papers have traditionally enjoyed a high status in academic publishing because of the important role they can play in summarising and synthesising a field of research. They can also attract significantly more citations than primary research papers presenting original research, making them attractive to authors. There has been a dramatic increase in the publication of review papers in recent years, both in raw numbers and as a proportion of overall publication output. In this paper we demonstrate this increase across a wide range of fields of study. We quantify the citation dividend associated with review papers, but also demonstrate that it is declining and discuss the reasons for this decline. We further show that, since the arrival of GenAI tools in 2022 there is evidence of widespread use of GenAI in research paper writing, and we present evidence for a stronger AI signal among review papers compared to primary research papers. We suggest that the potential for GenAI to accelerate and even automate the production review papers will have a further significant impact on their status.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07490v1</guid>
      <category>cs.DL</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Barry Smyth, Padraig Cunningham</dc:creator>
    </item>
    <item>
      <title>Quantifying the Impact of CU: A Systematic Literature Review</title>
      <link>https://arxiv.org/abs/2511.07491</link>
      <description>arXiv:2511.07491v1 Announce Type: new 
Abstract: Community Unionism has served as a pivotal concept in debates on trade union renewal since the early 2000s, yet its theoretical coherence and political significance remain unresolved. This article investigates why CU has gained such prominence -- not by testing its efficacy, but by mapping how it is constructed, cited, and contested across the scholarly literature. Using two complementary systematic approaches -- a citation network analysis of 114 documents and a thematic review of 18 core CU case studies -- I examine how CU functions as both an empirical descriptor and a normative ideal. The analysis reveals CU's dual genealogy: positioned by British scholars as an indigenous return to historic rank-and-file practices, yet structurally aligned with transnational social movement unionism. Thematic coding shows near-universal emphasis on coalition-building and alliances, but deep ambivalence toward class politics. This tension suggests CU's significance lies less in operationalising a new union model, and more in managing contradictions -- between workplace and community, leadership and rank-and-file, reform and radicalism -- within a shrinking labour movement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07491v1</guid>
      <category>cs.DL</category>
      <category>cs.CL</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Compton</dc:creator>
    </item>
    <item>
      <title>CC30k: A Citation Contexts Dataset for Reproducibility-Oriented Sentiment Analysis</title>
      <link>https://arxiv.org/abs/2511.07790</link>
      <description>arXiv:2511.07790v1 Announce Type: new 
Abstract: Sentiments about the reproducibility of cited papers in downstream literature offer community perspectives and have shown as a promising signal of the actual reproducibility of published findings. To train effective models to effectively predict reproducibility-oriented sentiments and further systematically study their correlation with reproducibility, we introduce the CC30k dataset, comprising a total of 30,734 citation contexts in machine learning papers. Each citation context is labeled with one of three reproducibility-oriented sentiment labels: Positive, Negative, or Neutral, reflecting the cited paper's perceived reproducibility or replicability. Of these, 25,829 are labeled through crowdsourcing, supplemented with negatives generated through a controlled pipeline to counter the scarcity of negative labels. Unlike traditional sentiment analysis datasets, CC30k focuses on reproducibility-oriented sentiments, addressing a research gap in resources for computational reproducibility studies. The dataset was created through a pipeline that includes robust data cleansing, careful crowd selection, and thorough validation. The resulting dataset achieves a labeling accuracy of 94%. We then demonstrated that the performance of three large language models significantly improves on the reproducibility-oriented sentiment classification after fine-tuning using our dataset. The dataset lays the foundation for large-scale assessments of the reproducibility of machine learning papers. The CC30k dataset and the Jupyter notebooks used to produce and analyze the dataset are publicly available at https://github.com/lamps-lab/CC30k .</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07790v1</guid>
      <category>cs.DL</category>
      <category>cs.CL</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rochana R. Obadage, Sarah M. Rajtmajer, Jian Wu</dc:creator>
    </item>
  </channel>
</rss>
