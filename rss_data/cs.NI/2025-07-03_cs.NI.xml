<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 04 Jul 2025 04:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Comprehensive Survey on Network Traffic Synthesis: From Statistical Models to Deep Learning</title>
      <link>https://arxiv.org/abs/2507.01976</link>
      <description>arXiv:2507.01976v1 Announce Type: new 
Abstract: Synthetic network traffic generation has emerged as a promising alternative for various data-driven applications in the networking domain. It enables the creation of synthetic data that preserves real-world characteristics while addressing key challenges such as data scarcity, privacy concerns, and purity constraints associated with real data. In this survey, we provide a comprehensive review of synthetic network traffic generation approaches, covering essential aspects such as data types, generation models, and evaluation methods. With the rapid advancements in AI and machine learning, we focus particularly on deep learning-based techniques while also providing a detailed discussion of statistical methods and their extensions, including commercially available tools. Furthermore, we highlight open challenges in this domain and discuss potential future directions for further research and development. This survey serves as a foundational resource for researchers and practitioners, offering a structured analysis of existing methods, challenges, and opportunities in synthetic network traffic generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01976v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nirhoshan Sivaroopan, Kaushitha Silva, Chamara Madarasingha, Thilini Dahanayaka, Guillaume Jourjon, Anura Jayasumana, Kanchana Thilakarathna</dc:creator>
    </item>
    <item>
      <title>Scaling Out Chip Interconnect Networks with Implicit Sequence Numbers</title>
      <link>https://arxiv.org/abs/2507.01988</link>
      <description>arXiv:2507.01988v1 Announce Type: new 
Abstract: As AI models outpace the capabilities of single processors, interconnects across chips have become a critical enabler for scalable computing. These processors exchange massive amounts of data at cache-line granularity, prompting the adoption of new interconnect protocols like CXL, NVLink, and UALink, designed for high bandwidth and small payloads. However, the increasing transfer rates of these protocols heighten susceptibility to errors. While mechanisms like Cyclic Redundancy Check (CRC) and Forward Error Correction (FEC) are standard for reliable data transmission, scaling chip interconnects to multi-node configurations introduces new challenges, particularly in managing silently dropped flits in switching devices. This paper introduces Implicit Sequence Number (ISN), a novel mechanism that ensures precise flit drop detection and in-order delivery without adding header overhead. Additionally, we propose Reliability Extended Link (RXL), an extension of CXL that incorporates ISN to support scalable, reliable multi-node interconnects while maintaining compatibility with the existing flit structure. By elevating CRC to a transport-layer mechanism for end-to-end data and sequence integrity, and relying on FEC for link-layer error correction and detection, RXL delivers robust reliability and scalability without compromising bandwidth efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01988v1</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giyong Jung, Saeid Gorgin, John Kim, Jungrae Kim</dc:creator>
    </item>
    <item>
      <title>Curated Collaborative AI Edge with Network Data Analytics for B5G/6G Radio Access Networks</title>
      <link>https://arxiv.org/abs/2507.01994</link>
      <description>arXiv:2507.01994v1 Announce Type: new 
Abstract: Despite advancements, Radio Access Networks (RAN) still account for over 50\% of the total power consumption in 5G networks. Existing RAN split options do not fully harness data potential, presenting an opportunity to reduce operational expenditures. This paper addresses this opportunity through a twofold approach. First, highly accurate network traffic and user predictions are achieved using the proposed Curated Collaborative Learning (CCL) framework, which selectively collaborates with relevant correlated data for traffic forecasting. CCL optimally determines whom, when, and what to collaborate with, significantly outperforming state-of-the-art approaches, including global, federated, personalized federated, and cyclic institutional incremental learnings by 43.9%, 39.1%, 40.8%, and 31.35%, respectively. Second, the Distributed Unit Pooling Scheme (DUPS) is proposed, leveraging deep reinforcement learning and prediction inferences from CCL to reduce the number of active DU servers efficiently. DUPS dynamically redirects traffic from underutilized DU servers to optimize resource use, improving energy efficiency by up to 89% over conventional strategies, translating into substantial monetary benefits for operators. By integrating CCL-driven predictions with DUPS, this paper demonstrates a transformative approach for minimizing energy consumption and operational costs in 5G RANs, significantly enhancing efficiency and cost-effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01994v1</guid>
      <category>cs.NI</category>
      <category>cs.MA</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sardar Jaffar Ali, Syed M. Raza, Duc-Tai Le, Rajesh Challa, Min Young Chung, Ness Shroff, Hyunseung Choo</dc:creator>
    </item>
    <item>
      <title>Towards a Playground to Democratize Experimentation and Benchmarking of AI Agents for Network Troubleshooting</title>
      <link>https://arxiv.org/abs/2507.01997</link>
      <description>arXiv:2507.01997v1 Announce Type: new 
Abstract: Recent research has demonstrated the effectiveness of Artificial Intelligence (AI), and more specifically, Large Language Models (LLMs), in supporting network configuration synthesis and automating network diagnosis tasks, among others. In this preliminary work, we restrict our focus to the application of AI agents to network troubleshooting and elaborate on the need for a standardized, reproducible, and open benchmarking platform, where to build and evaluate AI agents with low operational effort.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01997v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhihao Wang, Alessandro Cornacchia, Franco Galante, Carlo Centofanti, Alessio Sacco, Dingde Jiang</dc:creator>
    </item>
    <item>
      <title>AI-Empowered Channel Generation for IoV Semantic Communications in Dynamic Conditions</title>
      <link>https://arxiv.org/abs/2507.02013</link>
      <description>arXiv:2507.02013v1 Announce Type: new 
Abstract: The Internet of Vehicles (IoV) transforms the transportation ecosystem promising pervasive connectivity and data-driven approaches. Deep learning and generative Artificial Intelligence (AI) have the potential to significantly enhance the operation of applications within IoV by facilitating efficient decision-making and predictive capabilities, including intelligent navigation, vehicle safety monitoring, accident prevention, and intelligent traffic management. Nevertheless, efficiently transmitting and processing the massive volumes of data generated by the IoV in real-time remains a significant challenge, particularly in dynamic and unpredictable wireless channel conditions. To address these challenges, this paper proposes a semantic communication framework based on channel perception to improve the accuracy and efficiency of data transmission. The semantic communication model extracts and compresses the information to be transmitted. In addition, the wireless channel is estimated by using a generative diffusion model, which is employed to predict the dynamic channel states, thereby improving the quality of IoV service. In dynamic scenarios, however, the channel estimation performance may be degraded when substantially new scenarios take place, which will adversely affect user experience. To mitigate this limitation, we employ a large model to fine-tune the channel generation model to enhance its adaptability for varying scenarios. The performance and reliability of the proposed framework are evaluated on the two public datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02013v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao Liu, Bo Yang, Zhiwen Yu, Xuelin Cao, George C. Alexandropoulos, Yan Zhang, Chau Yuen</dc:creator>
    </item>
    <item>
      <title>REDUS: Adaptive Resampling for Efficient Deep Learning in Centralized and Federated IoT Networks</title>
      <link>https://arxiv.org/abs/2507.02021</link>
      <description>arXiv:2507.02021v1 Announce Type: new 
Abstract: With the rise of Software-Defined Networking (SDN) for managing traffic and ensuring seamless operations across interconnected devices, challenges arise when SDN controllers share infrastructure with deep learning (DL) workloads. Resource contention between DL training and SDN operations, especially in latency-sensitive IoT environments, can degrade SDN's responsiveness and compromise network performance. Federated Learning (FL) helps address some of these concerns by decentralizing DL training to edge devices, thus reducing data transmission costs and enhancing privacy. Yet, the computational demands of DL training can still interfere with SDN's performance, especially under the continuous data streams characteristic of IoT systems. To mitigate this issue, we propose REDUS (Resampling for Efficient Data Utilization in Smart-Networks), a resampling technique that optimizes DL training by prioritizing misclassified samples and excluding redundant data, inspired by AdaBoost. REDUS reduces the number of training samples per epoch, thereby conserving computational resources, reducing energy consumption, and accelerating convergence without significantly impacting accuracy. Applied within an FL setup, REDUS enhances the efficiency of model training on resource-limited edge devices while maintaining network performance. In this paper, REDUS is evaluated on the CICIoT2023 dataset for IoT attack detection, showing a training time reduction of up to 72.6% with a minimal accuracy loss of only 1.62%, offering a scalable and practical solution for intelligent networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02021v1</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eyad Gad, Gad Gad, Mostafa M. Fouda, Mohamed I. Ibrahem, Muhammad Ismail, Zubair Md Fadlullah</dc:creator>
    </item>
    <item>
      <title>MULTI-SCOUT: Multistatic Integrated Sensing and Communications in 5G and Beyond for Moving Target Detection, Positioning, and Tracking</title>
      <link>https://arxiv.org/abs/2507.02613</link>
      <description>arXiv:2507.02613v1 Announce Type: new 
Abstract: This paper presents a complete signal-processing chain for multistatic integrated sensing and communications (ISAC) using 5G Positioning Reference Signal (PRS). We consider a distributed architecture in which one gNB transmits a periodic OFDM-PRS waveform while multiple spatially separated receivers exploit the same signal for target detection, parameter estimation and tracking. A coherent cross-ambiguity function (CAF) is evaluated to form a range-Doppler map from which the bistatic delay and radial velocity are extracted for every target. For a single target, the resulting bistatic delays are fused through nonlinear least-squares trilateration, yielding a geometric position estimate, and a regularized linear inversion of the radial-speed equations yields a two-dimensional velocity vector, where speed and heading are obtained. The approach is applied to 2D and 3D settings, extended to account for time synchronization bias, and generalized to multiple targets by resolving target association. The sequence of position-velocity estimates is then fed to standard and extended Kalman filters to obtain smoothed tracks. Our results show high-fidelity moving-target detection, positioning, and tracking using 5G PRS signals for multistatic ISAC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02613v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <category>eess.SP</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yalin E. Sagduyu, Kemal Davaslioglu, Tugba Erpek, Sastry Kompella, Gustave Anderson, Jonathan Ashdown</dc:creator>
    </item>
    <item>
      <title>On the Architectural Split and Radio Intelligence Controller Placement in Integrated O-RAN-enabled Non-Terrestrial Networks</title>
      <link>https://arxiv.org/abs/2507.02680</link>
      <description>arXiv:2507.02680v1 Announce Type: new 
Abstract: The integration of Terrestrial Networks (TNs) with Non-Terrestrial Networks (NTNs) poses unique architectural and functional challenges due to heterogeneous propagation conditions, dynamic topologies and limited on-board processing capabilities. This paper examines architectural and functional split strategies that are consistent with O-RAN principles for future integrated TN-NTN systems. A taxonomy of split options is proposed that distributes RAN and core functions between satellites and ground nodes, and trade-offs in terms of performance, latency, autonomy and deployment are analysed. In particular, we evaluate configurations ranging from pure on-board DU deployments to full gNB and UPF integration into satellites, including variations based on intra- and inter-satellite processing. In addition, the placement of Near-RT and Non-RT RAN Intelligent Controllers (RICs) is discussed, proposing flexible split strategies between space and ground to optimise the performance and scalability of the control loop. A comprehensive mapping between architectural splits and RIC placement options is provided, emphasising implementation constraints and interoperability considerations. The paper concludes by identifying key challenges and outlining future directions to enable standardised, modular and efficient TN-NTN convergence in the context of the O-RAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02680v1</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jorge Baranda, Marius Caus, Luis Blanco, Cristian J. Vaca-Rubio, Engin Zeydan, Kapal Dev, Zheng Li, Tomaso DeCola</dc:creator>
    </item>
    <item>
      <title>SAKURAONE: Empowering Transparent and Open AI Platforms through Private-Sector HPC Investment in Japan</title>
      <link>https://arxiv.org/abs/2507.02124</link>
      <description>arXiv:2507.02124v1 Announce Type: cross 
Abstract: SAKURAONE is a managed high performance computing (HPC) cluster developed and operated by the SAKURA Internet Research Center. It reinforces the ``KOKARYOKU PHY'' configuration of bare-metal GPU servers and is designed as a cluster computing resource optimized for advanced workloads, including large language model (LLM) training.
  In the ISC 2025 edition of the TOP500 list, SAKURAONE was ranked \textbf{49th} in the world based on its High Performance Linpack (HPL) score, demonstrating its global competitiveness. In particular, it is the \textbf{only system within the top 100} that employs a fully open networking stack based on \textbf{800~GbE (Gigabit Ethernet)} and the \textbf{SONiC (Software for Open Networking in the Cloud)} operating system, highlighting the viability of open and vendor-neutral technologies in large-scale HPC infrastructure.
  SAKURAONE achieved a sustained performance of 33.95~PFLOP/s on the HPL benchmark (Rmax), and 396.295~TFLOP/s on the High Performance Conjugate Gradient (HPCG) benchmark. For the HPL-MxP benchmark, which targets low-precision workloads representative of AI applications, SAKURAONE delivered an impressive 339.86~PFLOP/s using FP8 precision.
  The system comprises 100 compute nodes, each equipped with eight NVIDIA H100 GPUs. It is supported by an all-flash Lustre storage subsystem with a total physical capacity of 2~petabytes, providing high-throughput and low-latency data access. Internode communication is enabled by a full-bisection bandwidth interconnect based on a Rail-Optimized topology, where the Leaf and Spine layers are interconnected via 800~GbE links. This topology, in combination with RoCEv2 (RDMA over Converged Ethernet version 2), enables high-speed, lossless data transfers and mitigates communication bottlenecks in large-scale parallel workloads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02124v1</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fumikazu Konishi</dc:creator>
    </item>
    <item>
      <title>Seeing Through the Fog: Empowering Mobile Devices to Expose and Mitigate RAN Buffer Effects on Delay-Sensitive Protocols</title>
      <link>https://arxiv.org/abs/2507.00337</link>
      <description>arXiv:2507.00337v2 Announce Type: replace 
Abstract: Delay-based protocols rely on end-to-end delay measurements to detect network congestion. However, in cellular networks, Radio Access Network (RAN) buffers introduce significant delays unrelated to congestion, fundamentally challenging these protocols' assumptions. We identify two major types of RAN buffers - retransmission buffers and uplink scheduling buffers - that can introduce delays comparable to congestion-induced delays, severely degrading protocol performance. We present CellNinjia, a software-based system providing real-time visibility into RAN operations, and Gandalf, which leverages this visibility to systematically handle RAN-induced delays. Unlike existing approaches that treat these delays as random noise, Gandalf identifies specific RAN operations and compensates for their effects. Our evaluation in commercial 4G LTE and 5G networks shows that Gandalf enables substantial performance improvements - up to 7.49x for Copa and 9.53x for PCC Vivace - without modifying the protocols' core algorithms, demonstrating that delay-based protocols can realize their full potential in cellular networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00337v2</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxin Liu, Tianyang Zhang, Kyle Jamieson, Yaxiong Xie</dc:creator>
    </item>
    <item>
      <title>QUIC Delay Control: an implementation of congestion and delay control</title>
      <link>https://arxiv.org/abs/2507.00896</link>
      <description>arXiv:2507.00896v3 Announce Type: replace 
Abstract: A new congestion and delay control algorithm named QUIC Delay Control (QUIC-DC) is proposed for controlling not only congestion but also the queueing delay encountered along the forward communication path. The core idea is to estimate the one-way queueing delay of a connection to trigger an early reaction to congestion. This idea, along with a the TCP Westwood+ congestion control algorithm, has been implemented in QUIC-DC and compared with QUIC Cubic, BBRv2, NewReno, Westwood+. The results obtained in the emulated and real network environments show that QUIC-DC can significantly reduce packet losses along with end-to-end communication delays, while preserving network utilization, features that are both very useful for real-time applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00896v3</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Saverio Mascolo, Andrea Vittorio Balillo, Gioacchino Manfredi, Davide D'Agostino, Luca De Cicco</dc:creator>
    </item>
    <item>
      <title>GeMID: Generalizable Models for IoT Device Identification</title>
      <link>https://arxiv.org/abs/2411.14441</link>
      <description>arXiv:2411.14441v2 Announce Type: replace-cross 
Abstract: With the proliferation of devices on the Internet of Things (IoT), ensuring their security has become paramount. Device identification (DI), which distinguishes IoT devices based on their traffic patterns, plays a crucial role in both differentiating devices and identifying vulnerable ones, closing a serious security gap. However, existing approaches to DI that build machine learning models often overlook the challenge of model generalizability across diverse network environments. In this study, we propose a novel framework to address this limitation and to evaluate the generalizability of DI models across data sets collected within different network environments. Our approach involves a two-step process: first, we develop a feature and model selection method that is more robust to generalization issues by using a genetic algorithm with external feedback and datasets from distinct environments to refine the selections. Second, the resulting DI models are then tested on further independent datasets to robustly assess their generalizability. We demonstrate the effectiveness of our method by empirically comparing it to alternatives, highlighting how fundamental limitations of commonly employed techniques such as sliding window and flow statistics limit their generalizability. Moreover, we show that statistical methods, widely used in the literature, are unreliable for device identification due to their dependence on network-specific characteristics rather than device-intrinsic properties, challenging the validity of a significant portion of existing research. Our findings advance research in IoT security and device identification, offering insight into improving model effectiveness and mitigating risks in IoT networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14441v2</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kahraman Kostas, Rabia Yasa Kostas, Mike Just, Michael A. Lones</dc:creator>
    </item>
    <item>
      <title>UniNet: A Unified Multi-granular Traffic Modeling Framework for Network Security</title>
      <link>https://arxiv.org/abs/2503.04174</link>
      <description>arXiv:2503.04174v2 Announce Type: replace-cross 
Abstract: As modern networks grow increasingly complex--driven by diverse devices, encrypted protocols, and evolving threats--network traffic analysis has become critically important. Existing machine learning models often rely only on a single representation of packets or flows, limiting their ability to capture the contextual relationships essential for robust analysis. Furthermore, task-specific architectures for supervised, semi-supervised, and unsupervised learning lead to inefficiencies in adapting to varying data formats and security tasks. To address these gaps, we propose UniNet, a unified framework that introduces a novel multi-granular traffic representation (T-Matrix), integrating session, flow, and packet-level features to provide comprehensive contextual information. Combined with T-Attent, a lightweight attention-based model, UniNet efficiently learns latent embeddings for diverse security tasks. Extensive evaluations across four key network security and privacy problems--anomaly detection, attack classification, IoT device identification, and encrypted website fingerprinting--demonstrate UniNet's significant performance gain over state-of-the-art methods, achieving higher accuracy, lower false positive rates, and improved scalability. By addressing the limitations of single-level models and unifying traffic analysis paradigms, UniNet sets a new benchmark for modern network security.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04174v2</guid>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TCCN.2025.3585170</arxiv:DOI>
      <dc:creator>Binghui Wu, Dinil Mon Divakaran, Mohan Gurusamy</dc:creator>
    </item>
    <item>
      <title>Stealtooth: Breaking Bluetooth Security Abusing Silent Automatic Pairing</title>
      <link>https://arxiv.org/abs/2507.00847</link>
      <description>arXiv:2507.00847v2 Announce Type: replace-cross 
Abstract: Bluetooth is a pervasive wireless communication technology used by billions of devices for short-range connectivity. The security of Bluetooth relies on the pairing process, where devices establish shared long-term keys for secure communications. However, many commercial Bluetooth devices implement automatic pairing functions to improve user convenience, creating a previously unexplored attack surface.
  We present Stealtooth, a novel attack that abuses unknown vulnerabilities in the automatic pairing functions in commercial Bluetooth devices to achieve completely silent device link key overwriting. The Stealtooth attack leverages the fact that Bluetooth audio devices automatically transition to pairing mode under specific conditions, enabling attackers to hijack pairing processes without user awareness or specialized tools. We also extend the attack into the MitM Stealtooth attack, combining automatic pairing abuse with power-saving mode techniques to enable man-in-the-middle attacks.
  We evaluate the attacks against 10 commercial Bluetooth devices from major manufacturers, demonstrating widespread vulnerabilities across diverse device types and manufacturers. Our practical implementation requires only commodity hardware and open-source software, highlighting the low barrier to entry for attackers.
  We propose defenses both device and protocol levels, including enhanced user notifications and standardized automatic pairing guidelines. Our findings reveal a critical tension between security and usability, showing that current automatic pairing implementations create systematic vulnerabilities. We responsibly disclosed our findings to affected vendors, with several already releasing patches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00847v2</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keiichiro Kimura, Hiroki Kuzuno, Yoshiaki Shiraishi, Masakatu Morii</dc:creator>
    </item>
  </channel>
</rss>
