<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 15 May 2025 04:00:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Bounding Neyman-Pearson Region with $f$-Divergences</title>
      <link>https://arxiv.org/abs/2505.08899</link>
      <description>arXiv:2505.08899v1 Announce Type: new 
Abstract: The Neyman-Pearson region of a simple binary hypothesis testing is the set of points whose coordinates represent the false positive rate and false negative rate of some test. The lower boundary of this region is given by the Neyman-Pearson lemma, and is up to a coordinate change, equivalent to the optimal ROC curve. We establish a novel lower bound for the boundary in terms of any $f$-divergence. Since the bound generated by hockey-stick $f$-divergences characterizes the Neyman-Pearson boundary, this bound is best possible. In the case of KL divergence, this bound improves Pinsker's inequality. Furthermore, we obtain a closed-form refined upper bound for the Neyman-Pearson boundary in terms of the Chernoff $\alpha$-coefficient. Finally, we present methods for constructing pairs of distributions that can approximately or exactly realize any given Neyman-Pearson boundary.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08899v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Mullhaupt, Cheng Peng</dc:creator>
    </item>
    <item>
      <title>Statistical Decision Theory with Counterfactual Loss</title>
      <link>https://arxiv.org/abs/2505.08908</link>
      <description>arXiv:2505.08908v1 Announce Type: new 
Abstract: Classical statistical decision theory evaluates treatment choices based solely on observed outcomes. However, by ignoring counterfactual outcomes, it cannot assess the quality of decisions relative to feasible alternatives. For example, the quality of a physician's decision may depend not only on patient survival, but also on whether a less invasive treatment could have produced a similar result. To address this limitation, we extend standard decision theory to incorporate counterfactual losses--criteria that evaluate decisions using all potential outcomes. The central challenge in this generalization is identification: because only one potential outcome is observed for each unit, the associated risk under a counterfactual loss is generally not identifiable. We show that under the assumption of strong ignorability, a counterfactual risk is identifiable if and only if the counterfactual loss function is additive in the potential outcomes. Moreover, we demonstrate that additive counterfactual losses can yield treatment recommendations that differ from those based on standard loss functions, provided that the decision problem involves more than two treatment options.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08908v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>econ.TH</category>
      <category>stat.TH</category>
      <pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Benedikt Koch, Kosuke Imai</dc:creator>
    </item>
    <item>
      <title>Sequential Scoring Rule Evaluation for Forecast Method Selection</title>
      <link>https://arxiv.org/abs/2505.09090</link>
      <description>arXiv:2505.09090v1 Announce Type: new 
Abstract: This paper shows that sequential statistical analysis techniques can be generalised to the problem of selecting between alternative forecasting methods using scoring rules. A return to basic principles is necessary in order to show that ideas and concepts from sequential statistical methods can be adapted and applied to sequential scoring rule evaluation (SSRE). One key technical contribution of this paper is the development of a large deviations type result for SSRE schemes using a change of measure that parallels a traditional exponential tilting form. Further, we also show that SSRE will terminate in finite time with probability one, and that the moments of the SSRE stopping time exist. A second key contribution is to show that the exponential tilting form underlying our large deviations result allows us to cast SSRE within the framework of generalised e-values. Relying on this formulation, we devise sequential testing approaches that are both powerful and maintain control on error probabilities underlying the analysis. Through several simulated examples, we demonstrate that our e-values based SSRE approach delivers reliable results that are more powerful than more commonly applied testing methods precisely in the situations where these commonly applied methods can be expected to fail.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09090v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David T. Frazier, Donald S. Poskitt</dc:creator>
    </item>
    <item>
      <title>Nelson-Aalen kernel estimator to the tail index of right censored Pareto-type data</title>
      <link>https://arxiv.org/abs/2505.09152</link>
      <description>arXiv:2505.09152v1 Announce Type: new 
Abstract: On the basis of Nelson-Aalen product-limit estimator of a randomly censored distribution function, we introduce a kernel estimator to the tail index of right-censored Pareto-like data. Under some regularity assumptions, the consistency and asymptotic normality of the proposed estimator are established. A small simulation study shows that the proposed estimator performs much better, in terms of bias and stability, than the existing ones with, a slight increase in the mean squared error. The results are applied to insurance loss data to illustrate the practical effectiveness of our estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09152v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nour Elhouda Guesmia, Abdelhakim Necir, Djamel Meraghni</dc:creator>
    </item>
    <item>
      <title>Robust Representation and Estimation of Barycenters and Modes of Probability Measures on Metric Spaces</title>
      <link>https://arxiv.org/abs/2505.09609</link>
      <description>arXiv:2505.09609v1 Announce Type: new 
Abstract: This paper is concerned with the problem of defining and estimating statistics for distributions on spaces such as Riemannian manifolds and more general metric spaces. The challenge comes, in part, from the fact that statistics such as means and modes may be unstable: for example, a small perturbation to a distribution can lead to a large change in Fr\'echet means on spaces as simple as a circle. We address this issue by introducing a new merge tree representation of barycenters called the barycentric merge tree (BMT), which takes the form of a measured metric graph and summarizes features of the distribution in a multiscale manner. Modes are treated as special cases of barycenters through diffusion distances. In contrast to the properties of classical means and modes, we prove that BMTs are stable -- this is quantified as a Lipschitz estimate involving optimal transport metrics. This stability allows us to derive a consistency result for approximating BMTs from empirical measures, with explicit convergence rates. We also give a provably accurate method for discretely approximating the BMT construction and use this to provide numerical examples for distributions on spheres and shape spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09609v1</guid>
      <category>math.ST</category>
      <category>math.MG</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Washington Mio, Tom Needham</dc:creator>
    </item>
    <item>
      <title>Exploratory Hierarchical Factor Analysis with an Application to Psychological Measurement</title>
      <link>https://arxiv.org/abs/2505.09043</link>
      <description>arXiv:2505.09043v1 Announce Type: cross 
Abstract: Hierarchical factor models, which include the bifactor model as a special case, are useful in social and behavioural sciences for measuring hierarchically structured constructs. Specifying a hierarchical factor model involves imposing hierarchically structured zero constraints on a factor loading matrix, which is a demanding task that can result in misspecification. Therefore, an exploratory analysis is often needed to learn the hierarchical factor structure from data. Unfortunately, we lack an identifiability theory for the learnability of this hierarchical structure and a computationally efficient method with provable performance. The method of Schmid-Leiman transformation, which is often regarded as the default method for exploratory hierarchical factor analysis, is flawed and likely to fail. The contribution of this paper is three-fold. First, an identifiability result is established for general hierarchical factor models, which shows that the hierarchical factor structure is learnable under mild regularity conditions. Second, a computationally efficient divide-and-conquer approach is proposed for learning the hierarchical factor structure. This approach has two building blocks:(1) a constraint-based continuous optimisation algorithm and (2) a search algorithm based on an information criterion, that together explore the structure of factors nested within a given factor. Finally, asymptotic theory is established for the proposed method, showing that it can consistently recover the true hierarchical factor structure as the sample size grows to infinity. The power of the proposed method is shown via simulation studies and a real data application to a personality test. The computation code for the proposed method is publicly available at https://anonymous.4open.science/r/Exact-Exploratory-Hierarchical-Factor-Analysis-F850.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09043v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiawei Qiao, Yunxiao Chen, Zhiliang Ying</dc:creator>
    </item>
    <item>
      <title>Statistical Mean Estimation with Coded Relayed Observations</title>
      <link>https://arxiv.org/abs/2505.09098</link>
      <description>arXiv:2505.09098v1 Announce Type: cross 
Abstract: We consider a problem of statistical mean estimation in which the samples are not observed directly, but are instead observed by a relay (``teacher'') that transmits information through a memoryless channel to the decoder (``student''), who then produces the final estimate. We consider the minimax estimation error in the large deviations regime, and establish achievable error exponents that are tight in broad regimes of the estimation accuracy and channel quality. In contrast, two natural baseline methods are shown to yield strictly suboptimal error exponents. We initially focus on Bernoulli sources and binary symmetric channels, and then generalize to sub-Gaussian and heavy-tailed settings along with arbitrary discrete memoryless channels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09098v1</guid>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yan Hao Ling, Zhouhao Yang, Jonathan Scarlett</dc:creator>
    </item>
    <item>
      <title>Fairness-aware Bayes optimal functional classification</title>
      <link>https://arxiv.org/abs/2505.09471</link>
      <description>arXiv:2505.09471v1 Announce Type: cross 
Abstract: Algorithmic fairness has become a central topic in machine learning, and mitigating disparities across different subpopulations has emerged as a rapidly growing research area. In this paper, we systematically study the classification of functional data under fairness constraints, ensuring the disparity level of the classifier is controlled below a pre-specified threshold. We propose a unified framework for fairness-aware functional classification, tackling an infinite-dimensional functional space, addressing key challenges from the absence of density ratios and intractability of posterior probabilities, and discussing unique phenomena in functional classification. We further design a post-processing algorithm, Fair Functional Linear Discriminant Analysis classifier (Fair-FLDA), which targets at homoscedastic Gaussian processes and achieves fairness via group-wise thresholding. Under weak structural assumptions on eigenspace, theoretical guarantees on fairness and excess risk controls are established. As a byproduct, our results cover the excess risk control of the standard FLDA as a special case, which, to the best of our knowledge, is first time seen. Our theoretical findings are complemented by extensive numerical experiments on synthetic and real datasets, highlighting the practicality of our designed algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09471v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoyu Hu, Gengyu Xue, Zhenhua Lin, Yi Yu</dc:creator>
    </item>
    <item>
      <title>Adaptively-weighted Nearest Neighbors for Matrix Completion</title>
      <link>https://arxiv.org/abs/2505.09612</link>
      <description>arXiv:2505.09612v1 Announce Type: cross 
Abstract: In this technical note, we introduce and analyze AWNN: an adaptively weighted nearest neighbor method for performing matrix completion. Nearest neighbor (NN) methods are widely used in missing data problems across multiple disciplines such as in recommender systems and for performing counterfactual inference in panel data settings. Prior works have shown that in addition to being very intuitive and easy to implement, NN methods enjoy nice theoretical guarantees. However, the performance of majority of the NN methods rely on the appropriate choice of the radii and the weights assigned to each member in the nearest neighbor set and despite several works on nearest neighbor methods in the past two decades, there does not exist a systematic approach of choosing the radii and the weights without relying on methods like cross-validation. AWNN addresses this challenge by judiciously balancing the bias variance trade off inherent in weighted nearest-neighbor regression. We provide theoretical guarantees for the proposed method under minimal assumptions and support the theory via synthetic experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09612v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tathagata Sadhukhan, Manit Paul, Raaz Dwivedi</dc:creator>
    </item>
    <item>
      <title>Hypothesis testing on invariant subspaces of non-symmetric matrices with applications to network statistics</title>
      <link>https://arxiv.org/abs/2303.18233</link>
      <description>arXiv:2303.18233v3 Announce Type: replace 
Abstract: We extend the inference procedure for eigenvectors of Tyler (1981), which assumes symmetrizable matrices to generic invariant and singular subspaces of non-diagonalisable matrices to test whether $\nu \in \mathbb{R}^{p \times r}$ is an element of an invariant subspace of $M \in \mathbb{R}^{p \times p}$. Our results include a Wald test for full-vector hypotheses and a $t$-test for coefficient-wise hypotheses. We employ perturbation expansions of invariant subspaces from Sun (1991) and singular subspaces from Liu et al. (2007). Based on the former, we extend the popular Davis-Kahan bound to estimations of its higher-order polynomials and study how the bound simplifies for eigenspaces but attains complexity for generic invariant subspaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.18233v3</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>J\'er\^ome R. Simons</dc:creator>
    </item>
    <item>
      <title>Nonparametric estimation of the jump-size distribution for an M/G/1 storage system with Poisson sampling</title>
      <link>https://arxiv.org/abs/2307.10116</link>
      <description>arXiv:2307.10116v2 Announce Type: replace 
Abstract: This work presents a non-parametric estimator for the cumulative distribution function (CDF) of the jump-size distribution for a storage system with compound Poisson input. The workload process is observed according to an independent Poisson sampling process. The nonparametric estimator is constructed by first estimating the characteristic function (CF) and then applying an inversion formula. The convergence rate of the CF estimator at $s$ is shown to be of the order of $s^2/n$, where $n$ is the sample size. This convergence rate is leveraged to explore the bias-variance tradeoff of the inversion estimator. It is demonstrated that within a certain class of continuous distributions, the risk, in terms of MSE, is uniformly bounded by $C n^{-\frac{\eta}{1+\eta}}$, where $C$ is a positive constant and the parameter $\eta&gt;0$ depends on the smoothness of the underlying class of distributions. A heuristic method is further developed to address the case of an unknown rate of the compound Poisson input process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.10116v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liron Ravner</dc:creator>
    </item>
    <item>
      <title>Simple proof of robustness for Bayesian heavy-tailed linear regression models</title>
      <link>https://arxiv.org/abs/2501.06349</link>
      <description>arXiv:2501.06349v2 Announce Type: replace 
Abstract: In the Bayesian literature, a line of research called resolution of conflict is about the characterization of robustness against outliers of statistical models. The robustness characterization of a model is achieved by establishing the limiting behaviour of the posterior distribution under an asymptotic framework in which the outliers move away from the bulk of the data. The proofs of the robustness characterization results, especially the recent ones for regression models, are technical and not intuitive, limiting the accessibility of and preventing the development of theory in that line of research. In this paper, we highlight that the proof complexity is due to the generality of the assumptions on the prior distribution. To address the issue of accessibility, we present a significantly simpler proof for a linear regression model with a specific class of prior distributions, among which we find typically used prior distributions. The proof is intuitive and uses classical results of probability theory. To promote the development of theory in resolution of conflict, we highlight the key steps and present an application of the proof technique for a different model, allowing to understand how these key steps should be adapted. The generality of the assumption on the error distribution is also appealing; essentially, it can be any distribution with regularly varying or log-regularly varying tails. So far, there does not exist a result in such generality for models with regularly varying distributions. Finally, we analyse the necessity of the assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06349v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philippe Gagnon</dc:creator>
    </item>
    <item>
      <title>Proper scoring rules for estimation and forecast evaluation</title>
      <link>https://arxiv.org/abs/2504.01781</link>
      <description>arXiv:2504.01781v2 Announce Type: replace 
Abstract: Proper scoring rules have been a subject of growing interest in recent years, not only as tools for evaluation of probabilistic forecasts but also as methods for estimating probability distributions. In this article, we review the mathematical foundations of proper scoring rules including general characterization results and important families of scoring rules. We discuss their role in statistics and machine learning for estimation and forecast evaluation. Furthermore, we comment on interesting developments of their usage in applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01781v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kartik Waghmare, Johanna Ziegel</dc:creator>
    </item>
    <item>
      <title>Eigen-inference by Marchenko-Pastur inversion</title>
      <link>https://arxiv.org/abs/2504.03390</link>
      <description>arXiv:2504.03390v2 Announce Type: replace 
Abstract: A new formula for Marchenko-Pastur inversion is derived and used for inference of population linear spectral statistics. The formula allows for fast and accurate estimation of the Stieltjes transform of the population spectral distribution $s_H(z)$, when $z$ is sufficiently far from the support of the population spectral distribution $H$. If the dimension $d$ and the sample size $n$ go to infinity simultaneously such that $\frac{d}{n} \rightarrow c&gt;0$, the estimation error is shown to be asymptotically less than $\frac{n^{\varepsilon}}{n}$ for arbitrary $\varepsilon &gt; 0$. By integrating along a curve around the support of $H$, estimators for population linear spectral statistics are constructed, which benefit from this convergence speed of $\frac{n^{\varepsilon}}{n}$.
  The new method of estimating the Stieltjes transforms $s_H(z)$ is also applied to the numerical construction of estimators for the population eigenvalues, which in a simulation study are demonstrated to outperform state-of-the-art Ledoit-Wolf estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03390v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ben Deitmar</dc:creator>
    </item>
    <item>
      <title>Detection and inference of changes in high-dimensional linear regression with non-sparse structures</title>
      <link>https://arxiv.org/abs/2402.06915</link>
      <description>arXiv:2402.06915v5 Announce Type: replace-cross 
Abstract: For data segmentation in high-dimensional linear regression settings, the regression parameters are often assumed to be sparse segment-wise, which enables many existing methods to estimate the parameters locally via $\ell_1$-regularised maximum likelihood-type estimation and then contrast them for change point detection. Contrary to this common practice, we show that the exact sparsity of neither regression parameters nor their differences, a.k.a.\ differential parameters, is necessary for consistency in multiple change point detection. In fact, both statistically and computationally, better efficiency is attained by a simple strategy that scans for large discrepancies in local covariance between the regressors and the response. We go a step further and propose a suite of tools for directly inferring about the differential parameters post-segmentation, which are applicable even when the regression parameters themselves are non-sparse. Theoretical investigations are conducted under general conditions permitting non-Gaussianity, temporal dependence and ultra-high dimensionality. Numerical results from simulated and macroeconomic datasets demonstrate the competitiveness and efficacy of the proposed methods. Implementation of all methods is provided in the R package \texttt{inferchange} on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.06915v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haeran Cho, Tobias Kley, Housen Li</dc:creator>
    </item>
    <item>
      <title>Restricted maximum likelihood estimation in generalized linear mixed models</title>
      <link>https://arxiv.org/abs/2402.12719</link>
      <description>arXiv:2402.12719v2 Announce Type: replace-cross 
Abstract: Restricted maximum likelihood (REML) estimation is a widely accepted and frequently used method for fitting linear mixed models, with its principal advantage being that it produces less biased estimates of the variance components. However, the concept of REML does not immediately generalize to the setting of non-normally distributed responses, and it is not always clear the extent to which, either asymptotically or in finite samples, such generalizations reduce the bias of variance component estimates compared to standard unrestricted maximum likelihood estimation. In this article, we review various attempts that have been made over the past four decades to extend REML estimation in generalized linear mixed models. We establish four major classes of approaches, namely approximate linearization, integrated likelihood, modified profile likelihoods, and direct bias correction of the score function, and show that while these four classes may have differing motivations and derivations, they often arrive at a similar if not the same REML estimate. We compare the finite sample performance of these four classes, along with methods for REML estimation in hierarchical generalized linear models, through a numerical study involving binary and count data, with results demonstrating that all approaches perform similarly well reducing the finite sample size bias of variance components. Overall, we believe REML estimation should more widely adopted by practitioners using generalized linear mixed models, and that the exact choice of which REML approach to use should, at this point in time, be driven by software availability and ease of implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.12719v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luca Maestrini, Francis K. C. Hui, Alan H. Welsh</dc:creator>
    </item>
    <item>
      <title>Approximate D-optimal design and equilibrium measure</title>
      <link>https://arxiv.org/abs/2409.04058</link>
      <description>arXiv:2409.04058v3 Announce Type: replace-cross 
Abstract: We introduce a minor variant of the approximate D-optimal design of experiments with a more general information matrix that takes into account the representation of the design space S. The main motivation (and result) is that if S in R^d is the unit ball, the unit box or the canonical simplex, then remarkably, for every dimension d and every degree n, one obtains an optimal solution in closed form, namely the equilibrium measure of S (in pluripotential theory). Equivalently, for each degree n, the unique optimal solution is the vector of moments (up to degree 2n) of the equilibrium measure of S. Hence finding an optimal design reduces to finding a cubature for the equilibrium measure, with atoms in S, positive weights, and exact up to degree 2n. In addition, any resulting sequence of atomic D-optimal measures converges to the equilibrium measure of S for the weak-star topology, as n increases. Links with Fekete sets of points are also discussed. More general compact basic semi-algebraic sets are also considered, and a previously developed two-step design algorithm is easily adapted to this new variant of D-optimal design problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04058v3</guid>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Didier Henrion (LAAS-POP), Jean Bernard Lasserre (LAAS-POP, TSE-R)</dc:creator>
    </item>
  </channel>
</rss>
