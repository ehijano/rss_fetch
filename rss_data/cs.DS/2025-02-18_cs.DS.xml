<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Feb 2025 05:00:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Customizable Contraction Hierarchies -- A Survey</title>
      <link>https://arxiv.org/abs/2502.10519</link>
      <description>arXiv:2502.10519v1 Announce Type: new 
Abstract: This work establishes the technical fundamentals of a well-tuned Customizable Contraction Hierarchies (CCH) implementation that is simple and elegant. We give a detailed overview of the state of the art of CCH, review recent advances on CCH and show how to combine them. Additionally, we propose further refinements that improve the performance of CCH. An extensive evaluation confirms that a CCH framework is not only comprehensive in supported features but also competitive in performance to both Contraction Hierarchies (CH) and Customizable Route Planning (CRP).</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10519v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Thomas Bl\"asius, Valentin Buchhold, Dorothea Wagner, Tim Zeitz, Michael Z\"undorf</dc:creator>
    </item>
    <item>
      <title>Algorithms and Hardness for Estimating Statistical Similarity</title>
      <link>https://arxiv.org/abs/2502.10527</link>
      <description>arXiv:2502.10527v1 Announce Type: new 
Abstract: We study the problem of computing statistical similarity between probability distributions. For distributions $P$ and $Q$ over a finite sample space, their statistical similarity is defined as $S_{\mathrm{stat}}(P, Q) := \sum_{x} \min(P(x), Q(x))$. Statistical similarity is a basic measure of similarity between distributions, with several natural interpretations, and captures the Bayes error in prediction and hypothesis testing problems. Recent work has established that, somewhat surprisingly, even for the simple class of product distributions, exactly computing statistical similarity is $\#\mathsf{P}$-hard. This motivates the question of designing approximation algorithms for statistical similarity. Our primary contribution is a Fully Polynomial-Time deterministic Approximation Scheme (FPTAS) for estimating statistical similarity between two product distributions. To obtain this result, we introduce a new variant of the Knapsack problem, which we call the Masked Knapsack problem, and design an FPTAS to estimate the number of solutions of a multidimensional version of this problem. This new technical contribution could be of independent interest. Furthermore, we also establish a complementary hardness result. We show that it is $\mathsf{NP}$-hard to estimate statistical similarity when $P$ and $Q$ are Bayes net distributions of in-degree $2$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10527v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arnab Bhattacharyya, Sutanu Gayen, Kuldeep S. Meel, Dimitrios Myrisiotis, A. Pavan, N. V. Vinodchandran</dc:creator>
    </item>
    <item>
      <title>Local Gibbs sampling beyond local uniformity</title>
      <link>https://arxiv.org/abs/2502.10795</link>
      <description>arXiv:2502.10795v1 Announce Type: new 
Abstract: Local samplers are algorithms that generate random samples based on local queries to high-dimensional distributions, ensuring the samples follow the correct induced distributions while maintaining time complexity that scales locally with the query size. These samplers have broad applications, including deterministic approximate counting [He, Wang, Yin, SODA '23; Feng et al., FOCS '23], sampling from infinite or high-dimensional Gibbs distributions [Anand, Jerrum, SICOMP '22; He, Wang, Yin, FOCS '22], and providing local access to large random objects [Biswas, Rubinfield, Yodpinyanee, ITCS '20].
  In this work, we present a local sampler for Gibbs distributions of spin systems whose efficiency does not rely on the "local uniformity" property, which imposes unconditional marginal lower bounds -- a key assumption required by all prior local samplers. For fundamental models such as the Ising model, our algorithm achieves local efficiency in near-critical regimes, providing an exponential improvement over existing methods. Additionally, our approach is applicable to spin systems on graphs with unbounded degrees and supports dynamic sampling within the same near-critical regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10795v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongyang Liu, Chunyang Wang, Yitong Yin</dc:creator>
    </item>
    <item>
      <title>Light Edge Fault Tolerant Graph Spanners</title>
      <link>https://arxiv.org/abs/2502.10890</link>
      <description>arXiv:2502.10890v1 Announce Type: new 
Abstract: There has recently been significant interest in fault tolerant spanners, which are spanners that still maintain their stretch guarantees after some nodes or edges fail. This work has culminated in an almost complete understanding of the three-way tradeoff between stretch, sparsity, and number of faults tolerated. However, despite some progress in metric settings, there have been no results to date on the tradeoff in general graphs between stretch, lightness, and number of faults tolerated.
  We initiate the study of light edge fault tolerant (EFT) graph spanners, obtaining the first such results. First, we observe that lightness can be unbounded if we use the traditional definition (normalizing by the MST). We then argue that a natural definition of fault-tolerant lightness is to instead normalize by a min-weight fault tolerant connectivity preserver; essentially, a fault-tolerant version of the MST. However, even with this, we show that it is still not generally possible to construct $f$-EFT spanners whose weight compares reasonably to the weight of a min-weight $f$-EFT connectivity preserver.
  In light of this lower bound, it is natural to then consider bicriteria notions of lightness, where we compare the weight of an $f$-EFT spanner to a min-weight $(f' &gt; f)$-EFT connectivity preserver. The most interesting question is to determine the minimum value of $f'$ that allows for reasonable lightness upper bounds. Our main result is a precise answer to this question: $f' = 2f$. In particular, we show that the lightness can be untenably large (roughly $n/k$ for a $k$-spanner) if one normalizes by the min-weight $(2f-1)$-EFT connectivity preserver. But if one normalizes by the min-weight $2f$-EFT connectivity preserver, then we show that the lightness is bounded by just $O(f^{1/2})$ times the non-fault tolerant lightness (roughly $n^{1/k}$, for a $(1+\epsilon)(2k-1)$-spanner).</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10890v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Greg Bodwin, Michael Dinitz, Ama Koranteng, Lily Wang</dc:creator>
    </item>
    <item>
      <title>Exponential-Time Approximation (Schemes) for Vertex-Ordering Problems</title>
      <link>https://arxiv.org/abs/2502.10909</link>
      <description>arXiv:2502.10909v1 Announce Type: new 
Abstract: In this paper, we begin the exploration of vertex-ordering problems through the lens of exponential-time approximation algorithms. In particular, we ask the following question: Can we simultaneously beat the running times of the fastest known (exponential-time) exact algorithms and the best known approximation factors that can be achieved in polynomial time? Following the recent research initiated by Esmer et al. (ESA 2022, IPEC 2023, SODA 2024) on vertex-subset problems, and by Inamdar et al. (ITCS 2024) on graph-partitioning problems, we focus on vertex-ordering problems. In particular, we give positive results for Feedback Arc Set, Optimal Linear Arrangement, Cutwidth, and Pathwidth. Most of our algorithms build upon a novel ``balanced-cut'' approach, which is our main conceptual contribution. This allows us to solve various problems in very general settings allowing for directed and arc-weighted input graphs. Our main technical contribution is a (1+{\epsilon})-approximation for any {\epsilon} &gt; 0 for (weighted) Feedback Arc Set in O*((2-{\delta})^n) time, where {\delta} &gt; 0 is a constant only depending on {\epsilon}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10909v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthias Bentert, Fedor V. Fomin, Tanmay Inamdar, Saket Saurabh</dc:creator>
    </item>
    <item>
      <title>Probabilistic analysis of arithmetic coding showing its robustness</title>
      <link>https://arxiv.org/abs/2502.10935</link>
      <description>arXiv:2502.10935v1 Announce Type: new 
Abstract: We probabilistically analyze the performance of the arithmetic coding algorithm under a probability model for binary data in which a message is received by a coder from a source emitting independent equally distributed bits, with 1 occurring with probability $p\in(0,1)$ and 0 occurring with probability $1-p$.
  We establish a functional equation for the bivariate moment generating function for the two ends of the final interval delivered by the algorithm. Via the method of moments, we show that the transmitted message converges in distribution to the standard continuous uniform random variable on the interval [0,1]. It is remarkable that the limiting distribution is the same for all $p$, indicating robustness in the performance of arithmetic coding across an entire family of bit distributions. The nuance with $p$ appears only in the rate of convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10935v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hosam M. Mahmoud, Hans J. Rivertz</dc:creator>
    </item>
    <item>
      <title>The Bathroom Model: A Realistic Approach to Hash Table Algorithm Optimization</title>
      <link>https://arxiv.org/abs/2502.10977</link>
      <description>arXiv:2502.10977v1 Announce Type: new 
Abstract: Hash table search algorithms have been a fundamental research topic in computer science for decades. The widely accepted belief, originating from early theoretical work by Professor Yao, suggests that random probing is the optimal approach for open-addressing hash tables. However, a recent study by an undergraduate at the University of Cambridge challenges this notion, introducing an elastic search method with fixed interval thresholds. While this approach offers improvements over prior methods, we argue that its reliance on static threshold values limits its theoretical optimality.
  In this paper, we present the Bathroom Model, a novel approach to hash table search optimization inspired by real-world stall selection behavior. Unlike existing techniques, our method dynamically adjusts search strategies based on prior occupancy information, resulting in a more efficient probing mechanism. We formalize this model, analyze its theoretical performance, and compare it against state-of-the-art hash table search methods. Our results demonstrate that adaptive probing strategies significantly enhance lookup performance while maintaining low computational overhead. This research highlights the potential for fundamental algorithmic advancements in long-established domains and suggests new directions for optimizing hash table performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10977v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiantong Wang</dc:creator>
    </item>
    <item>
      <title>A linear-time algorithm computing the resident fitness in interacting trajectories</title>
      <link>https://arxiv.org/abs/2502.11561</link>
      <description>arXiv:2502.11561v1 Announce Type: new 
Abstract: The notion of a system of interacting trajectories was recently introduced by Hermann, Gonz\'alez Casanova, Soares dos Santos, T\'obi\'as and Wakolbinger. Such a system of $[0,1]$-valued piecewise linear trajectories arises as a scaling limit of the system of logarithmic subpopulation sizes in a certain population-genetic model (more precisely, a Moran model) with mutation and selection. By definition, the resident fitness is initially 0 and afterwards it increases by the ultimate slope of each trajectory that reaches height 1.
  We show that although the interaction of $n$ trajectories may yield $\Omega(n^2)$ slope changes in total, the resident fitness (at all times) can be computed algorithmically in $O(n)$ time. Our algorithm is given in terms of the so-called continued lines representation of the system of interacting trajectories. In the special case of Poissonian interacting trajectories where the birth times of the trajectories form a Poisson process and the initial slopes are random and i.i.d., we show that even the expected number of slope changes grows only linearly in time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11561v1</guid>
      <category>cs.DS</category>
      <category>math.PR</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Katalin Friedl, Vikt\'oria Nemkin, Andr\'as T\'obi\'as</dc:creator>
    </item>
    <item>
      <title>Cheesemap: A High-Performance Point-Indexing Data Structure for Neighbor~Search in LiDAR Data</title>
      <link>https://arxiv.org/abs/2502.11602</link>
      <description>arXiv:2502.11602v1 Announce Type: new 
Abstract: Point cloud data, as the representation of three-dimensional spatial information, is a fundamental piece of information in various domains where indexing and querying these point clouds efficiently is crucial for tasks such as object recognition, autonomous navigation, and environmental modeling. In this paper, we present a comprehensive comparative analysis of various data structures combined with neighboring search methods across different types of point clouds. Additionally, we introduce a novel data structure, cheesemap, to handle 3D LiDAR point clouds. Exploring the sparsity and irregularity in the distribution of points, there are three flavors of the cheesemap: dense, sparse, and mixed. Results show that the cheesemap can outperform state-of-the-art data structures in terms of execution time per query, particularly for ALS (Aerial Laser Scanning) point clouds. Memory consumption is also minimal, especially in the sparse and mixed representations, making the cheesemap a suitable choice for applications involving three-dimensional point clouds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11602v1</guid>
      <category>cs.DS</category>
      <category>cs.PF</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruben Laso, Miguel Yermo</dc:creator>
    </item>
    <item>
      <title>On the Locality of the Lov\'asz Local Lemma</title>
      <link>https://arxiv.org/abs/2502.11690</link>
      <description>arXiv:2502.11690v1 Announce Type: new 
Abstract: The Lov\'asz Local Lemma is a versatile result in probability theory, characterizing circumstances in which a collection of $n$ `bad events', each occurring with probability at most $p$ and dependent on a set of underlying random variables, can be avoided. It is a central tool of the probabilistic method, since it can be used to show that combinatorial objects satisfying some desirable properties must exist. While the original proof was existential, subsequent work has shown algorithms for the Lov\'asz Local Lemma: that is, in circumstances in which the lemma proves the existence of some object, these algorithms can constructively find such an object. One main strand of these algorithms, which began with Moser and Tardos's well-known result (JACM 2010), involves iteratively resampling the dependent variables of satisfied bad events until none remain satisfied.
  In this paper, we present a novel analysis that can be applied to resampling-style Lov\'asz Local Lemma algorithms. This analysis shows that an output assignment for the dependent variables of most events can be determined only from $O(\log \log_{1/p} n)$-radius local neighborhoods, and that the events whose variables may still require resampling can be identified from these neighborhoods. This allows us to improve randomized complexities for the constructive Lov\'asz Local Lemma (with polynomial criterion) in several parallel and distributed models. In particular, we obtain:
  1) A LOCAL algorithm with $O(\log\log_{1/p} n)$ node-averaged complexity (while matching the $O(\log_{1/p} n)$ worst-case complexity of Chung, Pettie, and Su).
  2) An algorithm for the LCA and VOLUME models requiring $d^{O(\log\log_{1/p} n)}$ probes per query.
  3) An $O(\log\log\log_{1/p} n)$-round algorithm for CONGESTED CLIQUE, linear space MPC, and Heterogenous MPC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11690v1</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <category>math.PR</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3717823.3718103</arxiv:DOI>
      <dc:creator>Peter Davies-Peck</dc:creator>
    </item>
    <item>
      <title>Parameterised algorithms for temporal reconfiguration problems</title>
      <link>https://arxiv.org/abs/2502.11961</link>
      <description>arXiv:2502.11961v1 Announce Type: new 
Abstract: Given a static vertex-selection problem (e.g. independent set, dominating set) on a graph, we can define a corresponding temporal reconfiguration problem on a temporal graph which asks for a sequence of solutions to the vertex-selection problem at each time such that we can reconfigure from one solution to the next. We can think of each solution in the sequence as a set of vertices with tokens placed on them; our reconfiguration model allows us to slide tokens along active edges of a temporal graph.
  We show that it is possible to efficiently check whether one solution can be reconfigured to another, and show that approximation results on the static vertex-selection problem can be adapted with a lifetime factor to the reconfiguration version. Our main contributions are fixed-parameter tractable algorithms with respect to: enumeration time of the related static problem; the combination of temporal neighbourhood diversity and lifetime of the input graph; and the combination of lifetime and treewidth of the footprint graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11961v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tom Davot, Jessica Enright, Laura Larios-Jones</dc:creator>
    </item>
    <item>
      <title>Logarithmic Approximation for Road Pricing on Grids</title>
      <link>https://arxiv.org/abs/2502.11979</link>
      <description>arXiv:2502.11979v1 Announce Type: new 
Abstract: Consider a graph $G = (V, E)$ and some commuters, each specified by a tuple $(u, v, b)$ consisting of two nodes in the graph $u, v \in V$ and a non-negative real number $b$, specifying their budget. The goal is to find a pricing function $p$ of the edges of $G$ that maximizes the revenue generated by the commuters. Here, each commuter $(u, v, b)$ either pays the lowest-cost of a $u$-$v$ path under the pricing $p$, or 0, if this exceeds their budget $b$. We study this problem for the case where $G$ is a bounded-width grid graph and give a polynomial-time approximation algorithm with approximation ratio $O(\log |E|)$. Our approach combines existing ideas with new insights. Most notably, we employ a rather seldom-encountered technique that we coin under the name 'assume-implement dynamic programming.' This technique involves dynamic programming where some information about the future decisions of the dynamic program is guessed in advance and 'assumed' to hold, and then subsequent decisions are forced to 'implement' the guess. This enables computing the cost of the current transition by using information that would normally only be available in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11979v1</guid>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrei Constantinescu, Andrzej Turko, Roger Wattenhofer</dc:creator>
    </item>
    <item>
      <title>Algorithm Engineering of SSSP With Negative Edge Weights</title>
      <link>https://arxiv.org/abs/2502.11999</link>
      <description>arXiv:2502.11999v1 Announce Type: new 
Abstract: Computing shortest paths is one of the most fundamental algorithmic graph problems. It is known since decades that this problem can be solved in near-linear time if all weights are nonnegative. A recent break-through by [Bernstein, Nanongkai, Wulff-Nilsen '22] presented a randomized near-linear time algorithm for this problem. A subsequent improvement in [Bringmann, Cassis, Fischer '23] significantly reduced the number of logarithmic factors and thereby also simplified the algorithm. It is surprising and exciting that both of these algorithms are combinatorial and do not contain any fundamental obstacles for being practical.
  We launch the, to the best of our knowledge, first extensive investigation towards a practical implementation of [Bringmann, Cassis, Fischer '23]. To this end, we give an accessible overview of the algorithm, discussing what adaptions are necessary to obtain a fast algorithm in practice. We manifest these adaptions in an efficient implementation. We test our implementation on a benchmark data set that is adapted to be more difficult for our implementation in order to allow for a fair comparison. As in [Bringmann, Cassis, Fischer '23] as well as in our implementation there are multiple parameters to tune, we empirically evaluate their effect and thereby determine the best choices. Our implementation is then extensively compared to one of the state-of-the-art algorithms for this problem [Goldberg, Radzik '93]. On the hardest instance type, we are faster by up to almost two orders of magnitude.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11999v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alejandro Cassis, Andreas Karrenbauer, Andr\'e Nusser, Paolo Luigi Rinaldi</dc:creator>
    </item>
    <item>
      <title>Fully Dynamic LZ77 in Sublinear Time</title>
      <link>https://arxiv.org/abs/2502.12000</link>
      <description>arXiv:2502.12000v1 Announce Type: new 
Abstract: The Lempel-Ziv 77 (LZ77) factorization is a fundamental compression scheme widely used in text processing and data compression. While efficient static algorithms exist for computing LZ77, maintaining it dynamically remains a challenging problem. Recently, Bannai, Charalampopoulos, and Radoszewski introduced an algorithm that maintains the size of the LZ77 factorization of a dynamic text in $\tilde{O}(\sqrt{n})$ per update. Their data structure works in the semi-dynamic model, where the only allowed updates are insertions at the end of the string or deletions from the start.
  In contrast, we present an algorithm that operates in a significantly more general setting of arbitrary edit operations. Our algorithm maintains the size of the LZ77 factorization of a string undergoing symbol substitutions, deletions, and insertions in $\tilde{O}(n^{2/3})$ time per update. Additionally, our data structure supports random access to the LZ77 factorization in polylogarithmic time, providing enhanced functionality for dynamic text processing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12000v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Itai Boneh, Matan Kraus</dc:creator>
    </item>
    <item>
      <title>New Rates in Stochastic Decision-Theoretic Online Learning under Differential Privacy</title>
      <link>https://arxiv.org/abs/2502.10997</link>
      <description>arXiv:2502.10997v1 Announce Type: cross 
Abstract: Hu and Mehta (2024) posed an open problem: what is the optimal instance-dependent rate for the stochastic decision-theoretic online learning (with $K$ actions and $T$ rounds) under $\varepsilon$-differential privacy? Before, the best known upper bound and lower bound are $O\left(\frac{\log K}{\Delta_{\min}} + \frac{\log K\log T}{\varepsilon}\right)$ and $\Omega\left(\frac{\log K}{\Delta_{\min}} + \frac{\log K}{\varepsilon}\right)$ (where $\Delta_{\min}$ is the gap between the optimal and the second actions). In this paper, we partially address this open problem by having two new results. First, we provide an improved upper bound for this problem $O\left(\frac{\log K}{\Delta_{\min}} + \frac{\log^2K}{\varepsilon}\right)$, where the $T$-dependency has been removed. Second, we introduce the deterministic setting, a weaker setting of this open problem, where the received loss vector is deterministic and we can focus on the analysis for $\varepsilon$ regardless of the sampling error. At the deterministic setting, we prove upper and lower bounds that match at $\Theta\left(\frac{\log K}{\varepsilon}\right)$, while a direct application of the analysis and algorithms from the original setting still leads to an extra log factor. Technically, we introduce the Bernoulli resampling trick, which enforces a monotonic property for the output from report-noisy-max mechanism that enables a tighter analysis. Moreover, by replacing the Laplace noise with Gumbel noise, we derived explicit integral form that gives a tight characterization of the regret in the deterministic case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10997v1</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruihan Wu, Yu-Xiang Wang</dc:creator>
    </item>
    <item>
      <title>Fast Maximum Common Subgraph Search: A Redundancy-Reduced Backtracking Approach</title>
      <link>https://arxiv.org/abs/2502.11557</link>
      <description>arXiv:2502.11557v1 Announce Type: cross 
Abstract: Given two input graphs, finding the largest subgraph that occurs in both, i.e., finding the maximum common subgraph, is a fundamental operator for evaluating the similarity between two graphs in graph data analysis. Existing works for solving the problem are of either theoretical or practical interest, but not both. Specifically, the algorithms with a theoretical guarantee on the running time are known to be not practically efficient; algorithms following the recently proposed backtracking framework called McSplit, run fast in practice but do not have any theoretical guarantees. In this paper, we propose a new backtracking algorithm called RRSplit, which at once achieves better practical efficiency and provides a non-trivial theoretical guarantee on the worst-case running time. To achieve the former, we develop a series of reductions and upper bounds for reducing redundant computations, i.e., the time for exploring some unpromising branches of exploration that hold no maximum common subgraph. To achieve the latter, we formally prove that RRSplit incurs a worst-case time complexity which matches the best-known complexity for the problem. Finally, we conduct extensive experiments on four benchmark graph collections, and the results demonstrate that our algorithm outperforms the practical state-of-the-art by several orders of magnitude.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11557v1</guid>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaiqiang Yu, Kaixin Wang, Cheng Long, Laks Lakshmanan, Reynold Cheng</dc:creator>
    </item>
    <item>
      <title>On a tree-based variant of bandwidth and forbidding simple topological minors</title>
      <link>https://arxiv.org/abs/2502.11674</link>
      <description>arXiv:2502.11674v1 Announce Type: cross 
Abstract: We obtain structure theorems for graphs excluding a fan (a path with a universal vertex) or a dipole ($K_{2,k}$) as a topological minor. The corresponding decompositions can be computed in FPT linear time. This is motivated by the study of a graph parameter we call treebandwidth which extends the graph parameter bandwidth by replacing the linear layout by a rooted tree such that neighbours in the graph are in ancestor-descendant relation in the tree.
  We deduce an approximation algorithm for treebandwidth running in FPT linear time from our structure theorems. We complement this result with a precise characterisation of the parameterised complexity of computing the parameter exactly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11674v1</guid>
      <category>cs.DM</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hugo Jacob, William Lochet, Christophe Paul</dc:creator>
    </item>
    <item>
      <title>Private Synthetic Graph Generation and Fused Gromov-Wasserstein Distance</title>
      <link>https://arxiv.org/abs/2502.11778</link>
      <description>arXiv:2502.11778v1 Announce Type: cross 
Abstract: Networks are popular for representing complex data. In particular, differentially private synthetic networks are much in demand for method and algorithm development. The network generator should be easy to implement and should come with theoretical guarantees. Here we start with complex data as input and jointly provide a network representation as well as a synthetic network generator. Using a random connection model, we devise an effective algorithmic approach for generating attributed synthetic graphs which is $\epsilon$-differentially private at the vertex level, while preserving utility under an appropriate notion of distance which we develop. We provide theoretical guarantees for the accuracy of the private synthetic graphs using the fused Gromov-Wasserstein distance, which extends the Wasserstein metric to structured data. Our method draws inspiration from the PSMM method of \citet{he2023}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11778v1</guid>
      <category>stat.ML</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leoni Carla Wirth, Gholamali Aminian, Gesine Reinert</dc:creator>
    </item>
    <item>
      <title>Approximating Dasgupta Cost in Sublinear Time from a Few Random Seeds</title>
      <link>https://arxiv.org/abs/2207.02581</link>
      <description>arXiv:2207.02581v2 Announce Type: replace 
Abstract: Testing graph cluster structure has been a central object of study in property testing since the foundational work of Goldreich and Ron [STOC'96] on expansion testing, i.e. the problem of distinguishing between a single cluster (an expander) and a graph that is far from a single cluster. More generally, a $(k, \epsilon)$-clusterable graph $G$ is a graph whose vertex set admits a partition into $k$ induced expanders, each with outer conductance bounded by $\epsilon$. A recent line of work initiated by Czumaj, Peng and Sohler [STOC'15] has shown how to test whether a graph is close to $(k, \epsilon)$-clusterable, and to locally determine which cluster a given vertex belongs to with misclassification rate $\approx \epsilon$, but no sublinear time algorithms for learning the structure of inter-cluster connections are known. As a simple example, can one locally distinguish between the `cluster graph' forming a line and a clique?
  In this paper, we consider the problem of testing the hierarchical cluster structure of $(k, \epsilon)$-clusterable graphs in sublinear time. Our measure of hierarchical clusterability is the well-established Dasgupta cost, and our main result is an algorithm that approximates Dasgupta cost of a $(k, \epsilon)$-clusterable graph in sublinear time, using a small number of randomly chosen seed vertices for which cluster labels are known. Our main result is an $O(\sqrt{\log k})$ approximation to Dasgupta cost of $G$ in $\approx n^{1/2+O(\epsilon)}$ time using $\approx n^{1/3}$ seeds, effectively giving a sublinear time simulation of the algorithm of Charikar and Chatziafratis [SODA'17] on clusterable graphs. To the best of our knowledge, ours is the first result on approximating the hierarchical clustering properties of such graphs in sublinear time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.02581v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Kapralov, Akash Kumar, Silvio Lattanzi, Aida Mousavifar, Weronika Wrzos-Kaminska</dc:creator>
    </item>
    <item>
      <title>Computing maximal palindromes in non-standard matching models</title>
      <link>https://arxiv.org/abs/2210.02067</link>
      <description>arXiv:2210.02067v4 Announce Type: replace 
Abstract: Palindromes are popular and important objects in textual data processing, bioinformatics, and combinatorics on words. Let $S = XaY$ be a string where $X$ and $Y$ are of the same length, and $a$ is either a single character or the empty string. Then, there exist two alternative definitions for palindromes: $S$ is said to be a palindrome if $S$ is equal to its reversal $S^R$ (Reversal-based definition); or if its right-arm $Y$ is equal to the reversal of its left-arm $X^R$ (Symmetry-based definition). It is clear that if the ``equality'' ($\approx$) used in both definitions is exact character matching ($=$), then the two definitions are the same. However, if we apply other string-equality criteria $\approx$, including the complementary-matching model for biological sequences, the Cartesian-tree model [Park et al., TCS 2020], the parameterized model [Baker, JCSS 1996], the order-preserving model [Kim et al., TCS 2014], and the palindromic-structure model [I et al., TCS 2013], then are the reversal-based palindromes and the symmetry-based palindromes the same? To the best of our knowledge, no previous work has considered or answered this natural question. In this paper, we first provide answers to this question, and then present efficient algorithms for computing all maximal palindromes under the non-standard matching models in a given string. After confirming that Gusfield's offline suffix-tree-based algorithm for computing maximal symmetry-based palindromes can be readily extended to the aforementioned matching models, we show how to extend Manacher's online algorithm for computing maximal reversal-based palindromes in linear time for all the aforementioned matching models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.02067v4</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takuya Mieno, Mitsuru Funakoshi, Yuto Nakashima, Shunsuke Inenaga, Hideo Bannai, Masayuki Takeda</dc:creator>
    </item>
    <item>
      <title>The Berlekamp-Massey Algorithm revisited</title>
      <link>https://arxiv.org/abs/2211.11721</link>
      <description>arXiv:2211.11721v3 Announce Type: replace 
Abstract: We propose a slight modification of the Berlekamp-Massey Algorithm for obtaining the minimal polynomial of a given linearly recurrent sequence. Such a modification enables to explain it in a simpler way and to adapt it to lazy evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.11721v3</guid>
      <category>cs.DS</category>
      <category>math.AG</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s00200-005-0190-z</arxiv:DOI>
      <arxiv:journal_reference>AAECC 17 (1) (2006), 75--82</arxiv:journal_reference>
      <dc:creator>Nadia Ben Atti, Gema M. Diaz--Toca, Henri Lombardi</dc:creator>
    </item>
    <item>
      <title>Optimality of Non-Adaptive Algorithms in Online Submodular Welfare Maximization with Stochastic Outcomes</title>
      <link>https://arxiv.org/abs/2403.18059</link>
      <description>arXiv:2403.18059v4 Announce Type: replace 
Abstract: We generalize the problem of online submodular welfare maximization to incorporate various stochastic elements that have gained significant attention in recent years. We show that a non-adaptive Greedy algorithm, which is oblivious to the realization of these stochastic elements, achieves the best possible competitive ratio among all polynomial-time algorithms, including adaptive ones, unless NP=RP. This result holds even when the objective function is not submodular but instead satisfies the weaker submodular order property. Our results unify and strengthen existing competitive ratio bounds across well-studied settings and diverse arrival models, showing that, in general, adaptivity to stochastic elements offers no advantage in terms of competitive ratio.
  To establish these results, we introduce a technique that lifts known results from the deterministic setting to the generalized stochastic setting. The technique has broad applicability, enabling us to show that, in certain special cases, non-adaptive Greedy-like algorithms outperform the Greedy algorithm and achieve the optimal competitive ratio. We also apply the technique in reverse to derive new upper bounds on the performance of Greedy-like algorithms in deterministic settings by leveraging upper bounds on the performance of non-adaptive algorithms in stochastic settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18059v4</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rajan Udwani</dc:creator>
    </item>
    <item>
      <title>Faster run-length compressed suffix arrays</title>
      <link>https://arxiv.org/abs/2408.04537</link>
      <description>arXiv:2408.04537v5 Announce Type: replace 
Abstract: We first review how we can store a run-length compressed suffix array (RLCSA) for a text $T$ of length $n$ over an alphabet of size $\sigma$ whose Burrows-Wheeler Transform (BWT) consists of $r$ runs in $O \left( \rule{0ex}{2ex} r \log (n / r) + r \log \sigma + \sigma \right)$ bits such that later, given character $a$ and the suffix array interval for $P$, we can find the suffix-array (SA) interval for $a P$ in $O (\log r_a + \log \log n)$ time, where $r_a$ is the number of runs of copies of $a$ in the BWT. We then show how to modify the RLCSA such that we find the SA interval for $a P$ in only $O (\log r_a)$ time, without increasing its asymptotic space bound. Our key idea is applying a result by Nishimoto and Tabei (ICALP 2021) and then replacing rank queries on sparse bitvectors by a constant number of select queries. Finally, we review two-level indexing and discuss how our faster RLCSA may be useful in improving it.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.04537v5</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nathaniel K. Brown, Travis Gagie, Giovanni Manzini, Gonzalo Navarro, Marinella Sciortino</dc:creator>
    </item>
    <item>
      <title>Subsetwise and Multi-Level Additive Spanners with Lightness Guarantees</title>
      <link>https://arxiv.org/abs/2411.07505</link>
      <description>arXiv:2411.07505v2 Announce Type: replace 
Abstract: An \emph{additive +$\beta W$ spanner} of an edge weighted graph $G=(V,E)$ is a subgraph $H$ of $G$ such that for every pair of vertices $u$ and $v$, $d_{H}(u,v) \le d_G(u,v) + \beta W$, where $d_G(u,v)$ is the shortest path length from $u$ to $v$ in $G$. While additive spanners are very well studied in the literature, spanners that are both additive and lightweight have been introduced more recently [Ahmed et al., WG 2021]. Here the \emph{lightness} is the ratio of the spanner weight to the weight of a minimum spanning tree of $G$. In this paper, we examine the widely known subsetwise setting when the distance conditions need to hold only among the pairs of a given subset $S$. We generalize the concept of lightness to subset-lightness using a Steiner tree and provide polynomial-time algorithms to compute subsetwise additive $+\epsilon W$ spanner and $+(4+\epsilon) W$ spanner with $O_\epsilon(|S|)$ and $O_\epsilon(|V_H|^{1/3} |S|^{1/3})$ subset-lightness, respectively, where $\epsilon$ is an arbitrary positive constant. We next examine a multi-level version of spanners that often arises in network visualization and modeling the quality of service requirements in communication networks. The goal here is to compute a nested sequence of spanners with the minimum total edge weight. We provide an $e$-approximation algorithm to compute multi-level spanners assuming that an oracle is given to compute single-level spanners, improving a previously known 4-approximation [Ahmed et al., IWOCA 2023].</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07505v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Reyan Ahmed, Debajyoti Mondal, Rahnuma Islam Nishat</dc:creator>
    </item>
    <item>
      <title>Improved Differentially Private Continual Observation Using Group Algebra</title>
      <link>https://arxiv.org/abs/2412.02840</link>
      <description>arXiv:2412.02840v2 Announce Type: replace 
Abstract: Differentially private weighted prefix sum under continual observation is a crucial component in the production-level deployment of private next-word prediction for Gboard, which, according to Google, has over a billion users. More specifically, Google uses a differentially private mechanism to sum weighted gradients in its \emph{private follow-the-regularized leader} algorithm. Apart from efficiency, the additive error of the private mechanism is crucial as multiplied with the square root of the model's dimension $d$ (with $d$ ranging up to $10$ trillion, for example, Switch Transformers or M6-10T), it determines the accuracy of the learning system. So, any improvement in leading constant matters significantly in practice.
  In this paper, we show a novel connection between mechanisms for continual weighted prefix sum and a concept in representation theory known as the group matrix introduced in correspondence between Dedekind and Frobenius (1897) and generalized by Schur (1904). To the best of our knowledge, this is the first application of group algebra to analyze differentially private algorithms. Using this connection, we analyze a class of matrix norms known as {\em factorization norms} that give upper and lower bounds for the additive error under general $\ell_p$-norms of the matrix mechanism. This allows us to give the first efficient factorization that matches the best-known non-constructive upper bound on the factorization norm by Mathias (1993) for the matrix used in Google's deployment and also improves on the previous best-known constructive bound of Fichtenberger et al. (ICML 2023) and Henzinger et al. (SODA 2023) and the first upper bound on the additive error for a large class of weight functions for weighted prefix sum problems, including the sliding window matrix (Bolot et al. (ICDT 2013).</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02840v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Monika Henzinger, Jalaj Upadhyay</dc:creator>
    </item>
    <item>
      <title>Sequential Diversification with Provable Guarantees</title>
      <link>https://arxiv.org/abs/2412.10944</link>
      <description>arXiv:2412.10944v2 Announce Type: replace 
Abstract: Diversification is a useful tool for exploring large collections of information items. It has been used to reduce redundancy and cover multiple perspectives in information-search settings. Diversification finds applications in many different domains, including presenting search results of information-retrieval systems and selecting suggestions for recommender systems.
  Interestingly, existing measures of diversity are defined over \emph{sets} of items, rather than evaluating \emph{sequences} of items. This design choice comes in contrast with commonly-used relevance measures, which are distinctly defined over sequences of items, taking into account the ranking of items. The importance of employing sequential measures is that information items are almost always presented in a sequential manner, and during their information-exploration activity users tend to prioritize items with higher~ranking.
  In this paper, we study the problem of \emph{maximizing sequential diversity}. This is a new measure of \emph{diversity}, which accounts for the \emph{ranking} of the items, and incorporates \emph{item relevance} and \emph{user behavior}. The overarching framework can be instantiated with different diversity measures, and here we consider the measures of \emph{sum~diversity} and \emph{coverage~diversity}. The problem was recently proposed by Coppolillo et al.~\citep{coppolillo2024relevance}, where they introduce empirical methods that work well in practice. Our paper is a theoretical treatment of the problem: we establish the problem hardness and present algorithms with constant approximation guarantees for both diversity measures we consider. Experimentally, we demonstrate that our methods are competitive against strong baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10944v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Honglian Wang, Sijing Tu, Aristides Gionis</dc:creator>
    </item>
    <item>
      <title>A LP-rounding based algorithm for soft capacitated facility location problem with submodular penalties</title>
      <link>https://arxiv.org/abs/2502.09412</link>
      <description>arXiv:2502.09412v2 Announce Type: replace 
Abstract: The soft capacitated facility location problem (SCFLP) is a classic combinatorial optimization problem, with its variants widely applied in the fields of operations research and computer science. In the SCFLP, given a set $\mathcal{F}$ of facilities and a set $\mathcal{D}$ of clients, each facility has a capacity and an open cost, allowing to open multiple times, and each client has a demand.
  This problem is to find a subset of facilities in $\mathcal{F}$ and connect each client to the facilities opened, such that the total cost including open cost and connection cost is minimied. SCFLP is a NP-hard problem, which has led to a focus on approximation algorithms. Based on this, we consider a variant, that is, soft capacitated facility location problem with submodular penalties (SCFLPSP), which allows some clients not to be served by accepting the penalty cost. And we consider the integer splittable case of demand, that is, the demand of each client is served by multiple facilities with the integer service amount by each facility. Based on LP-rounding, we propose a $(\lambda R+4)$-approximation algorithm, where $R=\frac{\max_{i \in \mathcal{F} }f_i}{\min_{i \in \mathcal{F} }f_i},\lambda=\frac{R+\sqrt{R^2+8R}}{2R}$. In particular, when the open cost is uniform, the approximation ratio is 6.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09412v2</guid>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hanyin Xiao, Jiaming Zhang, Zhikang Zhang, Weidong Li</dc:creator>
    </item>
    <item>
      <title>GreedyML: A Parallel Algorithm for Maximizing Constrained Submodular Functions</title>
      <link>https://arxiv.org/abs/2403.10332</link>
      <description>arXiv:2403.10332v3 Announce Type: replace-cross 
Abstract: We describe a parallel approximation algorithm for maximizing monotone submodular functions subject to hereditary constraints on distributed memory multiprocessors. Our work is motivated by the need to solve submodular optimization problems on massive data sets, for practical contexts such as data summarization, machine learning, and graph sparsification.
  Our work builds on the randomized distributed RandGreedi algorithm, proposed by Barbosa, Ene, Nguyen, and Ward (2015). This algorithm computes a distributed solution by randomly partitioning the data among all the processors and then employing \emph{a single} accumulation step in which all processors send their partial solutions to one processor. However, for large problems, the accumulation step exceeds the memory available on a processor, and the processor that performs the accumulation becomes a computational bottleneck.
  Hence we propose a generalization of the RandGreedi algorithm that employs multiple accumulation steps to reduce the memory required. We analyze the approximation ratio and the time complexity of the algorithm (in the BSP model). We evaluate the new GreedyML algorithm on three classes of problems, and report results from large-scale data sets with millions of elements. The results show that the GreedyML algorithm can solve problems where the sequential Greedy and distributed RandGreedi algorithms fail due to memory constraints. For certain computationally intensive problems, the GreedyML algorithm is faster than the RandGreedi algorithm. The observed approximation quality of the solutions computed by the GreedyML algorithm closely matches those obtained by the RandGreedi algorithm on these problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10332v3</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shivaram Gopal, S M Ferdous, Hemanta K. Maji, Alex Pothen</dc:creator>
    </item>
    <item>
      <title>Efficient PAC Learning of Halfspaces with Constant Malicious Noise Rate</title>
      <link>https://arxiv.org/abs/2410.01186</link>
      <description>arXiv:2410.01186v4 Announce Type: replace-cross 
Abstract: Understanding noise tolerance of machine learning algorithms is a central quest in learning theory. In this work, we study the problem of computationally efficient PAC learning of halfspaces in the presence of malicious noise, where an adversary can corrupt both instances and labels of training samples. The best-known noise tolerance either depends on a target error rate under distributional assumptions or on a margin parameter under large-margin conditions. In this work, we show that when both types of conditions are satisfied, it is possible to achieve constant noise tolerance by minimizing a reweighted hinge loss. Our key ingredients include: 1) an efficient algorithm that finds weights to control the gradient deterioration from corrupted samples, and 2) a new analysis on the robustness of the hinge loss equipped with such weights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01186v4</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jie Shen</dc:creator>
    </item>
    <item>
      <title>Weighted Envy Freeness With Limited Subsidies</title>
      <link>https://arxiv.org/abs/2411.12696</link>
      <description>arXiv:2411.12696v2 Announce Type: replace-cross 
Abstract: We explore solutions for fairly allocating indivisible items among agents assigned weights representing their entitlements. Our fairness goal is weighted-envy-freeness (WEF), where each agent deems their allocated portion relative to their entitlement at least as favorable as any other's relative to their own. In many cases, achieving WEF necessitates monetary transfers, which can be modeled as third-party subsidies. The goal is to attain WEF with bounded subsidies.
  Previous work in the unweighted setting of subsidies relied on basic characterizations of EF that fail in the weighted settings. This makes our new setting challenging and theoretically intriguing. We present polynomial-time algorithms that compute WEF-able allocations with an upper bound on the subsidy per agent in three distinct additive valuation scenarios: (1) general, (2) identical, and (3) binary. When all weights are equal, our bounds reduce to the bounds derived in the literature for the unweighted setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12696v2</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Noga Klein Elmalem, Rica Gonen, Erel Segal-Halevi</dc:creator>
    </item>
    <item>
      <title>Weakly acyclic diagrams: A data structure for infinite-state symbolic verification</title>
      <link>https://arxiv.org/abs/2411.17250</link>
      <description>arXiv:2411.17250v2 Announce Type: replace-cross 
Abstract: Ordered binary decision diagrams (OBDDs) are a fundamental data structure for the manipulation of Boolean functions, with strong applications to finite-state symbolic model checking. OBDDs allow for efficient algorithms using top-down dynamic programming. From an automata-theoretic perspective, OBDDs essentially are minimal deterministic finite automata recognizing languages whose words have a fixed length (the arity of the Boolean function). We introduce weakly acyclic diagrams (WADs), a generalization of OBDDs that maintains their algorithmic advantages, but can also represent infinite languages. We develop the theory of WADs and show that they can be used for symbolic model checking of various models of infinite-state systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17250v2</guid>
      <category>cs.LO</category>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Blondin, Micha\"el Cadilhac, Xin-Yi Cui, Philipp Czerner, Javier Esparza, Jakob Schulz</dc:creator>
    </item>
  </channel>
</rss>
