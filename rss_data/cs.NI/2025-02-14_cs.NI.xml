<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Feb 2025 05:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Framework to Develop and Validate RL-Based Obstacle-Aware UAV Positioning Algorithms</title>
      <link>https://arxiv.org/abs/2502.08787</link>
      <description>arXiv:2502.08787v1 Announce Type: new 
Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly being utilized to enhance the Quality of Service (QoS) in wireless networks due to their flexibility and cost-effectiveness. However, optimizing UAV placement in dynamic and obstacle-prone environments remains a research challenge. Reinforcement Learning (RL) has proven to be an effective approach that offers adaptability and robustness in such environments.
  This paper introduces RLpos-3, a novel framework that integrates standard RL techniques and existing libraries with Network Simulator 3 (ns-3) to facilitate the development and evaluation of UAV positioning algorithms. RLpos-3 serves as a complementary tool for researchers, enabling the implementation, analysis, and benchmarking of UAV positioning strategies across different environmental settings while ensuring user traffic demands are met. To validate its effectiveness, we present a use case demonstrating the performance of RLpos-3 in optimizing UAV placement under realistic conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08787v1</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kamran Shafafi, Manuel Ricardo, Rui Campos</dc:creator>
    </item>
    <item>
      <title>Geofeed Adoption and Authentication</title>
      <link>https://arxiv.org/abs/2502.08849</link>
      <description>arXiv:2502.08849v1 Announce Type: new 
Abstract: IP Geofeed is a recently proposed informational standard that allows network operators to publish the geographical location of deployed IPv4 and IPv6 prefixes. In this work we study the adoption of IP geofeed, assess deployment of geofeed at Regional Internet Registry and Autonomous System levels, and analyze adherence to RFC 8805 and RFC 9092 in deployed geofeeds. We evaluate the authentication mechanism proposed in RFC 9092 and find that it lacks key features from a security perspective. We propose a novel approach to simplify the authentication of geofeeds and assess its efficiency using different benchmarks. Our findings highlight the challenges in current geofeed adoption and the potential for improving both security and scalability in geofeed validation processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08849v1</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dipsy Desai, Kicho Yu, Sulyab Thottungal Valapu</dc:creator>
    </item>
    <item>
      <title>Fast Userspace Networking for the Rest of Us</title>
      <link>https://arxiv.org/abs/2502.09281</link>
      <description>arXiv:2502.09281v1 Announce Type: new 
Abstract: After a decade of research in userspace network stacks, why do new solutions remain inaccessible to most developers? We argue that this is because they ignored (1) the hardware constraints of public cloud NICs (vNICs) and (2) the flexibility required by applications. Concerning the former, state-of-the-art proposals rely on specific NIC features (e.g., flow steering, deep buffers) that are not broadly available in vNICs. As for the latter, most of these stacks enforce a restrictive execution model that does not align well with cloud application requirements.
  We propose a new userspace network stack, Machnet, built for public cloud VMs. Central to Machnet is a new ''Least Common Denominator'' model, a conceptual NIC with a minimal feature set supported by all kernel-bypass vNICs. The challenge is to build a new solution with performance comparable to existing stacks while relying only on basic features (e.g., no flow steering, no RSS reconfiguration). Machnet uses a microkernel design to provide higher flexibility in application execution compared to a library OS design; we show that microkernels' inter-process communication overhead is negligible on large cloud networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09281v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <category>cs.OS</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alireza Sanaee, Vahab Jabrayilov, Ilias Marinos, Anuj Kalia, Divyanshu Saxena, Prateesh Goyal, Kostis Kaffes, Gianni Antichi</dc:creator>
    </item>
    <item>
      <title>Predicting Drive Test Results in Mobile Networks Using Optimization Techniques</title>
      <link>https://arxiv.org/abs/2502.09305</link>
      <description>arXiv:2502.09305v1 Announce Type: new 
Abstract: Mobile network operators constantly optimize their networks to ensure superior service quality and coverage. This optimization is crucial for maintaining an optimal user experience and requires extensive data collection and analysis. One of the primary methods for gathering this data is through drive tests, where technical teams use specialized equipment to collect signal information across various regions. However, drive tests are both costly and time-consuming, and they face challenges such as traffic conditions, environmental factors, and limited access to certain areas. These constraints make it difficult to replicate drive tests under similar conditions. In this study, we propose a method that enables operators to predict received signal strength at specific locations using data from other drive test points. By reducing the need for widespread drive tests, this approach allows operators to save time and resources while still obtaining the necessary data to optimize their networks and mitigate the challenges associated with traditional drive tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09305v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>MohammadJava Taheri, Abolfazl Diyanat, MortezaAli Ahmadi, Ali Nazari</dc:creator>
    </item>
    <item>
      <title>Delay Performance Analysis with Short Packets in Intelligent Machine Network</title>
      <link>https://arxiv.org/abs/2502.09313</link>
      <description>arXiv:2502.09313v1 Announce Type: new 
Abstract: With the rapid development of delay-sensitive services happened in industrial manufacturing, Internet of Vehicles, and smart logistics, more stringent delay requirements are put forward for the intelligent machine (IM) network. Short packet transmissions are widely adopted to reduce delay in IM networks. However, the delay performance of an IM network has not been sufficiently analyzed. This paper applies queuing theory and stochastic geometry to construct network model and transmission model for downlink communication, respectively, proposes and derives the following three metrics, e.g., the transmission success probability (with delay as the threshold), expected delay, and delay jitter. To accurately characterize the transmission delay with short packets, the finite blocklength capacity is used to measure the channel transmission rate. Simulation results show that the increase of packet length and IM density significantly deteriorates the three metrics. Short packets are needed to improve the three metrics, especially in high IM density scenarios. The outcomes of this paper provide an important theoretical basis for the optimization design and performance improvement of IM networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09313v1</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenyan Xu, Zhiging Wei, Zhiqun Song, Yixin Zhang, Haotian Liu, Ying Zhou, Xiaoyu Yang, Yashan Pang</dc:creator>
    </item>
    <item>
      <title>Investigation of Advanced Persistent Threats Network-based Tactics, Techniques and Procedures</title>
      <link>https://arxiv.org/abs/2502.08830</link>
      <description>arXiv:2502.08830v1 Announce Type: cross 
Abstract: The scarcity of data and the high complexity of Advanced Persistent Threats (APTs) attacks have created challenges in comprehending their behavior and hindered the exploration of effective detection techniques. To create an effective APT detection strategy, it is important to examine the Tactics, Techniques, and Procedures (TTPs) that have been reported by the industry. These TTPs can be difficult to classify as either malicious or legitimate. When developing an approach for the next generation of network intrusion detection systems (NIDS), it is necessary to take into account the specific context of the attack explained in this paper.
  In this study, we select 33 APT campaigns based on the fair distribution over the past 22 years to observe the evolution of APTs over time. We focus on their evasion techniques and how they stay undetected for months or years. We found that APTs cannot continue their operations without C&amp;C servers, which are mostly addressed by Domain Name System (DNS). We identify several TTPs used for DNS, such as Dynamic DNS, typosquatting, and TLD squatting. The next step for APT operators is to start communicating with a victim. We found that the most popular protocol to deploy evasion techniques is using HTTP(S) with 81% of APT campaigns. HTTP(S) can evade firewall filtering and pose as legitimate web-based traffic. DNS protocol is also widely used by 45% of APTs for DNS resolution and tunneling. We identify and analyze the TTPs associated with using HTTP(S) based on real artifacts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08830v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <category>cs.OS</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Almuthanna Alageel, Sergio Maffeis, Imperial College London</dc:creator>
    </item>
    <item>
      <title>Semantic Communication Meets Heterogeneous Network: Emerging Trends, Opportunities, and Challenges</title>
      <link>https://arxiv.org/abs/2502.08999</link>
      <description>arXiv:2502.08999v1 Announce Type: cross 
Abstract: Recent developments in machine learning (ML) techniques enable users to extract, transmit, and reproduce information semantics via ML-based semantic communication (SemCom). This significantly increases network spectral efficiency and transmission robustness. In the network, the semantic encoders and decoders among various users, based on ML, however, require collaborative updating according to new transmission tasks. The various heterogeneous characteristics of most networks in turn introduce emerging but unique challenges for semantic codec updating that are different from other general ML model updating. In this article, we first overview the key components of the SemCom system. We then discuss the unique challenges associated with semantic codec updates in heterogeneous networks. Accordingly, we point out a potential framework and discuss the pros and cons thereof. Finally, several future research directions are also discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08999v1</guid>
      <category>eess.SP</category>
      <category>cs.NI</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guhan Zheng, Qiang Ni, Aryan Kaushik, Lixia Yang</dc:creator>
    </item>
    <item>
      <title>Application of Tabular Transformer Architectures for Operating System Fingerprinting</title>
      <link>https://arxiv.org/abs/2502.09084</link>
      <description>arXiv:2502.09084v1 Announce Type: cross 
Abstract: Operating System (OS) fingerprinting is essential for network management and cybersecurity, enabling accurate device identification based on network traffic analysis. Traditional rule-based tools such as Nmap and p0f face challenges in dynamic environments due to frequent OS updates and obfuscation techniques. While Machine Learning (ML) approaches have been explored, Deep Learning (DL) models, particularly Transformer architectures, remain unexploited in this domain. This study investigates the application of Tabular Transformer architectures-specifically TabTransformer and FT-Transformer-for OS fingerprinting, leveraging structured network data from three publicly available datasets. Our experiments demonstrate that FT-Transformer generally outperforms traditional ML models, previous approaches and TabTransformer across multiple classification levels (OS family, major, and minor versions). The results establish a strong foundation for DL-based OS fingerprinting, improving accuracy and adaptability in complex network environments. Furthermore, we ensure the reproducibility of our research by providing an open-source implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09084v1</guid>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rub\'en P\'erez-Jove, Cristian R. Munteanu, Alejandro Pazos, Jose V\'azquez-Naya</dc:creator>
    </item>
    <item>
      <title>An Overview and Solution for Democratizing AI Workflows at the Network Edge</title>
      <link>https://arxiv.org/abs/2407.11905</link>
      <description>arXiv:2407.11905v2 Announce Type: replace 
Abstract: With the process of democratization of the network edge, hardware and software for networks are becoming available to the public, overcoming the confines of traditional cloud providers and network operators. This trend, coupled with the increasing importance of AI in 6G and beyond cellular networks, presents opportunities for innovative AI applications and systems at the network edge. While AI models and services are well-managed in cloud systems, achieving similar maturity for serving network needs remains an open challenge. Existing open solutions are emerging and are yet to consider democratization requirements. In this work, we identify key requirements for democratization and propose NAOMI, a solution for democratizing AI/ML workflows at the network edge designed based on those requirements. Guided by the functionality and overlap analysis of the O-RAN AI/ML workflow architecture and MLOps systems, coupled with the survey of open-source AI/ML tools, we develop a modular, scalable, and distributed hardware architecture-independent solution. NAOMI leverages state-of-the-art open-source tools and can be deployed on distributed clusters of heterogeneous devices. The results show that NAOMI performs up to 40% better in deployment time and up to 73% faster in AI/ML workflow execution for larger datasets compared to AI/ML Framework, a representative open network access solution, while performing inference and utilizing resources on par with its counterpart.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11905v2</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrej \v{C}op, Bla\v{z} Bertalani\v{c}, Carolina Fortuna</dc:creator>
    </item>
    <item>
      <title>MPLS Network Actions: Technological Overview and P4-Based Implementation on a High-Speed Switching ASIC</title>
      <link>https://arxiv.org/abs/2410.20400</link>
      <description>arXiv:2410.20400v2 Announce Type: replace 
Abstract: In MPLS, packets are encapsulated with labels that add domain-specific forwarding information. Special purpose labels were introduced to trigger special behavior in MPLS nodes but their number is limited. Therefore, the IETF proposed the MPLS Network Actions (MNA) framework. It extends MPLS with new features, some of which have already been defined to support relevant use cases. This paper provides a comprehensive technological overview of MNA concepts and use cases. It compares MNA to IPv6 extension headers (EHs) that serve a similar purpose, and argues that MNA can be better deployed than EHs. It then presents P4-MNA, a first hardware implementation running at 400 Gb/s per port. Scalability and performance of P4-MNA are evaluated, showing negligible impact on processing delay caused by network actions. Moreover, the applicability of MNA is demonstrated by implementing the use cases of link-specific packet loss measurement using the alternate-marking-method (AMM) and bandwidth reservation using network slicing. We identify header stacking constraints resulting from hardware resources and from the number of network actions that must be supported according to the MNA encoding. They make an implementation for hardware that can only parse a few MPLS headers infeasible. We propose to make the number of supported network actions a node parameter and signal this in the network. Then, an upgrade to MNA is also feasible for hardware with fewer available resources. We explain that for MNA with in-stack data (ISD), some header bits must remain unchanged during forwarding, and give an outlook on post-stack data (PSD).</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20400v2</guid>
      <category>cs.NI</category>
      <category>cs.ET</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabian Ihle, Michael Menth</dc:creator>
    </item>
    <item>
      <title>Overview of AI and Communication for 6G Network: Fundamentals, Challenges, and Future Research Opportunities</title>
      <link>https://arxiv.org/abs/2412.14538</link>
      <description>arXiv:2412.14538v4 Announce Type: replace 
Abstract: With the growing demand for seamless connectivity and intelligent communication, the integration of artificial intelligence (AI) and sixth-generation (6G) communication networks has emerged as a transformative paradigm. By embedding AI capabilities across various network layers, this integration enables optimized resource allocation, improved efficiency, and enhanced system robust performance, particularly in intricate and dynamic environments. This paper presents a comprehensive overview of AI and communication for 6G networks, with a focus on emphasizing their foundational principles, inherent challenges, and future research opportunities. We first review the integration of AI and communications in the context of 6G, exploring the driving factors behind incorporating AI into wireless communications, as well as the vision for the convergence of AI and 6G. The discourse then transitions to a detailed exposition of the envisioned integration of AI within 6G networks, delineated across three progressive developmental stages. The first stage, AI for Network, focuses on employing AI to augment network performance, optimize efficiency, and enhance user service experiences. The second stage, Network for AI, highlights the role of the network in facilitating and buttressing AI operations and presents key enabling technologies, such as digital twins for AI and semantic communication. In the final stage, AI as a Service, it is anticipated that future 6G networks will innately provide AI functions as services, supporting application scenarios like immersive communication and intelligent industrial robots. In addition, we conduct an in-depth analysis of the critical challenges faced by the integration of AI and communications in 6G. Finally, we outline promising future research opportunities that are expected to drive the development and refinement of AI and 6G communications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14538v4</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>eess.SP</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qimei Cui, Xiaohu You, Ni Wei, Guoshun Nan, Xuefei Zhang, Jianhua Zhang, Xinchen Lyu, Ming Ai, Xiaofeng Tao, Zhiyong Feng, Ping Zhang, Qingqing Wu, Meixia Tao, Yongming Huang, Chongwen Huang, Guangyi Liu, Chenghui Peng, Zhiwen Pan, Tao Sun, Dusit Niyato, Tao Chen, Muhammad Khurram Khan, Abbas Jamalipour, Mohsen Guizani, Chau Yuen</dc:creator>
    </item>
    <item>
      <title>Enhancements to P4TG: Protocols, Performance, and Automation</title>
      <link>https://arxiv.org/abs/2501.17127</link>
      <description>arXiv:2501.17127v2 Announce Type: replace 
Abstract: P4TG is a hardware-based traffic generator (TG) running on the Intel Tofino 1 ASIC and was programmed using the programming language P4. In its initial version, P4TG could generate up to 10x100 Gb/s of traffic and directly measure rates, packet loss, and other metrics in the data plane. Many researchers and industrial partners requested new features to be incorporated into P4TG since its publication in 2023. With the recently added features, P4TG supports the generation of packets encapsulated with a customizable VLAN, QinQ, VxLAN, MPLS, and SRv6 header. Further, generation of IPv6 traffic is added and P4TG is ported to the Intel Tofino 2 platform enabling a generation capability of up to 10x400 Gb/s. The improvement in user experience focuses on ease of operation. Features like automated ARP replies, improved visualization, report generation, and automated testing based on the IMIX distribution and RFC 2544 are added. Future work on P4TG includes NDP to facilitate IPv6 traffic, and a NETCONF integration to further ease the configuration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17127v2</guid>
      <category>cs.NI</category>
      <category>cs.PF</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabian Ihle, Etienne Zink, Steffen Lindner, Michael Menth</dc:creator>
    </item>
    <item>
      <title>Rust Barefoot Runtime (RBFRT): Fast Runtime Control for the Intel Tofino</title>
      <link>https://arxiv.org/abs/2501.17271</link>
      <description>arXiv:2501.17271v2 Announce Type: replace 
Abstract: Data plane programming enables the programmability of network devices with domain-specific programming languages, like P4. One commonly used P4-programmable hardware target is the Intel Tofino switching ASIC. The runtime behavior of an implemented P4 program on Tofino can be configured with shell scripts or a Python library from Barefoot provided with the Tofino. Both are limited in their capabilities and usability. This paper introduces the Rust Barefoot Runtime (RBFRT), a Rust-based control plane library. The RBFRT provides a fast and memory-safe interface to configure the Intel Tofino. We showed that the RBFRT achieves a higher insertion rate for MAT entries and has a shorter response time compared to the Python library.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17271v2</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Etienne Zink, Moritz Fl\"uchter, Steffen Lindner, Fabian Ihle, Michael Menth</dc:creator>
    </item>
    <item>
      <title>Sequential Binary Classification for Intrusion Detection</title>
      <link>https://arxiv.org/abs/2406.06099</link>
      <description>arXiv:2406.06099v2 Announce Type: replace-cross 
Abstract: Network Intrusion Detection Systems (IDS) have become increasingly important as networks become more vulnerable to new and sophisticated attacks. Machine Learning (ML)-based IDS are increasingly seen as the most effective approach to handle this issue. However, IDS datasets suffer from high class imbalance, which impacts the performance of standard ML models. Different from existing data-driven techniques to handling class imbalance, this paper explores a structural approach to handling class imbalance in multi-class classification (MCC) problems. The proposed approach - Sequential Binary Classification (SBC), is a hierarchical cascade of (regular) binary classifiers. Experiments on benchmark IDS datasets demonstrate that the structural approach to handling class-imbalance, as exemplified by SBC, is a viable approach to handling the issue.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06099v2</guid>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shrihari Vasudevan, Ishan Chokshi, Raaghul Ranganathan, Nachiappan Sundaram</dc:creator>
    </item>
    <item>
      <title>AI Flow at the Network Edge</title>
      <link>https://arxiv.org/abs/2411.12469</link>
      <description>arXiv:2411.12469v4 Announce Type: replace-cross 
Abstract: Recent advancements in large language models (LLMs) and their multimodal variants have led to remarkable progress across various domains, demonstrating impressive capabilities and unprecedented potential. In the era of ubiquitous connectivity, leveraging communication networks to distribute intelligence is a transformative concept, envisioning AI-powered services accessible at the network edge. However, pushing large models from the cloud to resource-constrained environments faces critical challenges. Model inference on low-end devices leads to excessive latency and performance bottlenecks, while raw data transmission over limited bandwidth networks causes high communication overhead. This article presents AI Flow, a framework that streamlines the inference process by jointly leveraging the heterogeneous resources available across devices, edge nodes, and cloud servers, making intelligence flow across networks. To facilitate cooperation among multiple computational nodes, the proposed framework explores a paradigm shift in the design of communication network systems from transmitting information flow to intelligence flow, where the goal of communications is task-oriented and folded into the inference process. Experimental results demonstrate the effectiveness of the proposed framework through an image captioning use case, showcasing the ability to reduce response latency while maintaining high-quality captions. This article serves as a position paper for identifying the motivation, challenges, and principles of AI Flow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12469v4</guid>
      <category>eess.SP</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiawei Shao, Xuelong Li</dc:creator>
    </item>
  </channel>
</rss>
