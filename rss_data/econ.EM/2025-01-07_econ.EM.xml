<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 07 Jan 2025 05:01:49 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Prediction with Differential Covariate Classification: Illustrated by Racial/Ethnic Classification in Medical Risk Assessment</title>
      <link>https://arxiv.org/abs/2501.02318</link>
      <description>arXiv:2501.02318v1 Announce Type: new 
Abstract: A common practice in evidence-based decision-making uses estimates of conditional probabilities P(y|x) obtained from research studies to predict outcomes y on the basis of observed covariates x. Given this information, decisions are then based on the predicted outcomes. Researchers commonly assume that the predictors used in the generation of the evidence are the same as those used in applying the evidence: i.e., the meaning of x in the two circumstances is the same. This may not be the case in real-world settings. Across a wide-range of settings, ranging from clinical practice or education policy, demographic attributes (e.g., age, race, ethnicity) are often classified differently in research studies than in decision settings. This paper studies identification in such settings. We propose a formal framework for prediction with what we term differential covariate classification (DCC). Using this framework, we analyze partial identification of probabilistic predictions and assess how various assumptions influence the identification regions. We apply the findings to a range of settings, focusing mainly on differential classification of individuals' race and ethnicity in clinical medicine. We find that bounds on P(y|x) can be wide, and the information needed to narrow them available only in special cases. These findings highlight an important problem in using evidence in decision making, a problem that has not yet been fully appreciated in debates on classification in public policy and medicine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02318v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charles F. Manski, John Mullahy, Atheendar S. Venkataramani</dc:creator>
    </item>
    <item>
      <title>Estimating Discrete Choice Demand Models with Sparse Market-Product Shocks</title>
      <link>https://arxiv.org/abs/2501.02381</link>
      <description>arXiv:2501.02381v1 Announce Type: new 
Abstract: We propose a new approach to estimating the random coefficient logit demand model for differentiated products when the vector of market-product level shocks is sparse. Assuming sparsity, we establish nonparametric identification of the distribution of random coefficients and demand shocks under mild conditions. Then we develop a Bayesian procedure, which exploits the sparsity structure using shrinkage priors, to conduct inference about the model parameters and counterfactual quantities. Comparing to the standard BLP (Berry, Levinsohn, &amp; Pakes, 1995) method, our approach does not require demand inversion or instrumental variables (IVs), thus provides a compelling alternative when IVs are not available or their validity is questionable. Monte Carlo simulations validate our theoretical findings and demonstrate the effectiveness of our approach, while empirical applications reveal evidence of sparse demand shocks in well-known datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02381v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhentong Lu, Kenichi Shimizu</dc:creator>
    </item>
    <item>
      <title>Spatially-clustered spatial autoregressive models with application to agricultural market concentration in Europe</title>
      <link>https://arxiv.org/abs/2407.15874</link>
      <description>arXiv:2407.15874v1 Announce Type: cross 
Abstract: In this paper, we present an extension of the spatially-clustered linear regression models, namely, the spatially-clustered spatial autoregression (SCSAR) model, to deal with spatial heterogeneity issues in clustering procedures. In particular, we extend classical spatial econometrics models, such as the spatial autoregressive model, the spatial error model, and the spatially-lagged model, by allowing the regression coefficients to be spatially varying according to a cluster-wise structure. Cluster memberships and regression coefficients are jointly estimated through a penalized maximum likelihood algorithm which encourages neighboring units to belong to the same spatial cluster with shared regression coefficients. Motivated by the increase of observed values of the Gini index for the agricultural production in Europe between 2010 and 2020, the proposed methodology is employed to assess the presence of local spatial spillovers on the market concentration index for the European regions in the last decade. Empirical findings support the hypothesis of fragmentation of the European agricultural market, as the regions can be well represented by a clustering structure partitioning the continent into three-groups, roughly approximated by a division among Western, North Central and Southeastern regions. Also, we detect heterogeneous local effects induced by the selected explanatory variables on the regional market concentration. In particular, we find that variables associated with social, territorial and economic relevance of the agricultural sector seem to act differently throughout the spatial dimension, across the clusters and with respect to the pooled model, and temporal dimension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15874v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roy Cerqueti (Department of Social,Economic Sciences, Sapienza University of Rome, Italy,GRANEM, University of Angers, France), Paolo Maranzano (Department Economics, Management,Statistics), Raffaele Mattera (Department of Social,Economic Sciences, Sapienza University of Rome, Italy)</dc:creator>
    </item>
    <item>
      <title>HMM-LSTM Fusion Model for Economic Forecasting</title>
      <link>https://arxiv.org/abs/2501.02002</link>
      <description>arXiv:2501.02002v1 Announce Type: cross 
Abstract: This paper explores the application of Hidden Markov Models (HMM) and Long Short-Term Memory (LSTM) neural networks for economic forecasting, focusing on predicting CPI inflation rates. The study explores a new approach that integrates HMM-derived hidden states and means as additional features for LSTM modeling, aiming to enhance the interpretability and predictive performance of the models. The research begins with data collection and preprocessing, followed by the implementation of the HMM to identify hidden states representing distinct economic conditions. Subsequently, LSTM models are trained using the original and augmented data sets, allowing for comparative analysis and evaluation. The results demonstrate that incorporating HMM-derived data improves the predictive accuracy of LSTM models, particularly in capturing complex temporal patterns and mitigating the impact of volatile economic conditions. Additionally, the paper discusses the implementation of Integrated Gradients for model interpretability and provides insights into the economic dynamics reflected in the forecasting outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02002v1</guid>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guhan Sivakumar</dc:creator>
    </item>
    <item>
      <title>Efficient estimation of average treatment effects with unmeasured confounding and proxies</title>
      <link>https://arxiv.org/abs/2501.02214</link>
      <description>arXiv:2501.02214v1 Announce Type: cross 
Abstract: One approach to estimating the average treatment effect in binary treatment with unmeasured confounding is the proximal causal inference, which assumes the availability of outcome and treatment confounding proxies. The key identifying result relies on the existence of a so-called bridge function. A parametric specification of the bridge function is usually postulated and estimated using standard techniques. The estimated bridge function is then plugged in to estimate the average treatment effect. This approach may have two efficiency losses. First, the bridge function may not be efficiently estimated since it solves an integral equation. Second, the sequential procedure may fail to account for the correlation between the two steps. This paper proposes to approximate the integral equation with increasing moment restrictions and jointly estimate the bridge function and the average treatment effect. Under sufficient conditions, we show that the proposed estimator is efficient. To assist implementation, we propose a data-driven procedure for selecting the tuning parameter (i.e., number of moment restrictions). Simulation studies reveal that the proposed method performs well in finite samples, and application to the right heart catheterization dataset from the SUPPORT study demonstrates its practical value.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02214v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chunrong Ai, Jiawei Shan</dc:creator>
    </item>
    <item>
      <title>Revealed Social Networks</title>
      <link>https://arxiv.org/abs/2501.02609</link>
      <description>arXiv:2501.02609v1 Announce Type: cross 
Abstract: People are influenced by their peers when making decisions. In this paper, we study the linear-in-means model which is the standard empirical model of peer effects. As data on the underlying social network is often difficult to come by, we focus on data that only captures an agent's choices. Under exogenous agent participation variation, we study two questions. We first develop a revealed preference style test for the linear-in-means model. We then study the identification properties of the linear-in-means model. With sufficient participation variation, we show how an analyst is able to recover the underlying network structure and social influence parameters from choice data. Our identification result holds when we allow the social network to vary across contexts. To recover predictive power, we consider a refinement which allows us to extrapolate the underlying network structure across groups and provide a test of this version of the model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02609v1</guid>
      <category>econ.TH</category>
      <category>econ.EM</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Christopher P. Chambers, Yusufcan Masatlioglu, Christopher Turansick</dc:creator>
    </item>
    <item>
      <title>Re-examining Granger Causality from Causal Bayesian Networks Perspective</title>
      <link>https://arxiv.org/abs/2501.02672</link>
      <description>arXiv:2501.02672v1 Announce Type: cross 
Abstract: Characterizing cause-effect relationships in complex systems could be critical to understanding these systems. For many, Granger causality (GC) remains a computational tool of choice to identify causal relations in time series data. Like other causal discovery tools, GC has limitations and has been criticized as a non-causal framework. Here, we addressed one of the recurring criticisms of GC by endowing it with proper causal interpretation. This was achieved by analyzing GC from Reichenbach's Common Cause Principles (RCCPs) and causal Bayesian networks (CBNs) lenses. We showed theoretically and graphically that this reformulation endowed GC with a proper causal interpretation under certain assumptions and achieved satisfactory results on simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02672v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>S. A. Adedayo</dc:creator>
    </item>
    <item>
      <title>Identifying the Hidden Nexus between Benford Law Establishment in Stock Market and Market Efficiency: An Empirical Investigation</title>
      <link>https://arxiv.org/abs/2501.02674</link>
      <description>arXiv:2501.02674v1 Announce Type: cross 
Abstract: Benford's law, or the law of the first significant digit, has been subjected to numerous studies due to its unique applications in financial fields, especially accounting and auditing. However, studies that addressed the law's establishment in the stock markets generally concluded that stock prices do not comply with the underlying distribution. The present research, emphasizing data randomness as the underlying assumption of Benford's law, has conducted an empirical investigation of the Warsaw Stock Exchange. The outcomes demonstrated that since stock prices are not distributed randomly, the law cannot be held in the stock market. Besides, the Chi-square goodness-of-fit test also supported the obtained results. Moreover, it is discussed that the lack of randomness originated from market inefficiency. In other words, violating the efficient market hypothesis has caused the time series non-randomness and the failure to establish Benford's law.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02674v1</guid>
      <category>econ.GN</category>
      <category>econ.EM</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>M. R. Sarkandiz (University of Palermo)</dc:creator>
    </item>
    <item>
      <title>A data-driven merit order: Learning a fundamental electricity price model</title>
      <link>https://arxiv.org/abs/2501.02963</link>
      <description>arXiv:2501.02963v1 Announce Type: cross 
Abstract: Power prices can be forecasted using data-driven models or fundamental models. Data-driven models learn from historical patterns, while fundamental models simulate electricity markets. Traditionally, fundamental models have been too computationally demanding to allow for intrinsic parameter estimation or frequent updates, which are essential for short-term forecasting. In this paper, we propose a novel data-driven fundamental model that combines the strengths of both approaches. We estimate the parameters of a fully fundamental merit order model using historical data, similar to how data-driven models work. This removes the need for fixed technical parameters or expert assumptions, allowing most parameters to be calibrated directly to observations. The model is efficient enough for quick parameter estimation and forecast generation. We apply it to forecast German day-ahead electricity prices and demonstrate that it outperforms both classical fundamental and purely data-driven models. The hybrid model effectively captures price volatility and sequential price clusters, which are becoming increasingly important with the expansion of renewable energy sources. It also provides valuable insights, such as fuel switches, marginal power plant contributions, estimated parameters, dispatched plants, and power generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02963v1</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul Ghelasi, Florian Ziel</dc:creator>
    </item>
    <item>
      <title>GDP nowcasting with artificial neural networks: How much does long-term memory matter?</title>
      <link>https://arxiv.org/abs/2304.05805</link>
      <description>arXiv:2304.05805v4 Announce Type: replace 
Abstract: We apply artificial neural networks (ANNs) to nowcast quarterly GDP growth for the U.S. economy. Using the monthly FRED-MD database, we compare the nowcasting performance of five different ANN architectures: the multilayer perceptron (MLP), the one-dimensional convolutional neural network (1D CNN), the Elman recurrent neural network (RNN), the long short-term memory network (LSTM), and the gated recurrent unit (GRU). The empirical analysis presents results from two distinctively different evaluation periods. The first (2012:Q1 -- 2019:Q4) is characterized by balanced economic growth, while the second (2012:Q1 -- 2024:Q2) also includes periods of the COVID-19 recession. During the first evaluation period, longer input sequences slightly improve nowcasting performance for some ANNs, but the best accuracy is still achieved with 8-month-long input sequences at the end of the nowcasting window. Results from the second test period depict the role of long-term memory even more clearly. The MLP, the 1D CNN, and the Elman RNN work best with 8-month-long input sequences at each step of the nowcasting window. The relatively weak performance of the gated RNNs also suggests that architectural features enabling long-term memory do not result in more accurate nowcasts for GDP growth. The combined results indicate that the 1D CNN seems to represent a \textit{``sweet spot''} between the simple time-agnostic MLP and the more complex (gated) RNNs. The network generates nearly as accurate nowcasts as the best competitor for the first test period, while it achieves the overall best accuracy during the second evaluation period. Consequently, as a first in the literature, we propose the application of the 1D CNN for economic nowcasting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.05805v4</guid>
      <category>econ.EM</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Krist\'of N\'emeth, D\'aniel Hadh\'azi</dc:creator>
    </item>
    <item>
      <title>Least squares estimation in nonstationary nonlinear cohort panels with learning from experience</title>
      <link>https://arxiv.org/abs/2309.08982</link>
      <description>arXiv:2309.08982v4 Announce Type: replace 
Abstract: We discuss techniques of estimation and inference for nonstationary nonlinear cohort panels with learning from experience, showing, inter alia, the consistency and asymptotic normality of the nonlinear least squares estimator used in empirical practice. Potential pitfalls for hypothesis testing are identified and solutions proposed. Monte Carlo simulations verify the properties of the estimator and corresponding test statistics in finite samples, while an application to a panel of survey expectations demonstrates the usefulness of the theory developed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.08982v4</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alexander Mayer, Michael Massmann</dc:creator>
    </item>
    <item>
      <title>Robustify and Tighten the Lee Bounds: A Sample Selection Model under Stochastic Monotonicity and Symmetry Assumptions</title>
      <link>https://arxiv.org/abs/2311.00439</link>
      <description>arXiv:2311.00439v4 Announce Type: replace 
Abstract: In the presence of sample selection, Lee's (2009) nonparametric bounds are a popular tool for estimating a treatment effect. However, the Lee bounds rely on the monotonicity assumption, whose empirical validity is sometimes unclear. Furthermore, the bounds are often regarded to be wide and less informative even under monotonicity. To address these issues, this study introduces a stochastic version of the monotonicity assumption alongside a nonparametric distributional shape constraint. The former enhances the robustness of the Lee bounds with respect to monotonicity, while the latter helps tighten these bounds. The obtained bounds do not rely on the exclusion restriction and can be root-$n$ consistently estimable, making them practically viable. The potential usefulness of the proposed methods is illustrated by their application on experimental data from the after-school instruction programme studied by Muralidharan, Singh, and Ganimian (2019).</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.00439v4</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuta Okamoto</dc:creator>
    </item>
    <item>
      <title>Was Javert right to be suspicious? Unpacking treatment effect heterogeneity of alternative sentences on time-to-recidivism in Brazil</title>
      <link>https://arxiv.org/abs/2311.13969</link>
      <description>arXiv:2311.13969v4 Announce Type: replace 
Abstract: This paper presents econometric tools to unpack the treatment effect heterogeneity of punishing misdemeanor offenses on time-to-recidivism. We show how one can identify, estimate, and make inferences on the distributional, quantile, and average marginal treatment effects in setups where the treatment selection is endogenous and the outcome of interest, usually a duration variable, is potentially right-censored. We explore our proposed econometric methodology to evaluate the effect of fines and community service sentences as a form of punishment on time-to-recidivism in the State of S\~ao Paulo, Brazil, between 2010 and 2019, leveraging the as-if random assignment of judges to cases. Our results highlight substantial treatment effect heterogeneity that other tools are not meant to capture. For instance, we find that people whom most judges would punish take longer to recidivate as a consequence of the punishment, while people who would be punished only by strict judges recidivate at an earlier date than if they were not punished.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.13969v4</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Santiago Acerenza, Vitor Possebom, Pedro H. C. Sant'Anna</dc:creator>
    </item>
    <item>
      <title>Inference on many jumps in nonparametric panel regression models</title>
      <link>https://arxiv.org/abs/2312.01162</link>
      <description>arXiv:2312.01162v3 Announce Type: replace 
Abstract: We investigate the significance of change-points within fully nonparametric regression contexts, with a particular focus on panel data where data generation processes vary across units, and error terms may display complex dependency structures. In our setting the threshold effect depends on one specific covariate, and we permit the true nonparametric regression to vary based on additional (latent) variables. We propose two uniform testing procedures: one to assess the existence of change-points and another to evaluate the uniformity of such effects across units. Our approach involves deriving a straightforward analytical expression to approximate the variance-covariance structure of change-point effects under general dependency conditions. Notably, when Gaussian approximations are made to these test statistics, the intricate dependency structures within the data can be safely disregarded owing to the localized nature of the statistics. This finding bears significant implications for obtaining critical values. Through extensive simulations, we demonstrate that our tests exhibit excellent control over size and reasonable power performance in finite samples, irrespective of strong cross-sectional and weak serial dependency within the data. Furthermore, applying our tests to two datasets reveals the existence of significant nonsmooth effects in both cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.01162v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Likai Chen, Georg Keilbar, Liangjun Su, Weining Wang</dc:creator>
    </item>
    <item>
      <title>Macroeconomic Forecasting with Large Language Models</title>
      <link>https://arxiv.org/abs/2407.00890</link>
      <description>arXiv:2407.00890v2 Announce Type: replace 
Abstract: This paper presents a comparative analysis evaluating the accuracy of Large Language Models (LLMs) against traditional macro time series forecasting approaches. In recent times, LLMs have surged in popularity for forecasting due to their ability to capture intricate patterns in data and quickly adapt across very different domains. However, their effectiveness in forecasting macroeconomic time series data compared to conventional methods remains an area of interest. To address this, we conduct a rigorous evaluation of LLMs against traditional macro forecasting methods, using as common ground the FRED-MD database. Our findings provide valuable insights into the strengths and limitations of LLMs in forecasting macroeconomic time series, shedding light on their applicability in real-world scenarios</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00890v2</guid>
      <category>econ.EM</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrea Carriero, Davide Pettenuzzo, Shubhranshu Shekhar</dc:creator>
    </item>
    <item>
      <title>Finely Stratified Rerandomization Designs</title>
      <link>https://arxiv.org/abs/2407.03279</link>
      <description>arXiv:2407.03279v3 Announce Type: replace 
Abstract: We study estimation and inference on causal parameters under finely stratified rerandomization designs, which use baseline covariates to match units into groups (e.g. matched pairs), then rerandomize within-group treatment assignments until a balance criterion is satisfied. We show that finely stratified rerandomization does partially linear regression adjustment by design, providing nonparametric control over the stratified covariates and linear control over the rerandomized covariates. We introduce several new forms of rerandomization, allowing for imbalance metrics based on nonlinear estimators, and proposing a minimax scheme that minimizes the computational cost of rerandomization subject to a bound on estimation error. While the asymptotic distribution of GMM estimators under stratified rerandomization is generically non-normal, we show how to restore asymptotic normality using ex-post linear adjustment tailored to the stratification. We derive new variance bounds that enable conservative inference on finite population causal parameters, and provide asymptotically exact inference on their superpopulation counterparts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03279v3</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Max Cytrynbaum</dc:creator>
    </item>
    <item>
      <title>Copula Central Asymmetry of Equity Portfolios</title>
      <link>https://arxiv.org/abs/2501.00634</link>
      <description>arXiv:2501.00634v2 Announce Type: replace 
Abstract: Financial crises are usually associated with increased cross-sectional dependence between asset returns, causing asymmetry between the lower and upper tail of return distribution. The detection of asymmetric dependence is now understood to be essential for market supervision, risk management, and portfolio allocation. I propose a non-parametric test procedure for the hypothesis of copula central symmetry based on the Cram\'er-von Mises distance of the empirical copula and its survival counterpart, deriving the asymptotic properties of the test under standard assumptions for stationary time series. I use the powerful tie-break bootstrap that, as the included simulation study implies, allows me to detect asymmetries with up to 25 series and the number of observations corresponding to one year of daily returns. Applying the procedure to US portfolio returns separately for each year shows that the amount of copula central asymmetry is time-varying and less present in the recent past. Asymmetry is more critical in portfolios based on size and less in portfolios based on book-to-market and momentum. In portfolios based on industry classification, asymmetry is present during market downturns, coherently with the financial contagion narrative.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00634v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lorenzo Frattarolo</dc:creator>
    </item>
    <item>
      <title>Is completeness necessary? Estimation in nonidentified linear models</title>
      <link>https://arxiv.org/abs/1709.03473</link>
      <description>arXiv:1709.03473v5 Announce Type: replace-cross 
Abstract: Modern data analysis depends increasingly on estimating models via flexible high-dimensional or nonparametric machine learning methods, where the identification of structural parameters is often challenging and untestable. In linear settings, this identification hinges on the completeness condition, which requires the nonsingularity of a high-dimensional matrix or operator and may fail for finite samples or even at the population level. Regularized estimators provide a solution by enabling consistent estimation of structural or average structural functions, sometimes even under identification failure. We show that the asymptotic distribution in these cases can be nonstandard. We develop a comprehensive theory of regularized estimators, which include methods such as high-dimensional ridge regularization, gradient descent, and principal component analysis (PCA). The results are illustrated for high-dimensional and nonparametric instrumental variable regressions and are supported through simulation experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:1709.03473v5</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrii Babii, Jean-Pierre Florens</dc:creator>
    </item>
    <item>
      <title>Grid-level impacts of renewable energy on thermal generation: efficiency, emissions and flexibility</title>
      <link>https://arxiv.org/abs/2501.01954</link>
      <description>arXiv:2501.01954v2 Announce Type: replace-cross 
Abstract: Wind and solar generation constitute an increasing share of electricity supply globally. We find that this leads to shifts in the operational dynamics of thermal power plants. Using fixed effects panel regression across seven major U.S. balancing authorities, we analyze the impact of renewable generation on coal, natural gas combined cycle plants, and natural gas combustion turbines. Wind generation consistently displaces thermal output, while effects from solar vary significantly by region, achieving substantial displacement in areas with high solar penetration such as the California Independent System Operator but limited impacts in coal reliant grids such as the Midcontinent Independent System Operator. Renewable energy sources effectively reduce carbon dioxide emissions in regions with flexible thermal plants, achieving displacement effectiveness as high as one hundred and two percent in the California Independent System Operator and the Electric Reliability Council of Texas. However, in coal heavy areas such as the Midcontinent Independent System Operator and the Pennsylvania New Jersey Maryland Interconnection, inefficiencies from ramping and cycling reduce carbon dioxide displacement to as low as seventeen percent and often lead to elevated nitrogen oxides and sulfur dioxide emissions. These findings underscore the critical role of grid design, fuel mix, and operational flexibility in shaping the emissions benefits of renewables. Targeted interventions, including retrofitting high emitting plants and deploying energy storage, are essential to maximize emissions reductions and support the decarbonization of electricity systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01954v2</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dhruv Suri, Jacques de Chalendar, Ines Azevedo</dc:creator>
    </item>
  </channel>
</rss>
