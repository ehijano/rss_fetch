<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 29 Apr 2024 04:00:50 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 29 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Notes on the Practical Application of Nested Sampling: MultiNest, (Non)convergence, and Rectification</title>
      <link>https://arxiv.org/abs/2404.16928</link>
      <description>arXiv:2404.16928v1 Announce Type: cross 
Abstract: Nested sampling is a promising tool for Bayesian statistical analysis because it simultaneously performs parameter estimation and facilitates model comparison. MultiNest is one of the most popular nested sampling implementations, and has been applied to a wide variety of problems in the physical sciences. However, MultiNest results are frequently unreliable, and accompanying convergence tests are a necessary component of any analysis. Using simple, analytically tractable test problems, I illustrate how MultiNest (1) can produce systematically biased estimates of the Bayesian evidence, which are more significantly biased for problems of higher dimensionality; (2) can derive posterior estimates with errors on the order of $\sim100\%$; (3) is more likely to underestimate the width of a credible interval than to overestimate it - to a minor degree for smooth problems, but much more so when sampling noisy likelihoods. Nevertheless, I show how MultiNest can be used to jump-start Markov chain Monte Carlo sampling or more rigorous nested sampling techniques, potentially accelerating more trustworthy measurements of posterior distributions and Bayesian evidences, and overcoming the challenge of Markov chain Monte Carlo initialization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16928v1</guid>
      <category>astro-ph.IM</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander J. Dittmann</dc:creator>
    </item>
    <item>
      <title>Bayesian Federated Inference for Survival Models</title>
      <link>https://arxiv.org/abs/2404.17464</link>
      <description>arXiv:2404.17464v1 Announce Type: cross 
Abstract: In cancer research, overall survival and progression free survival are often analyzed with the Cox model. To estimate accurately the parameters in the model, sufficient data and, more importantly, sufficient events need to be observed. In practice, this is often a problem. Merging data sets from different medical centers may help, but this is not always possible due to strict privacy legislation and logistic difficulties. Recently, the Bayesian Federated Inference (BFI) strategy for generalized linear models was proposed. With this strategy the statistical analyses are performed in the local centers where the data were collected (or stored) and only the inference results are combined to a single estimated model; merging data is not necessary. The BFI methodology aims to compute from the separate inference results in the local centers what would have been obtained if the analysis had been based on the merged data sets. In this paper we generalize the BFI methodology as initially developed for generalized linear models to survival models. Simulation studies and real data analyses show excellent performance; i.e., the results obtained with the BFI methodology are very similar to the results obtained by analyzing the merged data. An R package for doing the analyses is available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17464v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hassan Pazira, Emanuele Massa, Jetty AM Weijers, Anthony CC Coolen, Marianne A Jonker</dc:creator>
    </item>
    <item>
      <title>A comparison of the discrimination performance of lasso and maximum likelihood estimation in logistic regression model</title>
      <link>https://arxiv.org/abs/2404.17482</link>
      <description>arXiv:2404.17482v1 Announce Type: cross 
Abstract: Logistic regression is widely used in many areas of knowledge. Several works compare the performance of lasso and maximum likelihood estimation in logistic regression. However, part of these works do not perform simulation studies and the remaining ones do not consider scenarios in which the ratio of the number of covariates to sample size is high. In this work, we compare the discrimination performance of lasso and maximum likelihood estimation in logistic regression using simulation studies and applications. Variable selection is done both by lasso and by stepwise when maximum likelihood estimation is used. We consider a wide range of values for the ratio of the number of covariates to sample size. The main conclusion of the work is that lasso has a better discrimination performance than maximum likelihood estimation when the ratio of the number of covariates to sample size is high.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17482v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gilberto P. Alc\^antara Junior, Gustavo H. A. Pereira</dc:creator>
    </item>
    <item>
      <title>Bayesian Machine Learning meets Formal Methods: An application to spatio-temporal data</title>
      <link>https://arxiv.org/abs/2110.01360</link>
      <description>arXiv:2110.01360v3 Announce Type: replace 
Abstract: We propose an interdisciplinary framework that combines Bayesian predictive inference, a well-established tool in Machine Learning, with Formal Methods rooted in the computer science community. Bayesian predictive inference allows for coherently incorporating uncertainty about unknown quantities by making use of methods or models that produce predictive distributions, which in turn inform decision problems. By formalizing these decision problems into properties with the help of spatio-temporal logic, we can formulate and predict how likely such properties are to be satisfied in the future at a certain location. Moreover, we can leverage our methodology to evaluate and compare models directly on their ability to predict the satisfaction of application-driven properties. The approach is illustrated in an urban mobility application, where the crowdedness in the center of Milan is proxied by aggregated mobile phone traffic data. We specify several desirable spatio-temporal properties related to city crowdedness such as a fault-tolerant network or the reachability of hospitals. After verifying these properties on draws from the posterior predictive distributions, we compare several spatio-temporal Bayesian models based on their overall and property-based predictive performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.01360v3</guid>
      <category>stat.CO</category>
      <category>cs.LO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Laura Vana, Ennio Visconti, Laura Nenzi, Annalisa Cadonna, Gregor Kastner</dc:creator>
    </item>
    <item>
      <title>Assigning Stationary Distributions to Sparse Stochastic Matrices</title>
      <link>https://arxiv.org/abs/2312.16011</link>
      <description>arXiv:2312.16011v3 Announce Type: replace-cross 
Abstract: The target stationary distribution problem (TSDP) is the following: given an irreducible stochastic matrix $G$ and a target stationary distribution $\hat \mu$, construct a minimum norm perturbation, $\Delta$, such that $\hat G = G+\Delta$ is also stochastic and has the prescribed target stationary distribution, $\hat \mu$. In this paper, we revisit the TSDP under a constraint on the support of $\Delta$, that is, on the set of non-zero entries of $\Delta$. This is particularly meaningful in practice since one cannot typically modify all entries of $G$. We first show how to construct a feasible solution $\hat G$ that has essentially the same support as the matrix $G$. Then we show how to compute globally optimal and sparse solutions using the component-wise $\ell_1$ norm and linear optimization. We propose an efficient implementation that relies on a column-generation approach which allows us to solve sparse problems of size up to $10^5 \times 10^5$ in a few minutes. We illustrate the proposed algorithms with several numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.16011v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Gillis, Paul Van Dooren</dc:creator>
    </item>
    <item>
      <title>A Weibull Mixture Cure Frailty Model for High-dimensional Covariates</title>
      <link>https://arxiv.org/abs/2401.06575</link>
      <description>arXiv:2401.06575v2 Announce Type: replace-cross 
Abstract: A novel mixture cure frailty model is introduced for handling censored survival data. Mixture cure models are preferable when the existence of a cured fraction among patients can be assumed. However, such models are heavily underexplored: frailty structures within cure models remain largely undeveloped, and furthermore, most existing methods do not work for high-dimensional datasets, when the number of predictors is significantly larger than the number of observations. In this study, we introduce a novel extension of the Weibull mixture cure model that incorporates a frailty component, employed to model an underlying latent population heterogeneity with respect to the outcome risk. Additionally, high-dimensional covariates are integrated into both the cure rate and survival part of the model, providing a comprehensive approach to employ the model in the context of high-dimensional omics data. We also perform variable selection via an adaptive elastic-net penalization, and propose a novel approach to inference using the expectation-maximization (EM) algorithm. Extensive simulation studies are conducted across various scenarios to demonstrate the performance of the model, and results indicate that our proposed method outperforms competitor models. We apply the novel approach to analyze RNAseq gene expression data from bulk breast cancer patients included in The Cancer Genome Atlas (TCGA) database. A set of prognostic biomarkers is then derived from selected genes, and subsequently validated via both functional enrichment analysis and comparison to the existing biological literature. Finally, a prognostic risk score index based on the identified biomarkers is proposed and validated by exploring the patients' survival.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.06575v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fatih K{\i}z{\i}laslan, David Michael Swanson, Valeria Vitelli</dc:creator>
    </item>
    <item>
      <title>Leveraging Quadratic Polynomials in Python for Advanced Data Analysis</title>
      <link>https://arxiv.org/abs/2402.06133</link>
      <description>arXiv:2402.06133v2 Announce Type: replace-cross 
Abstract: Objectives: This study aims to provide a comprehensive overview of the role of quadratic polynomials in data modeling and analysis, particularly in representing the curvature of natural phenomena. Methods: We begin with a fundamental explanation of quadratic polynomials and describe their general forms and theoretical significance. We then explored the application of these polynomials in regression analysis, detailing the process of fitting quadratic models to the data using Python libraries NumPy and Matplotlib. The methodology also included calculation of the coefficient of determination (R-squared) to evaluate the polynomial model fit. Results: Using practical examples accompanied by Python scripts, this study demonstrated the application of quadratic polynomials to analyze data patterns. These examples illustrate the utility of quadratic models in applied analytics. Conclusions: This study bridges the gap between theoretical mathematical concepts and practical data analysis, thereby enhancing the understanding and interpretation of the data patterns. Furthermore, its implementation in Python, released under MIT license, offers an accessible tool for public use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.06133v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rostyslav Sipakov, Olena Voloshkina, Anastasiia Kovalova</dc:creator>
    </item>
  </channel>
</rss>
