<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 16 Jul 2024 02:45:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 15 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Comparison of Optimizers for Fault Isolation and Diagnostics of Control Rod Drives</title>
      <link>https://arxiv.org/abs/2407.06557</link>
      <description>arXiv:2407.06557v1 Announce Type: cross 
Abstract: This paper explores the optimization of fault detection and diagnostics (FDD) in the Control Rod Drive System (CRDS) of GE-Hitachi's BWRX-300 small modular reactor (SMR), focusing on the electrically powered fine motion control rod drive (FMCRD) servomotors. Leveraging the coordinated motion of multiple FMCRDs for control rod adjustments, the study proposes a deep learning approach, utilizing one-dimensional convolutional neural network (1D CNN)-based autoencoders for anomaly detection and encoder-decoder structured 1D CNN classifiers for fault classification. Simulink models simulate normal and fault operations, monitoring electric current and electromagnetic torque. The training of the fault isolation and fault classification models is optimized. Various optimizers, including Adaptive Moment Estimation (Adam), Nesterov Adam (Nadam), Stochastic Gradient Descent (SGD), and Root Mean Square Propagation (RMSProp), are evaluated, with Nadam demonstrating a relatively superior performance across the isolation and classification tasks due to its adaptive gradient and Nesterov components. The research underscores the importance of considering the number of runs (each run has a different set of initial model parameters) as a hyperparameter during empirical optimizer comparisons and contributes insights crucial for enhancing FDD in SMR control systems and for the application of 1D CNN to FDD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06557v1</guid>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ark Ifeanyi, Jamie Coble</dc:creator>
    </item>
    <item>
      <title>ROLCH: Regularized Online Learning for Conditional Heteroskedasticity</title>
      <link>https://arxiv.org/abs/2407.08750</link>
      <description>arXiv:2407.08750v1 Announce Type: cross 
Abstract: Large-scale streaming data are common in modern machine learning applications and have led to the development of online learning algorithms. Many fields, such as supply chain management, weather and meteorology, energy markets, and finance, have pivoted towards using probabilistic forecasts, which yields the need not only for accurate learning of the expected value but also for learning the conditional heteroskedasticity. Against this backdrop, we present a methodology for online estimation of regularized linear distributional models for conditional heteroskedasticity. The proposed algorithm is based on a combination of recent developments for the online estimation of LASSO models and the well-known GAMLSS framework. We provide a case study on day-ahead electricity price forecasting, in which we show the competitive performance of the adaptive estimation combined with strongly reduced computational effort. Our algorithms are implemented in a computationally efficient Python package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08750v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simon Hirsch, Jonathan Berrisch, Florian Ziel</dc:creator>
    </item>
    <item>
      <title>Estimating Methane Emissions from the Upstream Oil and Gas Industry Using a Multi-Stage Framework</title>
      <link>https://arxiv.org/abs/2407.08827</link>
      <description>arXiv:2407.08827v1 Announce Type: cross 
Abstract: Measurement-based methane inventories, which involve surveying oil and gas facilities and compiling data to estimate methane emissions, are becoming the gold standard for quantifying emissions. However, there is a current lack of statistical guidance for the design and analysis of such surveys. The only existing method is a Monte Carlo procedure which is difficult to interpret, computationally intensive, and lacks available open-source code for its implementation. We provide an alternative method by framing methane surveys in the context of multi-stage sampling designs. We contribute estimators of the total emissions along with variance estimators which do not require simulation, as well as stratum-level total estimators. We show that the variance contribution from each stage of sampling can be estimated to inform the design of future surveys. We also introduce a more efficient modification of the estimator. Finally, we propose combining the multi-stage approach with a simple Monte Carlo procedure to model measurement error. The resulting methods are interpretable and require minimal computational resources. We apply the methods to aerial survey data of oil and gas facilities in British Columbia, Canada, to estimate the methane emissions in the province. An R package is provided to facilitate the use of the methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08827v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Augustine Wigle, Audrey Beliveau</dc:creator>
    </item>
    <item>
      <title>Computationally efficient and statistically accurate conditional independence testing with spaCRT</title>
      <link>https://arxiv.org/abs/2407.08911</link>
      <description>arXiv:2407.08911v1 Announce Type: cross 
Abstract: We introduce the saddlepoint approximation-based conditional randomization test (spaCRT), a novel conditional independence test that effectively balances statistical accuracy and computational efficiency, inspired by applications to single-cell CRISPR screens. Resampling-based methods like the distilled conditional randomization test (dCRT) offer statistical precision but at a high computational cost. The spaCRT leverages a saddlepoint approximation to the resampling distribution of the dCRT test statistic, achieving very similar finite-sample statistical performance with significantly reduced computational demands. We prove that the spaCRT p-value approximates the dCRT p-value with vanishing relative error, and that these two tests are asymptotically equivalent. Through extensive simulations and real data analysis, we demonstrate that the spaCRT controls Type-I error and maintains high power, outperforming other asymptotic and resampling-based tests. Our method is particularly well-suited for large-scale single-cell CRISPR screen analyses, facilitating the efficient and accurate assessment of perturbation-gene associations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08911v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziang Niu, Jyotishka Ray Choudhury, Eugene Katsevich</dc:creator>
    </item>
    <item>
      <title>Nationwide frequency-dependent seismic site amplification models for Iceland</title>
      <link>https://arxiv.org/abs/2407.09338</link>
      <description>arXiv:2407.09338v1 Announce Type: cross 
Abstract: Seismic wave amplification due to localized site conditions is an important aspect of regional seismic hazard assessment. Without systematic studies of frequency-dependent site-effects during strong Icelandic earthquakes, various local site proxies of large-scale studies in other seismic regions have been used in Iceland. Recently, earthquake site-effects were rigorously quantified for 34 stations in Southwest Iceland for the first time and correlated to distinct Icelandic geological units of hard rock, rock, lava rock, and sedimentary soil. These units are prevalent throughout Iceland and herein we present 1) nationwide maps of proxies (slope, Vs30, geological units) that may contribute to a better estimation of site effects and associated, 2) frequency-dependent site-amplification maps of Iceland. The frequency-dependent site factors for each geological unit are presented at 1-30 Hz and PGA. Finally, we generate site amplification maps based on recent large-scale models developed in other seismic regions (ESRM20) and various site proxies they are based on (geology- and slope-based inferred Vs30, geomorphological sedimentary thickness). We compare site-proxy maps and amplification maps from both Icelandic and large-scale, non-Icelandic, models. Neither spatial patterns nor amplification levels in either proxy or amplification maps from large-scale non-Icelandic studies resemble those observed from local quantitative strong-motion research as presented in this study. We attribute the discrepancy primarily to the young geology of Iceland and its formation history. Additionally, we compare model performance across frequencies by assessing the bias of model predictions against empirical site amplifications in the South Iceland Seismic Zone, accounting for site-to-site variability of residuals indicating the superior performance of the local amplification model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09338v1</guid>
      <category>physics.geo-ph</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.soildyn.2024.108798</arxiv:DOI>
      <arxiv:journal_reference>Soil Dynamics and Earthquake Engineering, Volume 183, 2024, 108798, ISSN 0267-7261,</arxiv:journal_reference>
      <dc:creator>Atefe Darzi, Benedikt Halldorsson, Fabrice Cotton, Sahar Rahpeyma</dc:creator>
    </item>
    <item>
      <title>Calibrating dimension reduction hyperparameters in the presence of noise</title>
      <link>https://arxiv.org/abs/2312.02946</link>
      <description>arXiv:2312.02946v5 Announce Type: replace 
Abstract: The goal of dimension reduction tools is to construct a low-dimensional representation of high-dimensional data. These tools are employed for a variety of reasons such as noise reduction, visualization, and to lower computational costs. However, there is a fundamental issue that is discussed in other modeling problems that is often overlooked in dimension reduction -- overfitting. In the context of other modeling problems, techniques such as feature-selection, cross-validation, and regularization are employed to combat overfitting, but rarely are such precautions taken when applying dimension reduction. Prior applications of the two most popular non-linear dimension reduction methods, t-SNE and UMAP, fail to acknowledge data as a combination of signal and noise when assessing performance. These methods are typically calibrated to capture the entirety of the data, not just the signal. In this paper, we demonstrate the importance of acknowledging noise when calibrating hyperparameters and present a framework that enables users to do so. We use this framework to explore the role hyperparameter calibration plays in overfitting the data when applying t-SNE and UMAP. More specifically, we show previously recommended values for perplexity and n_neighbors are too small and overfit the noise. We also provide a workflow others may use to calibrate hyperparameters in the presence of noise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.02946v5</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Justin Lin, Julia Fukuyama</dc:creator>
    </item>
    <item>
      <title>Question-Score Identity Detection (Q-SID): A Statistical Algorithm to Detect Collusion Groups with Error Quantification from Exam Question Scores</title>
      <link>https://arxiv.org/abs/2407.07420</link>
      <description>arXiv:2407.07420v2 Announce Type: replace 
Abstract: Collusion between students in online exams is a major problem that undermines the integrity of the exam results. Although there exist methods that use exam data to identify pairs of students who have likely copied each other's answers, these methods are restricted to specific formats of multiple-choice exams. Here we present a statistical algorithm, Q-SID, that efficiently detects groups of students who likely have colluded, i.e., collusion groups, with error quantification. Q-SID uses graded numeric question scores only, so it works for many formats of multiple-choice and non-multiple-choice exams. Q-SID reports two false-positive rates (FPRs) for each collusion group: (1) empirical FPR, whose null data are from 36 strictly proctored exam datasets independent of the user-input exam data and (2) synthetic FPR, whose null data are simulated from a copula-based probabilistic model, which is first fitted to the user-input exam data and then modified to have no collusion. On 34 unproctored exam datasets, including two benchmark datasets with true positives and negatives verified by textural analysis, we demonstrate that Q-SID is a collusion detection algorithm with powerful and robust performance across exam formats, numbers of questions and students, and exam complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07420v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Guanao Yan, Jingyi Jessica Li, Mark D. Biggin</dc:creator>
    </item>
  </channel>
</rss>
