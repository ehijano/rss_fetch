<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 06 Aug 2025 04:00:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Low-Communication Resilient Distributed Estimation Algorithm Based on Memory Mechanism</title>
      <link>https://arxiv.org/abs/2508.02705</link>
      <description>arXiv:2508.02705v1 Announce Type: new 
Abstract: In multi-task adversarial networks, the accurate estimation of unknown parameters in a distributed algorithm is hindered by attacked nodes or links. To tackle this challenge, this brief proposes a low-communication resilient distributed estimation algorithm. First, a node selection strategy based on reputation is introduced that allows nodes to communicate with more reliable subset of neighbors. Subsequently, to discern trustworthy intermediate estimates, the Weighted Support Vector Data Description (W-SVDD) model is employed to train the memory data. This trained model contributes to reinforce the resilience of the distributed estimation process against the impact of attacked nodes or links. Additionally, an event-triggered mechanism is introduced to minimize ineffective updates to the W-SVDD model, and a suitable threshold is derived based on assumptions. The convergence of the algorithm is analyzed. Finally, simulation results demonstrate that the proposed algorithm achieves superior performance with less communication cost compared to other algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02705v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Li, Limei Hu, Feng Chen, Ye Yao</dc:creator>
    </item>
    <item>
      <title>A DataOps Toolbox Enabling Continuous Semantic Integration of Devices for Edge-Cloud AI Applications</title>
      <link>https://arxiv.org/abs/2508.02708</link>
      <description>arXiv:2508.02708v1 Announce Type: new 
Abstract: The implementation of AI-based applications in complex environments often requires the collaboration of several devices spanning from edge to cloud. Identifying the required devices and configuring them to collaborate is a challenge relevant to different scenarios, like industrial shopfloors, road infrastructures, and healthcare therapies. We discuss the design and implementation of a DataOps toolbox leveraging Semantic Web technologies and a low-code mechanism to address heterogeneous data interoperability requirements in the development of such applications. The toolbox supports a continuous semantic integration approach to tackle various types of devices, data formats, and semantics, as well as different communication interfaces. The paper presents the application of the toolbox to three use cases from different domains, the DataOps pipelines implemented, and how they guarantee interoperability of static nodes' information and runtime data exchanges. Finally, we discuss the results from the piloting activities in the use cases and the lessons learned.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02708v1</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mario Scrocca, Marco Grassi, Alessio Carenini, Jean-Paul Calbimonte, Darko Anicic, Irene Celino</dc:creator>
    </item>
    <item>
      <title>PROV-AGENT: Unified Provenance for Tracking AI Agent Interactions in Agentic Workflows</title>
      <link>https://arxiv.org/abs/2508.02866</link>
      <description>arXiv:2508.02866v1 Announce Type: new 
Abstract: Foundation models, such as Large Language Models (LLMs), are increasingly used as core components of AI agents in complex, large-scale workflows across federated and heterogeneous environments. In agentic workflows, autonomous agents plan tasks, interact with humans and peers, and shape scientific outcomes. This makes transparency, traceability, reproducibility, and reliability essential. However, AI-based agents can hallucinate or reason incorrectly, and their decisions may propagate errors through the workflow, especially when one agent's output feeds into another's input. Therefore, fine-grained provenance is essential to link agent decisions, their end-to-end context, and downstream impacts. While provenance techniques have long supported reproducibility and workflow data understanding, they fail to capture and relate agent-centric metadata (prompts, responses, and decisions) with the rest of the workflow. In this paper, we introduce PROV-AGENT, a provenance model that extends W3C PROV and leverages the Model Context Protocol (MCP) to integrate agent interactions into end-to-end workflow provenance. Our contributions include: (1) a provenance model tailored for agentic workflows, (2) a near real-time, open-source system for capturing agentic provenance, and (3) a cross-facility evaluation spanning edge, cloud, and HPC environments, demonstrating support for critical provenance queries and agent reliability analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02866v1</guid>
      <category>cs.DC</category>
      <category>cs.DB</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Renan Souza, Amal Gueroudji, Stephen DeWitt, Daniel Rosendo, Tirthankar Ghosal, Robert Ross, Prasanna Balaprakash, Rafael Ferreira da Silva</dc:creator>
    </item>
    <item>
      <title>Optimal Simultaneous Byzantine Agreement, Common Knowledge and Limited Information Exchange</title>
      <link>https://arxiv.org/abs/2508.03418</link>
      <description>arXiv:2508.03418v1 Announce Type: new 
Abstract: In order to develop solutions that perform actions as early as possible, analysis of distributed algorithms using epistemic logic has generally concentrated on ``full information protocols'', which may be inefficient with respect to space and computation time. The paper reconsiders the epistemic analysis of the problem of Simultaneous Byzantine Agreement with respect to weaker, but more practical, exchanges of information. The paper first clarifies some issues concerning both the specification of this problem and the knowledge based program characterizing its solution, concerning the distinction between the notions of ``nonfaulty'' and ``not yet failed'', on which there are variances in the literature. It is then shown that, when implemented relative to a given failure model and an information exchange protocol satisfying certain conditions, this knowledge based program yields a protocol that is optimal relative to solutions using the same information exchange. Conditions are also identified under which this implementation is also an optimum, but an example is provided that shows this does not hold in general.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03418v1</guid>
      <category>cs.DC</category>
      <category>cs.LO</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ron van der Meyden</dc:creator>
    </item>
    <item>
      <title>Understanding the Landscape of Ampere GPU Memory Errors</title>
      <link>https://arxiv.org/abs/2508.03513</link>
      <description>arXiv:2508.03513v1 Announce Type: new 
Abstract: Graphics Processing Units (GPUs) have become a de facto solution for accelerating high-performance computing (HPC) applications. Understanding their memory error behavior is an essential step toward achieving efficient and reliable HPC systems. In this work, we present a large-scale cross-supercomputer study to characterize GPU memory reliability, covering three supercomputers - Delta, Polaris, and Perlmutter - all equipped with NVIDIA A100 GPUs. We examine error logs spanning 67.77 million GPU device-hours across 10,693 GPUs. We compare error rates and mean-time-between-errors (MTBE) and highlight both shared and distinct error characteristics among these three systems. Based on these observations and analyses, we discuss the implications and lessons learned, focusing on the reliable operation of supercomputers, the choice of checkpointing interval, and the comparison of reliability characteristics with those of previous-generation GPUs. Our characterization study provides valuable insights into fault-tolerant HPC system design and operation, enabling more efficient execution of HPC applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03513v1</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhu Zhu, Yu Sun, Dhatri Parakal, Bo Fang, Steven Farrell, Gregory H. Bauer, Brett Bode, Ian T. Foster, Michael E. Papka, William Gropp, Zhao Zhang, Lishan Yang</dc:creator>
    </item>
    <item>
      <title>In-Memory Non-Binary LDPC Decoding</title>
      <link>https://arxiv.org/abs/2508.03567</link>
      <description>arXiv:2508.03567v1 Announce Type: new 
Abstract: Low-density parity-check (LDPC) codes are an important feature of several communication and storage applications, offering a flexible and effective method for error correction. These codes are computationally complex and require the exploitation of parallel processing to meet real-time constraints. As advancements in arithmetic and logic unit technology allowed for higher performance of computing systems, memory technology has not kept the same pace of development, creating a data movement bottleneck and affecting parallel processing systems more dramatically. To alleviate the severity of this bottleneck, several solutions have been proposed, namely the processing in-memory (PiM) paradigm that involves the design of compute units to where (or near) the data is stored, utilizing thousands of low-complexity processing units to perform out bit-wise and simple arithmetic operations. This paper presents a novel efficient solution for near-memory non-binary LDPC decoders in the UPMEM system, for the best of our knowledge the first real hardware PiM-based non-binary LDPC decoder that is benchmarked against low-power GPU parallel solutions highly optimized for throughput performance. PiM-based non-binary LDPC decoders can achieve 76 Mbit/s of decoding throughput, which is even competitive when compared against implementations running in edge GPUs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03567v1</guid>
      <category>cs.DC</category>
      <category>cs.CE</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Oscar Ferraz, Vitor Silva, Gabriel Falcao</dc:creator>
    </item>
    <item>
      <title>Block: Balancing Load in LLM Serving with Context, Knowledge and Predictive Scheduling</title>
      <link>https://arxiv.org/abs/2508.03611</link>
      <description>arXiv:2508.03611v1 Announce Type: new 
Abstract: This paper presents Block, a distributed scheduling framework designed to optimize load balancing and auto-provisioning across instances in large language model serving frameworks by leveraging contextual information from incoming requests. Unlike popular model serving systems that rely on monolithic and heuristic task schedulers, Block operates as a fully distributed, stateless, and predictive scheduling system to achieve low overhead, reliability, and scalability. It leverages the deterministic and predictable characteristics of LLM inferences, such as host configurations, response lengths, and hardware performance, to make scheduling decisions based on accurately predicted metrics. Evaluation on a 12 GPUs cluster shows that Block significantly outperforms heuristic schedulers, boosting serving capacity by up to 16.7\% and reducing P99 tail latency by up to 49.5\%. These performance gains remain consistent across diverse models, workloads and configurations. Code and data are open-sourced.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03611v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wei Da, Evangelia Kalyvianaki</dc:creator>
    </item>
    <item>
      <title>Frontier: Simulating the Next Generation of LLM Inference Systems</title>
      <link>https://arxiv.org/abs/2508.03148</link>
      <description>arXiv:2508.03148v1 Announce Type: cross 
Abstract: Large Language Model (LLM) inference is growing increasingly complex with the rise of Mixture-of-Experts (MoE) models and disaggregated architectures that decouple components like prefill/decode (PD) or attention/FFN (AF) for heterogeneous scaling. Existing simulators, architected for co-located, dense models, are unable to capture the intricate system dynamics of these emerging paradigms. We present Frontier, a high-fidelity simulator designed from the ground up for this new landscape. Frontier introduces a unified framework to model both co-located and disaggregated systems, providing native support for MoE inference with expert parallelism (EP). It enables the simulation of complex workflows like cross-cluster expert routing and advanced pipelining strategies for latency hiding. To ensure fidelity and usability, Frontier incorporates refined operator models for improved accuracy. Frontier empowers the community to design and optimize the future of LLM inference at scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03148v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yicheng Feng, Xin Tan, Kin Hang Sew, Yimin Jiang, Yibo Zhu, Hong Xu</dc:creator>
    </item>
    <item>
      <title>Directives for Function Offloading in 5G Networks Based on a Performance Characteristics Analysis</title>
      <link>https://arxiv.org/abs/2508.03287</link>
      <description>arXiv:2508.03287v1 Announce Type: cross 
Abstract: Cloud-based offloading helps address energy consumption and performance challenges in executing resource-intensive vehicle algorithms. Utilizing 5G, with its low latency and high bandwidth, enables seamless vehicle-to-cloud integration. Currently, only non-standalone 5G is publicly available, and real-world applications remain underexplored compared to theoretical studies. This paper evaluates 5G non-standalone networks for cloud execution of vehicle functions, focusing on latency, Round Trip Time, and packet delivery. Tests used two AI-based algorithms -- emotion recognition and object recognition -- along an 8.8 km route in Baden-W\"urttemberg, Germany, encompassing urban, rural, and forested areas. Two platforms were analyzed: a cloudlet in Frankfurt and a cloud in Mannheim, employing various deployment strategies like conventional applications and containerized and container-orchestrated setups. Key findings highlight an average signal quality of 84 %, with no connectivity interruptions despite minor drops in built-up areas. Packet analysis revealed a Packet Error Rate below 0.1 % for both algorithms. Transfer times varied significantly depending on the geographical location and the backend servers' network connections, while processing times were mainly influenced by the computation hardware in use. Additionally, cloud offloading seems only be a suitable option, when a round trip time of more than 150 ms is possible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03287v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Falk Dettinger, Matthias Wei{\ss}, Daniel Baumann, Martin Sommer, Michael Weyrich</dc:creator>
    </item>
    <item>
      <title>ZenFlow: Enabling Stall-Free Offloading Training via Asynchronous Updates</title>
      <link>https://arxiv.org/abs/2505.12242</link>
      <description>arXiv:2505.12242v3 Announce Type: replace 
Abstract: Fine-tuning large language models (LLMs) often exceeds GPU memory limits, prompting systems to offload model states to CPU memory. However, existing offloaded training frameworks like ZeRO-Offload treat all parameters equally and update the full model on the CPU, causing severe GPU stalls, where fast, expensive GPUs sit idle waiting for slow CPU updates and limited-bandwidth PCIe transfers. We present ZenFlow, a new offloading framework that prioritizes important parameters and decouples updates between GPU and CPU. ZenFlow performs in-place updates of important gradients on GPU, while asynchronously offloading and accumulating less important ones on CPU, fully overlapping CPU work with GPU computation. To scale across GPUs, ZenFlow introduces a lightweight gradient selection method that exploits a novel spatial and temporal locality property of important gradients, avoiding costly global synchronization. ZenFlow achieves up to 5x end-to-end speedup, 2x lower PCIe traffic, and reduces GPU stalls by over 85 percent, all while preserving accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12242v3</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tingfeng Lan, Yusen Wu, Bin Ma, Zhaoyuan Su, Rui Yang, Tekin Bicer, Masahiro Tanaka, Olatunji Ruwase, Dong Li, Yue Cheng</dc:creator>
    </item>
    <item>
      <title>xDeepServe: Model-as-a-Service on Huawei CloudMatrix384</title>
      <link>https://arxiv.org/abs/2508.02520</link>
      <description>arXiv:2508.02520v2 Announce Type: replace 
Abstract: The rise of scaled-out LLMs and scaled-up SuperPods signals a new era in large-scale AI infrastructure. LLMs continue to scale out via MoE, as seen in recent models like DeepSeek, Kimi, and Qwen. In parallel, AI hardware is scaling up, with Huawei's CloudMatrix384 SuperPod offering hundreds of GB/s high-speed interconnects. Running large MoE models on SuperPod-scale hardware brings new challenges. It requires new execution models, scalable scheduling, efficient expert load balancing, and elimination of single points of failure. This paper presents xDeepServe, Huawei Cloud's LLM serving system designed for SuperPod-scale infrastructure. At its core is Transformerless, a disaggregated architecture that decomposes transformer models into modular units--attention, feedforward, and MoE--executed independently on NPUs connected via high-speed fabric. We implement this design in two forms: disaggregated prefill-decode and disaggregated MoE-attention. This fully disaggregated setup enables independent scaling of compute and memory without sacrificing performance. To support this architecture, we propose XCCL, a communication library that leverages CloudMatrix384's global shared memory to implement efficient point-to-point and all-to-all primitives. We also extend our serving engine FlowServe with system-level techniques, enabling scalable inference across hundreds of NPUs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02520v2</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ao Xiao, Bangzheng He, Baoquan Zhang, Baoxing Huai, Bingji Wang, Bo Wang, Bo Xu, Boyi Hou, Chan Yang, Changhong Liu, Cheng Cui, Chenyu Zhu, Cong Feng, Daohui Wang, Dayun Lin, Duo Zhao, Fengshao Zou, Fu Wang, Gangqiang Zhang, Gengyuan Dan, Guanjie Chen, Guodong Guan, Guodong Yang, Haifeng Li, Haipei Zhu, Hao Feng, Hao Huang, Hao Xu, Hengrui Ma, Hengtao Fan, Hui Liu, Jia Li, Jiang Liu, Jiang Xu, Jie Meng, Jinhan Xin, Junhao Hu, Juwei Chen, Lan Yu, Lanxin Miao, Liang Liu, Linan Jing, Lu Zhou, Meina Han, Mingkun Deng, Mingyu Deng, Naitian Deng, Nizhong Lin, Peihan Zhao, Peng Pan, Pengfei Shen, Ping Li, Qi Zhang, Qin Zhang, Qingrong Xia, Qingyi Zhang, Qunchao Fu, Ren Guo, Ruimin Gao, Shaochun Li, Sheng Long, Shentian Li, Shining Wan, Shuai Shen, Shuangfu Zeng, Shuming Jing, Siqi Yang, Song Zhang, Tao Xu, Tianlin Du, Ting Chen, Wanxu Wu, Wei Jiang, Weinan Tong, Weiwei Chen, Wen Peng, Wenli Zhou, Wenquan Yang, Wenxin Liang, Xiang Liu, Xiaoli Zhou, Xin Jin, Xinyu Duan, Xu Li, Xu Zhang, Xusheng Chen, Yalong Shan, Yang Gan, Yao Lu, Yi Deng, Yi Zheng, Yingfei Zheng, Yiyun Zheng, Yizhou Shan, Yong Gao, Yongqiang Yang, Yuanjin Gong, Yue Yu, Yuetao Chen, Yukun Zhu, Yulong He, Yusu Zhao, Yuyan Wu, Zenan Zhang, Zhaojin Zhuo, Zhaoyang Ji, Zhefeng Wang, Zheng Wang, Zhenhua Yang, Zhenli Sheng, Zhibin Yu, Zhigang Ji, Zhihao Ren, Zhipeng Bian, Zhixia Liu, Zhiyu Dong, Zhonghua Li, Zhou Yu, Zhuoming Shen, Zhuwei Peng, Zi Ye, Zihao Xiang, Zimin Fu, Zixuan Zhang</dc:creator>
    </item>
    <item>
      <title>TAPAS: Fast and Automatic Derivation of Tensor Parallel Strategies for Large Neural Networks</title>
      <link>https://arxiv.org/abs/2302.00247</link>
      <description>arXiv:2302.00247v2 Announce Type: replace-cross 
Abstract: Tensor parallelism is an essential technique for distributed training of large neural networks. However, automatically determining an optimal tensor parallel strategy is challenging due to the gigantic search space, which grows exponentially with model size and tensor dimension. This prohibits the adoption of auto-parallel systems on larger models.
  We observe that neural networks usually contain repeated substructures, and build an automatic parallelism framework named TAPAS that eliminates redundant search efforts. TAPAS employs a divide-and-conquer approach that efficiently folds the search space by identifying those unique substructures. As a result, it runs at sub-linear complexity concerning the model size, making it a scalable solution for training large-scale networks. Our evaluations demonstrate that TAPAS outperforms the state-of-the-art automatic parallelism frameworks by up to $160\times$ in search speed on a wide range of models, and the performance of derived strategies is competitive or even better compared with the expert-engineered Megatron-LM library.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.00247v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3754598.3754677</arxiv:DOI>
      <dc:creator>Ziji Shi, Le Jiang, Ang Wang, Jie Zhang, Chencan Wu, Yong Li, Xiaokui Xiao, Wei Lin, Jialin Li</dc:creator>
    </item>
    <item>
      <title>Fast algorithms for Vizing's theorem on bounded degree graphs</title>
      <link>https://arxiv.org/abs/2303.05408</link>
      <description>arXiv:2303.05408v5 Announce Type: replace-cross 
Abstract: Vizing's theorem states that every graph $G$ of maximum degree $\Delta$ can be properly edge-colored using $\Delta + 1$ colors. The fastest currently known $(\Delta+1)$-edge-coloring algorithm for general graphs is due to Sinnamon and runs in time $O(m\sqrt{n})$, where $n :=|V(G)|$ and $m :=|E(G)|$. We investigate the case when $\Delta$ is constant, i.e., $\Delta = O(1)$. In this regime, the runtime of Sinnamon's algorithm is $O(n^{3/2})$, which can be improved to $O(n \log n)$, as shown by Gabow, Nishizeki, Kariv, Leven, and Terada. Here we give an algorithm whose running time is only $O(n)$, which is obviously best possible. Prior to this work, no linear-time $(\Delta+1)$-edge-coloring algorithm was known for any $\Delta \geq 4$. Using some of the same ideas, we also develop new algorithms for $(\Delta+1)$-edge-coloring in the $\mathsf{LOCAL}$ model of distributed computation. Namely, when $\Delta$ is constant, we design a deterministic $\mathsf{LOCAL}$ algorithm with running time $\tilde{O}(\log^5 n)$ and a randomized $\mathsf{LOCAL}$ algorithm with running time $O(\log ^2 n)$. Although our focus is on the constant $\Delta$ regime, our results remain interesting for $\Delta$ up to $\log^{o(1)} n$, since the dependence of their running time on $\Delta$ is polynomial. The key new ingredient in our algorithms is a novel application of the entropy compression method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.05408v5</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anton Bernshteyn, Abhishek Dhawan</dc:creator>
    </item>
    <item>
      <title>P3SL: Personalized Privacy-Preserving Split Learning on Heterogeneous Edge Devices</title>
      <link>https://arxiv.org/abs/2507.17228</link>
      <description>arXiv:2507.17228v2 Announce Type: replace-cross 
Abstract: Split Learning (SL) is an emerging privacy-preserving machine learning technique that enables resource constrained edge devices to participate in model training by partitioning a model into client-side and server-side sub-models. While SL reduces computational overhead on edge devices, it encounters significant challenges in heterogeneous environments where devices vary in computing resources, communication capabilities, environmental conditions, and privacy requirements. Although recent studies have explored heterogeneous SL frameworks that optimize split points for devices with varying resource constraints, they often neglect personalized privacy requirements and local model customization under varying environmental conditions. To address these limitations, we propose P3SL, a Personalized Privacy-Preserving Split Learning framework designed for heterogeneous, resource-constrained edge device systems. The key contributions of this work are twofold. First, we design a personalized sequential split learning pipeline that allows each client to achieve customized privacy protection and maintain personalized local models tailored to their computational resources, environmental conditions, and privacy needs. Second, we adopt a bi-level optimization technique that empowers clients to determine their own optimal personalized split points without sharing private sensitive information (i.e., computational resources, environmental conditions, privacy requirements) with the server. This approach balances energy consumption and privacy leakage risks while maintaining high model accuracy. We implement and evaluate P3SL on a testbed consisting of 7 devices including 4 Jetson Nano P3450 devices, 2 Raspberry Pis, and 1 laptop, using diverse model architectures and datasets under varying environmental conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17228v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Fan, JinYi Yoon, Xiaochang Li, Huajie Shao, Bo Ji</dc:creator>
    </item>
    <item>
      <title>VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo</title>
      <link>https://arxiv.org/abs/2508.02317</link>
      <description>arXiv:2508.02317v2 Announce Type: replace-cross 
Abstract: Recent advances in large language models (LLMs) have driven impressive progress in omni-modal understanding and generation. However, training omni-modal LLMs remains a significant challenge due to the heterogeneous model architectures required to process diverse modalities, necessitating sophisticated system design for efficient large-scale training. Existing frameworks typically entangle model definition with parallel logic, incurring limited scalability and substantial engineering overhead for end-to-end omni-modal training. We present VeOmni, a modular and efficient training framework to accelerate the development of omni-modal LLMs. VeOmni introduces model-centric distributed recipes that decouples communication from computation, enabling efficient 3D parallelism on omni-modal LLMs. VeOmni also features a flexible configuration interface supporting seamless integration of new modalities with minimal code change. Using VeOmni, a omni-modal mixture-of-experts (MoE) model with 30B parameters can be trained with over 2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D parallelism on 128 GPUs, showcasing its superior efficiency and scalability for training large omni-modal LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02317v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Qianli Ma, Yaowei Zheng, Zhelun Shi, Zhongkai Zhao, Bin Jia, Ziyue Huang, Zhiqi Lin, Youjie Li, Jiacheng Yang, Yanghua Peng, Zhi Zhang, Xin Liu</dc:creator>
    </item>
  </channel>
</rss>
