<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GR</link>
    <description>cs.GR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 30 May 2025 04:00:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Quality assessment of 3D human animation: Subjective and objective evaluation</title>
      <link>https://arxiv.org/abs/2505.23301</link>
      <description>arXiv:2505.23301v1 Announce Type: new 
Abstract: Virtual human animations have a wide range of applications in virtual and augmented reality. While automatic generation methods of animated virtual humans have been developed, assessing their quality remains challenging. Recently, approaches introducing task-oriented evaluation metrics have been proposed, leveraging neural network training. However, quality assessment measures for animated virtual humans that are not generated with parametric body models have yet to be developed. In this context, we introduce a first such quality assessment measure leveraging a novel data-driven framework. First, we generate a dataset of virtual human animations together with their corresponding subjective realism evaluation scores collected with a user study. Second, we use the resulting dataset to learn predicting perceptual evaluation scores. Results indicate that training a linear regressor on our dataset results in a correlation of 90%, which outperforms a state of the art deep learning baseline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23301v1</guid>
      <category>cs.GR</category>
      <category>cs.CV</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rim Rekik, Stefanie Wuhrer, Ludovic Hoyet, Katja Zibrek, Anne-H\'el\`ene Olivier</dc:creator>
    </item>
    <item>
      <title>To Measure What Isn't There -- Visual Exploration of Missingness Structures Using Quality Metrics</title>
      <link>https://arxiv.org/abs/2505.23447</link>
      <description>arXiv:2505.23447v1 Announce Type: new 
Abstract: This paper contributes a set of quality metrics for identification and visual analysis of structured missingness in high-dimensional data. Missing values in data are a frequent challenge in most data generating domains and may cause a range of analysis issues. Structural missingness in data may indicate issues in data collection and pre-processing, but may also highlight important data characteristics. While research into statistical methods for dealing with missing data are mainly focusing on replacing missing values with plausible estimated values, visualization has great potential to support a more in-depth understanding of missingness structures in data. Nonetheless, while the interest in missing data visualization has increased in the last decade, it is still a relatively overlooked research topic with a comparably small number of publications, few of which address scalability issues. Efficient visual analysis approaches are needed to enable exploration of missingness structures in large and high-dimensional data, and to support informed decision-making in context of potential data quality issues. This paper suggests a set of quality metrics for identification of patterns of interest for understanding of structural missingness in data. These quality metrics can be used as guidance in visual analysis, as demonstrated through a use case exploring structural missingness in data from a real-life walking monitoring study. All supplemental materials for this paper are available at https://doi.org/10.25405/data.ncl.c.7741829.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23447v1</guid>
      <category>cs.GR</category>
      <category>cs.HC</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sara Johansson Fernstad, Sarah Alsufyani, Silvia Del Din, Alison Yarnall, Lynn Rochester</dc:creator>
    </item>
    <item>
      <title>One Trajectory, One Token: Grounded Video Tokenization via Panoptic Sub-object Trajectory</title>
      <link>https://arxiv.org/abs/2505.23617</link>
      <description>arXiv:2505.23617v1 Announce Type: cross 
Abstract: Effective video tokenization is critical for scaling transformer models for long videos. Current approaches tokenize videos using space-time patches, leading to excessive tokens and computational inefficiencies. The best token reduction strategies degrade performance and barely reduce the number of tokens when the camera moves. We introduce grounded video tokenization, a paradigm that organizes tokens based on panoptic sub-object trajectories rather than fixed patches. Our method aligns with fundamental perceptual principles, ensuring that tokenization reflects scene complexity rather than video duration. We propose TrajViT, a video encoder that extracts object trajectories and converts them into semantically meaningful tokens, significantly reducing redundancy while maintaining temporal coherence. Trained with contrastive learning, TrajViT significantly outperforms space-time ViT (ViT3D) across multiple video understanding benchmarks, e.g., TrajViT outperforms ViT3D by a large margin of 6% top-5 recall in average at video-text retrieval task with 10x token deduction. We also show TrajViT as a stronger model than ViT3D for being the video encoder for modern VideoLLM, obtaining an average of 5.2% performance improvement across 6 VideoQA benchmarks while having 4x faster training time and 18x less inference FLOPs. TrajViT is the first efficient encoder to consistently outperform ViT3D across diverse video analysis tasks, making it a robust and scalable solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23617v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.GR</category>
      <category>cs.LG</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenhao Zheng, Jieyu Zhang, Mohammadreza Salehi, Ziqi Gao, Vishnu Iyengar, Norimasa Kobori, Quan Kong, Ranjay Krishna</dc:creator>
    </item>
    <item>
      <title>Errors in Stereo Geometry Induce Distance Misperception</title>
      <link>https://arxiv.org/abs/2505.23685</link>
      <description>arXiv:2505.23685v1 Announce Type: cross 
Abstract: Stereoscopic head-mounted displays (HMDs) render and present binocular images to create an egocentric, 3D percept to the HMD user. Within this render and presentation pipeline there are potential rendering camera and viewing position errors that can induce deviations in the depth and distance that a user perceives compared to the underlying intended geometry. For example, rendering errors can arise when HMD render cameras are incorrectly positioned relative to the assumed centers of projections of the HMD displays and viewing errors can arise when users view stereo geometry from the incorrect location in the HMD eyebox. In this work we present a geometric framework that predicts errors in distance perception arising from inaccurate HMD perspective geometry and build an HMD platform to reliably simulate render and viewing error in a Quest 3 HMD with eye tracking to experimentally test these predictions. We present a series of five experiments to explore the efficacy of this geometric framework and show that errors in perspective geometry can induce both under- and over-estimations in perceived distance. We further demonstrate how real-time visual feedback can be used to dynamically recalibrate visuomotor mapping so that an accurate reach distance is achieved even if the perceived visual distance is negatively impacted by geometric error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23685v1</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Raffles Xingqi Zhu, Charlie S. Burlingham, Olivier Mercier, Phillip Guan</dc:creator>
    </item>
    <item>
      <title>AMOR: Adaptive Character Control through Multi-Objective Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2505.23708</link>
      <description>arXiv:2505.23708v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has significantly advanced the control of physics-based and robotic characters that track kinematic reference motion. However, methods typically rely on a weighted sum of conflicting reward functions, requiring extensive tuning to achieve a desired behavior. Due to the computational cost of RL, this iterative process is a tedious, time-intensive task. Furthermore, for robotics applications, the weights need to be chosen such that the policy performs well in the real world, despite inevitable sim-to-real gaps. To address these challenges, we propose a multi-objective reinforcement learning framework that trains a single policy conditioned on a set of weights, spanning the Pareto front of reward trade-offs. Within this framework, weights can be selected and tuned after training, significantly speeding up iteration time. We demonstrate how this improved workflow can be used to perform highly dynamic motions with a robot character. Moreover, we explore how weight-conditioned policies can be leveraged in hierarchical settings, using a high-level policy to dynamically select weights according to the current task. We show that the multi-objective policy encodes a diverse spectrum of behaviors, facilitating efficient adaptation to novel tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23708v1</guid>
      <category>cs.RO</category>
      <category>cs.GR</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3721238.3730656</arxiv:DOI>
      <dc:creator>Lucas N. Alegre, Agon Serifi, Ruben Grandia, David M\"uller, Espen Knoop, Moritz B\"acher</dc:creator>
    </item>
    <item>
      <title>How Animals Dance (When You're Not Looking)</title>
      <link>https://arxiv.org/abs/2505.23738</link>
      <description>arXiv:2505.23738v1 Announce Type: cross 
Abstract: We present a keyframe-based framework for generating music-synchronized, choreography aware animal dance videos. Starting from a few keyframes representing distinct animal poses -- generated via text-to-image prompting or GPT-4o -- we formulate dance synthesis as a graph optimization problem: find the optimal keyframe structure that satisfies a specified choreography pattern of beats, which can be automatically estimated from a reference dance video. We also introduce an approach for mirrored pose image generation, essential for capturing symmetry in dance. In-between frames are synthesized using an video diffusion model. With as few as six input keyframes, our method can produce up to 30 second dance videos across a wide range of animals and music tracks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23738v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xiaojuan Wang, Aleksander Holynski, Brian Curless, Ira Kemelmacher, Steve Seitz</dc:creator>
    </item>
    <item>
      <title>LayerPeeler: Autoregressive Peeling for Layer-wise Image Vectorization</title>
      <link>https://arxiv.org/abs/2505.23740</link>
      <description>arXiv:2505.23740v1 Announce Type: cross 
Abstract: Image vectorization is a powerful technique that converts raster images into vector graphics, enabling enhanced flexibility and interactivity. However, popular image vectorization tools struggle with occluded regions, producing incomplete or fragmented shapes that hinder editability. While recent advancements have explored rule-based and data-driven layer-wise image vectorization, these methods face limitations in vectorization quality and flexibility. In this paper, we introduce LayerPeeler, a novel layer-wise image vectorization approach that addresses these challenges through a progressive simplification paradigm. The key to LayerPeeler's success lies in its autoregressive peeling strategy: by identifying and removing the topmost non-occluded layers while recovering underlying content, we generate vector graphics with complete paths and coherent layer structures. Our method leverages vision-language models to construct a layer graph that captures occlusion relationships among elements, enabling precise detection and description for non-occluded layers. These descriptive captions are used as editing instructions for a finetuned image diffusion model to remove the identified layers. To ensure accurate removal, we employ localized attention control that precisely guides the model to target regions while faithfully preserving the surrounding content. To support this, we contribute a large-scale dataset specifically designed for layer peeling tasks. Extensive quantitative and qualitative experiments demonstrate that LayerPeeler significantly outperforms existing techniques, producing vectorization results with superior path semantics, geometric regularity, and visual fidelity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23740v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ronghuan Wu, Wanchao Su, Jing Liao</dc:creator>
    </item>
    <item>
      <title>Parametric/direct CAD integration</title>
      <link>https://arxiv.org/abs/2203.02252</link>
      <description>arXiv:2203.02252v2 Announce Type: replace 
Abstract: In the history of computer-aided design (CAD), feature-based parametric modeling and boundary representation-based direct modeling are two of the most important CAD paradigms, developed respectively in the late 1980s and the late 2000s. They have complementary advantages and limitations, thereby offering huge potential for improvement towards an integrated CAD modeling scheme. Some believe that their integration will be the key characteristic of next generation CAD software. This paper provides a brief review on current parametric/direct integration approaches. Their basic ideas, advantages, and disadvantages will be discussed. The main result reads that existing integration approaches are far from being completed if seamless parametric/direct integration is desired. It is hoped that, by outlining what has already been made possible and what still remains problematic, more researchers will be attracted to work on this very important research topic of parametric/direct integration.
  This paper serves as a complement to the CAD paper titled ``Variational Direct Modeling: A Framework Towards Integration of Parametric Modeling and Direct Modeling in CAD." Cite this work as follows: Qiang Zou, Hsi-Yung Feng, and Shuming Gao. Variational Direct Modeling: A Framework Towards Integration of Parametric Modeling and Direct Modeling in CAD. Computer-Aided Design 157 (2023): 103465.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.02252v2</guid>
      <category>cs.GR</category>
      <category>cs.CG</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.cad.2022.103465</arxiv:DOI>
      <arxiv:journal_reference>Computer-Aided Design 157 (2023): 103465</arxiv:journal_reference>
      <dc:creator>Qiang Zou</dc:creator>
    </item>
    <item>
      <title>CraftsMan3D: High-fidelity Mesh Generation with 3D Native Generation and Interactive Geometry Refiner</title>
      <link>https://arxiv.org/abs/2405.14979</link>
      <description>arXiv:2405.14979v3 Announce Type: replace 
Abstract: We present a novel generative 3D modeling system, coined CraftsMan, which can generate high-fidelity 3D geometries with highly varied shapes, regular mesh topologies, and detailed surfaces, and, notably, allows for refining the geometry in an interactive manner. Despite the significant advancements in 3D generation, existing methods still struggle with lengthy optimization processes, irregular mesh topologies, noisy surfaces, and difficulties in accommodating user edits, consequently impeding their widespread adoption and implementation in 3D modeling software. Our work is inspired by the craftsman, who usually roughs out the holistic figure of the work first and elaborates the surface details subsequently. Specifically, we employ a 3D native diffusion model, which operates on latent space learned from latent set-based 3D representations, to generate coarse geometries with regular mesh topology in seconds. In particular, this process takes as input a text prompt or a reference image and leverages a powerful multi-view (MV) diffusion model to generate multiple views of the coarse geometry, which are fed into our MV-conditioned 3D diffusion model for generating the 3D geometry, significantly improving robustness and generalizability. Following that, a normal-based geometry refiner is used to significantly enhance the surface details. This refinement can be performed automatically, or interactively with user-supplied edits. Extensive experiments demonstrate that our method achieves high efficacy in producing superior-quality 3D assets compared to existing methods. HomePage: https://craftsman3d.github.io/, Code: https://github.com/wyysf-98/CraftsMan</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14979v3</guid>
      <category>cs.GR</category>
      <category>cs.CV</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weiyu Li, Jiarui Liu, Hongyu Yan, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, Xiaoxiao Long</dc:creator>
    </item>
  </channel>
</rss>
