<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.RO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.RO</link>
    <description>cs.RO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.RO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 21 Oct 2024 04:00:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Experimental Validation of Light Cable-Driven Elbow-Assisting Device L-CADEL Design</title>
      <link>https://arxiv.org/abs/2410.13870</link>
      <description>arXiv:2410.13870v1 Announce Type: new 
Abstract: This paper presents a new design of CADEL, a cable-driven elbow-assisting device, with light weighting and control improvements. The new device design is appropriate to be more portable and user-oriented solution, presenting additional facilities with respect to the original design. One of potential benefits of improved portability can be envisaged in the possibility of house and hospital usage keeping social distancing while allowing rehabilitation treatments even during a pandemic spread. Specific attention has been devoted to design main mechatronic components by developing specific kinematics models. The design process includes an implementation of specific control hardware and software. The kinematic model of the new design is formulated and features are evaluated through numerical simulations and experimental tests. An evaluation from original design highlights the proposed improvements mainly in terms of comfort, portability and user-oriented operation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13870v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s42235-021-00133-5</arxiv:DOI>
      <arxiv:journal_reference>Journal of Bionic Engineering, 2022, 19 (2), pp.416-428</arxiv:journal_reference>
      <dc:creator>Med Amine Laribi (COBRA), Marco Ceccarelli (COBRA), Juan Sandoval (COBRA), Matteo Bottin (Unipd), Giulio Rosati</dc:creator>
    </item>
    <item>
      <title>MarineFormer: A Transformer-based Navigation Policy Model for Collision Avoidance in Marine Environment</title>
      <link>https://arxiv.org/abs/2410.13973</link>
      <description>arXiv:2410.13973v1 Announce Type: new 
Abstract: In this work, we investigate the problem of Unmanned Surface Vehicle (USV) navigation in a dense marine environment with a high-intensity current flow. The complexities arising from static and dynamic obstacles and the disturbance forces caused by current flow render existing navigation protocols inadequate for ensuring safety and avoiding collisions at sea. To learn a safe and efficient robot policy, we propose a novel methodology that leverages attention mechanisms to capture heterogeneous interactions of the agents with the static and moving obstacles and the flow disturbances from the environment in space and time. In particular, we refine a temporal function with MarineFormer, a Transformer navigation policy for spatially variable Marine environment, trained end-to-end with reinforcement learning (RL). MarineFormer uses foundational spatio-temporal graph attention with transformer architecture to process spatial attention and temporal sequences in an environment that simulates a 2D turbulent marine condition. We propose architectural modifications that improve the stability and learning speed of the recurrent models. The flow velocity estimation, which can be derived from flow simulations or sensors, is incorporated into a model-free RL framework to prevent the robot from entering into high-intensity current flow regions including intense vortices, while potentially leveraging the flow to assist in transportation. The investigated 2D marine environment encompasses flow singularities, including vortices, sinks, and sources, representing fundamental planar flow patterns associated with flood or maritime thunderstorms. Our proposed method is trained with a new reward model to deal with static and dynamic obstacles and disturbances from the current flow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13973v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ehsan Kazemi, Iman Soltani</dc:creator>
    </item>
    <item>
      <title>RecoveryChaining: Learning Local Recovery Policies for Robust Manipulation</title>
      <link>https://arxiv.org/abs/2410.13979</link>
      <description>arXiv:2410.13979v1 Announce Type: new 
Abstract: Model-based planners and controllers are commonly used to solve complex manipulation problems as they can efficiently optimize diverse objectives and generalize to long horizon tasks. However, they are limited by the fidelity of their model which oftentimes leads to failures during deployment. To enable a robot to recover from such failures, we propose to use hierarchical reinforcement learning to learn a separate recovery policy. The recovery policy is triggered when a failure is detected based on sensory observations and seeks to take the robot to a state from which it can complete the task using the nominal model-based controllers. Our approach, called RecoveryChaining, uses a hybrid action space, where the model-based controllers are provided as additional \emph{nominal} options which allows the recovery policy to decide how to recover, when to switch to a nominal controller and which controller to switch to even with \emph{sparse rewards}. We evaluate our approach in three multi-step manipulation tasks with sparse rewards, where it learns significantly more robust recovery policies than those learned by baselines. Finally, we successfully transfer recovery policies learned in simulation to a physical robot to demonstrate the feasibility of sim-to-real transfer with our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13979v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shivam Vats, Devesh K. Jha, Maxim Likhachev, Oliver Kroemer, Diego Romeres</dc:creator>
    </item>
    <item>
      <title>Whisker-Inspired Tactile Sensing: A Sim2Real Approach for Precise Underwater Contact Tracking</title>
      <link>https://arxiv.org/abs/2410.14005</link>
      <description>arXiv:2410.14005v1 Announce Type: new 
Abstract: Aquatic mammals, such as pinnipeds, utilize their whiskers to detect and discriminate objects and analyze water movements, inspiring the development of robotic whiskers for sensing contacts, surfaces, and water flows. We present the design and application of underwater whisker sensors based on Fiber Bragg Grating (FBG) technology. These passive whiskers are mounted along the robot$'$s exterior to sense its surroundings through light, non-intrusive contacts. For contact tracking, we employ a sim-to-real learning framework, which involves extensive data collection in simulation followed by a sim-to-real calibration process to transfer the model trained in simulation to the real world. Experiments with whiskers immersed in water indicate that our approach can track contact points with an accuracy of $&lt;2$ mm, without requiring precise robot proprioception. We demonstrate that the approach also generalizes to unseen objects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14005v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao Li, Chengyi Xing, Saad Khan, Miaoya Zhong, Mark R. Cutkosky</dc:creator>
    </item>
    <item>
      <title>Vision-Language-Action Model and Diffusion Policy Switching Enables Dexterous Control of an Anthropomorphic Hand</title>
      <link>https://arxiv.org/abs/2410.14022</link>
      <description>arXiv:2410.14022v1 Announce Type: new 
Abstract: To advance autonomous dexterous manipulation, we propose a hybrid control method that combines the relative advantages of a fine-tuned Vision-Language-Action (VLA) model and diffusion models. The VLA model provides language commanded high-level planning, which is highly generalizable, while the diffusion model handles low-level interactions which offers the precision and robustness required for specific objects and environments. By incorporating a switching signal into the training-data, we enable event based transitions between these two models for a pick-and-place task where the target object and placement location is commanded through language. This approach is deployed on our anthropomorphic ADAPT Hand 2, a 13DoF robotic hand, which incorporates compliance through series elastic actuation allowing for resilience for any interactions: showing the first use of a multi-fingered hand controlled with a VLA model. We demonstrate this model switching approach results in a over 80\% success rate compared to under 40\% when only using a VLA model, enabled by accurate near-object arm motion by the VLA model and a multi-modal grasping motion with error recovery abilities from the diffusion model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14022v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cheng Pan, Kai Junge, Josie Hughes</dc:creator>
    </item>
    <item>
      <title>Self Supervised Deep Learning for Robot Grasping</title>
      <link>https://arxiv.org/abs/2410.14084</link>
      <description>arXiv:2410.14084v1 Announce Type: new 
Abstract: Learning Based Robot Grasping currently involves the use of labeled data. This approach has two major disadvantages. Firstly, labeling data for grasp points and angles is a strenuous process, so the dataset remains limited. Secondly, human labeling is prone to bias due to semantics.
  In order to solve these problems we propose a simpler self-supervised robotic setup, that will train a Convolutional Neural Network (CNN). The robot will label and collect the data during the training process. The idea is to make a robot that is less costly, small and easily maintainable in a lab setup. The robot will be trained on a large data set for several hundred hours and then the trained Neural Network can be mapped onto a larger grasping robot.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14084v1</guid>
      <category>cs.RO</category>
      <category>cs.CV</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Danyal Saqib, Wajahat Hussain</dc:creator>
    </item>
    <item>
      <title>MarineGym: Accelerated Training for Underwater Vehicles with High-Fidelity RL Simulation</title>
      <link>https://arxiv.org/abs/2410.14117</link>
      <description>arXiv:2410.14117v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) is a promising solution, allowing Unmanned Underwater Vehicles (UUVs) to learn optimal behaviors through trial and error. However, existing simulators lack efficient integration with RL methods, limiting training scalability and performance. This paper introduces MarineGym, a novel simulation framework designed to enhance RL training efficiency for UUVs by utilizing GPU acceleration. MarineGym offers a 10,000-fold performance improvement over real-time simulation on a single GPU, enabling rapid training of RL algorithms across multiple underwater tasks. Key features include realistic dynamic modeling of UUVs, parallel environment execution, and compatibility with popular RL frameworks like PyTorch and TorchRL. The framework is validated through four distinct tasks: station-keeping, circle tracking, helical tracking, and lemniscate tracking. This framework sets the stage for advancing RL in underwater robotics and facilitating efficient training in complex, dynamic environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14117v1</guid>
      <category>cs.RO</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuguang Chu, Zebin Huang, Mingwei Lin, Dejun Li, Ignacio Carlucho</dc:creator>
    </item>
    <item>
      <title>Skill Generalization with Verbs</title>
      <link>https://arxiv.org/abs/2410.14118</link>
      <description>arXiv:2410.14118v1 Announce Type: new 
Abstract: It is imperative that robots can understand natural language commands issued by humans. Such commands typically contain verbs that signify what action should be performed on a given object and that are applicable to many objects. We propose a method for generalizing manipulation skills to novel objects using verbs. Our method learns a probabilistic classifier that determines whether a given object trajectory can be described by a specific verb. We show that this classifier accurately generalizes to novel object categories with an average accuracy of 76.69% across 13 object categories and 14 verbs. We then perform policy search over the object kinematics to find an object trajectory that maximizes classifier prediction for a given verb. Our method allows a robot to generate a trajectory for a novel object based on a verb, which can then be used as input to a motion planner. We show that our model can generate trajectories that are usable for executing five verb commands applied to novel instances of two different object categories on a real robot.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14118v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/IROS55552.2023.10341472</arxiv:DOI>
      <dc:creator>Rachel Ma, Lyndon Lam, Benjamin A. Spiegel, Aditya Ganeshan, Roma Patel, Ben Abbatematteo, David Paulius, Stefanie Tellex, George Konidaris</dc:creator>
    </item>
    <item>
      <title>Coherence-Driven Multimodal Safety Dialogue with Active Learning for Embodied Agents</title>
      <link>https://arxiv.org/abs/2410.14141</link>
      <description>arXiv:2410.14141v1 Announce Type: new 
Abstract: When assisting people in daily tasks, robots need to accurately interpret visual cues and respond effectively in diverse safety-critical situations, such as sharp objects on the floor. In this context, we present M-CoDAL, a multimodal-dialogue system specifically designed for embodied agents to better understand and communicate in safety-critical situations. The system leverages discourse coherence relations to enhance its contextual understanding and communication abilities. To train this system, we introduce a novel clustering-based active learning mechanism that utilizes an external Large Language Model (LLM) to identify informative instances. Our approach is evaluated using a newly created multimodal dataset comprising 1K safety violations extracted from 2K Reddit images. These violations are annotated using a Large Multimodal Model (LMM) and verified by human annotators. Results with this dataset demonstrate that our approach improves resolution of safety situations, user sentiment, as well as safety of the conversation. Next, we deploy our dialogue system on a Hello Robot Stretch robot and conduct a within-subject user study with real-world participants. In the study, participants role-play two safety scenarios with different levels of severity with the robot and receive interventions from our model and a baseline system powered by OpenAI's ChatGPT. The study results corroborate and extend the findings from automated evaluation, showing that our proposed system is more persuasive and competent in a real-world embodied agent setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14141v1</guid>
      <category>cs.RO</category>
      <category>cs.CL</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sabit Hassan, Hye-Young Chung, Xiang Zhi Tan, Malihe Alikhani</dc:creator>
    </item>
    <item>
      <title>Learning autonomous driving from aerial imagery</title>
      <link>https://arxiv.org/abs/2410.14177</link>
      <description>arXiv:2410.14177v1 Announce Type: new 
Abstract: In this work, we consider the problem of learning end to end perception to control for ground vehicles solely from aerial imagery. Photogrammetric simulators allow the synthesis of novel views through the transformation of pre-generated assets into novel views.However, they have a large setup cost, require careful collection of data and often human effort to create usable simulators. We use a Neural Radiance Field (NeRF) as an intermediate representation to synthesize novel views from the point of view of a ground vehicle. These novel viewpoints can then be used for several downstream autonomous navigation applications. In this work, we demonstrate the utility of novel view synthesis though the application of training a policy for end to end learning from images and depth data. In a traditional real to sim to real framework, the collected data would be transformed into a visual simulator which could then be used to generate novel views. In contrast, using a NeRF allows a compact representation and the ability to optimize over the parameters of the visual simulator as more data is gathered in the environment. We demonstrate the efficacy of our method in a custom built mini-city environment through the deployment of imitation policies on robotic cars. We additionally consider the task of place localization and demonstrate that our method is able to relocalize the car in the real world.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14177v1</guid>
      <category>cs.RO</category>
      <category>cs.CV</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Varun Murali, Guy Rosman, Sertac Karaman, Daniela Rus</dc:creator>
    </item>
    <item>
      <title>A Probabilistic Model for Skill Acquisition with Switching Latent Feedback Controllers</title>
      <link>https://arxiv.org/abs/2410.14191</link>
      <description>arXiv:2410.14191v1 Announce Type: new 
Abstract: Manipulation tasks often consist of subtasks, each representing a distinct skill. Mastering these skills is essential for robots, as it enhances their autonomy, efficiency, adaptability, and ability to work in their environment. Learning from demonstrations allows robots to rapidly acquire new skills without starting from scratch, with demonstrations typically sequencing skills to achieve tasks. Behaviour cloning approaches to learning from demonstration commonly rely on mixture density network output heads to predict robot actions. In this work, we first reinterpret the mixture density network as a library of feedback controllers (or skills) conditioned on latent states. This arises from the observation that a one-layer linear network is functionally equivalent to a classical feedback controller, with network weights corresponding to controller gains. We use this insight to derive a probabilistic graphical model that combines these elements, describing the skill acquisition process as segmentation in a latent space, where each skill policy functions as a feedback control law in this latent space. Our approach significantly improves not only task success rate, but also robustness to observation noise when trained with human demonstrations. Our physical robot experiments further show that the induced robustness improves model deployment on robots.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14191v1</guid>
      <category>cs.RO</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juyan Zhang, Dana Kulic, Michael Burke</dc:creator>
    </item>
    <item>
      <title>EPIC: A Lightweight LiDAR-Based UAV Exploration Framework for Large-Scale Scenarios</title>
      <link>https://arxiv.org/abs/2410.14203</link>
      <description>arXiv:2410.14203v1 Announce Type: new 
Abstract: Autonomous exploration is a fundamental problem for various applications of unmanned aerial vehicles (UAVs). Recently, LiDAR-based exploration has gained significant attention due to its ability to generate high-precision point cloud maps of large-scale environments. While the point clouds are inherently informative for navigation, many existing exploration methods still rely on additional, often expensive, environmental representations. This reliance stems from two main reasons: the need for frontier detection or information gain computation, which typically depends on memory-intensive occupancy grid maps, and the high computational complexity of path planning directly on point clouds, primarily due to costly collision checking. To address these limitations, we present EPIC, a lightweight LiDAR-based UAV exploration framework that directly exploits point cloud data to explore large-scale environments. EPIC introduces a novel observation map derived directly from the quality of point clouds, eliminating the need for global occupancy grid maps while preserving comprehensive exploration capabilities. We also propose an incremental topological graph construction method operating directly on point clouds, enabling real-time path planning in large-scale environments. Leveraging these components, we build a hierarchical planning framework that generates agile and energy-efficient trajectories, achieving significantly reduced memory consumption and computation time compared to most existing methods. Extensive simulations and real-world experiments demonstrate that EPIC achieves faster exploration while significantly reducing memory consumption compared to state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14203v1</guid>
      <category>cs.RO</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuang Geng, Zelin Ning, Fu Zhang, Boyu Zhou</dc:creator>
    </item>
    <item>
      <title>A Tactile Feedback Approach to Path Recovery after High-Speed Impacts for Collision-Resilient Drones</title>
      <link>https://arxiv.org/abs/2410.14249</link>
      <description>arXiv:2410.14249v1 Announce Type: new 
Abstract: Aerial robots are a well-established solution for exploration, monitoring, and inspection, thanks to their superior maneuverability and agility. However, in many environments of interest, they risk crashing and sustaining damage following collisions. Traditional methods focus on avoiding obstacles entirely to prevent damage, but these approaches can be limiting, particularly in complex environments where collisions may be unavoidable, or on weight and compute-constrained platforms. This paper presents a novel approach to enhance the robustness and autonomy of drones in such scenarios by developing a path recovery and adjustment method for a high-speed collision-resistant drone equipped with binary contact sensors. The proposed system employs an estimator that explicitly models collisions, using pre-collision velocities and rates to predict post-collision dynamics, thereby improving the drone's state estimation accuracy. Additionally, we introduce a vector-field-based path representation which guarantees convergence to the path. Post-collision, the contact point is incorporated into the vector field as a repulsive potential, enabling the drone to avoid obstacles while naturally converging to the original path. The effectiveness of this method is validated through Monte Carlo simulations and demonstrated on a physical prototype, showing successful path following and adjustment through collisions as well as recovery from collisions at speeds up to 3.7 m / s.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14249v1</guid>
      <category>cs.RO</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anton Bredenbeck, Teaya Yang, Salua Hamaza, Mark W. Mueller</dc:creator>
    </item>
    <item>
      <title>Error Decomposition for Hybrid Localization Systems</title>
      <link>https://arxiv.org/abs/2410.14264</link>
      <description>arXiv:2410.14264v1 Announce Type: new 
Abstract: Future advanced driver assistance systems and autonomous vehicles rely on accurate localization, which can be divided into three classes: a) viewpoint localization about local references (e.g., via vision-based localization), b) absolute localization about a global reference system (e.g., via satellite navigation), and c) hybrid localization, which presents a combination of the former two. Hybrid localization shares characteristics and strengths of both absolute and viewpoint localization. However, new sources of error, such as inaccurate sensor-setup calibration, complement the potential errors of the respective sub-systems. Therefore, this paper introduces a general approach to analyzing error sources in hybrid localization systems. More specifically, we propose the Kappa-Phi method, which allows for the decomposition of localization errors into individual components, i.e., into a sum of parameterized functions of the measured state (e.g., agent kinematics). The error components can then be leveraged to, e.g., improve localization predictions, correct map data, or calibrate sensor setups. Theoretical derivations and evaluations show that the algorithm presents a promising approach to improve hybrid localization and counter the weaknesses of the system's individual components.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14264v1</guid>
      <category>cs.RO</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/ITSC48978.2021.9564415</arxiv:DOI>
      <dc:creator>Benedict Flade, Simon Kohaut, Julian Eggert</dc:creator>
    </item>
    <item>
      <title>Optimizing Collaborative Robotics since Pre-Deployment via Cyber-Physical Systems' Digital Twins</title>
      <link>https://arxiv.org/abs/2410.14298</link>
      <description>arXiv:2410.14298v1 Announce Type: new 
Abstract: The collaboration between humans and robots re-quires a paradigm shift not only in robot perception, reasoning, and action, but also in the design of the robotic cell. This paper proposes an optimization framework for designing collaborative robotics cells using a digital twin during the pre-deployment phase. This approach mitigates the limitations of experience-based sub-optimal designs by means of Bayesian optimization to find the optimal layout after a certain number of iterations. By integrating production KPIs into a black-box optimization frame-work, the digital twin supports data-driven decision-making, reduces the need for costly prototypes, and ensures continuous improvement thanks to the learning nature of the algorithm. The paper presents a case study with preliminary results that show how this methodology can be applied to obtain safer, more efficient, and adaptable human-robot collaborative environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14298v1</guid>
      <category>cs.RO</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ETFA61755.2024.10710805</arxiv:DOI>
      <arxiv:journal_reference>IEEE ETFA 2024</arxiv:journal_reference>
      <dc:creator>Christian Cella, Marco Faroni, Andrea Zanchettin, Paolo Rocco</dc:creator>
    </item>
    <item>
      <title>Optimizing Modeling of Continuum Robots: Integration of Lie Group Kinematics and Evolutionary Algorithms</title>
      <link>https://arxiv.org/abs/2410.14305</link>
      <description>arXiv:2410.14305v1 Announce Type: new 
Abstract: Continuum robots, known for their high flexibility and adaptability, offer immense potential for applications such as medical surgery, confined-space inspections, and wearable devices. However, their non-linear elastic properties and complex kinematics present significant challenges in digital modeling and effective control. This research proposes a novel computational framework that integrates Lie group kinematics with an evolutionary algorithm (EA) to identify optimal control coefficients for specific robot models. Our method starts by generating datasets from physics-based simulations and fractional order control, defining both ideal configurations and models to be optimized. By using EA, we iteratively minimize deviations through two fitness objectives \textemdash deviation mean squared error (\(\text{MSE}_1\)) and TCP vector error (\(\text{MSE}_2\)) \textemdash to align the robot's backbone with the desired configuration. Built on the Computer-Aided Design (CAD) platform Grasshopper, this framework provides real-time visualization, enabling dynamic control of robot configurations. Results show that the proposed method achieves precise alignment of the robot's backbone with minimal computation. This approach not only simplifies the coefficient identification process but also demonstrates the advantages of EA in multi-objective optimization, contributing to efficient modeling and control of continuum robots.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14305v1</guid>
      <category>cs.RO</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Po-Yu Hsieh, June-Hao Hou</dc:creator>
    </item>
    <item>
      <title>Transferring Tactile Data Across Sensors</title>
      <link>https://arxiv.org/abs/2410.14310</link>
      <description>arXiv:2410.14310v1 Announce Type: new 
Abstract: Tactile perception is essential for human interaction with the environment and is becoming increasingly crucial in robotics. Tactile sensors like the BioTac mimic human fingertips and provide detailed interaction data. Despite its utility in applications like slip detection and object identification, this sensor is now deprecated, making many existing datasets obsolete. This article introduces a novel method for translating data between tactile sensors by exploiting sensor deformation information rather than output signals. We demonstrate the approach by translating BioTac signals into the DIGIT sensor. Our framework consists of three steps: first, converting signal data into corresponding 3D deformation meshes; second, translating these 3D deformation meshes from one sensor to another; and third, generating output images using the converted meshes. Our approach enables the continued use of valuable datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14310v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wadhah Zai El Amri, Malte Kuhlmann, Nicol\'as Navarro-Guerrero</dc:creator>
    </item>
    <item>
      <title>Perception of Emotions in Human and Robot Faces: Is the Eye Region Enough?</title>
      <link>https://arxiv.org/abs/2410.14337</link>
      <description>arXiv:2410.14337v1 Announce Type: new 
Abstract: The increased interest in developing next-gen social robots has raised questions about the factors affecting the perception of robot emotions. This study investigates the impact of robot appearances (humanlike, mechanical) and face regions (full-face, eye-region) on human perception of robot emotions. A between-subjects user study (N = 305) was conducted where participants were asked to identify the emotions being displayed in videos of robot faces, as well as a human baseline. Our findings reveal three important insights for effective social robot face design in Human-Robot Interaction (HRI): Firstly, robots equipped with a back-projected, fully animated face - regardless of whether they are more human-like or more mechanical-looking - demonstrate a capacity for emotional expression comparable to that of humans. Secondly, the recognition accuracy of emotional expressions in both humans and robots declines when only the eye region is visible. Lastly, within the constraint of only the eye region being visible, robots with more human-like features significantly enhance emotion recognition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14337v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chinmaya Mishra, Gabriel Skantze, Peter Hagoort, Rinus Verdonschot</dc:creator>
    </item>
    <item>
      <title>Quadrotor Guidance for Window Traversal: A Bearings-Only Approach</title>
      <link>https://arxiv.org/abs/2410.14367</link>
      <description>arXiv:2410.14367v1 Announce Type: new 
Abstract: This paper focuses on developing a bearings-only measurement-based three-dimensional window traversal guidance method for quadrotor Uninhabitated Aerial Vehicles (UAVs). The desired flight path and heading angles of the quadrotor are proposed as functions of the bearing angle information of the four vertices of the window. These angular guidance inputs employ a bearing angle bisector term and an elliptic shaping angle term, which directs the quadrotor towards the centroid of the window. Detailed stability analysis of the resulting kinematics demonstrates that all quadrotor trajectories lead to the centroid of the window along a direction which is normal to the window plane. A qualitative comparison with existing traversal methodologies showcases the superiority of the proposed guidance approach with regard to the nature of information, computations for generating the guidance commands, and flexibility of replanning the traversal path. Realistic simulations considering six degree-of-freedom quadrotor model and Monte Carlo studies validate the effectiveness, accuracy, and robustness of the proposed guidance solution. Representative flight validation trials are carried out using an indoor motion capture system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14367v1</guid>
      <category>cs.RO</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Midhun E K, Ashwini Ratnoo</dc:creator>
    </item>
    <item>
      <title>MARLIN: Multi-Agent Reinforcement Learning Guided by Language-Based Inter-Robot Negotiation</title>
      <link>https://arxiv.org/abs/2410.14383</link>
      <description>arXiv:2410.14383v1 Announce Type: new 
Abstract: Multi-agent reinforcement learning is a key method for training multi-robot systems over a series of episodes in which robots are rewarded or punished according to their performance; only once the system is trained to a suitable standard is it deployed in the real world. If the system is not trained enough, the task will likely not be completed and could pose a risk to the surrounding environment. Therefore, reaching high performance in a shorter training period can lead to significant reductions in time and resource consumption. We introduce Multi-Agent Reinforcement Learning guided by Language-based Inter-Robot Negotiation (MARLIN), which makes the training process both faster and more transparent. We equip robots with large language models that negotiate and debate the task, producing a plan that is used to guide the policy during training. We dynamically switch between using reinforcement learning and the negotiation-based approach throughout training. This offers an increase in training speed when compared to standard multi-agent reinforcement learning and allows the system to be deployed to physical hardware earlier. As robots negotiate in natural language, we can better understand the behaviour of the robots individually and as a collective. We compare the performance of our approach to multi-agent reinforcement learning and a large language model to show that our hybrid method trains faster at little cost to performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14383v1</guid>
      <category>cs.RO</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Toby Godfrey, William Hunt, Mohammad D. Soorati</dc:creator>
    </item>
    <item>
      <title>On the Benefits of Robot Platooning for Navigating Crowded Environments</title>
      <link>https://arxiv.org/abs/2410.14406</link>
      <description>arXiv:2410.14406v1 Announce Type: new 
Abstract: This paper studies how groups of robots can effectively navigate through a crowd of agents. It quantifies the performance of platooning and less constrained, greedy strategies, and the extent to which these strategies disrupt the crowd agents. Three scenarios are considered: (i) passive crowds, (ii) counter-flow crowds, and (iii) perpendicular-flow crowds. Through simulations consisting of up to 200 robots, we show that for navigating passive and counter-flow crowds, the platooning strategy is less disruptive and more effective in dense crowds than the greedy strategy, whereas for navigating perpendicular-flow crowds, the greedy strategy outperforms the platooning strategy in either aspect. Moreover, we propose an adaptive strategy that can switch between platooning and greedy behavioral states, and demonstrate that it combines the strengths of both strategies in all the scenarios considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14406v1</guid>
      <category>cs.RO</category>
      <category>cs.MA</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jahir Argote-Gerald, Genki Miyauchi, Paul Trodden, Roderich Gross</dc:creator>
    </item>
    <item>
      <title>Formation Control for Moving Target Enclosing and Tracking via Relative Localization</title>
      <link>https://arxiv.org/abs/2410.14407</link>
      <description>arXiv:2410.14407v1 Announce Type: new 
Abstract: This paper proposes an integrated framework for coordinating multiple unmanned aerial vehicles (UAVs) in a distributed fashion to persistently enclose and track a moving target without external localization systems. It is assumed that the UAV can obtain self-displacement and the target's relative position using vision-based methods within its local frame. Additionally, UAVs can measure relative distances and communicate with each other, e.g. by ultrawideband (UWB) sensors. Due to the absence of a global coordinate system, measurements from neighbors cannot be directly utilized for collaborative estimation of the target state. To address this, a recursive least squares estimator (RLSE) for estimating the relative positions between UAVs is integrated into a distributed Kalman filter (DKF), enabling a persistent estimation of the target state. When the UAV loses direct measurements of the target due to environmental occlusion, measurements from neighbors will be aligned into the UAV's local frame to provide indirect measurements. Furthermore, simultaneously ensuring the convergence of the estimators and maintaining effective target tracking is a significant challenge. To tackle this problem, a consensus-based formation controller with bounded inputs is developed by integrating a coupled oscillator-based circular formation design. Theoretical analysis shows that the proposed framework ensures asymptotic tracking of a target with constant velocity. For a target with varying velocity, the tracking error converges to a bounded region related to the target's maximum acceleration. Simulations and experiments validate the effectiveness of the proposed algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14407v1</guid>
      <category>cs.RO</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xueming Liu, Dengyu Zhang, Qingrui Zhang, Tianjiang Hu</dc:creator>
    </item>
    <item>
      <title>Sim2real Cattle Joint Estimation in 3D point clouds</title>
      <link>https://arxiv.org/abs/2410.14419</link>
      <description>arXiv:2410.14419v1 Announce Type: new 
Abstract: Understanding the well-being of cattle is crucial in various agricultural contexts. Cattle's body shape and joint articulation carry significant information about their welfare, yet acquiring comprehensive datasets for 3D body pose estimation presents a formidable challenge. This study delves into the construction of such a dataset specifically tailored for cattle. Leveraging the expertise of digital artists, we use a single animated 3D model to represent diverse cattle postures. To address the disparity between virtual and real-world data, we augment the 3D model's shape to encompass a range of potential body appearances, thereby narrowing the "sim2real" gap. We use these annotated models to train a deep-learning framework capable of estimating internal joints solely based on external surface curvature. Our contribution is specifically the use of geodesic distance over the surface manifold, coupled with multilateration to extract joints in a semantic keypoint detection encoder-decoder architecture. We demonstrate the robustness of joint extraction by comparing the link lengths extracted on real cattle mobbing and walking within a race. Furthermore, inspired by the established allometric relationship between bone length and the overall height of mammals, we utilise the estimated joints to predict hip height within a real cattle dataset, extending the utility of our approach to offer insights into improving cattle monitoring practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14419v1</guid>
      <category>cs.RO</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>IEEE/RSJ International Conference on Intelligent Robots and Systems 2024</arxiv:journal_reference>
      <dc:creator>Okour Mohammad, Falque Raphael, Alempijevic Alen</dc:creator>
    </item>
    <item>
      <title>From Simple to Complex: Knowledge Transfer in Safe and Efficient Reinforcement Learning for Autonomous Driving</title>
      <link>https://arxiv.org/abs/2410.14468</link>
      <description>arXiv:2410.14468v1 Announce Type: new 
Abstract: A safe and efficient decision-making system is crucial for autonomous vehicles. However, the complexity of driving environments limit the effectiveness of many rule-based and machine learning-based decision-making approaches. The introduction of Reinforcement Learning in autonomous driving presents a promising solution to these challenges, although concerns about safety and efficiency during training remain major obstacles to its widespread application. To address these concerns, we propose a novel framework named Simple to Complex Collaborative Decision. First, we rapidly train the teacher model using the Proximal Policy Optimization algorithm in a lightweight autonomous driving simulation environment. In the more complex simulation environment, the teacher model intervenes when the student agent exhibits sub-optimal behavior by assessing the value of actions to avert dangerous situations. Next, we developed an innovative algorithm called Adaptive Clipping Proximal Policy Optimization. It trains using a combination of samples generated by both the teacher and student policies and applies dynamic clipping strategies based on sample importance, enabling the algorithm to utilize samples from diverse sources more efficiently. Additionally, we employ the KL divergence between the teacher's and student's policies as a constraint for policy optimization to facilitate the student agent's rapid learning of the teacher's policy. Finally, by adopting an appropriate weaning strategy to gradually reduce teacher intervention, we ensure that the student agent can fully explore the environment independently during the later stages of training. Simulation experiments in highway lane-change scenarios demonstrate that, compared to baseline algorithms, our proposed framework not only improves learning efficiency and reduces training costs but also significantly enhances safety during training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14468v1</guid>
      <category>cs.RO</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rongliang Zhou, Jiakun Huang, Mingjun Li, Hepeng Li, Haotian Cao, Xiaolin Song</dc:creator>
    </item>
    <item>
      <title>Graph Optimality-Aware Stochastic LiDAR Bundle Adjustment with Progressive Spatial Smoothing</title>
      <link>https://arxiv.org/abs/2410.14565</link>
      <description>arXiv:2410.14565v1 Announce Type: new 
Abstract: Large-scale LiDAR Bundle Adjustment (LBA) for refining sensor orientation and point cloud accuracy simultaneously is a fundamental task in photogrammetry and robotics, particularly as low-cost 3D sensors are increasingly used for 3D mapping in complex scenes. Unlike pose-graph-based methods that rely solely on pairwise relationships between LiDAR frames, LBA leverages raw LiDAR correspondences to achieve more precise results, especially when initial pose estimates are unreliable for low-cost sensors. However, existing LBA methods face challenges such as simplistic planar correspondences, extensive observations, and dense normal matrices in the least-squares problem, which limit robustness, efficiency, and scalability. To address these issues, we propose a Graph Optimality-aware Stochastic Optimization scheme with Progressive Spatial Smoothing, namely PSS-GOSO, to achieve \textit{robust}, \textit{efficient}, and \textit{scalable} LBA. The Progressive Spatial Smoothing (PSS) module extracts \textit{robust} LiDAR feature association exploiting the prior structure information obtained by the polynomial smooth kernel. The Graph Optimality-aware Stochastic Optimization (GOSO) module first sparsifies the graph according to optimality for an \textit{efficient} optimization. GOSO then utilizes stochastic clustering and graph marginalization to solve the large-scale state estimation problem for a \textit{scalable} LBA. We validate PSS-GOSO across diverse scenes captured by various platforms, demonstrating its superior performance compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14565v1</guid>
      <category>cs.RO</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jianping Li, Thien-Minh Nguyen, Muqing Cao, Shenghai Yuan, Tzu-Yi Hung, Lihua Xie</dc:creator>
    </item>
    <item>
      <title>Reimagining partial thickness keratoplasty: An eye mountable robot for autonomous big bubble needle insertion</title>
      <link>https://arxiv.org/abs/2410.14577</link>
      <description>arXiv:2410.14577v1 Announce Type: new 
Abstract: Autonomous surgical robots have demonstrated significant potential to standardize surgical outcomes, driving innovations that enhance safety and consistency regardless of individual surgeon experience. Deep anterior lamellar keratoplasty (DALK), a partial thickness corneal transplant surgery aimed at replacing the anterior part of cornea above Descemet membrane (DM), would greatly benefit from an autonomous surgical approach as it highly relies on surgeon skill with high perforation rates. In this study, we proposed a novel autonomous surgical robotic system (AUTO-DALK) based on a customized neural network capable of precise needle control and consistent big bubble demarcation on cadaver and live rabbit models. We demonstrate the feasibility of an AI-based image-guided vertical drilling approach for big bubble generation, in contrast to the conventional horizontal needle approach. Our system integrates an optical coherence tomography (OCT) fiber optic distal sensor into the eye-mountable micro robotic system, which automatically segments OCT M-mode depth signals to identify corneal layers using a custom deep learning algorithm. It enables the robot to autonomously guide the needle to targeted tissue layers via a depth-controlled feedback loop. We compared autonomous needle insertion performance and resulting pneumo-dissection using AUTO-DALK against 1) freehand insertion, 2) OCT sensor guided manual insertion, and 3) teleoperated robotic insertion, reporting significant improvements in insertion depth, pneumo-dissection depth, task completion time, and big bubble formation. Ex vivo and in vivo results indicate that the AI-driven, AUTO-DALK system, is a promising solution to standardize pneumo-dissection outcomes for partial thickness keratoplasty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14577v1</guid>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Y. Wang, J. D. Opfermann, J. Yu, H. Yi, J. Kaluna, R. Biswas, R. Zuo, W. Gensheimer, A. Krieger, J. U. Kang</dc:creator>
    </item>
    <item>
      <title>Benchmarking Deep Reinforcement Learning for Navigation in Denied Sensor Environments</title>
      <link>https://arxiv.org/abs/2410.14616</link>
      <description>arXiv:2410.14616v1 Announce Type: new 
Abstract: Deep Reinforcement learning (DRL) is used to enable autonomous navigation in unknown environments. Most research assume perfect sensor data, but real-world environments may contain natural and artificial sensor noise and denial. Here, we present a benchmark of both well-used and emerging DRL algorithms in a navigation task with configurable sensor denial effects. In particular, we are interested in comparing how different DRL methods (e.g. model-free PPO vs. model-based DreamerV3) are affected by sensor denial. We show that DreamerV3 outperforms other methods in the visual end-to-end navigation task with a dynamic goal - and other methods are not able to learn this. Furthermore, DreamerV3 generally outperforms other methods in sensor-denied environments. In order to improve robustness, we use adversarial training and demonstrate an improved performance in denied environments, although this generally comes with a performance cost on the vanilla environments. We anticipate this benchmark of different DRL methods and the usage of adversarial training to be a starting point for the development of more elaborate navigation strategies that are capable of dealing with uncertain and denied sensor readings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14616v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mariusz Wisniewski, Paraskevas Chatzithanos, Weisi Guo, Antonios Tsourdos</dc:creator>
    </item>
    <item>
      <title>Goal Inference from Open-Ended Dialog</title>
      <link>https://arxiv.org/abs/2410.13957</link>
      <description>arXiv:2410.13957v1 Announce Type: cross 
Abstract: We present an online method for embodied agents to learn and accomplish diverse user goals. While offline methods like RLHF can represent various goals but require large datasets, our approach achieves similar flexibility with online efficiency. We extract natural language goal representations from conversations with Large Language Models (LLMs). We prompt an LLM to role play as a human with different goals and use the corresponding likelihoods to run Bayesian inference over potential goals. As a result, our method can represent uncertainty over complex goals based on unrestricted dialog. We evaluate our method in grocery shopping and home robot assistance domains using a text-based interface and AI2Thor simulation respectively. Results show our method outperforms ablation baselines that lack either explicit goal representation or probabilistic inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13957v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rachel Ma, Jingyi Qu, Andreea Bobu, Dylan Hadfield-Menell</dc:creator>
    </item>
    <item>
      <title>Latent Weight Diffusion: Generating Policies from Trajectories</title>
      <link>https://arxiv.org/abs/2410.14040</link>
      <description>arXiv:2410.14040v1 Announce Type: cross 
Abstract: With the increasing availability of open-source robotic data, imitation learning has emerged as a viable approach for both robot manipulation and locomotion. Currently, large generalized policies are trained to predict controls or trajectories using diffusion models, which have the desirable property of learning multimodal action distributions. However, generalizability comes with a cost - namely, larger model size and slower inference. Further, there is a known trade-off between performance and action horizon for Diffusion Policy (i.e., diffusing trajectories): fewer diffusion queries accumulate greater trajectory tracking errors. Thus, it is common practice to run these models at high inference frequency, subject to robot computational constraints.
  To address these limitations, we propose Latent Weight Diffusion (LWD), a method that uses diffusion to learn a distribution over policies for robotic tasks, rather than over trajectories. Our approach encodes demonstration trajectories into a latent space and then decodes them into policies using a hypernetwork. We employ a diffusion denoising model within this latent space to learn its distribution. We demonstrate that LWD can reconstruct the behaviors of the original policies that generated the trajectory dataset. LWD offers the benefits of considerably smaller policy networks during inference and requires fewer diffusion model queries. When tested on the Metaworld MT10 benchmark, LWD achieves a higher success rate compared to a vanilla multi-task policy, while using models up to ~18x smaller during inference. Additionally, since LWD generates closed-loop policies, we show that it outperforms Diffusion Policy in long action horizon settings, with reduced diffusion queries during rollout.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14040v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.RO</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shashank Hegde, Gautam Salhotra, Gaurav S. Sukhatme</dc:creator>
    </item>
    <item>
      <title>Optimal DLT-based Solutions for the Perspective-n-Point</title>
      <link>https://arxiv.org/abs/2410.14164</link>
      <description>arXiv:2410.14164v1 Announce Type: cross 
Abstract: We propose a modified normalized direct linear transform (DLT) algorithm for solving the perspective-n-point (PnP) problem with much better behavior than the conventional DLT. The modification consists of analytically weighting the different measurements in the linear system with a negligible increase in computational load. Our approach exhibits clear improvements -- in both performance and runtime -- when compared to popular methods such as EPnP, CPnP, RPnP, and OPnP. Our new non-iterative solution approaches that of the true optimal found via Gauss-Newton optimization, but at a fraction of the computational cost. Our optimal DLT (oDLT) implementation, as well as the experiments, are released in open source.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14164v1</guid>
      <category>cs.CV</category>
      <category>cs.RO</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>S\'ebastien Henry, John A. Christian</dc:creator>
    </item>
    <item>
      <title>CoMAL: Collaborative Multi-Agent Large Language Models for Mixed-Autonomy Traffic</title>
      <link>https://arxiv.org/abs/2410.14368</link>
      <description>arXiv:2410.14368v1 Announce Type: cross 
Abstract: The integration of autonomous vehicles into urban traffic has great potential to improve efficiency by reducing congestion and optimizing traffic flow systematically. In this paper, we introduce CoMAL (Collaborative Multi-Agent LLMs), a framework designed to address the mixed-autonomy traffic problem by collaboration among autonomous vehicles to optimize traffic flow. CoMAL is built upon large language models, operating in an interactive traffic simulation environment. It utilizes a Perception Module to observe surrounding agents and a Memory Module to store strategies for each agent. The overall workflow includes a Collaboration Module that encourages autonomous vehicles to discuss the effective strategy and allocate roles, a reasoning engine to determine optimal behaviors based on assigned roles, and an Execution Module that controls vehicle actions using a hybrid approach combining rule-based models. Experimental results demonstrate that CoMAL achieves superior performance on the Flow benchmark. Additionally, we evaluate the impact of different language models and compare our framework with reinforcement learning approaches. It highlights the strong cooperative capability of LLM agents and presents a promising solution to the mixed-autonomy traffic challenge. The code is available at https://github.com/Hyan-Yao/CoMAL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14368v1</guid>
      <category>cs.AI</category>
      <category>cs.RO</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huaiyuan Yao, Longchao Da, Vishnu Nandam, Justin Turnau, Zhiwei Liu, Linsey Pang, Hua Wei</dc:creator>
    </item>
    <item>
      <title>Domain Adaptive Safety Filters via Deep Operator Learning</title>
      <link>https://arxiv.org/abs/2410.14528</link>
      <description>arXiv:2410.14528v1 Announce Type: cross 
Abstract: Learning-based approaches for constructing Control Barrier Functions (CBFs) are increasingly being explored for safety-critical control systems. However, these methods typically require complete retraining when applied to unseen environments, limiting their adaptability. To address this, we propose a self-supervised deep operator learning framework that learns the mapping from environmental parameters to the corresponding CBF, rather than learning the CBF directly. Our approach leverages the residual of a parametric Partial Differential Equation (PDE), where the solution defines a parametric CBF approximating the maximal control invariant set. This framework accommodates complex safety constraints, higher relative degrees, and actuation limits. We demonstrate the effectiveness of the method through numerical experiments on navigation tasks involving dynamic obstacles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14528v1</guid>
      <category>eess.SY</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lakshmideepakreddy Manda, Shaoru Chen, Mahyar Fazlyab</dc:creator>
    </item>
    <item>
      <title>Open-Structure: Structural Benchmark Dataset for SLAM Algorithms</title>
      <link>https://arxiv.org/abs/2310.10931</link>
      <description>arXiv:2310.10931v2 Announce Type: replace 
Abstract: This paper presents Open-Structure, a novel benchmark dataset for evaluating visual odometry and SLAM methods. Compared to existing public datasets that primarily offer raw images, Open-Structure provides direct access to point and line measurements, correspondences, structural associations, and co-visibility factor graphs, which can be fed to various stages of SLAM pipelines to mitigate the impact of data preprocessing modules in ablation experiments. The dataset comprises two distinct types of sequences from the perspective of scenarios. The first type maintains reasonable observation and occlusion relationships, as these critical elements are extracted from public image-based sequences using our dataset generator. In contrast, the second type consists of carefully designed simulation sequences that enhance dataset diversity by introducing a wide range of trajectories and observations. Furthermore, a baseline is proposed using our dataset to evaluate widely used modules, including camera pose tracking, parametrization, and factor graph optimization, within SLAM systems. By evaluating these state-of-the-art algorithms across different scenarios, we discern each module's strengths and weaknesses in the context of camera tracking and optimization processes. The Open-Structure dataset and baseline system are openly accessible on website: \url{https://open-structure.github.io}, encouraging further research and development in the field of SLAM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.10931v2</guid>
      <category>cs.RO</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanyan Li, Zhao Guo, Ze Yang, Yanbiao Sun, Liang Zhao, Federico Tombari</dc:creator>
    </item>
    <item>
      <title>A Convex Formulation of Frictional Contact for the Material Point Method and Rigid Bodies</title>
      <link>https://arxiv.org/abs/2403.13783</link>
      <description>arXiv:2403.13783v3 Announce Type: replace 
Abstract: In this paper, we introduce a novel convex formulation that seamlessly integrates the Material Point Method (MPM) with articulated rigid body dynamics in frictional contact scenarios. We extend the linear corotational hyperelastic model into the realm of elastoplasticity and include an efficient return mapping algorithm. This approach is particularly effective for MPM simulations involving significant deformation and topology changes, while preserving the convexity of the optimization problem. Our method ensures global convergence, enabling the use of large simulation time steps without compromising robustness. We have validated our approach through rigorous testing and performance evaluations, highlighting its superior capabilities in managing complex simulations relevant to robotics. Compared to previous MPM-based robotic simulators, our method significantly improves the stability of contact resolution - a critical factor in robot manipulation tasks. We make our method available in the open-source robotics toolkit, Drake. The supplemental video is available at https://youtu.be/5jrQtF5D0DA</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13783v3</guid>
      <category>cs.RO</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zeshun Zong, Chenfanfu Jiang, Xuchen Han</dc:creator>
    </item>
    <item>
      <title>LocoMan: Advancing Versatile Quadrupedal Dexterity with Lightweight Loco-Manipulators</title>
      <link>https://arxiv.org/abs/2403.18197</link>
      <description>arXiv:2403.18197v2 Announce Type: replace 
Abstract: Quadrupedal robots have emerged as versatile agents capable of locomoting and manipulating in complex environments. Traditional designs typically rely on the robot's inherent body parts or incorporate top-mounted arms for manipulation tasks. However, these configurations may limit the robot's operational dexterity, efficiency and adaptability, particularly in cluttered or constrained spaces. In this work, we present LocoMan, a dexterous quadrupedal robot with a novel morphology to perform versatile manipulation in diverse constrained environments. By equipping a Unitree Go1 robot with two low-cost and lightweight modular 3-DoF loco-manipulators on its front calves, LocoMan leverages the combined mobility and functionality of the legs and grippers for complex manipulation tasks that require precise 6D positioning of the end effector in a wide workspace. To harness the loco-manipulation capabilities of LocoMan, we introduce a unified control framework that extends the whole-body controller (WBC) to integrate the dynamics of loco-manipulators. Through experiments, we validate that the proposed whole-body controller can accurately and stably follow desired 6D trajectories of the end effector and torso, which, when combined with the large workspace from our design, facilitates a diverse set of challenging dexterous loco-manipulation tasks in confined spaces, such as opening doors, plugging into sockets, picking objects in narrow and low-lying spaces, and bimanual manipulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18197v2</guid>
      <category>cs.RO</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Changyi Lin, Xingyu Liu, Yuxiang Yang, Yaru Niu, Wenhao Yu, Tingnan Zhang, Jie Tan, Byron Boots, Ding Zhao</dc:creator>
    </item>
    <item>
      <title>Preference-Based Planning in Stochastic Environments: From Partially-Ordered Temporal Goals to Most Preferred Policies</title>
      <link>https://arxiv.org/abs/2403.18212</link>
      <description>arXiv:2403.18212v2 Announce Type: replace 
Abstract: Human preferences are not always represented via complete linear orders: It is natural to employ partially-ordered preferences for expressing incomparable outcomes. In this work, we consider decision-making and probabilistic planning in stochastic systems modeled as Markov decision processes (MDPs), given a partially ordered preference over a set of temporally extended goals. Specifically, each temporally extended goal is expressed using a formula in Linear Temporal Logic on Finite Traces (LTL$_f$). To plan with the partially ordered preference, we introduce order theory to map a preference over temporal goals to a preference over policies for the MDP. Accordingly, a most preferred policy under a stochastic ordering induces a stochastic nondominated probability distribution over the finite paths in the MDP. To synthesize a most preferred policy, our technical approach includes two key steps. In the first step, we develop a procedure to transform a partially ordered preference over temporal goals into a computational model, called preference automaton, which is a semi-automaton with a partial order over acceptance conditions. In the second step, we prove that finding a most preferred policy is equivalent to computing a Pareto-optimal policy in a multi-objective MDP that is constructed from the original MDP, the preference automaton, and the chosen stochastic ordering relation. Throughout the paper, we employ running examples to illustrate the proposed preference specification and solution approaches. We demonstrate the efficacy of our algorithm using these examples, providing detailed analysis, and then discuss several potential future directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18212v2</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.FL</category>
      <category>cs.LO</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hazhar Rahmani, Abhishek N. Kulkarni, Jie Fu</dc:creator>
    </item>
    <item>
      <title>Keypoint Action Tokens Enable In-Context Imitation Learning in Robotics</title>
      <link>https://arxiv.org/abs/2403.19578</link>
      <description>arXiv:2403.19578v3 Announce Type: replace 
Abstract: We show that off-the-shelf text-based Transformers, with no additional training, can perform few-shot in-context visual imitation learning, mapping visual observations to action sequences that emulate the demonstrator's behaviour. We achieve this by transforming visual observations (inputs) and trajectories of actions (outputs) into sequences of tokens that a text-pretrained Transformer (GPT-4 Turbo) can ingest and generate, via a framework we call Keypoint Action Tokens (KAT). Despite being trained only on language, we show that these Transformers excel at translating tokenised visual keypoint observations into action trajectories, performing on par or better than state-of-the-art imitation learning (diffusion policies) in the low-data regime on a suite of real-world, everyday tasks. Rather than operating in the language domain as is typical, KAT leverages text-based Transformers to operate in the vision and action domains to learn general patterns in demonstration data for highly efficient imitation learning, indicating promising new avenues for repurposing natural language models for embodied tasks. Videos are available at https://www.robot-learning.uk/keypoint-action-tokens.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19578v3</guid>
      <category>cs.RO</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Norman Di Palo, Edward Johns</dc:creator>
    </item>
    <item>
      <title>EC-SLAM: Effectively Constrained Neural RGB-D SLAM with Sparse TSDF Encoding and Global Bundle Adjustment</title>
      <link>https://arxiv.org/abs/2404.13346</link>
      <description>arXiv:2404.13346v2 Announce Type: replace 
Abstract: We introduce EC-SLAM, a real-time dense RGB-D simultaneous localization and mapping (SLAM) system leveraging Neural Radiance Fields (NeRF). While recent NeRF-based SLAM systems have shown promising results, they have yet to fully exploit NeRF's potential to constrain pose optimization. EC-SLAM addresses this by using sparse parametric encodings and Truncated Signed Distance Fields (TSDF) to represent the map, enabling efficient fusion, reducing model parameters, and accelerating convergence. Our system also employs a globally constrained Bundle Adjustment (BA) strategy that capitalizes on NeRF's implicit loop closure correction capability, improving tracking accuracy by reinforcing constraints on keyframes most relevant to the current optimized frame. Furthermore, by integrating a feature-based and uniform sampling strategy that minimizes ineffective constraint points for pose optimization, we reduce the impact of random sampling in NeRF. Extensive evaluations on the Replica, ScanNet, and TUM datasets demonstrate state-of-the-art performance, with precise tracking and reconstruction accuracy achieved alongside real-time operation at up to 21 Hz.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13346v2</guid>
      <category>cs.RO</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guanghao Li, Qi Chen, YuXiang Yan, Jian Pu</dc:creator>
    </item>
    <item>
      <title>Long-Term Human Trajectory Prediction using 3D Dynamic Scene Graphs</title>
      <link>https://arxiv.org/abs/2405.00552</link>
      <description>arXiv:2405.00552v3 Announce Type: replace 
Abstract: We present a novel approach for long-term human trajectory prediction in indoor human-centric environments, which is essential for long-horizon robot planning in these environments. State-of-the-art human trajectory prediction methods are limited by their focus on collision avoidance and short-term planning, and their inability to model complex interactions of humans with the environment. In contrast, our approach overcomes these limitations by predicting sequences of human interactions with the environment and using this information to guide trajectory predictions over a horizon of up to 60s. We leverage Large Language Models (LLMs) to predict interactions with the environment by conditioning the LLM prediction on rich contextual information about the scene. This information is given as a 3D Dynamic Scene Graph that encodes the geometry, semantics, and traversability of the environment into a hierarchical representation. We then ground these interaction sequences into multi-modal spatio-temporal distributions over human positions using a probabilistic approach based on continuous-time Markov Chains. To evaluate our approach, we introduce a new semi-synthetic dataset of long-term human trajectories in complex indoor environments, which also includes annotations of human-object interactions. We show in thorough experimental evaluations that our approach achieves a 54% lower average negative log-likelihood and a 26.5% lower Best-of-20 displacement error compared to the best non-privileged (i.e., evaluated in a zero-shot fashion on the dataset) baselines for a time horizon of 60s.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00552v3</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/LRA.2024.3482169</arxiv:DOI>
      <dc:creator>Nicolas Gorlo, Lukas Schmid, Luca Carlone</dc:creator>
    </item>
    <item>
      <title>Mission Design for Unmanned Aerial Vehicles using Hybrid Probabilistic Logic Programs</title>
      <link>https://arxiv.org/abs/2406.03454</link>
      <description>arXiv:2406.03454v2 Announce Type: replace 
Abstract: Advanced Air Mobility (AAM) is a growing field that demands a deep understanding of legal, spatial and temporal concepts in navigation. Hence, any implementation of AAM is forced to deal with the inherent uncertainties of human-inhabited spaces. Enabling growth and innovation requires the creation of a system for safe and robust mission design, i.e., the way we formalize intentions and decide their execution as trajectories for the Unmanned Aerial Vehicle (UAV). Although legal frameworks have emerged to govern urban air spaces, their full integration into the decision process of autonomous agents and operators remains an open task. In this work we present ProMis, a system architecture for probabilistic mission design. It links the data available from various static and dynamic data sources with legal text and operator requirements by following principles of formal verification and probabilistic modeling. Hereby, ProMis enables the combination of low-level perception and high-level rules in AAM to infer validity over the UAV's state-space. To this end, we employ Hybrid Probabilistic Logic Programs (HPLP) as a unifying, intermediate representation between perception and action-taking. Furthermore, we present methods to connect ProMis with crowd-sourced map data by generating HPLP atoms that represent spatial relations in a probabilistic fashion. Our claims of the utility and generality of ProMis are supported by experiments on a diverse set of scenarios and a discussion of the computational demands associated with probabilistic missions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03454v2</guid>
      <category>cs.RO</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/ITSC57777.2023.10422083</arxiv:DOI>
      <dc:creator>Simon Kohaut, Benedict Flade, Devendra Singh Dhami, Julian Eggert, Kristian Kersting</dc:creator>
    </item>
    <item>
      <title>FetchBench: A Simulation Benchmark for Robot Fetching</title>
      <link>https://arxiv.org/abs/2406.11793</link>
      <description>arXiv:2406.11793v2 Announce Type: replace 
Abstract: Fetching, which includes approaching, grasping, and retrieving, is a critical challenge for robot manipulation tasks. Existing methods primarily focus on table-top scenarios, which do not adequately capture the complexities of environments where both grasping and planning are essential. To address this gap, we propose a new benchmark FetchBench, featuring diverse procedural scenes that integrate both grasping and motion planning challenges. Additionally, FetchBench includes a data generation pipeline that collects successful fetch trajectories for use in imitation learning methods. We implement multiple baselines from the traditional sense-plan-act pipeline to end-to-end behavior models. Our empirical analysis reveals that these methods achieve a maximum success rate of only 20%, indicating substantial room for improvement. Additionally, we identify key bottlenecks within the sense-plan-act pipeline and make recommendations based on the systematic analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11793v2</guid>
      <category>cs.RO</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Beining Han, Meenal Parakh, Derek Geng, Jack A Defay, Gan Luyang, Jia Deng</dc:creator>
    </item>
    <item>
      <title>Learning Social Cost Functions for Human-Aware Path Planning</title>
      <link>https://arxiv.org/abs/2407.10547</link>
      <description>arXiv:2407.10547v2 Announce Type: replace 
Abstract: Achieving social acceptance is one of the main goals of Social Robotic Navigation. Despite this topic has received increasing interest in recent years, most of the research has focused on driving the robotic agent along obstacle-free trajectories, planning around estimates of future human motion to respect personal distances and optimize navigation. However, social interactions in everyday life are also dictated by norms that do not strictly depend on movement, such as when standing at the end of a queue rather than cutting it. In this paper, we propose a novel method to recognize common social scenarios and modify a traditional planner's cost function to adapt to them. This solution enables the robot to carry out different social navigation behaviors that would not arise otherwise, maintaining the robustness of traditional navigation. Our approach allows the robot to learn different social norms with a single learned model, rather than having different modules for each task. As a proof of concept, we consider the tasks of queuing and respect interaction spaces of groups of people talking to one another, but the method can be extended to other human activities that do not involve motion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10547v2</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Eirale, Matteo Leonetti, Marcello Chiaberge</dc:creator>
    </item>
    <item>
      <title>Trajectory Optimization under Contact Timing Uncertainties</title>
      <link>https://arxiv.org/abs/2407.11478</link>
      <description>arXiv:2407.11478v2 Announce Type: replace 
Abstract: Most interesting problems in robotics (e.g., locomotion and manipulation) are realized through intermittent contact with the environment. Due to the perception and modeling errors, assuming an exact time for establishing contact with the environment is unrealistic. On the other hand, handling uncertainties in contact timing is notoriously difficult as it gives rise to either handling uncertain complementarity systems or solving combinatorial optimization problems at run-time. This work presents a novel optimal control formulation to find robust control policies under contact timing uncertainties. Our main novelty lies in casting the stochastic problem to a deterministic optimization over the uncertainty set that ensures robustness criterion satisfaction of candidate pre-contact states and optimizes for contact-relevant objectives. This way, we only need to solve a manageable standard nonlinear programming problem without complementarity constraints or combinatorial explosion. Our simulation results on multiple simplified locomotion and manipulation tasks demonstrate the robustness of our uncertainty-aware formulation compared to the nominal optimal control formulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11478v2</guid>
      <category>cs.RO</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haizhou Zhao, Majid Khadiv</dc:creator>
    </item>
    <item>
      <title>Context-aware Mamba-based Reinforcement Learning for social robot navigation</title>
      <link>https://arxiv.org/abs/2408.02661</link>
      <description>arXiv:2408.02661v2 Announce Type: replace 
Abstract: Social robot navigation (SRN) is a relevant problem that involves navigating a pedestrian-rich environment in a socially acceptable manner. It is an essential part of making social robots effective in pedestrian-rich settings. The use cases of such robots could vary from companion robots to warehouse robots to autonomous wheelchairs. In recent years, deep reinforcement learning has been increasingly used in research on social robot navigation. Our work introduces CAMRL (Context-Aware Mamba-based Reinforcement Learning). Mamba is a new deep learning-based State Space Model (SSM) that has achieved results comparable to transformers in sequencing tasks. CAMRL uses Mamba to determine the robot's next action, which maximizes the value of the next state predicted by the neural network, enabling the robot to navigate effectively based on the rewards assigned. We evaluate CAMRL alongside existing solutions (CADRL, LSTM-RL, SARL) using a rigorous testing dataset which involves a variety of densities and environment behaviors based on ORCA and SFM, thus, demonstrating that CAMRL achieves higher success rates, minimizes collisions, and maintains safer distances from pedestrians. This work introduces a new SRN planner, showcasing the potential for deep-state space models for robot navigation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02661v2</guid>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Syed Muhammad Mustafa, Omema Rizvi, Zain Ahmed Usmani, Abdul Basit Memon, Muhammad Mobeen Movania</dc:creator>
    </item>
    <item>
      <title>Text2Interaction: Establishing Safe and Preferable Human-Robot Interaction</title>
      <link>https://arxiv.org/abs/2408.06105</link>
      <description>arXiv:2408.06105v3 Announce Type: replace 
Abstract: Adjusting robot behavior to human preferences can require intensive human feedback, preventing quick adaptation to new users and changing circumstances. Moreover, current approaches typically treat user preferences as a reward, which requires a manual balance between task success and user satisfaction. To integrate new user preferences in a zero-shot manner, our proposed Text2Interaction framework invokes large language models to generate a task plan, motion preferences as Python code, and parameters of a safety controller. By maximizing the combined probability of task completion and user satisfaction instead of a weighted sum of rewards, we can reliably find plans that fulfill both requirements. We find that 83 % of users working with Text2Interaction agree that it integrates their preferences into the plan of the robot, and 94 % prefer Text2Interaction over the baseline. Our ablation study shows that Text2Interaction aligns better with unseen preferences than other baselines while maintaining a high success rate. Real-world demonstrations and code are made available at sites.google.com/view/text2interaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06105v3</guid>
      <category>cs.RO</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jakob Thumm, Christopher Agia, Marco Pavone, Matthias Althoff</dc:creator>
    </item>
    <item>
      <title>Pyramid-Monozone Synergistic Grasping Policy in Dense Clutter</title>
      <link>https://arxiv.org/abs/2409.06959</link>
      <description>arXiv:2409.06959v2 Announce Type: replace 
Abstract: Grasping a diverse range of novel objects in dense clutter poses a great challenge to robotic automation mainly due to the occlusion problem. In this work, we propose the Pyramid-Monozone Synergistic Grasping Policy (PMSGP) that enables robots to effectively handle occlusions during grasping. Specifically, we initially construct the Pyramid Sequencing Policy (PSP) to sequence each object in cluttered scenes into a pyramid structure. By isolating objects layer-by-layer, the grasp detection model is allowed to focus on a single layer during each grasp. Then, we devise the Monozone Sampling Policy (MSP) to sample the grasp candidates in the top layer. Through this manner, each grasp targets the topmost object, thereby effectively avoiding most occlusions. We performed more than 7,000 real-world grasping in densely cluttered scenes with 300 novel objects, demonstrating that PMSGP significantly outperforms seven competitive grasping methods. More importantly, we tested the grasping performance of PMSGP in extremely cluttered scenes involving 100 different household goods, and found that PMSGP pushed the grasp success rate to 84.9\%. To the best of our knowledge, no previous work has demonstrated similar performance. All grasping videos are available at: https://www.youtube.com/@chenghaoli4532/playlists.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06959v2</guid>
      <category>cs.RO</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chenghao Li, Nak Young Chong</dc:creator>
    </item>
    <item>
      <title>Discrete time model predictive control for humanoid walking with step adjustment</title>
      <link>https://arxiv.org/abs/2410.06790</link>
      <description>arXiv:2410.06790v2 Announce Type: replace 
Abstract: This paper presents a Discrete-Time Model Predictive Controller (MPC) for humanoid walking with online footstep adjustment. The proposed controller utilizes a hierarchical control approach. The high-level controller uses a low-dimensional Linear Inverted Pendulum Model (LIPM) to determine desired foot placement and Center of Mass (CoM) motion, to prevent falls while maintaining the desired velocity. A Task Space Controller (TSC) then tracks the desired motion obtained from the high-level controller, exploiting the whole-body dynamics of the humanoid. Our approach differs from existing MPC methods for walking pattern generation by not relying on a predefined foot-plan or a reference center of pressure (CoP) trajectory. The overall approach is tested in simulation on a torque-controlled Humanoid Robot. Results show that proposed control approach generates stable walking and prevents fall against push disturbances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06790v2</guid>
      <category>cs.RO</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vishnu Joshi, Suraj Kumar, Nithin V, Shishir Kolathaya</dc:creator>
    </item>
    <item>
      <title>PAPL-SLAM: Principal Axis-Anchored Monocular Point-Line SLAM</title>
      <link>https://arxiv.org/abs/2410.12324</link>
      <description>arXiv:2410.12324v2 Announce Type: replace 
Abstract: In point-line SLAM systems, the utilization of line structural information and the optimization of lines are two significant problems. The former is usually addressed through structural regularities, while the latter typically involves using minimal parameter representations of lines in optimization. However, separating these two steps leads to the loss of constraint information to each other. We anchor lines with similar directions to a principal axis and optimize them with $n+2$ parameters for $n$ lines, solving both problems together. Our method considers scene structural information, which can be easily extended to different world hypotheses while significantly reducing the number of line parameters to be optimized, enabling rapid and accurate mapping and tracking. To further enhance the system's robustness and avoid mismatch, we have modeled the line-axis probabilistic data association and provided the algorithm for axis creation, updating, and optimization. Additionally, considering that most real-world scenes conform to the Atlanta World hypothesis, we provide a structural line detection strategy based on vertical priors and vanishing points. Experimental results and ablation studies on various indoor and outdoor datasets demonstrate the effectiveness of our system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12324v2</guid>
      <category>cs.RO</category>
      <category>cs.CV</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Guanghao Li, Yu Cao, Qi Chen, Yifan Yang, Jian Pu</dc:creator>
    </item>
    <item>
      <title>Learning a Stable, Safe, Distributed Feedback Controller for a Heterogeneous Platoon of Autonomous Vehicles</title>
      <link>https://arxiv.org/abs/2404.12474</link>
      <description>arXiv:2404.12474v2 Announce Type: replace-cross 
Abstract: Platooning of autonomous vehicles has the potential to increase safety and fuel efficiency on highways. The goal of platooning is to have each vehicle drive at a specified speed (set by the leader) while maintaining a safe distance from its neighbors. Many prior works have analyzed various controllers for platooning, most commonly linear feedback and distributed model predictive controllers. In this work, we introduce an algorithm for learning a stable, safe, distributed controller for a heterogeneous platoon. Our algorithm relies on recent developments in learning neural network stability certificates. We train a controller for autonomous platooning in simulation and evaluate its performance on hardware with a platoon of four F1Tenth vehicles. We then perform further analysis in simulation with a platoon of 100 vehicles. Experimental results demonstrate the practicality of the algorithm and the learned controller by comparing the performance of the neural network controller to linear feedback and distributed model predictive controllers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12474v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael H. Shaham, Taskin Padir</dc:creator>
    </item>
    <item>
      <title>LED: Light Enhanced Depth Estimation at Night</title>
      <link>https://arxiv.org/abs/2409.08031</link>
      <description>arXiv:2409.08031v2 Announce Type: replace-cross 
Abstract: Nighttime camera-based depth estimation is a highly challenging task, especially for autonomous driving applications, where accurate depth perception is essential for ensuring safe navigation. We aim to improve the reliability of perception systems at night time, where models trained on daytime data often fail in the absence of precise but costly LiDAR sensors. In this work, we introduce Light Enhanced Depth (LED), a novel cost-effective approach that significantly improves depth estimation in low-light environments by harnessing a pattern projected by high definition headlights available in modern vehicles. LED leads to significant performance boosts across multiple depth-estimation architectures (encoder-decoder, Adabins, DepthFormer) both on synthetic and real datasets. Furthermore, increased performances beyond illuminated areas reveal a holistic enhancement in scene understanding. Finally, we release the Nighttime Synthetic Drive Dataset, a new synthetic and photo-realistic nighttime dataset, which comprises 49,990 comprehensively annotated images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08031v2</guid>
      <category>cs.CV</category>
      <category>cs.RO</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Simon de Moreau, Yasser Almehio, Andrei Bursuc, Hafid El-Idrissi, Bogdan Stanciulescu, Fabien Moutarde</dc:creator>
    </item>
    <item>
      <title>Trust or Bust: Ensuring Trustworthiness in Autonomous Weapon Systems</title>
      <link>https://arxiv.org/abs/2410.10284</link>
      <description>arXiv:2410.10284v2 Announce Type: replace-cross 
Abstract: The integration of Autonomous Weapon Systems (AWS) into military operations presents both significant opportunities and challenges. This paper explores the multifaceted nature of trust in AWS, emphasising the necessity of establishing reliable and transparent systems to mitigate risks associated with bias, operational failures, and accountability. Despite advancements in Artificial Intelligence (AI), the trustworthiness of these systems, especially in high-stakes military applications, remains a critical issue. Through a systematic review of existing literature, this research identifies gaps in the understanding of trust dynamics during the development and deployment phases of AWS. It advocates for a collaborative approach that includes technologists, ethicists, and military strategists to address these ongoing challenges. The findings underscore the importance of Human-Machine teaming and enhancing system intelligibility to ensure accountability and adherence to International Humanitarian Law. Ultimately, this paper aims to contribute to the ongoing discourse on the ethical implications of AWS and the imperative for trustworthy AI in defense contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10284v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.RO</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kasper Cools, Clara Maathuis</dc:creator>
    </item>
  </channel>
</rss>
