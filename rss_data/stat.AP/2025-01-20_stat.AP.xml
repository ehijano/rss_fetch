<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 20 Jan 2025 05:00:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>An optimal transport based embedding to quantify the distance between playing styles in collective sports</title>
      <link>https://arxiv.org/abs/2501.10299</link>
      <description>arXiv:2501.10299v1 Announce Type: new 
Abstract: This study presents a quantitative framework to compare teams in collective sports with respect to their style of play. The style of play is characterized by the team's spatial distribution over a collection of frames. As a first step, we introduce an optimal transport-based embedding to map frames into Euclidean space, allowing for the efficient computation of a distance. Then, building on this frame-level analysis, we leverage quantization to establish a similarity metric between teams based on a collection of frames from their games. For illustration, we present an analysis of a collection of games from the 2021-2022 Ligue 1 season. We are able to retrieve relevant clusters of game situations and calculate the similarity matrix between teams in terms of style of play. Additionally, we demonstrate the strength of the embedding as a preprocessing tool for relevant prediction tasks. Likewise, we apply our framework to analyze the dynamics in the first half of the NBA season in 2015-2016.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10299v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Ali Baouan, Mathieu Rosenbaum, Sergio Pulido</dc:creator>
    </item>
    <item>
      <title>Cheap Subsampling bootstrap confidence intervals for fast and robust inference in biostatistics</title>
      <link>https://arxiv.org/abs/2501.10289</link>
      <description>arXiv:2501.10289v1 Announce Type: cross 
Abstract: Bootstrapping is often applied to get confidence limits for semiparametric inference of a target parameter in the presence of nuisance parameters. Bootstrapping with replacement can be computationally expensive and problematic when cross-validation is used in the estimation algorithm due to duplicate observations in the bootstrap samples. We provide a valid, fast, easy-to-implement subsampling bootstrap method for constructing confidence intervals for asymptotically linear estimators and discuss its application to semiparametric causal inference. Our method, inspired by the Cheap Bootstrap (Lam, 2022), leverages the quantiles of a t-distribution and has the desired coverage with few bootstrap replications. We show that the method is asymptotically valid if the subsample size is chosen appropriately as a function of the sample size. We illustrate our method with data from the LEADER trial (Marso et al., 2016), obtaining confidence intervals for a longitudinal targeted minimum loss-based estimator (van der Laan and Gruber, 2012). Through a series of empirical experiments, we also explore the impact of subsample size, sample size, and the number of bootstrap repetitions on the performance of the confidence interval.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10289v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johan Sebastian Ohlendorff, Anders Munch, Kathrine Kold S{\o}rensen, Thomas Alexander Gerds</dc:creator>
    </item>
    <item>
      <title>Coordinated Trading Strategies for Battery Storage in Reserve and Spot Markets</title>
      <link>https://arxiv.org/abs/2406.08390</link>
      <description>arXiv:2406.08390v3 Announce Type: replace-cross 
Abstract: Quantity and price risks are key uncertainties market participants face in electricity markets with increased volatility, for instance, due to high shares of renewables. From day ahead until real-time, there is a large variation in the best available information, leading to price changes that flexible assets, such as battery storage, can exploit economically. This study contributes to understanding how coordinated bidding strategies can enhance multi-market trading and large-scale energy storage integration. Our findings shed light on the complexities arising from interdependencies and the high-dimensional nature of the problem. We show how stochastic dual dynamic programming is a suitable solution technique for such an environment. We include the three markets of the frequency containment reserve, day-ahead, and intraday in stochastic modelling and develop a multi-stage stochastic program. Prices are represented in a multidimensional Markov Chain, following the scheduling of the markets and allowing for time-dependent randomness. Using the example of a battery storage in the German energy sector, we provide valuable insights into the technical aspects of our method and the economic feasibility of battery storage operation. We find that capacity reservation in the frequency containment reserve dominates over the battery's cycling in spot markets at the given resolution on prices in 2022. In an adjusted price environment, we find that coordination can yield an additional value of up to 12.5%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08390v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Paul E. Seifert, Emil Kraft, Steffen Bakker, Stein-Erik Fleten</dc:creator>
    </item>
    <item>
      <title>Bayesian Event Categorization Matrix Approach for Explosion Monitoring</title>
      <link>https://arxiv.org/abs/2409.18227</link>
      <description>arXiv:2409.18227v3 Announce Type: replace-cross 
Abstract: Current efforts to correctly categorize natural events from suspected explosion sources with data that is collected by ground- or space-based sensors presents historical challenges that remain unaddressed by the Event Categorization Matrix (ECM) model. Smaller historical events (lower yield explosions) often include only sparse observations among few modalities and can therefore lack a complete set of discriminants. The covariance structures can also vary significantly between such observations of event (source-type) categories. Both obstacles are problematic for the ``classic'' Event Categorization Matrix model. Our work addresses this gap and presents a Bayesian update to the previous Event Categorization Matrix model, termed the Bayesian Event Categorization Matrix model, which can be trained on partial observations and does not rely on a pooled covariance structure. We further augment the Event Categorization Matrix model with Bayesian Decision Theory so that false negative or false positive rates of an event categorization can be reduced in an intuitive manner. To demonstrate improved categorization rates for the Bayesian Event Categorization Matrix model, we compare an array of Bayesian and classic models with multiple performance metrics using Monte Carlo experiments. We use both synthetic and real data. Our Bayesian models show consistent gains in overall accuracy and a lower false negative rates relative to the classic Event Categorization Matrix model. We propose future avenues to improve Bayesian Event Categorization Matrix models for further improving decision-making and predictive capability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18227v3</guid>
      <category>physics.geo-ph</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Scott Koermer, Joshua D. Carmichael, Brian J. Williams</dc:creator>
    </item>
  </channel>
</rss>
