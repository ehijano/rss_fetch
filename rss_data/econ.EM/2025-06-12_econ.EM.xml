<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Jun 2025 04:07:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Diffusion index forecasts under weaker loadings: PCA, ridge regression, and random projections</title>
      <link>https://arxiv.org/abs/2506.09575</link>
      <description>arXiv:2506.09575v1 Announce Type: new 
Abstract: We study the accuracy of forecasts in the diffusion index forecast model with possibly weak loadings. The default option to construct forecasts is to estimate the factors through principal component analysis (PCA) on the available predictor matrix, and use the estimated factors to forecast the outcome variable. Alternatively, we can directly relate the outcome variable to the predictors through either ridge regression or random projections. We establish that forecasts based on PCA, ridge regression and random projections are consistent for the conditional mean under the same assumptions on the strength of the loadings. However, under weaker loadings the convergence rate is lower for ridge and random projections if the time dimension is small relative to the cross-section dimension. We assess the relevance of these findings in an empirical setting by comparing relative forecast accuracy for monthly macroeconomic and financial variables using different window sizes. The findings support the theoretical results, and at the same time show that regularization-based procedures may be more robust in settings not covered by the developed theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09575v1</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tom Boot, Bart Keijsers</dc:creator>
    </item>
    <item>
      <title>Estimating the Number of Components in Panel Data Finite Mixture Regression Models with an Application to Production Function Heterogeneity</title>
      <link>https://arxiv.org/abs/2506.09666</link>
      <description>arXiv:2506.09666v1 Announce Type: new 
Abstract: This paper develops statistical methods for determining the number of components in panel data finite mixture regression models with regression errors independently distributed as normal or more flexible normal mixtures. We analyze the asymptotic properties of the likelihood ratio test (LRT) and information criteria (AIC and BIC) for model selection in both conditionally independent and dynamic panel settings. Unlike cross-sectional normal mixture models, we show that panel data structures eliminate higher-order degeneracy problems while retaining issues of unbounded likelihood and infinite Fisher information. Addressing these challenges, we derive the asymptotic null distribution of the LRT statistic as the maximum of random variables and develop a sequential testing procedure for consistent selection of the number of components. Our theoretical analysis also establishes the consistency of BIC and the inconsistency of AIC. Empirical application to Chilean manufacturing data reveals significant heterogeneity in production technology, with substantial variation in output elasticities of material inputs and factor-augmented technological processes within narrowly defined industries, indicating plant-specific variation in production functions beyond Hicks-neutral technological differences. These findings contrast sharply with the standard practice of assuming a homogeneous production function and highlight the necessity of accounting for unobserved plant heterogeneity in empirical production analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09666v1</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yu Hao, Hiroyuki Kasahara</dc:creator>
    </item>
    <item>
      <title>How much is too much? Measuring divergence from Benford's Law with the Equivalent Contamination Proportion (ECP)</title>
      <link>https://arxiv.org/abs/2506.09915</link>
      <description>arXiv:2506.09915v1 Announce Type: new 
Abstract: Conformity with Benford's Law is widely used to detect irregularities in numerical datasets, particularly in accounting, finance, and economics. However, the statistical tools commonly used for this purpose (such as Chi-squared, MAD, or KS) suffer from three key limitations: sensitivity to sample size, lack of interpretability of their scale, and the absence of a common metric that allows for comparison across different statistics. This paper introduces the Equivalent Contamination Proportion (ECP) to address these issues. Defined as the proportion of contamination in a hypothetical Benford-conforming sample such that the expected value of the divergence statistic matches the one observed in the actual data, the ECP provides a continuous and interpretable measure of deviation (ranging from 0 to 1), is robust to sample size, and offers consistent results across different divergence statistics under mild conditions. Closed-form and simulation-based methods are developed for estimating the ECP, and, through a retrospective analysis of three influential studies, it is shown how the ECP can complement the information provided by traditional divergence statistics and enhance the interpretation of results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09915v1</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manuel Cano-Rodriguez</dc:creator>
    </item>
    <item>
      <title>Difference-in-Differences Designs: A Practitioner's Guide</title>
      <link>https://arxiv.org/abs/2503.13323</link>
      <description>arXiv:2503.13323v2 Announce Type: replace 
Abstract: Difference-in-differences (DiD) is arguably the most popular quasi-experimental research design. Its canonical form, with two groups and two periods, is well-understood. However, empirical practices can be ad hoc when researchers go beyond that simple case. This article provides an organizing framework for discussing different types of DiD designs and their associated DiD estimators. It discusses covariates, weights, handling multiple periods, and staggered treatments. The organizational framework, however, applies to other extensions of DiD methods as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13323v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Andrew Baker, Brantly Callaway, Scott Cunningham, Andrew Goodman-Bacon, Pedro H. C. Sant'Anna</dc:creator>
    </item>
  </channel>
</rss>
