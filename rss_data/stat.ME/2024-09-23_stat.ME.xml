<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Sep 2024 03:14:27 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Properly constrained reference priors decay rates for efficient and robust posterior inference</title>
      <link>https://arxiv.org/abs/2409.13041</link>
      <description>arXiv:2409.13041v1 Announce Type: new 
Abstract: In Bayesian analysis, reference priors are widely recognized for their objective nature. Yet, they often lead to intractable and improper priors, which complicates their application. Besides, informed prior elicitation methods are penalized by the subjectivity of the choices they require. In this paper, we aim at proposing a reconciliation of the aforementioned aspects. Leveraging the objective aspect of reference prior theory, we introduce two strategies of constraint incorporation to build tractable reference priors. One provides a simple and easy-to-compute solution when the improper aspect is not questioned, and the other introduces constraints to ensure the reference prior is proper, or it provides proper posterior. Our methodology emphasizes the central role of Jeffreys prior decay rates in this process, and the practical applicability of our results is demonstrated using an example taken from the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13041v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antoine Van Biesbroeck</dc:creator>
    </item>
    <item>
      <title>Forecasting Causal Effects of Future Interventions: Confounding and Transportability Issues</title>
      <link>https://arxiv.org/abs/2409.13060</link>
      <description>arXiv:2409.13060v1 Announce Type: new 
Abstract: Recent developments in causal inference allow us to transport a causal effect of a time-fixed treatment from a randomized trial to a target population across space but within the same time frame. In contrast to transportability across space, transporting causal effects across time or forecasting causal effects of future interventions is more challenging due to time-varying confounders and time-varying effect modifiers. In this article, we seek to formally clarify the causal estimands for forecasting causal effects over time and the structural assumptions required to identify these estimands. Specifically, we develop a set of novel nonparametric identification formulas--g-computation formulas--for these causal estimands, and lay out the conditions required to accurately forecast causal effects from a past observed sample to a future population in a future time window. Our overaching objective is to leverage the modern causal inference theory to provide a theoretical framework for investigating whether the effects seen in a past sample would carry over to a new future population. Throughout the article, a working example addressing the effect of public policies or social events on COVID-related deaths is considered to contextualize the developments of analytical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13060v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laura Forastiere, Fan Li, Michela Baccini</dc:creator>
    </item>
    <item>
      <title>Incremental Causal Effect for Time to Treatment Initialization</title>
      <link>https://arxiv.org/abs/2409.13097</link>
      <description>arXiv:2409.13097v1 Announce Type: new 
Abstract: We consider time to treatment initialization. This can commonly occur in preventive medicine, such as disease screening and vaccination; it can also occur with non-fatal health conditions such as HIV infection without the onset of AIDS; or in tech industry where items wait to be reviewed manually as abusive or not, etc. While traditional causal inference focused on `when to treat' and its effects, including their possible dependence on subject characteristics, we consider the incremental causal effect when the intensity of time to treatment initialization is intervened upon. We provide identification of the incremental causal effect without the commonly required positivity assumption, as well as an estimation framework using inverse probability weighting. We illustrate our approach via simulation, and apply it to a rheumatoid arthritis study to evaluate the incremental effect of time to start methotrexate on joint pain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13097v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew Ying, Zhichen Zhao, Ronghui Xu</dc:creator>
    </item>
    <item>
      <title>Non-parametric Replication of Instrumental Variable Estimates Across Studies</title>
      <link>https://arxiv.org/abs/2409.13140</link>
      <description>arXiv:2409.13140v1 Announce Type: new 
Abstract: Replicating causal estimates across different cohorts is crucial for increasing the integrity of epidemiological studies. However, strong assumptions regarding unmeasured confounding and effect modification often hinder this goal. By employing an instrumental variable (IV) approach and targeting the local average treatment effect (LATE), these assumptions can be relaxed to some degree; however, little work has addressed the replicability of IV estimates. In this paper, we propose a novel survey weighted LATE (SWLATE) estimator that incorporates unknown sampling weights and leverages machine learning for flexible modeling of nuisance functions, including the weights. Our approach, based on influence function theory and cross-fitting, provides a doubly-robust and efficient framework for valid inference, aligned with the growing "double machine learning" literature. We further extend our method to provide bounds on a target population ATE. The effectiveness of our approach, particularly in non-linear settings, is demonstrated through simulations and applied to a Mendelian randomization analysis of the relationship between triglycerides and cognitive decline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13140v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roy S. Zawadzki, Daniel L. Gillen</dc:creator>
    </item>
    <item>
      <title>Nonparametric Causal Survival Analysis with Clustered Interference</title>
      <link>https://arxiv.org/abs/2409.13190</link>
      <description>arXiv:2409.13190v1 Announce Type: new 
Abstract: Inferring treatment effects on a survival time outcome based on data from an observational study is challenging due to the presence of censoring and possible confounding. An additional challenge occurs when a unit's treatment affects the outcome of other units, i.e., there is interference. In some settings, units may be grouped into clusters such that it is reasonable to assume interference only occurs within clusters, i.e., there is clustered interference. In this paper, methods are developed which can accommodate confounding, censored outcomes, and clustered interference. The approach avoids parametric assumptions and permits inference about counterfactual scenarios corresponding to any stochastic policy which modifies the propensity score distribution, and thus may have application across diverse settings. The proposed nonparametric sample splitting estimators allow for flexible data-adaptive estimation of nuisance functions and are consistent and asymptotically normal with parametric convergence rates. Simulation studies demonstrate the finite sample performance of the proposed estimators, and the methods are applied to a cholera vaccine study in Bangladesh.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13190v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chanhwa Lee, Donglin Zeng, Michael Emch, John D. Clemens, Michael G. Hudgens</dc:creator>
    </item>
    <item>
      <title>Spatial Sign based Principal Component Analysis for High Dimensional Data</title>
      <link>https://arxiv.org/abs/2409.13267</link>
      <description>arXiv:2409.13267v1 Announce Type: new 
Abstract: This article focuses on the robust principal component analysis (PCA) of high-dimensional data with elliptical distributions. We investigate the PCA of the sample spatial-sign covariance matrix in both nonsparse and sparse contexts, referring to them as SPCA and SSPCA, respectively. We present both nonasymptotic and asymptotic analyses to quantify the theoretical performance of SPCA and SSPCA. In sparse settings, we demonstrate that SSPCA, implemented through a combinatoric program, achieves the optimal rate of convergence. Our proposed SSPCA method is computationally efficient and exhibits robustness against heavy-tailed distributions compared to existing methods. Simulation studies and real-world data applications further validate the superiority of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13267v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Long Feng</dc:creator>
    </item>
    <item>
      <title>A Two-stage Inference Procedure for Sample Local Average Treatment Effects in Randomized Experiments</title>
      <link>https://arxiv.org/abs/2409.13300</link>
      <description>arXiv:2409.13300v1 Announce Type: new 
Abstract: In a given randomized experiment, individuals are often volunteers and can differ in important ways from a population of interest. It is thus of interest to focus on the sample at hand. This paper focuses on inference about the sample local average treatment effect (LATE) in randomized experiments with non-compliance. We present a two-stage procedure that provides asymptotically correct coverage rate of the sample LATE in randomized experiments. The procedure uses a first-stage test to decide whether the instrument is strong or weak, and uses different confidence sets depending on the first-stage result. Proofs of the procedure is developed for the situation with and without regression adjustment and for two experimental designs (complete randomization and Mahalaonobis distance based rerandomization). Finite sample performance of the methods are studied using extensive Monte Carlo simulations and the methods are applied to data from a voter encouragement experiment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13300v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhen Zhong, Per Johansson, Junni L. Zhang</dc:creator>
    </item>
    <item>
      <title>Two-level D- and A-optimal main-effects designs with run sizes one and two more than a multiple of four</title>
      <link>https://arxiv.org/abs/2409.13336</link>
      <description>arXiv:2409.13336v1 Announce Type: new 
Abstract: For run sizes that are a multiple of four, the literature offers many two-level designs that are D- and A-optimal for the main-effects model and minimize the aliasing between main effects and interaction effects and among interaction effects. For run sizes that are not a multiple of four, no conclusive results are known. In this paper, we propose two algorithms that generate all non-isomorphic D- and A-optimal main-effects designs for run sizes that are one and two more than a multiple of four. We enumerate all such designs for run sizes up to 18, report the numbers of designs we obtained, and identify those that minimize the aliasing between main effects and interaction effects and among interaction effects. Finally, we compare the minimally aliased designs we found with benchmark designs from the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13336v1</guid>
      <category>stat.ME</category>
      <category>math.CO</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mohammed Saif Ismail Hameed, Jose N\'u\~nez Ares, Eric D. Schoen, Peter Goos</dc:creator>
    </item>
    <item>
      <title>Interpretable meta-analysis of model or marker performance</title>
      <link>https://arxiv.org/abs/2409.13458</link>
      <description>arXiv:2409.13458v1 Announce Type: new 
Abstract: Conventional meta analysis of model performance conducted using datasources from different underlying populations often result in estimates that cannot be interpreted in the context of a well defined target population. In this manuscript we develop methods for meta-analysis of several measures of model performance that are interpretable in the context of a well defined target population when the populations underlying the datasources used in the meta analysis are heterogeneous. This includes developing identifiablity conditions, inverse-weighting, outcome model, and doubly robust estimator. We illustrate the methods using simulations and data from two large lung cancer screening trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13458v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jon A. Steingrimsson, Lan Wen, Sarah Voter, Issa J. Dahabreh</dc:creator>
    </item>
    <item>
      <title>Efficiency gain in association studies based on population surveys by augmenting outcome data from the target population</title>
      <link>https://arxiv.org/abs/2409.13479</link>
      <description>arXiv:2409.13479v1 Announce Type: new 
Abstract: Routinely collected nation-wide registers contain socio-economic and health-related information from a large number of individuals. However, important information on lifestyle, biological and other risk factors is available at most for small samples of the population through surveys. A majority of health surveys lack detailed medical information necessary for assessing the disease burden. Hence, traditionally data from the registers and the surveys are combined to have necessary information for the survey sample. Our idea is to base analyses on a combined sample obtained by adding a (large) sample of individuals from the population to the survey sample. The main objective is to assess the bias and gain in efficiency of such combined analyses with a binary or time-to-event outcome. We employ (i) the complete-case analysis (CCA) using the respondents of the survey, (ii) analysis of the full survey sample with both unit- and item-nonresponse under the missing at random (MAR) assumption and (iii) analysis of the combined sample under mixed type of missing data mechanism. We handle the missing data using multiple imputation (MI)-based analysis in (ii) and (iii). We utilize simulated as well as empirical data on ischemic heart disease obtained from the Finnish population. Our results suggested that the MI methods improved the efficiency of the estimates when we used the combined data for a binary outcome, but in the case of a time-to-event outcome the CCA was at least as good as the MI using the larger datasets, in terms of the the mean absolute and squared errors. Increasing the participation in the surveys and having good statistical methods for handling missing covariate data when the outcome is time-to-event would be needed for implementation of the proposed ideas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13479v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tommi H\"ark\"anen, Sangita Kulathinal, Arya Panthalanickal Vijayakumar</dc:creator>
    </item>
    <item>
      <title>A simple but powerful tail index regression</title>
      <link>https://arxiv.org/abs/2409.13531</link>
      <description>arXiv:2409.13531v1 Announce Type: cross 
Abstract: This paper introduces a flexible framework for the estimation of the conditional tail index of heavy tailed distributions. In this framework, the tail index is computed from an auxiliary linear regression model that facilitates estimation and inference based on established econometric methods, such as ordinary least squares (OLS), least absolute deviations, or M-estimation. We show theoretically and via simulations that OLS provides interesting results. Our Monte Carlo results highlight the adequate finite sample properties of the OLS tail index estimator computed from the proposed new framework and contrast its behavior to that of tail index estimates obtained by maximum likelihood estimation of exponential regression models, which is one of the approaches currently in use in the literature. An empirical analysis of the impact of determinants of the conditional left- and right-tail indexes of commodities' return distributions highlights the empirical relevance of our proposed approach. The novel framework's flexibility allows for extensions and generalizations in various directions, empowering researchers and practitioners to straightforwardly explore a wide range of research questions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13531v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jo\~ao Nicolau, Paulo M. M. Rodrigues</dc:creator>
    </item>
    <item>
      <title>Morphological Detection and Classification of Microplastics and Nanoplastics Emerged from Consumer Products by Deep Learning</title>
      <link>https://arxiv.org/abs/2409.13688</link>
      <description>arXiv:2409.13688v1 Announce Type: cross 
Abstract: Plastic pollution presents an escalating global issue, impacting health and environmental systems, with micro- and nanoplastics found across mediums from potable water to air. Traditional methods for studying these contaminants are labor-intensive and time-consuming, necessitating a shift towards more efficient technologies. In response, this paper introduces micro- and nanoplastics (MiNa), a novel and open-source dataset engineered for the automatic detection and classification of micro and nanoplastics using object detection algorithms. The dataset, comprising scanning electron microscopy images simulated under realistic aquatic conditions, categorizes plastics by polymer type across a broad size spectrum. We demonstrate the application of state-of-the-art detection algorithms on MiNa, assessing their effectiveness and identifying the unique challenges and potential of each method. The dataset not only fills a critical gap in available resources for microplastic research but also provides a robust foundation for future advancements in the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13688v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hadi Rezvani, Navid Zarrabi, Ishaan Mehta, Christopher Kolios, Hussein Ali Jaafar, Cheng-Hao Kao, Sajad Saeedi, Nariman Yousefi</dc:creator>
    </item>
    <item>
      <title>Perturbation graphs, invariant prediction and causal relations in psychology</title>
      <link>https://arxiv.org/abs/2109.00404</link>
      <description>arXiv:2109.00404v2 Announce Type: replace 
Abstract: Networks (graphs) in psychology are often restricted to settings without interventions. Here we consider a framework borrowed from biology that involves multiple interventions from different contexts (observations and experiments) in a single analysis. The method is called perturbation graphs. In gene regulatory networks, the induced change in one gene is measured on all other genes in the analysis, thereby assessing possible causal relations. This is repeated for each gene in the analysis. A perturbation graph leads to the correct set of causes (not necessarily direct causes). Subsequent pruning of paths in the graph (called transitive reduction) should reveal direct causes. We show that transitive reduction will not in general lead to the correct underlying graph. We also show that invariant causal prediction is a generalisation of the perturbation graph method, where including additional variables does reveal direct causes, and thereby replacing transitive reduction. We conclude that perturbation graphs provide a promising new tool for experimental designs in psychology, and combined with invariant causal prediction make it possible to reveal direct causes instead of causal paths. As an illustration we apply the perturbation graphs and invariant causal prediction to a data set about attitudes on meat consumption and to a time series of a patient diagnosed with major depression disorder.</description>
      <guid isPermaLink="false">oai:arXiv.org:2109.00404v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lourens Waldorp, Jolanda Kossakowski, Han L. J. van der Maas</dc:creator>
    </item>
    <item>
      <title>Causal Identification for Complex Functional Longitudinal Studies</title>
      <link>https://arxiv.org/abs/2206.12525</link>
      <description>arXiv:2206.12525v5 Announce Type: replace 
Abstract: Real-time monitoring in modern medical research introduces functional longitudinal data, characterized by continuous-time measurements of outcomes, treatments, and confounders. This complexity leads to uncountably infinite treatment-confounder feedbacks, which traditional causal inference methodologies cannot handle. Inspired by the coarsened data framework, we adopt stochastic process theory, measure theory, and net convergence to propose a nonparametric causal identification framework. This framework generalizes classical g-computation, inverse probability weighting, and doubly robust formulas, accommodating time-varying outcomes subject to mortality and censoring for functional longitudinal data. We examine our framework through Monte Carlo simulations. Our approach addresses significant gaps in current methodologies, providing a solution for functional longitudinal data and paving the way for future estimation work in this domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.12525v5</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew Ying</dc:creator>
    </item>
    <item>
      <title>Estimating the limiting shape of bivariate scaled sample clouds: with additional benefits of self-consistent inference for existing extremal dependence properties</title>
      <link>https://arxiv.org/abs/2207.02626</link>
      <description>arXiv:2207.02626v4 Announce Type: replace 
Abstract: The key to successful statistical analysis of bivariate extreme events lies in flexible modelling of the tail dependence relationship between the two variables. In the extreme value theory literature, various techniques are available to model separate aspects of tail dependence, based on different asymptotic limits. Results from Balkema and Nolde (2010) and Nolde (2014) highlight the importance of studying the limiting shape of an appropriately-scaled sample cloud when characterising the whole joint tail. We now develop the first statistical inference for this limit set, which has considerable practical importance for a unified inference framework across different aspects of the joint tail. Moreover, Nolde and Wadsworth (2022) link this limit set to various existing extremal dependence frameworks. Hence, a by-product of our new limit set inference is the first set of self-consistent estimators for several extremal dependence measures, avoiding the current possibility of contradictory conclusions. In simulations, our limit set estimator is successful across a range of distributions, and the corresponding extremal dependence estimators provide a major joint improvement and small marginal improvements over existing techniques. We consider an application to sea wave heights, where our estimates successfully capture the expected weakening extremal dependence as the distance between locations increases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.02626v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emma S. Simpson, Jonathan A. Tawn</dc:creator>
    </item>
    <item>
      <title>Vaccine efficacy for binary post-infection outcomes under misclassification without monotonicity</title>
      <link>https://arxiv.org/abs/2211.16502</link>
      <description>arXiv:2211.16502v5 Announce Type: replace 
Abstract: In order to meet regulatory approval, pharmaceutical companies often must demonstrate that new vaccines reduce the total risk of a post-infection outcome like transmission, symptomatic disease, severe illness, or death in randomized, placebo-controlled trials. Given that infection is a necessary precondition for a post-infection outcome, one can use principal stratification to partition the total causal effect of vaccination into two causal effects: vaccine efficacy against infection, and the principal effect of vaccine efficacy against a post-infection outcome in the patients that would be infected under both placebo and vaccination. Despite the importance of such principal effects to policymakers, these estimands are generally unidentifiable, even under strong assumptions that are rarely satisfied in real-world trials. We develop a novel method to nonparametrically point identify these principal effects while eliminating the monotonicity assumption and allowing for measurement error. Furthermore, our results allow for multiple treatments, and are general enough to be applicable outside of vaccine efficacy. Our method relies on the fact that many vaccine trials are run at geographically disparate health centers, and measure biologically-relevant categorical pretreatment covariates. We show that our method can be applied to a variety of clinical trial settings where vaccine efficacy against infection and a post-infection outcome can be jointly inferred. This can yield new insights from existing vaccine efficacy trial data and will aid researchers in designing new multi-arm clinical trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.16502v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rob Trangucci, Yang Chen, Jon Zelner</dc:creator>
    </item>
    <item>
      <title>Causal Inference for Continuous Multiple Time Point Interventions</title>
      <link>https://arxiv.org/abs/2305.06645</link>
      <description>arXiv:2305.06645v5 Announce Type: replace 
Abstract: There are limited options to estimate the treatment effects of variables which are continuous and measured at multiple time points, particularly if the true dose-response curve should be estimated as closely as possible. However, these situations may be of relevance: in pharmacology, one may be interested in how outcomes of people living with -- and treated for -- HIV, such as viral failure, would vary for time-varying interventions such as different drug concentration trajectories. A challenge for doing causal inference with continuous interventions is that the positivity assumption is typically violated. To address positivity violations, we develop projection functions, which reweigh and redefine the estimand of interest based on functions of the conditional support for the respective interventions. With these functions, we obtain the desired dose-response curve in areas of enough support, and otherwise a meaningful estimand that does not require the positivity assumption. We develop $g$-computation type plug-in estimators for this case. Those are contrasted with g-computation estimators which are applied to continuous interventions without specifically addressing positivity violations, which we propose to be presented with diagnostics. The ideas are illustrated with longitudinal data from HIV positive children treated with an efavirenz-based regimen as part of the CHAPAS-3 trial, which enrolled children $&lt;13$ years in Zambia/Uganda. Simulations show in which situations a standard g-computation approach is appropriate, and in which it leads to bias and how the proposed weighted estimation approach then recovers the alternative estimand of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.06645v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Michael Schomaker, Helen McIlleron, Paolo Denti, Iv\'an D\'iaz</dc:creator>
    </item>
    <item>
      <title>Fusion regression methods with repeated functional data</title>
      <link>https://arxiv.org/abs/2308.01747</link>
      <description>arXiv:2308.01747v4 Announce Type: replace 
Abstract: Linear regression and classification methods with repeated functional data are considered. For each statistical unit in the sample, a real-valued parameter is observed over time under different conditions related by some neighborhood structure (spatial, group, etc.). Two regression methods based on fusion penalties are proposed to consider the dependence induced by this structure. These methods aim to obtain parsimonious coefficient regression functions, by determining if close conditions are associated with common regression coefficient functions. The first method is a generalization to functional data of the variable fusion methodology based on the 1-nearest neighbor. The second one relies on the group fusion lasso penalty which assumes some grouping structure of conditions and allows for homogeneity among the regression coefficient functions within groups. Numerical simulations and an application of electroencephalography data are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.01747v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Issam-Ali Moindji\'e, Cristian Preda, Sophie Dabo-Niang</dc:creator>
    </item>
    <item>
      <title>Are Information criteria good enough to choose the right the number of regimes in Hidden Markov Models?</title>
      <link>https://arxiv.org/abs/2308.04374</link>
      <description>arXiv:2308.04374v2 Announce Type: replace 
Abstract: Selecting the number of regimes in Hidden Markov models is an important problem. There are many criteria that are used to select this number, such as Akaike information criterion (AIC), Bayesian information criterion (BIC), integrated completed likelihood (ICL), deviance information criterion (DIC), and Watanabe-Akaike information criterion (WAIC), to name a few. In this article, we introduced goodness-of-fit tests for general Hidden Markov models with covariates, where the distribution of the observations is arbitrary, i.e., continuous, discrete, or a mixture of both. Then, a selection procedure is proposed based on this goodness-of-fit test. The main aim of this article is to compare the classical information criterion with the new criterion, when the outcome is either continuous, discrete or zero-inflated. Numerical experiments assess the finite sample performance of the goodness-of-fit tests, and comparisons between the different criteria are made.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.04374v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bouchra R Nasri, Bruno N R\'emillard, Mamadou Y Thioub</dc:creator>
    </item>
    <item>
      <title>Randomization Inference When N Equals One</title>
      <link>https://arxiv.org/abs/2310.16989</link>
      <description>arXiv:2310.16989v2 Announce Type: replace 
Abstract: N-of-1 experiments, where a unit serves as its own control and treatment in different time windows, have been used in certain medical contexts for decades. However, due to effects that accumulate over long time windows and interventions that have complex evolution, a lack of robust inference tools has limited the widespread applicability of such N-of-1 designs. This work combines techniques from experiment design in causal inference and system identification from control theory to provide such an inference framework. We derive a model of the dynamic interference effect that arises in linear time-invariant dynamical systems. We show that a family of causal estimands analogous to those studied in potential outcomes are estimable via a standard estimator derived from the method of moments. We derive formulae for higher moments of this estimator and describe conditions under which N-of-1 designs may provide faster ways to estimate the effects of interventions in dynamical systems. We also provide conditions under which our estimator is asymptotically normal and derive valid confidence intervals for this setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.16989v2</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tengyuan Liang, Benjamin Recht</dc:creator>
    </item>
    <item>
      <title>Prediction of causal genes at GWAS loci with pleiotropic gene regulatory effects using sets of correlated instrumental variables</title>
      <link>https://arxiv.org/abs/2401.06261</link>
      <description>arXiv:2401.06261v3 Announce Type: replace 
Abstract: Multivariate Mendelian randomization (MVMR) is a statistical technique that uses sets of genetic instruments to estimate the direct causal effects of multiple exposures on an outcome of interest. At genomic loci with pleiotropic gene regulatory effects, that is, loci where the same genetic variants are associated to multiple nearby genes, MVMR can potentially be used to predict candidate causal genes. However, consensus in the field dictates that the genetic instruments in MVMR must be independent, which is usually not possible when considering a group of candidate genes from the same locus. We used causal inference theory to show that MVMR with correlated instruments satisfies the instrumental set condition. This is a classical result by Brito and Pearl (2002) for structural equation models that guarantees the identifiability of causal effects in situations where multiple exposures collectively, but not individually, separate a set of instrumental variables from an outcome variable. Extensive simulations confirmed the validity and usefulness of these theoretical results even at modest sample sizes. Importantly, the causal effect estimates remain unbiased and their variance small when instruments are highly correlated. We applied MVMR with correlated instrumental variable sets at risk loci from genome-wide association studies (GWAS) for coronary artery disease using eQTL data from the STARNET study. Our method predicts causal genes at twelve loci, each associated with multiple colocated genes in multiple tissues. However, the extensive degree of regulatory pleiotropy across tissues and the limited number of causal variants in each locus still require that MVMR is run on a tissue-by-tissue basis, and testing all gene-tissue pairs at a given locus in a single model to predict causal gene-tissue combinations remains infeasible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.06261v3</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mariyam Khan, Adriaan-Alexander Ludl, Sean Bankier, Johan Bjorkegren, Tom Michoel</dc:creator>
    </item>
    <item>
      <title>Robust Score-Based Quickest Change Detection</title>
      <link>https://arxiv.org/abs/2407.11094</link>
      <description>arXiv:2407.11094v2 Announce Type: replace 
Abstract: Methods in the field of quickest change detection rapidly detect in real-time a change in the data-generating distribution of an online data stream. Existing methods have been able to detect this change point when the densities of the pre- and post-change distributions are known. Recent work has extended these results to the case where the pre- and post-change distributions are known only by their score functions. This work considers the case where the pre- and post-change score functions are known only to correspond to distributions in two disjoint sets. This work employs a pair of "least-favorable" distributions to robustify the existing score-based quickest change detection algorithm, the properties of which are studied. This paper calculates the least-favorable distributions for specific model classes and provides methods of estimating the least-favorable distributions for common constructions. Simulation results are provided demonstrating the performance of our robust change detection algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11094v2</guid>
      <category>stat.ME</category>
      <category>eess.SP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sean Moushegian, Suya Wu, Enmao Diao, Jie Ding, Taposh Banerjee, Vahid Tarokh</dc:creator>
    </item>
    <item>
      <title>Testing Normality of Data Transformed by Maximum Likelihood Box Cox</title>
      <link>https://arxiv.org/abs/2407.19329</link>
      <description>arXiv:2407.19329v2 Announce Type: replace 
Abstract: Transforming a random variable to improve its normality leads to a followup test for whether the transformed variable follows a normal distribution. Previous work has shown that the Anderson Darling test for normality suffers from resubstitution bias following Box-Cox transformation, and indicates normality much too often. The work reported here extends this by adding the Shapiro-Wilk statistic and the two-parameter Box Cox transformation, all of which show severe bias. We also develop a recalibration to correct the bias in all four settings. The methodology was motivated by finding reference ranges in biomarker studies where parametric analysis, possibly on a power-transformed measurand, can be much more informative than nonparametric. Setting environmental standards illustrates another potential application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19329v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Douglas M Hawkins</dc:creator>
    </item>
    <item>
      <title>Nonparametric estimation of trawl processes: Theory and Applications</title>
      <link>https://arxiv.org/abs/2209.05894</link>
      <description>arXiv:2209.05894v2 Announce Type: replace-cross 
Abstract: Trawl processes belong to the class of continuous-time, strictly stationary, infinitely divisible processes; they are defined as L\'{e}vy bases evaluated over deterministic trawl sets. This article presents the first nonparametric estimator of the trawl function characterising the trawl set and the serial correlation of the process. Moreover, it establishes a detailed asymptotic theory for the proposed estimator, including a law of large numbers and a central limit theorem for various asymptotic relations between an in-fill and a long-span asymptotic regime. In addition, it develops consistent estimators for both the asymptotic bias and variance, which are subsequently used for establishing feasible central limit theorems which can be applied to data. A simulation study shows the good finite sample performance of the proposed estimators. The new methodology is applied to forecasting high-frequency financial spread data from a limit order book and to estimating the busy-time distribution of a stochastic queue.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.05894v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Orimar Sauri, Almut E. D. Veraart</dc:creator>
    </item>
    <item>
      <title>Boundary Peeling: Outlier Detection Method Using One-Class Peeling</title>
      <link>https://arxiv.org/abs/2309.05630</link>
      <description>arXiv:2309.05630v2 Announce Type: replace-cross 
Abstract: Unsupervised outlier detection constitutes a crucial phase within data analysis and remains a dynamic realm of research. A good outlier detection algorithm should be computationally efficient, robust to tuning parameter selection, and perform consistently well across diverse underlying data distributions. We introduce One-Class Boundary Peeling, an unsupervised outlier detection algorithm. One-class Boundary Peeling uses the average signed distance from iteratively-peeled, flexible boundaries generated by one-class support vector machines. One-class Boundary Peeling has robust hyperparameter settings and, for increased flexibility, can be cast as an ensemble method. In synthetic data simulations One-Class Boundary Peeling outperforms all state of the art methods when no outliers are present while maintaining comparable or superior performance in the presence of outliers, as compared to benchmark methods. One-Class Boundary Peeling performs competitively in terms of correct classification, AUC, and processing time using common benchmark data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.05630v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sheikh Arafat, Na Sun, Maria L. Weese, Waldyn G. Martinez</dc:creator>
    </item>
  </channel>
</rss>
