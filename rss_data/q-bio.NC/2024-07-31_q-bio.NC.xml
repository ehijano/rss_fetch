<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 01 Aug 2024 04:00:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 01 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Mean-field approximation for networks with synchrony-driven adaptive coupling</title>
      <link>https://arxiv.org/abs/2407.21393</link>
      <description>arXiv:2407.21393v1 Announce Type: new 
Abstract: Synaptic plasticity is a key component of neuronal dynamics, describing the process by which the connections between neurons change in response to experiences. In this study, we extend a network model of $\theta$-neuron oscillators to include a realistic form of adaptive plasticity. In place of the less tractable spike-timing-dependent plasticity, we employ recently validated phase-difference-dependent plasticity rules, which adjust coupling strengths based on the relative phases of $\theta$-neuron oscillators. We investigate two approaches for implementing this plasticity: pairwise coupling strength updates and global coupling strength updates. A mean-field approximation of the system is derived and we investigate its validity through comparison with the $\theta$-neuron simulations across various stability states. The synchrony of the system is examined using the Kuramoto order parameter. A bifurcation analysis, by means of numerical continuation and the calculation of maximal Lyapunov exponents, reveals interesting phenomena, including bistability and evidence of period-doubling and boundary crisis routes to chaos, that would otherwise not exist in the absence of adaptive coupling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21393v1</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Niamh Fennelly, Alannah Neff, Renaud Lambiotte, Andrew Keane, \'Aine Byrne</dc:creator>
    </item>
    <item>
      <title>Diffusion-Based Generation of Neural Activity from Disentangled Latent Codes</title>
      <link>https://arxiv.org/abs/2407.21195</link>
      <description>arXiv:2407.21195v1 Announce Type: cross 
Abstract: Recent advances in recording technology have allowed neuroscientists to monitor activity from thousands of neurons simultaneously. Latent variable models are increasingly valuable for distilling these recordings into compact and interpretable representations. Here we propose a new approach to neural data analysis that leverages advances in conditional generative modeling to enable the unsupervised inference of disentangled behavioral variables from recorded neural activity. Our approach builds on InfoDiffusion, which augments diffusion models with a set of latent variables that capture important factors of variation in the data. We apply our model, called Generating Neural Observations Conditioned on Codes with High Information (GNOCCHI), to time series neural data and test its application to synthetic and biological recordings of neural activity during reaching. In comparison to a VAE-based sequential autoencoder, GNOCCHI learns higher-quality latent spaces that are more clearly structured and more disentangled with respect to key behavioral variables. These properties enable accurate generation of novel samples (unseen behavioral conditions) through simple linear traversal of the latent spaces produced by GNOCCHI. Our work demonstrates the potential of unsupervised, information-based models for the discovery of interpretable latent spaces from neural data, enabling researchers to generate high-quality samples from unseen conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21195v1</guid>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jonathan D. McCart, Andrew R. Sedler, Christopher Versteeg, Domenick Mifsud, Mattia Rigotti-Thompson, Chethan Pandarinath</dc:creator>
    </item>
    <item>
      <title>Robustly encoding certainty in a metastable neural circuit model</title>
      <link>https://arxiv.org/abs/2405.13182</link>
      <description>arXiv:2405.13182v2 Announce Type: replace 
Abstract: Localized persistent neural activity can encode delayed estimates of continuous variables. Common experiments require that subjects store and report the feature value (e.g., orientation) of a particular cue (e.g., oriented bar on a screen) after a delay. Visualizing recorded activity of neurons along their feature tuning reveals activity bumps whose centers wander stochastically, degrading the estimate over time. Bump position therefore represents the remembered estimate. Recent work suggests bump amplitude may represent estimate certainty reflecting a probabilistic population code for a Bayesian posterior. Idealized models of this type are fragile due to the fine tuning common to constructed continuum attractors in dynamical systems. Here we propose an alternative metastable model for robustly supporting multiple bump amplitudes by extending neural circuit models to include quantized nonlinearities. Asymptotic projections of circuit activity produce low-dimensional evolution equations for the amplitude and position of bump solutions in response to external stimuli and noise perturbations. Analysis of reduced equations accurately characterizes phase variance and the dynamics of amplitude transitions between stable discrete values. More salient cues generate bumps of higher amplitude which wander less, consistent with the experimental finding that greater certainty correlates with more accurate memories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13182v2</guid>
      <category>q-bio.NC</category>
      <category>nlin.PS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Heather L Cihak, Zachary P Kilpatrick</dc:creator>
    </item>
    <item>
      <title>AI-Driven Physics-Informed Bio-Silicon Intelligence System: Integrating Hybrid Systems, Biocomputing, Neural Networks, and Machine Learning, for Advanced Neurotechnology</title>
      <link>https://arxiv.org/abs/2407.11939</link>
      <description>arXiv:2407.11939v3 Announce Type: replace 
Abstract: We present the Bio-Silicon Intelligence System (BSIS), an innovative hybrid platform that integrates biological neural networks with silicon-based computing. The BSIS, a Physics-Informed Hybrid Hierarchical Reinforcement Learning State Machine, employs carbon nanotube-coated electrodes to interface rat brains with computational systems, enabling high-fidelity neural interfacing and bidirectional communication through self-organizing systems in both biological and silicon forms. Our system leverages both analogue and digital AI theory, incorporating concepts from computational theory, chaos theory, dynamical systems theory, physics, and quantum mechanics. Additionally, the BSIS replicates the neuronal dynamics typical of intelligent brain tissue, employing nonlinear operations underlying learning and information storage. Neural signals are read through the FreeEEG32 board and BrainFlow software, then features are extracted and mapped to game actions by tracking feature changes in continuous data. Metadata is encoded into both analogue and digital brain stimulation signals at the microvolt level using our proprietary software and hardware. The system employs a dual signaling approach for training the rat brain, incorporating a reward solution and sound as well as human-inaudible distress sounds. This paper details the design, theory, functionality, and technical specifications of the BSIS, highlighting its interdisciplinary approach and advanced technological integration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11939v3</guid>
      <category>q-bio.NC</category>
      <category>nlin.AO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Vincent Jorgsson, Raghav Kumar, Mustaf Ahmed, Maxx Yung, Aryaman Pattnayak, Sri Pradhyumna Sridhar, Vaishnav Varma, Arun Ram Ponnambalam, Georg Weidlich, Dimitris Pinotsis</dc:creator>
    </item>
    <item>
      <title>Synchronization in a small-world network of non-identical Chialvo neurons</title>
      <link>https://arxiv.org/abs/2407.18922</link>
      <description>arXiv:2407.18922v2 Announce Type: replace 
Abstract: Synchronization dynamics is a phenomenon of great interest in many fields of science. One of the most important fields is neuron dynamics, as synchronization in certain regions of the brain is related to some of the most common mental illnesses. In this work, we study synchronization in a small-world network of non-identical Chialvo neurons that are electrically coupled. We introduce a mismatch in one of the model parameters to construct non-identical neurons. Our study examines the effects of this parameter mismatch, the noise intensity in the stochastic model, and the coupling strength between neurons on synchronization and firing frequency. We have identified critical values of noise intensity, parameter mismatch, and rewiring probability that facilitate effective synchronization within the network. Furthermore, we observe that the balance between excitatory and inhibitory connections plays a crucial role in achieving global synchronization. Our findings offer insights into the mechanisms driving synchronization dynamics in complex neuron networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18922v2</guid>
      <category>q-bio.NC</category>
      <category>nlin.CD</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J. Used, J. M. Seoane, I. Bashkirtseva, L. Ryashko, M. A. F. Sanju\'an</dc:creator>
    </item>
    <item>
      <title>Position: An Inner Interpretability Framework for AI Inspired by Lessons from Cognitive Neuroscience</title>
      <link>https://arxiv.org/abs/2406.01352</link>
      <description>arXiv:2406.01352v2 Announce Type: replace-cross 
Abstract: Inner Interpretability is a promising emerging field tasked with uncovering the inner mechanisms of AI systems, though how to develop these mechanistic theories is still much debated. Moreover, recent critiques raise issues that question its usefulness to advance the broader goals of AI. However, it has been overlooked that these issues resemble those that have been grappled with in another field: Cognitive Neuroscience. Here we draw the relevant connections and highlight lessons that can be transferred productively between fields. Based on these, we propose a general conceptual framework and give concrete methodological strategies for building mechanistic explanations in AI inner interpretability research. With this conceptual framework, Inner Interpretability can fend off critiques and position itself on a productive path to explain AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01352v2</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martina G. Vilas, Federico Adolfi, David Poeppel, Gemma Roig</dc:creator>
    </item>
    <item>
      <title>Investigating the Timescales of Language Processing with EEG and Language Models</title>
      <link>https://arxiv.org/abs/2406.19884</link>
      <description>arXiv:2406.19884v2 Announce Type: replace-cross 
Abstract: This study explores the temporal dynamics of language processing by examining the alignment between word representations from a pre-trained transformer-based language model, and EEG data. Using a Temporal Response Function (TRF) model, we investigate how neural activity corresponds to model representations across different layers, revealing insights into the interaction between artificial language models and brain responses during language comprehension. Our analysis reveals patterns in TRFs from distinct layers, highlighting varying contributions to lexical and compositional processing. Additionally, we used linear discriminant analysis (LDA) to isolate part-of-speech (POS) representations, offering insights into their influence on neural responses and the underlying mechanisms of syntactic processing. These findings underscore EEG's utility for probing language processing dynamics with high temporal resolution. By bridging artificial language models and neural activity, this study advances our understanding of their interaction at fine timescales.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19884v2</guid>
      <category>cs.CL</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Davide Turco, Conor Houghton</dc:creator>
    </item>
  </channel>
</rss>
