<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 19 Dec 2024 02:52:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Estimating HIV Cross-sectional Incidence Using Recency Tests from a Non-representative Sample</title>
      <link>https://arxiv.org/abs/2412.12316</link>
      <description>arXiv:2412.12316v1 Announce Type: new 
Abstract: Cross-sectional incidence estimation based on recency testing has become a widely used tool in HIV research. Recently, this method has gained prominence in HIV prevention trials to estimate the "placebo" incidence that participants might experience without preventive treatment. The application of this approach faces challenges due to non-representative sampling, as individuals aware of their HIV-positive status may be less likely to participate in screening for an HIV prevention trial. To address this, a recent phase 3 trial excluded individuals based on whether they have had a recent HIV test. To the best of our knowledge, the validity of this approach has yet to be studied. In our work, we investigate the performance of cross-sectional HIV incidence estimation when excluding individuals based on prior HIV tests in realistic trial settings. We develop a statistical framework that incorporates a testing-based criterion and possible non-representative sampling. We introduce a metric we call the effective mean duration of recent infection (MDRI) that mathematically quantifies bias in incidence estimation. We conduct an extensive simulation study to evaluate incidence estimator performance under various scenarios. Our findings reveal that when screening attendance is affected by knowledge of HIV status, incidence estimators become unreliable unless all individuals with recent HIV tests are excluded. Additionally, we identified a trade-off between bias and variability: excluding more individuals reduces bias from non-representative sampling but in many cases increases the variability of incidence estimates. These findings highlight the need for caution when applying testing-based criteria and emphasize the importance of refining incidence estimation methods to improve the design and evaluation of future HIV prevention trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12316v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianan Pan, Marlena Bannick, Fei Gao</dc:creator>
    </item>
    <item>
      <title>Target Aggregate Data Adjustment Method for Transportability Analysis Utilizing Summary-Level Data from the Target Population</title>
      <link>https://arxiv.org/abs/2412.12335</link>
      <description>arXiv:2412.12335v1 Announce Type: new 
Abstract: Transportability analysis is a causal inference framework used to evaluate the external validity of randomized clinical trials (RCTs) or observational studies. Most existing transportability analysis methods require individual patient-level data (IPD) for both the source and the target population, narrowing its applicability when only target aggregate-level data (AgD) is available. Besides, accounting for censoring is essential to reduce bias in longitudinal data, yet AgD-based transportability methods in the presence of censoring remain underexplored. Here, we propose a two-stage weighting framework named "Target Aggregate Data Adjustment" (TADA) to address the mentioned challenges simultaneously. TADA is designed as a two-stage weighting scheme to simultaneously adjust for both censoring bias and distributional imbalances of effect modifiers (EM), where the final weights are the product of the inverse probability of censoring weights and participation weights derived using the method of moments. We have conducted an extensive simulation study to evaluate TADA's performance. Our results indicate that TADA can effectively control the bias resulting from censoring within a non-extreme range suitable for most practical scenarios, and enhance the application and clinical interpretability of transportability analyses in settings with limited data availability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12335v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yichen Yan, Quang Vuong, Rebecca K Metcalfe, Tianyu Guan, Haolun Shi, Jay JH Park</dc:creator>
    </item>
    <item>
      <title>On the Role of Surrogates in Conformal Inference of Individual Causal Effects</title>
      <link>https://arxiv.org/abs/2412.12365</link>
      <description>arXiv:2412.12365v1 Announce Type: new 
Abstract: Learning the Individual Treatment Effect (ITE) is essential for personalized decision making, yet causal inference has traditionally focused on aggregated treatment effects. While integrating conformal prediction with causal inference can provide valid uncertainty quantification for ITEs, the resulting prediction intervals are often excessively wide, limiting their practical utility. To address this limitation, we introduce \underline{S}urrogate-assisted \underline{C}onformal \underline{I}nference for \underline{E}fficient I\underline{N}dividual \underline{C}ausal \underline{E}ffects (SCIENCE), a framework designed to construct more efficient prediction intervals for ITEs. SCIENCE applies to various data configurations, including semi-supervised and surrogate-assisted semi-supervised learning. It accommodates covariate shifts between source data, which contain primary outcomes, and target data, which may include only surrogate outcomes or covariates. Leveraging semi-parametric efficiency theory, SCIENCE produces rate double-robust prediction intervals under mild rate convergence conditions, permitting the use of flexible non-parametric models to estimate nuisance functions. We quantify efficiency gains by comparing semi-parametric efficiency bounds with and without the incorporation of surrogates. Simulation studies demonstrate that our surrogate-assisted intervals offer substantial efficiency improvements over existing methods while maintaining valid group-conditional coverage. Applied to the phase 3 Moderna COVE COVID-19 vaccine trial, SCIENCE illustrates how multiple surrogate markers can be leveraged to generate more efficient prediction intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12365v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Chenyin Gao, Peter B. Gilbert, Larry Han</dc:creator>
    </item>
    <item>
      <title>The Estimand Framework and Causal Inference: Complementary not Competing Paradigms</title>
      <link>https://arxiv.org/abs/2412.12380</link>
      <description>arXiv:2412.12380v1 Announce Type: new 
Abstract: The creation of the ICH E9 (R1) estimands framework has led to more precise specification of the treatment effects of interest in the design and statistical analysis of clinical trials. However, it is unclear how the new framework relates to causal inference, as both approaches appear to define what is being estimated and have a quantity labelled an estimand. Using illustrative examples, we show that both approaches can be used to define a population-based summary of an effect on an outcome for a specified population and highlight the similarities and differences between these approaches. We demonstrate that the ICH E9 (R1) estimand framework offers a descriptive, structured approach that is more accessible to non-mathematicians, facilitating clearer communication of trial objectives and results. We then contrast this with the causal inference framework, which provides a mathematically precise definition of an estimand, and allows the explicit articulation of assumptions through tools such as causal graphs. Despite these differences, the two paradigms should be viewed as complementary rather than competing. The combined use of both approaches enhances the ability to communicate what is being estimated. We encourage those familiar with one framework to appreciate the concepts of the other to strengthen the robustness and clarity of clinical trial design, analysis, and interpretation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12380v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Drury, Jonathan W. Bartlett, David Wright, Oliver N. Keene</dc:creator>
    </item>
    <item>
      <title>Generalized entropy calibration for analyzing voluntary survey data</title>
      <link>https://arxiv.org/abs/2412.12405</link>
      <description>arXiv:2412.12405v1 Announce Type: new 
Abstract: Statistical analysis of voluntary survey data is an important area of research in survey sampling. We consider a unified approach to voluntary survey data analysis under the assumption that the sampling mechanism is ignorable. Generalized entropy calibration is introduced as a unified tool for calibration weighting to control the selection bias. We first establish the relationship between the generalized calibration weighting and its dual expression for regression estimation. The dual relationship is critical in identifying the implied regression model and developing model selection for calibration weighting. Also, if a linear regression model for an important study variable is available, then two-step calibration method can be used to smooth the final weights and achieve the statistical efficiency. Asymptotic properties of the proposed estimator are investigated. Results from a limited simulation study are also presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12405v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yonghyun Kwon, Jae Kwang Kim, Yumou Qiu</dc:creator>
    </item>
    <item>
      <title>Inside-out cross-covariance for spatial multivariate data</title>
      <link>https://arxiv.org/abs/2412.12407</link>
      <description>arXiv:2412.12407v1 Announce Type: new 
Abstract: As the spatial features of multivariate data are increasingly central in researchers' applied problems, there is a growing demand for novel spatially-aware methods that are flexible, easily interpretable, and scalable to large data. We develop inside-out cross-covariance (IOX) models for multivariate spatial likelihood-based inference. IOX leads to valid cross-covariance matrix functions which we interpret as inducing spatial dependence on independent replicates of a correlated random vector. The resulting sample cross-covariance matrices are "inside-out" relative to the ubiquitous linear model of coregionalization (LMC). However, unlike LMCs, our methods offer direct marginal inference, easy prior elicitation of covariance parameters, the ability to model outcomes with unequal smoothness, and flexible dimension reduction. As a covariance model for a q-variate Gaussian process, IOX leads to scalable models for noisy vector data as well as flexible latent models. For large n cases, IOX complements Vecchia approximations and related process-based methods based on sparse graphical models. We demonstrate superior performance of IOX on synthetic datasets as well as on colorectal cancer proteomics data. An R package implementing the proposed methods is available at github.com/mkln/spiox.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12407v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Michele Peruzzi</dc:creator>
    </item>
    <item>
      <title>Bayesian nonparametric partial clustering: Quantifying the effectiveness of agricultural subsidies across Europe</title>
      <link>https://arxiv.org/abs/2412.12868</link>
      <description>arXiv:2412.12868v1 Announce Type: new 
Abstract: The global climate has underscored the need for effective policies to reduce greenhouse gas emissions from all sources, including those resulting from agricultural expansion, which is regulated by the Common Agricultural Policy (CAP) across the European Union (EU). To assess the effectiveness of these mitigation policies, statistical methods must account for the heterogeneous impact of policies across different countries. We propose a Bayesian approach that combines the multinomial logit model, which is suitable for compositional land-use data, with a Bayesian nonparametric (BNP) prior to cluster regions with similar policy impacts. To simultaneously control for other relevant factors, we distinguish between cluster-specific and global covariates, coining this approach the Bayesian nonparametric partial clustering model. We develop a novel and efficient Markov Chain Monte Carlo (MCMC) algorithm, leveraging recent advances in the Bayesian literature. Using economic, geographic, and subsidy-related data from 22 EU member states, we examine the effectiveness of policies influencing land-use decisions across Europe and highlight the diversity of the problem. Our results indicate that the impact of CAP varies widely across the EU, emphasizing the need for subsidies to be tailored to optimize their effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12868v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Mozdzen, Felicity Addo, Tamas Krisztin, Gregor Kastner</dc:creator>
    </item>
    <item>
      <title>Meta-analysis models relaxing the random effects normality assumption: methodological systematic review and simulation study</title>
      <link>https://arxiv.org/abs/2412.12945</link>
      <description>arXiv:2412.12945v1 Announce Type: new 
Abstract: Random effects meta-analysis is widely used for synthesizing studies under the assumption that underlying effects come from a normal distribution. However, under certain conditions the use of alternative distributions might be more appropriate. We conducted a systematic review to identify articles introducing alternative meta-analysis models assuming non-normal between-study distributions. We identified 27 eligible articles suggesting 24 alternative meta-analysis models based on long-tail and skewed distributions, on mixtures of distributions, and on Dirichlet process priors. Subsequently, we performed a simulation study to evaluate the performance of these models and to compare them with the standard normal model. We considered 22 scenarios varying the amount of between-study variance, the shape of the true distribution, and the number of included studies. We compared 15 models implemented in the Frequentist or in the Bayesian framework. We found small differences with respect to bias between the different models but larger differences in the level of coverage probability. In scenarios with large between-study variance, all models were substantially biased in the estimation of the mean treatment effect. This implies that focusing only on the mean treatment effect of random effects meta-analysis can be misleading when substantial heterogeneity is suspected or outliers are present.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12945v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kanella Panagiotopoulou, Theodoros Evrenoglou, Christopher H Schmid, Silvia Metelli, Anna Chaimani</dc:creator>
    </item>
    <item>
      <title>Neural Posterior Estimation for Stochastic Epidemic Modeling</title>
      <link>https://arxiv.org/abs/2412.12967</link>
      <description>arXiv:2412.12967v1 Announce Type: new 
Abstract: Stochastic infectious disease models capture uncertainty in public health outcomes and have become increasingly popular in epidemiological practice. However, calibrating these models to observed data is challenging with existing methods for parameter estimation. Stochastic epidemic models are nonlinear dynamical systems with potentially large latent state spaces, resulting in computationally intractable likelihood densities. We develop an approach to calibrating complex epidemiological models to high-dimensional data using Neural Posterior Estimation, a novel technique for simulation-based inference. In NPE, a neural conditional density estimator trained on simulated data learns to "invert" a stochastic simulator, returning a parametric approximation to the posterior distribution. We introduce a stochastic, discrete-time Susceptible Infected (SI) model with heterogeneous transmission for healthcare-associated infections (HAIs). HAIs are a major burden on healthcare systems. They exhibit high rates of asymptotic carriage, making it difficult to estimate infection rates. Through extensive simulation experiments, we show that NPE produces accurate posterior estimates of infection rates with greater sample efficiency compared to Approximate Bayesian Computation (ABC). We then use NPE to fit our SI model to an outbreak of carbapenem-resistant Klebsiella pneumoniae in a long-term acute care facility, finding evidence of location-based heterogeneity in patient-to-patient transmission risk. We argue that our methodology can be fruitfully applied to a wide range of mechanistic transmission models and problems in the epidemiology of infectious disease.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12967v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Prayag Chatha, Fan Bu, Jeffrey Regier, Evan Snitkin, Jon Zelner</dc:creator>
    </item>
    <item>
      <title>Fr\'echet Sufficient Dimension Reduction for Metric Space-Valued Data via Distance Covariance</title>
      <link>https://arxiv.org/abs/2412.13122</link>
      <description>arXiv:2412.13122v1 Announce Type: new 
Abstract: We propose a novel Fr\'echet sufficient dimension reduction (SDR) method based on kernel distance covariance, tailored for metric space-valued responses such as count data, probability densities, and other complex structures. The method leverages a kernel-based transformation to map metric space-valued responses into a feature space, enabling efficient dimension reduction. By incorporating kernel distance covariance, the proposed approach offers enhanced flexibility and adaptability for datasets with diverse and non-Euclidean characteristics. The effectiveness of the method is demonstrated through synthetic simulations and several real-world applications. In all cases, the proposed method runs faster and consistently outperforms the existing Fr\'echet SDR approaches, demonstrating its broad applicability and robustness in addressing complex data challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13122v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hsin-Hsiung Huang, Feng Yu, Kang Li, Teng Zhang</dc:creator>
    </item>
    <item>
      <title>The Logic of Counterfactuals and the Epistemology of Causal Inference</title>
      <link>https://arxiv.org/abs/2405.11284</link>
      <description>arXiv:2405.11284v2 Announce Type: cross 
Abstract: The 2021 Nobel Prize in Economics recognizes a type of causal model known as the Rubin causal model, or potential outcome framework, which deserves far more attention from philosophers than it currently receives. To spark philosophers' interest, I develop a dialectic connecting the Rubin causal model to the Lewis-Stalnaker debate on a logical principle of counterfactuals: Conditional Excluded Middle (CEM). I begin by playing good cop for CEM, developing a new argument in its favor -- a Quine-Putnam-style indispensability argument. This argument is based on the observation that CEM seems to be indispensable to the Rubin causal model, which underpins our best scientific theory of causal inference in health and social sciences -- a Nobel Prize-winning theory. Indeed, CEM has long remained a core assumption of the Rubin causal model, despite challenges from within the statistics and economics communities over twenty years ago. I then switch sides to play bad cop for CEM, undermining the indispensability argument by developing a new theory of causal inference that dispenses with CEM while preserving the successes of the original theory (thanks to a new theorem proved here). The key, somewhat surprisingly, is to integrate two approaches to causal modeling: the Rubin causal model, more familiar in health and social sciences, and the causal Bayes net, more familiar in philosophy. The good cop/bad cop dialectic is concluded with a connection to broader philosophical issues, including intertheory relations, the revisability of logic, and the role of background assumptions in justifying scientific inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11284v2</guid>
      <category>cs.AI</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hanti Lin</dc:creator>
    </item>
    <item>
      <title>Unlocking new capabilities in the analysis of GC$\times$GC-TOFMS data with shift-invariant multi-linearity</title>
      <link>https://arxiv.org/abs/2412.12114</link>
      <description>arXiv:2412.12114v1 Announce Type: cross 
Abstract: This paper introduces a novel deconvolution algorithm, shift-invariant multi-linearity (SIML), which significantly enhances the analysis of data from a comprehensive two-dimensional gas chromatograph coupled to a mass spectrometric detector (GC$\times$GC-TOFMS). Designed to address the challenges posed by retention time shifts and high noise levels, SIML incorporates wavelet-based smoothing and Fourier-Transform based shift-correction within the multivariate curve resolution-alternating least squares (MCR-ALS) framework. We benchmarked the SIML algorithm against traditional methods such as MCR-ALS and Parallel Factor Analysis 2 with flexible coupling (PARAFAC2$\times$N) using both simulated and real GC$\times$GC-TOFMS datasets. Our results demonstrate that SIML provides unique solutions with significantly improved robustness, particularly in low signal-to-noise ratio scenarios, where it maintains high accuracy in estimating mass spectra and concentrations. The enhanced reliability of quantitative analyses afforded by SIML underscores its potential for broad application in complex matrix analyses across environmental science, food chemistry, and biological research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12114v1</guid>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul-Albert Schneide, Michael Sorochan Armstrong, Neal Gallagher, Rasmus Bro</dc:creator>
    </item>
    <item>
      <title>Pop-out vs. Glue: A Study on the pre-attentive and focused attention stages in Visual Search tasks</title>
      <link>https://arxiv.org/abs/2412.12198</link>
      <description>arXiv:2412.12198v1 Announce Type: cross 
Abstract: This study explores visual search asymmetry and the detection process between parallel and serial search strategies, building upon Treisman's Feature Integration Theory [3]. Our experiment examines how easy it is to locate an oblique line among vertical distractors versus a vertical line among oblique distractors, a framework previously validated by Treisman &amp; Gormican (1988) [4] and Gupta et al. (2015) [1]. We hypothesised that an oblique target among vertical lines would produce a perceptual 'pop-out' effect, allowing for faster, parallel search, while the reverse condition would require serial search strategy. Seventy-eight participants from Utrecht University engaged in trials with varied target-distractor orientations and number of items. We measured reaction times and found a significant effect of target type on search speed: oblique targets were identified more quickly, reflecting 'pop-out' behaviour, while vertical targets demanded focused attention ('glue phase'). Our results align with past findings, supporting our hypothesis on search asymmetry and its dependency on distinct visual features. Future research could benefit from eye-tracking and neural network analysis, particularly for identifying the neural processing of visual features in both parallel and serial search conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12198v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hendrik Beukelman, Wilder C. Rodrigues</dc:creator>
    </item>
    <item>
      <title>Ask for More Than Bayes Optimal: A Theory of Indecisions for Classification</title>
      <link>https://arxiv.org/abs/2412.12807</link>
      <description>arXiv:2412.12807v1 Announce Type: cross 
Abstract: Selective classification frameworks are useful tools for automated decision making in highly risky scenarios, since they allow for a classifier to only make highly confident decisions, while abstaining from making a decision when it is not confident enough to do so, which is otherwise known as an indecision. For a given level of classification accuracy, we aim to make as many decisions as possible. For many problems, this can be achieved without abstaining from making decisions. But when the problem is hard enough, we show that we can still control the misclassification rate of a classifier up to any user specified level, while only abstaining from the minimum necessary amount of decisions, even if this level of misclassification is smaller than the Bayes optimal error rate. In many problem settings, the user could obtain a dramatic decrease in misclassification while only paying a comparatively small price in terms of indecisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12807v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohamed Ndaoud, Peter Radchenko, Bradley Rava</dc:creator>
    </item>
    <item>
      <title>Unified calibration and spatial mapping of fine particulate matter data from multiple low-cost air pollution sensor networks in Baltimore, Maryland</title>
      <link>https://arxiv.org/abs/2412.13034</link>
      <description>arXiv:2412.13034v1 Announce Type: cross 
Abstract: Low-cost air pollution sensor networks are increasingly being deployed globally, supplementing sparse regulatory monitoring with localized air quality data. In some areas, like Baltimore, Maryland, there are only few regulatory (reference) devices but multiple low-cost networks. While there are many available methods to calibrate data from each network individually, separate calibration of each network leads to conflicting air quality predictions. We develop a general Bayesian spatial filtering model combining data from multiple networks and reference devices, providing dynamic calibrations (informed by the latest reference data) and unified predictions (combining information from all available sensors) for the entire region. This method accounts for network-specific bias and noise (observation models), as different networks can use different types of sensors, and uses a Gaussian process (state-space model) to capture spatial correlations. We apply the method to calibrate PM$_{2.5}$ data from Baltimore in June and July 2023 -- a period including days of hazardous concentrations due to wildfire smoke. Our method helps mitigate the effects of preferential sampling of one network in Baltimore, results in better predictions and narrower confidence intervals. Our approach can be used to calibrate low-cost air pollution sensor data in Baltimore and any other areas with multiple low-cost networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13034v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Claire Heffernan, Kirsten Koehler, Drew R. Gentner, Roger D. Peng, Abhirup Datta</dc:creator>
    </item>
    <item>
      <title>First-order integer-valued autoregressive processes with Generalized Katz innovations</title>
      <link>https://arxiv.org/abs/2202.02029</link>
      <description>arXiv:2202.02029v2 Announce Type: replace 
Abstract: A new integer--valued autoregressive process (INAR) with Generalised Lagrangian Katz (GLK) innovations is defined. This process family provides a flexible modelling framework for count data, allowing for under and over--dispersion, asymmetry, and excess of kurtosis and includes standard INAR models such as Generalized Poisson and Negative Binomial as special cases. We show that the GLK--INAR process is discrete semi--self--decomposable, infinite divisible, stable by aggregation and provides stationarity conditions. Some extensions are discussed, such as the Markov--Switching and the zero--inflated GLK--INARs. A Bayesian inference framework and an efficient posterior approximation procedure are introduced. The proposed models are applied to 130 time series from Google Trend, which proxy the worldwide public concern about climate change. New evidence is found of heterogeneity across time, countries and keywords in the persistence, uncertainty, and long--run public awareness level.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.02029v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ovielt Baltodano Lopez, Federico Bassetti, Giulia Carallo, Roberto Casarin</dc:creator>
    </item>
    <item>
      <title>An Ising Similarity Regression Model for Modeling Multivariate Binary Data</title>
      <link>https://arxiv.org/abs/2401.13379</link>
      <description>arXiv:2401.13379v2 Announce Type: replace 
Abstract: Understanding the dependence structure between response variables is an important component in the analysis of correlated multivariate data. This article focuses on modeling dependence structures in multivariate binary data, motivated by a study aiming to understand how patterns in different U.S. senators' votes are determined by similarities (or lack thereof) in their attributes, e.g., political parties and social network profiles. To address such a research question, we propose a new Ising similarity regression model which regresses pairwise interaction coefficients in the Ising model against a set of similarity measures available/constructed from covariates. Model selection approaches are further developed through regularizing the pseudo-likelihood function with an adaptive lasso penalty to enable the selection of relevant similarity measures. We establish estimation and selection consistency of the proposed estimator under a general setting where the number of similarity measures and responses tend to infinity. Simulation study demonstrates the strong finite sample performance of the proposed estimator, particularly compared with several existing Ising model estimators in estimating the matrix of pairwise interaction coefficients. Applying the Ising similarity regression model to a dataset of roll call voting records of 100 U.S. senators, we are able to quantify how similarities in senators' parties, businessman occupations and social network profiles drive their voting associations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.13379v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhi Yang Tho, Francis K. C. Hui, Tao Zou</dc:creator>
    </item>
    <item>
      <title>Moving Aggregate Modified Autoregressive Copula-Based Time Series Models (MAGMAR-Copulas)</title>
      <link>https://arxiv.org/abs/2402.01491</link>
      <description>arXiv:2402.01491v2 Announce Type: replace 
Abstract: Copula-based time series models implicitly assume a finite Markov order. In reality a time series may not follow the Markov property. We modify the copula-based time series models by introducing a moving aggregate (MAG) part into the model updating equation. The functional form of the MAG-part is given as the inverse of a conditional copula. The resulting MAG-modified Autoregressive Copula-Based Time Series model (MAGMAR-Copula) is discussed in detail and distributional properties are derived in a D-vine framework. The model nests the classical ARMA model and can be interpreted as a non-linear generalization of the ARMA-model. The modeling performance is evaluated by modeling US inflation. Our model is competitive with benchmark models in terms of information criteria.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.01491v2</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sven Pappert</dc:creator>
    </item>
    <item>
      <title>"Clustering and Conquer" Procedures for Parallel Large-Scale Ranking and Selection</title>
      <link>https://arxiv.org/abs/2402.02196</link>
      <description>arXiv:2402.02196v3 Announce Type: replace 
Abstract: This work breaks the sample efficiency bottleneck in parallel large-scale ranking and selection (R&amp;S) problem by leveraging correlation information. We modify the commonly used "divide and conquer" framework in parallel computing by adding a correlation-based clustering step, transforming it into "clustering and conquer". This seemingly simple modification can achieve an $\mathcal{O}(p)$ sample complexity reduction rate, which represents the maximum attainable reduction for the class of sample-optimal R&amp;S methods. Our approach enjoys two key advantages: 1) it does not require highly accurate correlation estimation or precise clustering, and 2) it allows for seamless integration with various existing R&amp;S method, while achieving optimal sample complexity. Theoretically, we develop a novel gradient analysis framework to analyze sample efficiency and guide the design of large-scale R&amp;S procedures. Building upon this framework, we propose a gradient-based budget allocation policy. We also introduce a new clustering algorithm, selection policy, and precision criterion tailored for large-scale scenarios. Finally, in large-scale AI applications such as neural architecture search, our methods demonstrate superior performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02196v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zishi Zhang, Yijie Peng</dc:creator>
    </item>
    <item>
      <title>Extremal graphical modeling with latent variables via convex optimization</title>
      <link>https://arxiv.org/abs/2403.09604</link>
      <description>arXiv:2403.09604v3 Announce Type: replace 
Abstract: Extremal graphical models encode the conditional independence structure of multivariate extremes and provide a powerful tool for quantifying the risk of rare events. Prior work on learning these graphs from data has focused on the setting where all relevant variables are observed. For the popular class of H\"usler-Reiss models, we propose the \texttt{eglatent} method, a tractable convex program for learning extremal graphical models in the presence of latent variables. Our approach decomposes the H\"usler-Reiss precision matrix into a sparse component encoding the graphical structure among the observed variables after conditioning on the latent variables, and a low-rank component encoding the effect of a few latent variables on the observed variables. We provide finite-sample guarantees of \texttt{eglatent} and show that it consistently recovers the conditional graph as well as the number of latent variables. We highlight the improved performances of our approach on synthetic and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09604v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sebastian Engelke, Armeen Taeb</dc:creator>
    </item>
    <item>
      <title>Efficient Analysis of Latent Spaces in Heterogeneous Networks</title>
      <link>https://arxiv.org/abs/2412.02151</link>
      <description>arXiv:2412.02151v2 Announce Type: replace 
Abstract: This work proposes a unified framework for efficient estimation under latent space modeling of heterogeneous networks. We consider a class of latent space models that decompose latent vectors into shared and network-specific components across networks. We develop a novel procedure that first identifies the shared latent vectors and further refines estimates through efficient score equations to achieve statistical efficiency. Oracle error rates for estimating the shared and heterogeneous latent vectors are established simultaneously. The analysis framework offers remarkable flexibility, accommodating various types of edge weights under exponential family distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02151v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuang Tian, Jiajin Sun, Yinqiu He</dc:creator>
    </item>
    <item>
      <title>The BPgWSP test: a Bayesian Weibull Shape Parameter signal detection test for adverse drug reactions</title>
      <link>https://arxiv.org/abs/2412.05463</link>
      <description>arXiv:2412.05463v2 Announce Type: replace 
Abstract: We develop a Bayesian Power generalized Weibull shape parameter (PgWSP) test as statistical method for signal detection of possible drug-adverse event associations using electronic health records for pharmacovigilance. The Bayesian approach allows the incorporation of prior knowledge about the likely time of occurrence along time-to-event data. The test is based on the shape parameters of the Power generalized Weibull (PgW) distribution. When both shape parameters are equal to one, the PgW distribution reduces to an exponential distribution, i.e. a constant hazard function. This is interpreted as no temporal association between drug and adverse event. The Bayesian PgWSP test involves comparing a region of practical equivalence (ROPE) around one reflecting the null hypothesis with estimated credibility intervals reflecting the posterior means of the shape parameters. The decision to raise a signal is based on the outcomes of the ROPE test and the selected combination rule for these outcomes. The development of the test requires a simulation study for tuning of the ROPE and credibility intervals to optimize specifcity and sensitivity of the test. Samples are generated under various conditions, including differences in sample size, prevalence of adverse drug reactions (ADRs), and the proportion of adverse events. We explore prior assumptions reflecting the belief in the presence or absence of ADRs at different points in the observation period. Various types of ROPE, credibility intervals, and combination rules are assessed and optimal tuning parameters are identifed based on the area under the curve. The tuned Bayesian PgWSP test is illustrated in a case study in which the time-dependent correlation between the intake of bisphosphonates and four adverse events is investigated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05463v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julia Dyck, Odile Sauzet</dc:creator>
    </item>
    <item>
      <title>Adaptive Nonparametric Perturbations of Parametric Bayesian Models</title>
      <link>https://arxiv.org/abs/2412.10683</link>
      <description>arXiv:2412.10683v2 Announce Type: replace 
Abstract: Parametric Bayesian modeling offers a powerful and flexible toolbox for scientific data analysis. Yet the model, however detailed, may still be wrong, and this can make inferences untrustworthy. In this paper we study nonparametrically perturbed parametric (NPP) Bayesian models, in which a parametric Bayesian model is relaxed via a distortion of its likelihood. We analyze the properties of NPP models when the target of inference is the true data distribution or some functional of it, such as in causal inference. We show that NPP models can offer the robustness of nonparametric models while retaining the data efficiency of parametric models, achieving fast convergence when the parametric model is close to true. To efficiently analyze data with an NPP model, we develop a generalized Bayes procedure to approximate its posterior. We demonstrate our method by estimating causal effects of gene expression from single cell RNA sequencing data. NPP modeling offers an efficient approach to robust Bayesian inference and can be used to robustify any parametric Bayesian model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10683v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bohan Wu, Eli N. Weinstein, Sohrab Salehi, Yixin Wang, David M. Blei</dc:creator>
    </item>
    <item>
      <title>Causal Invariance Learning via Efficient Optimization of a Nonconvex Objective</title>
      <link>https://arxiv.org/abs/2412.11850</link>
      <description>arXiv:2412.11850v2 Announce Type: replace 
Abstract: Data from multiple environments offer valuable opportunities to uncover causal relationships among variables. Leveraging the assumption that the causal outcome model remains invariant across heterogeneous environments, state-of-the-art methods attempt to identify causal outcome models by learning invariant prediction models and rely on exhaustive searches over all (exponentially many) covariate subsets. These approaches present two major challenges: 1) determining the conditions under which the invariant prediction model aligns with the causal outcome model, and 2) devising computationally efficient causal discovery algorithms that scale polynomially, instead of exponentially, with the number of covariates. To address both challenges, we focus on the additive intervention regime and propose nearly necessary and sufficient conditions for ensuring that the invariant prediction model matches the causal outcome model. Exploiting the essentially necessary identifiability conditions, we introduce Negative Weight Distributionally Robust Optimization (NegDRO), a nonconvex continuous minimax optimization whose global optimizer recovers the causal outcome model. Unlike standard group DRO problems that maximize over the simplex, NegDRO allows negative weights on environment losses, which break the convexity. Despite its nonconvexity, we demonstrate that a standard gradient method converges to the causal outcome model, and we establish the convergence rate with respect to the sample size and the number of iterations. Our algorithm avoids exhaustive search, making it scalable especially when the number of covariates is large. The numerical results further validate the efficiency of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11850v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhenyu Wang, Yifan Hu, Peter B\"uhlmann, Zijian Guo</dc:creator>
    </item>
    <item>
      <title>Identification and Estimation of Average Causal Effects in Fixed Effects Logit Models</title>
      <link>https://arxiv.org/abs/2105.00879</link>
      <description>arXiv:2105.00879v4 Announce Type: replace-cross 
Abstract: This paper studies identification and estimation of average causal effects, such as average marginal or treatment effects, in fixed effects logit models with short panels. Relating the identified set of these effects to an extremal moment problem, we first show how to obtain sharp bounds on such effects simply, without any optimization. We also consider even simpler outer bounds, which, contrary to the sharp bounds, do not require any first-step nonparametric estimators. We build confidence intervals based on these two approaches and show their asymptotic validity. Monte Carlo simulations suggest that both approaches work well in practice, the second being typically competitive in terms of interval length. Finally, we show that our method is also useful to measure treatment effect heterogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.00879v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Laurent Davezies, Xavier D'Haultf{\oe}uille, Louise Laage</dc:creator>
    </item>
  </channel>
</rss>
