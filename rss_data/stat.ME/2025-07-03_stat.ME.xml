<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 03 Jul 2025 04:01:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Minority Representation in Network Rankings: Methods for Estimation, Testing, and Fairness</title>
      <link>https://arxiv.org/abs/2507.01136</link>
      <description>arXiv:2507.01136v1 Announce Type: new 
Abstract: Networks, composed of nodes and their connections, are widely used to model complex relationships across various fields. Centrality metrics often inform decisions such as identifying key nodes or prioritizing resources. However, networks frequently suffer from missing or incorrect edges, which can systematically centrality-based decisions and distort the representation of certain protected groups. To address this issue, we introduce a formal definition of minority representation, measured as the proportion of minority nodes among the top-ranked nodes. We model systematic bias against minority groups by using group-dependent missing edge errors. We propose methods to estimate and detect systematic bias. Asymptotic limits of minority representation statistics are derived under canonical network models and used to correct representation of minority groups in node rankings. Simulation results demonstrate the effectiveness of our estimation, testing, and ranking correction procedures, and we apply our methods to a contact network, showcasing their practical applicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01136v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hui Shen, Peter W. MacDonald, Eric D. Kolaczyk</dc:creator>
    </item>
    <item>
      <title>A robust Likelihood Ratio Test for high-dimensional MANOVA -- with excellent performance</title>
      <link>https://arxiv.org/abs/2507.01261</link>
      <description>arXiv:2507.01261v1 Announce Type: new 
Abstract: The present paper answers the following questions related with high-dimensional manova: (i) is it possible to develop a likelihood ratio test for high-dimensional manova? (ii) would such test perform well? (iii) would it be able to outperform existing tests? (iv) would it be applicable to extremely small samples? (v) would it be applicable to non-normal random variables, as uniform, extremely skewed distributions, or even heavy tailed distributions with success? (vi) would it have a nice, rather simple to compute and well performing, asymptotic distribution? And what about if the answer to all the above questions would be a clear 'Yes'? Surprisingly enough, it is exactly the case. Extensive simulations, with both normal and non-normal distributions, some of which are heavy tailed and/or highly skewed, and even discrete distributions, are carried out in order to evaluate the performance of the proposed test and to compare its performance with other tests. Two real data applications are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01261v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Carlos A. Coelho</dc:creator>
    </item>
    <item>
      <title>Semi-supervised learning for linear extremile regression</title>
      <link>https://arxiv.org/abs/2507.01314</link>
      <description>arXiv:2507.01314v1 Announce Type: new 
Abstract: Extremile regression, as a least squares analog of quantile regression, is potentially useful tool for modeling and understanding the extreme tails of a distribution. However, existing extremile regression methods, as nonparametric approaches, may face challenges in high-dimensional settings due to data sparsity, computational inefficiency, and the risk of overfitting. While linear regression serves as the foundation for many other statistical and machine learning models due to its simplicity, interpretability, and relatively easy implementation, particularly in high-dimensional settings, this paper introduces a novel definition of linear extremile regression along with an accompanying estimation methodology. The regression coefficient estimators of this method achieve $\sqrt{n}$-consistency, which nonparametric extremile regression may not provide. In particular, while semi-supervised learning can leverage unlabeled data to make more accurate predictions and avoid overfitting to small labeled datasets in high-dimensional spaces, we propose a semi-supervised learning approach to enhance estimation efficiency, even when the specified linear extremile regression model may be misspecified. Both simulation studies and real data analyses demonstrate the finite-sample performance of our proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01314v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rong Jiang, Keming Yu, Jiangfeng Wang</dc:creator>
    </item>
    <item>
      <title>Mixtures of Neural Network Experts with Application to Phytoplankton Flow Cytometry Data</title>
      <link>https://arxiv.org/abs/2507.01375</link>
      <description>arXiv:2507.01375v1 Announce Type: new 
Abstract: Flow cytometry is a valuable technique that measures the optical properties of particles at a single-cell resolution. When deployed in the ocean, flow cytometry allows oceanographers to study different types of photosynthetic microbes called phytoplankton. It is of great interest to study how phytoplankton properties change in response to environmental conditions. In our work, we develop a nonlinear mixture of experts model for simultaneous clustering and regression utilizing random-weight neural networks. Our model allows one to flexibly estimate how cell properties and relative abundances depend on environmental covariates, without the computational burden of backpropagation. We show that the proposed model provides superior predictive performance in simulated examples compared to a mixture of linear experts. Also, applying our model to real data, we show that our model has (1) comparable out-of-sample prediction performance, and (2) more realistic estimates of phytoplankton behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01375v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ethan Pawl, Fran\c{c}ois Ribalet, Paul A. Parker, Sangwon Hyun</dc:creator>
    </item>
    <item>
      <title>Targeted tuning of random forests for quantile estimation and prediction intervals</title>
      <link>https://arxiv.org/abs/2507.01430</link>
      <description>arXiv:2507.01430v1 Announce Type: new 
Abstract: We present a novel tuning procedure for random forests (RFs) that improves the accuracy of estimated quantiles and produces valid, relatively narrow prediction intervals. While RFs are typically used to estimate mean responses (conditional on covariates), they can also be used to estimate quantiles by estimating the full distribution of the response. However, standard approaches for building RFs often result in excessively biased quantile estimates. To reduce this bias, our proposed tuning procedure minimizes "quantile coverage loss" (QCL), which we define as the estimated bias of the marginal quantile coverage probability estimate based on the out-of-bag sample. We adapt QCL tuning to handle censored data and demonstrate its use with random survival forests. We show that QCL tuning results in quantile estimates with more accurate coverage probabilities than those achieved using default parameter values or traditional tuning (using MSPE for uncensored data and C-index for censored data), while also reducing the estimated MSE of these coverage probabilities. We discuss how the superior performance of QCL tuning is linked to its alignment with the estimation goal. Finally, we explore the validity and width of prediction intervals created using this method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01430v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Berkowitz, Rachel MacKay Altman, Thomas M. Loughin</dc:creator>
    </item>
    <item>
      <title>Nonparametric learning of heterogeneous graphical model on network-linked data</title>
      <link>https://arxiv.org/abs/2507.01473</link>
      <description>arXiv:2507.01473v1 Announce Type: new 
Abstract: Graphical models have been popularly used for capturing conditional independence structure in multivariate data, which are often built upon independent and identically distributed observations, limiting their applicability to complex datasets such as network-linked data. This paper proposes a nonparametric graphical model that addresses these limitations by accommodating heterogeneous graph structures without imposing any specific distributional assumptions. The proposed estimation method effectively integrates network embedding with nonparametric graphical model estimation. It further transforms the graph learning task into solving a finite-dimensional linear equation system by leveraging the properties of vector-valued reproducing kernel Hilbert space. Moreover, theoretical guarantees are established for the proposed method in terms of the estimation consistency and exact recovery of the heterogeneous graph structures. Its effectiveness is also demonstrated through a variety of simulated examples and a real application to the statistician coauthorship dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01473v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuwen Wang, Changyu Liu, Xin He, Junhui Wang</dc:creator>
    </item>
    <item>
      <title>Root/Additional Metric (RoAM) framework: a guide for goal-centred metric construction</title>
      <link>https://arxiv.org/abs/2507.01526</link>
      <description>arXiv:2507.01526v1 Announce Type: new 
Abstract: The use of metrics underpins the quantification, communication and, ultimately, the functioning of a wide range of disciplines as diverse as labour recruitment, institutional management, economics and science. For application of metrics, customised scores are widely employed to optimise progress monitoring towards a goal, to contribute to decision-making, and to quantify situations under evaluation. However, the development of such metrics in complex and rigorous settings intrinsically relies on mathematical processes which are not always readily accessible. Here, we propose a framework for construction of metrics suitable for a wide range of disciplines, following a specified workflow that combines existing decision analysis and utility theory concepts to create a customisable performance metric (with corresponding uncertainty) that can be used to quantitatively evaluate goal achievement. It involves dividing criteria into two groups (root and additional) to utilise a newly proposed alternative form of utility function designed to build such customised metrics. Once the metrics are produced by this approach, these metrics can be used on a varied set of contexts, including their use in subsequent statistical analysis with the metric values as a response variable, or informing a decision-making process. The flexibility of the metric construction makes it suitable for a wide range of fields and applications, and could provide a valuable first step for monitoring and comparison in many different settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01526v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Luke E. B. Goodyear, Daniel Pincheira-Donoso</dc:creator>
    </item>
    <item>
      <title>Tensor-product interactions in Markov-switching models</title>
      <link>https://arxiv.org/abs/2507.01555</link>
      <description>arXiv:2507.01555v1 Announce Type: new 
Abstract: Markov-switching models are a powerful tool for modelling time series data that are driven by underlying latent states. As such, they are widely used in behavioural ecology, where discrete states can serve as proxies for behavioural modes and enable inference on latent behaviour driving e.g. observed movement. To understand drivers of behavioural changes, it is common to link model parameters to covariates. Over the last decade, nonparametric approaches have gained traction in this context to avoid unrealistic parametric assumptions. Nonetheless, existing methods are largely limited to univariate smooth functions of covariates, based on penalised splines, while real processes are typically complex requiring consideration of interaction effects. We address this gap by incorporating tensor-product interactions into Markov-switching models, enabling flexible modelling of multidimensional effects in a computationally efficient manner. Based on the extended Fellner-Schall method, we develop an efficient automatic smoothness selection procedure that is robust and scales well with the number of smooth functions in the model. The method builds on a random effects view of the spline coefficients and yields a recursive penalised likelihood procedure. As special cases, this general framework accommodates bivariate smoothing, function-valued random effects, and space-time interactions. We demonstrate its practical utility through three ecological case studies of an African elephant, common fruitflies, and Arctic muskoxen. The methodology is implemented in the LaMa R package, providing applied ecologists with an accessible and flexible tool for semiparametric inference in hidden-state models. The approach has the potential to drastically improve the level of detail in inference, allowing to fit HMMs with hundreds of parameters, 10-20 (potentially bivariate) smooths to thousands of observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01555v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jan-Ole Koslik</dc:creator>
    </item>
    <item>
      <title>A Bayesian framework for change-point detection with uncertainty quantification</title>
      <link>https://arxiv.org/abs/2507.01558</link>
      <description>arXiv:2507.01558v1 Announce Type: new 
Abstract: We introduce a novel Bayesian method that can detect multiple structural breaks in the mean and variance of a length $T$ time-series. Our method quantifies the uncertainty by returning $\alpha$-level credible sets around the estimated locations of the breaks. In the case of a single change in the mean and/or the variance of an independent sub-Gaussian sequence, we prove that our method attains a localization rate that is minimax optimal up to a $\log T$ factor. For an $\alpha$-mixing sequence with dependence, we prove this optimality holds up to $\log^2 T$ factor. For $d$-dimensional mean changes, we show that if $d \gtrsim \log T$ and the mean signal is dense, then our method exactly recovers the location of the change at the optimal rate. We show that we can modularly combine single change-point models to detect multiple change-points. This approach enables efficient inference using a variational approximation of the posterior distribution for the change-points. The proposal is applicable to both continuous and count data. Extensive simulation studies demonstrate that our method is competitive with the state-of-the-art and returns credible sets that are an order of magnitude smaller than those returned by competitors without sacrificing nominal coverage guarantees. We test our method on real data by detecting i) gating of the ion channels in the outer membrane of a bacterial cell, and ii) changes in the lithological structure of an oil well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01558v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Davis Berlind, Lorenzo Cappello, Oscar Hernan Madrid Padilla</dc:creator>
    </item>
    <item>
      <title>Gold after Randomized Sand: Model-X Split Knockoffs for Controlled Transformation Selection</title>
      <link>https://arxiv.org/abs/2507.01732</link>
      <description>arXiv:2507.01732v1 Announce Type: new 
Abstract: Controlling the False Discovery Rate (FDR) in variable selection is crucial for reproducibility and preventing over-selection, particularly with the increasing prevalence of predictive modeling. The Split Knockoff method, a recent extension of the canonical Knockoffs framework, offers finite-sample FDR control for selecting sparse transformations, finding applications across signal processing, economics, information technology, and the life sciences. However, its current formulation is limited to fixed design settings, restricting its use to linear models. The question of whether it can be generalized to random designs, thereby accommodating a broader range of models beyond the linear case -- similar to the Model-X Knockoff framework -- remains unanswered. A major challenge in addressing transformational sparsity within random design settings lies in reconciling the combination of a random design with a deterministic transformation. To overcome this limitation, we propose the Model-X Split Knockoff method. Our method achieves FDR control for transformation selection in random designs, bridging the gap between existing approaches. This is accomplished by introducing an auxiliary randomized design that interacts with both the existing random design and the deterministic transformation, enabling the construction of Model-X Split Knockoffs. Like the classical Model-X framework, our method provides provable finite-sample FDR control under known or accurately estimated covariate distributions, regardless of the conditional distribution of the response. Importantly, it guarantees at least the same selection power as Model-X Knockoffs when both are applicable. Empirical studies, including simulations and real-world applications to Alzheimer's disease imaging and university ranking analysis, demonstrate robust FDR control and suggest improved selection power over the original Model-X approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01732v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Cao, Hangyu Lin, Xinwei Sun, Yuan Yao</dc:creator>
    </item>
    <item>
      <title>Functional Renormalization for Signal Detection: Dimensional Analysis and Dimensional Phase Transition for Nearly Continuous Spectra Effective Field Theory</title>
      <link>https://arxiv.org/abs/2507.01064</link>
      <description>arXiv:2507.01064v1 Announce Type: cross 
Abstract: Signal detection is one of the main challenges of data science. According to the nature of the data, the presence of noise may corrupt measurements and hinder the discovery of significant patterns. A wide range of techniques aiming at extracting the relevant degrees of freedom from data has been thus developed over the years. However, signal detection in almost continuous spectra, for small signal-to-noise ratios, remains a known difficult issue. This paper develops over recent advancements proposing to tackle this issue by analysing the properties of the underlying effective field theory arising as a sort of maximal entropy distribution in the vicinity of universal random matrix distributions. Nearly continuous spectra provide an intrinsic and non-conventional scaling law for field and couplings, the scaling dimensions depending on the energy scale. The coarse-graining over small eigenvalues of the empirical spectrum defines a specific renormalization group, whose characteristics change when the collective behaviour of "informational" modes become significant, that is, stronger than the intrinsic fluctuations of noise. This paper pursues three different goals. First, we propose to quantify the real effects of fluctuations relative to what can be called "signal", while improving the robustness of the results obtained in our previous work. Second, we show that quantitative changes in the presence of a signal result in a counterintuitive modification of the distribution of eigenvectors. Finally, we propose a method for estimating the number of noise components and define a limit of detection in a general nearly continuous spectrum using the renormalization group. The main statements of this paper are essentially numeric, and their reproducibility can be checked using the associated code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01064v1</guid>
      <category>physics.data-an</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.IT</category>
      <category>hep-th</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Riccardo Finotello, Vincent Lahoche, Dine Ousmane Samary</dc:creator>
    </item>
    <item>
      <title>Shrinkage-Based Regressions with Many Related Treatments</title>
      <link>https://arxiv.org/abs/2507.01202</link>
      <description>arXiv:2507.01202v1 Announce Type: cross 
Abstract: When using observational causal models, practitioners often want to disentangle the effects of many related, partially-overlapping treatments. Examples include estimating treatment effects of different marketing touchpoints, ordering different types of products, or signing up for different services. Common approaches that estimate separate treatment coefficients are too noisy for practical decision-making. We propose a computationally light model that uses a customized ridge regression to move between a heterogeneous and a homogenous model: it substantially reduces MSE for the effects of each individual sub-treatment while allowing us to easily reconstruct the effects of an aggregated treatment. We demonstrate the properties of this estimator in theory and simulation, and illustrate how it has unlocked targeted decision-making at Wayfair.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01202v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Enes Dilber, Colin Gray</dc:creator>
    </item>
    <item>
      <title>Parsimonious Gaussian mixture models with piecewise-constant eigenvalue profiles</title>
      <link>https://arxiv.org/abs/2507.01542</link>
      <description>arXiv:2507.01542v1 Announce Type: cross 
Abstract: Gaussian mixture models (GMMs) are ubiquitous in statistical learning, particularly for unsupervised problems. While full GMMs suffer from the overparameterization of their covariance matrices in high-dimensional spaces, spherical GMMs (with isotropic covariance matrices) certainly lack flexibility to fit certain anisotropic distributions. Connecting these two extremes, we introduce a new family of parsimonious GMMs with piecewise-constant covariance eigenvalue profiles. These extend several low-rank models like the celebrated mixtures of probabilistic principal component analyzers (MPPCA), by enabling any possible sequence of eigenvalue multiplicities. If the latter are prespecified, then we can naturally derive an expectation-maximization (EM) algorithm to learn the mixture parameters. Otherwise, to address the notoriously-challenging issue of jointly learning the mixture parameters and hyperparameters, we propose a componentwise penalized EM algorithm, whose monotonicity is proven. We show the superior likelihood-parsimony tradeoffs achieved by our models on a variety of unsupervised experiments: density fitting, clustering and single-image denoising.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01542v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tom Szwagier, Pierre-Alexandre Mattei, Charles Bouveyron, Xavier Pennec</dc:creator>
    </item>
    <item>
      <title>Fitting Sparse Markov Models to Categorical Time Series Using Convex Clustering</title>
      <link>https://arxiv.org/abs/2202.05485</link>
      <description>arXiv:2202.05485v2 Announce Type: replace 
Abstract: Higher-order Markov chains are frequently used to model categorical time series. However, a major problem with fitting such models is the exponentially growing number of parameters in the model order. A popular approach to parsimonious modeling is to use a Variable Length Markov Chain (VLMC), which determines relevant contexts (recent pasts) of variable orders and forms a context tree. A more general parsimonious modeling approach is given by Sparse Markov Models (SMMs), where all possible histories of order $m$ are partitioned such that the transition probability vectors are identical for the histories belonging to any particular group. In this paper, we develop an elegant method of fitting SMMs based on convex clustering and regularization. The regularization parameter is selected using the BIC criterion. Theoretical results establish model selection consistency of our method for large sample size. Extensive simulation results under different set-ups are presented to study finite sample performance of the method. Real data analysis on modelling and classifying disease sub-types demonstrates the applicability of our method as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.05485v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tuhin Majumder, Soumendra Lahiri, Donald Martin</dc:creator>
    </item>
    <item>
      <title>Static and Dynamic BART for Rank-Order Data</title>
      <link>https://arxiv.org/abs/2308.10231</link>
      <description>arXiv:2308.10231v3 Announce Type: replace 
Abstract: Ranking lists are often provided at regular time intervals in a range of applications, including economics, sports, marketing, and politics. Most popular methods for rank-order data postulate a linear specification for the latent scores, which determine the observed ranks, and ignore the temporal dependence of the ranking lists. To address these issues, novel nonparametric static (ROBART) and autoregressive (ARROBART) models are developed, with latent scores defined as nonlinear Bayesian additive regression tree functions of covariates. To make inferences in the dynamic ARROBART model, closed-form filtering, predictive, and smoothing distributions for the latent time-varying scores are derived. These results are applied in a Gibbs sampler with data augmentation for posterior inference. The proposed methods are shown to outperform existing competitors in simulation studies, static data applications to electoral data, stated preferences for sushi and movies, and dynamic data applications to economic complexity rankings of countries and weekly pollster rankings of NCAA football teams.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.10231v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matteo Iacopini, Eoghan O'Neill, Luca Rossini</dc:creator>
    </item>
    <item>
      <title>Inference on the state process of periodically inhomogeneous hidden Markov models for animal behavior</title>
      <link>https://arxiv.org/abs/2312.14583</link>
      <description>arXiv:2312.14583v2 Announce Type: replace 
Abstract: Over the last decade, hidden Markov models (HMMs) have become increasingly popular in statistical ecology, where they constitute natural tools for studying animal behavior based on complex sensor data. Corresponding analyses sometimes explicitly focus on - and in any case need to take into account - periodic variation, for example by quantifying the activity distribution over the daily cycle or seasonal variation such as migratory behavior. For HMMs including periodic components, we establish important mathematical properties that allow for comprehensive statistical inference related to periodic variation, thereby also providing guidance for model building and model checking. Specifically, we derive the periodically varying unconditional state distribution as well as the time-varying and overall state dwell-time distributions - all of which are of key interest when the inferential focus lies on the dynamics of the state process. We use the associated novel inference and model-checking tools to investigate changes in the diel activity patterns of fruit flies in response to changing light conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14583v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jan-Ole Koslik, Carlina C. Feldmann, Sina Mews, Rouven Michels, Roland Langrock</dc:creator>
    </item>
    <item>
      <title>Bridging Item Response Theory and Factor Analysis: A Four-Parameter Mixture-Dichotomized Model with Bayesian Estimation</title>
      <link>https://arxiv.org/abs/2407.04071</link>
      <description>arXiv:2407.04071v2 Announce Type: replace 
Abstract: Item Response Theory (IRT) and Factor Analysis (FA) are two major frameworks used to model multi-item measurements of latent traits. While the relationship between two-parameter IRT models and dichotomized FA models is well established, IRT models with additional parameters have lacked corresponding FA formulations. This work introduces a four-parameter factor analytic (4P FA) model for multi-item measurements composed of binary items, building on the traditional dichotomized single-factor FA model. We derive the relationship between the proposed 4P FA model and its counterpart in the IRT framework, the 4P IRT model. A Bayesian estimation method is developed to estimate the four item parameters, the respondents' latent scores, and the scores adjusted for guessing and inattention effects. The proposed algorithm is implemented in R and Python, and the relationship between the 4P FA and 4P IRT models is empirically examined using two real datasets: a standardized admission test and a psychological anxiety inventory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04071v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J\'an Pavlech, Patr\'icia Martinkov\'a</dc:creator>
    </item>
    <item>
      <title>Skew-symmetric approximations of posterior distributions</title>
      <link>https://arxiv.org/abs/2409.14167</link>
      <description>arXiv:2409.14167v2 Announce Type: replace 
Abstract: Routinely-implemented deterministic approximations of posterior distributions from, e.g., Laplace method, variational Bayes and expectation-propagation, generally rely on symmetric approximating densities, often taken to be Gaussian. This choice facilitates optimization and inference, but typically affects the quality of the overall approximation. In fact, even in basic parametric models, the posterior distribution often displays asymmetries that yield bias and a reduced accuracy when considering symmetric approximations. Recent research has moved towards more flexible approximating densities that incorporate skewness. However, current solutions are often model-specific, lack general supporting theory, increase the computational complexity of the optimization problem, and do not provide a broadly-applicable solution to include skewness in any symmetric approximation. This article addresses such a gap by introducing a general and provably-optimal strategy to perturb any off-the-shelf symmetric approximation of a generic posterior distribution. Crucially, this novel perturbation is derived without additional optimization steps, and yields a similarly-tractable approximation within the class of skew-symmetric densities that provably enhances the finite-sample accuracy of the original symmetric counterpart. Furthermore, under suitable assumptions, it improves the convergence rate to the exact posterior by at least a $\sqrt{n}$ factor, in asymptotic regimes. These advancements are illustrated in numerical studies focusing on skewed perturbations of state-of-the-art Gaussian approximations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14167v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francesco Pozza, Daniele Durante, Botond Szabo</dc:creator>
    </item>
    <item>
      <title>A regression framework for studying relationships among attributes under network interference</title>
      <link>https://arxiv.org/abs/2410.07555</link>
      <description>arXiv:2410.07555v2 Announce Type: replace 
Abstract: To understand how the interconnected and interdependent world of the twenty-first century operates and make model-based predictions, joint probability models for networks and interdependent outcomes are needed. We propose a comprehensive regression framework for networks and interdependent outcomes with multiple advantages, including interpretability, scalability, and provable theoretical guarantees. The regression framework can be used for studying relationships among attributes of connected units and captures complex dependencies among connections and attributes, while retaining the virtues of linear regression, logistic regression, and other regression models by being interpretable and widely applicable. On the computational side, we show that the regression framework is amenable to scalable statistical computing based on convex optimization of pseudo-likelihoods using minorization-maximization methods. On the theoretical side, we establish convergence rates for pseudo-likelihood estimators based on a single observation of dependent connections and attributes. We demonstrate the regression framework using simulations and an application to hate speech on the social media platform X.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07555v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Cornelius Fritz, Michael Schweinberger, Subhankar Bhadra, David R. Hunter</dc:creator>
    </item>
    <item>
      <title>Flexible space-time models for extreme data</title>
      <link>https://arxiv.org/abs/2411.19184</link>
      <description>arXiv:2411.19184v3 Announce Type: replace 
Abstract: Extreme value analysis is an essential methodology in the study of rare and extreme events, which hold significant interest in various fields, particularly in the context of environmental sciences. Models that employ the exceedances of values above suitably selected high thresholds possess the advantage of capturing the "sub-asymptotic" dependence of data. This paper presents an extension of spatial random scale mixture models to the spatio-temporal domain. A comprehensive framework for characterizing the dependence structure of extreme events across both dimensions is provided. Indeed, the model is capable of distinguishing between asymptotic dependence and independence, both in space and time, through the use of parametric inference. The high complexity of the likelihood function for the proposed model necessitates a simulation approach based on neural networks for parameter estimation, which leverages summaries of the sub-asymptotic dependence present in the data. The effectiveness of the model in assessing the limiting dependence structure of spatio-temporal processes is demonstrated through both simulation studies and an application to rainfall datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19184v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Dell'Oro, Carlo Gaetan</dc:creator>
    </item>
    <item>
      <title>Nonparanormal Adjusted Marginal Inference</title>
      <link>https://arxiv.org/abs/2503.01657</link>
      <description>arXiv:2503.01657v2 Announce Type: replace 
Abstract: Although treatment effects can be estimated from observed outcome distributions obtained from proper randomization in clinical trials, covariate adjustment is recommended to increase precision. For important treatment effects, such as odds or hazard ratios, conditioning on covariates in binary logistic or proportional hazards models changes the interpretation of the treatment effect and conditioning on different sets of covariates renders the resulting effect estimates incomparable.
  We propose a novel nonparanormal model formulation for adjusted marginal inference. This model for the joint distribution of outcome and covariates directly features a marginally defined treatment effect parameter, such as a marginal odds or hazard ratio. Not only the marginal treatment effect of interest can be estimated based on this model, it also provides an overall coefficient of determination and covariate-specific measures of prognostic strength.
  For the special case of Cohen's standardized mean difference d, we theoretically show that adjusting for an informative prognostic variable improves the precision of the marginal, noncollapsible effect. Empirical results confirm this not only for Cohen's d but also for odds and hazard ratios in simulations and three applications. A reference implementation is available in the R add-on package tram.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01657v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Susanne Dandl, Torsten Hothorn</dc:creator>
    </item>
    <item>
      <title>Continuously updated estimation of conditional hazard functions</title>
      <link>https://arxiv.org/abs/2503.08356</link>
      <description>arXiv:2503.08356v2 Announce Type: replace 
Abstract: Motivated by the need to analyze continuously updated data sets in the context of time-to-event modeling, we propose a novel nonparametric approach to estimate the conditional hazard function given a set of continuous and discrete predictors. The method is based on a representation of the conditional hazard as a ratio between a joint density and a conditional expectation determined by the distribution of the observed variables. It is shown that such ratio representations are available for uni- and bivariate time-to-events, in the presence of common types of random censoring, truncation, and with possibly cured individuals, as well as for competing risks. This opens the door to nonparametric approaches in many time-to-event predictive models. To estimate joint densities and conditional expectations we propose the recursive kernel smoothing, which is well suited for online estimation. Asymptotic results for such estimators are derived and it is shown that they achieve optimal convergence rates. Simulation experiments show the good finite sample performance of our recursive estimator with right censoring. The method is applied to a real dataset of primary breast cancer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08356v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daphn\'e Aurouet, Valentin Patilea</dc:creator>
    </item>
    <item>
      <title>Wordkrill: Extending Wordfish into the multidimensional political space</title>
      <link>https://arxiv.org/abs/2506.20275</link>
      <description>arXiv:2506.20275v2 Announce Type: replace 
Abstract: Spatial models are central to the study of political conflict, yet their empirical application often depends on text-based methods. A prominent example is the Wordfish model, which estimates actor positions from political texts. However, a key limitation of Wordfish is its unidimensionality, despite the well-established multidimensional nature of political competition. This contribution introduces Wordkrill, a multidimensional extension of Wordfish that retains the original model's interpretability while allowing for efficient estimation of political positions along multiple latent dimensions. After presenting the mathematical framework of Wordkrill, its utility through brief applications to party manifestos and parliamentary speeches is demonstrated. These examples illustrate both the practical advantages and current limitations of the approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20275v2</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Benjamin Riesch</dc:creator>
    </item>
    <item>
      <title>Bayesian analysis of the causal reference-based model for missing data in clinical trials</title>
      <link>https://arxiv.org/abs/2507.00680</link>
      <description>arXiv:2507.00680v2 Announce Type: replace 
Abstract: The statistical analysis of clinical trials is often complicated by missing data. Patients sometimes experience intercurrent events (ICEs), which usually (although not always) lead to missing subsequent outcome measurements for such individuals. The reference-based imputation methods were proposed by Carpenter et al. (2013) and have been commonly adopted for handling missing data due to ICEs when estimating treatment policy strategy estimands. Conventionally, the variance for reference-based estimators was obtained using Rubin's rules. However, Rubin's rules variance estimator is biased compared to the repeated sampling variance of the point estimator, due to uncongeniality. Repeated sampling variance estimators were proposed as an alternative to variance estimation for reference-based estimators. However, these have the property that they decrease as the proportion of ICEs increases. White et al. (2019) introduced a causal model incorporating the concept of a 'maintained treatment effect' following the occurrence of ICEs and showed that this causal model included common reference-based estimators as special cases. Building on this framework, we propose introducing a prior distribution for the maintained effect parameter to account for uncertainty in this assumption. Our approach provides inference for reference-based estimators that explicitly reflects our uncertainty about how much treatment effects are maintained after the occurrence of ICEs. In trials where no or little post-ICE data are observed, our proposed Bayesian reference-based causal model approach can be used to estimate the treatment policy treatment effect, incorporating uncertainty about the reference-based assumption. We compare the frequentist properties of this approach with existing reference-based methods through simulations and by application to an antidepressant trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00680v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brendah Nansereko, Marcel Wolbers, James Carpenter, Jonathan Bartlett</dc:creator>
    </item>
    <item>
      <title>New stochastic highway capacity estimation method and why product limit method is unsuitable</title>
      <link>https://arxiv.org/abs/2003.05355</link>
      <description>arXiv:2003.05355v2 Announce Type: replace-cross 
Abstract: Kaplan-Meier estimate, commonly known as product limit method (PLM), and maximum likelihood estimate (MLE) methods in general are often cited as means of stochastic highway capacity estimation. This article discusses their unsuitability for such application as properties of traffic flow do not meet the assumptions for use of the methods. They assume the observed subject has a history which it went through and did not fail. However, due to its nature, each traffic flow measurement behaves as a separate subject which did not go through all the lower levels of intensity (did not "age"). An alternative method is proposed. It fits the resulting cumulative frequency of breakdowns with respect to the traffic flow intensity leading to the breakdown instead of directly estimating the underlying probability distribution of capacity. Analyses of accuracy and sensitivity to data quantity and censoring rate of the new method are provided along with comparison to the PLM. The results prove unsuitability of the PLM and MLE methods in general. The new method is then used in a case study which compares capacity of a work-zone with and without a traffic flow speed harmonisation system installed. The results confirm positive effect of harmonisation on capacity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2003.05355v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Igor Mikolasek</dc:creator>
    </item>
    <item>
      <title>Uniform Inference on High-dimensional Spatial Panel Networks</title>
      <link>https://arxiv.org/abs/2105.07424</link>
      <description>arXiv:2105.07424v5 Announce Type: replace-cross 
Abstract: We propose employing a high-dimensional generalized method of moments (GMM) estimator, regularized for dimension reduction and subsequently debiased to correct for shrinkage bias (referred to as a debiased-regularized estimator), for inference on large-scale spatial panel networks. In particular, the network structure, which incorporates a flexible sparse deviation that can be regarded either as a latent component or as a misspecification of a predetermined adjacency matrix, is estimated using a debiased machine learning approach. The theoretical analysis establishes the consistency and asymptotic normality of our proposed estimator, taking into account general temporal and spatial dependencies inherent in the data-generating processes. A primary contribution of our study is the development of a uniform inference theory, which enables hypothesis testing on the parameters of interest, including zero or non-zero elements in the network structure. Additionally, the asymptotic properties of the estimator are derived for both linear and nonlinear moments. Simulations demonstrate the superior performance of our proposed approach. Finally, we apply our methodology to investigate the spatial network effects of stock returns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.07424v5</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Victor Chernozhukov, Chen Huang, Weining Wang</dc:creator>
    </item>
    <item>
      <title>Distribution Matching for Self-Supervised Transfer Learning</title>
      <link>https://arxiv.org/abs/2502.14424</link>
      <description>arXiv:2502.14424v2 Announce Type: replace-cross 
Abstract: In this paper, we propose a novel self-supervised transfer learning method called \underline{\textbf{D}}istribution \underline{\textbf{M}}atching (DM), which drives the representation distribution toward a predefined reference distribution while preserving augmentation invariance. DM results in a learned representation space that is intuitively structured and therefore easy to interpret.
  Experimental results across multiple real-world datasets and evaluation metrics demonstrate that DM performs competitively on target classification tasks compared to existing self-supervised transfer learning methods. Additionally, we provide robust theoretical guarantees for DM, including a population theorem and an end-to-end sample theorem. The population theorem bridges the gap between the self-supervised learning task and target classification accuracy, while the sample theorem shows that, even with a limited number of samples from the target domain, DM can deliver exceptional classification performance, provided the unlabeled sample size is sufficiently large.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14424v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuling Jiao, Wensen Ma, Defeng Sun, Hansheng Wang, Yang Wang</dc:creator>
    </item>
  </channel>
</rss>
