<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 13 Mar 2025 04:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Bayesian local clustering of functional data via semi-Markovian random partitions</title>
      <link>https://arxiv.org/abs/2503.08881</link>
      <description>arXiv:2503.08881v1 Announce Type: new 
Abstract: We introduce a Bayesian framework for indirect local clustering of functional data, leveraging B-spline basis expansions and a novel dependent random partition model. By exploiting the local support properties of B-splines, our approach allows partially coincident functional behaviors, achieved when shared basis coefficients span sufficiently contiguous regions. This is accomplished through a cutting-edge dependent random partition model that enforces semi-Markovian dependence across a sequence of partitions. By matching the order of the B-spline basis with the semi-Markovian dependence structure, the proposed model serves as a highly flexible prior, enabling efficient modeling of localized features in functional data. Furthermore, we extend the utility of the dependent random partition model beyond functional data, demonstrating its applicability to a broad class of problems where sequences of dependent partitions are central, and standard Markovian assumptions prove overly restrictive. Empirical illustrations, including analyses of simulated data and tide level measurements from the Venice Lagoon, showcase the effectiveness and versatility of the proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08881v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giovanni Toto, Antonio Canale</dc:creator>
    </item>
    <item>
      <title>Data-Driven Adjustment for Multiple Treatments</title>
      <link>https://arxiv.org/abs/2503.08971</link>
      <description>arXiv:2503.08971v1 Announce Type: new 
Abstract: Covariate adjustment is one method of causal effect identification in non-experimental settings. Prior research provides routes for finding appropriate adjustments sets, but much of this research assumes knowledge of the underlying causal graph. In this paper, we present two routes for finding adjustment sets that do not require knowledge of a graph -- and instead rely on dependencies and independencies in the data directly. We consider a setting where the adjustment set is unaffected by treatment or outcome. Our first route shows how to extend prior research in this area using a concept known as c-equivalence. Our second route provides sufficient criteria for finding adjustment sets in the setting of multiple treatments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08971v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sara LaPlante, Emilija Perkovi\'c, Sofia Triantafillou</dc:creator>
    </item>
    <item>
      <title>Multilevel Primary Aim Analyses of Clustered SMARTs: With Applications in Health Policy</title>
      <link>https://arxiv.org/abs/2503.08987</link>
      <description>arXiv:2503.08987v1 Announce Type: new 
Abstract: In many health policy settings, adaptive interventions target a population of clusters (e.g., schools), with the ultimate intent of impacting outcomes at the level of individuals within the clusters. Health policy researchers can use clustered, sequential, multiple assignment, randomized trials (SMARTs) to answer important scientific questions concerning clustered adaptive interventions. A common primary aim is to compare the mean of a nested, end-of-study outcome between two clustered adaptive interventions. However, existing methods are not suitable when the primary outcome in a clustered SMART is nested and longitudinal (e.g., repeated outcome measures nested within mental healthcare providers, and mental healthcare providers nested within schools). This manuscript proposes a three-level marginal mean modeling and estimation approach for comparing adaptive interventions in a clustered SMART. The proposed method enables policy analysts to answer a wider array of scientific questions in the marginal comparison of clustered adaptive interventions. Further, relative to using an existing two-level method with a nested end-of-study outcome, the proposed method benefits from improved statistical efficiency. With this approach, we examine longitudinal comparisons of adaptive interventions for improving school-based mental healthcare and contrast its performance with existing approaches for studying static end-of-study outcomes. Methods were motivated by the Adaptive School-Based Implementation of CBT (ASIC) study, a clustered SMART designed to construct an adaptive health policy to improve the adoption of evidence-based CBT by mental healthcare professionals in high schools across Michigan.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08987v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriel Durham, Anil Battalahalli, Amy Kilbourne, Andrew Quanbeck, Wenchu Pan, Tim Lycurgus, Daniel Almirall</dc:creator>
    </item>
    <item>
      <title>A Sparse Linear Model for Positive Definite Estimation of Covariance Matrices</title>
      <link>https://arxiv.org/abs/2503.09026</link>
      <description>arXiv:2503.09026v1 Announce Type: new 
Abstract: Sparse covariance matrices play crucial roles by encoding the interdependencies between variables in numerous fields such as genetics and neuroscience. Despite substantial studies on sparse covariance matrices, existing methods face several challenges such as the correlation among the elements in the sample covariance matrix, positive definiteness and unbiased estimation of the diagonal elements. To address these challenges, we formulate a linear covariance model for estimating sparse covariance matrices and propose a penalized regression. This method is general enough to encompass existing sparse covariance estimators and can additionally consider correlation among the elements in the sample covariance matrix while preserving positive definiteness and fixing the diagonal elements to the sample variance, hence avoiding unnecessary bias in the diagonal elements. We apply our estimator to simulated data and real data from neuroscience and genetics to describe the efficacy of our proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09026v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rakheon Kim, Irina Gaynanova</dc:creator>
    </item>
    <item>
      <title>High-dimensional covariance matrix regularization using informative targets</title>
      <link>https://arxiv.org/abs/2503.09072</link>
      <description>arXiv:2503.09072v1 Announce Type: new 
Abstract: The sample covariance matrix becomes non-invertible in high-dimensional settings, making classical multivariate statistical methods inapplicable. Various regularization techniques address this issue by imposing a structured target matrix to improve stability and invertibility. While diagonal matrices are commonly used as targets due to their simplicity, more informative target matrices can enhance performance. This paper explores the use of such targets and estimates the underlying correlation parameter using maximum likelihood. The proposed method is analytically straightforward, computationally efficient, and more accurate than recent regularization techniques when targets are correctly specified. Its effectiveness is demonstrated through extensive simulations and a real-world application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09072v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Atiq Ur Rehman, Muhammad Farooq</dc:creator>
    </item>
    <item>
      <title>Spectral Clustering on Multilayer Networks with Covariates</title>
      <link>https://arxiv.org/abs/2503.09156</link>
      <description>arXiv:2503.09156v1 Announce Type: new 
Abstract: The community detection problem on multilayer networks have drawn much interest. When the nodal covariates ar also present, few work has been done to integrate information from both sources. To leverage the multilayer networks and the covariates, we propose two new algorithms: the spectral clustering on aggregated networks with covariates (SCANC), and the spectral clustering on aggregated Laplacian with covariates (SCALC). These two algorithms are easy to implement, computationally fast, and feature a data-driven approach for tuning parameter selection.
  We establish theoretical guarantees for both methods under the Multilayer Stochastic Blockmodel with Covariates (MSBM-C), demonstrating their consistency in recovering community structure. Our analysis reveals that increasing the number of layers, incorporating covariate information, and enhancing network density all contribute to improved clustering accuracy. Notably, SCANC is most effective when all layers exhibit similar assortativity, whereas SCALC performs better when both assortative and disassortative layers are present. On the simulation studies and a primary school contact data analysis, our method outperforms other methods. Our results highlight the advantages of spectral-based aggregation techniques in leveraging both network structure and nodal attributes for robust community detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09156v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Da Zhao, Wanjie Wang, Jialiang Li</dc:creator>
    </item>
    <item>
      <title>Competing-risk Weibull survival model with multiple causes</title>
      <link>https://arxiv.org/abs/2503.09310</link>
      <description>arXiv:2503.09310v1 Announce Type: new 
Abstract: The failure of a system can result from the simultaneous effects of multiple causes, where assigning a specific cause may be inappropriate or unavailable. Examples include contributing causes of death in epidemiology and the aetiology of neurodegenerative diseases like Alzheimer's. We propose a parametric Weibull accelerated failure time model for multiple causes, incorporating a data-driven, individualized, and time-varying winning probability (relative importance) matrix. Using maximum likelihood estimation and the expectation-maximization (EM) algorithm, our approach enables simultaneous estimation of regression coefficients and relative cause importance, ensuring consistency and asymptotic normality. A simulation study and an application to Alzheimer's disease demonstrate its effectiveness in addressing cause-mixture problems and identifying informative biomarker combinations, with comparisons to Weibull and Cox proportional hazards models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09310v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kai Wang, Yuqin Mu, Shenyi Zhang, Zhengjun Zhang, Chengxiu Ling</dc:creator>
    </item>
    <item>
      <title>Bayesian nonparametric modeling of mixed-type bounded data</title>
      <link>https://arxiv.org/abs/2503.09451</link>
      <description>arXiv:2503.09451v1 Announce Type: new 
Abstract: We propose a Bayesian nonparametric model for mixed-type bounded data, where some variables are compositional and others are interval-bounded. Compositional variables are non-negative and sum to a given constant, such as the proportion of time an individual spends on different activities during the day or the fraction of different types of nutrients in a person's diet. Interval-bounded variables, on the other hand, are real numbers constrained by both a lower and an upper bound. Our approach relies on a novel class of random multivariate Bernstein polynomials, which induce a Dirichlet process mixture model of products of Dirichlet and beta densities. We study the theoretical properties of the model, including its topological support and posterior consistency. The model can be used for density and conditional density estimation, where both the response and predictors take values in the simplex space and/or hypercube. We illustrate the model's behavior through the analysis of simulated data and data from the 2005-2006 cycle of the U.S. National Health and Nutrition Examination Survey.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09451v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rufeng Liu, Claudia Wehrhahn, Andr\'es F. Barrientos, Alejandro Jara</dc:creator>
    </item>
    <item>
      <title>Questioning Normality: A study of wavelet leaders distribution</title>
      <link>https://arxiv.org/abs/2503.08821</link>
      <description>arXiv:2503.08821v1 Announce Type: cross 
Abstract: The motivation of this article is to estimate multifractality classification and model selection parameters: the first-order scaling exponent $c_1$ and the second-order scaling exponent (or intermittency coefficient) $c_2$. These exponents are built on wavelet leaders, which therefore constitute fundamental tools in applied multifractal analysis. While most estimation methods, particularly Bayesian approaches, rely on the assumption of log-normality, we challenge this hypothesis by statistically testing the normality of log-leaders. Upon rejecting this common assumption, we propose instead a novel model based on log-concave distributions. We validate this new model on well-known stochastic processes, including fractional Brownian motion, the multifractal random walk, and the canonical Mandelbrot cascade, as well as on real-world marathon runner data. Furthermore, we revisit the estimation procedure for $c_1$, providing confidence intervals, and for $c_2$, applying it to fractional Brownian motions with various Hurst indices as well as to the multifractal random walk. Finally, we establish several theoretical results on the distribution of log-leaders in random wavelet series, which are consistent with our numerical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08821v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wejdene Ben Nasr, H\'el\`ene Halconruy, St\'ephane Jaffard</dc:creator>
    </item>
    <item>
      <title>Self-Consistent Equation-guided Neural Networks for Censored Time-to-Event Data</title>
      <link>https://arxiv.org/abs/2503.09097</link>
      <description>arXiv:2503.09097v1 Announce Type: cross 
Abstract: In survival analysis, estimating the conditional survival function given predictors is often of interest. There is a growing trend in the development of deep learning methods for analyzing censored time-to-event data, especially when dealing with high-dimensional predictors that are complexly interrelated. Many existing deep learning approaches for estimating the conditional survival functions extend the Cox regression models by replacing the linear function of predictor effects by a shallow feed-forward neural network while maintaining the proportional hazards assumption. Their implementation can be computationally intensive due to the use of the full dataset at each iteration because the use of batch data may distort the at-risk set of the partial likelihood function. To overcome these limitations, we propose a novel deep learning approach to non-parametric estimation of the conditional survival functions using the generative adversarial networks leveraging self-consistent equations. The proposed method is model-free and does not require any parametric assumptions on the structure of the conditional survival function. We establish the convergence rate of our proposed estimator of the conditional survival function. In addition, we evaluate the performance of the proposed method through simulation studies and demonstrate its application on a real-world dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09097v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sehwan Kim, Rui Wang, Wenbin Lu</dc:creator>
    </item>
    <item>
      <title>Representation Retrieval Learning for Heterogeneous Data Integration</title>
      <link>https://arxiv.org/abs/2503.09494</link>
      <description>arXiv:2503.09494v1 Announce Type: cross 
Abstract: In the era of big data, large-scale, multi-modal datasets are increasingly ubiquitous, offering unprecedented opportunities for predictive modeling and scientific discovery. However, these datasets often exhibit complex heterogeneity, such as covariate shift, posterior drift, and missing modalities, that can hinder the accuracy of existing prediction algorithms. To address these challenges, we propose a novel Representation Retrieval ($R^2$) framework, which integrates a representation learning module (the representer) with a sparsity-induced machine learning model (the learner). Moreover, we introduce the notion of "integrativeness" for representers, characterized by the effective data sources used in learning representers, and propose a Selective Integration Penalty (SIP) to explicitly improve the property. Theoretically, we demonstrate that the $R^2$ framework relaxes the conventional full-sharing assumption in multi-task learning, allowing for partially shared structures, and that SIP can improve the convergence rate of the excess risk bound. Extensive simulation studies validate the empirical performance of our framework, and applications to two real-world datasets further confirm its superiority over existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09494v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qi Xu, Annie Qu</dc:creator>
    </item>
    <item>
      <title>Neural Network-Based Change Point Detection for Large-Scale Time-Evolving Data</title>
      <link>https://arxiv.org/abs/2503.09541</link>
      <description>arXiv:2503.09541v1 Announce Type: cross 
Abstract: The paper studies the problem of detecting and locating change points in multivariate time-evolving data. The problem has a long history in statistics and signal processing and various algorithms have been developed primarily for simple parametric models. In this work, we focus on modeling the data through feed-forward neural networks and develop a detection strategy based on the following two-step procedure. In the first step, the neural network is trained over a prespecified window of the data, and its test error function is calibrated over another prespecified window. Then, the test error function is used over a moving window to identify the change point. Once a change point is detected, the procedure involving these two steps is repeated until all change points are identified. The proposed strategy yields consistent estimates for both the number and the locations of the change points under temporal dependence of the data-generating process. The effectiveness of the proposed strategy is illustrated on synthetic data sets that provide insights on how to select in practice tuning parameters of the algorithm and in real data sets. Finally, we note that although the detection strategy is general and can work with different neural network architectures, the theoretical guarantees provided are specific to feed-forward neural architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09541v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jialiang Geng, George Michailidis</dc:creator>
    </item>
    <item>
      <title>Characterization and Greedy Learning of Gaussian Structural Causal Models under Unknown Interventions</title>
      <link>https://arxiv.org/abs/2211.14897</link>
      <description>arXiv:2211.14897v4 Announce Type: replace 
Abstract: We consider the problem of recovering the causal structure underlying observations from different experimental conditions when the targets of the interventions in each experiment are unknown. We assume a linear structural causal model with additive Gaussian noise and consider interventions that perturb their targets while maintaining the causal relationships in the system. Different models may entail the same distributions, offering competing causal explanations for the given observations. We fully characterize this equivalence class and offer identifiability results, which we use to derive a greedy algorithm called GnIES to recover the equivalence class of the data-generating model without knowledge of the intervention targets. In addition, we develop a novel procedure to generate semi-synthetic data sets with known causal ground truth but distributions closely resembling those of a real data set of choice. We leverage this procedure and evaluate the performance of GnIES on an array of synthetic and semi-synthetic data sets, and real data from a biological system and a tightly controlled physical system. We provide, in the Python packages gnies and sempler, implementations of GnIES and our semi-synthetic data generation procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.14897v4</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan L. Gamella, Armeen Taeb, Christina Heinze-Deml, Peter B\"uhlmann</dc:creator>
    </item>
    <item>
      <title>Regression analysis of multiplicative hazards model with time-dependent coefficient for sparse longitudinal covariates</title>
      <link>https://arxiv.org/abs/2310.15877</link>
      <description>arXiv:2310.15877v3 Announce Type: replace 
Abstract: We study the multiplicative hazards model with intermittently observed longitudinal covariates and time-varying coefficients. For such models, the existing ad hoc approach, such as the last value carried forward, is biased. We propose a kernel weighting approach to get an unbiased estimation of the non-parametric coefficient function and establish asymptotic normality for any fixed time point. Furthermore, we construct the simultaneous confidence band to examine the overall magnitude of the variation. Simulation studies support our theoretical predictions and show favorable performance of the proposed method. A data set from Alzheimer's Disease Neuroimaging Initiative study is used to illustrate our methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.15877v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhuowei Sun, Hongyuan Cao</dc:creator>
    </item>
    <item>
      <title>A flexible model for Record Linkage</title>
      <link>https://arxiv.org/abs/2407.06835</link>
      <description>arXiv:2407.06835v2 Announce Type: replace 
Abstract: Combining data from various sources empowers researchers to explore innovative questions, for example those raised by conducting healthcare monitoring studies. However, the lack of a unique identifier often poses challenges. Record linkage procedures determine whether pairs of observations collected on different occasions belong to the same individual using partially identifying variables (e.g. birth year, postal code). Existing methodologies typically involve a compromise between computational efficiency and accuracy. Traditional approaches simplify this task by condensing information, yet they neglect dependencies among linkage decisions and disregard the one-to-one relationship required to establish coherent links. Modern approaches offer a comprehensive representation of the data generation process, at the expense of computational overhead and reduced flexibility. We propose a flexible method, that adapts to varying data complexities, addressing registration errors and accommodating changes of the identifying information over time. Our approach balances accuracy and scalability, estimating the linkage using a Stochastic Expectation Maximisation algorithm on a latent variable model. We illustrate the ability of our methodology to connect observations using large real data applications and demonstrate the robustness of our model to the linking variables quality in a simulation study. The proposed algorithm FlexRL is implemented and available in an open source R package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06835v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1093/jrsssc/qlaf016</arxiv:DOI>
      <dc:creator>Kayan\'e Robach, St\'ephanie L van der Pas, Mark A van de Wiel, Michel H Hof</dc:creator>
    </item>
    <item>
      <title>Hypothesis tests and model parameter estimation on data sets with missing correlation information</title>
      <link>https://arxiv.org/abs/2410.22333</link>
      <description>arXiv:2410.22333v4 Announce Type: replace 
Abstract: Ideally, all analyses of normally distributed data should include the full covariance information between all data points. In practice, the full covariance matrix between all data points is not always available. Either because a result was published without a covariance matrix, or because one tries to combine multiple results from separate publications. For simple hypothesis tests, it is possible to define robust test statistics that will behave conservatively in the presence on unknown correlations. For model parameter fits, one can inflate the variance by a factor to ensure that things remain conservative at least up to a chosen confidence level. This paper describes a class of robust test statistics for simple hypothesis tests, as well as an algorithm to determine the necessary inflation factor for model parameter fits and Goodness of Fit tests and composite hypothesis tests. It then presents some example applications of the methods to real neutrino interaction data and model comparisons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22333v4</guid>
      <category>stat.ME</category>
      <category>hep-ph</category>
      <category>stat.AP</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lukas Koch</dc:creator>
    </item>
    <item>
      <title>A generalized Bayesian approach for high-dimensional robust regression with serially correlated errors and predictors</title>
      <link>https://arxiv.org/abs/2412.05673</link>
      <description>arXiv:2412.05673v3 Announce Type: replace 
Abstract: This paper introduces a loss-based generalized Bayesian methodology for high-dimensional robust regression with serially correlated errors and predictors. The proposed framework employs a novel scaled pseudo-Huber (SPH) loss function, which smooths the well-known Huber loss, effectively balancing quadratic ($\ell_2$) and absolute linear ($\ell_1$) loss behaviors. This flexibility enables the framework to accommodate both thin-tailed and heavy-tailed data efficiently. The generalized Bayesian approach constructs a working likelihood based on the SPH loss, facilitating efficient and stable estimation while providing rigorous uncertainty quantification for all model parameters. Notably, this approach allows formal statistical inference without requiring ad hoc tuning parameter selection while adaptively addressing a wide range of tail behavior in the errors. By specifying appropriate prior distributions for the regression coefficients--such as ridge priors for small or moderate-dimensional settings and spike-and-slab priors for high-dimensional settings--the framework ensures principled inference. We establish rigorous theoretical guarantees for accurate parameter estimation and correct predictor selection under sparsity assumptions for a wide range of data generating setups. Extensive simulation studies demonstrate the superior performance of our approach compared to traditional Bayesian regression methods based on $\ell_2$ and $\ell_1$-loss functions. The results highlight its flexibility and robustness, particularly in challenging high-dimensional settings characterized by data contamination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05673v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saptarshi Chakraborty, Kshitij Khare, George Michailidis</dc:creator>
    </item>
    <item>
      <title>A cheat sheet for probability distributions of orientational data</title>
      <link>https://arxiv.org/abs/2412.08934</link>
      <description>arXiv:2412.08934v2 Announce Type: replace 
Abstract: The need for statistical models of orientations arises in many applications in engineering and computer science. Orientational data appear as sets of angles, unit vectors, rotation matrices or quaternions. In the field of directional statistics, a lot of advances have been made in modelling such types of data. However, only a few of these tools are used in engineering and computer science applications. Hence, this paper aims to serve as a cheat sheet for those probability distributions of orientations. Models for 1-DOF, 2-DOF and 3-DOF orientations are discussed. For each of them, expressions for the density function, fitting to data, and sampling are presented. The paper is written with a compromise between engineering and statistics in terms of notation and terminology. A Python library with functions for some of these models is provided. Using this library, two examples of applications to real data are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08934v2</guid>
      <category>stat.ME</category>
      <category>cs.RO</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>P. C. Lopez-Custodio</dc:creator>
    </item>
    <item>
      <title>Bayesian nonparametric mixtures of Archimedean copulas</title>
      <link>https://arxiv.org/abs/2412.09539</link>
      <description>arXiv:2412.09539v2 Announce Type: replace 
Abstract: Copula-based dependence modeling often relies on parametric formulations. This is mathematically convenient, but can be statistically inefficient when the parametric families are not suitable for the data and model in focus. A Bayesian nonparametric mixture of Archimedean copulas is introduced to increase the flexibility of copula-based dependence modeling. Specifically, the Poisson-Dirichlet process is used as a mixing distribution over the Archimedean copulas' parameter. Properties of the mixture model are studied for the main Archimedean families, and posterior distributions are sampled via their full conditional distributions. Performance of the model is shown via numerical experiments involving simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09539v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruyi Pan, Luis E. Nieto-Barajas, Radu V. Craiu</dc:creator>
    </item>
    <item>
      <title>Doubly Robust and Efficient Calibration of Prediction Sets for Censored Time-to-Event Outcomes</title>
      <link>https://arxiv.org/abs/2501.04615</link>
      <description>arXiv:2501.04615v2 Announce Type: replace 
Abstract: Our objective is to construct well-calibrated prediction sets for a time-to-event outcome subject to right-censoring with guaranteed coverage. Our approach is inspired by modern conformal inference literature in that, unlike classical frameworks, we obviate the need for a well-specified parametric or semiparametric survival model to accomplish our goal. In contrast to existing conformal prediction methods for survival data, which restrict censoring to be of Type I, whereby potential censoring times are assumed to be fully observed on all units in both training and validation samples, we consider the more common right-censoring setting in which either only the censoring time or only the event time of primary interest is directly observed, whichever comes first. Under a standard conditional independence assumption between the potential survival and censoring times given covariates, we propose and analyze two methods to construct valid and efficient lower predictive bounds for the survival time of a future observation. The proposed methods build upon modern semiparametric efficiency theory for censored data, in that the first approach incorporates inverse-probability-of-censoring weighting to account for censoring, while the second approach is based on augmenting this method with an additional correction term. For both methods, we formally establish asymptotic coverage guarantees and demonstrate, both theoretically and through empirical experiments, that the augmented approach substantially improves efficiency over the inverse-probability-of-censoring weighting method. Specifically, its coverage error bound is of second-order mixed bias type, that is doubly robust, and therefore guaranteed to be asymptotically negligible relative to the coverage error of the non-augmented method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04615v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rebecca Farina, Eric J. Tchetgen Tchetgen, Arun Kumar Kuchibhotla</dc:creator>
    </item>
    <item>
      <title>Distributional Instrumental Variable Method</title>
      <link>https://arxiv.org/abs/2502.07641</link>
      <description>arXiv:2502.07641v3 Announce Type: replace 
Abstract: The instrumental variable (IV) approach is commonly used to infer causal effects in the presence of unmeasured confounding. Existing methods typically aim to estimate the mean causal effects, whereas a few other methods focus on quantile treatment effects. The aim of this work is to estimate the entire interventional distribution. We propose a method called Distributional Instrumental Variable (DIV), which uses generative modelling in a nonlinear IV setting. We establish identifiability of the interventional distribution under general assumptions and demonstrate an 'under-identified' case, where DIV can identify the causal effects while two-step least squares fails to. Our empirical results show that the DIV method performs well for a broad range of simulated data, exhibiting advantages over existing IV approaches in terms of the identifiability and estimation error of the mean or quantile treatment effects. Furthermore, we apply DIV to an economic data set to examine the causal relation between institutional quality and economic development and our results align well with the original study. We also apply DIV to a single-cell data set, where we study the generalizability and stability in predicting gene expression under unseen interventions. The software implementations of DIV are available in R and Python.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07641v3</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anastasiia Holovchak, Sorawit Saengkyongam, Nicolai Meinshausen, Xinwei Shen</dc:creator>
    </item>
    <item>
      <title>Model-based bi-clustering using multivariate Poisson-lognormal with general block-diagonal covariance matrix and its applications</title>
      <link>https://arxiv.org/abs/2503.05961</link>
      <description>arXiv:2503.05961v2 Announce Type: replace 
Abstract: While several Gaussian mixture models-based biclustering approaches currently exist in the literature for continuous data, approaches to handle discrete data have not been well researched. A multivariate Poisson-lognormal (MPLN) model-based bi-clustering approach that utilizes a block-diagonal covariance structure is introduced to allow for a more flexible structure of the covariance matrix. Two variations of the algorithm are developed where the number of column clusters: 1) are assumed equal across groups or 2) can vary across groups. Variational Gaussian approximation is utilized for parameter estimation, and information criteria are used for model selection. The proposed models are investigated in the context of clustering multivariate count data. Using simulated data the models display strong accuracy and computational efficiency and is applied to breast cancer RNA-sequence data from The Cancer Genome Atlas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05961v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Caitlin Kral, Evan Chance, Ryan Browne, Sanjeena Subedi</dc:creator>
    </item>
    <item>
      <title>The Logic of Counterfactuals and the Epistemology of Causal Inference</title>
      <link>https://arxiv.org/abs/2405.11284</link>
      <description>arXiv:2405.11284v3 Announce Type: replace-cross 
Abstract: The 2021 Nobel Prize in Economics recognized an epistemology of causal inference based on the Rubin causal model (Rubin 1974), which merits broader attention in philosophy. This model, in fact, presupposes a logical principle of counterfactuals, Conditional Excluded Middle (CEM), the locus of a pivotal debate between Stalnaker (1968) and Lewis (1973) on the semantics of counterfactuals. Proponents of CEM should recognize that this connection points to a new argument for CEM -- a Quine-Putnam indispensability argument grounded in the Nobel-winning applications of the Rubin model in health and social sciences. To advance the dialectic, I challenge this argument with an updated Rubin causal model that retains its successes while dispensing with CEM. This novel approach combines the strengths of the Rubin causal model and a causal model familiar in philosophy, the causal Bayes net. The takeaway: deductive logic and inductive inference, often studied in isolation, are deeply interconnected.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11284v3</guid>
      <category>cs.AI</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hanti Lin</dc:creator>
    </item>
  </channel>
</rss>
