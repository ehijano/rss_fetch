<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PF updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PF</link>
    <description>cs.PF updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PF" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 23 Oct 2025 04:01:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Tidying Up the Address Space</title>
      <link>https://arxiv.org/abs/2510.19765</link>
      <description>arXiv:2510.19765v1 Announce Type: cross 
Abstract: Memory tiering in datacenters does not achieve its full potential due to hotness fragmentation -- the intermingling of hot and cold objects within memory pages. This fragmentation prevents page-based reclamation systems from distinguishing truly hot pages from pages containing mostly cold objects, fundamentally limiting memory efficiency despite highly skewed accesses. We introduce address-space engineering: dynamically reorganizing application virtual address spaces to create uniformly hot and cold regions that any page-level tiering backend can manage effectively. HADES demonstrates this frontend/backend approach through a compiler-runtime system that tracks and migrates objects based on access patterns, requiring minimal developer intervention. Evaluations across ten data structures achieve up to 70% memory reduction with 3% performance overhead, showing that address space engineering enables existing reclamation systems to reclaim memory aggressively without performance degradation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19765v1</guid>
      <category>cs.OS</category>
      <category>cs.PF</category>
      <category>cs.PL</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3764862.3768179</arxiv:DOI>
      <dc:creator>Vinay Banakar, Suli Yang, Kan Wu, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau, Kimberly Keeton</dc:creator>
    </item>
    <item>
      <title>On the Power Saving in High-Speed Ethernet-based Networks for Supercomputers and Data Centers</title>
      <link>https://arxiv.org/abs/2510.19783</link>
      <description>arXiv:2510.19783v1 Announce Type: cross 
Abstract: The increase in computation and storage has led to a significant growth in the scale of systems powering applications and services, raising concerns about sustainability and operational costs. In this paper, we explore power-saving techniques in high-performance computing (HPC) and datacenter networks, and their relation with performance degradation. From this premise, we propose leveraging Energy Efficient Ethernet (EEE), with the flexibility to extend to conventional Ethernet or upcoming Ethernet-derived interconnect versions of BXI and Omnipath.
  We analyze the PerfBound proposal, identifying possible improvements and modeling it into a simulation framework. Through different experiments, we examine its impact on performance and determine the most appropriate interconnect. We also study traffic patterns generated by selected HPC and machine learning applications to evaluate the behavior of power-saving techniques.
  From these experiments, we provide an analysis of how applications affect system and network energy consumption. Based on this, we disclose the weakness of dynamic power-down mechanisms and propose an approach that improves energy reduction with minimal or no performance penalty. To our knowledge, this is the first power management proposal tailored to future Ethernet-based HPC architectures, with promising results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19783v1</guid>
      <category>cs.NI</category>
      <category>cs.PF</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Miguel S\'anchez de la Rosa, Francisco J. and\'ujar, Jesus Escudero-Sahuquillo, Jos\'e L. S\'anchez, Francisco J. Alfaro-Cort\'es</dc:creator>
    </item>
    <item>
      <title>Efficient and scalable atmospheric dynamics simulations using non-conforming meshes</title>
      <link>https://arxiv.org/abs/2408.08129</link>
      <description>arXiv:2408.08129v3 Announce Type: replace-cross 
Abstract: We present the massively parallel performance of a $h$-adaptive solver for atmosphere dynamics that allows for non-conforming mesh refinement. The numerical method is based on a Discontinuous Galerkin (DG) spatial discretization, highly scalable thanks to its data locality properties, and on a second order Implicit-Explicit Runge-Kutta (IMEX-RK) method for time discretization, particularly well suited for low Mach number flows. Simulations with non-conforming meshes for flows over orography can increase the accuracy of the local flow description without affecting the larger scales, which can be solved on coarser meshes. We show that the local refining procedure has no significant impact on the parallel performance and, therefore, both efficiency and scalability can be achieved in this framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08129v3</guid>
      <category>math.NA</category>
      <category>cs.DC</category>
      <category>cs.NA</category>
      <category>cs.PF</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.procs.2025.02.258</arxiv:DOI>
      <arxiv:journal_reference>Procedia Computer Science 255 (2025): 33-42</arxiv:journal_reference>
      <dc:creator>Giuseppe Orlando, Tommaso Benacchio, Luca Bonaventura</dc:creator>
    </item>
    <item>
      <title>Optimizing Asynchronous Federated Learning: A Delicate Trade-Off Between Model-Parameter Staleness and Update Frequency</title>
      <link>https://arxiv.org/abs/2502.08206</link>
      <description>arXiv:2502.08206v4 Announce Type: replace-cross 
Abstract: Synchronous federated learning (FL) scales poorly with the number of clients due to the straggler effect. Algorithms like FedAsync and GeneralizedFedAsync address this limitation by enabling asynchronous communication between clients and the central server. In this work, we rely on stochastic modeling and analysis to better understand the impact of design choices in asynchronous FL algorithms, such as the concurrency level and routing probabilities, and we leverage this knowledge to optimize loss. Compared to most existing studies, we account for the joint impact of heterogeneous and variable service speeds and heterogeneous datasets at the clients. We characterize in particular a fundamental trade-off for optimizing asynchronous FL: minimizing gradient estimation errors by avoiding model parameter staleness, while also speeding up the system by increasing the throughput of model updates. Our two main contributions can be summarized as follows. First, we prove a discrete variant of Little's law to derive a closed-form expression for relative delay, a metric that quantifies staleness. This allows us to efficiently minimize the average loss per model update, which has been the gold standard in literature to date, using the upper-bound of Leconte et al. as a proxy. Second, we observe that naively optimizing this metric drastically slows down the system by overemphasizing staleness at the expense of throughput. This motivates us to introduce an alternative metric that also accounts for speed, for which we derive a tractable upper-bound that can be minimized numerically. Extensive numerical results show these optimizations enhance accuracy by 10% to 30%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08206v4</guid>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>European Conference on Artificial Intelligence (ECAI), Oct 2025, Bologna, Italy</arxiv:journal_reference>
      <dc:creator>Abdelkrim Alahyane (LAAS-SARA, LAAS), C\'eline Comte (CNRS, LAAS-SARA, LAAS), Matthieu Jonckheere (CNRS, LAAS-SARA, LAAS), \'Eric Moulines (X)</dc:creator>
    </item>
    <item>
      <title>Linear Layouts: Robust Code Generation of Efficient Tensor Computation Using $\mathbb{F}_2$</title>
      <link>https://arxiv.org/abs/2505.23819</link>
      <description>arXiv:2505.23819v3 Announce Type: replace-cross 
Abstract: Efficient tensor computation is a cornerstone of modern deep learning (DL) workloads, yet existing approaches struggle to achieve flexible and performant design and implementation of tensor layouts -- mappings between logical tensors and hardware resources. The increasing complexity of DL algorithms and hardware demands a generic and systematic approach to handling tensor layouts. In this work, we introduce Linear Layouts, a novel approach that models tensor layouts using linear algebra over $\mathbb{F}_2$. By representing tensor layouts as binary matrices acting on the bits of the hardware representation, our approach enables a generic layout definition -- as opposed to the classical case-by-case approach -- and allows for generic layout-to-layout conversions, eliminating the quadratic explosion that plagues existing solutions. We integrate linear layouts with Triton and demonstrate their effectiveness in optimizing individual Triton operators as well as kernels written in Triton. We also show that linear layouts reduce engineering effort in the compiler backend while fixing several bugs in Triton's legacy layout system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23819v3</guid>
      <category>cs.PL</category>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keren Zhou, Mario Lezcano, Adam Goucher, Akhmed Rakhmati, Jeff Niu, Justin Lebar, Pawel Szczerbuk, Peter Bell, Phil Tillet, Thomas Raoux, Zahi Moudallal</dc:creator>
    </item>
  </channel>
</rss>
