<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GR</link>
    <description>cs.GR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 10 Jan 2025 05:00:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Exact computation of the color function for triangular element interfaces</title>
      <link>https://arxiv.org/abs/2501.04744</link>
      <description>arXiv:2501.04744v1 Announce Type: new 
Abstract: The calculation of the volume enclosed by curved surfaces discretized into triangular elements, and a cube is of great importance in different domains, such as computer graphics and multiphase flow simulations. We propose a robust algorithm, the Front2VOF (F2V) algorithm, to address this problem. The F2V algorithm consists of two main steps. First, it identifies the polygons within the cube by segmenting the triangular elements on the surface, retaining only the portions inside the cube boundaries. Second, it computes the volume enclosed by these polygons in combination with the cube faces. To validate the algorithm's accuracy and robustness, we tested it using a range of synthetic configurations with known analytical solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04744v1</guid>
      <category>cs.GR</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jieyun Pan, D\'esir-Andr\'e Koffi Bi, Ahmed Basil Kottilingal, Serena Costanzo, Jiacai Lu, Yue Ling, Ruben Scardovelli, Gr\'etar Tryggvason, St\'ephane Zaleski</dc:creator>
    </item>
    <item>
      <title>A Scalable System for Visual Analysis of Ocean Data</title>
      <link>https://arxiv.org/abs/2501.05009</link>
      <description>arXiv:2501.05009v1 Announce Type: new 
Abstract: Oceanographers rely on visual analysis to interpret model simulations, identify events and phenomena, and track dynamic ocean processes. The ever increasing resolution and complexity of ocean data due to its dynamic nature and multivariate relationships demands a scalable and adaptable visualization tool for interactive exploration. We introduce pyParaOcean, a scalable and interactive visualization system designed specifically for ocean data analysis. pyParaOcean offers specialized modules for common oceanographic analysis tasks, including eddy identification and salinity movement tracking. These modules seamlessly integrate with ParaView as filters, ensuring a user-friendly and easy-to-use system while leveraging the parallelization capabilities of ParaView and a plethora of inbuilt general-purpose visualization functionalities. The creation of an auxiliary dataset stored as a Cinema database helps address I/O and network bandwidth bottlenecks while supporting the generation of quick overview visualizations. We present a case study on the Bay of Bengal (BoB) to demonstrate the utility of the system and scaling studies to evaluate the efficiency of the system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05009v1</guid>
      <category>cs.GR</category>
      <category>cs.CV</category>
      <category>cs.DC</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1111/cgf.15279</arxiv:DOI>
      <arxiv:journal_reference>Computer Graphics Forum, 2025</arxiv:journal_reference>
      <dc:creator>Toshit Jain, Upkar Singh, Varun Singh, Vijay Kumar Boda, Ingrid Hotz, Sathish S. Vadhiyar, P. N. Vinayachandran, Vijay Natarajan</dc:creator>
    </item>
    <item>
      <title>Time-Variant Vector Field Visualization for Magnetic Fields of Neutron Star Simulations</title>
      <link>https://arxiv.org/abs/2501.05084</link>
      <description>arXiv:2501.05084v1 Announce Type: cross 
Abstract: We present a novel visualization application designed to explore the time-dependent development of magnetic fields of neutron stars. The strongest magnetic fields in the universe can be found within neutron stars, potentially playing a role in initiating astrophysical jets and facilitating the outflow of neutron-rich matter, ultimately resulting in the production of heavy elements during binary neutron star mergers. Since such effects may be dependent on the strength and configuration of the magnetic field, the formation and parameters of such fields are part of current research in astrophysics. Magnetic fields are investigated using simulations in which various initial configurations are tested. However, the long-term configuration is an open question, and current simulations do not achieve a stable magnetic field. Neutron star simulations produce data quantities in the range of several terabytes, which are both spatially in 3D and temporally resolved. Our tool enables physicists to interactively explore the generated data. We first convert the data in a pre-processing step and then we combine sparse vector field visualization using streamlines with dense vector field visualization using line integral convolution. We provide several methods to interact with the data responsively. This allows the user to intuitively investigate data-specific issues. Furthermore, diverse visualization techniques facilitate individual exploration of the data and enable real-time processing of specific domain tasks, like the investigation of the time-dependent evolution of the magnetic field. In a qualitative study, domain experts tested the tool, and the usability was queried. Experts rated the tool very positively and recommended it for their daily work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05084v1</guid>
      <category>astro-ph.HE</category>
      <category>cs.GR</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon J. Lieb, William Cook, Jan Hombeck, Sebastiano Bernuzzi, Kai Lawonn</dc:creator>
    </item>
    <item>
      <title>CMTNet: Convolutional Meets Transformer Network for Hyperspectral Images Classification</title>
      <link>https://arxiv.org/abs/2406.14080</link>
      <description>arXiv:2406.14080v3 Announce Type: replace-cross 
Abstract: Hyperspectral remote sensing (HIS) enables the detailed capture of spectral information from the Earth's surface, facilitating precise classification and identification of surface crops due to its superior spectral diagnostic capabilities. However, current convolutional neural networks (CNNs) focus on local features in hyperspectral data, leading to suboptimal performance when classifying intricate crop types and addressing imbalanced sample distributions. In contrast, the Transformer framework excels at extracting global features from hyperspectral imagery. To leverage the strengths of both approaches, this research introduces the Convolutional Meet Transformer Network (CMTNet). This innovative model includes a spectral-spatial feature extraction module for shallow feature capture, a dual-branch structure combining CNN and Transformer branches for local and global feature extraction, and a multi-output constraint module that enhances classification accuracy through multi-output loss calculations and cross constraints across local, international, and joint features. Extensive experiments conducted on three datasets (WHU-Hi-LongKou, WHU-Hi-HanChuan, and WHU-Hi-HongHu) demonstrate that CTDBNet significantly outperforms other state-of-the-art networks in classification performance, validating its effectiveness in hyperspectral crop classification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14080v3</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Faxu Guo, Quan Feng, Sen Yang, Wanxia Yang</dc:creator>
    </item>
    <item>
      <title>McGrids: Monte Carlo-Driven Adaptive Grids for Iso-Surface Extraction</title>
      <link>https://arxiv.org/abs/2409.06710</link>
      <description>arXiv:2409.06710v2 Announce Type: replace-cross 
Abstract: Iso-surface extraction from an implicit field is a fundamental process in various applications of computer vision and graphics. When dealing with geometric shapes with complicated geometric details, many existing algorithms suffer from high computational costs and memory usage. This paper proposes McGrids, a novel approach to improve the efficiency of iso-surface extraction. The key idea is to construct adaptive grids for iso-surface extraction rather than using a simple uniform grid as prior art does. Specifically, we formulate the problem of constructing adaptive grids as a probability sampling problem, which is then solved by Monte Carlo process. We demonstrate McGrids' capability with extensive experiments from both analytical SDFs computed from surface meshes and learned implicit fields from real multiview images. The experiment results show that our McGrids can significantly reduce the number of implicit field queries, resulting in significant memory reduction, while producing high-quality meshes with rich geometric details.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06710v2</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-72998-0_8</arxiv:DOI>
      <dc:creator>Daxuan Ren, Hezi Shi, Jianmin Zheng, Jianfei Cai</dc:creator>
    </item>
    <item>
      <title>STITCH: Surface reconstrucTion using Implicit neural representations with Topology Constraints and persistent Homology</title>
      <link>https://arxiv.org/abs/2412.18696</link>
      <description>arXiv:2412.18696v2 Announce Type: replace-cross 
Abstract: We present STITCH, a novel approach for neural implicit surface reconstruction of a sparse and irregularly spaced point cloud while enforcing topological constraints (such as having a single connected component). We develop a new differentiable framework based on persistent homology to formulate topological loss terms that enforce the prior of a single 2-manifold object. Our method demonstrates excellent performance in preserving the topology of complex 3D geometries, evident through both visual and empirical comparisons. We supplement this with a theoretical analysis, and provably show that optimizing the loss with stochastic (sub)gradient descent leads to convergence and enables reconstructing shapes with a single connected component. Our approach showcases the integration of differentiable topological data analysis tools for implicit surface reconstruction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18696v2</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <category>cs.LG</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Anushrut Jignasu, Ethan Herron, Zhanhong Jiang, Soumik Sarkar, Chinmay Hegde, Baskar Ganapathysubramanian, Aditya Balu, Adarsh Krishnamurthy</dc:creator>
    </item>
    <item>
      <title>Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video Generation Control</title>
      <link>https://arxiv.org/abs/2501.03847</link>
      <description>arXiv:2501.03847v2 Announce Type: replace-cross 
Abstract: Diffusion models have demonstrated impressive performance in generating high-quality videos from text prompts or images. However, precise control over the video generation process, such as camera manipulation or content editing, remains a significant challenge. Existing methods for controlled video generation are typically limited to a single control type, lacking the flexibility to handle diverse control demands. In this paper, we introduce Diffusion as Shader (DaS), a novel approach that supports multiple video control tasks within a unified architecture. Our key insight is that achieving versatile video control necessitates leveraging 3D control signals, as videos are fundamentally 2D renderings of dynamic 3D content. Unlike prior methods limited to 2D control signals, DaS leverages 3D tracking videos as control inputs, making the video diffusion process inherently 3D-aware. This innovation allows DaS to achieve a wide range of video controls by simply manipulating the 3D tracking videos. A further advantage of using 3D tracking videos is their ability to effectively link frames, significantly enhancing the temporal consistency of the generated videos. With just 3 days of fine-tuning on 8 H800 GPUs using less than 10k videos, DaS demonstrates strong control capabilities across diverse tasks, including mesh-to-video generation, camera control, motion transfer, and object manipulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03847v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.GR</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Zekai Gu, Rui Yan, Jiahao Lu, Peng Li, Zhiyang Dou, Chenyang Si, Zhen Dong, Qifeng Liu, Cheng Lin, Ziwei Liu, Wenping Wang, Yuan Liu</dc:creator>
    </item>
  </channel>
</rss>
