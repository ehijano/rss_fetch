<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 06 Feb 2025 02:49:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A New Rejection Sampling Approach to $k$-$\mathtt{means}$++ With Improved Trade-Offs</title>
      <link>https://arxiv.org/abs/2502.02085</link>
      <description>arXiv:2502.02085v1 Announce Type: new 
Abstract: The $k$-$\mathtt{means}$++ seeding algorithm (Arthur &amp; Vassilvitskii, 2007) is widely used in practice for the $k$-means clustering problem where the goal is to cluster a dataset $\mathcal{X} \subset \mathbb{R} ^d$ into $k$ clusters.
  The popularity of this algorithm is due to its simplicity and provable guarantee of being $O(\log k)$ competitive with the optimal solution in expectation. However, its running time is $O(|\mathcal{X}|kd)$, making it expensive for large datasets.
  In this work, we present a simple and effective rejection sampling based approach for speeding up $k$-$\mathtt{means}$++.
  Our first method runs in time $\tilde{O}(\mathtt{nnz} (\mathcal{X}) + \beta k^2d)$ while still being $O(\log k )$ competitive in expectation. Here, $\beta$ is a parameter which is the ratio of the variance of the dataset to the optimal $k$-$\mathtt{means}$ cost in expectation and $\tilde{O}$ hides logarithmic factors in $k$ and $|\mathcal{X}|$.
  Our second method presents a new trade-off between computational cost and solution quality. It incurs an additional scale-invariant factor of $ k^{-\Omega( m/\beta)} \operatorname{Var} (\mathcal{X})$ in addition to the $O(\log k)$ guarantee of $k$-$\mathtt{means}$++ improving upon a result of (Bachem et al, 2016a) who get an additional factor of $m^{-1}\operatorname{Var}(\mathcal{X})$ while still running in time $\tilde{O}(\mathtt{nnz}(\mathcal{X}) + mk^2d)$. We perform extensive empirical evaluations to validate our theoretical results and to show the effectiveness of our approach on real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02085v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Poojan Shah, Shashwat Agrawal, Ragesh Jaiswal</dc:creator>
    </item>
    <item>
      <title>Efficient and Practical Approximation Algorithms for Advertising in Content Feeds</title>
      <link>https://arxiv.org/abs/2502.02115</link>
      <description>arXiv:2502.02115v1 Announce Type: new 
Abstract: Content feeds provided by platforms such as X (formerly Twitter) and TikTok are consumed by users on a daily basis. In this paper, we revisit the native advertising problem in content feeds, initiated by Ieong et al. Given a sequence of organic items (e.g., videos or posts) relevant to a user's interests or to an information search, the goal is to place ads within the organic content so as to maximize a reward function (e.g., number of clicks), while accounting for two considerations: (1) an ad can only be inserted after a relevant content item; (2) the users' attention decays after consuming content or ads. These considerations provide a natural model for capturing both the advertisement effectiveness and the user experience. In this paper, we design fast and practical 2-approximation greedy algorithms for the associated optimization problem, improving over the best-known practical algorithm that only achieves an approximation factor of~4. Our algorithms exploit a counter-intuitive observation, namely, while top items are seemingly more important due to the decaying attention of the user, taking good care of the bottom items is key for obtaining improved approximation guarantees. We then provide the first comprehensive empirical evaluation on the problem, showing the strong empirical performance of our~methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02115v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guangyi Zhang, Ilie Sarpe, Aristides Gionis</dc:creator>
    </item>
    <item>
      <title>Extending the Applicability of Bloom Filters by Relaxing their Parameter Constraints</title>
      <link>https://arxiv.org/abs/2502.02193</link>
      <description>arXiv:2502.02193v1 Announce Type: new 
Abstract: These days, Key-Value Stores are widely used for scalable data storage. In this environment, Bloom filters serve as an efficient probabilistic data structure for the representation of sets of keys as they allow for set membership queries with controllable false positive rates and no false negatives. For optimal error rates, the right choice of the main parameters, namely the length of the Bloom filter array, the number of hash functions used to map an element to the array's indices, and the number of elements to be inserted in one filter, is crucial. However, these parameters are constrained: The number of hash functions is bounded to integer values, and the length of a Bloom filter is usually chosen to be a power-of-two to allow for efficient modulo operations using binary arithmetics. These modulo calculations are necessary to map from the output universe of the applied universal hash functions, like Murmur, to the set of indices of the Bloom filter. In this paper, we relax these constraints by proposing the Rational Bloom filter, which allows for non-integer numbers of hash functions. This results in optimized fraction-of-zero values for a known number of elements to be inserted. Based on this, we construct the Variably-Sized Block Bloom filters to allow for a flexible filter length, especially for large filters, while keeping computation efficient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02193v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul Walther, Wejdene Mansour, Martin Werner</dc:creator>
    </item>
    <item>
      <title>A note on Ordered Ruzsa-Szemer\'edi graphs</title>
      <link>https://arxiv.org/abs/2502.02455</link>
      <description>arXiv:2502.02455v1 Announce Type: new 
Abstract: A recent breakthrough of Behnezhad and Ghafari [FOCS 2024] and subsequent work of Assadi, Khanna, and Kiss [SODA 2025] gave algorithms for the fully dynamic $(1-\varepsilon)$-approximate maximum matching problem whose runtimes are determined by a purely combinatorial quantity: the maximum density of Ordered Ruzsa-Szemer\'edi (ORS) graphs. We say a graph $G$ is an $(r,t)$-ORS graph if its edges can be partitioned into $t$ matchings $M_1,M_2, \ldots, M_t$ each of size $r$, such that for every $i$, $M_i$ is an induced matching in the subgraph $M_{i} \cup M_{i+1} \cup \cdots \cup M_t$. This is a relaxation of the extensively-studied notion of a Ruzsa-Szemer\'edi (RS) graph, the difference being that in an RS graph each $M_i$ must be an induced matching in $G$.
  In this note, we show that these two notions are roughly equivalent. Specifically, let $\mathrm{ORS}(n)$ be the largest $t$ such that there exists an $n$-vertex ORS-$(\Omega(n), t)$ graph, and define $\mathrm{RS}(n)$ analogously. We show that if $\mathrm{ORS}(n) \ge \Omega(n^c)$, then for any fixed $\delta &gt; 0$, $\mathrm{RS}(n) \ge \Omega(n^{c(1-\delta)})$. This resolves a question of Behnezhad and Ghafari.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02455v1</guid>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kevin Pratt</dc:creator>
    </item>
    <item>
      <title>A Clique Partitioning-Based Algorithm for Graph Compression</title>
      <link>https://arxiv.org/abs/2502.02477</link>
      <description>arXiv:2502.02477v1 Announce Type: new 
Abstract: Reducing the running time of graph algorithms is vital for tackling real-world problems such as shortest paths and matching in large-scale graphs, where path information plays a crucial role. This paper addresses this critical challenge of reducing the running time of graph algorithms by proposing a new graph compression algorithm that partitions the graph into bipartite cliques and uses the partition to obtain a compressed graph having a smaller number of edges while preserving the path information. This compressed graph can then be used as input to other graph algorithms for which path information is essential, leading to a significant reduction of their running time, especially for large, dense graphs. The running time of the proposed algorithm is~$O(mn^\delta)$, where $0 \leq \delta \leq 1$, which is better than $O(mn^\delta \log^2 n)$, the running time of the best existing clique partitioning-based graph compression algorithm (the Feder-Motwani (\textsf{FM}) algorithm). Our extensive experimental analysis show that our algorithm achieves a compression ratio of up to~$26\%$ greater and executes up to~105.18 times faster than the \textsf{FM} algorithm. In addition, on large graphs with up to 1.05 billion edges, it achieves a compression ratio of up to~3.9, reducing the number of edges up to~$74.36\%$. Finally, our tests with a matching algorithm on sufficiently large, dense graphs, demonstrate a reduction in the running time of up to 72.83\% when the input is the compressed graph obtained by our algorithm, compared to the case where the input is the original uncompressed graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02477v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akshar Chavan, Sanaz Rabina, Daniel Grosu, Marco Brocanelli</dc:creator>
    </item>
    <item>
      <title>Max-Min Diversification with Asymmetric Distances</title>
      <link>https://arxiv.org/abs/2502.02530</link>
      <description>arXiv:2502.02530v1 Announce Type: new 
Abstract: One of the most well-known and simplest models for diversity maximization is the Max-Min Diversification (MMD) model, which has been extensively studied in the data mining and database literature. In this paper, we initiate the study of the Asymmetric Max-Min Diversification (AMMD) problem. The input is a positive integer $k$ and a complete digraph over $n$ vertices, together with a nonnegative distance function over the edges obeying the directed triangle inequality. The objective is to select a set of $k$ vertices, which maximizes the smallest pairwise distance between them. AMMD reduces to the well-studied MMD problem in case the distances are symmetric, and has natural applications to query result diversification, web search, and facility location problems. Although the MMD problem admits a simple $\frac{1}{2}$-approximation by greedily selecting the next-furthest point, this strategy fails for AMMD and it remained unclear how to design good approximation algorithms for AMMD. We propose a combinatorial $\frac{1}{6k}$-approximation algorithm for AMMD by leveraging connections with the Maximum Antichain problem. We discuss several ways of speeding up the algorithm and compare its performance against heuristic baselines on real-life and synthetic datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02530v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3637528.3671757</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. (2024) 1440-1450</arxiv:journal_reference>
      <dc:creator>Iiro Kumpulainen, Florian Adriaens, Nikolaj Tatti</dc:creator>
    </item>
    <item>
      <title>Algorithms and Hardness Results for the $(k,\ell)$-Cover Problem</title>
      <link>https://arxiv.org/abs/2502.02572</link>
      <description>arXiv:2502.02572v1 Announce Type: new 
Abstract: A connected graph has a $(k,\ell)$-cover if each of its edges is contained in at least $\ell$ cliques of order $k$. Motivated by recent advances in extremal combinatorics and the literature on edge modification problems, we study the algorithmic version of the $(k,\ell)$-cover problem. Given a connected graph $G$, the $(k, \ell)$-cover problem is to identify the smallest subset of non-edges of $G$ such that their addition to $G$ results in a graph with a $(k, \ell)$-cover. For every constant $k\geq3$, we show that the $(k,1)$-cover problem is $\mathbb{NP}$-complete for general graphs. Moreover, we show that for every constant $k\geq 3$, the $(k,1)$-cover problem admits no polynomial-time constant-factor approximation algorithm unless $\mathbb{P}=\mathbb{NP}$. However, we show that the $(3,1)$-cover problem can be solved in polynomial time when the input graph is chordal. For the class of trees and general values of $k$, we show that the $(k,1)$-cover problem is $\mathbb{NP}$-hard even for spiders. However, we show that for every $k\geq4$, the $(3,k-2)$-cover and the $(k,1)$-cover problems are constant-factor approximable when the input graph is a tree.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02572v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amirali Madani, Anil Maheshwari, Babak Miraftab, Bodhayan Roy</dc:creator>
    </item>
    <item>
      <title>Posted Price Mechanisms for Online Allocation with Diseconomies of Scale</title>
      <link>https://arxiv.org/abs/2502.02543</link>
      <description>arXiv:2502.02543v1 Announce Type: cross 
Abstract: This paper addresses the online $k$-selection problem with diseconomies of scale (OSDoS), where a seller seeks to maximize social welfare by optimally pricing items for sequentially arriving buyers, accounting for increasing marginal production costs. Previous studies have investigated deterministic dynamic pricing mechanisms for such settings. However, significant challenges remain, particularly in achieving optimality with small or finite inventories and developing effective randomized posted price mechanisms. To bridge this gap, we propose a novel randomized dynamic pricing mechanism for OSDoS, providing a tighter lower bound on the competitive ratio compared to prior work. Our approach ensures optimal performance in small inventory settings (i.e., when $k$ is small) and surpasses existing online mechanisms in large inventory settings (i.e., when $k$ is large), leading to the best-known posted price mechanism for optimizing online selection and allocation with diseconomies of scale across varying inventory sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02543v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hossein Nekouyan Jazi, Bo Sun, Raouf Boutaba, Xiaoqi Tan</dc:creator>
    </item>
    <item>
      <title>Complexity Classes for Online Problems with and without Predictions</title>
      <link>https://arxiv.org/abs/2406.18265</link>
      <description>arXiv:2406.18265v2 Announce Type: replace 
Abstract: With the developments in machine learning, there has been a surge in interest and results focused on algorithms utilizing predictions, not least in online algorithms where most new results incorporate the prediction aspect for concrete online problems. While the structural computational hardness of problems with regards to time and space is quite well developed, not much is known about online problems where time and space resources are typically not in focus. Some information-theoretical insights were gained when researchers considered online algorithms with oracle advice, but predictions of uncertain quality is a very different matter.
  We initiate the development of a complexity theory for online problems with predictions, focusing on binary predictions for minimization problems. Based on the most generic hard online problem type, string guessing, we define a family of hierarchies of complexity classes (indexed by pairs of error measures) and develop notions of reductions, class membership, hardness, and completeness. Our framework contains all the tools one expects to find when working with complexity, and we illustrate our tools by analyzing problems with different characteristics. In addition, we show that known lower bounds for paging with predictions apply directly to all hard problems for each class in the hierarchy based on the canonical pair of error measures.
  Our work also implies corresponding complexity classes for classic online problems without predictions, with the corresponding complete problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18265v2</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Magnus Berg, Joan Boyar, Lene M. Favrholdt, Kim S. Larsen</dc:creator>
    </item>
    <item>
      <title>Improved fixed-parameter bounds for Min-Sum-Radii and Diameters $k$-clustering and their fair variants</title>
      <link>https://arxiv.org/abs/2501.17708</link>
      <description>arXiv:2501.17708v2 Announce Type: replace 
Abstract: We provide improved upper and lower bounds for the Min-Sum-Radii (MSR) and Min-Sum-Diameters (MSD) clustering problems with a bounded number of clusters $k$. In particular, we propose an exact MSD algorithm with running-time $n^{O(k)}$. We also provide $(1+\epsilon)$ approximation algorithms for both MSR and MSD with running-times of $O(kn) +(1/\epsilon)^{O(dk)}$ in metrics spaces of doubling dimension $d$. Our algorithms extend to $k$-center, improving upon previous results, and to $\alpha$-MSR, where radii are raised to the $\alpha$ power for $\alpha&gt;1$. For $\alpha$-MSD we prove an exponential time ETH-based lower bound for $\alpha&gt;\log 3$. All algorithms can also be modified to handle outliers. Moreover, we can extend the results to variants that observe fairness constraints, as well as to the general framework of mergeable clustering, which includes many other popular clustering variants. We complement these upper bounds with ETH-based lower bounds for these problems, in particular proving that $n^{O(k)}$ time is tight for MSR and $\alpha$-MSR even in doubling spaces, and that $2^{o(k)}$ bounds are impossible for MSD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17708v2</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sandip Banerjee, Yair Bartal, Lee-Ad Gottlieb, Alon Hovav</dc:creator>
    </item>
    <item>
      <title>Minimum Riesz s-Energy Subset Selection in Ordered Point Sets via Dynamic Programming</title>
      <link>https://arxiv.org/abs/2502.01163</link>
      <description>arXiv:2502.01163v2 Announce Type: replace 
Abstract: We present a dynamic programming algorithm for selecting a representative subset of size $k$ from a given set with $n$ points such that the Riesz s-Energy is near minimized. While NP-hard in general dimensions, the one-dimensional case can use the natural data ordering for efficient dynamic programming as an effective heuristic solution approach. This approach is then extended to problems related to two-dimensional Pareto front representations arising in biobjective optimization problems. Under the assumption of sorted (or non-dominated) input, the method typically yields near-optimal solutions in most cases. We also show that the approach avoids mistakes of greedy subset-selection by means of example. However, as we demonstrate, there are exceptions where DP does not identify the global minimum; for example, in one of our examples, the DP solution slightly deviates from the configuration found by a brute-force search.This is because the DP scheme's recurrence is approximate. The total time complexity of our algorithm is shown to be $O(n^2 k)$. Alongside the theory, we offer computational examples and an open-source Python implementation of the algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01163v2</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Emmerich</dc:creator>
    </item>
    <item>
      <title>A subquadratic certification scheme for P5-free graphs</title>
      <link>https://arxiv.org/abs/2410.14658</link>
      <description>arXiv:2410.14658v2 Announce Type: replace-cross 
Abstract: In local certification, vertices of a $n$-vertex graph perform a local verification to check if a given property is satisfied by the graph. This verification is performed thanks to certificates, which are pieces of information that are given to the vertices. In this work, we focus on the local certification of $P_5$-freeness, and we prove a $O(n^{3/2})$ upper bound on the size of the certificates, which is (to our knowledge) the first subquadratic upper bound for this property.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14658v2</guid>
      <category>cs.DC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Bousquet, S\'ebastien Zeitoun</dc:creator>
    </item>
    <item>
      <title>Classical Algorithms for Constant Approximation of the Ground State Energy of Local Hamiltonians</title>
      <link>https://arxiv.org/abs/2410.21833</link>
      <description>arXiv:2410.21833v2 Announce Type: replace-cross 
Abstract: We construct classical algorithms computing an approximation of the ground state energy of an arbitrary $k$-local Hamiltonian acting on $n$ qubits.
  We first consider the setting where a good ``guiding state'' is available, which is the main setting where quantum algorithms are expected to achieve an exponential speedup over classical methods. We show that a constant approximation (i.e., an approximation with constant relative accuracy) of the ground state energy can be computed classically in $\mathrm{poly}\left(1/\chi,n\right)$ time and $\mathrm{poly}(n)$ space, where $\chi$ denotes the overlap between the guiding state and the ground state (as in prior works in dequantization, we assume sample-and-query access to the guiding state). This gives a significant improvement over the recent classical algorithm by Gharibian and Le Gall (SICOMP 2023), and matches (up a to polynomial overhead) both the time and space complexities of quantum algorithms for constant approximation of the ground state energy. We also obtain classical algorithms for higher-precision approximation.
  For the setting where no guided state is given (i.e., the standard version of the local Hamiltonian problem), we obtain a classical algorithm computing a constant approximation of the ground state energy in $2^{O(n)}$ time and $\mathrm{poly}(n)$ space. To our knowledge, before this work it was unknown how to classically achieve these bounds simultaneously, even for constant approximation. We also discuss complexity-theoretic aspects of our results and their implications for the quantum PCP conjecture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21833v2</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fran\c{c}ois Le Gall</dc:creator>
    </item>
  </channel>
</rss>
