<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 19 Feb 2026 05:00:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Evidence for Daily and Weekly Periodic Variability in GPT-4o Performance</title>
      <link>https://arxiv.org/abs/2602.15889</link>
      <description>arXiv:2602.15889v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used in research both as tools and as objects of investigation. Much of this work implicitly assumes that LLM performance under fixed conditions (identical model snapshot, hyperparameters, and prompt) is time-invariant. If average output quality changes systematically over time, this assumption is violated, threatening the reliability, validity, and reproducibility of findings. To empirically examine this assumption, we conducted a longitudinal study on the temporal variability of GPT-4o's average performance. Using a fixed model snapshot, fixed hyperparameters, and identical prompting, GPT-4o was queried via the API to solve the same multiple-choice physics task every three hours for approximately three months. Ten independent responses were generated at each time point and their scores were averaged. Spectral (Fourier) analysis of the resulting time series revealed notable periodic variability in average model performance, accounting for approximately 20% of the total variance. In particular, the observed periodic patterns are well explained by the interaction of a daily and a weekly rhythm. These findings indicate that, even under controlled conditions, LLM performance may vary periodically over time, calling into question the assumption of time invariance. Implications for ensuring validity and replicability of research that uses or investigates LLMs are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.15889v1</guid>
      <category>stat.AP</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>physics.ed-ph</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul Tschisgale, Peter Wulff</dc:creator>
    </item>
    <item>
      <title>Surrogate-Based Prevalence Measurement for Large-Scale A/B Testing</title>
      <link>https://arxiv.org/abs/2602.16111</link>
      <description>arXiv:2602.16111v1 Announce Type: new 
Abstract: Online media platforms often need to measure how frequently users are exposed to specific content attributes in order to evaluate trade-offs in A/B experiments. A direct approach is to sample content, label it using a high-quality rubric (e.g., an expert-reviewed LLM prompt), and estimate impression-weighted prevalence. However, repeatedly running such labeling for every experiment arm and segment is too costly and slow to serve as a default measurement at scale.
  We present a scalable \emph{surrogate-based prevalence measurement} framework that decouples expensive labeling from per-experiment evaluation. The framework calibrates a surrogate signal to reference labels offline and then uses only impression logs to estimate prevalence for arbitrary experiment arms and segments. We instantiate this framework using \emph{score bucketing} as the surrogate: we discretize a model score into buckets, estimate bucket-level prevalences from an offline labeled sample, and combine these calibrated bucket level prevalences with the bucket distribution of impressions in each arm to obtain fast, log-based estimates.
  Across multiple large-scale A/B tests, we validate that the surrogate estimates closely match the reference estimates for both arm-level prevalence and treatment--control deltas. This enables scalable, low-latency prevalence measurement in experimentation without requiring per-experiment labeling jobs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16111v1</guid>
      <category>stat.AP</category>
      <category>cs.AI</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zehao Xu, Tony Paek, Kevin O'Sullivan, Attila Dobi</dc:creator>
    </item>
    <item>
      <title>Phase Transitions in Collective Damage of Civil Structures under Natural Hazards</title>
      <link>https://arxiv.org/abs/2602.16195</link>
      <description>arXiv:2602.16195v1 Announce Type: new 
Abstract: The fate of cities under natural hazards depends not only on hazard intensity but also on the coupling of structural damage, a collective process that remains poorly understood. Here we show that urban structural damage exhibits phase-transition phenomena. As hazard intensity increases, the system can shift abruptly from a largely safe to a largely damaged state, analogous to a first-order phase transition in statistical physics. Higher diversity in the building portfolio smooths this transition, but multiscale damage clustering traps the system in an extended critical-like regime (analogous to a Griffiths phase), suppressing the emergence of a more predictable disordered (Gaussian) phase. These phenomenological patterns are characterized by a random-field Ising model, with the external field, disorder strength, and temperature interpreted as the effective hazard demand, structural diversity, and modeling uncertainty, respectively. Applying this framework to real urban inventories reveals that widely used engineering modeling practices can shift urban damage patterns between synchronized and volatile regimes, systematically biasing exceedance-based risk metrics by up to 50% under moderate earthquakes ($M_w \approx 5.5$--$6.0$), equivalent to a several-fold gap in repair costs. This phase-aware description turns the collective behavior of civil infrastructure damage into actionable diagnostics for urban risk assessment and planning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16195v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sebin Oh, Jinyan Zhao, Raul Rincon, Jamie E. Padgett, Ziqi Wang</dc:creator>
    </item>
    <item>
      <title>Physical Activity Trajectories Preceding Incident Major Depressive Disorder Diagnosis Using Consumer Wearable Devices in the All of Us Research Program: Case-Control Study</title>
      <link>https://arxiv.org/abs/2602.16583</link>
      <description>arXiv:2602.16583v1 Announce Type: new 
Abstract: Low physical activity is a known risk factor for major depressive disorder (MDD), but changes in activity before a first clinical diagnosis remain unclear, especially using long-term objective measurements. This study characterized trajectories of wearable-measured physical activity during the year preceding incident MDD diagnosis.
  We conducted a retrospective nested case-control study using linked electronic health record and Fitbit data from the All of Us Research Program. Adults with at least 6 months of valid wearable data in the year before diagnosis were eligible. Incident MDD cases were matched to controls on age, sex, body mass index, and index time (up to four controls per case). Daily step counts and moderate-to-vigorous physical activity (MVPA) were aggregated into monthly averages. Linear mixed-effects models compared trajectories from 12 months before diagnosis to diagnosis. Within cases, contrasts identified when activity first significantly deviated from levels 12 months prior.
  The cohort included 4,104 participants (829 cases and 3,275 controls; 81.7% women; median age 48.4 years). Compared with controls, cases showed consistently lower activity and significant downward trajectories in both step counts and MVPA during the year before diagnosis (P &lt; 0.001). Significant declines appeared about 4 months before diagnosis for step counts and 5 months for MVPA. Exploratory analyses suggested subgroup differences, including steeper declines in men, greater intensity reductions at older ages, and persistently low activity among individuals with obesity.
  Sustained within-person declines in physical activity emerged months before incident MDD diagnosis. Longitudinal wearable monitoring may provide early signals to support risk stratification and earlier intervention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16583v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuezhou Zhang, Amos Folarin, Hugh Logan Ellis, Rongrong Zhong, Callum Stewart, Heet Sankesara, Hyunju Kim, Shaoxiong Sun, Abhishek Pratap, Richard JB Dobson</dc:creator>
    </item>
    <item>
      <title>Design and Analysis Strategies for Pooling in High Throughput Screening: Application to the Search for a New Anti-Microbial</title>
      <link>https://arxiv.org/abs/2602.16616</link>
      <description>arXiv:2602.16616v1 Announce Type: new 
Abstract: A major public health issue is the growing resistance of bacteria to antibiotics. An important part of the needed response is the discovery and development of new antimicrobial strategies. These require the screening of potential new drugs, typically accomplished using high-throughput screening (HTS). Traditionally, HTS is performed by examining one compound per well, but a more efficient strategy pools multiple compounds per well. In this work, we study several recently proposed pooling construction methods, as well as a variety of pooled high-throughput screening analysis methods, in order to provide guidance to practitioners on which methods to use. This is done in the context of an application of the methods to the search for new drugs to combat bacterial infection. We discuss both an extensive pilot study as well as a small screening campaign, and highlight both the successes and challenges of the pooling approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16616v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Byran Smucker, Benjamin Brennan, Emily Rego, Meng Wu, Zhihong Lin, Brian Ahmer, Blake Peterson</dc:creator>
    </item>
    <item>
      <title>Adaptive Semi-Supervised Training of P300 ERP-BCI Speller System with Minimum Calibration Effort</title>
      <link>https://arxiv.org/abs/2602.15955</link>
      <description>arXiv:2602.15955v1 Announce Type: cross 
Abstract: A P300 ERP-based Brain-Computer Interface (BCI) speller is an assistive communication tool. It searches for the P300 event-related potential (ERP) elicited by target stimuli, distinguishing it from the neural responses to non-target stimuli embedded in electroencephalogram (EEG) signals. Conventional methods require a lengthy calibration procedure to construct the binary classifier, which reduced overall efficiency. Thus, we proposed a unified framework with minimum calibration effort such that, given a small amount of labeled calibration data, we employed an adaptive semi-supervised EM-GMM algorithm to update the binary classifier. We evaluated our method based on character-level prediction accuracy, information transfer rate (ITR), and BCI utility. We applied calibration on training data and reported results on testing data. Our results indicate that, out of 15 participants, 9 participants exceed the minimum character-level accuracy of 0.7 using either on our adaptive method or the benchmark, and 7 out of these 9 participants showed that our adaptive method performed better than the benchmark. The proposed semi-supervised learning framework provides a practical and efficient alternative to improve the overall spelling efficiency in the real-time BCI speller system, particularly in contexts with limited labeled data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.15955v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shumeng Chen, Jane E. Huggins, Tianwen Ma</dc:creator>
    </item>
    <item>
      <title>Competing Risk Analysis in Cardiovascular Outcome Trials: A Simulation Comparison of Cox and Fine-Gray Models</title>
      <link>https://arxiv.org/abs/2602.16031</link>
      <description>arXiv:2602.16031v1 Announce Type: cross 
Abstract: Cardiovascular outcome trials commonly face competing risks when non-CV death prevents observation of major adverse cardiovascular events (MACE). While Cox proportional hazards models treat competing events as independent censoring, Fine-Gray subdistribution hazard models explicitly handle competing risks, targeting different estimands. This simulation study using bivariate copula models systematically varies competing event rates (0.5%-5% annually), treatment effects on competing events (50% reduction to 50% increase), and correlation structures to compare these approaches. At competing event rates typical of CV outcome trials (~1% annually), Cox and Fine-Gray produce nearly identical hazard ratio estimates regardless of correlation strength or treatment effect direction. Substantial divergence occurs only with high competing rates and directionally discordant treatment effects, though neither estimator provides unbiased estimates of true marginal hazard ratios under these conditions. In typical CV trial settings with low competing event rates, Cox models remain appropriate for primary analysis due to superior interpretability. Pre-specified Cox models should not be abandoned for competing risk methods. Importantly, Fine-Gray models do not constitute proper sensitivity analyses to Cox models per ICH E9(R1), as they target different estimands rather than testing assumptions. As supplementary analysis, cumulative incidence using Aalen-Johansen estimator can provide transparency about competing risk impact. Under high competing-risk scenarios, alternative approaches such as inverse probability of censoring weighting, multiple imputation, or inclusion of all-cause mortality in primary endpoints warrant consideration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16031v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tuo Wang, Yu Du</dc:creator>
    </item>
    <item>
      <title>Harnessing Implicit Cooperation: A Multi-Agent Reinforcement Learning Approach Towards Decentralized Local Energy Markets</title>
      <link>https://arxiv.org/abs/2602.16062</link>
      <description>arXiv:2602.16062v1 Announce Type: cross 
Abstract: This paper proposes implicit cooperation, a framework enabling decentralized agents to approximate optimal coordination in local energy markets without explicit peer-to-peer communication. We formulate the problem as a decentralized partially observable Markov decision problem that is solved through a multi-agent reinforcement learning task in which agents use stigmergic signals (key performance indicators at the system level) to infer and react to global states. Through a 3x3 factorial design on an IEEE 34-node topology, we evaluated three training paradigms (CTCE, CTDE, DTDE) and three algorithms (PPO, APPO, SAC). Results identify APPO-DTDE as the optimal configuration, achieving a coordination score of 91.7% relative to the theoretical centralized benchmark (CTCE). However, a critical trade-off emerges between efficiency and stability: while the centralized benchmark maximizes allocative efficiency with a peer-to-peer trade ratio of 0.6, the fully decentralized approach (DTDE) demonstrates superior physical stability. Specifically, DTDE reduces the variance of grid balance by 31% compared to hybrid architectures, establishing a highly predictable, import-biased load profile that simplifies grid regulation. Furthermore, topological analysis reveals emergent spatial clustering, where decentralized agents self-organize into stable trading communities to minimize congestion penalties. While SAC excelled in hybrid settings, it failed in decentralized environments due to entropy-driven instability. This research proves that stigmergic signaling provides sufficient context for complex grid coordination, offering a robust, privacy-preserving alternative to expensive centralized communication infrastructure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16062v1</guid>
      <category>eess.SY</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nelson Salazar-Pena, Alejandra Tabares, Andres Gonzalez-Mancera</dc:creator>
    </item>
    <item>
      <title>Feature-based morphological analysis of shape graph data</title>
      <link>https://arxiv.org/abs/2602.16120</link>
      <description>arXiv:2602.16120v1 Announce Type: cross 
Abstract: This paper introduces and demonstrates a computational pipeline for the statistical analysis of shape graph datasets, namely geometric networks embedded in 2D or 3D spaces. Unlike traditional abstract graphs, our purpose is not only to retrieve and distinguish variations in the connectivity structure of the data but also geometric differences of the network branches. Our proposed approach relies on the extraction of a specifically curated and explicit set of topological, geometric and directional features, designed to satisfy key invariance properties. We leverage the resulting feature representation for tasks such as group comparison, clustering and classification on cohorts of shape graphs. The effectiveness of this representation is evaluated on several real-world datasets including urban road/street networks, neuronal traces and astrocyte imaging. These results are benchmarked against several alternative methods, both feature-based and not.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16120v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Murad Hossen, Demetrio Labate, Nicolas Charon</dc:creator>
    </item>
    <item>
      <title>Introducing the b-value: combining unbiased and biased estimators from a sensitivity analysis perspective</title>
      <link>https://arxiv.org/abs/2602.16310</link>
      <description>arXiv:2602.16310v1 Announce Type: cross 
Abstract: In empirical research, when we have multiple estimators for the same parameter of interest, a central question arises: how do we combine unbiased but less precise estimators with biased but more precise ones to improve the inference? Under this setting, the point estimation problem has attracted considerable attention. In this paper, we focus on a less studied inference question: how can we conduct valid statistical inference in such settings with unknown bias? We propose a strategy to combine unbiased and biased estimators from a sensitivity analysis perspective. We derive a sequence of confidence intervals indexed by the magnitude of the bias, which enable researchers to assess how conclusions vary with the bias levels. Importantly, we introduce the notion of the b-value, a critical value of the unknown maximum relative bias at which combining estimators does not yield a significant result. We apply this strategy to three canonical combined estimators: the precision-weighted estimator, the pretest estimator, and the soft-thresholding estimator. For each estimator, we characterize the sequence of confidence intervals and determine the bias threshold at which the conclusion changes. Based on the theory, we recommend reporting the b-value based on the soft-thresholding estimator and its associated confidence intervals, which are robust to unknown bias and achieve the lowest worst-case risk among the alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16310v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhexiao Lin, Peter J. Bickel, Peng Ding</dc:creator>
    </item>
    <item>
      <title>Two-way Clustering Robust Variance Estimator in Quantile Regression Models</title>
      <link>https://arxiv.org/abs/2602.16376</link>
      <description>arXiv:2602.16376v1 Announce Type: cross 
Abstract: We study inference for linear quantile regression with two-way clustered data. Using a separately exchangeable array framework and a projection decomposition of the quantile score, we characterize regime-dependent convergence rates and establish a self-normalized Gaussian approximation. We propose a two-way cluster-robust sandwich variance estimator with a kernel-based density ``bread'' and a projection-matched ``meat'', and prove consistency and validity of inference in Gaussian regimes. We also show an impossibility result for uniform inference in a non-Gaussian interaction regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16376v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ulrich Hounyo, Jiahao Lin</dc:creator>
    </item>
    <item>
      <title>Scattering and sputtering on the lunar surface; Insights from negative ions observed at the surface</title>
      <link>https://arxiv.org/abs/2602.16567</link>
      <description>arXiv:2602.16567v1 Announce Type: cross 
Abstract: Context. Airless planetary bodies are directly exposed to solar wind ions, which can scatter or become implanted upon impact with the regolith-covered surface, while also sputtering surface atoms.
  Aims. We construct a semi-analytical model for the scattering of ions of hundreds of eV and the sputtering of surface atoms, both resulting in the emission of negative ions from the lunar surface. Our model contains a novel description of the scattering process that is physics-based and constrained by observations.
  Methods. We use data from the Negative Ions at the Lunar Surface (NILS) instrument on the Chang'e-6 lander to update prior knowledge of ion scattering and sputtering from lunar regolith through Bayesian inference.
  Results. Our model shows good agreement with the NILS data. A precipitating solar wind proton has roughly a 22% chance of scattering from the lunar surface in any charge state, and about an 8% chance of sputtering a surface hydrogen atom. The resulting ratio of scattered to sputtered hydrogen flux is eta_sc / eta_sp = 1.5 for a proton speed of 300 km/s. We find a high probability (7-20%) that a hydrogen atom leaves the surface negatively charged. The angular emission distributions at near-grazing angles for both scattered and sputtered fluxes are controlled by surface roughness. Our model also indicates significant inelastic energy losses for hydrogen interacting with the regolith, suggesting a longer effective path length than previously assumed. Finally, we estimate a surface binding energy of 5.5 eV, consistent with the observations.
  Conclusions. Our model describes the scattering and sputtering of particles of any charge state from any homogeneous, multi-species surface. Using NILS data, we successfully applied the model to update our understanding of solar wind interacting with lunar regolith, and the emission of negative hydrogen ions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16567v1</guid>
      <category>physics.space-ph</category>
      <category>physics.atom-ph</category>
      <category>physics.ins-det</category>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Romain Canu-Blot, Martin Wieser, Umberto Rollero, Thomas Maynadi\'e, Stas Barabash, Gabriella Stenberg Wieser, Aibing Zhang, Wenjing Wang, Chi Wang</dc:creator>
    </item>
    <item>
      <title>A Cost-Effective Slag-based Mix Activated with Soda Ash and Hydrated Lime: A Pilot Study</title>
      <link>https://arxiv.org/abs/2305.12288</link>
      <description>arXiv:2305.12288v5 Announce Type: replace 
Abstract: The present study explores a cost-effective method for using activated ground granulated blast furnace slag (GGBFS) and silica fume (SF) as cement substitutes. Instead of activating them with expensive alkali solutions, the present study employs industrial-grade powdered soda ash (SA) and hydrated lime (HL) as activators, reducing expenses by about 94.5% compared to their corresponding analytical-grade counterparts. Herein, the exclusivity is depicted using less pure chemicals rather than relying on reagents with 99% purity. Two mixing techniques are compared: one involves directly introducing powdered SA and HL, while the other pre-mixes SA with water before adding it to a dry powder mixture of GGBFS, SF, and HL. Microstructural analysis reveals that the initial strength results from various hydrate phases, including calcium-sodium-aluminate-silicate hydrate (CNASH). The latter strength is attributed to the coexistence of calcium-silicate hydrate (CSH), calcium-aluminate-silicate hydrate (CASH) and sodium-aluminate-silicate hydrate (NASH), with contributions from calcite and hydrotalcite. The SF content significantly influenced the formation of these gel phases. Thermogravimetric analysis (TGA) reveals phase transitions and bound water related to hydration products. The optimal mix comprises 10% SF, 90% GGBFS, 9.26% HL, and 13.25% SA, with a water-to-solids ratio of 0.45. This approach yields a compressive strength of 35.1 MPa after 28 days and 41.33 MPa after 120 days, hence suitable for structural construction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.12288v5</guid>
      <category>stat.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1061/PPSCFX.SCENG-1426</arxiv:DOI>
      <dc:creator>Jayashree Sengupta, Nirjhar Dhang, Arghya Deb</dc:creator>
    </item>
    <item>
      <title>The Dataset of Daily Air Quality for the Years 2013-2023 in Italy</title>
      <link>https://arxiv.org/abs/2602.10749</link>
      <description>arXiv:2602.10749v2 Announce Type: replace 
Abstract: Air quality and climate are major issues in Italian society and lie at the intersection of many research fields, including public health and policy planning. There is an increasing need for readily available, easily accessible, ready-to-use and well-documented datasets on air quality and climate. In this paper, we present the GRINS AQCLIM dataset, created under the GRINS project framework covering the Italian domain for an extensive time period. It includes daily statistics (e.g., minimum, quartiles, mean, median and maximum) for a collection of air pollutant concentrations and climate variables at the locations of the 700+ available monitoring stations. Input data are retrieved from the European Environmental Agency and Copernicus Programme and were subjected to multiple processing steps to ensure their reliability and quality. These steps include automatic procedures for fixing raw files, manual inspection of stations information, the detection and removal of anomalies, and the temporal harmonisation on a daily basis. Datasets are hosted on Zenodo under open-access principles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10749v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Alessandro Fusta Moro, Alessandro Fass\`o, Jacopo Rodeschini</dc:creator>
    </item>
    <item>
      <title>From Chain-Ladder to Individual Claims Reserving</title>
      <link>https://arxiv.org/abs/2602.15385</link>
      <description>arXiv:2602.15385v2 Announce Type: replace 
Abstract: The chain-ladder (CL) method is the most widely used claims reserving technique in non-life insurance. This manuscript introduces a novel approach to computing the CL reserves based on a fundamental restructuring of the data utilization for the CL prediction procedure. Instead of rolling forward the cumulative claims with estimated CL factors, we estimate multi-period factors that project the latest observations directly to the ultimate claims. This alternative perspective on CL reserving creates a natural pathway for the application of machine learning techniques to individual claims reserving. As a proof of concept, we present a small-scale real data application employing neural networks for individual claims reserving.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.15385v2</guid>
      <category>stat.AP</category>
      <category>q-fin.RM</category>
      <category>stat.ML</category>
      <pubDate>Thu, 19 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ronald Richman, Mario V. W\"uthrich</dc:creator>
    </item>
  </channel>
</rss>
