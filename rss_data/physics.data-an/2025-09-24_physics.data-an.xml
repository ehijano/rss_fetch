<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Sep 2025 01:43:27 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Surrogate Modelling of Proton Dose with Monte Carlo Dropout Uncertainty Quantification</title>
      <link>https://arxiv.org/abs/2509.18155</link>
      <description>arXiv:2509.18155v1 Announce Type: cross 
Abstract: Accurate proton dose calculation using Monte Carlo (MC) is computationally demanding in workflows like robust optimisation, adaptive replanning, and probabilistic inference, which require repeated evaluations. To address this, we develop a neural surrogate that integrates Monte Carlo dropout to provide fast, differentiable dose predictions along with voxelwise predictive uncertainty. The method is validated through a series of experiments, starting with a one-dimensional analytic benchmark that establishes accuracy, convergence, and variance decomposition. Two-dimensional bone-water phantoms, generated using TOPAS Geant4, demonstrate the method's behavior under domain heterogeneity and beam uncertainty, while a three-dimensional water phantom confirms scalability for volumetric dose prediction. Across these settings, we separate epistemic (model) from parametric (input) contributions, showing that epistemic variance increases under distribution shift, while parametric variance dominates at material boundaries. The approach achieves significant speedups over MC while retaining uncertainty information, making it suitable for integration into robust planning, adaptive workflows, and uncertainty-aware optimisation in proton therapy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18155v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aaron Pim, Tristan Pryer</dc:creator>
    </item>
    <item>
      <title>Complexity-entropy analysis of solar photospheric turbulence: Hinode images of magnetic and Poynting fluxes</title>
      <link>https://arxiv.org/abs/2509.18444</link>
      <description>arXiv:2509.18444v1 Announce Type: cross 
Abstract: The spatiotemporal inhomogeneous-homogeneous transition in the dynamics and structures of solar photospheric turbulence is studied by applying the complexity-entropy analysis to Hinode images of a vortical region of supergranular junctions in the quiet Sun. During a period of supergranular vortex expansion of 37.5 min, the spatiotemporal dynamics of the line-of-sight magnetic field and the horizontal electromagnetic energy flux display the characteristics of inverse turbulent cascade, evidenced by the formation of a large magnetic coherent structure via the merger of two small magnetic elements trapped by a long-duration vortex. Both magnetic and Poynting fluxes exhibit an admixture of chaos and stochasticity in the complexity-entropy plane, involving a temporal transition from low to high complexity and a temporal transition from high to low entropy during the period of vortex expansion, consistent with Hinode observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18444v1</guid>
      <category>astro-ph.SR</category>
      <category>physics.data-an</category>
      <category>physics.space-ph</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abraham C. -L. Chian, Haroldo V. Ribeiro, Erico L. Rempel, Rodrigo A. Miranda, Luis B. Rubio, Milan Go\v{s}i\'c, Breno Raphaldini, Yasuhito Narita</dc:creator>
    </item>
    <item>
      <title>Extracting the geometric backbone of bipartite networks</title>
      <link>https://arxiv.org/abs/2509.18726</link>
      <description>arXiv:2509.18726v1 Announce Type: cross 
Abstract: Real bipartite networks combine degree-constrained random mixing with structured, locality-like rules. We introduce a statistical filter that benchmarks node-level bipartite clustering against degree-preserving randomizations to classify nodes as geometric (signal) or random-like (noise). In synthetic mixtures with known ground truth, the filter achieves high F-scores and sharpens inference of latent geometric parameters. Applied to four empirical systems --metabolism, online group membership, plant-pollinator interactions, and languages-- it isolates recurrent neighborhoods while removing ubiquitous or weakly co-occurring entities. Filtering exposes a compact geometric backbone that disproportionately sustains connectivity under percolation and preserves downstream classifier accuracy in node-feature tasks, offering a simple, scalable way to disentangle structure from noise in bipartite networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18726v1</guid>
      <category>physics.soc-ph</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Luc\'ia S. Ram\'irez, Roya Aliakbarisani, M. \'Angeles Serrano, Mari\'an Bogu\~n\'a</dc:creator>
    </item>
    <item>
      <title>Filtering amplitude dependence of correlation dynamics in complex systems: application to the cryptocurrency market</title>
      <link>https://arxiv.org/abs/2509.18820</link>
      <description>arXiv:2509.18820v1 Announce Type: cross 
Abstract: Based on the cryptocurrency market dynamics, this study presents a general methodology for analyzing evolving correlation structures in complex systems using the $q$-dependent detrended cross-correlation coefficient \rho(q,s). By extending traditional metrics, this approach captures correlations at varying fluctuation amplitudes and time scales. The method employs $q$-dependent minimum spanning trees ($q$MSTs) to visualize evolving network structures. Using minute-by-minute exchange rate data for 140 cryptocurrencies on Binance (Jan 2021-Oct 2024), a rolling window analysis reveals significant shifts in $q$MSTs, notably around April 2022 during the Terra/Luna crash. Initially centralized around Bitcoin (BTC), the network later decentralized, with Ethereum (ETH) and others gaining prominence. Spectral analysis confirms BTC's declining dominance and increased diversification among assets. A key finding is that medium-scale fluctuations exhibit stronger correlations than large-scale ones, with $q$MSTs based on the latter being more decentralized. Properly exploiting such facts may offer the possibility of a more flexible optimal portfolio construction. Distance metrics highlight that major disruptions amplify correlation differences, leading to fully decentralized structures during crashes. These results demonstrate $q$MSTs' effectiveness in uncovering fluctuation-dependent correlations, with potential applications beyond finance, including biology, social and other complex systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18820v1</guid>
      <category>q-fin.ST</category>
      <category>cs.CE</category>
      <category>econ.EM</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1103/v7cl-h7xr</arxiv:DOI>
      <dc:creator>Marcin W\k{a}torek, Marija Bezbradica, Martin Crane, Jaros{\l}aw Kwapie\'n, Stanis{\l}aw Dro\.zd\.z</dc:creator>
    </item>
    <item>
      <title>FAIR Universe HiggsML Uncertainty Dataset and Competition</title>
      <link>https://arxiv.org/abs/2410.02867</link>
      <description>arXiv:2410.02867v5 Announce Type: replace-cross 
Abstract: The FAIR Universe HiggsML Uncertainty Challenge focused on measuring the physical properties of elementary particles with imperfect simulators. Participants were required to compute and report confidence intervals for a parameter of interest regarding the Higgs boson while accounting for various systematic (epistemic) uncertainties. The dataset is a tabular dataset of 28 features and 280 million instances. Each instance represents a simulated proton-proton collision as observed at CERN's Large Hadron Collider in Geneva, Switzerland. The features of these simulations were chosen to capture key characteristics of different types of particles. These include primary attributes, such as the energy and three-dimensional momentum of the particles, as well as derived attributes, which are calculated from the primary ones using domain-specific knowledge. Additionally, a label feature designates each instance's type of proton-proton collision, distinguishing the Higgs boson events of interest from three background sources. As outlined in this paper, the permanent release of the dataset allows long-term benchmarking of new techniques. The leading submissions, including Contrastive Normalising Flows and Density Ratios estimation through classification, are described. Our challenge has brought together the physics and machine learning communities to advance our understanding and methodologies in handling systematic uncertainties within AI techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02867v5</guid>
      <category>hep-ph</category>
      <category>cs.LG</category>
      <category>hep-ex</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lisa Benato, Wahid Bhimji, Paolo Calafiura, Ragansu Chakkappai, Po-Wen Chang, Yuan-Tang Chou, Sascha Diefenbacher, Jordan Dudley, Ibrahim Elsharkawy, Steven Farrell, Aishik Ghosh, Cristina Giordano, Isabelle Guyon, Chris Harris, Yota Hashizume, Shih-Chieh Hsu, Elham E. Khoda, Claudius Krause, Ang Li, Benjamin Nachman, Peter Nugent, David Rousseau, Robert Schoefbeck, Maryam Shooshtari, Dennis Schwarz, Benjamin Thorne, Ihsan Ullah, Daohan Wang, Yulei Zhang</dc:creator>
    </item>
    <item>
      <title>Homophily Within and Across Groups</title>
      <link>https://arxiv.org/abs/2412.07901</link>
      <description>arXiv:2412.07901v3 Announce Type: replace-cross 
Abstract: Homophily -- the tendency of individuals to interact with similar others -- shapes how networks form and function. Yet existing approaches typically collapse homophily to a single scale, either one parameter for the whole network or one per community, thereby detaching it from other structural features. Here, we introduce a maximum-entropy random graph model that moves beyond these limits, capturing homophily across all social scales in the network, with parameters for each group size. The framework decomposes homophily into within- and across-group contributions, recovering the stochastic block model as a special case. As an exponential-family model, it fits empirical data and enables inference of group-level variation of homophily that aggregate metrics miss. The group-dependence of homophily substantially impacts network percolation thresholds, altering predictions for epidemic spread, information diffusion, and the effectiveness of interventions. Ignoring such heterogeneity risks systematically misjudging connectivity and dynamics in complex systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07901v3</guid>
      <category>physics.soc-ph</category>
      <category>cs.CY</category>
      <category>math.PR</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abbas K. Rizi, Riccardo Michielan, Clara Stegehuis, Mikko Kivel\"a</dc:creator>
    </item>
    <item>
      <title>Data-driven performance optimization of gamma spectrometers with many channels</title>
      <link>https://arxiv.org/abs/2504.07166</link>
      <description>arXiv:2504.07166v2 Announce Type: replace-cross 
Abstract: In gamma spectrometers with variable spectroscopic performance across many channels (e.g., many pixels or voxels), a tradeoff exists between including data from successively worse-performing readout channels and increasing efficiency. Brute-force calculation of the optimal set of included channels is exponentially infeasible as the number of channels grows, and approximate methods are required. In this work, we present a data-driven framework for attempting to find near-optimal sets of included detector channels. The framework leverages non-negative matrix factorization (NMF) to learn the behavior of gamma spectra across the detector, and clusters similarly-performing detector channels together. Performance comparisons are then made between spectra with channel clusters removed, which is more feasible than brute force. The framework is general and can be applied to arbitrary, user-defined performance metrics depending on the application. We apply this framework to optimizing gamma spectra measured by H3D M400 CdZnTe spectrometers, which exhibit variable performance across their crystal volumes. In particular, we show several examples optimizing various performance metrics for uranium and plutonium gamma spectra in nondestructive assay for nuclear safeguards, and explore trends in performance vs.\ parameters such as clustering algorithm type. We also compare the NMF+clustering pipeline to several non-machine-learning algorithms, including several greedy algorithms. Overall, we find that the NMF+clustering pipeline tends to find the best-performing set of detector voxels, significantly improving over the un-optimized spectra, but that a greedy accumulation of spectra segmented by detector depth can in some cases give similar performance improvements in much less computation time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07166v2</guid>
      <category>physics.ins-det</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jayson R. Vavrek, Hannah S. Parrilla, Gabriel Aversano, Mark S. Bandstra, Micah Folsom, Daniel Hellfeld</dc:creator>
    </item>
  </channel>
</rss>
