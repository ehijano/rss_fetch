<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 23 Jan 2025 02:30:22 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Recovering Unobserved Network Links from Aggregated Relational Data: Discussions on Bayesian Latent Surface Modeling and Penalized Regression</title>
      <link>https://arxiv.org/abs/2501.10675</link>
      <description>arXiv:2501.10675v1 Announce Type: new 
Abstract: Accurate network data are essential in fields such as economics, sociology, and computer science. Aggregated Relational Data (ARD) provides a way to capture network structures using partial data. This article compares two main frameworks for recovering network links from ARD: Bayesian Latent Surface Modeling (BLSM) and Frequentist Penalized Regression (FPR). Using simulation studies and real-world applications, we evaluate their theoretical properties, computational efficiency, and practical utility in domains like financial risk assessment and epidemiology. Key findings emphasize the importance of trait design, privacy considerations, and hybrid modeling approaches to improve scalability and robustness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10675v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yen-hsuan Tseng</dc:creator>
    </item>
    <item>
      <title>Estimation of Linear models from Coarsened Observations Estimation of Linear models Estimation from Coarsened Observations A Method of Moments Approach</title>
      <link>https://arxiv.org/abs/2501.10726</link>
      <description>arXiv:2501.10726v1 Announce Type: new 
Abstract: In the last few decades, the study of ordinal data in which the variable of interest is not exactly observed but only known to be in a specific ordinal category has become important. In Psychometrics such variables are analysed under the heading of item response models (IRM). In Econometrics, subjective well-being (SWB) and self-assessed health (SAH) studies, and in marketing research, Ordered Probit, Ordered Logit, and Interval Regression models are common research platforms. To emphasize that the problem is not specific to a specific discipline we will use the neutral term coarsened observation. For single-equation models estimation of the latent linear model by Maximum Likelihood (ML) is routine. But, for higher -dimensional multivariate models it is computationally cumbersome as estimation requires the evaluation of multivariate normal distribution functions on a large scale. Our proposed alternative estimation method, based on the Generalized Method of Moments (GMM), circumvents this multivariate integration problem. The method is based on the assumed zero correlations between explanatory variables and generalized residuals. This is more general than ML but coincides with ML if the error distribution is multivariate normal. It can be implemented by repeated application of standard techniques. GMM provides a simpler and faster approach than the usual ML approach. It is applicable to multiple -equation models with -dimensional error correlation matrices and response categories for the equation. It also yields a simple method to estimate polyserial and polychoric correlations. Comparison of our method with the outcomes of the Stata ML procedure cmp yields estimates that are not statistically different, while estimation by our method requires only a fraction of the computing time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10726v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bernard M. S. van Praag, J. Peter Hop, William H. Greene</dc:creator>
    </item>
    <item>
      <title>Bias Analysis of Experiments for Multi-Item Multi-Period Inventory Control Policies</title>
      <link>https://arxiv.org/abs/2501.11996</link>
      <description>arXiv:2501.11996v1 Announce Type: cross 
Abstract: Randomized experiments, or A/B testing, are the gold standard for evaluating interventions but are underutilized in the area of inventory management. This study addresses this gap by analyzing A/B testing strategies in multi-item, multi-period inventory systems with lost sales and capacity constraints. We examine switchback experiments, item-level randomization, pairwise randomization, and staggered rollouts, analyzing their biases theoretically and comparing them through numerical experiments. Our findings provide actionable guidance for selecting experimental designs across various contexts in inventory management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11996v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinqi Chen, Xingyu Bai, Zeyu Zheng, Nian Si</dc:creator>
    </item>
    <item>
      <title>Dynamic CoVaR Modeling and Estimation</title>
      <link>https://arxiv.org/abs/2206.14275</link>
      <description>arXiv:2206.14275v4 Announce Type: replace 
Abstract: The popular systemic risk measure CoVaR (conditional Value-at-Risk) and its variants are widely used in economics and finance. In this article, we propose joint dynamic forecasting models for the Value-at-Risk (VaR) and CoVaR. The CoVaR version we consider is defined as a large quantile of one variable (e.g., losses in the financial system) conditional on some other variable (e.g., losses in a bank's shares) being in distress. We introduce a two-step M-estimator for the model parameters drawing on recently proposed bivariate scoring functions for the pair (VaR, CoVaR). We prove consistency and asymptotic normality of our parameter estimator and analyze its finite-sample properties in simulations. Finally, we apply a specific subclass of our dynamic forecasting models, which we call CoCAViaR models, to log-returns of large US banks. A formal forecast comparison shows that our CoCAViaR models generate CoVaR predictions which are superior to forecasts issued from current benchmark models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.14275v4</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>q-fin.RM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Timo Dimitriadis, Yannick Hoga</dc:creator>
    </item>
    <item>
      <title>The Robust F-Statistic as a Test for Weak Instruments</title>
      <link>https://arxiv.org/abs/2309.01637</link>
      <description>arXiv:2309.01637v3 Announce Type: replace 
Abstract: Montiel Olea and Pflueger (2013) proposed the effective F-statistic as a test for weak instruments in terms of the Nagar bias of the two-stage least squares (2SLS) estimator relative to a benchmark worst-case bias. We show that their methodology applies to a class of linear generalized method of moments (GMM) estimators with an associated class of generalized effective F-statistics. The standard nonhomoskedasticity robust F-statistic is a member of this class. The associated GMMf estimator, with the extension f for first-stage, is a novel and unusual estimator as the weight matrix is based on the first-stage residuals. As the robust F-statistic can also be used as a test for underidentification, expressions for the calculation of the weak-instruments critical values in terms of the Nagar bias of the GMMf estimator relative to the benchmark simplify and no simulation methods or Patnaik (1949) distributional approximations are needed. In the grouped-data IV designs of Andrews (2018), where the robust F-statistic is large but the effective F-statistic is small, the GMMf estimator is shown to behave much better in terms of bias than the 2SLS estimator, as expected by the weak-instruments test results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.01637v3</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Frank Windmeijer</dc:creator>
    </item>
    <item>
      <title>Estimating Heterogeneous Treatment Effects with Item-Level Outcome Data: Insights from Item Response Theory</title>
      <link>https://arxiv.org/abs/2405.00161</link>
      <description>arXiv:2405.00161v4 Announce Type: replace 
Abstract: Analyses of heterogeneous treatment effects (HTE) are common in applied causal inference research. However, when outcomes are latent variables assessed via psychometric instruments such as educational tests, standard methods ignore the potential HTE that may exist among the individual items of the outcome measure. Failing to account for "item-level" HTE (IL-HTE) can lead to both underestimated standard errors and identification challenges in the estimation of treatment-by-covariate interaction effects. We demonstrate how Item Response Theory (IRT) models that estimate a treatment effect for each assessment item can both address these challenges and provide new insights into HTE generally. This study articulates the theoretical rationale for the IL-HTE model and demonstrates its practical value using 75 datasets from 48 randomized controlled trials containing 5.8 million item responses in economics, education, and health research. Our results show that the IL-HTE model reveals item-level variation masked by single-number scores, provides more meaningful standard errors in many settings, allows for estimates of the generalizability of causal effects to untested items, resolves identification problems in the estimation of interaction effects, and provides estimates of standardized treatment effect sizes corrected for attenuation due to measurement error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00161v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Joshua B. Gilbert, Zachary Himmelsbach, James Soland, Mridul Joshi, Benjamin W. Domingue</dc:creator>
    </item>
    <item>
      <title>A Robust Residual-Based Test for Structural Changes in Factor Models</title>
      <link>https://arxiv.org/abs/2406.00941</link>
      <description>arXiv:2406.00941v2 Announce Type: replace 
Abstract: In this paper, we propose an easy-to-implement residual-based specification testing procedure for detecting structural changes in factor models, which is powerful against both smooth and abrupt structural changes with unknown break dates. The proposed test is robust against the over-specified number of factors, and serially and crosssectionally correlated error processes. A new central limit theorem is given for the quadratic forms of panel data with dependence over both dimensions, thereby filling a gap in the literature. We establish the asymptotic properties of the proposed test statistic, and accordingly develop a simulation-based scheme to select critical value in order to improve finite sample performance. Through extensive simulations and a real-world application, we confirm our theoretical results and demonstrate that the proposed test exhibits desirable size and power in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00941v2</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Bin Peng, Liangjun Su, Yayi Yan</dc:creator>
    </item>
    <item>
      <title>Difference-in-Differences with Multiple Events</title>
      <link>https://arxiv.org/abs/2409.05184</link>
      <description>arXiv:2409.05184v3 Announce Type: replace 
Abstract: This paper studies staggered Difference-in-Differences (DiD) design when there is a second event confounding the target event. When the events are correlated, the treatment and the control group are unevenly exposed to the effects of the second event, causing an omitted event bias. To address this bias, I propose a two-stage DiD design. In the first stage, I estimate the combined effects of both treatments using a control group that is neither treated nor confounded. In the second stage, I isolate the effects of the target treatment by leveraging a parallel treatment effect assumption and a control group that is treated but not yet confounded. Finally, I apply this method to revisit the effect of minimum wage increases on teen employment using state-level hikes between 2010 and 2020. I find that the Medicaid expansion under the ACA is a significant confounder: controlling for this bias reduces the short-term estimate of the minimum wage effect by two-thirds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05184v3</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin-Tung Tsai</dc:creator>
    </item>
    <item>
      <title>Measuring the Driving Forces of Predictive Performance: Application to Credit Scoring</title>
      <link>https://arxiv.org/abs/2212.05866</link>
      <description>arXiv:2212.05866v4 Announce Type: replace-cross 
Abstract: As they play an increasingly important role in determining access to credit, credit scoring models are under growing scrutiny from banking supervisors and internal model validators. These authorities need to monitor the model performance and identify its key drivers. To facilitate this, we introduce the XPER methodology to decompose a performance metric (e.g., AUC, $R^2$) into specific contributions associated with the various features of a forecasting model. XPER is theoretically grounded on Shapley values and is both model-agnostic and performance metric-agnostic. Furthermore, it can be implemented either at the model level or at the individual level. Using a novel dataset of car loans, we decompose the AUC of a machine-learning model trained to forecast the default probability of loan applicants. We show that a small number of features can explain a surprisingly large part of the model performance. Notably, the features that contribute the most to the predictive performance of the model may not be the ones that contribute the most to individual forecasts (SHAP). Finally, we show how XPER can be used to deal with heterogeneity issues and improve performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.05866v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hu\'e Sullivan, Hurlin Christophe, P\'erignon Christophe, Saurin S\'ebastien</dc:creator>
    </item>
    <item>
      <title>Sparse Asymptotic PCA: Identifying Sparse Latent Factors Across Time Horizon</title>
      <link>https://arxiv.org/abs/2407.09738</link>
      <description>arXiv:2407.09738v2 Announce Type: replace-cross 
Abstract: This paper introduces a novel sparse latent factor modeling framework using sparse asymptotic Principal Component Analysis (APCA) to analyze the co-movements of high-dimensional panel data over time. Unlike existing methods based on sparse PCA, which assume sparsity in the loading matrices, our approach posits sparsity in the factor processes while allowing non-sparse loadings. This is motivated by the fact that financial returns typically exhibit universal and non-sparse exposure to market factors. Unlike the commonly used $\ell_1$-relaxation in sparse PCA, the proposed sparse APCA employs a truncated power method to estimate the leading sparse factor and a sequential deflation method for multi-factor cases under $\ell_0$-constraints. Furthermore, we develop a data-driven approach to identify the sparsity of risk factors over the time horizon using a novel cross-sectional cross-validation method. We establish the consistency of our estimators under mild conditions as both the dimension $N$ and the sample size $T$ grow. Monte Carlo simulations demonstrate that the proposed method performs well in finite samples. Empirically, we apply our method to daily S&amp;P 500 stock returns (2004--2016) and identify nine risk factors influencing the stock market.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09738v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>q-fin.ST</category>
      <category>stat.ML</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaoxing Gao</dc:creator>
    </item>
  </channel>
</rss>
