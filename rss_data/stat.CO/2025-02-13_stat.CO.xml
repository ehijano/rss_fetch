<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Feb 2025 02:49:38 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Resampling Methods that Generate Time Series Data to Enable Sensitivity and Model Analysis in Energy Modeling</title>
      <link>https://arxiv.org/abs/2502.08102</link>
      <description>arXiv:2502.08102v1 Announce Type: new 
Abstract: Energy systems modeling frequently relies on time series data, whether observed or forecast. This is particularly the case, for example, in capacity planning models that use hourly production and load data forecast to occur over the coming several decades. This paper addresses the attendant problem of performing sensitivity, robustness, and other post-solution analyses using time series data. We explore two efficient and relatively simple, non-parametric, bootstrapping methods for generating arbitrary numbers of time series from a single observed or forecast series. The paper presents and assesses each method. We find that the generated series are both visually and by statistical summary measures close to the original observational data. In consequence these series are credibly taken as stochastic instances from a common distribution, that of the original series of observations. With climate change in mind, the paper further proposes and explores two general techniques for systematically altering (increasing or decreasing) time series. Both for the perturbed and unperturbed synthetic series data, we find that the generated series induce variability in properties of the series that are important for energy modeling, in particular periods of under- and over-production, and periods of increased ramping rates. In consequence, series produced in this way are apt for use in robustness, sensitivity, and in general post-solution analysis of energy planning models. These validity factors auger well for applications beyond energy modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08102v1</guid>
      <category>stat.CO</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kelly Wang, Steven O. Kimbrough</dc:creator>
    </item>
    <item>
      <title>Filtered Markovian Projection: Dimensionality Reduction in Filtering for Stochastic Reaction Networks</title>
      <link>https://arxiv.org/abs/2502.07918</link>
      <description>arXiv:2502.07918v1 Announce Type: cross 
Abstract: Stochastic reaction networks (SRNs) model stochastic effects for various applications, including intracellular chemical or biological processes and epidemiology. A typical challenge in practical problems modeled by SRNs is that only a few state variables can be dynamically observed. Given the measurement trajectories, one can estimate the conditional probability distribution of unobserved (hidden) state variables by solving a stochastic filtering problem. In this setting, the conditional distribution evolves over time according to an extensive or potentially infinite-dimensional system of coupled ordinary differential equations with jumps, known as the filtering equation. The current numerical filtering techniques, such as the Filtered Finite State Projection (DAmbrosio et al., 2022), are hindered by the curse of dimensionality, significantly affecting their computational performance. To address these limitations, we propose to use a dimensionality reduction technique based on the Markovian projection (MP), initially introduced for forward problems (Ben Hammouda et al., 2024). In this work, we explore how to adapt the existing MP approach to the filtering problem and introduce a novel version of the MP, the Filtered MP, that guarantees the consistency of the resulting estimator. The novel method combines a particle filter with reduced variance and solving the filtering equations in a low-dimensional space, exploiting the advantages of both approaches. The analysis and empirical results highlight the superior computational efficiency of projection methods compared to the existing filtered finite state projection in the large dimensional setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07918v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chiheb Ben Hammouda, Maksim Chupin, Sophia M\"unker, Ra\'ul Tempone</dc:creator>
    </item>
    <item>
      <title>What is a Sketch-and-Precondition Derivation for Low-Rank Approximation? Inverse Power Error or Inverse Power Estimation?</title>
      <link>https://arxiv.org/abs/2502.07993</link>
      <description>arXiv:2502.07993v1 Announce Type: cross 
Abstract: Randomized sketching accelerates large-scale numerical linear algebra by reducing computa- tional complexity. While the traditional sketch-and-solve approach reduces the problem size di- rectly through sketching, the sketch-and-precondition method leverages sketching to construct a computational friendly preconditioner. This preconditioner improves the convergence speed of iterative solvers applied to the original problem, maintaining accuracy in the full space. Further- more, the convergence rate of the solver improves at least linearly with the sketch size. Despite its potential, developing a sketch-and-precondition framework for randomized algorithms in low- rank matrix approximation remains an open challenge. We introduce the Error-Powered Sketched Inverse Iteration (EPSI) Method via run sketched Newton iteration for the Lagrange form as a sketch-and-precondition variant for randomized low-rank approximation. Our method achieves theoretical guarantees, including a convergence rate that improves at least linearly with the sketch size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07993v1</guid>
      <category>math.NA</category>
      <category>cs.CC</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruihan Xu, Yiping Lu</dc:creator>
    </item>
    <item>
      <title>From Clicks to Conversations: Evaluating the Effectiveness of Conversational Agents in Statistical Analysis</title>
      <link>https://arxiv.org/abs/2502.08114</link>
      <description>arXiv:2502.08114v1 Announce Type: cross 
Abstract: The rapid proliferation of data science forced different groups of individuals with different backgrounds to adapt to statistical analysis. We hypothesize that conversational agents are better suited for statistical analysis than traditional graphical user interfaces (GUI). In this work, we propose a novel conversational agent, StatZ, for statistical analysis. We evaluate the efficacy of StatZ relative to established statistical software:SPSS, SAS, Stata, and JMP in terms of accuracy, task completion time, user experience, and user satisfaction. We combined the proposed analysis question from state-of-the-art language models with suggestions from statistical analysis experts and tested with 51 participants from diverse backgrounds. Our experimental design assessed each participant's ability to perform statistical analysis tasks using traditional statistical analysis tools with GUI and our conversational agent. Results indicate that the proposed conversational agents significantly outperform GUI statistical software in all assessed metrics, including quantitative (task completion time, accuracy, and user experience), and qualitative (user satisfaction) metrics. Our findings underscore the potential of using conversational agents to enhance statistical analysis processes, reducing cognitive load and learning curves and thereby proliferating data analysis capabilities, to individuals with limited knowledge of statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08114v1</guid>
      <category>cs.HC</category>
      <category>stat.CO</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qifu Wen (Department of Computer Science, Boston University Metropolitan College), Prishita Kochhar (Department of Computer Science, Boston University Metropolitan College), Sherif Zeyada (Department of Computer Science, Boston University Metropolitan College), Tahereh Javaheri (Department of Computer Science, Boston University Metropolitan College, Health Informatics Lab, Boston University Metropolitan College), Reza Rawassizadeh (Department of Computer Science, Boston University Metropolitan College)</dc:creator>
    </item>
    <item>
      <title>Tutorial for Surrogate Endpoint Validation Using Joint modeling and Mediation Analysis</title>
      <link>https://arxiv.org/abs/2502.08443</link>
      <description>arXiv:2502.08443v1 Announce Type: cross 
Abstract: The use of valid surrogate endpoints is an important stake in clinical research to help reduce both the duration and cost of a clinical trial and speed up the evaluation of interesting treatments. Several methods have been proposed in the statistical literature to validate putative surrogate endpoints. Two main approaches have been proposed: the meta-analytic approach and the mediation analysis approach. The former uses data from meta-analyses to derive associations measures between the surrogate and the final endpoint at the individual and trial levels. The latter rather uses the proportion of the treatment effect on the final endpoint through the surrogate as a measure of surrogacy in a causal inference framework. Both approaches have remained separated as the meta-analytic approach does not estimate the treatment effect on the final endpoint through the surrogate while the mediation analysis approach have been limited to single-trial setting. However, these two approaches are complementary. In this work we propose an approach that combines the meta-analytic and mediation analysis approaches using joint modeling for surrogate validation. We focus on the cases where the final endpoint is a time-to-event endpoint (such as time-to-death) and the surrogate is either a time-to-event or a longitudinal biomarker. Two new joint models were proposed depending on the nature of the surrogate. These model are implemented in the R package frailtypack. We illustrate the developed approaches in three applications on real datasets in oncology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08443v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Quentin Le Coent, Virginie Rondeau, Catherine Legrand</dc:creator>
    </item>
    <item>
      <title>Scalable marginalization of correlated latent variables with applications to learning particle interaction kernels</title>
      <link>https://arxiv.org/abs/2203.08389</link>
      <description>arXiv:2203.08389v5 Announce Type: replace 
Abstract: Marginalization of latent variables or nuisance parameters is a fundamental aspect of Bayesian inference and uncertainty quantification. In this work, we focus on scalable marginalization of latent variables in modeling correlated data, such as spatio-temporal or functional observations. We first introduce Gaussian processes (GPs) for modeling correlated data and highlight the computational challenge, where the computational complexity increases cubically fast along with the number of observations. We then review the connection between the state space model and GPs with Mat{\'e}rn covariance for temporal inputs. The Kalman filter and Rauch-Tung-Striebel smoother were introduced as a scalable marginalization technique for computing the likelihood and making predictions of GPs without approximation. We then introduce recent efforts on extending the scalable marginalization idea to the linear model of coregionalization for multivariate correlated output and spatio-temporal observations. In the final part of this work, we introduce a novel marginalization technique to estimate interaction kernels and forecast particle trajectories. The achievement lies in the sparse representation of covariance function, then applying conjugate gradient for solving the computational challenges and improving predictive accuracy. The computational advances achieved in this work outline a wide range of applications in molecular dynamic simulation, cellular migration, and agent-based models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.08389v5</guid>
      <category>stat.CO</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.51387/22-NEJSDS13</arxiv:DOI>
      <arxiv:journal_reference>The New England Journal of Statistics in Data Science (2023), 1(2), 172-186</arxiv:journal_reference>
      <dc:creator>Mengyang Gu, Xubo Liu, Xinyi Fang, Sui Tang</dc:creator>
    </item>
    <item>
      <title>Improving sampling efficacy on high dimensional distributions with thin high density regions using Conservative Hamiltonian Monte Carlo</title>
      <link>https://arxiv.org/abs/2206.06901</link>
      <description>arXiv:2206.06901v2 Announce Type: replace-cross 
Abstract: Hamiltonian Monte Carlo is a prominent Markov Chain Monte Carlo algorithm, which employs symplectic integrators to sample from high dimensional target distributions in many applications, such as statistical mechanics, Bayesian statistics and generative models. However, such distributions tend to have thin high density regions, posing a significant challenge for symplectic integrators to maintain the small energy errors needed for a high acceptance probability. Instead, we propose a variant called Conservative Hamiltonian Monte Carlo, using $R$--reversible energy-preserving integrators to retain a high acceptance probability. We show our algorithm can achieve approximate stationarity with an error determined by the Jacobian approximation of the energy-preserving proposal map. Numerical evidence shows improved convergence and robustness over integration parameters on target distributions with thin high density regions and in high dimensions. Moreover, a version of our algorithm can also be applied to target distributions without gradient information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.06901v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Geoffrey McGregor, Andy T. S. Wan</dc:creator>
    </item>
    <item>
      <title>CMHSU: An R Statistical Software Package to Detect Mental Health Status, Substance Use Status, and their Concurrent Status in the North American Healthcare Administrative Databases</title>
      <link>https://arxiv.org/abs/2501.06435</link>
      <description>arXiv:2501.06435v2 Announce Type: replace-cross 
Abstract: The concept of concurrent mental health and substance use (MHSU) and its detection in patients has garnered growing interest among psychiatrists and healthcare policymakers over the past four decades. Researchers have proposed various diagnostic methods, including the Data-Driven Diagnostic Method (DDDM), for the identification of MHSU. However, the absence of a standalone statistical software package to facilitate DDDM for large healthcare administrative databases has remained a significant gap. This paper introduces the R statistical software package CMHSU , available on the Comprehensive R Archive Network (CRAN), for the diagnosis of mental health (MH), substance use (SU), and their concurrent status (MHSU). The package implements DDDM using hospital and medical service physician visit counts along with maximum time span parameters for MH, SU, and MHSU diagnoses. A working example with a simulated real-world dataset is presented to explore three critical dimensions of MHSU detection based on the DDDM. Additionally, the limitations of the CMHSU package and potential directions for its future extension are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06435v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohsen Soltanifar, Chel Hee Lee</dc:creator>
    </item>
  </channel>
</rss>
