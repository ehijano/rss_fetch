<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 09 Sep 2025 02:22:38 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Mechanisms for anesthesia, unawareness, respiratory depression, memory replay and sleep: MHb &gt; IPN &gt; PAG + DRN + MRN &gt; claustrum &gt; cortical slow-waves</title>
      <link>https://arxiv.org/abs/2509.04454</link>
      <description>arXiv:2509.04454v1 Announce Type: new 
Abstract: My findings show, for the first time, what causes loss of awareness, anesthesia, memory replay, opioids induced respiratory depression (OIRD), and slow wave sleep. Opiates are fast pain relievers and anesthetics that can cause respiratory arrest. I found how mu-opioids and other medial habenula activators slowdown respiration during SWS and anesthesia. Using DTI method I observed that human hippocampus is connected to MHb via posterior septum, while amygdala via anteromedial BNST. MHb projected to pineal gland and contralateral MHb (Vadovi\v{c}ov\'a, 2014). MHb has dense mu-opioids receptors (Gardon and Faget, 2014) and strong projections to IPN. Herkenham (1981) found increased glucose intake during anesthesia in MHb and IPN. The IPN projects to serotonergic MRN/DRN, and pain/interoception/arousal linked PAG. The question is: What is the MHb-IPN circuit doing? This extended circuit model explains role of the dentate gyrus &gt;posterior septum &gt;MHb &gt;IPN &gt;MRN &gt;hippocampus + BF + claustrum &gt;cortical slow-wave activity (SWA) in memory replay, loss of awareness, anesthesia and SWS. It proposes new neural mechanisms for anesthetic ketamine, nitrous oxide, and phencyclidine effects: activation of the IPN &gt;MRN &gt;claustrum &gt;cortical SWA circuit by the 5-HT2a receptors in the IPN and claustrum. This brain model shows why are ketamine and psychedelics anxiolytic and antidepressant. How they by activating the 5-HT2a receptors in vACC/infralimbic cortex increase safety, well-being signal, socializing, and cognitive flexibility, and attenuate fear, worries, anger, impulsivity, self-defence and wanting. This model suggests that mu-opioids, acetylcholine, nicotine, cannabinoids, adenosine, GLP-1RA, neuropeptide Y, and substance P activate the MHb-IPN-MRN circuit which promotes rest, recovery, repair, serotonin-BDNF-proteins production-spines/synapses growth-anti-inflammatory state.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04454v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Karin Vadovi\v{c}ov\'a</dc:creator>
    </item>
    <item>
      <title>Network Models of Neurodegeneration: Bridging Neuronal Dynamics and Disease Progression</title>
      <link>https://arxiv.org/abs/2509.05151</link>
      <description>arXiv:2509.05151v1 Announce Type: new 
Abstract: Neurodegenerative diseases are characterized by the accumulation of misfolded proteins and widespread disruptions in brain function. Computational modeling has advanced our understanding of these processes, but efforts have traditionally focused on either neuronal dynamics or the underlying biological mechanisms of disease. One class of models uses neural mass and whole-brain frameworks to simulate changes in oscillations, connectivity, and network stability. A second class focuses on biological processes underlying disease progression, particularly prion-like propagation through the connectome, and glial responses and vascular mechanisms. Each modeling tradition has provided important insights, but experimental evidence shows these processes are interconnected: neuronal activity modulates protein release and clearance, while pathological burden feeds back to disrupt circuit function. Modeling these domains in isolation limits our understanding. To determine where and why disease emerges, how it spreads, and how it might be altered, we must develop integrated frameworks that capture feedback between neuronal dynamics and disease biology. In this review, we survey the two modeling approaches and highlight efforts to unify them. We argue that such integration is necessary to address key questions in neurodegeneration and to inform interventions, from targeted stimulation to control-theoretic strategies that slow progression and restore function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05151v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christoffer G. Alexandersen, Georgia S. Brennan, Julia K. Brynildsen, Michael X. Henderson, Yasser Iturria-Medina, Dani S. Bassett</dc:creator>
    </item>
    <item>
      <title>Scaling Environments for Organoid Intelligence with LLM-Automated Design and Plasticity-Based Evaluation</title>
      <link>https://arxiv.org/abs/2509.04633</link>
      <description>arXiv:2509.04633v1 Announce Type: cross 
Abstract: As the complexity of artificial agents increases, the design of environments that can effectively shape their behavior and capabilities has become a critical research frontier. We propose a framework that extends this principle to a novel class of agents: biological neural networks in the form of neural organoids. This paper introduces three scalable, closed-loop virtual environments designed to train organoid-based biological agents and probe the underlying mechanisms of learning, such as long-term potentiation (LTP) and long-term depression (LTD). We detail the design of three distinct task environments with increasing complexity: (1) a conditional avoidance task, (2) a one-dimensional predator-prey scenario, and (3) a replication of the classic Pong game. For each environment, we formalize the state and action spaces, the sensory encoding and motor decoding mechanisms, and the feedback protocols based on predictable (reward) and unpredictable (punishment) stimulation. Furthermore, we propose a novel meta-learning approach where a Large Language Model (LLM) is used to automate the generation and optimization of experimental protocols, scaling the process of environment and curriculum design. Finally, we outline a multi-modal approach for evaluating learning by measuring synaptic plasticity at electrophysiological, cellular, and molecular levels. This work bridges the gap between computational neuroscience and agent-based AI, offering a unique platform for studying embodiment, learning, and intelligence in a controlled biological substrate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04633v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brennen Hill</dc:creator>
    </item>
    <item>
      <title>Dynamical Learning in Deep Asymmetric Recurrent Neural Networks</title>
      <link>https://arxiv.org/abs/2509.05041</link>
      <description>arXiv:2509.05041v1 Announce Type: cross 
Abstract: We show that asymmetric deep recurrent neural networks, enhanced with additional sparse excitatory couplings, give rise to an exponentially large, dense accessible manifold of internal representations which can be found by different algorithms, including simple iterative dynamics. Building on the geometrical properties of the stable configurations, we propose a distributed learning scheme in which input-output associations emerge naturally from the recurrent dynamics, without any need of gradient evaluation. A critical feature enabling the learning process is the stability of the configurations reached at convergence, even after removal of the supervisory output signal. Extensive simulations demonstrate that this approach performs competitively on standard AI benchmarks. The model can be generalized in multiple directions, both computational and biological, potentially contributing to narrowing the gap between AI and computational neuroscience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05041v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Davide Badalotti, Carlo Baldassi, Marc M\'ezard, Mattia Scardecchia, Riccardo Zecchina</dc:creator>
    </item>
    <item>
      <title>Stochastic synaptic dynamics under learning</title>
      <link>https://arxiv.org/abs/2508.13846</link>
      <description>arXiv:2508.13846v2 Announce Type: replace 
Abstract: Learning is based on synaptic plasticity, which affects and is driven by neural activity. Because pre- and postsynaptic spiking activity is shaped by randomness, the synaptic weights follow a stochastic process, requiring a probabilistic framework to capture the noisy synaptic dynamics. We consider a paradigmatic supervised learning example: a presynaptic neural population impinging in a sequence of episodes on a recurrent network of integrate-and-fire neurons through synapses undergoing spike-timing-dependent plasticity (STDP) with additive potentiation and multiplicative depression. We first analytically compute the drift- and diffusion coefficients for a single synapse within a single episode (microscopic dynamics), mapping the true jump process to a Langevin and the associated Fokker-Planck equations. Leveraging new analytical tools, we include spike-time--resolving cross-correlations between pre- and postsynaptic spikes, which corrects substantial deviations seen in standard theories purely based on firing rates. We then apply this microdynamical description to the network setup in which hetero-associations are trained over one-shot episodes into a feed-forward matrix of STDP synapses connecting to neurons of the recurrent network (macroscopic dynamics). By mapping statistically distinct synaptic populations to instances of the single-synapse process above, we self-consistently determine the joint neural and synaptic dynamics and, ultimately, the time course of memory degradation and the memory capacity. We demonstrate that specifically in the relevant case of sparse coding, our theory can quantitatively capture memory capacities which are strongly overestimated if spike-time--resolving cross-correlations are ignored. We conclude with a discussion of the many directions in which our framework can be extended.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13846v2</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <category>physics.bio-ph</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jakob Stubenrauch, Naomi Auer, Richard Kempter, Benjamin Lindner</dc:creator>
    </item>
    <item>
      <title>Net2Brain: A Toolbox to compare artificial vision models with human brain responses</title>
      <link>https://arxiv.org/abs/2208.09677</link>
      <description>arXiv:2208.09677v4 Announce Type: replace-cross 
Abstract: We introduce Net2Brain, a graphical and command-line user interface toolbox for comparing the representational spaces of artificial deep neural networks (DNNs) and human brain recordings. While different toolboxes facilitate only single functionalities or only focus on a small subset of supervised image classification models, Net2Brain allows the extraction of activations of more than 600 DNNs trained to perform a diverse range of vision-related tasks (e.g semantic segmentation, depth estimation, action recognition, etc.), over both image and video datasets. The toolbox computes the representational dissimilarity matrices (RDMs) over those activations and compares them to brain recordings using representational similarity analysis (RSA), weighted RSA, both in specific ROIs and with searchlight search. In addition, it is possible to add a new data set of stimuli and brain recordings to the toolbox for evaluation. We demonstrate the functionality and advantages of Net2Brain with an example showcasing how it can be used to test hypotheses of cognitive computational neuroscience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.09677v4</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.3389/fninf.2025.1515873</arxiv:DOI>
      <arxiv:journal_reference>Front. Neuroinform. 19 (2025) 1515873</arxiv:journal_reference>
      <dc:creator>Domenic Bersch, Kshitij Dwivedi, Martina Vilas, Radoslaw M. Cichy, Gemma Roig</dc:creator>
    </item>
    <item>
      <title>The dynamic interplay between in-context and in-weight learning in humans and neural networks</title>
      <link>https://arxiv.org/abs/2402.08674</link>
      <description>arXiv:2402.08674v5 Announce Type: replace-cross 
Abstract: Human learning embodies a striking duality: sometimes, we appear capable of following logical, compositional rules and benefit from structured curricula (e.g., in formal education), while other times, we rely on an incremental approach or trial-and-error, learning better from curricula that are randomly interleaved. Influential psychological theories explain this seemingly disparate behavioral evidence by positing two qualitatively different learning systems -- one for rapid, rule-based inferences and another for slow, incremental adaptation. It remains unclear how to reconcile such theories with neural networks, which learn via incremental weight updates and are thus a natural model for the latter type of learning, but are not obviously compatible with the former. However, recent evidence suggests that metalearning neural networks and large language models are capable of "in-context learning" (ICL) -- the ability to flexibly grasp the structure of a new task from a few examples. Here, we show that the dynamic interplay between ICL and default in-weight learning (IWL) naturally captures a broad range of learning phenomena observed in humans, reproducing curriculum effects on category-learning and compositional tasks, and recapitulating a tradeoff between flexibility and retention. Our work shows how emergent ICL can equip neural networks with fundamentally different learning properties that can coexist with their native IWL, thus offering a novel perspective on dual-process theories and human cognitive flexibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08674v5</guid>
      <category>cs.NE</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1073/pnas.2510270122</arxiv:DOI>
      <arxiv:journal_reference>Published in Proceedings of the National Academy of Sciences, U.S.A., 122 (35) e251027012 (2025)</arxiv:journal_reference>
      <dc:creator>Jacob Russin, Ellie Pavlick, Michael J. Frank</dc:creator>
    </item>
    <item>
      <title>Revealing higher-order neural representations of uncertainty with the Noise Estimation through Reinforcement-based Diffusion (NERD) model</title>
      <link>https://arxiv.org/abs/2503.14333</link>
      <description>arXiv:2503.14333v3 Announce Type: replace-cross 
Abstract: Studies often aim to reveal ``first-order" representations (FORs), which encode aspects of an observer's environment, such as contents or structure. A less-common target is ``higher-order" representations (HORs), which are ``about" FORs -- e.g., their strength or uncertainty -- and which may contribute to learning. HORs about uncertainty are unlikely to be direct ``read-outs" of FOR characteristics, instead reflecting noisy estimation processes incorporating prior expectations about uncertainty, but how the brain represents such expected uncertainty distributions remains largely unexplored. Here, we study ``noise expectation" HORs using neural data from a task which may require the brain to learn about its own noise: decoded neurofeedback, wherein human subjects learn to volitionally produce target neural patterns. We develop and apply a Noise Estimation through Reinforcement-based Diffusion (NERD) model to characterize how brains may undertake this process, and show that NERD offers high explanatory power for human behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14333v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hojjat Azimi Asrari, Megan A. K. Peters</dc:creator>
    </item>
  </channel>
</rss>
