<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 17 Apr 2025 01:47:22 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Mitigating Eddington and Malmquist Biases in Latent-Inclination Regression of the Tully-Fisher Relation</title>
      <link>https://arxiv.org/abs/2504.10589</link>
      <description>arXiv:2504.10589v1 Announce Type: new 
Abstract: Precise estimation of the Tully-Fisher relation is compromised by statistical biases and uncertain inclination corrections. To account for selection effects (Malmquist bias) while avoiding individual inclination corrections, I introduce a Bayesian method based on likelihood functions that incorporate Sine-distributed scatter of rotation velocities, Gaussian scatter from intrinsic dispersion and measurement error, and the observational selection function. However, tests of unidirectional models on simulated datasets reveal an additional bias arising from neglect of the Gaussian scatter in the independent variable. This additional bias is identified as a generalized Eddington bias, which distorts the data distribution independently of Malmuqist bias. I introduce two extensions to the Bayesian method that successfully mitigate the Eddington bias: (1) analytical bias corrections of the dependent variable prior to likelihood computation, and (2) a bidirectional dual-scatter model that includes the Gaussian scatter of the independent variable in the likelihood function. By rigorously accounting for Malmquist and Eddington biases in a latent-inclination regression analysis, this work establishes a framework for unbiased distance estimates from standardizable candles, critical for improving determinations of the Hubble constant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10589v1</guid>
      <category>stat.ME</category>
      <category>astro-ph.GA</category>
      <category>astro-ph.IM</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hai Fu</dc:creator>
    </item>
    <item>
      <title>Enhancing the Tensor Normal via Geometrically Parameterized Cholesky Factors</title>
      <link>https://arxiv.org/abs/2504.10645</link>
      <description>arXiv:2504.10645v1 Announce Type: new 
Abstract: In this article, we explore Bayesian extensions of the tensor normal model through a geometric expansion of the multi-way covariance's Cholesky factor inspired by the Fr\'echet mean under the log-Cholesky metric. Specifically, within a tensor normal framework, we identify three structural components in the covariance of the vectorized data. By parameterizing vector normal covariances through such a Cholesky factor representation, analogous to a finite average of multiway Cholesky factors, we eliminate one of these structural components without compromising the analytical tractability of the likelihood, in which the multiway covariance is a special case. Furthermore, we demonstrate that a specific class of structured Cholesky factors can be precisely represented under this parameterization, serving as an analogue to the Pitsianis-Van Loan decomposition. We apply this model using Hamiltonian Monte Carlo in a fixed-mean setting for two-way covariance relevancy detection of components, where efficient analytical gradient updates are available, as well as in a seasonally-varying covariance process regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10645v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Quinn Simonis, Martin T. Wells</dc:creator>
    </item>
    <item>
      <title>A Nonparametric Bayesian Local-Global Model for Enhanced Adverse Event Signal Detection in Spontaneous Reporting System Data</title>
      <link>https://arxiv.org/abs/2504.10881</link>
      <description>arXiv:2504.10881v1 Announce Type: new 
Abstract: Spontaneous reporting system databases are key resources for post-marketing surveillance, providing real-world evidence (RWE) on the adverse events (AEs) of regulated drugs or other medical products. Various statistical methods have been proposed for AE signal detection in these databases, flagging drug-specific AEs with disproportionately high observed counts compared to expected counts under independence. However, signal detection remains challenging for rare AEs or newer drugs, which receive small observed and expected counts and thus suffer from reduced statistical power. Principled information sharing on signal strengths across drugs/AEs is crucial in such cases to enhance signal detection. However, existing methods typically ignore complex between-drug associations on AE signal strengths, limiting their ability to detect signals. We propose novel local-global mixture Dirichlet process (DP) prior-based nonparametric Bayesian models to capture these associations, enabling principled information sharing between drugs while balancing flexibility and shrinkage for each drug, thereby enhancing statistical power. We develop efficient Markov chain Monte Carlo algorithms for implementation and employ a false discovery rate (FDR)-controlled, false negative rate (FNR)-optimized hypothesis testing framework for AE signal detection. Extensive simulations demonstrate our methods' superior sensitivity -- often surpassing existing approaches by a twofold or greater margin -- while strictly controlling the FDR. An application to FDA FAERS data on statin drugs further highlights our methods' effectiveness in real-world AE signal detection. Software implementing our methods is provided as supplementary material.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10881v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Xin-Wei Huang, Saptarshi Chakraborty</dc:creator>
    </item>
    <item>
      <title>A conceptual synthesis of causal assumptions for causal discovery and inference</title>
      <link>https://arxiv.org/abs/2504.11035</link>
      <description>arXiv:2504.11035v1 Announce Type: new 
Abstract: This work presents a conceptual synthesis of causal discovery and inference frameworks, with a focus on how foundational assumptions -- causal sufficiency, causal faithfulness, and the causal Markov condition -- are formalized and operationalized across methodological traditions. Through structured tables and comparative summaries, I map core assumptions, tasks, and analytical choices from multiple causal frameworks, highlighting their connections and differences. The synthesis provides practical guidance for researchers designing causal studies, especially in settings where observational or experimental constraints challenge standard approaches. This guide spans all phases of causal analysis, including question formulation, formalization of background knowledge, selection of appropriate frameworks, choice of study design or algorithm, and interpretation. It is intended as a tool to support rigorous causal reasoning across diverse empirical domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11035v1</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hannah E. Correia</dc:creator>
    </item>
    <item>
      <title>Using Time Structure to Estimate Causal Effects</title>
      <link>https://arxiv.org/abs/2504.11076</link>
      <description>arXiv:2504.11076v1 Announce Type: new 
Abstract: There exist several approaches for estimating causal effects in time series when latent confounding is present. Many of these approaches rely on additional auxiliary observed variables or time series such as instruments, negative controls or time series that satisfy the front- or backdoor criterion in certain graphs. In this paper, we present a novel approach for estimating direct (and via Wright's path rule total) causal effects in a time series setup which does not rely on additional auxiliary observed variables or time series. This approach assumes that the underlying time series is a Structural Vector Autoregressive (SVAR) process and estimates direct causal effects by solving certain linear equation systems made up of different covariances and model parameters. We state sufficient graphical criteria in terms of the so-called full time graph under which these linear equations systems are uniquely solvable and under which their solutions contain the to-be-identified direct causal effects as components. We also state sufficient lag-based criteria under which the previously mentioned graphical conditions are satisfied and, thus, under which direct causal effects are identifiable. Several numerical experiments underline the correctness and applicability of our results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11076v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tom Hochsprung, Jakob Runge, Andreas Gerhardus</dc:creator>
    </item>
    <item>
      <title>Spatial Sign based Direct Sparse Linear Discriminant Analysis for High Dimensional Data</title>
      <link>https://arxiv.org/abs/2504.11117</link>
      <description>arXiv:2504.11117v1 Announce Type: new 
Abstract: This paper investigates the robust linear discriminant analysis (LDA) problem with elliptical distributions in high-dimensional data. We propose a robust classification method, named SSLDA, that is intended to withstand heavy-tailed distributions. We demonstrate that SSLDA achieves an optimal convergence rate in terms of both misclassification rate and estimate error. Our theoretical results are further confirmed by extensive numerical experiments on both simulated and real datasets. Compared with current approaches, the SSLDA method offers superior improved finite sample performance and notable robustness against heavy-tailed distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11117v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dan Zhuang, Long Feng</dc:creator>
    </item>
    <item>
      <title>Robust Bayesian Inference for Censored Survival Models</title>
      <link>https://arxiv.org/abs/2504.11147</link>
      <description>arXiv:2504.11147v1 Announce Type: new 
Abstract: This paper proposes a robust Bayesian accelerated failure time model for censored survival data. We develop a new family of life-time distributions using a scale mixture of the generalized gamma distributions, where we propose a novel super heavy-tailed distribution as a mixing density. We theoretically show that, under some conditions, the proposed method satisfies the full posterior robustness, which guarantees robustness of point estimation as well as uncertainty quantification. For posterior computation, we employ an integral expression of the proposed heavy-tailed distribution to develop an efficient posterior computation algorithm based on the Markov chain Monte Carlo. The performance of the proposed method is illustrated through numerical experiments and real data example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11147v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yasuyuki Hamura, Takahiro Onizuka, Shintaro Hashimoto, Shonosuke Sugasawa</dc:creator>
    </item>
    <item>
      <title>A Spatial-Sign based Direct Approach for High Dimensional Sparse Quadratic Discriminant Analysis</title>
      <link>https://arxiv.org/abs/2504.11187</link>
      <description>arXiv:2504.11187v1 Announce Type: new 
Abstract: In this paper, we study the problem of high-dimensional sparse quadratic discriminant analysis (QDA). We propose a novel classification method, termed SSQDA, which is constructed via constrained convex optimization based on the sample spatial median and spatial sign covariance matrix under the assumption of an elliptically symmetric distribution. The proposed classifier is shown to achieve the optimal convergence rate over a broad class of parameter spaces, up to a logarithmic factor. Extensive simulation studies and real data applications demonstrate that SSQDA is both robust and efficient, particularly in the presence of heavy-tailed distributions, highlighting its practical advantages in high-dimensional classification tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11187v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anqing Shen, Long Feng</dc:creator>
    </item>
    <item>
      <title>Statistical few-shot learning for large-scale classification via parameter pooling</title>
      <link>https://arxiv.org/abs/2504.11404</link>
      <description>arXiv:2504.11404v1 Announce Type: new 
Abstract: In large-scale few-shot learning for classification problems, often there are a large number of classes and few high-dimensional observations per class. Previous model-based methods, such as Fisher's linear discriminant analysis (LDA), require the strong assumptions of a shared covariance matrix between all classes. Quadratic discriminant analysis will often lead to singular or unstable covariance matrix estimates. Both of these methods can lead to lower-than-desired classification performance. We introduce a novel, model-based clustering method that can relax the shared covariance assumptions of LDA by clustering sample covariance matrices, either singular or non-singular. In addition, we study the statistical properties of parameter estimates. This will lead to covariance matrix estimates which are pooled within each cluster of classes. We show, using simulated and real data, that our classification method tends to yield better discrimination compared to other methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11404v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Andrew Simpson, Semhar Michael</dc:creator>
    </item>
    <item>
      <title>Who is More Bayesian: Humans or ChatGPT?</title>
      <link>https://arxiv.org/abs/2504.10636</link>
      <description>arXiv:2504.10636v1 Announce Type: cross 
Abstract: We compare the performance of human and artificially intelligent (AI) decision makers in simple binary classification tasks where the optimal decision rule is given by Bayes Rule. We reanalyze choices of human subjects gathered from laboratory experiments conducted by El-Gamal and Grether and Holt and Smith. We confirm that while overall, Bayes Rule represents the single best model for predicting human choices, subjects are heterogeneous and a significant share of them make suboptimal choices that reflect judgement biases described by Kahneman and Tversky that include the ``representativeness heuristic'' (excessive weight on the evidence from the sample relative to the prior) and ``conservatism'' (excessive weight on the prior relative to the sample). We compare the performance of AI subjects gathered from recent versions of large language models (LLMs) including several versions of ChatGPT. These general-purpose generative AI chatbots are not specifically trained to do well in narrow decision making tasks, but are trained instead as ``language predictors'' using a large corpus of textual data from the web. We show that ChatGPT is also subject to biases that result in suboptimal decisions. However we document a rapid evolution in the performance of ChatGPT from sub-human performance for early versions (ChatGPT 3.5) to superhuman and nearly perfect Bayesian classifications in the latest versions (ChatGPT 4o).</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10636v1</guid>
      <category>econ.GN</category>
      <category>cs.AI</category>
      <category>q-fin.EC</category>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianshi Mu, Pranjal Rawat, John Rust, Chengjun Zhang, Qixuan Zhong</dc:creator>
    </item>
    <item>
      <title>Bayesian analysis of regression discontinuity designs with heterogeneous treatment effects</title>
      <link>https://arxiv.org/abs/2504.10652</link>
      <description>arXiv:2504.10652v1 Announce Type: cross 
Abstract: Regression Discontinuity Design (RDD) is a popular framework for estimating a causal effect in settings where treatment is assigned if an observed covariate exceeds a fixed threshold. We consider estimation and inference in the common setting where the sample consists of multiple known sub-populations with potentially heterogeneous treatment effects. In the applied literature, it is common to account for heterogeneity by either fitting a parametric model or considering each sub-population separately. In contrast, we develop a Bayesian hierarchical model using Gaussian process regression which allows for non-parametric regression while borrowing information across sub-populations. We derive the posterior distribution, prove posterior consistency, and develop a Metropolis-Hastings within Gibbs sampling algorithm. In extensive simulations, we show that the proposed procedure outperforms existing methods in both estimation and inferential tasks. Finally, we apply our procedure to U.S. Senate election data and discover an incumbent party advantage which is heterogeneous over different time periods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10652v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Kevin Tao, Y. Samuel Wang, David Ruppert</dc:creator>
    </item>
    <item>
      <title>Optimal inference for the mean of random functions</title>
      <link>https://arxiv.org/abs/2504.11025</link>
      <description>arXiv:2504.11025v1 Announce Type: cross 
Abstract: We study estimation and inference for the mean of real-valued random functions defined on a hypercube. The independent random functions are observed on a discrete, random subset of design points, possibly with heteroscedastic noise. We propose a novel optimal-rate estimator based on Fourier series expansions and establish a sharp non-asymptotic error bound in $L^2-$norm. Additionally, we derive a non-asymptotic Gaussian approximation bound for our estimated Fourier coefficients. Pointwise and uniform confidence sets are constructed. Our approach is made adaptive by a plug-in estimator for the H\"older regularity of the mean function, for which we derive non-asymptotic concentration bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11025v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Omar Kassi, Valentin Patilea</dc:creator>
    </item>
    <item>
      <title>On relative universality, regression operator, and conditional independence</title>
      <link>https://arxiv.org/abs/2504.11044</link>
      <description>arXiv:2504.11044v1 Announce Type: cross 
Abstract: The notion of relative universality with respect to a {\sigma}-field was introduced to establish the unbiasedness and Fisher consistency of an estimator in nonlinear sufficient dimension reduction. However, there is a gap in the proof of this result in the existing literature. The existing definition of relative universality seems to be too strong for the proof to be valid. In this note we modify the definition of relative universality using the concept of \k{o}-measurability, and rigorously establish the mentioned unbiasedness and Fisher consistency. The significance of this result is beyond its original context of sufficient dimension reduction, because relative universality allows us to use the regression operator to fully characterize conditional independence, a crucially important statistical relation that sits at the core of many areas and methodologies in statistics and machine learning, such as dimension reduction, graphical models, probability embedding, causal inference, and Bayesian estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11044v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bing Li, Ben Jones, Andreas Artemiou</dc:creator>
    </item>
    <item>
      <title>Simulation-based inference for stochastic nonlinear mixed-effects models with applications in systems biology</title>
      <link>https://arxiv.org/abs/2504.11279</link>
      <description>arXiv:2504.11279v1 Announce Type: cross 
Abstract: The analysis of data from multiple experiments, such as observations of several individuals, is commonly approached using mixed-effects models, which account for variation between individuals through hierarchical representations. This makes mixed-effects models widely applied in fields such as biology, pharmacokinetics, and sociology. In this work, we propose a novel methodology for scalable Bayesian inference in hierarchical mixed-effects models. Our framework first constructs amortized approximations of the likelihood and the posterior distribution, which are then rapidly refined for each individual dataset, to ultimately approximate the parameters posterior across many individuals. The framework is easily trainable, as it uses mixtures of experts but without neural networks, leading to parsimonious yet expressive surrogate models of the likelihood and the posterior. We demonstrate the effectiveness of our methodology using challenging stochastic models, such as mixed-effects stochastic differential equations emerging in systems biology-driven problems. However, the approach is broadly applicable and can accommodate both stochastic and deterministic models. We show that our approach can seamlessly handle inference for many parameters. Additionally, we applied our method to a real-data case study of mRNA transfection. When compared to exact pseudomarginal Bayesian inference, our approach proved to be both fast and competitive in terms of statistical accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11279v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Henrik H\"aggstr\"om, Sebastian Persson, Marija Cvijovic, Umberto Picchini</dc:creator>
    </item>
    <item>
      <title>Asymptotic properties of generalized closed-form maximum likelihood estimators</title>
      <link>https://arxiv.org/abs/2102.07356</link>
      <description>arXiv:2102.07356v4 Announce Type: replace 
Abstract: The maximum likelihood estimator (MLE) is pivotal in statistical inference, yet its application is often hindered by the absence of closed-form solutions for many models. This poses challenges in real-time computation scenarios, particularly within embedded systems technology, where numerical methods are impractical. This study introduces a generalized form of the MLE that yields closed-form estimators under certain conditions. We derive the asymptotic properties of the proposed estimator and demonstrate that our approach retains key properties such as invariance under one-to-one transformations, strong consistency, and an asymptotic normal distribution. The effectiveness of the generalized MLE is exemplified through its application to the Gamma, Nakagami, and Beta distributions, showcasing improvements over the traditional MLE. Additionally, we extend this methodology to a bivariate gamma distribution, successfully deriving closed-form estimators. This advancement presents significant implications for real-time statistical analysis across various applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2102.07356v4</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pedro L. Ramos, Eduardo Ramos, Francisco A. Rodrigues, Francisco Louzada</dc:creator>
    </item>
    <item>
      <title>Scalable Variational Bayes Inference for Dynamic Variable Selection</title>
      <link>https://arxiv.org/abs/2304.07096</link>
      <description>arXiv:2304.07096v2 Announce Type: replace 
Abstract: We develop a variational Bayes approach for dynamic variable selection in high-dimensional regression models with time-varying parameters and predictors that exhibit a predefined group structure. Through comprehensive simulation studies, we demonstrate that our method yields more accurate parameter estimates than existing Bayesian static and dynamic variable selection approaches while maintaining computational efficiency. We illustrate the performance of our approach within the context of a popular problem in economics: forecasting inflation based on a large set of macroeconomic predictors. Our approach demonstrates significant improvements in out-of-sample point and density forecasting accuracy. A retrospective analysis of the time-varying parameter estimates reveals economically interpretable patterns in inflation dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.07096v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Bianco, Mauro Bernardi, Daniele Bianchi</dc:creator>
    </item>
    <item>
      <title>High-dimensional statistical inference for linkage disequilibrium score regression and its cross-ancestry extensions</title>
      <link>https://arxiv.org/abs/2306.15779</link>
      <description>arXiv:2306.15779v2 Announce Type: replace 
Abstract: Linkage disequilibrium score regression (LDSC) has emerged as an essential tool for genetic and genomic analyses of complex traits, utilizing high-dimensional data derived from genome-wide association studies (GWAS). LDSC computes the linkage disequilibrium (LD) scores using an external reference panel, and integrates the LD scores with only summary data from the original GWAS. In this paper, we investigate LDSC within a fixed-effect data integration framework, underscoring its ability to merge multi-source GWAS data and reference panels. In particular, we take account of the genome-wide dependence among the high-dimensional GWAS summary statistics, along with the block-diagonal dependence pattern in estimated LD scores. Our analysis uncovers several key factors of both the original GWAS and reference panel datasets that determine the performance of LDSC. We show that it is relatively feasible for LDSC-based estimators to achieve asymptotic normality when applied to genome-wide genetic variants (e.g., in genetic variance and covariance estimation), whereas it becomes considerably challenging when we focus on a much smaller subset of genetic variants (e.g., in partitioned heritability analysis). Moreover, by modeling the disparities in LD patterns across different populations, we unveil that LDSC can be expanded to conduct cross-ancestry analyses using data from distinct global populations (such as European and Asian). We validate our theoretical findings through extensive numerical evaluations using real genetic data from the UK Biobank study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15779v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fei Xue, Bingxin Zhao</dc:creator>
    </item>
    <item>
      <title>New flexible versions of extended generalized Pareto model for count data</title>
      <link>https://arxiv.org/abs/2409.18719</link>
      <description>arXiv:2409.18719v2 Announce Type: replace 
Abstract: Accurate modeling is essential in integer-valued real phenomena, including the distribution of entire data, zero-inflated (ZI) data, and discrete exceedances. The Poisson and Negative Binomial distributions, along with their ZI variants, are considered suitable for modeling the entire data distribution, but they fail to capture the heavy tail behavior effectively alongside the bulk of the distribution. In contrast, the discrete generalized Pareto distribution (DGPD) is preferred for high threshold exceedances, but it becomes less effective for low threshold exceedances. However, in some applications, the selection of a suitable high threshold is challenging, and the asymptotic conditions required for using DGPD are not always met. To address these limitations, extended versions of DGPD are proposed. These extensions are designed to model one of three scenarios: first, the entire distribution of the data, including both bulk and tail and bypassing the threshold selection step; second, the entire distribution along with ZI; and third, the tail of the distribution for low threshold exceedances. The proposed extensions offer improved estimates across all three scenarios compared to existing models, providing more accurate and reliable results in simulation studies and real data applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18719v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Touqeer Ahmad, Irshad Ahmad Arshad</dc:creator>
    </item>
    <item>
      <title>Model-free Estimation of Latent Structure via Multiscale Nonparametric Maximum Likelihood</title>
      <link>https://arxiv.org/abs/2410.22248</link>
      <description>arXiv:2410.22248v2 Announce Type: replace 
Abstract: Multivariate distributions often carry latent structures that are difficult to identify and estimate, and which better reflect the data generating mechanism than extrinsic structures exhibited simply by the raw data. In this paper, we propose a model-free approach for estimating such latent structures whenever they are present, without assuming they exist a priori. Given an arbitrary density $p_0$, we construct a multiscale representation of the density and propose data-driven methods for selecting representative models that capture meaningful discrete structure. Our approach uses a nonparametric maximum likelihood estimator to estimate the latent structure at different scales and we further characterize their asymptotic limits. By carrying out such a multiscale analysis, we obtain coarseto-fine structures inherent in the original distribution, which are integrated via a model selection procedure to yield an interpretable discrete representation of it. As an application, we design a clustering algorithm based on the proposed procedure and demonstrate its effectiveness in capturing a wide range of latent structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22248v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bryon Aragam, Ruiyi Yang</dc:creator>
    </item>
    <item>
      <title>Design-based causal inference in bipartite experiments</title>
      <link>https://arxiv.org/abs/2501.09844</link>
      <description>arXiv:2501.09844v2 Announce Type: replace 
Abstract: Bipartite experiments arise in various fields, in which the treatments are randomized over one set of units, while the outcomes are measured over another separate set of units. However, existing methods often rely on strong model assumptions about the data-generating process. Under the potential outcomes formulation, we explore design-based causal inference in bipartite experiments under weak assumptions by leveraging the sparsity structure of the bipartite graph that connects the treatment units and outcome units. We make several contributions. First, we formulate the causal inference problem under the design-based framework that can account for the bipartite interference. Second, we propose a consistent point estimator for the total treatment effect, a policy-relevant parameter that measures the difference in the outcome means if all treatment units receive the treatment or control. Third, we establish a central limit theorem for the estimator and propose a conservative variance estimator for statistical inference. Fourth, we discuss a covariate adjustment strategy to enhance estimation efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09844v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sizhu Lu, Lei Shi, Yue Fang, Wenxin Zhang, Peng Ding</dc:creator>
    </item>
    <item>
      <title>Cramming Contextual Bandits for On-policy Statistical Evaluation</title>
      <link>https://arxiv.org/abs/2403.07031</link>
      <description>arXiv:2403.07031v2 Announce Type: replace-cross 
Abstract: We introduce the cram method as a general statistical framework for evaluating the final learned policy from a multi-armed contextual bandit algorithm, using the dataset generated by the same bandit algorithm. The proposed on-policy evaluation methodology differs from most existing methods that focus on off-policy performance evaluation of contextual bandit algorithms. Cramming utilizes an entire bandit sequence through a single pass of data, leading to both statistically and computationally efficient evaluation. We prove that if a bandit algorithm satisfies a certain stability condition, the resulting crammed evaluation estimator is consistent and asymptotically normal under mild regularity conditions. Furthermore, we show that this stability condition holds for commonly used linear contextual bandit algorithms, including epsilon-greedy, Thompson Sampling, and Upper Confidence Bound algorithms. Using both synthetic and publicly available datasets, we compare the empirical performance of cramming with the state-of-the-art methods. The results demonstrate that the proposed cram method reduces the evaluation standard error by approximately 40% relative to off-policy evaluation methods while preserving unbiasedness and valid confidence interval coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07031v2</guid>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zeyang Jia, Kosuke Imai, Michael Lingzhi Li</dc:creator>
    </item>
    <item>
      <title>Bridging Root-$n$ and Non-standard Asymptotics: Adaptive Inference in M-Estimation</title>
      <link>https://arxiv.org/abs/2501.07772</link>
      <description>arXiv:2501.07772v3 Announce Type: replace-cross 
Abstract: This manuscript studies a general approach to construct confidence sets for the solution of population-level optimization, commonly referred to as M-estimation. Statistical inference for M-estimation poses significant challenges due to the non-standard limiting behaviors of the corresponding estimator, which arise in settings with increasing dimension of parameters, non-smooth objectives, or constraints. We propose a simple and unified method that guarantees validity in both regular and irregular cases. Moreover, we provide a comprehensive width analysis of the proposed confidence set, showing that the convergence rate of the diameter is adaptive to the unknown degree of instance-specific regularity. We apply the proposed method to several high-dimensional and irregular statistical problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07772v3</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kenta Takatsu, Arun Kumar Kuchibhotla</dc:creator>
    </item>
    <item>
      <title>A Metropolis-Adjusted Langevin Algorithm for Sampling Jeffreys Prior</title>
      <link>https://arxiv.org/abs/2504.06372</link>
      <description>arXiv:2504.06372v2 Announce Type: replace-cross 
Abstract: Inference and estimation are fundamental aspects of statistics, system identification and machine learning. For most inference problems, prior knowledge is available on the system to be modeled, and Bayesian analysis is a natural framework to impose such prior information in the form of a prior distribution. However, in many situations, coming out with a fully specified prior distribution is not easy, as prior knowledge might be too vague, so practitioners prefer to use a prior distribution that is as `ignorant' or `uninformative' as possible, in the sense of not imposing subjective beliefs, while still supporting reliable statistical analysis. Jeffreys prior is an appealing uninformative prior because it offers two important benefits: (i) it is invariant under any re-parameterization of the model, (ii) it encodes the intrinsic geometric structure of the parameter space through the Fisher information matrix, which in turn enhances the diversity of parameter samples. Despite these benefits, drawing samples from Jeffreys prior is a challenging task. In this paper, we propose a general sampling scheme using the Metropolis-Adjusted Langevin Algorithm that enables sampling of parameter values from Jeffreys prior, and provide numerical illustrations of our approach through several examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06372v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yibo Shi, Braghadeesh Lakshminarayanan, Cristian R. Rojas</dc:creator>
    </item>
  </channel>
</rss>
