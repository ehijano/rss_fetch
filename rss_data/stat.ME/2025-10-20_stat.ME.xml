<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 21 Oct 2025 02:41:54 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Estimand framework and intercurrent events handling for clinical trials with time-to-event outcomes</title>
      <link>https://arxiv.org/abs/2510.15000</link>
      <description>arXiv:2510.15000v1 Announce Type: new 
Abstract: The ICH E9(R1) guideline presents a framework of estimand for clinical trials, proposes five strategies for handling intercurrent events (ICEs), and provides a comprehensive discussion and many real-life clinical examples for quantitative outcomes and categorical outcomes. However, in ICH E9(R1) the discussion is lacking for time-to-event (TTE) outcomes. In this paper, we discuss how to define estimands and how to handle ICEs for clinical trials with TTE outcomes. Specifically, we discuss six ICE handling strategies, including those five strategies proposed by ICH E9(R1) and a new strategy, the competing-risk strategy. Compared with ICH E9(R1), the novelty of this paper is three-fold: (1) the estimands are defined in terms of potential outcomes, (2) the methods can utilize time-dependent covariates straightforwardly, and (3) the efficient estimators are discussed accordingly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15000v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yixin Fang, Man Jin</dc:creator>
    </item>
    <item>
      <title>Asymptotic distribution of the global clustering coefficient in a random annulus graph</title>
      <link>https://arxiv.org/abs/2510.15003</link>
      <description>arXiv:2510.15003v1 Announce Type: new 
Abstract: The global clustering coefficient is an effective measure for analyzing and comparing the structures of complex networks. The random annulus graph is a modified version of the well-known Erd\H{o}s-R\'{e}nyi random graph. It has been recently proposed in modeling network communities. This paper investigates the asymptotic distribution of the global clustering coefficient in a random annulus graph. It is demonstrated that the standardized global clustering coefficient converges in law to the standard normal distribution. The result is established using the asymptotic theory of degenerate U-statistics with a sample-size dependent kernel. As far as we know, this method is different from established approaches for deriving asymptotic distributions of network statistics. Moreover, we get the explicit expression of the limit of the global clustering coefficient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15003v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mingao Yuan</dc:creator>
    </item>
    <item>
      <title>Conditional GLMMs for reaction times in choice tasks</title>
      <link>https://arxiv.org/abs/2510.15203</link>
      <description>arXiv:2510.15203v1 Announce Type: new 
Abstract: This study connects two methods for modeling reaction times (RTs) in choice tasks: (1) the first-hitting time of a simple diffusion model with a single barrier, representing the cognitive process leading to a response, and (2) Generalized Linear Mixed Models (GLMMs). We achieve this by analyzing RT distributions conditioned on each response alternative. Because certain diffusion model variants yield Inverse Gaussian (IG) and Gamma distributions for first-hitting times, we can justify using these distributions in RT models. Conversely, employing IG and Gamma distributions within GLMMs allows us to infer the underlying cognitive processes. We demonstrate this concept through simulations and apply it to previously published real-world data. Finally, we discuss the scope and potential extensions of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15203v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mauricio Tejo, Cristian Meza, Fernando Marmolejo-Ramos</dc:creator>
    </item>
    <item>
      <title>Bayesian Sequential Modeling of Time-to-Urination for Dynamic ED Triage</title>
      <link>https://arxiv.org/abs/2510.15272</link>
      <description>arXiv:2510.15272v1 Announce Type: new 
Abstract: Triage tools in routine emergency care are largely static, failing to exploit simple behavioral cues clinicians notice in real time. Here, we developed a Bayesian, sequentially updating framework that integrates incoming cues to produce calibrated, time-consistent risk. Using a prospective single-center cohort of ambulance arrivals in Japan (February-August 2025; n=2,221), we evaluated time to first urination (TTU) as a proof-of-concept bedside cue for predicting hospital admission. Population-level fit to the cumulative admission curve was excellent (integrated squared error 0.002; RMSE 0.003; Kolmogorov-Smirnov 0.008; coverage 0.98). At the patient level, performance improved markedly with age/sex adjustment (AUC[t] 0.70 vs. 0.50 unadjusted), with lower Brier scores and positive calibration slopes. Platt recalibration refined probability scaling without altering discrimination, and decision-curve analysis showed small, favorable net benefit at common thresholds. This framework is readily extensible to multimodal inputs and external validation and is designed to complement, not replace, existing triage systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15272v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Atsushi Senda, Yuki Takatsu, Ryokan Ikebe, Hiroshi Suginaka, Koji Morishita, Akira Endo</dc:creator>
    </item>
    <item>
      <title>Nonparametric Testing of Spatial Dependence in 2D and 3D Random Fields</title>
      <link>https://arxiv.org/abs/2510.15381</link>
      <description>arXiv:2510.15381v1 Announce Type: new 
Abstract: We propose a flexible and robust nonparametric framework for testing spatial dependence in two- and three-dimensional random fields. Our approach involves converting spatial data into one-dimensional time series using space-filling Hilbert curves. We then apply ordinal pattern-based tests for serial dependence to this series. Because Hilbert curves preserve spatial locality, spatial dependence in the original field manifests as serial dependence in the transformed sequence. The approach is easy to implement, accommodates arbitrary grid sizes through generalized Hilbert (``gilbert'') curves, and naturally extends beyond three dimensions. This provides a practical and general alternative to existing methods based on spatial ordinal patterns, which are typically limited to two-dimensional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15381v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian H. Wei{\ss}, Philipp Ad\"ammer</dc:creator>
    </item>
    <item>
      <title>Adaptive Influence Diagnostics in High-Dimensional Regression</title>
      <link>https://arxiv.org/abs/2510.15618</link>
      <description>arXiv:2510.15618v1 Announce Type: new 
Abstract: An adaptive Cook's distance (ACD) for diagnosing influential observations in high-dimensional single-index models with multicollinearity and outlier contamination is proposed. ACD is a model-free technique built on sparse local linear gradients to temper leverage effects. In simulations spanning low- and high-dimensional design settings with strong correlation, ACD based on LASSO (ACD-LASSO) and SCAD (ACD-SCAD) penalties reduced masking and swamping relative to classical Cook's distance and local influence as well as the DF-Model and Case-Weight adjusted solution for LASSO. Trimming points flagged by ACD stabilizes variable selection while preserving core signals. Applications to two datasets--the 1960 US cities pollution study and a high-dimensional riboflavin genomics experiment show consistent gains in selection stability and interpretability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15618v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdul-Nasah Soale, Adewale Lukman</dc:creator>
    </item>
    <item>
      <title>Robust Estimation of Polyserial Correlation</title>
      <link>https://arxiv.org/abs/2510.15632</link>
      <description>arXiv:2510.15632v1 Announce Type: new 
Abstract: The association between a continuous and an ordinal variable is commonly modeled through the polyserial correlation model. However, this model, which is based on a partially-latent normality assumption, may be misspecified in practice, due to, for example (but not limited to), outliers or careless responses. We demonstrate that the typically used maximum likelihood (ML) estimator is highly susceptible to such misspecification: One single observation not generated by partially-latent normality can suffice to produce arbitrarily poor estimates. As a remedy, we propose a novel estimator of the polyserial correlation model designed to be robust against the adverse effects of observations discrepant to that model. The estimator achieves robustness by implicitly downweighting such observations; the ensuing weights constitute a useful tool for pinpointing potential sources of model misspecification. We show that the proposed estimator generalizes ML and is consistent as well as asymptotically Gaussian. As price for robustness, some efficiency must be sacrificed, but substantial robustness can be gained while maintaining more than 98% of ML efficiency. We demonstrate our estimator's robustness and practical usefulness in simulation experiments and an empirical application in personality psychology where our estimator helps identify outliers. Finally, the proposed methodology is implemented in free open-source software.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15632v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Max Welz</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference for PDE-based Inverse Problems using the Optimization of a Discrete Loss</title>
      <link>https://arxiv.org/abs/2510.15664</link>
      <description>arXiv:2510.15664v1 Announce Type: new 
Abstract: Inverse problems are crucial for many applications in science, engineering and medicine that involve data assimilation, design, and imaging. Their solution infers the parameters or latent states of a complex system from noisy data and partially observable processes. When measurements are an incomplete or indirect view of the system, additional knowledge is required to accurately solve the inverse problem. Adopting a physical model of the system in the form of partial differential equations (PDEs) is a potent method to close this gap. In particular, the method of optimizing a discrete loss (ODIL) has shown great potential in terms of robustness and computational cost. In this work, we introduce B-ODIL, a Bayesian extension of ODIL, that integrates the PDE loss of ODIL as prior knowledge and combines it with a likelihood describing the data. B-ODIL employs a Bayesian formulation of PDE-based inverse problems to infer solutions with quantified uncertainties. We demonstrate the capabilities of B-ODIL in a series of synthetic benchmarks involving PDEs in one, two, and three dimensions. We showcase the application of B-ODIL in estimating tumor concentration and its uncertainty in a patient's brain from MRI scans using a three-dimensional tumor growth model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15664v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>physics.comp-ph</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas Amoudruz, Sergey Litvinov, Costas Papadimitriou, Petros Koumoutsakos</dc:creator>
    </item>
    <item>
      <title>A Multiclass ROC Curve</title>
      <link>https://arxiv.org/abs/2510.15670</link>
      <description>arXiv:2510.15670v1 Announce Type: new 
Abstract: This paper introduces a novel methodology for constructing multiclass ROC curves using the multidimensional Gini index. The proposed methodology leverages the established relationship between the Gini coefficient and the ROC Curve and extends it to multiclass settings through the multidimensional Gini index. The framework is validated by means of two comprehensive case studies in health care and finance. The paper provides a theoretically grounded solution to multiclass performance evaluation, particularly valuable for imbalanced datasets, for which a prudential assessment should take precedence over class frequency considerations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15670v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paolo Giudici, Rosa C. Rosciano, Johanna Schrader, Delf-Magnus Kummerfeld</dc:creator>
    </item>
    <item>
      <title>Selection Confidence Sets for Equally Weighted Portfolios</title>
      <link>https://arxiv.org/abs/2510.14988</link>
      <description>arXiv:2510.14988v1 Announce Type: cross 
Abstract: Given a universe of N assets, investors often form equally weighted portfolios (EWPs) by selecting subsets of assets. EWPs are simple, robust, and competitive out-of-sample, yet the uncertainty about which subset truly performs best is largely ignored. Traditional approaches typically rely on a single selected portfolio, but this fails to consider alternative investment strategies that may perform just as well when accounting for statistical uncertainty. To address this selection uncertainty, we introduce the Selection Confidence Set (SCS) for EWPs: the set of all portfolios that, under a given loss function and at a specified confidence level, contains the unknown set of optimal portfolios under repeated sampling. The SCS quantifies selection uncertainty by identifying a range of plausible portfolios, challenging the idea of a uniquely optimal choice. Like a confidence set, its size reflects uncertainty -- growing with noisy or limited data, and shrinking as the sample size increases. Theoretically, we establish that the SCS covers the unknown optimal selection with high probability and characterize how its size grows with underlying uncertainty, corroborating these results through Monte Carlo experiments. Applications to the French 17-Industry Portfolios and Layer-1 cryptocurrencies underscore the importance of accounting for selection uncertainty when comparing equally weighted strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14988v1</guid>
      <category>q-fin.PM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Davide Ferrari, Alessandro Fulci, Sandra Paterlini</dc:creator>
    </item>
    <item>
      <title>Reliable data clustering with Bayesian community detection</title>
      <link>https://arxiv.org/abs/2510.15013</link>
      <description>arXiv:2510.15013v1 Announce Type: cross 
Abstract: From neuroscience and genomics to systems biology and ecology, researchers rely on clustering similarity data to uncover modular structure. Yet widely used clustering methods, such as hierarchical clustering, k-means, and WGCNA, lack principled model selection, leaving them susceptible to noise. A common workaround sparsifies a correlation matrix representation to remove noise before clustering, but this extra step introduces arbitrary thresholds that can distort the structure and lead to unreliable results. To detect reliable clusters, we capitalize on recent advances in network science to unite sparsification and clustering with principled model selection. We test two Bayesian community detection methods, the Degree-Corrected Stochastic Block Model and the Regularized Map Equation, both grounded in the Minimum Description Length principle for model selection. In synthetic data, they outperform traditional approaches, detecting planted clusters under high-noise conditions and with fewer samples. Compared to WGCNA on gene co-expression data, the Regularized Map Equation identifies more robust and functionally coherent gene modules. Our results establish Bayesian community detection as a principled and noise-resistant framework for uncovering modular structure in high-dimensional data across fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15013v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <category>stat.ME</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Magnus Neuman, Jelena Smiljani\'c, Martin Rosvall</dc:creator>
    </item>
    <item>
      <title>Foresighted Online Policy Optimization with Interference</title>
      <link>https://arxiv.org/abs/2510.15273</link>
      <description>arXiv:2510.15273v1 Announce Type: cross 
Abstract: Contextual bandits, which leverage the baseline features of sequentially arriving individuals to optimize cumulative rewards while balancing exploration and exploitation, are critical for online decision-making. Existing approaches typically assume no interference, where each individual's action affects only their own reward. Yet, such an assumption can be violated in many practical scenarios, and the oversight of interference can lead to short-sighted policies that focus solely on maximizing the immediate outcomes for individuals, which further results in suboptimal decisions and potentially increased regret over time. To address this significant gap, we introduce the foresighted online policy with interference (FRONT) that innovatively considers the long-term impact of the current decision on subsequent decisions and rewards. The proposed FRONT method employs a sequence of exploratory and exploitative strategies to manage the intricacies of interference, ensuring robust parameter inference and regret minimization. Theoretically, we establish a tail bound for the online estimator and derive the asymptotic distribution of the parameters of interest under suitable conditions on the interference network. We further show that FRONT attains sublinear regret under two distinct definitions, capturing both the immediate and consequential impacts of decisions, and we establish these results with and without statistical inference. The effectiveness of FRONT is further demonstrated through extensive simulations and a real-world application to urban hotel profits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15273v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liner Xiang, Jiayi Wang, Hengrui Cai</dc:creator>
    </item>
    <item>
      <title>Dynamic Spatial Treatment Effects as Continuous Functionals: Theory and Evidence from Healthcare Access</title>
      <link>https://arxiv.org/abs/2510.15324</link>
      <description>arXiv:2510.15324v2 Announce Type: cross 
Abstract: I develop a continuous functional framework for spatial treatment effects grounded in Navier-Stokes partial differential equations. Rather than discrete treatment parameters, the framework characterizes treatment intensity as continuous functions $\tau(\mathbf{x}, t)$ over space-time, enabling rigorous analysis of boundary evolution, spatial gradients, and cumulative exposure. Empirical validation using 32,520 U.S. ZIP codes demonstrates exponential spatial decay for healthcare access ($\kappa = 0.002837$ per km, $R^2 = 0.0129$) with detectable boundaries at 37.1 km. The framework successfully diagnoses when scope conditions hold: positive decay parameters validate diffusion assumptions near hospitals, while negative parameters correctly signal urban confounding effects. Heterogeneity analysis reveals 2-13 $\times$ stronger distance effects for elderly populations and substantial education gradients. Model selection strongly favors logarithmic decay over exponential ($\Delta \text{AIC} &gt; 10,000$), representing a middle ground between exponential and power-law decay. Applications span environmental economics, banking, and healthcare policy. The continuous functional framework provides predictive capability ($d^*(t) = \xi^* \sqrt{t}$), parameter sensitivity ($\partial d^*/\partial \nu$), and diagnostic tests unavailable in traditional difference-in-differences approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15324v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>Temporal Functional Factor Analysis of Brain Connectivity</title>
      <link>https://arxiv.org/abs/2510.15580</link>
      <description>arXiv:2510.15580v1 Announce Type: cross 
Abstract: Many analyses of functional magnetic resonance imaging (fMRI) examine functional connectivity (FC), or the statistical dependencies among distant brain regions. These analyses are typically exploratory, guiding future confirmatory research. In this work, we present an approach based on factor analysis (FA) that is well-suited to studying FC. FA is appealing in this context because its flexible model assumptions permit a guided investigation of its target subspace consistent with the exploratory role of connectivity analyses. However, applying FA to fMRI data poses three problems: (1) its target subspace captures short-range spatial dependencies that should be treated as noise, (2) it requires factorization of a massive spatial covariance, and (3) it overlooks temporal dependencies in the data. To address these limitations, we develop a factor model within the framework of functional data analysis--a field which views certain data as arising from smooth underlying curves. The proposed approach (1) uses matrix completion techniques to filter short-range spatial dependencies out of its target subspace, (2) employs a distributed algorithm for factorizing large-scale covariance matrices, and (3) leverages functional regression to exploit temporal dynamics. Together, these innovations yield a comprehensive and scalable method for studying FC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15580v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kyle Stanley, Nicole Lazar, Matthew Reimherr</dc:creator>
    </item>
    <item>
      <title>Cross-Population Amplitude Coupling in High-Dimensional Oscillatory Neural Time Series</title>
      <link>https://arxiv.org/abs/2105.03508</link>
      <description>arXiv:2105.03508v3 Announce Type: replace 
Abstract: Neural oscillations have long been considered important markers of interaction across brain regions, yet identifying coordinated oscillatory activity from high-dimensional multiple-electrode recordings remains challenging. We sought to quantify time-varying covariation of oscillatory amplitudes across two brain regions, during a memory task, based on local field potentials recorded from 96 electrodes in each region. We extended Canonical Correlation Analysis (CCA) to multiple time series through the cross-correlation of latent time series. This, however, introduces a large number of possible lead-lag cross-correlations across the two regions. To manage that high dimensionality we developed rigorous statistical procedures aimed at finding a small number of dominant lead-lag effects. The method correctly identified ground truth structure in realistic simulation-based settings. When we used it to analyze local field potentials recorded from prefrontal cortex and visual area V4 we obtained highly plausible results. The new statistical methodology could also be applied to other slowly-varying high-dimensional time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.03508v3</guid>
      <category>stat.ME</category>
      <category>q-bio.NC</category>
      <category>stat.AP</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heejong Bong, Val\'erie Ventura, Eric A. Yttri, Matthew A. Smith, Robert E. Kass</dc:creator>
    </item>
    <item>
      <title>Scale Invariant Correspondence Analysis</title>
      <link>https://arxiv.org/abs/2311.17594</link>
      <description>arXiv:2311.17594v2 Announce Type: replace 
Abstract: Correspondence analysis is a dimension reduction method for visualization of nonnegative data sets, in particular contingency tables ; but it depends on the marginals of the data set. Two transformations of the data have been proposed to render correspondence analysis row and column scales invariant : These two kinds of transformations change the initial form of the data set into a bistochastic form. The power transorfmation applied by Greenacre (2010) has one positive parameter. While the transormation applied by Mosteller (1968) and Goodman (1996) has (I+J) positive parameters, where the raw data is row and column scaled by the Sinkhorn (RAS or ipf) algorithm to render it bistochastic. Goodman (1996) named correspondence analsis of a bistochastic matrix marginal-free correspondence analysis. We discuss these two transformations, and further generalize Mosteller-Goodman approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.17594v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vartan Choulakian</dc:creator>
    </item>
    <item>
      <title>Random Measures, ANOVA Models and Quantifying Uncertainty in Randomized Controlled Trials</title>
      <link>https://arxiv.org/abs/2312.10541</link>
      <description>arXiv:2312.10541v2 Announce Type: replace 
Abstract: The paper introduces a novel approach to global sensitivity analysis, grounded in the variance-covariance structure of random variables derived from random measures. The proposed methodology facilitates the application of information-theoretic rules for uncertainty quantification, offering several advantages. Specifically, the approach provides valuable insights into the decomposition of variance within discrete subspaces, similar to the standard ANOVA analysis. To illustrate this point, the method is applied to datasets obtained from the analysis of randomized controlled trials on evaluating the efficacy of the COVID-19 vaccine and assessing clinical endpoints in a lung cancer study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.10541v2</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Caleb Deen Bastian, Herschel Rabitz, Grzegorz A Rempala</dc:creator>
    </item>
    <item>
      <title>Unifying area and unit-level small area estimation through calibration</title>
      <link>https://arxiv.org/abs/2403.15384</link>
      <description>arXiv:2403.15384v2 Announce Type: replace 
Abstract: When estimating area means, direct estimators based on area-specific data, are usually consistent under the sampling design without model assumptions. However, they are inefficient if the area sample size is small. In small area estimation, model assumptions linking the areas are used to "borrow strength" from other areas. The basic area-level model provides design-consistent estimators but error variances are assumed to be known. In practice, they are estimated with the (scarce) area-specific data. These estimators are inefficient, and their error is not accounted for in the associated mean squared error estimators. Unit-level models do not require to know the error variances but do not account for the survey design. Here we describe a unified estimator of an area mean that may be obtained both from an area-level model or a unit-level model and based on consistent estimators of the model error variances as the number of areas increases. We propose bootstrap mean squared error estimators that account for the uncertainty due to the estimation of the error variances. We show a better performance of the new small area estimators and our bootstrap estimators of the mean squared error. We apply the results to education data from Colombia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15384v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William Acero, Isabel Molina, J. Miguel Mar\'in</dc:creator>
    </item>
    <item>
      <title>New flexible versions of extended generalized Pareto model for count data</title>
      <link>https://arxiv.org/abs/2409.18719</link>
      <description>arXiv:2409.18719v4 Announce Type: replace 
Abstract: Accurate modeling is essential in integer-valued real phenomena, including the distribution of entire data, zero-inflated (ZI) data, and discrete exceedances. The Poisson and Negative Binomial distributions, along with their ZI variants, are considered suitable for modeling the entire data distribution, but they fail to capture the heavy tail behavior effectively alongside the bulk of the distribution. In contrast, the discrete generalized Pareto distribution (DGPD) is preferred for high threshold exceedances, but it becomes less effective for low threshold exceedances. However, in some applications, the selection of a suitable high threshold is challenging, and the asymptotic conditions required for using DGPD are not always met. To address these limitations, extended versions of DGPD are proposed. These extensions are designed to model one of three scenarios: first, the entire distribution of the data, including both bulk and tail and bypassing the threshold selection step; second, the entire distribution along with ZI; and third, the tail of the distribution for low threshold exceedances. The proposed extensions offer improved estimates across all three scenarios compared to existing models, providing more accurate and reliable results in simulation studies and real data applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18719v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Touqeer Ahmad, Irshad Ahmad Arshad</dc:creator>
    </item>
    <item>
      <title>Safe Individualized Treatment Rules with Controllable Harm Rates</title>
      <link>https://arxiv.org/abs/2505.05308</link>
      <description>arXiv:2505.05308v2 Announce Type: replace 
Abstract: Estimating individualized treatment rules (ITRs) is crucial for tailoring interventions in precision medicine. Typical ITR estimation methods rely on conditional average treatment effects (CATEs) to guide treatment assignments. However, such methods overlook individual-level harm within covariate-specific subpopulations, potentially leading many individuals to experience worse outcomes under CATE-based ITRs. In this article, we aim to estimate ITRs that maximize the reward while ensuring that the harm rate induced by the ITR remains below a pre-specified threshold. We first derive the explicit form of the oracle ITR. However, the oracle ITR is not achievable without strong assumptions, as the harm rate is generally unidentifiable due to its dependence on the joint distribution of potential outcomes. To address this, we propose two strategies for estimating ITRs with a harm rate constraint under partial identification and establish their large-sample properties. By accounting for both reward and harm, our method provides a reliable solution for developing ITRs in high-stakes domains where harm is a critical consideration. Extensive simulations demonstrate the effectiveness of the proposed methods in controlling harm rates. We apply the proposed method to analyze two real-world datasets from a new perspective, assessing the potential reduction in harm rate compared with historical interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05308v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peng Wu, Qing Jiang, Shanshan Luo, Zhi Geng</dc:creator>
    </item>
    <item>
      <title>Uncertainty Quantification for Prior-Data Fitted Networks using Martingale Posteriors</title>
      <link>https://arxiv.org/abs/2505.11325</link>
      <description>arXiv:2505.11325v2 Announce Type: replace 
Abstract: Prior-data fitted networks (PFNs) have emerged as promising foundation models for prediction from tabular data sets, achieving state-of-the-art performance on small to moderate data sizes without tuning. While PFNs are motivated by Bayesian ideas, they do not provide any uncertainty quantification for predictive means, quantiles, or similar quantities. We propose a principled and efficient sampling procedure to construct Bayesian posteriors for such estimates based on Martingale posteriors, and prove its convergence. Several simulated and real-world data examples showcase the uncertainty quantification of our method in inference applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11325v2</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Nagler, David R\"ugamer</dc:creator>
    </item>
    <item>
      <title>Optimal and Efficient Sample Size Re-estimation: A Dynamic Cost Framework</title>
      <link>https://arxiv.org/abs/2509.16636</link>
      <description>arXiv:2509.16636v2 Announce Type: replace 
Abstract: Adaptive sample size re-estimation (SSR) is a well-established strategy for improving the efficiency and flexibility of clinical trials. Its central challenge is determining whether, and by how much, to increase the sample size at an interim analysis. This decision requires a rational framework for balancing the potential gain in statistical power against the risk and cost of further investment. Prevailing optimization approaches, such as the Jennison and Turnbull (JT) method, address this by maximizing power for a fixed cost per additional participant. While statistically efficient, this paradigm assumes the cost of enrolling another patient is constant, regardless of whether the interim evidence is promising or weak. This can lead to impractical recommendations and inefficient resource allocation, particularly in weak-signal scenarios.
  We reframe SSR as a decision problem under dynamic costs, where the effective cost of additional enrollment reflects the interim strength of evidence. Within this framework, we derive two novel rules: (i) a likelihood-ratio based rule, shown to be Pareto optimal in achieving smaller average sample size under the null without loss of power under the alternative; and (ii) a return-on-investment (ROI) rule that directly incorporates economic considerations by linking SSR decisions to expected net benefit. To unify existing methods, we further establish a representation theorem demonstrating that a broad class of SSR rules can be expressed through implicit dynamic cost functions, providing a common analytical foundation for their comparison. Simulation studies calibrated to Phase III trial settings confirm that dynamic-cost approaches improve resource allocation relative to fixed-cost methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16636v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rui Jin, Cai Wu, Qiqi Deng</dc:creator>
    </item>
    <item>
      <title>lqmix: an R package for longitudinal data analysis via linear quantile mixtures</title>
      <link>https://arxiv.org/abs/2302.11363</link>
      <description>arXiv:2302.11363v4 Announce Type: replace-cross 
Abstract: The analysis of longitudinal data gives the chance to observe how unit behaviors change over time, but it also poses a series of issues. These have been the focus of an extensive literature in the context of linear and generalized linear regression moving also, in the last ten years or so, to the context of linear quantile regression for continuous responses. In this paper, we present \texttt{lqmix}, a novel \texttt{R} package that assists in estimating a class of linear quantile regression models for longitudinal data, in the presence of time-constant and/or time-varying, unit-specific, random coefficients, with unspecified distribution. Model parameters are estimated in a maximum likelihood framework via an extended EM algorithm, while parameters' standard errors are derived via a block-bootstrap procedure. The analysis of a benchmark dataset is used to give details on the package functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.11363v4</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Alf\'o, Maria Francesca Marino, Maria Giovanna Ranalli, Nicola Salvati</dc:creator>
    </item>
  </channel>
</rss>
