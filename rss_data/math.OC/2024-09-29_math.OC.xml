<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 30 Sep 2024 04:06:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Primal-dual Accelerated Mirror-Descent Method for Constrained Bilinear Saddle-Point Problems</title>
      <link>https://arxiv.org/abs/2409.18285</link>
      <description>arXiv:2409.18285v1 Announce Type: new 
Abstract: We develop a first-order accelerated algorithm for a class of constrained bilinear saddle-point problems with applications to network systems. The algorithm is a modified time-varying primal-dual version of an accelerated mirror-descent dynamics. It deals with constraints such as simplices and convex set constraints effectively, and converges with a rate of $O(1/t^2)$. Furthermore, we employ the acceleration scheme to constrained distributed optimization and bilinear zero-sum games, and obtain two variants of distributed accelerated algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18285v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weijian Li, Xianlin Zeng, and Lacra Pavel</dc:creator>
    </item>
    <item>
      <title>Average Distance of Random Bipartite Matching in Discrete Networks</title>
      <link>https://arxiv.org/abs/2409.18292</link>
      <description>arXiv:2409.18292v1 Announce Type: new 
Abstract: The bipartite matching problem is widely applied in the field of transportation; e.g., to find optimal matches between supply and demand over time and space. Recent efforts have been made on developing analytical formulas to estimate the expected matching distance in bipartite matching with randomly distributed vertices in two- or higher-dimensional spaces, but no accurate formulas currently exist for one-dimensional problems. This paper presents a set of closed-form formulas, without curve-fitting, that can provide accurate average distance estimates for one-dimensional random bipartite matching problems (RBMP). We first focus on one-dimensional space and propose a new method that relates the corresponding matching distance to the area size between a random walk path and the x-axis. This result directly leads to a straightforward closed-form formula for balanced RBMPs. For unbalanced RBMPs, we first analyze the properties of an unbalanced random walk that can be related to balanced RBPMs after optimally removing a subset of unmatched points, and then derive a set of approximate formulas. Additionally, we build upon an optimal point removal strategy to derive a set of recursive formulas that can provide more accurate estimates. Then, we shift our focus to regular discrete networks, and use the one-dimensional results as building blocks to derive RBMP formulas. To verify the accuracy of the proposed formulas, a set of Monte-Carlo simulations are generated for a variety of matching problems settings. Results indicate that our proposed formulas provide quite accurate distance estimations for one-dimensional line segments and discrete networks under a variety of conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18292v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuhui Zhai, Shiyu Shen, Yanfeng Ouyang</dc:creator>
    </item>
    <item>
      <title>Designing Sparse AC False Data Injection Attack</title>
      <link>https://arxiv.org/abs/2409.18331</link>
      <description>arXiv:2409.18331v1 Announce Type: new 
Abstract: False Data Injection (FDI) attacks pose significant threats by manipulating measurement data, leading to incorrect state estimation. Although numerous studies have focused on designing DC FDI attacks, few have addressed AC FDI attacks due to the complexity of incorporating non-linear AC power flows in the design process. Additionally, designing a sparse AC FDI attack presents another challenge because it involves solving a mixed-integer nonlinear programming problem with nonconvex constraints, which is inherently difficult. This paper explores the design and implementation of a sparse AC FDI attack, where the attacker strategically selects a minimal set of measurements to manipulate while maintaining the nonlinearity and interdependence of AC power flow equations. The objective is to minimize the number of altered measurements, thereby reducing the attack's detectability while achieving the desired state estimation error. The problem is formulated as a Mixed Integer Nonlinear Programming (MINLP) problem. Binary variables indicate the selection of measurements to be manipulated, and continuous variables represent the measurement values. An optimization problem is designed to minimize the number of binary variables, translating into a sparse attack, while ensuring the attack remains efficient and hard to detect. The big-M method and conditional constraints are utilized to handle the fixed and variable measurement parameters effectively. Simulation results on the standard IEEE 57-bus test system demonstrate the efficacy of the sparse AC FDI attack in terms of its impact on state estimation and the minimal number of measurements required for successful implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18331v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammadreza Iranpour, Mohammad Rasoul Narimani</dc:creator>
    </item>
    <item>
      <title>AC-DC Power Systems Optimization with Droop Control Smooth Approximation</title>
      <link>https://arxiv.org/abs/2409.18376</link>
      <description>arXiv:2409.18376v1 Announce Type: new 
Abstract: This paper addresses the challenges of embedding common droop control characteristics in ac-dc power system steady-state simulation and optimization problems. We propose a smooth approximation methodology to construct differentiable functions that encode the attributes of piecewise linear droop control with saturation. We transform the nonsmooth droop curves into smooth nonlinear equality constraints, solvable with Newton methods and interior point solvers. These constraints are then added to power flow, optimal power flow, and security-constrained optimal power flow problems in ac-dc power systems. The results demonstrate significant improvements in accuracy in terms of power sharing response, voltage regulation, and system efficiency, while outperforming existing mixed-integer formulations in computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18376v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ghulam Mohy-ud-din, Rahmat Heidari, Frederik Geth, Hakan Ergun, S M Muslem Uddin</dc:creator>
    </item>
    <item>
      <title>Concave tents: a new tool for constructing concave reformulations of a large class of nonconvex optimization problems</title>
      <link>https://arxiv.org/abs/2409.18451</link>
      <description>arXiv:2409.18451v1 Announce Type: new 
Abstract: Optimizing a nonlinear function over nonconvex sets is challenging since solving convex relaxations may lead to substantial relaxation gaps and infeasible solutions, that must be "rounded" to feasible ones, often with uncontrollable losses in objective function performance. For this reason, these convex hulls are especially useful if the objective function is linear or even concave since concave optimization is invariant to taking the convex hull of the feasible set. Motivated by this observation, we propose the notion of concave tents, which are concave approximations of the original objective function that agree with this objective function on the feasible set, and allow for concave reformulations of the problem. We derive these concave tents for a large class of objective functions as the optimal value functions of conic optimization problems. Hence, evaluating our concave tents requires solving a conic problem. Interestingly, we can find supergradients by solving the conic dual problem, so that differentiation is of the same complexity as evaluation. For feasible sets that are contained in the extreme points of their convex hull, we construct these concave tents in the original space of variables. For general feasible sets, we propose a double lifting strategy, where the original optimization problem is lifted into a higher dimensional space in which the concave tent can be constructed with a similar effort. We investigate the relation of the so-constructed concave tents to concave envelopes and a naive concave tent based on concave quadratic updates. Based on these ideas we propose a primal heuristic for a class of robust discrete quadratic optimization problems, that can be used instead of classical rounding techniques. Numerical experiments suggest that our techniques can be beneficial as an upper bounding procedure in a branch and bound solution scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18451v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Markus Gabl</dc:creator>
    </item>
    <item>
      <title>A note on first order quasi-stationary Mean Field Games</title>
      <link>https://arxiv.org/abs/2409.18483</link>
      <description>arXiv:2409.18483v1 Announce Type: new 
Abstract: Quasi-stationary Mean Field Games models consider agents who base their strategies on current information without forecasting future states. In this paper we address the first-order quasi-stationary Mean Field Games system, which involves an ergodic Hamilton-Jacobi equation and an evolutive continuity equation. Our approach relies on weak KAM theory. We introduce assumptions on the Hamiltonian and coupling cost to ensure continuity of the Peierls barrier and the Aubry set over time. These assumptions, though restrictive, cover interesting cases such as perturbed mechanical Hamiltonians.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18483v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabio Camilli, Claudio Marchi, Cristian Mendico</dc:creator>
    </item>
    <item>
      <title>Ca{\Sigma}oS: A nonlinear sum-of-squares optimization suite</title>
      <link>https://arxiv.org/abs/2409.18549</link>
      <description>arXiv:2409.18549v1 Announce Type: new 
Abstract: We present Ca{\Sigma}oS, the first MATLAB software specifically designed for nonlinear sum-of-squares optimization. A symbolic polynomial algebra system allows to formulate parametrized sum-of-squares optimization problems and facilitates their fast, repeated evaluations. To that extent, we make use of CasADi's symbolic framework and realize concepts of monomial sparsity, linear operators (including duals), and functions between polynomials. Ca{\Sigma}oS currently provides interfaces to the conic solvers SeDuMi, Mosek, and SCS as well as methods to solve quasiconvex optimization problems (via bisection) and nonconvex optimization problems (via sequential convexification). Numerical examples for benchmark problems including region-of-attraction and reachable set estimation for nonlinear dynamic systems demonstrate significant improvements in computation time compared to existing toolboxes.. Ca{\Sigma}oS is available open-source at https://github.com/ ifr-acso/casos.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18549v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Torbj{\o}rn Cunis, Jan Olucak</dc:creator>
    </item>
    <item>
      <title>DP-SCC-PL:Differentially Private Decentralized Byzantine-Resilient Stochastic Optimization via Self-Centered Clipping Under Polyak-{\L}ojasiewicz Condition</title>
      <link>https://arxiv.org/abs/2409.18632</link>
      <description>arXiv:2409.18632v1 Announce Type: new 
Abstract: Privacy leakage and Byzantine agents are two critical issues that bring great challenges to the intelligent decision-making process of multi-agent systems (MASs). Considering the presence of these two issues, this paper targets the resolution of a class of nonconvex optimization problems under the Polyak-{\L}ojasiewicz (P-{\L}) condition. To address this problem, we mask the local gradients with Gaussian noises and adopt a resilient aggregation method self-centered clipping (SCC) to design a differentially private (DP) decentralized Byzantine-resilient algorithm, namely DP-SCC-PL, which simultaneously achieves differential privacy and Byzantine resilience. Theoretical analysis demonstrates that DP-SCC-PL achieves the consensus among all reliable agents with a decaying step-size and sublinear (inexact) convergence with a constant step-size, where the asymptotic convergence error is characterized in both cases. It has also been proved that if there are no privacy issues and Byzantine agents, then the asymptotic exact convergence can be recovered when adopting a well-designed decaying step-size. Numerical experiments verify the differential privacy, resilience, and effectiveness of DP-SCC-PL via tackling a nonconvex optimization problem satisfying the P-{\L} condition under various Byzantine attacks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18632v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinhui Hu, Xiaoyu Guo, Huaqing Li, Huqiang Cheng, Guo Chen</dc:creator>
    </item>
    <item>
      <title>A convex, finite and lower semicontinuous function with empty subdifferential</title>
      <link>https://arxiv.org/abs/2409.18650</link>
      <description>arXiv:2409.18650v1 Announce Type: new 
Abstract: We give an example of a convex, finite and lower semicontinuous function whose subdifferential is everywhere empty. This is possible since the function is defined on an incomplete normed space. The function serves as a universal counterexample to various statements in convex analysis in which completeness is required.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18650v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gerd Wachsmuth</dc:creator>
    </item>
    <item>
      <title>Sparse Signal Recovery via $L_1/L_2$ Minimization: Bound Theory and Complexity</title>
      <link>https://arxiv.org/abs/2409.18748</link>
      <description>arXiv:2409.18748v1 Announce Type: new 
Abstract: The \(L_1/L_2\) norm ratio has gained significant attention as a measure of sparsity due to three primal advantages: sharper approximation to the \(L_0\) norm compared to the widely-used \(L_1\) norm, being parameter-free and scale-invariant, and exceptional performance with highly coherent matrices. In this note, we establish uniform upper bounds in $L_2$ norm for any local minimizer of constrained and unconstrained \(L_1/L_2\)-minimization models. Furthermore, we derive some upper/lower bound for the magnitudes of nonzero entries in any local minimizer of the unconstrained \(L_1/L_2\) minimization problem. Moreover, we prove that finding the global minimum of both constrained and unconstrained \(L_1/L_2\) models is strongly NP-Hard. Lastly, we point out that finding the global minimum of constrained and unconstrained \(L_p\) (\(0 &lt; p \leq 1\)) over \(L_q\) (\(1 &lt; q &lt; +\infty\)) models is also strongly NP-Hard.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18748v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Min Tao, Xiao-Ping Zhang, Yun-Bin Zhao</dc:creator>
    </item>
    <item>
      <title>Uniform exponential convergence of SAA with AMIS and asymptotics of its optimal value</title>
      <link>https://arxiv.org/abs/2409.18818</link>
      <description>arXiv:2409.18818v1 Announce Type: new 
Abstract: We discuss in this paper uniform exponential convergence of sample average approximation (SAA) with adaptive multiple importance sampling (AMIS) and asymptotics of its optimal value. Using a concentration inequality for bounded martingale differences, we obtain a new exponential convergence rate. To study the asymptotics, we first derive an important functional central limit theorem (CLT) for martingale difference sequences. Subsequently, exploiting this result with the Delta theorem, we prove the asymptotics of optimal values for SAA with AMIS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18818v1</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenjin Zhang, Yong Li</dc:creator>
    </item>
    <item>
      <title>Mean-Field Control Barrier Functions: A Framework for Real-Time Swarm Control</title>
      <link>https://arxiv.org/abs/2409.18945</link>
      <description>arXiv:2409.18945v1 Announce Type: new 
Abstract: Control Barrier Functions (CBFs) are an effective methodology to ensure safety and performative efficacy in real-time control applications such as power systems, resource allocation, autonomous vehicles, robotics, etc. This approach ensures safety independently of the high-level tasks that may have been pre-planned offline. For example, CBFs can be used to guarantee that a vehicle will remain in its lane. However, when the number of agents is large, computation of CBFs can suffer from the curse of dimensionality in the multi-agent setting. In this work, we present Mean-field Control Barrier Functions (MF-CBFs), which extends the CBF framework to the mean-field (or swarm control) setting. The core idea is to model a population of agents as probability measures in the state space and build corresponding control barrier functions. Similar to traditional CBFs, we derive safety constraints on the (distributed) controls but now relying on the differential calculus in the space of probability measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18945v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samy Wu Fung, Levon Nurbekyan</dc:creator>
    </item>
    <item>
      <title>Robust optimization and uncertainty quantification in the nonlinear mechanics of an elevator brake system</title>
      <link>https://arxiv.org/abs/2409.18139</link>
      <description>arXiv:2409.18139v1 Announce Type: cross 
Abstract: This paper deals with nonlinear mechanics of an elevator brake system subjected to uncertainties. A deterministic model that relates the braking force with uncertain parameters is deduced from mechanical equilibrium conditions. In order to take into account parameters variabilities, a parametric probabilistic approach is employed. In this stochastic formalism, the uncertain parameters are modeled as random variables, with distributions specified by the maximum entropy principle. The uncertainties are propagated by the Monte Carlo method, which provides a detailed statistical characterization of the response. This work still considers the optimum design of the brake system, formulating and solving nonlinear optimization problems, with and without the uncertainties effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18139v1</guid>
      <category>cs.CE</category>
      <category>math.OC</category>
      <category>physics.class-ph</category>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s11012-019-00992-7</arxiv:DOI>
      <arxiv:journal_reference>Meccanica, vol. 54, pp. 1057-1069, 2019</arxiv:journal_reference>
      <dc:creator>Piotr Wolszczak, Pawel Lonkwic, Americo Cunha Jr, Grzegorz Litak, Szymon Molski</dc:creator>
    </item>
    <item>
      <title>Reinforcement Learning for Finite Space Mean-Field Type Games</title>
      <link>https://arxiv.org/abs/2409.18152</link>
      <description>arXiv:2409.18152v1 Announce Type: cross 
Abstract: Mean field type games (MFTGs) describe Nash equilibria between large coalitions: each coalition consists of a continuum of cooperative agents who maximize the average reward of their coalition while interacting non-cooperatively with a finite number of other coalitions. Although the theory has been extensively developed, we are still lacking efficient and scalable computational methods. Here, we develop reinforcement learning methods for such games in a finite space setting with general dynamics and reward functions. We start by proving that MFTG solution yields approximate Nash equilibria in finite-size coalition games. We then propose two algorithms. The first is based on quantization of the mean-field spaces and Nash Q-learning. We provide convergence and stability analysis. We then propose an deep reinforcement learning algorithm, which can scale to larger spaces. Numerical examples on 5 environments show the scalability and the efficiency of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18152v1</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kai Shao, Jiacheng Shen, Chijie An, Mathieu Lauri\`ere</dc:creator>
    </item>
    <item>
      <title>On Inverse Problems for Two-Dimensional Steady Supersonic Euler Flows past Curved Wedges</title>
      <link>https://arxiv.org/abs/2409.18241</link>
      <description>arXiv:2409.18241v1 Announce Type: cross 
Abstract: We are concerned with the well-posedness of an inverse problem for determining the wedge boundary and associated two-dimensional steady supersonic Euler flow past the wedge, provided that the pressure distribution on the boundary surface of the wedge and the incoming state of the flow are given. We first establish the existence of wedge boundaries and associated entropy solutions of the inverse problem when the pressure on the wedge boundary is larger than that of the incoming flow but less than a critical value, and the total variation of the incoming flow and the pressure distribution is sufficiently small. This is achieved by carefully constructing suitable approximate solutions and approximate boundaries via developing a wave-front tracking algorithm and the rigorous proof of their strong convergence to a global entropy solution and a wedge boundary respectively. Then we establish the $L^{\infty}$--stability of the wedge boundaries, by introducing a modified Lyapunov functional for two different solutions with two distinct boundaries, each of which may contain a strong shock-front. The modified Lyapunov functional is carefully designed to control the distance between the two boundaries and is proved to be Lipschitz continuous with respect to the differences of the incoming flow and the pressure on the wedge, which leads to the existence of the Lipschitz semigroup. Finally, when the pressure distribution on the wedge boundary is sufficiently close to that of the incoming flow, using this semigroup, we compare two solutions of the inverse problem in the respective supersonic full Euler flow and potential flow and prove that, at $x&gt;0$, the distance between the two boundaries and the difference of the two solutions are of the same order of $x$ multiplied by the cube of the perturbations of the initial boundary data in $L^\infty\cap BV$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18241v1</guid>
      <category>math.AP</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.OC</category>
      <category>physics.flu-dyn</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gui-Qiang G. Chen, Yun Pu, Yongqian Zhang</dc:creator>
    </item>
    <item>
      <title>PNOD: An Efficient Projected Newton Framework for Exact Optimal Experimental Designs</title>
      <link>https://arxiv.org/abs/2409.18392</link>
      <description>arXiv:2409.18392v1 Announce Type: cross 
Abstract: Computing the exact optimal experimental design has been a longstanding challenge in various scientific fields. This problem, when formulated using a specific information function, becomes a mixed-integer nonlinear programming (MINLP) problem, which is typically NP-hard, thus making the computation of a globally optimal solution extremely difficult. The branch and bound (BnB) method is a widely used approach for solving such MINLPs, but its practical efficiency heavily relies on the ability to solve continuous relaxations effectively within the BnB search tree. In this paper, we propose a novel projected Newton framework, combining with a vertex exchange method for efficiently solving the associated subproblems, designed to enhance the BnB method. This framework offers strong convergence guarantees by utilizing recent advances in solving self-concordant optimization and convex quadratic programming problems. Extensive numerical experiments on A-optimal and D-optimal design problems, two of the most commonly used models, demonstrate the framework's promising numerical performance. Specifically, our framework significantly improves the efficiency of node evaluation within the BnB search tree and enhances the accuracy of solutions compared to state-of-the-art methods. The proposed framework is implemented in an open source Julia package called \texttt{PNOD.jl}, which opens up possibilities for its application in a wide range of real-world scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18392v1</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ling Liang, Haizhao Yang</dc:creator>
    </item>
    <item>
      <title>Dual Cone Gradient Descent for Training Physics-Informed Neural Networks</title>
      <link>https://arxiv.org/abs/2409.18426</link>
      <description>arXiv:2409.18426v1 Announce Type: cross 
Abstract: Physics-informed neural networks (PINNs) have emerged as a prominent approach for solving partial differential equations (PDEs) by minimizing a combined loss function that incorporates both boundary loss and PDE residual loss. Despite their remarkable empirical performance in various scientific computing tasks, PINNs often fail to generate reasonable solutions, and such pathological behaviors remain difficult to explain and resolve. In this paper, we identify that PINNs can be adversely trained when gradients of each loss function exhibit a significant imbalance in their magnitudes and present a negative inner product value. To address these issues, we propose a novel optimization framework, Dual Cone Gradient Descent (DCGD), which adjusts the direction of the updated gradient to ensure it falls within a dual cone region. This region is defined as a set of vectors where the inner products with both the gradients of the PDE residual loss and the boundary loss are non-negative. Theoretically, we analyze the convergence properties of DCGD algorithms in a non-convex setting. On a variety of benchmark equations, we demonstrate that DCGD outperforms other optimization algorithms in terms of various evaluation metrics. In particular, DCGD achieves superior predictive accuracy and enhances the stability of training for failure modes of PINNs and complex PDEs, compared to existing optimally tuned models. Moreover, DCGD can be further improved by combining it with popular strategies for PINNs, including learning rate annealing and the Neural Tangent Kernel (NTK).</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18426v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Youngsik Hwang, Dong-Young Lim</dc:creator>
    </item>
    <item>
      <title>Towards Event-Triggered NMPC for Efficient 6G Communications: Experimental Results and Open Problems</title>
      <link>https://arxiv.org/abs/2409.18589</link>
      <description>arXiv:2409.18589v1 Announce Type: cross 
Abstract: Networked control systems enable real-time control and coordination of distributed systems, leveraging the low latency, high reliability, and massive connectivity offered by 5G and future 6G networks. Applications include autonomous vehicles, robotics, industrial automation, and smart grids. Despite networked control algorithms admitting nominal stability guarantees even in the presence of delays and packet dropouts, their practical performance still heavily depends on the specific characteristics and conditions of the underlying network. To achieve the desired performance while efficiently using communication resources, co-design of control and communication is pivotal. Although periodic schemes, where communication instances are fixed, can provide reliable control performance, unnecessary transmissions, when updates are not needed, result in inefficient usage of network resources. In this paper, we investigate the potential for co-design of model predictive control and network communication. To this end, we design and implement an event-triggered nonlinear model predictive controller for stabilizing a Furuta pendulum communicating over a tailored open radio access network 6G research platform. We analyze the control performance as well as network utilization under varying channel conditions and event-triggering criteria. Our results show that the event-triggered control scheme achieves similar performance to periodic control with reduced communication demand.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18589v1</guid>
      <category>eess.SY</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jens P\"uttschneider, Julian Golembiewski, Niklas A. Wagner, Christian Wietfeld, Timm Faulwasser</dc:creator>
    </item>
    <item>
      <title>Quantum Algorithms for Drone Mission Planning</title>
      <link>https://arxiv.org/abs/2409.18631</link>
      <description>arXiv:2409.18631v1 Announce Type: cross 
Abstract: Mission planning often involves optimising the use of ISR (Intelligence, Surveillance and Reconnaissance) assets in order to achieve a set of mission objectives within allowed parameters subject to constraints. The missions of interest here, involve routing multiple UAVs visiting multiple targets, utilising sensors to capture data relating to each target. Finding such solutions is often an NP-Hard problem and cannot be solved efficiently on classical computers. Furthermore, during the mission new constraints and objectives may arise, requiring a new solution to be computed within a short time period. To achieve this we investigate near term quantum algorithms that have the potential to offer speed-ups against current classical methods. We demonstrate how a large family of these problems can be formulated as a Mixed Integer Linear Program (MILP) and then converted to a Quadratic Unconstrained Binary Optimisation (QUBO). The formulation provided is versatile and can be adapted for many different constraints with clear qubit scaling provided. We discuss the results of solving the QUBO formulation using commercial quantum annealers and compare the solutions to current edge classical solvers. We also analyse the results from solving the QUBO using Quantum Approximate Optimisation Algorithms (QAOA) and discuss their results. Finally, we also provide efficient methods to encode to the problem into the Variational Quantum Eigensolver (VQE) formalism, where we have tailored the ansatz to the problem making efficient use of the qubits available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18631v1</guid>
      <category>quant-ph</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ethan Davies, Pranav Kalidindi</dc:creator>
    </item>
    <item>
      <title>X-arability of mixed quantum states</title>
      <link>https://arxiv.org/abs/2409.18948</link>
      <description>arXiv:2409.18948v1 Announce Type: cross 
Abstract: The problem of determining when entanglement is present in a quantum system is one of the most active areas of research in quantum physics. Depending on the setting at hand, different notions of entanglement (or lack thereof) become relevant. Examples include separability (of bosons, fermions, and distinguishable particles), Schmidt number, biseparability, entanglement depth, and bond dimension. In this work, we propose and study a unified notion of separability, which we call X-arability, that captures a wide range of applications including these. For a subset (more specifically, an algebraic variety) of pure states X, we say that a mixed quantum state is X-arable if it lies in the convex hull of X. We develop unified tools and provable guarantees for X-arability, which already give new results for the standard separability problem. Our results include:
  -- An X-tension hierarchy of semidefinite programs for X-arability (generalizing the symmetric extensions hierarchy for separability), and a new de Finetti theorem for fermionic separability.
  -- A hierarchy of eigencomputations for optimizing a Hermitian operator over X, with applications to X-tanglement witnesses and polynomial optimization.
  -- A hierarchy of linear systems for the X-tangled subspace problem, with improved polynomial time guarantees even for the standard entangled subspace problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18948v1</guid>
      <category>quant-ph</category>
      <category>math.AG</category>
      <category>math.OC</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harm Derksen, Nathaniel Johnston, Benjamin Lovitz, Aravindan Vijayaraghavan</dc:creator>
    </item>
    <item>
      <title>Improved guarantees for optimal Nash equilibrium seeking and bilevel variational inequalities</title>
      <link>https://arxiv.org/abs/2307.12511</link>
      <description>arXiv:2307.12511v4 Announce Type: replace 
Abstract: We consider a class of hierarchical variational inequality (VI) problems that subsumes VI-constrained optimization and several other problem classes including the optimal solution selection problem and the optimal Nash equilibrium (NE) seeking problem. Our main contributions are threefold. (i) We consider bilevel VIs with monotone and Lipschitz continuous mappings and devise a single-timescale iteratively regularized extragradient method, named IR-EG$_{{\texttt{m,m}}}$. We improve the existing iteration complexity results for addressing both bilevel VI and VI-constrained convex optimization problems. (ii) Under the strong monotonicity of the outer level mapping, we develop a method named IR-EG$_{{\texttt{s,m}}}$ and derive faster guarantees than those in (i). We also study the iteration complexity of this method under a constant regularization parameter. These results appear to be new for both bilevel VIs and VI-constrained optimization. (iii) To our knowledge, complexity guarantees for computing the optimal NE in nonconvex settings do not exist. Motivated by this lacuna, we consider VI-constrained nonconvex optimization problems and devise an inexactly-projected gradient method, named IPR-EG, where the projection onto the unknown set of equilibria is performed using IR-EG$_{{\texttt{s,m}}}$ with a prescribed termination criterion and an adaptive regularization parameter. We obtain new complexity guarantees in terms of a residual map and an infeasibility metric for computing a stationary point. We validate the theoretical findings using preliminary numerical experiments for computing the best and the worst Nash equilibria.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.12511v4</guid>
      <category>math.OC</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sepideh Samadi, Farzad Yousefian</dc:creator>
    </item>
    <item>
      <title>The Black-Box Optimization Problem: Zero-Order Accelerated Stochastic Method via Kernel Approximation</title>
      <link>https://arxiv.org/abs/2310.02371</link>
      <description>arXiv:2310.02371v2 Announce Type: replace 
Abstract: In this paper, we study the standard formulation of an optimization problem when the computation of gradient is not available. Such a problem can be classified as a "black box" optimization problem, since the oracle returns only the value of the objective function at the requested point, possibly with some stochastic noise. Assuming convex, and higher-order of smoothness of the objective function, this paper provides a zero-order accelerated stochastic gradient descent (ZO-AccSGD) method for solving this problem, which exploits the higher-order of smoothness information via kernel approximation. As theoretical results, we show that the ZO-AccSGD algorithm proposed in this paper improves the convergence results of state-of-the-art (SOTA) algorithms, namely the estimate of iteration complexity. In addition, our theoretical analysis provides an estimate of the maximum allowable noise level at which the desired accuracy can be achieved. Validation of our theoretical results is demonstrated both on the model function and on functions of interest in the field of machine learning. We also provide a discussion in which we explain the results obtained and the superiority of the proposed algorithm over SOTA algorithms for solving the original problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.02371v2</guid>
      <category>math.OC</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aleksandr Lobanov, Nail Bashirov, Alexander Gasnikov</dc:creator>
    </item>
    <item>
      <title>A divergence-based condition to ensure quantile improvement in black-box global optimization</title>
      <link>https://arxiv.org/abs/2402.01277</link>
      <description>arXiv:2402.01277v3 Announce Type: replace 
Abstract: Black-box global optimization aims at minimizing an objective function whose analytical form is not known. To do so, many state-of-the-art methods rely on sampling-based strategies, where sampling distributions are built in an iterative fashion, so that their mass concentrate where the objective function is low. Despite empirical success, the theoretical study of these methods remains difficult. In this work, we introduce a new framework, based on divergence-decrease conditions, to study and design black-box global optimization algorithms. Our approach allows to establish and quantify the improvement of proposals at each iteration, in terms of expected value or quantile of the objective. We show that the information-geometric optimization approach fits within our framework, yielding a new approach for its analysis. We also establish proposal improvement results for two novel algorithms, one related with the cross-entropy approach with mixture models, and another one using heavy-tailed sampling proposal distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.01277v3</guid>
      <category>math.OC</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Thomas Guilmeau, Emilie Chouzenoux, V\'ictor Elvira</dc:creator>
    </item>
    <item>
      <title>Distributed Quasi-Newton Method for Multi-Agent Optimization</title>
      <link>https://arxiv.org/abs/2402.06778</link>
      <description>arXiv:2402.06778v2 Announce Type: replace 
Abstract: We present a distributed quasi-Newton (DQN) method, which enables a group of agents to compute an optimal solution of a separable multi-agent optimization problem locally using an approximation of the curvature of the aggregate objective function. Each agent computes a descent direction from its local estimate of the aggregate Hessian, obtained from quasi-Newton approximation schemes using the gradient of its local objective function. Moreover, we introduce a distributed quasi-Newton method for equality-constrained optimization (EC-DQN), where each agent takes Karush-Kuhn-Tucker-like update steps to compute an optimal solution. In our algorithms, each agent communicates with its one-hop neighbors over a peer-to-peer communication network to compute a common solution. We prove convergence of our algorithms to a stationary point of the optimization problem. In addition, we demonstrate the competitive empirical convergence of our algorithm in both well-conditioned and ill-conditioned optimization problems, in terms of the computation time and communication cost incurred by each agent for convergence, compared to existing distributed first-order and second-order methods. Particularly, in ill-conditioned problems, our algorithms achieve a faster computation time for convergence, while requiring a lower communication cost, across a range of communication networks with different degrees of connectedness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.06778v2</guid>
      <category>math.OC</category>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ola Shorinwa, Mac Schwager</dc:creator>
    </item>
    <item>
      <title>Distributed Model Predictive Control for Piecewise Affine Systems Based on Switching ADMM</title>
      <link>https://arxiv.org/abs/2404.16712</link>
      <description>arXiv:2404.16712v2 Announce Type: replace 
Abstract: This paper presents a novel approach for distributed model predictive control (MPC) for piecewise affine (PWA) systems. Existing approaches rely on solving mixed-integer optimization problems, requiring significant computation power or time. We propose a distributed MPC scheme that requires solving only convex optimization problems. The key contribution is a novel method, based on the alternating direction method of multipliers, for solving the non-convex optimal control problem that arises due to the PWA dynamics. We present a distributed MPC scheme, leveraging this method, that explicitly accounts for the coupling between subsystems by reaching agreement on the values of coupled states. Stability and recursive feasibility are shown under additional assumptions on the underlying system. Two numerical examples are provided, in which the proposed controller is shown to significantly improve the CPU time and closed-loop performance over existing state-of-the-art approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16712v2</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel Mallick, Azita Dabiri, Bart De Schutter</dc:creator>
    </item>
    <item>
      <title>Feature-Based Interpretable Surrogates for Optimization</title>
      <link>https://arxiv.org/abs/2409.01869</link>
      <description>arXiv:2409.01869v2 Announce Type: replace 
Abstract: For optimization models to be used in practice, it is crucial that users trust the results. A key factor in this aspect is the interpretability of the solution process. A previous framework for inherently interpretable optimization models used decision trees to map instances to solutions of the underlying optimization model. Based on this work, we investigate how we can use more general optimization rules to further increase interpretability and, at the same time, give more freedom to the decision-maker. The proposed rules do not map to a concrete solution but to a set of solutions characterized by common features. To find such optimization rules, we present an exact methodology using mixed-integer programming formulations as well as heuristics. We also outline the challenges and opportunities that these methods present. In particular, we demonstrate the improvement in solution quality that our approach offers compared to existing interpretable surrogates for optimization, and we discuss the relationship between interpretability and performance. These findings are supported by experiments using both synthetic and real-world data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01869v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marc Goerigk, Michael Hartisch, Sebastian Merten, Kartikey Sharma</dc:creator>
    </item>
    <item>
      <title>Linear Convergence in Hilbert's Projective Metric for Computing Augustin Information and a R\'{e}nyi Information Measure</title>
      <link>https://arxiv.org/abs/2409.02640</link>
      <description>arXiv:2409.02640v2 Announce Type: replace 
Abstract: Consider the problems of computing the Augustin information and a R\'{e}nyi information measure of statistical independence, previously explored by Lapidoth and Pfister (IEEE Information Theory Workshop, 2018) and Tomamichel and Hayashi (IEEE Trans. Inf. Theory, 64(2):1064--1082, 2018). Both quantities are defined as solutions to optimization problems and lack closed-form expressions. This paper analyzes two iterative algorithms: Augustin's fixed-point iteration for computing the Augustin information, and the algorithm by Kamatsuka et al. (arXiv:2404.10950) for the R\'{e}nyi information measure. Previously, it was only known that these algorithms converge asymptotically. We establish the linear convergence of Augustin's algorithm for the Augustin information of order $\alpha \in (1/2, 1) \cup (1, 3/2)$ and Kamatsuka et al.'s algorithm for the R\'{e}nyi information measure of order $\alpha \in [1/2, 1) \cup (1, \infty)$, using Hilbert's projective metric.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02640v2</guid>
      <category>math.OC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chung-En Tsai, Guan-Ren Wang, Hao-Chung Cheng, Yen-Huan Li</dc:creator>
    </item>
    <item>
      <title>A Proximal Variable Smoothing for Nonsmooth Minimization Involving Weakly Convex Composite with MIMO Application</title>
      <link>https://arxiv.org/abs/2409.10934</link>
      <description>arXiv:2409.10934v2 Announce Type: replace 
Abstract: We propose a proximal variable smoothing algorithm for nonsmooth optimization problem with sum of three functions involving weakly convex composite function. The proposed algorithm is designed as a time-varying forward-backward splitting algorithm with two steps: (i) a time-varying forward step with the gradient of a smoothed surrogate function, designed with the Moreau envelope, of the sum of two functions; (ii) the backward step with a proximity operator of the remaining function. For the proposed algorithm, we present a convergence analysis in terms of a stationary point by using a newly smoothed surrogate stationarity measure. As an application of the target problem, we also present a formulation of multiple-input-multiple-output (MIMO) signal detection with phase-shift keying. Numerical experiments demonstrate the efficacy of the proposed formulation and algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10934v2</guid>
      <category>math.OC</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Keita Kume, Isao Yamada</dc:creator>
    </item>
    <item>
      <title>Efficient Saddle Point Evasion and Local Minima Escape in High-Dimensional Non-Convex Optimization</title>
      <link>https://arxiv.org/abs/2409.12604</link>
      <description>arXiv:2409.12604v2 Announce Type: replace 
Abstract: This paper addresses the challenges of high-dimensional non-convex optimization, particularly the inefficiencies caused by saddle points. The authors propose several techniques for detecting, evading, and optimizing in the presence of these saddle points. We begin by analyzing saddle point detection through the Hessian spectrum, showing that the likelihood of encountering saddle points increases with dimensionality. We introduce stochastic gradient perturbation, which adds noise to escape saddle points and avoid premature convergence, and emphasize the importance of gradient flow dynamics and adaptive learning rates in ensuring convergence to local minima. The paper validates these methods within constrained optimization problems and explores randomized subspace optimization, reducing search space dimensionality while maintaining global convergence efficiency. These findings offer a comprehensive framework for enhancing the reliability and efficiency of high-dimensional non-convex optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12604v2</guid>
      <category>math.OC</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ronald Katende, Henry Kasumba</dc:creator>
    </item>
    <item>
      <title>Tikhonov regularized mixed-order primal-dual dynamical system for convex optimization problems with linear equality constraints</title>
      <link>https://arxiv.org/abs/2409.17493</link>
      <description>arXiv:2409.17493v2 Announce Type: replace 
Abstract: In Hilbert spaces, we consider a Tikhonov regularized mixed-order primal-dual dynamical system for a convex optimization problem with linear equality constraints. The dynamical system with general time-dependent parameters: viscous damping and temporal scaling can derive certain existing systems when special parameters are selected. When these parameters satisfy appropriate conditions and the Tikhonov regularization parameter \epsilon(t) approaches zero at an appropriate rate, we analyze the asymptotic convergence properties of the proposed system by constructing suitable Lyapunov functions. And we obtain that the objective function error enjoys O(1/(t^2\beta(t))) convergence rate. Under suitable conditions, it can be better than O(1/(t^2)). In addition, we utilize the Lyapunov analysis method to obtain the strong convergence of the trajectory generated by the Tikhonov regularized dynamical system. In particular, when Tikhonov regularization parameter \epsilon(t) vanishes to 0 at some suitable rate, the convergence rate of the primal-dual gap can be o(1/(\beta(t))). The effectiveness of our theoretical results has been demonstrated through numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17493v2</guid>
      <category>math.OC</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Honglu Li, Xin He, Yibin Xiao</dc:creator>
    </item>
    <item>
      <title>Covariance-Based Activity Detection in Cooperative Multi-Cell Massive MIMO: Scaling Law and Efficient Algorithms</title>
      <link>https://arxiv.org/abs/2311.15299</link>
      <description>arXiv:2311.15299v2 Announce Type: replace-cross 
Abstract: This paper focuses on the covariance-based activity detection problem in a multi-cell massive multiple-input multiple-output (MIMO) system. In this system, active devices transmit their signature sequences to multiple base stations (BSs), and the BSs cooperatively detect the active devices based on the received signals. While the scaling law for the covariance-based activity detection in the single-cell scenario has been extensively analyzed in the literature, this paper aims to analyze the scaling law for the covariance-based activity detection in the multi-cell massive MIMO system. Specifically, this paper demonstrates a quadratic scaling law in the multi-cell system, under the assumption that the path-loss exponent of the fading channel $\gamma &gt; 2.$ This finding shows that, in the multi-cell massive MIMO system, the maximum number of active devices that can be correctly detected in each cell increases quadratically with the length of the signature sequence and decreases logarithmically with the number of cells (as the number of antennas tends to infinity). Moreover, in addition to analyzing the scaling law for the signature sequences randomly and uniformly distributed on a sphere, the paper also establishes the scaling law for signature sequences based on a finite alphabet, which are easier to generate and store. Finally, this paper proposes two efficient accelerated coordinate descent (CD) algorithms with a convergence guarantee for solving the device activity detection problem. The first algorithm reduces the complexity of CD by using an inexact coordinate update strategy. The second algorithm avoids unnecessary computations of CD by using an active set selection strategy. Simulation results show that the proposed algorithms exhibit excellent performance in terms of computational efficiency and detection error probability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.15299v2</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyue Wang, Ya-Feng Liu, Zhaorui Wang, Wei Yu</dc:creator>
    </item>
    <item>
      <title>CMA-ES with Learning Rate Adaptation</title>
      <link>https://arxiv.org/abs/2401.15876</link>
      <description>arXiv:2401.15876v2 Announce Type: replace-cross 
Abstract: The covariance matrix adaptation evolution strategy (CMA-ES) is one of the most successful methods for solving continuous black-box optimization problems. A practically useful aspect of the CMA-ES is that it can be used without hyperparameter tuning. However, the hyperparameter settings still have a considerable impact on performance, especially for difficult tasks, such as solving multimodal or noisy problems. This study comprehensively explores the impact of learning rate on the CMA-ES performance and demonstrates the necessity of a small learning rate by considering ordinary differential equations. Thereafter, it discusses the setting of an ideal learning rate. Based on these discussions, we develop a novel learning rate adaptation mechanism for the CMA-ES that maintains a constant signal-to-noise ratio. Additionally, we investigate the behavior of the CMA-ES with the proposed learning rate adaptation mechanism through numerical experiments, and compare the results with those obtained for the CMA-ES with a fixed learning rate and with population size adaptation. The results show that the CMA-ES with the proposed learning rate adaptation works well for multimodal and/or noisy problems without extremely expensive learning rate tuning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.15876v2</guid>
      <category>cs.NE</category>
      <category>math.OC</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3698203</arxiv:DOI>
      <dc:creator>Masahiro Nomura, Youhei Akimoto, Isao Ono</dc:creator>
    </item>
    <item>
      <title>On the Convergence of A Data-Driven Regularized Stochastic Gradient Descent for Nonlinear Ill-Posed Problems</title>
      <link>https://arxiv.org/abs/2403.11787</link>
      <description>arXiv:2403.11787v2 Announce Type: replace-cross 
Abstract: Stochastic gradient descent (SGD) is a promising method for solving large-scale inverse problems, due to its excellent scalability with respect to data size. In this work, we analyze a new data-driven regularized stochastic gradient descent for the efficient numerical solution of a class of nonlinear ill-posed inverse problems in infinite dimensional Hilbert spaces. At each step of the iteration, the method randomly selects one equation from the nonlinear system combined with a corresponding equation from the learned system based on training data to obtain a stochastic estimate of the gradient and then performs a descent step with the estimated gradient. We prove the regularizing property of this method under the tangential cone condition and a priori parameter choice and then derive the convergence rates under the additional source condition and range invariance conditions. Several numerical experiments are provided to complement the analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11787v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zehui Zhou</dc:creator>
    </item>
    <item>
      <title>The Star Geometry of Critic-Based Regularizer Learning</title>
      <link>https://arxiv.org/abs/2408.16852</link>
      <description>arXiv:2408.16852v2 Announce Type: replace-cross 
Abstract: Variational regularization is a classical technique to solve statistical inference tasks and inverse problems, with modern data-driven approaches parameterizing regularizers via deep neural networks showcasing impressive empirical performance. Recent works along these lines learn task-dependent regularizers. This is done by integrating information about the measurements and ground-truth data in an unsupervised, critic-based loss function, where the regularizer attributes low values to likely data and high values to unlikely data. However, there is little theory about the structure of regularizers learned via this process and how it relates to the two data distributions. To make progress on this challenge, we initiate a study of optimizing critic-based loss functions to learn regularizers over a particular family of regularizers: gauges (or Minkowski functionals) of star-shaped bodies. This family contains regularizers that are commonly employed in practice and shares properties with regularizers parameterized by deep neural networks. We specifically investigate critic-based losses derived from variational representations of statistical distances between probability measures. By leveraging tools from star geometry and dual Brunn-Minkowski theory, we illustrate how these losses can be interpreted as dual mixed volumes that depend on the data distribution. This allows us to derive exact expressions for the optimal regularizer in certain cases. Finally, we identify which neural network architectures give rise to such star body gauges and when do such regularizers have favorable properties for optimization. More broadly, this work highlights how the tools of star geometry can aid in understanding the geometry of unsupervised regularizer learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16852v2</guid>
      <category>cs.LG</category>
      <category>math.MG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oscar Leong, Eliza O'Reilly, Yong Sheng Soh</dc:creator>
    </item>
    <item>
      <title>FracGM: A Fast Fractional Programming Technique for Geman-McClure Robust Estimator</title>
      <link>https://arxiv.org/abs/2409.13978</link>
      <description>arXiv:2409.13978v2 Announce Type: replace-cross 
Abstract: Robust estimation is essential in computer vision, robotics, and navigation, aiming to minimize the impact of outlier measurements for improved accuracy. We present a fast algorithm for Geman-McClure robust estimation, FracGM, leveraging fractional programming techniques. This solver reformulates the original non-convex fractional problem to a convex dual problem and a linear equation system, iteratively solving them in an alternating optimization pattern. Compared to graduated non-convexity approaches, this strategy exhibits a faster convergence rate and better outlier rejection capability. In addition, the global optimality of the proposed solver can be guaranteed under given conditions. We demonstrate the proposed FracGM solver with Wahba's rotation problem and 3-D point-cloud registration along with relaxation pre-processing and projection post-processing. Compared to state-of-the-art algorithms, when the outlier rates increase from 20% to 80%, FracGM shows 53% and 88% lower rotation and translation increases. In real-world scenarios, FracGM achieves better results in 13 out of 18 outcomes, while having a 19.43% improvement in the computation time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13978v2</guid>
      <category>cs.CV</category>
      <category>cs.RO</category>
      <category>math.OC</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bang-Shien Chen, Yu-Kai Lin, Jian-Yu Chen, Chih-Wei Huang, Jann-Long Chern, Ching-Cherng Sun</dc:creator>
    </item>
  </channel>
</rss>
