<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.OT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.OT</link>
    <description>stat.OT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.OT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 07 Aug 2025 04:03:59 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Blueprint to Design Curriculum and Pedagogy for Introductory Data Science</title>
      <link>https://arxiv.org/abs/2508.03952</link>
      <description>arXiv:2508.03952v1 Announce Type: new 
Abstract: As the demand for jobs in data science increases, so does the demand for universities to develop and facilitate modernized data science curricula to train students for these positions. Yet, the development of these courses remains challenging, especially at the introductory level. To help instructors to meet this demand, we present a flexible blueprint that supports the development of a modernized introductory data science curriculum. This blueprint is narrated through the lens and experience in teaching the introductory data science course at \university{}. This is a large course that serves both STEM and non-STEM majors and includes the incorporation and facilitation of technologies such as R, RStudio, Quarto, Git, and GitHub. We identify and provide discussion around common challenges in teaching a modernized introductory data science course, detail a learning model for students to grow their understanding of data science concepts, and provide reproducible materials to help empower teachers to adopt and adapt such curriculum at their universities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03952v1</guid>
      <category>stat.OT</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elijah Meyer, Mine \c{C}etinkaya-Rundel</dc:creator>
    </item>
    <item>
      <title>GeoSR: Cognitive-Agentic Framework for Probing Geospatial Knowledge Boundaries via Iterative Self-Refinement</title>
      <link>https://arxiv.org/abs/2508.04080</link>
      <description>arXiv:2508.04080v1 Announce Type: cross 
Abstract: Recent studies have extended the application of large language models (LLMs) to geographic problems, revealing surprising geospatial competence even without explicit spatial supervision. However, LLMs still face challenges in spatial consistency, multi-hop reasoning, and geographic bias. To address these issues, we propose GeoSR, a self-refining agentic reasoning framework that embeds core geographic principles -- most notably Tobler's First Law of Geography -- into an iterative prediction loop. In GeoSR, the reasoning process is decomposed into three collaborating agents: (1) a variable-selection agent that selects relevant covariates from the same location; (2) a point-selection agent that chooses reference predictions at nearby locations generated by the LLM in previous rounds; and (3) a refine agent that coordinates the iterative refinement process by evaluating prediction quality and triggering further rounds when necessary. This agentic loop progressively improves prediction quality by leveraging both spatial dependencies and inter-variable relationships. We validate GeoSR on tasks ranging from physical-world property estimation to socioeconomic prediction. Experimental results show consistent improvements over standard prompting strategies, demonstrating that incorporating geostatistical priors and spatially structured reasoning into LLMs leads to more accurate and equitable geospatial predictions. The code of GeoSR is available at https://github.com/JinfanTang/GeoSR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04080v1</guid>
      <category>cs.AI</category>
      <category>stat.OT</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinfan Tang, Kunming Wu, Ruifeng Gongxie, Yuya He, Yuankai Wu</dc:creator>
    </item>
    <item>
      <title>Producing treatment hierarchies in network meta-analysis using probabilistic models and treatment-choice criteria</title>
      <link>https://arxiv.org/abs/2406.10612</link>
      <description>arXiv:2406.10612v2 Announce Type: replace-cross 
Abstract: A key output of network meta-analysis (NMA) is the relative ranking of treatments; nevertheless, it has attracted substantial criticism. Existing ranking methods often lack clear interpretability and fail to adequately account for uncertainty, over-emphasizing small differences in treatment effects. We propose a novel framework to estimate treatment hierarchies in NMA using a probabilistic model, focusing on a clinically relevant treatment-choice criterion (TCC). Initially, we formulate a mathematical expression to define a TCC based on smallest worthwhile differences (SWD), converting NMA relative treatment effects into treatment preference format. This data is then synthesized using a probabilistic ranking model, assigning each treatment a latent 'ability' parameter, representing its propensity to yield clinically important and beneficial true treatment effects relative to the rest of the treatments in the network. Parameter estimation relies on the maximum likelihood theory, with standard errors derived asymptotically from Fisher's information matrix. To facilitate the use of our methods, we launched the R package mtrank. We applied our method to two clinical datasets: one comparing 18 antidepressants for major depression and another comparing 6 antihypertensives for the incidence of diabetes. Our approach provided robust, interpretable treatment hierarchies that account for a concrete TCC. We further examined the agreement between the proposed method and existing ranking metrics in 153 published networks, concluding that the degree of agreement depends on the precision of the NMA estimates. Our framework offers a valuable alternative for NMA treatment ranking, mitigating over-interpretation of minor differences. This enables more reliable and clinically meaningful treatment hierarchies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10612v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Theodoros Evrenoglou, Adriani Nikolakopoulou, Guido Schwarzer, Gerta R\"ucker, Anna Chaimani</dc:creator>
    </item>
    <item>
      <title>Survey Data Integration for Distribution Function Estimation</title>
      <link>https://arxiv.org/abs/2409.14284</link>
      <description>arXiv:2409.14284v3 Announce Type: replace-cross 
Abstract: Integration of probabilistic and non-probabilistic samples for the estimation of finite population totals (or means) has recently received considerable attention in the field of survey sampling; yet, to the best of our knowledge, this framework has not been extended to cumulative distribution function (CDF) estimation. To address this gap, we propose a novel CDF estimator that integrates data from probability samples with data from, potentially big, nonprobability samples. Assuming that a set of shared covariates are observed in both, while the response variable is observed only in the latter, the proposed estimator uses a survey-weighted empirical CDF of regression residuals trained on the convenience sample to estimate the CDF of the response variable. Under some assumptions, we derive the asymptotic bias and variance of our CDF estimator and show that it is asymptotically unbiased for the finite population CDF if ignorability holds. Our empirical results imply that the proposed CDF estimator is robust to model misspecification under ignorability, and robust to ignorability under model misspecification; when both assumptions are violated, our residual-based CDF estimator still outperforms its `plug-in' mass imputation and naive siblings, albeit with noted decreases in efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14284v3</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeremy Flood, Sayed Mostafa</dc:creator>
    </item>
  </channel>
</rss>
