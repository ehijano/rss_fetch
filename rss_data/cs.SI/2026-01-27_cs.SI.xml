<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SI</link>
    <description>cs.SI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 27 Jan 2026 05:00:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Unveiling hidden features of social evolution by inferring Langevin dynamics from data</title>
      <link>https://arxiv.org/abs/2601.17772</link>
      <description>arXiv:2601.17772v1 Announce Type: new 
Abstract: Are there hidden dynamical common patterns in the evolution of social and cultural history? While the growing availability of digitized social data invites us to answer this question, prevailing quantitative methods often rely on deterministic snapshots or average effects. Such approaches overlook the continuous and inherently uncertain nature of historical trajectories. In this paper, we propose a framework for modeling historical dynamics as stochastic processes described by stochastic differential equations (SDEs). By viewing historical change through the lens of continuous-time dynamics, this framework provides a natural language to describe how structural trends and inherent random fluctuations interact to shape societal evolution. This approach allows us to handle the uncertainty in fragmentary historical records, moving beyond the dichotomy of structural determinism versus pure chance. We demonstrate that adopting this stochastic perspective unlocks a rich suite of analytical capabilities unavailable to static models. Specifically, we introduce methods to: (1) quantify the irreversibility; (2) detect exogenous perturbations; (3) perform multiple imputation for missing historical records. This framework offers a unified methodology for dissecting the stability, contingency, and dynamics of historical change.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17772v1</guid>
      <category>cs.SI</category>
      <category>cond-mat.stat-mech</category>
      <category>physics.soc-ph</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Youngkyoung Bae, Hajime Shimao, Seungwoong Ha, Luna Yang, David Wolpert</dc:creator>
    </item>
    <item>
      <title>Explaining Synergistic Effects in Social Recommendations</title>
      <link>https://arxiv.org/abs/2601.18151</link>
      <description>arXiv:2601.18151v1 Announce Type: new 
Abstract: In social recommenders, the inherent nonlinearity and opacity of synergistic effects across multiple social networks hinders users from understanding how diverse information is leveraged for recommendations, consequently diminishing explainability. However, existing explainers can only identify the topological information in social networks that significantly influences recommendations, failing to further explain the synergistic effects among this information. Inspired by existing findings that synergistic effects enhance mutual information between inputs and predictions to generate information gain, we extend this discovery to graph data. We quantify graph information gain to identify subgraphs embodying synergistic effects. Based on the theoretical insights, we propose SemExplainer, which explains synergistic effects by identifying subgraphs that embody them. SemExplainer first extracts explanatory subgraphs from multi-view social networks to generate preliminary importance explanations for recommendations. A conditional entropy optimization strategy to maximize information gain is developed, thereby further identifying subgraphs that embody synergistic effects from explanatory subgraphs. Finally, SemExplainer searches for paths from users to recommended items within the synergistic subgraphs to generate explanations for the recommendations. Extensive experiments on three datasets demonstrate the superiority of SemExplainer over baseline methods, providing superior explanations of synergistic effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18151v1</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3774904.3792174</arxiv:DOI>
      <dc:creator>Yicong Li, Shan Jin, Qi Liu, Shuo Wang, Jiaying Liu, Shuo Yu, Qiang Zhang, Kuanjiu Zhou, Feng Xia</dc:creator>
    </item>
    <item>
      <title>Bowling Online: Accounting for Civil Society Reshaped into Streamlined Photons within a Fiber Network</title>
      <link>https://arxiv.org/abs/2601.17139</link>
      <description>arXiv:2601.17139v1 Announce Type: cross 
Abstract: Civil society has been deemed by various scholars, such as Robert D. Putnam, to be a predictor and a cornerstone of a robust and consolidated democracy (Putnam et al., 1993). Putnam highlights in his book Bowling Alone (2000) that American civil society has become weaker: people organize less, and literally, they bowl alone. But what if there is yet another aspect to Putnam's story that has not been fully accounted for, namely the rise of Digital Civil Society (DCS)? Perhaps people in the third decade of the 21st century bowl online. They still organize, mobilize, and care for their civil liberties and democratic institutions; however, the public sphere in which this takes place has shifted online to cyberspace (Bernholz et al., 2013) or to what still needs to be conceptualized, the digital public sphere (DPS), which this article attempts to measure and demarcate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17139v1</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lukasz W. Niparko</dc:creator>
    </item>
    <item>
      <title>Why They Link: An Intent Taxonomy for Including Hyperlinks in Social Posts</title>
      <link>https://arxiv.org/abs/2601.17601</link>
      <description>arXiv:2601.17601v1 Announce Type: cross 
Abstract: URLs serve as bridges between social media platforms and the broader web, linking user-generated content to external information resources. On Twitter (X), approximately one in five tweets contains at least one URL, underscoring their central role in information dissemination. While prior studies have examined the motivations of authors who share URLs, such author-centered intentions are difficult to observe in practice. To enable broader downstream use, this work investigates reader-centered interpretations, i.e., how users perceive the intentions behind hyperlinks included in posts. We develop an intent taxonomy for including hyperlinks in social posts through a hybrid approach that begins with a bottom-up, data-driven process using large-scale crowdsourced annotations, and is then refined using large language model assistance to generate descriptive category names and precise definitions. The final taxonomy comprises 6 top-level categories and 26 fine-grained intention classes, capturing diverse communicative purposes. Applying this taxonomy, we annotate and analyze 1000 user posts, revealing that advertising, arguing, and sharing are the most prevalent intentions. This resulting taxonomy provides a foundation for intent-aware information retrieval and NLP applications, enabling more accurate retrieval, recommendation, and understanding of social media content.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17601v1</guid>
      <category>cs.IR</category>
      <category>cs.SI</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fangping Lan, Abdullah Aljebreen, Eduard C. Dragut</dc:creator>
    </item>
    <item>
      <title>FedGraph-VASP: Privacy-Preserving Federated Graph Learning with Post-Quantum Security for Cross-Institutional Anti-Money Laundering</title>
      <link>https://arxiv.org/abs/2601.17935</link>
      <description>arXiv:2601.17935v1 Announce Type: cross 
Abstract: Virtual Asset Service Providers (VASPs) face a fundamental tension between regulatory compliance and user privacy when detecting cross-institutional money laundering. Current approaches require either sharing sensitive transaction data or operating in isolation, leaving critical cross-chain laundering patterns undetected. We present FedGraph-VASP, a privacy-preserving federated graph learning framework that enables collaborative anti-money laundering (AML) without exposing raw user data. Our key contribution is a Boundary Embedding Exchange protocol that shares only compressed, non-invertible graph neural network representations of boundary accounts. These exchanges are secured using post-quantum cryptography, specifically the NIST-standardized Kyber-512 key encapsulation mechanism combined with AES-256-GCM authenticated encryption. Experiments on the Elliptic Bitcoin dataset with realistic Louvain partitioning show that FedGraph-VASP achieves an F1-score of 0.508, outperforming the state-of-the-art generative baseline FedSage+ (F1 = 0.453) by 12.1 percent on binary fraud detection. We further show robustness under low-connectivity settings where generative imputation degrades performance, while approaching centralized performance (F1 = 0.620) in high-connectivity regimes. We additionally evaluate generalization on an Ethereum fraud detection dataset, where FedGraph-VASP (F1 = 0.635) is less effective under sparse cross-silo connectivity, while FedSage+ excels (F1 = 0.855), outperforming even local training (F1 = 0.785). These results highlight a topology-dependent trade-off: embedding exchange benefits connected transaction graphs, whereas generative imputation can dominate in highly modular sparse graphs. A privacy audit shows embeddings are only partially invertible (R^2 = 0.32), limiting exact feature recovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17935v1</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.SI</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Commey, Matilda Nkoom, Yousef Alsenani, Sena G. Hounsinou, Garth V. Crosby</dc:creator>
    </item>
    <item>
      <title>Evolving Networks Created by Preferential Attachment and Decay</title>
      <link>https://arxiv.org/abs/2601.17989</link>
      <description>arXiv:2601.17989v1 Announce Type: cross 
Abstract: Growing synthetic networks that follow power law distributions of a node's degree often involves adding one node at a time. Each node is added to the network with a fixed amount of edges and those edges are frozen for all future time steps. Yet real world networks often continuously evolve with edges being added and removed while new nodes are added to the network. Many existing growth models based on preferential attachment do not account for this evolutionary capability and when you extend their growth methods to add and remove edges to existing nodes the node degree distribution quickly loses its scale-free structure. This paper will go over a method to extend well known preferential attachment growth models to allow for the evolution of edges within a network while still maintaining a power law node degree distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17989v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.SI</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Inverse Generative Social Science Workshop (2021)</arxiv:journal_reference>
      <dc:creator>Justin Downes</dc:creator>
    </item>
    <item>
      <title>A Generative AI-Driven Reliability Layer for Action-Oriented Disaster Resilience</title>
      <link>https://arxiv.org/abs/2601.18308</link>
      <description>arXiv:2601.18308v1 Announce Type: cross 
Abstract: As climate-related hazards intensify, conventional early warning systems (EWS) disseminate alerts rapidly but often fail to trigger timely protective actions, leading to preventable losses and inequities. We introduce Climate RADAR (Risk-Aware, Dynamic, and Action Recommendation system), a generative AI-based reliability layer that reframes disaster communication from alerts delivered to actions executed. It integrates meteorological, hydrological, vulnerability, and social data into a composite risk index and employs guardrail-embedded large language models (LLMs) to deliver personalized recommendations across citizen, volunteer, and municipal interfaces. Evaluation through simulations, user studies, and a municipal pilot shows improved outcomes, including higher protective action execution, reduced response latency, and increased usability and trust. By combining predictive analytics, behavioral science, and responsible AI, Climate RADAR advances people-centered, transparent, and equitable early warning systems, offering practical pathways toward compliance-ready disaster resilience infrastructures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18308v1</guid>
      <category>cs.AI</category>
      <category>cs.SI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Geunsik Lim</dc:creator>
    </item>
    <item>
      <title>Beyond the Checkbox: Strengthening DSA Compliance Through Social Media Algorithmic Auditing</title>
      <link>https://arxiv.org/abs/2601.18405</link>
      <description>arXiv:2601.18405v1 Announce Type: cross 
Abstract: Algorithms of online platforms are required under the Digital Services Act (DSA) to comply with specific obligations concerning algorithmic transparency, user protection and privacy. To verify compliance with these requirements, DSA mandates platforms to undergo independent audits. Little is known about current auditing practices and their effectiveness in ensuring such compliance. To this end, we bridge regulatory and technical perspectives by critically examining selected audit reports across three critical algorithmic-related provisions: restrictions on profiling minors, transparency in recommender systems, and limitations on targeted advertising using sensitive data. Our analysis shows significant inconsistencies in methodologies and lack of technical depth when evaluating AI-powered systems. To enhance the depth, scale, and independence of compliance assessments, we propose to employ algorithmic auditing -- a process of behavioural assessment of AI algorithms by means of simulating user behaviour, observing algorithm responses and analysing them for audited phenomena.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18405v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.IR</category>
      <category>cs.SI</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3772318.3791774</arxiv:DOI>
      <dc:creator>Sara Solarova, Mat\'u\v{s} Mesar\v{c}\'ik, Branislav Pecher, Ivan Srba</dc:creator>
    </item>
    <item>
      <title>The Cost of Inflation</title>
      <link>https://arxiv.org/abs/2601.18544</link>
      <description>arXiv:2601.18544v1 Announce Type: cross 
Abstract: Empirical evidence suggests that there is little to no correlation between the rate of inflation and the size of price change. Economists have hitherto taken this to mean that monetary shocks do not generate much deviation in relative prices and therefore inflation does not hurt the economy by impeding the workings of the price system. This paper presents a production network model of inflationary dynamics in which it is well possible for inflation to have near-zero correlation with the size of price change yet cause significant distortion of relative prices. The relative price distortion caused by inflation critically depends on the spectral gap, degree distribution, and assortativity of the production network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18544v1</guid>
      <category>econ.TH</category>
      <category>cs.SI</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vipin P Veetil</dc:creator>
    </item>
    <item>
      <title>Brazilian Social Media Anti-vaccine Information Disorder Dataset -- Telegram (2020-2025)</title>
      <link>https://arxiv.org/abs/2601.18622</link>
      <description>arXiv:2601.18622v1 Announce Type: cross 
Abstract: Over the past decade, Brazil has experienced a decline in vaccination coverage, reversing decades of public health progress achieved through the National Immunization Program (PNI). Growing evidence points to the widespread circulation of vaccine-related misinformation -- particularly on social media platforms -- as a key factor driving this decline. Among these platforms, Telegram remains the only major platform permitting accessible and ethical data collection, offering insight into public channels where vaccine misinformation circulates extensively. This data paper introduces a curated dataset of about four million Telegram posts collected from 119 prominent Brazilian anti-vaccine channels between 2020 and 2025. The dataset includes message content, metadata, associated media, and classification related to vaccine posts, enabling researchers to examine how false or misleading information spreads, evolves, and influences public sentiment. By providing this resource, our aim is to support the scientific and public health community in developing evidence-based strategies to counter misinformation, promote trust in vaccination, and engage compassionately with individuals and communities affected by false narratives. The dataset and documentation are openly available for non-commercial research, under strict ethical and privacy guidelines at https://doi.org/10.25824/redu/5JIVDT</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18622v1</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jo\~ao Phillipe Cardenuto, Ana Carolina Monari, Michelle Diniz Lopes, Leopoldo Lusquino Filho, Anderson Rocha</dc:creator>
    </item>
    <item>
      <title>MobileCity: An Efficient Framework for Large-Scale Urban Behavior Simulation</title>
      <link>https://arxiv.org/abs/2504.16946</link>
      <description>arXiv:2504.16946v4 Announce Type: replace 
Abstract: Generative agents offer promising capabilities for simulating realistic urban behaviors. However, existing methods oversimplify transportation choices, rely heavily on static agent profiles leading to behavioral homogenization, and inherit prohibitive computational costs. To address these limitations, we present MobileCity, a lightweight simulation platform designed to model realistic urban mobility with high computational efficiency. We introduce a comprehensive transportation system with multiple transport modes, and collect questionnaire data from respondents to construct agent profiles. To enable scalable simulation, agents perform action selection within a pre-generated action space and uses local models for efficient agent memory generation. Through extensive micro and macro-level evaluations on 4,000 agents, we demonstrate that MobileCity generates more realistic urban behaviors than baselines while maintaining computational efficiency. We further explore practical applications such as predicting movement patterns and analyzing demographic trends in transportation preferences. Our code is publicly available at https://github.com/Tony-Yip/MobileCity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16946v4</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaotong Ye, Nicolas Bougie, Toshihiko Yamasaki, Narimasa Watanabe</dc:creator>
    </item>
    <item>
      <title>From Aggregation to Selection: User-Validated Distributed Social Recommendation</title>
      <link>https://arxiv.org/abs/2505.21388</link>
      <description>arXiv:2505.21388v4 Announce Type: replace 
Abstract: Social recommender systems facilitate social connections by identifying potential friends for users. Each user maintains a local social network centered around themselves, resulting in a naturally distributed social structure. Recent research on distributed modeling for social recommender systems has gained increasing attention, as it naturally aligns with the user-centric structure of user interactions. Current distributed social recommender systems rely on automatically combining predictions from multiple models, often overlooking the user's active role in validating whether suggested connections are appropriate. Moreover, recommendation decisions are validated by individual users rather than derived from a single global ordering of candidates. As a result, standard ranking-based evaluation metrics make it difficult to evaluate whether a user-confirmed recommendation decision is actually correct. To address these limitations, we propose DeSocial, a distributed social recommendation framework with user-validation. DeSocial enables users to select recommendation algorithms to validate their potential connections, and the verification is processed through majority consensus among multiple independent user validators. To evaluate the distributed recommender system with user validator, we formulate this setting as a link prediction and verification task and introduce Acc@K, a consensus-based evaluation metric that measures whether user-approved recommendations are correct. Experiments on 4 real-world social networks shows that DeSocial improves decision correctness and robustness compared to single-point and distributed baselines. These findings highlight the potential of user-validated distributed recommender systems as a practical approach to social recommendation, with broader applicability to distributed and decentralized recommendations. Code: https://github.com/agiresearch/DeSocial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21388v4</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingyuan Huang, Dan Luo, Zihe Ye, Weixin Chen, Minghao Guo, Yongfeng Zhang</dc:creator>
    </item>
    <item>
      <title>Genomic-Informed Heterogeneous Graph Learning for Spatiotemporal Avian Influenza Outbreak Forecasting</title>
      <link>https://arxiv.org/abs/2505.22692</link>
      <description>arXiv:2505.22692v4 Announce Type: replace 
Abstract: Accurate forecasting of Avian Influenza Virus (AIV) outbreaks within wild bird populations necessitates models that account for complex, multi-scale transmission patterns driven by diverse factors. While conventional spatiotemporal epidemic models are robust for human-centric diseases, they rely on spatial homophily and diffusive transmission between geographic regions. This simplification is incomplete for AIV as it neglects valuable genomic information critical for capturing dynamics like high-frequency reassortment and lineage turnover at the case level (e.g., genetic descent across regions), which are essential for understanding AIV spread. To address these limitations, we systematically formulate the AIV forecasting problem and propose a Bi-Layer genomic-aware heterogeneous graph fusion pipeline. This pipeline integrates genetic, spatial, and ecological data to achieve highly accurate outbreak forecasting. It 1) defines a multi-layered graph structure incorporating information from diverse sources and multiple layers (case and location), 2) applies cross-relation smoothing to smooth information flow across edge types, 3) performs graph fusion that preserves critical structural patterns backed by theoretical spectral guarantees, and 4) forecasts future outbreaks using an autoregressive graph sequence model to capture transmission dynamics. To support research, we release the Avian-US dataset, which provides comprehensive genetic, spatial, and ecological data on US avian influenza outbreaks. BLUE demonstrates superior performance over existing baselines, highlighting the efficacy of integrating multi-layer information for infectious disease forecasting. The code is available at: https://github.com/jingdu-cs/BLUE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22692v4</guid>
      <category>cs.SI</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jing Du, Haley Stone, Yang Yang, Ashna Desai, Hao Xue, Andreas Z\"ufle, Chandini Raina MacIntyre, Flora D. Salim</dc:creator>
    </item>
    <item>
      <title>The Persistence of Retracted Papers on Wikipedia</title>
      <link>https://arxiv.org/abs/2509.18403</link>
      <description>arXiv:2509.18403v3 Announce Type: replace-cross 
Abstract: Wikipedia serves as a key infrastructure for public access to scientific knowledge, but it faces challenges in maintaining the credibility of cited sources--especially when scientific papers are retracted. This paper investigates how citations to retracted research are handled on English Wikipedia. We construct a novel dataset that integrates Wikipedia revision histories with metadata from Retraction Watch, Crossref, Altmetric, and OpenAlex, identifying 1,181 citations of retracted papers. We find that 71.6% of the citations were initially problematic and in need of reader-facing repair, defined as those added before the paper's retraction (51.5%) or introduced afterwards without proper warning (20.1%). While many are eventually corrected, our analysis reveals that these citations persist for a median of 3.68 years (1,344 days). Through survival analysis, we find that bot-mediated flagging (RetractionBot), open access availability, pre-existing online visibility (e.g., Twitter/X mention counts), and page-level organization (e.g., number of categories on a Wikipedia page) are associated with a higher hazard of correction. Conversely, a paper's established scholarly authority--a higher academic citation count--is associated with a slower time to correction. Our findings highlight how the Wikipedia community supports collaborative maintenance but leaves gaps in citation-level repair. We contribute to CSCW research by advancing our understanding of this sociotechnical vulnerability, which takes the form of a community coordination challenge, and by offering design directions to support citation credibility at scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18403v3</guid>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haohan Shi, Yulin Yu, Daniel M. Romero, Em\H{o}ke-\'Agnes Horv\'at</dc:creator>
    </item>
    <item>
      <title>Multiconnectivity for SAGIN: Current Trends, Challenges, AI-driven Solutions, and Opportunities</title>
      <link>https://arxiv.org/abs/2512.21717</link>
      <description>arXiv:2512.21717v2 Announce Type: replace-cross 
Abstract: Space-air-ground-integrated network (SAGIN)-enabled multiconnectivity (MC) is emerging as a key enabler for next-generation networks, enabling users to simultaneously utilize multiple links across multi-layer non-terrestrial networks (NTN) and multi-radio access technology (multi-RAT) terrestrial networks (TN). However, the heterogeneity of TN and NTN introduces complex architectural challenges that complicate MC implementation. Specifically, the diversity of link types, spanning air-to-air, air-to-space, space-to-space, space-to-ground, and ground-to-ground communications, renders optimal resource allocation highly complex. Recent advancements in reinforcement learning (RL) and agentic artificial intelligence (AI) have shown remarkable effectiveness in optimal decision-making in complex and dynamic environments. In this paper, we review the current developments in SAGIN-enabled MC and outline the key challenges associated with its implementation. We further highlight the transformative potential of AI-driven approaches for resource optimization in a heterogeneous SAGIN environment. To this end, we present a case study on resource allocation optimization enabled by agentic RL for SAGIN-enabled MC involving diverse radio access technologies (RATs). Results show that learning-based methods can effectively handle complex scenarios and substantially enhance network performance in terms of latency and capacity while incurring a moderate increase in power consumption as an acceptable tradeoff. Finally, open research problems and future directions are presented to realize efficient SAGIN-enabled MC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21717v2</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abd Ullah Khan, Adnan Shahid, Haejoon Jung, Hyundong Shin</dc:creator>
    </item>
    <item>
      <title>The R&amp;D Productivity Puzzle: Innovation Networks with Heterogeneous Firms</title>
      <link>https://arxiv.org/abs/2512.23337</link>
      <description>arXiv:2512.23337v2 Announce Type: replace-cross 
Abstract: We introduce heterogeneous R&amp;D productivities into an endogenous R&amp;D network formation model, generalizing the framework of Goyal and Moraga-Gonz\'{a}lez (2001). Heterogeneous productivities endogenously create asymmetric gains from collaboration: less productive firms benefit disproportionately from links, while more productive firms exert greater R&amp;D effort and incur higher costs. When productivity gaps are sufficiently large, more productive firms experience lower profits from collaborating with less productive partners. As a result, the complete network -- stable under homogeneity -- becomes unstable, and the positive assortative (PA) network, in which firms cluster by R&amp;D productivity, emerges as pairwise stable. Using simulations, we show that the clustered structure delivers higher welfare than the complete network; nevertheless, welfare under this formation follows an inverted U-shape as the fraction of high-productivity firms increases, reflecting crowding-out effects at high fractions. Altogether, we uncover an R&amp;D productivity puzzle: economies with higher average R&amp;D productivity may exhibit lower welfare through (i) the formation of alternative stable networks, or (ii) a crowding-out effect of high-productivity firms. Our findings highlight that productivity is a structural force reshaping the organization of innovation. Productivity-enhancing policies must therefore account for their effects on endogenous R&amp;D alliances and effort, as they may reverse the intended welfare gains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23337v2</guid>
      <category>econ.GN</category>
      <category>cs.SI</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M. Sadra Heydari, Zafer Kanik, Santiago Montoya-Bland\'on</dc:creator>
    </item>
    <item>
      <title>CooperBench: Why Coding Agents Cannot be Your Teammates Yet</title>
      <link>https://arxiv.org/abs/2601.13295</link>
      <description>arXiv:2601.13295v2 Announce Type: replace-cross 
Abstract: Resolving team conflicts requires not only task-specific competence, but also social intelligence to find common ground and build consensus. As AI agents increasingly collaborate on complex work, they must develop coordination capabilities to function as effective teammates. Yet we hypothesize that current agents lack these capabilities. To test this, we introduce CooperBench, a benchmark of over 600 collaborative coding tasks across 12 libraries in 4 programming languages. Each task assigns two agents different features that can be implemented independently but may conflict without proper coordination. Tasks are grounded in real open-source repositories with expert-written tests. Evaluating state-of-the-art coding agents, we observe the curse of coordination: agents achieve on average 30% lower success rates when working together compared to performing both tasks individually. This contrasts sharply with human teams, where adding teammates typically improves productivity. Our analysis reveals three key issues: (1) communication channels become jammed with vague, ill-timed, and inaccurate messages; (2) even with effective communication, agents deviate from their commitments; and (3) agents often hold incorrect expectations about others' plans and communication. Through large-scale simulation, we also observe rare but interesting emergent coordination behavior including role division, resource division, and negotiation. Our research presents a novel benchmark for collaborative coding and calls for a shift from pursuing individual agent capability to developing social intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13295v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.MA</category>
      <category>cs.SI</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Arpandeep Khatua, Hao Zhu, Peter Tran, Arya Prabhudesai, Frederic Sadrieh, Johann K. Lieberwirth, Xinkai Yu, Yicheng Fu, Michael J. Ryan, Jiaxin Pei, Diyi Yang</dc:creator>
    </item>
    <item>
      <title>Collective intelligence in science: direct elicitation of diverse information from experts with unknown information structure</title>
      <link>https://arxiv.org/abs/2601.14047</link>
      <description>arXiv:2601.14047v2 Announce Type: replace-cross 
Abstract: Suppose we need a deep collective analysis of an open scientific problem: there is a complex scientific hypothesis and a large online group of mutually unrelated experts with relevant private information of a diverse and unpredictable nature. This information may be results of experts' individual experiments, original reasoning of some of them, results of AI systems they use, etc. We propose a simple mechanism based on a self-resolving play-money prediction market entangled with a chat. We show that such a system can easily be brought to an equilibrium where participants directly share their private information on the hypothesis through the chat and trade as if the market were resolved in accordance with the truth of the hypothesis. This approach will lead to efficient aggregation of relevant information in a completely interpretable form even if the ground truth cannot be established and experts initially know nothing about each other and cannot perform complex Bayesian calculations. Finally, by rewarding the experts with some real assets proportionally to the play money they end up with, we can get an innovative way to fund large-scale collaborative studies of any type.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14047v2</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <category>cs.SI</category>
      <category>econ.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexey V. Osipov, Nikolay N. Osipov</dc:creator>
    </item>
  </channel>
</rss>
