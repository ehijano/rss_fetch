<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 06 Nov 2025 02:44:55 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A new stochastic diffusion process to model and predict electricity production from natural gas sources in the United States</title>
      <link>https://arxiv.org/abs/2511.01925</link>
      <description>arXiv:2511.01925v1 Announce Type: new 
Abstract: This paper introduces a new stochastic diffusion process to model the electricity production from natural gas sources (as a percentage of total electricity production) in the United States. The method employs trend function analysis to generate fits and forecasts with both conditional and unconditional estimated trend functions. Parameters are estimated using the maximum likelihood (ML) method, based on discrete sampling paths of the variable "electricity production from natural gas sources in the United States" with annual data from 1990 to 2021. The results show that the proposed model effectively fits the data and provides dependable medium-term forecasts for 2022-2023.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01925v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Safa' Alsheyab</dc:creator>
    </item>
    <item>
      <title>Enhancing Phenotype Discovery in Electronic Health Records through Prior Knowledge-Guided Unsupervised Learning</title>
      <link>https://arxiv.org/abs/2511.02102</link>
      <description>arXiv:2511.02102v1 Announce Type: new 
Abstract: Objectives: Unsupervised learning with electronic health record (EHR) data has shown promise for phenotype discovery, but approaches typically disregard existing clinical information, limiting interpretability. We operationalize a Bayesian latent class framework for phenotyping that incorporates domain-specific knowledge to improve clinical meaningfulness of EHR-derived phenotypes and illustrate its utility by identifying an asthma sub-phenotype informed by features of Type 2 (T2) inflammation.
  Materials and methods: We illustrate a framework for incorporating clinical knowledge into a Bayesian latent class model via informative priors to guide unsupervised clustering toward clinically relevant subgroups. This approach models missingness, accounting for potential missing-not-at-random patterns, and provides patient-level probabilities for phenotype assignment with uncertainty. Using reusable and flexible code, we applied the model to a large asthma EHR cohort, specifying informative priors for T2 inflammation-related features and weakly informative priors for other clinical variables, allowing the data to inform posterior distributions.
  Results and Conclusion: Using encounter data from January 2017 to February 2024 for 44,642 adult asthma patients, we found a bimodal posterior distribution of phenotype assignment, indicating clear class separation. The T2 inflammation-informed class (38.7%) was characterized by elevated eosinophil levels and allergy markers, plus high healthcare utilization and medication use, despite weakly informative priors on the latter variables. These patterns suggest an "uncontrolled T2-high" sub-phenotype. This demonstrates how our Bayesian latent class modeling approach supports hypothesis generation and cohort identification in EHR-based studies of heterogeneous diseases without well-established phenotype definitions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02102v1</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Melanie Mayer, Kimberly Lactaoen, Gary E. Weissman, Blanca E. Himes, Rebecca A. Hubbard</dc:creator>
    </item>
    <item>
      <title>Wavelet Based Cross Correlations with Applications</title>
      <link>https://arxiv.org/abs/2511.02174</link>
      <description>arXiv:2511.02174v1 Announce Type: new 
Abstract: Wavelet Transforms are a widely used technique for decomposing a signal into coefficient vectors that correspond to distinct frequency/scale bands while retaining time localization. This property enables an adaptive analysis of signals at different scales, capturing both temporal and spectral patterns. By examining how correlations between two signals vary across these scales, we obtain a more nuanced understanding of their relationship than what is possible from a single global correlation measure. In this work, we expand on the theory of wavelet-based correlations already used in the literature and elaborate on wavelet correlograms, partial wavelet correlations, and additive wavelet correlations using the Pearson and Kendall definitions. We use both Orthogonal and Non-decimated discrete Wavelet Transforms, and assess the robustness of these correlations under different wavelet bases. Simulation studies are conducted to illustrate these methods, and we conclude with applications to real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02174v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jack Kissell, Vijini Lakmini, Brani Vidakovic</dc:creator>
    </item>
    <item>
      <title>A generic network theoretic based model to classify SDG indicators</title>
      <link>https://arxiv.org/abs/2511.02289</link>
      <description>arXiv:2511.02289v1 Announce Type: new 
Abstract: To achieve the United Nations Sustainable Development Goals, coordinated action across their interlinked indicators is required. Although most of the research on the interlinkages of the SDGs is done at the goal level, policies are usually made and implemented at the level of indicators (or targets). Our study examines the existing literature on SDG interlinkages and indicator (or target) prioritization, highlighting important drawbacks of current methodologies. To address these limitations, we propose a generic network-based model that can quantify the importance of the SDG indicators and help policymakers in identifying indicators for maximum synergistic impact. Our model applies to any country, offering a tool for national policymakers. We illustrate the application of this model using data from India, identifying important indicators that are crucial for accelerating progress in the SDGs. While our main contribution lies in developing this network-theoretic methodology, we also provide supporting empirical evidence from existing literature for selected key observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02289v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gaurav Kottari, Qazi J. Azhad, Niteesh Sahni</dc:creator>
    </item>
    <item>
      <title>Identification of Separable OTUs for Multinomial Classification in Compositional Data Analysis</title>
      <link>https://arxiv.org/abs/2511.02509</link>
      <description>arXiv:2511.02509v1 Announce Type: new 
Abstract: High-throughput sequencing has transformed microbiome research, but it also produces inherently compositional data that challenge standard statistical and machine learning methods. In this work, we propose a multinomial classification framework for compositional microbiome data based on penalized log-ratio regression and pairwise separability screening. The method quantifies the discriminative ability of each OTU through the area under the receiver operating characteristic curve ($AUC$) for all pairwise log-ratios and aggregates these values into a global separability index $S_k$, yielding interpretable rankings of taxa together with confidence intervals. We illustrate the approach by reanalyzing the Baxter colorectal adenoma dataset and comparing our results with Greenacre's ordination-based analysis using Correspondence Analysis and Canonical Correspondence Analysis. Our models consistently recover a core subset of taxa previously identified as discriminant, thereby corroborating Greenacre's main findings, while also revealing additional OTUs that become important once demographic covariates are taken into account. In particular, adjustment for age, gender, and diabetes medication improves the precision of the separation index and highlights new, potentially relevant taxa, suggesting that part of the original signal may have been influenced by confounding. Overall, the integration of log-ratio modeling, covariate adjustment, and uncertainty estimation provides a robust and interpretable framework for OTU selection in compositional microbiome data. The proposed method complements existing ordination-based approaches by adding a probabilistic and inferential perspective, strengthening the identification of biologically meaningful microbial signatures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02509v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>R. Alberich, N. A. Cruz, R. Fern\'andez, I. Garc\'ia Mosquera, A. Mir, F. Rosell\'o</dc:creator>
    </item>
    <item>
      <title>Extended Kalman Filtering on Stiefel Manifolds</title>
      <link>https://arxiv.org/abs/2511.02682</link>
      <description>arXiv:2511.02682v1 Announce Type: new 
Abstract: A generalisation of the extended Kalman filter for Stiefel manifold-valued measurements is presented. We provide simulations on the 2-sphere and the space of orthogonal 4-by-2 matrices which show significant improvement of the Extended Kalman Filter compared to only relying on raw measurements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02682v1</guid>
      <category>stat.AP</category>
      <category>math.DG</category>
      <category>stat.CO</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jordi-Llu\'is Figueras, Aron Persson, Lauri Viitasaari</dc:creator>
    </item>
    <item>
      <title>Fast Measuring Pavement Crack Width by Cascading Principal Component Analysis</title>
      <link>https://arxiv.org/abs/2511.02144</link>
      <description>arXiv:2511.02144v1 Announce Type: cross 
Abstract: Accurate quantification of pavement crack width plays a pivotal role in assessing structural integrity and guiding maintenance interventions. However, achieving precise crack width measurements presents significant challenges due to: (1) the complex, non-uniform morphology of crack boundaries, which limits the efficacy of conventional approaches, and (2) the demand for rapid measurement capabilities from arbitrary pixel locations to facilitate comprehensive pavement condition evaluation. To overcome these limitations, this study introduces a cascaded framework integrating Principal Component Analysis (PCA) and Robust PCA (RPCA) for efficient crack width extraction from digital images. The proposed methodology comprises three sequential stages: (1) initial crack segmentation using established detection algorithms to generate a binary representation, (2) determination of the primary orientation axis for quasi-parallel cracks through PCA, and (3) extraction of the Main Propagation Axis (MPA) for irregular crack geometries using RPCA. Comprehensive evaluations were conducted across three publicly available datasets, demonstrating that the proposed approach achieves superior performance in both computational efficiency and measurement accuracy compared to existing state-of-the-art techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02144v1</guid>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhicheng Wang, Junbiao Pang</dc:creator>
    </item>
    <item>
      <title>Bayesian spatio-temporal weighted regression for integrating missing and misaligned environmental data</title>
      <link>https://arxiv.org/abs/2511.02149</link>
      <description>arXiv:2511.02149v1 Announce Type: cross 
Abstract: Estimating environmental exposures from multi-source data is central to public health research and policy. Integrating data from satellite products and ground monitors are increasingly used to produce exposure surfaces. However, spatio-temporal misalignment often induced from missing data introduces substantial uncertainty and reduces predictive accuracy. We propose a Bayesian weighted predictor regression framework that models spatio-temporal relationships when predictors are observed on irregular supports or have substantial missing data, and are not concurrent with the outcome. The key feature of our model is a spatio-temporal kernel that aggregates the predictor over local space-time neighborhoods, built directly into the likelihood, eliminating any separate gap-filling or forced data alignment stage. We introduce a numerical approximation using a Voronoi-based spatial quadrature combined with irregular temporal increments for estimation under data missingness and misalignment. We showed that misspecification of the spatial and temporal lags induced bias in the mean and parameter estimates, indicating the need for principled parameter selection. Simulation studies confirmed these theoretical findings, where careful tuning was critical to control bias and achieve accurate prediction, while the proposed quadrature performed well under severe missingness. As an illustrative application, we estimated fine particulate matter (PM$_{2.5}$) in northern California using satellite-derived aerosol optical depth (AOD) and wildfire smoke plume indicators. Relative to a traditional collocated linear model, our approach improved out-of-sample predictive performance (over 50\% increase in R$^2$), reduced uncertainty, and yielded robust temporal predictions and spatial surface estimation. Our framework is extensible to additional spatio-temporally varying covariates and other kernel families.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02149v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yovna Junglee, Vianey Leos Barajas, Meredith Franklin</dc:creator>
    </item>
    <item>
      <title>Cluster Size Matters: A Comparative Study of Notip and pARI for Post Hoc Inference in fMRI</title>
      <link>https://arxiv.org/abs/2511.02422</link>
      <description>arXiv:2511.02422v1 Announce Type: cross 
Abstract: All Resolutions Inference (ARI) is a post hoc inference method for functional Magnetic Resonance Imaging (fMRI) data analysis that provides valid lower bounds on the proportion of truly active voxels within any, possibly data-driven, cluster. As such, it addresses the paradox of spatial specificity encountered with more classical cluster-extent thresholding methods. It allows the cluster-forming threshold to be increased in order to locate the signal with greater spatial precision without overfitting, also known as the drill-down approach. Notip and pARI are two recent permutation-based extensions of ARI designed to increase statistical power by accounting for the strong dependence structure typical of fMRI data. A recent comparison between these papers based on large voxel clusters concluded that pARI outperforms Notip. We revisit this conclusion by conducting a systematic comparison of the two. Our reanalysis of the same fMRI data sets from the Neurovault database demonstrates the existence of complementary performance regimes: while pARI indeed achieves higher sensitivity for large clusters, Notip provides more informative and robust results for smaller clusters. In particular, while Notip supports informative ``drill-down'' exploration into subregions of activation, pARI often yields non-informative bounds in such cases, and can even underperform the baseline ARI method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02422v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nils Peyrouset (ENSAE), Pierre Neuvial (IMT), Bertrand Thirion (PARIETAL)</dc:creator>
    </item>
    <item>
      <title>Stochastic Redistribution of Indistinguishable Items in Shared Habitation: A Multi-Agent Simulation Framework</title>
      <link>https://arxiv.org/abs/2511.02648</link>
      <description>arXiv:2511.02648v1 Announce Type: cross 
Abstract: This paper presents a discrete-event stochastic model for the redistribution of indistinguishable personal items, exemplified by socks, among multiple cohabitants sharing a communal laundry system. Drawing on concepts from ecological population dynamics, diffusion processes, and stochastic exchange theory, the model captures the probabilistic mechanisms underlying item mixing, recovery, and loss. Each cohabitant is represented as an autonomous agent whose belongings interact through iterative cycles of collective washing, sorting, and partial correction. The system's evolution is characterized by random mixing events, selective recollection, and attrition over time. Implemented using the SimPy discrete-event simulation framework, the model demonstrates that even minimal exchange probabilities can generate emergent asymmetries, quasi-equilibrium distributions, and long-term disorder. The findings illustrate how stochastic processes inherent to shared domestic systems can produce persistent imbalances, offering a quantitative perspective on an everyday social phenomenon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02648v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.AP</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Syed Haseeb Shah</dc:creator>
    </item>
    <item>
      <title>On the Convergence of the Extended Kalman Filter on Stiefel Manifolds when Observing a Constant Particle with Measurement Errors</title>
      <link>https://arxiv.org/abs/2511.02680</link>
      <description>arXiv:2511.02680v1 Announce Type: cross 
Abstract: In this paper we first introduce the setting of filtering on Stiefel manifolds. Then, assuming the underlying system process is constant, the convergence of the extended Kalman filter with Stiefel manifold-valued observations is proved. This corresponds to the case where one has measurement errors that needs to be filtered. Finally, some simulations are presented for a selected few Stiefel manifolds and the speed of convergence is studied.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02680v1</guid>
      <category>math.ST</category>
      <category>math.DG</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jordi-Llu\'is Figueras, Aron Persson, Lauri Viitasaari</dc:creator>
    </item>
    <item>
      <title>Random time-shift approximation enables hierarchical Bayesian inference of mechanistic within-host viral dynamics models on large datasets</title>
      <link>https://arxiv.org/abs/2507.02884</link>
      <description>arXiv:2507.02884v3 Announce Type: replace 
Abstract: Mechanistic mathematical models of within-host viral dynamics are tools for understanding how a virus' biology and its interaction with the immune system shape the infectivity of a host. The biology of the process is encoded by the structure and parameters of the model that can be inferred statistically by fitting to viral load data. The main drawback of mechanistic models is that this inference is computationally expensive because the model must be repeatedly solved. This limits the size of the datasets that can be considered or the complexity of the models fitted. In this paper we develop a much cheaper inference method for this class of models by implementing a novel approximation of the model dynamics that uses a combination of random and deterministic processes. This approximation also properly accounts for process noise early in the infection when cell and virion numbers are small, which is important for the viral dynamics but often overlooked. Our method runs on a consumer laptop and is fast enough to facilitate a full hierarchical Bayesian treatment of the problem with sharing of information to allow for individual level parameter differences. We apply our method to simulated datasets and a reanalysis of COVID-19 monitoring data in an National Basketball Association cohort of 163 individuals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02884v3</guid>
      <category>stat.AP</category>
      <category>math.PR</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dylan J. Morris, Lauren Kennedy, Andrew J. Black</dc:creator>
    </item>
    <item>
      <title>Enhancing Psychometric Analysis with Interactive SIA Modules</title>
      <link>https://arxiv.org/abs/2407.18943</link>
      <description>arXiv:2407.18943v3 Announce Type: replace-cross 
Abstract: ShinyItemAnalysis (SIA) is an R package and shiny application for an interactive presentation of psychometric methods and analysis of multi-item measurements in psychology, education, and social sciences in general. In this article, we present a new feature introduced in the recent version of the package, called "SIA modules", which allows researchers and practitioners to offer new analytical methods for broader use via add-on extensions. SIA modules are designed to integrate with and build upon the SIA interactive application, enabling them to leverage the existing infrastructure for tasks such as data uploading and processing. They can access and further use a range of outputs from various analyses, including models and datasets. Because SIA modules come in R packages (or extend the existing ones), they may come bundled with their datasets, use object-oriented systems, or even compiled code. We illustrate the concepts using sample modules from the newly introduced SIAmodules package and other packages. After providing a general overview of building Shiny applications, we describe how to develop the SIA add-on modules with the support of the new SIAtools package. Finally, we discuss possibilities of future development and emphasize the importance of freely available, interactive psychometric software for dissemination of methodological innovations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18943v3</guid>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patr\'icia Martinkov\'a, Jan Net\'ik, Ad\'ela Hladk\'a</dc:creator>
    </item>
    <item>
      <title>The Curious Problem of the Normal Inverse Mean: Robustness and Shrinkage</title>
      <link>https://arxiv.org/abs/2410.20641</link>
      <description>arXiv:2410.20641v2 Announce Type: replace-cross 
Abstract: In astronomical observations, the estimation of distances from parallaxes is a challenging task due to the inherent measurement errors and the non-linear relationship between the parallax and the distance. This study leverages ideas from robust Bayesian inference to tackle these challenges, investigating a broad class of prior densities for estimating distances with a reduced bias and variance. Through theoretical analysis, simulation experiments, and the application to data from the Gaia Data Release 1 (GDR1), we demonstrate that heavy-tailed priors provide more reliable distance estimates, particularly in the presence of large fractional parallax errors. Theoretical results highlight the "curse of a single observation," where the likelihood dominates the posterior, limiting the impact of the prior. Nevertheless, heavy-tailed priors can delay the explosion of posterior risk, offering a more robust framework for distance estimation. The findings suggest that reciprocal invariant priors, with polynomial decay in their tails, such as the Half-Cauchy and Product Half-Cauchy, are particularly well-suited for this task, providing a balance between bias reduction and variance control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20641v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soham Ghosh, Uttaran Chatterjee, Jyotishka Datta</dc:creator>
    </item>
    <item>
      <title>A tutorial on conducting sample size and power calculations for detecting treatment effect heterogeneity in cluster randomized trials with linear mixed models</title>
      <link>https://arxiv.org/abs/2501.18383</link>
      <description>arXiv:2501.18383v2 Announce Type: replace-cross 
Abstract: Cluster-randomized trials (CRTs) are a well-established class of designs for evaluating community-based interventions. An essential task in planning these trials is determining the number of clusters and cluster sizes needed to achieve sufficient statistical power for detecting a clinically relevant effect size. While methods for evaluating the average treatment effect (ATE) for the entire study population are well-established, sample size methods for testing heterogeneity of treatment effects (HTEs), i.e., treatment-covariate interaction or difference in subpopulation-specific treatment effects, in CRTs have only recently been developed. For pre-specified analyses of HTEs in CRTs, effect-modifying covariates should, ideally, be accompanied by sample size or power calculations to ensure the trial has adequate power for the planned analyses. Power analysis for testing HTEs is more complex than for ATEs due to the additional design parameters that must be specified. Power and sample size formulas for testing HTEs via linear mixed effects (LME) models have been separately derived for different cluster-randomized designs, including single and multi-period parallel designs, crossover designs, and stepped-wedge designs, and for continuous and binary outcomes. This tutorial provides a consolidated reference guide for these methods and enhances their accessibility through an online R Shiny calculator. We further discuss key considerations for conducting sample size and power calculations to test pre-specified HTE hypotheses in CRTs, highlighting the importance of specifying advanced estimates of intracluster correlation coefficients for both outcomes and covariates, and their implications for power. The sample size methodology and calculator functionality are demonstrated through a real CRT example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18383v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mary Ryan Baumann, Monica Taljaard, Patrick J. Heagerty, Michael O. Harhay, Guangyu Tong, Rui Wang, Fan Li</dc:creator>
    </item>
    <item>
      <title>A Unified Framework for Variable Selection in Model-Based Clustering with Missing Not at Random</title>
      <link>https://arxiv.org/abs/2505.19093</link>
      <description>arXiv:2505.19093v3 Announce Type: replace-cross 
Abstract: Model-based clustering integrated with variable selection is a powerful tool for uncovering latent structures within complex data. However, its effectiveness is often hindered by challenges such as identifying relevant variables that define heterogeneous subgroups and handling data that are missing not at random, a prevalent issue in fields like transcriptomics. While several notable methods have been proposed to address these problems, they typically tackle each issue in isolation, thereby limiting their flexibility and adaptability. This paper introduces a unified framework designed to address these challenges simultaneously. Our approach incorporates a data-driven penalty matrix into penalized clustering to enable more flexible variable selection, along with a mechanism that explicitly models the relationship between missingness and latent class membership. We demonstrate that, under certain regularity conditions, the proposed framework achieves both asymptotic consistency and selection consistency, even in the presence of missing data. This unified strategy significantly enhances the capability and efficiency of model-based clustering, advancing methodologies for identifying informative variables that define homogeneous subgroups in the presence of complex missing data patterns. The performance of the framework, including its computational efficiency, is evaluated through simulations and demonstrated using both synthetic and real-world transcriptomic datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19093v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>39th Conference on Neural Information Processing Systems (NeurIPS 2025)</arxiv:journal_reference>
      <dc:creator>Binh H. Ho, Long Nguyen Chi, TrungTin Nguyen, Binh T. Nguyen, Van Ha Hoang, Christopher Drovandi</dc:creator>
    </item>
  </channel>
</rss>
