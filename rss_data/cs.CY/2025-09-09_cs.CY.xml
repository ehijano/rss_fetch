<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 09 Sep 2025 04:00:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>How candidates evoke identity and issues on TikTok</title>
      <link>https://arxiv.org/abs/2509.05310</link>
      <description>arXiv:2509.05310v1 Announce Type: new 
Abstract: Social media platforms are increasingly central to campaign communication, with both paid (advertising) and earned (organic) posts used for fundraising, mobilization, and persuasion. TikTok, and other short-form video platforms, with its short-video format and content-driven algorithms, demand unique content. We examine the final six months before the 2024 US Presidential Election to understand how major campaigns used TikTok. We frame our analysis around two political science theories. The first is the expressive (identity) model, where voters are motivated by their group memberships and candidates appeal to those identities. Alternatively, the instrumental (issues) model argues voters align with politicians advocating their key issues. We also examine how often candidates attacked opponents, reflecting literature showing attacks are common in politics. We combine two datasets: posts from the Harris and Trump campaigns on TikTok (July-November 2024) and a two-wave 2022 survey of around 1,000 respondents. Results show Trump more often disparaged Harris and emphasized identities and issues distinguishing Republicans, while Harris more often highlighted Democratic identities and valued issues. Although issues predict party ID, both candidates referenced identities more (34 percent of posts) than issues (25 percent), with most posts mentioning neither (55 percent).</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05310v1</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sabina Tomkins, Chang Ge, David Rothschild</dc:creator>
    </item>
    <item>
      <title>Validity Verification of the New TOEFL Writing Task Based on Classical Test Theory</title>
      <link>https://arxiv.org/abs/2509.05347</link>
      <description>arXiv:2509.05347v1 Announce Type: new 
Abstract: The TOEFL iBT has introduced the Academic Discussion Task (ADT) to assess test-takers' ability to engage in academic discourse, reflecting the growing emphasis on interactive communication skills in higher education. However, research on the ADT's validity and fairness particularly for culturally and linguistically diverse groups, such as Chinese students, remains limited. This study addresses this gap by employing Classical Test Theory (CTT) to evaluate the psychometric properties of the ADT among Chinese university students. This study finds a robust correlation between the ADT and the CET-6 writing and translation subscores. In addition, there is a high level of expert agreement regarding the construct validity evidence and the appropriateness of the scoring rubric. Furthermore, the results indicate that gender differences in validity indices are minimal. Taken together, these results suggest that the ADT is a valid measure for Chinese test-takers without gender discrimination. However, it is recommended that the cultural sensitivity of the scoring rubric be further refined and that the CET-6 subscores for writing be retained for predictive purposes, in order to better accommodate the needs of diverse test-taker populations. By addressing these issues, this study contributes to the broader discourse on fairness and validity in high-stakes language assessments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05347v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinyu Zhang</dc:creator>
    </item>
    <item>
      <title>Mobile Phone Sensor-based Nigerian Driving Dataset to Detect Alcohol-influenced Behaviours</title>
      <link>https://arxiv.org/abs/2509.05358</link>
      <description>arXiv:2509.05358v1 Announce Type: new 
Abstract: This paper presents a unique driving dataset collected in Nigeria via mobile phone sensors to support a machine learning model for detecting alcohol-influenced driving behaviours, with the long-term aim of integrating this model into a mobile application that encourages safer driving behaviours. Driving under the influence of alcohol is a major public safety concern, particularly in low-income countries like Nigeria, where traditional enforcement mechanisms may be limited. The proposed model leverages smartphone sensors such as accelerometers, gyroscopes, and GPS to provide a non-invasive, continuous solution for detecting impaired driving patterns in real time. This study adapts existing data processing and pattern matching methodologies to label real-world driving data collected from Nigerian drivers, which are then used to train the model. A decision tree classifier is developed to detect alcohol influence, based on behavioural and temporal features, achieving a recall of 100%, a precision of 60%, and an F1 score of 75%. The model's overall accuracy was 90.91%, ensuring that no alcohol influenced trips were missed. Key predictive features included speed variability, course deviation, and time of day, which align with established patterns of alcohol consumption. This study contributes to the field by demonstrating how machine learning can be applied in low-resource environments to improve road safety. The findings suggest that the model can significantly enhance the detection and prevention of risky driving behaviours, with the potential for future integration into mobile applications to provide real-time feedback and encourage safer driving practices. This scalable and accessible solution offers a new approach to addressing road safety challenges in regions where traditional interventions are inadequate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05358v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Iniakpokeikiye Peter Thompson, Yi Dewei, Reiter Ehud</dc:creator>
    </item>
    <item>
      <title>Governing AI R&amp;D: A Legal Framework for Constraining Dangerous AI</title>
      <link>https://arxiv.org/abs/2509.05361</link>
      <description>arXiv:2509.05361v1 Announce Type: new 
Abstract: As AI advances, governing its development may become paramount to public safety. Lawmakers may seek to restrict the development and release of AI models or of AI research itself. These governance actions could trigger legal challenges that invalidate the actions, so lawmakers should consider these challenges ahead of time. We investigate three classes of potential litigation risk for AI regulation in the U.S.: the First Amendment, administrative law, and the Fourteenth Amendment. We discuss existing precedent that is likely to apply to AI, which legal challenges are likely to arise, and how lawmakers might preemptively address them. Effective AI regulation is possible, but it requires careful implementation to avoid these legal challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05361v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alex Mark, Aaron Scher</dc:creator>
    </item>
    <item>
      <title>Prototyping an AI-powered Tool for Energy Efficiency in New Zealand Homes</title>
      <link>https://arxiv.org/abs/2509.05364</link>
      <description>arXiv:2509.05364v1 Announce Type: new 
Abstract: Residential buildings contribute significantly to energy use, health outcomes, and carbon emissions. In New Zealand, housing quality has historically been poor, with inadequate insulation and inefficient heating contributing to widespread energy hardship. Recent reforms, including the Warmer Kiwi Homes program, Healthy Homes Standards, and H1 Building Code upgrades, have delivered health and comfort improvements, yet challenges persist. Many retrofits remain partial, data on household performance are limited, and decision-making support for homeowners is fragmented. This study presents the design and evaluation of an AI-powered decision-support tool for residential energy efficiency in New Zealand. The prototype, developed using Python and Streamlit, integrates data ingestion, anomaly detection, baseline modeling, and scenario simulation (e.g., LED retrofits, insulation upgrades) into a modular dashboard. Fifteen domain experts, including building scientists, consultants, and policy practitioners, tested the tool through semi-structured interviews. Results show strong usability (M = 4.3), high value of scenario outputs (M = 4.5), and positive perceptions of its potential to complement subsidy programs and regulatory frameworks. The tool demonstrates how AI can translate national policies into personalized, household-level guidance, bridging the gap between funding, standards, and practical decision-making. Its significance lies in offering a replicable framework for reducing energy hardship, improving health outcomes, and supporting climate goals. Future development should focus on carbon metrics, tariff modeling, integration with national datasets, and longitudinal trials to assess real-world adoption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05364v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abdollah Baghaei Daemei</dc:creator>
    </item>
    <item>
      <title>Influence of Stakeholder Involvement in M&amp;E on the Performance of Donor-Funded Projects in Informal Settlements in Kisumu Central Sub-County, Kisumu County, Kenya</title>
      <link>https://arxiv.org/abs/2509.05369</link>
      <description>arXiv:2509.05369v1 Announce Type: new 
Abstract: Stakeholder engagement and participatory approaches influence the effectiveness of donor-funded projects. Participatory Monitoring and Evaluation (PM&amp;E) methodologies ensure that local communities play an active role in decision-making, leading to more sustainable outcomes. Given the complex socio-political landscape of Kisumu Central Sub-County, there is a critical need for inclusive and context-responsive project monitoring strategies. Initiatives that have integrated local leaders, youth groups, and women-led organizations into their M&amp;E processes tend to achieve stronger community buy-in, improved continuity, and more impactful outcomes. The current study explored the influence of stakeholder involvement in M&amp;E on the performance of donor-funded projects in informal settlements in Kisumu Central Sub-County, Kenya. The study was guided by Stakeholder Engagement Theory. The study used a Convergent Parallel design with a sample size of 364 respondents computed using Yamanes' Sampling formula, drawn from 27 donor-funded projects in Obunga and Nyalenda informal Settlements. Purposive sampling was used for project managers, project M&amp;E staff, and community members served, while community members were selected using stratified random sampling. The study findings revealed that there were regular opportunities for stakeholder interaction in the projects (x =4.05, SD 1.08), stakeholders contributed to the development of the organization/project (x= 3.79, SD=.940), although stakeholders' perspectives and opinions were not diligently incorporated into programming (x=2.06, SD=.879) as anticipated. The study, therefore, concluded that stakeholder involvement in M&amp;E influenced the performance of donor-funded projects. The study recommended that the project managers in donor-funded projects need to enhance stakeholder involvement for project ownership and sustainability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05369v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>0.56472/25835238/IRJEMS-V4I7P123</arxiv:DOI>
      <arxiv:journal_reference>Amuni.B.S, Karimi.S, Patrick.F.M. "Influence of Stakeholder Involvement in M&amp;E on the Performance of Donor-Funded Projects in Informal Settlements in Kisumu Central Sub-County, Kisumu County, Kenya" IRJEMS, Vol. 4, No. 7, pp. 185-192, 2025</arxiv:journal_reference>
      <dc:creator>Brighton Savayi Amuni, Stella Silas Karimi, Felistus Mwikali Patrick</dc:creator>
    </item>
    <item>
      <title>Cumplimiento del Reglamento (UE) 2024/1689 en rob\'otica y sistemas aut\'onomos: una revisi\'on sistem\'atica de la literatura</title>
      <link>https://arxiv.org/abs/2509.05380</link>
      <description>arXiv:2509.05380v1 Announce Type: new 
Abstract: This systematic literature review analyzes the current state of compliance with Regulation (EU) 2024/1689 in autonomous robotic systems, focusing on cybersecurity frameworks and methodologies. Using the PRISMA protocol, 22 studies were selected from 243 initial records across IEEE Xplore, ACM DL, Scopus, and Web of Science. Findings reveal partial regulatory alignment: while progress has been made in risk management and encrypted communications, significant gaps persist in explainability modules, real-time human oversight, and knowledge base traceability. Only 40% of reviewed solutions explicitly address transparency requirements, and 30% implement failure intervention mechanisms. The study concludes that modular approaches integrating risk, supervision, and continuous auditing are essential to meet the AI Act mandates in autonomous robotics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05380v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.RO</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yoana Pita Lorenzo</dc:creator>
    </item>
    <item>
      <title>User Privacy and Large Language Models: An Analysis of Frontier Developers' Privacy Policies</title>
      <link>https://arxiv.org/abs/2509.05382</link>
      <description>arXiv:2509.05382v1 Announce Type: new 
Abstract: Hundreds of millions of people now regularly interact with large language models via chatbots. Model developers are eager to acquire new sources of high-quality training data as they race to improve model capabilities and win market share. This paper analyzes the privacy policies of six U.S. frontier AI developers to understand how they use their users' chats to train models. Drawing primarily on the California Consumer Privacy Act, we develop a novel qualitative coding schema that we apply to each developer's relevant privacy policies to compare data collection and use practices across the six companies. We find that all six developers appear to employ their users' chat data to train and improve their models by default, and that some retain this data indefinitely. Developers may collect and train on personal information disclosed in chats, including sensitive information such as biometric and health data, as well as files uploaded by users. Four of the six companies we examined appear to include children's chat data for model training, as well as customer data from other products. On the whole, developers' privacy policies often lack essential information about their practices, highlighting the need for greater transparency and accountability. We address the implications of users' lack of consent for the use of their chat data for model training, data security issues arising from indefinite chat data retention, and training on children's chat data. We conclude by providing recommendations to policymakers and developers to address the data privacy challenges posed by LLM-powered chatbots.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05382v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jennifer King, Kevin Klyman, Emily Capstick, Tiffany Saade, Victoria Hsieh</dc:creator>
    </item>
    <item>
      <title>Authorship Without Writing: Large Language Models and the Senior Author Analogy</title>
      <link>https://arxiv.org/abs/2509.05390</link>
      <description>arXiv:2509.05390v1 Announce Type: new 
Abstract: The use of large language models (LLMs) in bioethical, scientific, and medical writing remains controversial. While there is broad agreement in some circles that LLMs cannot count as authors, there is no consensus about whether and how humans using LLMs can count as authors. In many fields, authorship is distributed among large teams of researchers, some of whom, including paradigmatic senior authors who guide and determine the scope of a project and ultimately vouch for its integrity, may not write a single word. In this paper, we argue that LLM use (under specific conditions) is analogous to a form of senior authorship. On this view, the use of LLMs, even to generate complete drafts of research papers, can be considered a legitimate form of authorship according to the accepted criteria in many fields. We conclude that either such use should be recognized as legitimate, or current criteria for authorship require fundamental revision. AI use declaration: GPT-5 was used to help format Box 1. AI was not used for any other part of the preparation or writing of this manuscript.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05390v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Clint Hurshman, Sebastian Porsdam Mann, Julian Savulescu, Brian D. Earp</dc:creator>
    </item>
    <item>
      <title>An Optimized Pipeline for Automatic Educational Knowledge Graph Construction</title>
      <link>https://arxiv.org/abs/2509.05392</link>
      <description>arXiv:2509.05392v1 Announce Type: new 
Abstract: The automatic construction of Educational Knowledge Graphs (EduKGs) is essential for domain knowledge modeling by extracting meaningful representations from learning materials. Despite growing interest, identifying a scalable and reliable approach for automatic EduKG generation remains a challenge. In an attempt to develop a unified and robust pipeline for automatic EduKG construction, in this study we propose a pipeline for automatic EduKG construction from PDF learning materials. The process begins with generating slide-level EduKGs from individual pages/slides, which are then merged to form a comprehensive EduKG representing the entire learning material. We evaluate the accuracy of the EduKG generated from the proposed pipeline in our MOOC platform, CourseMapper. The observed accuracy, while indicative of partial success, is relatively low particularly in the educational context, where the reliability of knowledge representations is critical for supporting meaningful learning. To address this, we introduce targeted optimizations across multiple pipeline components. The optimized pipeline achieves a 17.5% improvement in accuracy and a tenfold increase in processing efficiency. Our approach offers a holistic, scalable and end-to-end pipeline for automatic EduKG construction, adaptable to diverse educational contexts, and supports improved semantic representation of learning content.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05392v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Qurat Ul Ain, Mohamed Amine Chatti, Jean Qussa, Amr Shakhshir, Rawaa Alatrash, Shoeb Joarder</dc:creator>
    </item>
    <item>
      <title>Inferring Prerequisite Knowledge Concepts in Educational Knowledge Graphs: A Multi-criteria Approach</title>
      <link>https://arxiv.org/abs/2509.05393</link>
      <description>arXiv:2509.05393v1 Announce Type: new 
Abstract: Educational Knowledge Graphs (EduKGs) organize various learning entities and their relationships to support structured and adaptive learning. Prerequisite relationships (PRs) are critical in EduKGs for defining the logical order in which concepts should be learned. However, the current EduKG in the MOOC platform CourseMapper lacks explicit PR links, and manually annotating them is time-consuming and inconsistent. To address this, we propose an unsupervised method for automatically inferring concept PRs without relying on labeled data. We define ten criteria based on document-based, Wikipedia hyperlink-based, graph-based, and text-based features, and combine them using a voting algorithm to robustly capture PRs in educational content. Experiments on benchmark datasets show that our approach achieves higher precision than existing methods while maintaining scalability and adaptability, thus providing reliable support for sequence-aware learning in CourseMapper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05393v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rawaa Alatrash, Mohamed Amine Chatti, Nasha Wibowo, Qurat Ul Ain</dc:creator>
    </item>
    <item>
      <title>Unmasking COVID-19 Vulnerability in Nigeria: Mapping Risks Beyond Urban Hotspots</title>
      <link>https://arxiv.org/abs/2509.05398</link>
      <description>arXiv:2509.05398v1 Announce Type: new 
Abstract: The COVID-19 pandemic has presented significant challenges in Nigeria's public health systems since the first case reported on February 27, 2020. This study investigates key factors that contribute to state vulnerability, quantifying them through a composite risk score integrating population density (weight 0.2), poverty (0.4), access to healthcare (0.3), and age risk (0.1), adjusted by normalized case rates per 100,000. States were categorized into low-, medium-, and high-density areas to analyze trends and identify hotspots using geographic information system (GIS) mapping. The findings reveal that high-density urban areas, such as Lagos, accounting for 35.4% of national cases, had the highest risk scores (Lagos: 673.47 vs. national average: 28.16). These results align with global and local studies on the spatial variability of COVID-19 in Nigeria, including international frameworks such as the CDC Social Vulnerability Index. Google Trends data highlight variations in public health awareness, serving as a supplementary analysis to contextualize vulnerability. The risk score provides a prioritization tool for policymakers to allocate testing, vaccines, and healthcare resources to high-risk areas, though data gaps and rural underreporting call for further research. This framework can extend to other infectious diseases, offering lessons for future pandemics in resource-limited settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05398v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sheila Wafula, Blessed Madukoma</dc:creator>
    </item>
    <item>
      <title>From Vision to Validation: A Theory- and Data-Driven Construction of a GCC-Specific AI Adoption Index</title>
      <link>https://arxiv.org/abs/2509.05474</link>
      <description>arXiv:2509.05474v1 Announce Type: new 
Abstract: Artificial intelligence (AI) is rapidly transforming public-sector processes worldwide, yet standardized measures rarely address the unique drivers, governance models, and cultural nuances of the Gulf Cooperation Council (GCC) countries. This study employs a theory-driven foundation derived from an in-depth analysis of literature review and six National AI Strategies (NASs), coupled with a data-driven approach that utilizes a survey of 203 mid- and senior-level government employees and advanced statistical techniques (K-Means clustering, Principal Component Analysis, and Partial Least Squares Structural Equation Modeling). By combining policy insights with empirical evidence, the research develops and validates a novel AI Adoption Index specifically tailored to the GCC public sector. Findings indicate that robust infrastructure and clear policy mandates exert the strongest influence on successful AI implementations, overshadowing organizational readiness in early adoption stages. The combined model explains 70% of the variance in AI outcomes, suggesting that resource-rich environments and top-down policy directives can drive rapid but uneven technology uptake. By consolidating key dimensions (Infrastructure &amp; Resources, Organizational Readiness, and Policy &amp; Regulatory Environment) into a single composite index, this study provides a holistic yet context-sensitive tool for benchmarking AI maturity. The index offers actionable guidance for policymakers seeking to harmonize large-scale deployments with ethical and regulatory standards. Beyond advancing academic discourse, these insights inform more strategic allocation of resources, cross-country cooperation, and capacity-building initiatives, thereby supporting sustained AI-driven transformation in the GCC region and beyond.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05474v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad Rashed Albous, Anwaar AlKandari, Abdel Latef Anouze</dc:creator>
    </item>
    <item>
      <title>Audits Under Resource, Data, and Access Constraints: Scaling Laws For Less Discriminatory Alternatives</title>
      <link>https://arxiv.org/abs/2509.05627</link>
      <description>arXiv:2509.05627v1 Announce Type: new 
Abstract: AI audits play a critical role in AI accountability and safety. One branch of the law for which AI audits are particularly salient is anti-discrimination law. Several areas of anti-discrimination law implicate the "less discriminatory alternative" (LDA) requirement, in which a protocol (e.g., model) is defensible if no less discriminatory protocol that achieves comparable performance can be found with a reasonable amount of effort. Notably, the burden of proving an LDA exists typically falls on the claimant (the party alleging discrimination). This creates a significant hurdle in AI cases, as the claimant would seemingly need to train a less discriminatory yet high-performing model, a task requiring resources and expertise beyond most litigants. Moreover, developers often shield information about and access to their model and training data as trade secrets, making it difficult to reproduce a similar model from scratch.
  In this work, we present a procedure enabling claimants to determine if an LDA exists, even when they have limited compute, data, information, and model access. We focus on the setting in which fairness is given by demographic parity and performance by binary cross-entropy loss. As our main result, we provide a novel closed-form upper bound for the loss-fairness Pareto frontier (PF). We show how the claimant can use it to fit a PF in the "low-resource regime," then extrapolate the PF that applies to the (large) model being contested, all without training a single large model. The expression thus serves as a scaling law for loss-fairness PFs. To use this scaling law, the claimant would require a small subsample of the train/test data. Then, the claimant can fit the context-specific PF by training as few as 7 (small) models. We stress test our main result in simulations, finding that our scaling law holds even when the exact conditions of our theory do not.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05627v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sarah H. Cen, Salil Goyal, Zaynah Javed, Ananya Karthik, Percy Liang, Daniel E. Ho</dc:creator>
    </item>
    <item>
      <title>Bridging the Gap Between Theoretical and Practical Reinforcement Learning in Undergraduate Education</title>
      <link>https://arxiv.org/abs/2509.05689</link>
      <description>arXiv:2509.05689v1 Announce Type: new 
Abstract: This innovative practice category paper presents an innovative framework for teaching Reinforcement Learning (RL) at the undergraduate level. Recognizing the challenges posed by the complex theoretical foundations of the subject and the need for hands-on algorithmic practice, the proposed approach integrates traditional lectures with interactive lab-based learning. Drawing inspiration from effective pedagogical practices in computer science and engineering, the framework engages students through real-time coding exercises using simulated environments such as OpenAI Gymnasium. The effectiveness of this approach is evaluated through student surveys, instructor feedback, and course performance metrics, demonstrating improvements in understanding, debugging, parameter tuning, and model evaluation. Ultimately, the study provides valuable insight into making Reinforcement Learning more accessible and engaging, thereby equipping students with essential problem-solving skills for real-world applications in Artificial Intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05689v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Muhammad Ahmed Atif, Mohammad Shahid Shaikh</dc:creator>
    </item>
    <item>
      <title>Towards an Automated Framework to Audit Youth Safety on TikTok</title>
      <link>https://arxiv.org/abs/2509.05838</link>
      <description>arXiv:2509.05838v1 Announce Type: new 
Abstract: This paper investigates the effectiveness of TikTok's enforcement mechanisms for limiting the exposure of harmful content to youth accounts. We collect over 7000 videos, classify them as harmful vs not-harmful, and then simulate interactions using age-specific sockpuppet accounts through both passive and active engagement strategies. We also evaluate the performance of large language (LLMs) and vision-language models (VLMs) in detecting harmful content, identifying key challenges in precision and scalability.
  Preliminary results show minimal differences in content exposure between adult and youth accounts, raising concerns about the platform's age-based moderation. These findings suggest that the platform needs to strengthen youth safety measures and improve transparency in content moderation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05838v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Linda Xue, Francesco Corso, Nicolo' Fontana, Geng Liu, Stefano Ceri, Francesco Pierri</dc:creator>
    </item>
    <item>
      <title>Stack Overflow Is Not Dead Yet: Crowd Answers Still Matter</title>
      <link>https://arxiv.org/abs/2509.05879</link>
      <description>arXiv:2509.05879v1 Announce Type: new 
Abstract: Millions of users visit Stack Overflow regularly to ask community for answers to their programming questions. However, like many other platforms, Stack Overflow consistently struggles with low user retention and declining levels of user contributions to the platform. With the introduction of ChatGPT in November 2022, these ongoing difficulties on Stack Overflow were further magnified, as many users moved toward ChatGPT for programming help. In this paper, we build upon recent research on this phenomenon by analyzing the transformation of user-generated content on Stack Overflow during the post-ChatGPT period. Specifically, we analyze two years of Stack Overflow data and fit multiple causal regression models to estimate the effect of ChatGPT on the length and difficulty of user questions and code examples. We confirm an acceleration of decline in user contributions but find that ChatGPT had a significant positive effect on question and answer length, code length, and question difficulty on Stack Overflow across programming languages. Our results suggest that ChatGPT has effectively raised the bar for questions on Stack Overflow, as users increasingly turn to crowdsourced platforms for help with more complex and challenging problems. With our work we contribute to the ongoing discussion on the impact of tools such as ChatGPT on help-seeking in programming and, more broadly, on collaborative knowledge creation. Our results provide actionable insights for platform operators to support information management and user retention in the aftermath of ChatGPT's launch.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05879v1</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Denis Helic, Tiago Santos</dc:creator>
    </item>
    <item>
      <title>Operationalising AI Regulatory Sandboxes under the EU AI Act: The Triple Challenge of Capacity, Coordination and Attractiveness to Providers</title>
      <link>https://arxiv.org/abs/2509.05985</link>
      <description>arXiv:2509.05985v1 Announce Type: new 
Abstract: The EU AI Act provides a rulebook for all AI systems being put on the market or into service in the European Union. This article investigates the requirement under the AI Act that Member States establish national AI regulatory sandboxes for testing and validation of innovative AI systems under regulatory supervision to assist with fostering innovation and complying with regulatory requirements. Against the backdrop of the EU objective that AI regulatory sandboxes would both foster innovation and assist with compliance, considerable challenges are identified for Member States around capacity-building and design of regulatory sandboxes. While Member States are early movers in laying the ground for national AI regulatory sandboxes, the article contends that there is a risk that differing approaches being taken by individual national sandboxes could jeopardise a uniform interpretation of the AI Act and its application in practice. This could motivate innovators to play sandbox arbitrage. The article therefore argues that the European Commission and the AI Board need to act decisively in developing rules and guidance to ensure a cohesive, coordinated approach in national AI regulatory sandboxes. With sandbox participation being voluntary, the possibility that AI regulatory sandboxes may prove unattractive to innovators on their compliance journey is also explored. Confidentiality concerns, the inability to relax legal rules during the sandbox, and the inability of sandboxes to deliver a presumption of conformity with the AI Act are identified as pertinent concerns for innovators contemplating applying to AI regulatory sandboxes as compared with other direct compliance routes provided to them through application of harmonised standards and conformity assessment procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05985v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Deirdre Ahern</dc:creator>
    </item>
    <item>
      <title>The impact of gamification on learning outcomes: experiences from a Biomedical Engineering course</title>
      <link>https://arxiv.org/abs/2509.06126</link>
      <description>arXiv:2509.06126v1 Announce Type: new 
Abstract: This study examines the integration of digital tools in project-based learning within a Biomedical Engineering course to enhance collaboration, transparency, and assessment fairness. Building on prior pilot experiences, we implemented a structured learning environment that combined experiment tracking, real-time collaboration, and peer-assessment practices. The intervention was deployed across two consecutive academic years, involving master's-level students in Biomedical Image Processing. Data were collected through project outcomes, peer-assessment rubrics, and student surveys. Results show that the integration of digital platforms supported accountability, improved the quality of collaborative work, and fostered greater equity in the evaluation process. Students highlighted increased engagement, enhanced teamwork, and clearer criteria for performance assessment. Faculty reported more efficient monitoring of progress and improved feedback practices. Despite challenges such as technical adoption and the need for instructor guidance, the study demonstrates the potential of structured tool integration to support active and transparent learning environments. Findings contribute to the broader discourse on digital pedagogy, offering a replicable model for higher education contexts in science and technology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06126v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gonzalo R. R\'ios-Mu\~noz, Caterina Fuster-Barcelo, Arrate Mu\~noz-Barrutia</dc:creator>
    </item>
    <item>
      <title>Social Dynamics of DAOs: Power, Onboarding, and Inclusivity</title>
      <link>https://arxiv.org/abs/2509.06163</link>
      <description>arXiv:2509.06163v1 Announce Type: new 
Abstract: This report explores the often-overlooked cultural and social dynamics shaping participation and power in DAOs. Drawing on qualitative interviews and ethnographic observations, it shows how factors such as financial privilege, informal gatekeeping, visibility bias, and onboarding structures create barriers to meaningful inclusion. While DAOs are frequently framed as permissionless and egalitarian, the lived experiences of contributors reveal a more complex reality, one in which soft power and implicit norms determine people's position within DAOs. Instead of offering solutionist prescriptions, this report argues for a deeper cultural reflection within the DAO ecosystem. It highlights that decentralisation is not solely a protocol-level feature, but an ongoing social process that requires intentional cultivation of trust, belonging, and epistemic plurality. With this report, we want to sharpen the collective awareness of structural blind spots and call for building more inclusive and culturally conscious decentralised systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06163v1</guid>
      <category>cs.CY</category>
      <category>cs.DC</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Victoria Kozlova, Ben Biedermann</dc:creator>
    </item>
    <item>
      <title>AI Governance in Higher Education: A course design exploring regulatory, ethical and practical considerations</title>
      <link>https://arxiv.org/abs/2509.06176</link>
      <description>arXiv:2509.06176v1 Announce Type: new 
Abstract: As artificial intelligence (AI) systems permeate critical sectors, the need for professionals who can address ethical, legal and governance challenges has become urgent. Current AI ethics education remains fragmented, often siloed by discipline and disconnected from practice. This paper synthesizes literature and regulatory developments to propose a modular, interdisciplinary curriculum that integrates technical foundations with ethics, law and policy. We highlight recurring operational failures in AI - bias, misspecified objectives, generalization errors, misuse and governance breakdowns - and link them to pedagogical strategies for teaching AI governance. Drawing on perspectives from the EU, China and international frameworks, we outline a semester plan that emphasizes integrated ethics, stakeholder engagement and experiential learning. The curriculum aims to prepare students to diagnose risks, navigate regulation and engage diverse stakeholders, fostering adaptive and ethically grounded professionals for responsible AI governance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06176v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zsolt Alm\'asi (P\'azm\'any P\'eter Catholic University, Hungary), Hannah Bleher (University of Bonn, Germany), Johannes Bleher (University of Hohenheim, Germany), Rozanne Tuesday Flores (Bukidnon State University, Philippines), Guo Xuanyang (Southwest University of Political Science and Law, China), Pawe{\l} Pujszo (College of Europe, Natolin, Poland), Rapha\"el Weuts (KU Leuven, Belgium)</dc:creator>
    </item>
    <item>
      <title>Simulating Dispute Mediation with LLM-Based Agents for Legal Research</title>
      <link>https://arxiv.org/abs/2509.06586</link>
      <description>arXiv:2509.06586v1 Announce Type: new 
Abstract: Legal dispute mediation plays a crucial role in resolving civil disputes, yet its empirical study is limited by privacy constraints and complex multivariate interactions. To address this limitation, we present AgentMediation, the first LLM-based agent framework for simulating dispute mediation. It simulates realistic mediation processes grounded in real-world disputes and enables controlled experimentation on key variables such as disputant strategies, dispute causes, and mediator expertise. Our empirical analysis reveals patterns consistent with sociological theories, including Group Polarization and Surface-level Consensus. As a comprehensive and extensible platform, AgentMediation paves the way for deeper integration of social science and AI in legal research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06586v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junjie Chen, Haitao Li, Minghao Qin, Yujia Zhou, Yanxue Ren, Wuyue Wang, Yiqun Liu, Yueyue Wu, Qingyao Ai</dc:creator>
    </item>
    <item>
      <title>NeedForHeat DataGear: An Open Monitoring System to Accelerate the Residential Heating Transition</title>
      <link>https://arxiv.org/abs/2509.06927</link>
      <description>arXiv:2509.06927v1 Announce Type: new 
Abstract: We introduce NeedForHeat DataGear: an open hardware and open software data collection system designed to accelerate the residential heating transition. NeedForHeat DataGear collects time series monitoring data in homes that have not yet undergone a heating transition, enabling assessment of real-life thermal characteristics, heating system efficiency, and residents' comfort needs. This paper outlines its architecture and functionalities, emphasizing its modularity, adaptability, and cost-effectiveness for field data acquisition. Unlike conventional domestic monitoring solutions focused on home automation, direct feedback, or post-installation heat pump monitoring, it prioritizes time series data we deemed essential to evaluate the current situation in existing homes before the heating transition. Designed for seamless deployment across diverse households, NeedForHeat DataGear combines openness, security, and privacy with a low-cost, user-friendly approach, making it a valuable tool for researchers, energy professionals, and energy coaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06927v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Henri ter Hofte, Nick van Ravenzwaaij</dc:creator>
    </item>
    <item>
      <title>From Image Generation to Infrastructure Design: a Multi-agent Pipeline for Street Design Generation</title>
      <link>https://arxiv.org/abs/2509.05469</link>
      <description>arXiv:2509.05469v1 Announce Type: cross 
Abstract: Realistic visual renderings of street-design scenarios are essential for public engagement in active transportation planning. Traditional approaches are labor-intensive, hindering collective deliberation and collaborative decision-making. While AI-assisted generative design shows transformative potential by enabling rapid creation of design scenarios, existing generative approaches typically require large amounts of domain-specific training data and struggle to enable precise spatial variations of design/configuration in complex street-view scenes. We introduce a multi-agent system that edits and redesigns bicycle facilities directly on real-world street-view imagery. The framework integrates lane localization, prompt optimization, design generation, and automated evaluation to synthesize realistic, contextually appropriate designs. Experiments across diverse urban scenarios demonstrate that the system can adapt to varying road geometries and environmental conditions, consistently yielding visually coherent and instruction-compliant results. This work establishes a foundation for applying multi-agent pipelines to transportation infrastructure planning and facility design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05469v1</guid>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Chenguang Wang, Xiang Yan, Yilong Dai, Ziyi Wang, Susu Xu</dc:creator>
    </item>
    <item>
      <title>Ad hoc conventions generalize to new referents</title>
      <link>https://arxiv.org/abs/2509.05566</link>
      <description>arXiv:2509.05566v1 Announce Type: cross 
Abstract: How do people talk about things they've never talked about before? One view suggests that a new shared naming system establishes an arbitrary link to a specific target, like proper names that cannot extend beyond their bearers. An alternative view proposes that forming a shared way of describing objects involves broader conceptual alignment, reshaping each individual's semantic space in ways that should generalize to new referents. We test these competing accounts in a dyadic communication study (N=302) leveraging the recently-released KiloGram dataset containing over 1,000 abstract tangram images. After pairs of participants coordinated on referential conventions for one set of images through repeated communication, we measured the extent to which their descriptions aligned for undiscussed images. We found strong evidence for generalization: partners showed increased alignment relative to their pre-test labels. Generalization also decayed nonlinearly with visual similarity (consistent with Shepard's law) and was robust across levels of the images' nameability. These findings suggest that ad hoc conventions are not arbitrary labels but reflect genuine conceptual coordination, with implications for theories of reference and the design of more adaptive language agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05566v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anya Ji, Claire Augusta Bergey, Ron Eliav, Yoav Artzi, Robert D. Hawkins</dc:creator>
    </item>
    <item>
      <title>Finetuning LLMs for Human Behavior Prediction in Social Science Experiments</title>
      <link>https://arxiv.org/abs/2509.05830</link>
      <description>arXiv:2509.05830v1 Announce Type: cross 
Abstract: Large language models (LLMs) offer a powerful opportunity to simulate the results of social science experiments. In this work, we demonstrate that finetuning LLMs directly on individual-level responses from past experiments meaningfully improves the accuracy of such simulations across diverse social science domains. We construct SocSci210 via an automatic pipeline, a dataset comprising 2.9 million responses from 400,491 participants in 210 open-source social science experiments. Through finetuning, we achieve multiple levels of generalization. In completely unseen studies, our strongest model, Socrates-Qwen-14B, produces predictions that are 26% more aligned with distributions of human responses to diverse outcome questions under varying conditions relative to its base model (Qwen2.5-14B), outperforming GPT-4o by 13%. By finetuning on a subset of conditions in a study, generalization to new unseen conditions is particularly robust, improving by 71%. Since SocSci210 contains rich demographic information, we reduce demographic parity, a measure of bias, by 10.6% through finetuning. Because social sciences routinely generate rich, topic-specific datasets, our findings indicate that finetuning on such data could enable more accurate simulations for experimental hypothesis screening. We release our data, models and finetuning code at stanfordhci.github.io/socrates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05830v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akaash Kolluri, Shengguang Wu, Joon Sung Park, Michael S. Bernstein</dc:creator>
    </item>
    <item>
      <title>Explained, yet misunderstood: How AI Literacy shapes HR Managers' interpretation of User Interfaces in Recruiting Recommender Systems</title>
      <link>https://arxiv.org/abs/2509.06475</link>
      <description>arXiv:2509.06475v1 Announce Type: cross 
Abstract: AI-based recommender systems increasingly influence recruitment decisions. Thus, transparency and responsible adoption in Human Resource Management (HRM) are critical. This study examines how HR managers' AI literacy influences their subjective perception and objective understanding of explainable AI (XAI) elements in recruiting recommender dashboards. In an online experiment, 410 German-based HR managers compared baseline dashboards to versions enriched with three XAI styles: important features, counterfactuals, and model criteria. Our results show that the dashboards used in practice do not explain AI results and even keep AI elements opaque. However, while adding XAI features improves subjective perceptions of helpfulness and trust among users with moderate or high AI literacy, it does not increase their objective understanding. It may even reduce accurate understanding, especially with complex explanations. Only overlays of important features significantly aided the interpretations of high-literacy users. Our findings highlight that the benefits of XAI in recruitment depend on users' AI literacy, emphasizing the need for tailored explanation strategies and targeted literacy training in HRM to ensure fair, transparent, and effective adoption of AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06475v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yannick Kalff, Katharina Simbeck</dc:creator>
    </item>
    <item>
      <title>AI for Scientific Discovery is a Social Problem</title>
      <link>https://arxiv.org/abs/2509.06580</link>
      <description>arXiv:2509.06580v1 Announce Type: cross 
Abstract: Artificial intelligence promises to accelerate scientific discovery, yet its benefits remain unevenly distributed. While technical obstacles such as scarce data, fragmented standards, and unequal access to computation are significant, we argue that the primary barriers are social and institutional. Narratives that defer progress to speculative "AI scientists," the undervaluing of data and infrastructure contributions, misaligned incentives, and gaps between domain experts and machine learning researchers all constrain impact. We highlight four interconnected challenges: community dysfunction, research priorities misaligned with upstream needs, data fragmentation, and infrastructure inequities. We argue that their roots lie in cultural and organizational practices. Addressing them requires not only technical innovation but also intentional community-building, cross-disciplinary education, shared benchmarks, and accessible infrastructure. We call for reframing AI for science as a collective social project, where sustainable collaboration and equitable participation are treated as prerequisites for technical progress.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06580v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Georgia Channing, Avijit Ghosh</dc:creator>
    </item>
    <item>
      <title>An Ethically Grounded LLM-Based Approach to Insider Threat Synthesis and Detection</title>
      <link>https://arxiv.org/abs/2509.06920</link>
      <description>arXiv:2509.06920v1 Announce Type: cross 
Abstract: Insider threats are a growing organizational problem due to the complexity of identifying their technical and behavioral elements. A large research body is dedicated to the study of insider threats from technological, psychological, and educational perspectives. However, research in this domain has been generally dependent on datasets that are static and limited access which restricts the development of adaptive detection models. This study introduces a novel, ethically grounded approach that uses the large language model (LLM) Claude Sonnet 3.7 to dynamically synthesize syslog messages, some of which contain indicators of insider threat scenarios. The messages reflect real-world data distributions by being highly imbalanced (1% insider threats). The syslogs were analyzed for insider threats by both Claude Sonnet 3.7 and GPT-4o, with their performance evaluated through statistical metrics including precision, recall, MCC, and ROC AUC. Sonnet 3.7 consistently outperformed GPT-4o across nearly all metrics, particularly in reducing false alarms and improving detection accuracy. The results show strong promise for the use of LLMs in synthetic dataset generation and insider threat detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06920v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haywood Gelman, John D. Hastings, David Kenley</dc:creator>
    </item>
    <item>
      <title>Bias in Decision-Making for AI's Ethical Dilemmas: A Comparative Study of ChatGPT and Claude</title>
      <link>https://arxiv.org/abs/2501.10484</link>
      <description>arXiv:2501.10484v3 Announce Type: replace 
Abstract: Recent advances in Large Language Models (LLMs) have enabled human-like responses across various tasks, raising questions about their ethical decision-making capabilities and potential biases. This study systematically evaluates how nine popular LLMs (both open-source and closed-source) respond to ethical dilemmas involving protected attributes. Across 50,400 trials spanning single and intersectional attribute combinations in four dilemma scenarios (protective vs. harmful), we assess models' ethical preferences, sensitivity, stability, and clustering patterns. Results reveal significant biases in protected attributes in all models, with differing preferences depending on model type and dilemma context. Notably, open-source LLMs show stronger preferences for marginalized groups and greater sensitivity in harmful scenarios, while closed-source models are more selective in protective situations and tend to favor mainstream groups. We also find that ethical behavior varies across dilemma types: LLMs maintain consistent patterns in protective scenarios but respond with more diverse and cognitively demanding decisions in harmful ones. Furthermore, models display more pronounced ethical tendencies under intersectional conditions than in single-attribute settings, suggesting that complex inputs reveal deeper biases. These findings highlight the need for multi-dimensional, context-aware evaluation of LLMs' ethical behavior and offer a systematic evaluation and approach to understanding and addressing fairness in LLM decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10484v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yile Yan, Yuqi Zhu, Wentao Xu</dc:creator>
    </item>
    <item>
      <title>GreenDFL: a Framework for Assessing the Sustainability of Decentralized Federated Learning Systems</title>
      <link>https://arxiv.org/abs/2502.20242</link>
      <description>arXiv:2502.20242v3 Announce Type: replace 
Abstract: Decentralized Federated Learning (DFL) is an emerging paradigm that enables collaborative model training without centralized data and model aggregation, enhancing privacy and resilience. However, its sustainability remains underexplored, as energy consumption and carbon emissions vary across different system configurations. Understanding the environmental impact of DFL is crucial for optimizing its design and deployment. This work aims to develop a comprehensive and operational framework for assessing the sustainability of DFL systems. To address it, this work provides a systematic method for quantifying energy consumption and carbon emissions, offering insights into improving the sustainability of DFL. This work proposes GreenDFL, a fully implementable framework that has been integrated into a real-world DFL platform. GreenDFL systematically analyzes the impact of various factors, including hardware accelerators, model architecture, communication medium, data distribution, network topology, and federation size, on the sustainability of DFL systems. Besides, a sustainability-aware aggregation algorithm (GreenDFL-SA) and a node selection algorithm (GreenDFL-SN) are developed to optimize energy efficiency and reduce carbon emissions in DFL training. Empirical experiments are conducted on multiple datasets, measuring energy consumption and carbon emissions at different phases of the DFL lifecycle. The proposed GreenDFL provides a comprehensive and practical approach for assessing the sustainability of DFL systems. Furthermore, it offers best practices for improving environmental efficiency in DFL, making sustainability considerations more actionable in real-world deployments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20242v3</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chao Feng, Alberto Huertas Celdr\'an, Xi Cheng, G\'er\^ome Bovet, Burkhard Stiller</dc:creator>
    </item>
    <item>
      <title>Transit for All: Mapping Equitable Bike2Subway Connection using Region Representation Learning</title>
      <link>https://arxiv.org/abs/2506.15113</link>
      <description>arXiv:2506.15113v2 Announce Type: replace 
Abstract: Ensuring equitable public transit access remains challenging, particularly in densely populated cities like New York City (NYC), where low-income and minority communities often face limited transit accessibility. Bike-sharing systems (BSS) can bridge these equity gaps by providing affordable first- and last-mile connections. However, strategically expanding BSS into underserved neighborhoods is difficult due to uncertain bike-sharing demand at newly planned ("cold-start") station locations and limitations in traditional accessibility metrics that may overlook realistic bike usage potential. We introduce Transit for All (TFA), a spatial computing framework designed to guide the equitable expansion of BSS through three components: (1) spatially-informed bike-sharing demand prediction at cold-start stations using region representation learning that integrates multimodal geospatial data, (2) comprehensive transit accessibility assessment leveraging our novel weighted Public Transport Accessibility Level (wPTAL) by combining predicted bike-sharing demand with conventional transit accessibility metrics, and (3) strategic recommendations for new bike station placements that consider potential ridership and equity enhancement. Using NYC as a case study, we identify transit accessibility gaps that disproportionately impact low-income and minority communities in historically underserved neighborhoods. Our results show that strategically placing new stations guided by wPTAL notably reduces disparities in transit access related to economic and demographic factors. From our study, we demonstrate that TFA provides practical guidance for urban planners to promote equitable transit and enhance the quality of life in underserved urban communities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15113v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Min Namgung, JangHyeon Lee, Fangyi Ding, Yao-Yi Chiang</dc:creator>
    </item>
    <item>
      <title>Navigating the EU AI Act: Foreseeable Challenges in Qualifying Deep Learning-Based Automated Inspections of Class III Medical Devices</title>
      <link>https://arxiv.org/abs/2508.20144</link>
      <description>arXiv:2508.20144v2 Announce Type: replace 
Abstract: As deep learning (DL) technologies advance, their application in automated visual inspection for Class III medical devices offers significant potential to enhance quality assurance and reduce human error. However, the adoption of such AI-based systems introduces new regulatory complexities-particularly under the EU Artificial Intelligence (AI) Act, which imposes high-risk system obligations that differ in scope and depth from established regulatory frameworks such as the Medical Device Regulation (MDR) and the U.S. FDA Quality System Regulation (QSR). This paper presents a high-level technical assessment of the foreseeable challenges that manufacturers are likely to encounter when qualifying DL-based automated inspections -- specifically static models -- within the existing medical device compliance landscape. It examines divergences in risk management principles, dataset governance, model validation, explainability requirements, and post-deployment monitoring obligations. The discussion also explores potential implementation strategies and highlights areas of uncertainty, including data retention burdens, global compliance implications, and the practical difficulties of achieving statistical significance in validation with limited defect data. Disclaimer: This paper presents a technical perspective and does not constitute legal or regulatory advice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20144v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julio Zanon Diaz, Tommy Brennan, Peter Corcoran</dc:creator>
    </item>
    <item>
      <title>Can AI be Auditable?</title>
      <link>https://arxiv.org/abs/2509.00575</link>
      <description>arXiv:2509.00575v2 Announce Type: replace 
Abstract: Auditability is defined as the capacity of AI systems to be independently assessed for compliance with ethical, legal, and technical standards throughout their lifecycle. The chapter explores how auditability is being formalized through emerging regulatory frameworks, such as the EU AI Act, which mandate documentation, risk assessments, and governance structures. It analyzes the diverse challenges facing AI auditability, including technical opacity, inconsistent documentation practices, lack of standardized audit tools and metrics, and conflicting principles within existing responsible AI frameworks. The discussion highlights the need for clear guidelines, harmonized international regulations, and robust socio-technical methodologies to operationalize auditability at scale. The chapter concludes by emphasizing the importance of multi-stakeholder collaboration and auditor empowerment in building an effective AI audit ecosystem. It argues that auditability must be embedded in AI development practices and governance infrastructures to ensure that AI systems are not only functional but also ethically and legally aligned.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00575v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Himanshu Verma, Kirtan Padh, Eva Thelisson</dc:creator>
    </item>
    <item>
      <title>Assessing prompting frameworks for enhancing literature reviews among university students using ChatGPT</title>
      <link>https://arxiv.org/abs/2509.01128</link>
      <description>arXiv:2509.01128v2 Announce Type: replace 
Abstract: Writing literature reviews is a common component of university curricula, yet it often poses challenges for students. Since generative artificial intelligence (GenAI) tools have been made publicly accessible, students have been employing them for their academic writing tasks. However, there is limited evidence of structured training on how to effectively use these GenAI tools to support students in writing literature reviews. In this study, we explore how university students use one of the most popular GenAI tools, ChatGPT, to write literature reviews and how prompting frameworks can enhance their output. To this aim, prompts and literature reviews written by a group of university students were collected before and after they had been introduced to three prompting frameworks, namely CO-STAR, POSE, and Sandwich. The results indicate that after being exposed to these prompting frameworks, the students demonstrated improved prompting behaviour, resulting in more effective prompts and higher quality literature reviews. However, it was also found that the students did not fully utilise all the elements in the prompting frameworks, and aspects such as originality, critical analysis, and depth in their reviews remain areas for improvement. The study, therefore, raises important questions about the significance of utilising prompting frameworks in their entirety to maximise the quality of outcomes, as well as the extent of prior writing experience students should have before leveraging GenAI in the process of writing literature reviews. These findings are of interest for educators considering the integration of GenAI into academic writing tasks such as literature reviews or evaluating whether to permit students to use these tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01128v2</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aminul Islam, Mukta Bansal, Lena Felix Stephanie, Poernomo Gunawan, Pui Tze Sian, Sabrina Luk, Eunice Tan, Hortense Le Ferrand</dc:creator>
    </item>
    <item>
      <title>Linguistic Hooks: Investigating The Role of Language Triggers in Phishing Emails Targeting African Refugees and Students</title>
      <link>https://arxiv.org/abs/2509.04700</link>
      <description>arXiv:2509.04700v2 Announce Type: replace 
Abstract: Phishing and sophisticated email-based social engineering attacks disproportionately affect vulnerable populations, such as refugees and immigrant students. However, these groups remain understudied in cybersecurity research. This gap in understanding, coupled with their exclusion from broader security and privacy policies, increases their susceptibility to phishing and widens the digital security divide between marginalized and non-marginalized populations. To address this gap, we first conducted digital literacy workshops with newly resettled African refugee populations (n = 48) in the US to improve their understanding of how to safeguard sensitive and private information. Following the workshops, we conducted a real-world phishing deception study using carefully designed emails with linguistic cues for three participant groups: a subset of the African US-refugees recruited from the digital literacy workshops (n = 19), African immigrant students in the US (n = 142), and a control group of monolingual US-born students (n = 184). Our findings indicate that while digital literacy training for refugees improves awareness of safe cybersecurity practices, recently resettled African US-refugees still face significant challenges due to low digital literacy skills and limited English proficiency. This often leads them to ignore or fail to recognize phishing emails as phishing. Both African immigrant students and US-born students showed greater caution, though instances of data disclosure remained prevalent across groups. Our findings highlight, irrespective of literacy, the need to be trained to think critically about digital security. We conclude by discussing how the security and privacy community can better include marginalized populations in policy making and offer recommendations for designing equitable, inclusive cybersecurity initiatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04700v2</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mythili Menon, Nisha Vinayaga-Sureshkanth, Alec Schon, Kaitlyn Hemberger, Murtuza Jadliwala</dc:creator>
    </item>
    <item>
      <title>The Effect of Warm-Glow on User Behavioral Intention to Adopt Technology: Extending the UTAUT2 Model</title>
      <link>https://arxiv.org/abs/2210.01242</link>
      <description>arXiv:2210.01242v4 Announce Type: replace-cross 
Abstract: In this study, we enhance the Unified Theory of Acceptance and Use of Technology (UTAUT2) by incorporating the warm-glow phenomenon to clarify its impact on user decisions regarding the adoption of technology. We introduce two additional constructs aimed at capturing both the external and internal aspects of warm-glow, thus creating what we refer to as the UTAUT2 + WG model. To evaluate the effectiveness of our model, we conducted an experimental study in which participants were presented with a scenario describing a hypothetical technology designed to evoke warm-glow sensations. Using the partial least squares method, we analyzed the collected data to assess our expanded model. Our findings indicate that warm-glow significantly influences user behavior, with the internal aspect having the strongest influence, followed by hedonic motivation, performance expectancy, and finally the external aspect of warm-glow. We conclude by discussing the implications of our research, acknowledging its limitations, and suggesting directions for future exploration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.01242v4</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-99356-5_24</arxiv:DOI>
      <dc:creator>Antonios Saravanos (New York University), Neil Stott (Cambridge Judge Business School), Dongnanzi Zheng (New York University), Stavros Zervoudakis (New York University)</dc:creator>
    </item>
    <item>
      <title>Affective Computing in the Era of Large Language Models: A Survey from the NLP Perspective</title>
      <link>https://arxiv.org/abs/2408.04638</link>
      <description>arXiv:2408.04638v2 Announce Type: replace-cross 
Abstract: Affective Computing (AC) integrates computer science, psychology, and cognitive science to enable machines to recognize, interpret, and simulate human emotions across domains such as social media, finance, healthcare, and education. AC commonly centers on two task families: Affective Understanding (AU) and Affective Generation (AG). While fine-tuned pre-trained language models (PLMs) have achieved solid AU performance, they often generalize poorly across tasks and remain limited for AG, especially in producing diverse, emotionally appropriate responses. The advent of Large Language Models (LLMs) (e.g., ChatGPT and LLaMA) has catalyzed a paradigm shift by offering in-context learning, broader world knowledge, and stronger sequence generation. This survey presents an NLP-oriented overview of AC in the LLM era. We (i) consolidate traditional AC tasks and preliminary LLM-based studies; (ii) review adaptation techniques that improve AU/AG, including Instruction Tuning (full and parameter-efficient methods such as LoRA, P-/Prompt-Tuning), Prompt Engineering (zero/few-shot, chain-of-thought, agent-based prompting), and Reinforcement Learning. For the latter, we summarize RL from human preferences (RLHF), verifiable/programmatic rewards (RLVR), and AI feedback (RLAIF), which provide preference- or rule-grounded optimization signals that can help steer AU/AG toward empathy, safety, and planning, achieving finer-grained or multi-objective control. To assess progress, we compile benchmarks and evaluation practices for both AU and AG. We also discuss open challenges-from ethics, data quality, and safety to robust evaluation and resource efficiency-and outline research directions. We hope this survey clarifies the landscape and offers practical guidance for building affect-aware, reliable, and responsible LLM systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.04638v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiqun Zhang, Xiaocui Yang, Xingle Xu, Zeran Gao, Yijie Huang, Shiyi Mu, Shi Feng, Daling Wang, Yifei Zhang, Kaisong Song, Ge Yu</dc:creator>
    </item>
    <item>
      <title>Robust blue-green urban flood risk management optimised with a genetic algorithm for multiple rainstorm return periods</title>
      <link>https://arxiv.org/abs/2502.12174</link>
      <description>arXiv:2502.12174v2 Announce Type: replace-cross 
Abstract: Flood risk managers seek to optimise Blue-Green Infrastructure (BGI) designs to maximise return on investment. Current systems often use optimisation algorithms and detailed flood models to maximise benefit-cost ratios for single rainstorm return periods. However, these schemes may lack robustness in mitigating flood risks across different storm magnitudes. For example, a BGI scheme optimised for a 100-year return period may differ from one optimised for a 10-year return period. This study introduces a novel methodology incorporating five return periods (T = 10, 20, 30, 50, and 100 years) into a multi-objective BGI optimisation framework. The framework combines a Non-dominated Sorting Genetic Algorithm II (NSGA-II) with a fully distributed hydrodynamic model to optimise the spatial placement and combined size of BGI features. For the first time, direct damage cost (DDC) and expected annual damage (EAD), calculated for various building types, are used as risk objective functions, transforming a many-objective problem into a multi-objective one. Performance metrics such as Median Risk Difference (MedRD), Maximum Risk Difference (MaxRD), and Area Under Pareto Front (AUPF) reveal that a 100-year optimised BGI design performs poorly when evaluated for other return periods, particularly shorter ones. In contrast, a BGI design optimised using composite return periods enhances performance metrics across all return periods, with the greatest improvements observed in MedRD (22%) and AUPF (73%) for the 20-year return period, and MaxRD (23%) for the 50-year return period. Furthermore, climate uplift stress testing confirms the robustness of the proposed design to future rainfall extremes. This study advocates a paradigm shift in flood risk management, moving from single maximum to multiple rainstorm return period-based designs to enhance resilience and adaptability to future climate extremes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12174v2</guid>
      <category>cs.NE</category>
      <category>cs.CE</category>
      <category>cs.CY</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1111/jfr3.70118</arxiv:DOI>
      <arxiv:journal_reference>Journal of Flood Risk Management, Volume18, Issue3, September 2025, e70118</arxiv:journal_reference>
      <dc:creator>Asid Ur Rehman, Vassilis Glenis, Elizabeth Lewis, Chris Kilsby, Claire Walsh</dc:creator>
    </item>
    <item>
      <title>Locating Risk: Task Designers and the Challenge of Risk Disclosure in RAI Content Work</title>
      <link>https://arxiv.org/abs/2505.24246</link>
      <description>arXiv:2505.24246v2 Announce Type: replace-cross 
Abstract: As AI systems are increasingly tested and deployed in open-ended and high-stakes domains, crowd workers are often tasked with responsible AI (RAI) content work. These tasks include labeling violent content, moderating disturbing text, or simulating harmful behavior for red teaming exercises to shape AI system behaviors. While prior efforts have highlighted the risks to worker well-being associated with RAI content work, far less attention has been paid to how these risks are communicated to workers. Existing transparency frameworks and guidelines such as model cards, datasheets, and crowdworksheets focus on documenting model information and dataset collection processes, but they overlook an important aspect of disclosing well-being risks to workers. In the absence of standard workflows or clear guidance, the consistent application of content warnings, consent flows, or other forms of well-being risk disclosure remain unclear. This study investigates how task designers approach risk disclosure in crowdsourced RAI tasks. Drawing on interviews with 23 task designers across academic and industry sectors, we examine how well-being risk is recognized, interpreted, and communicated in practice. Our findings surface a need to support task designers in identifying and communicating well-being risk not only to support crowdworker well-being but also to strengthen the ethical integrity and technical efficacy of AI development pipelines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24246v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Alice Qian, Ryland Shaw, Laura Dabbish, Jina Suh, Hong Shen</dc:creator>
    </item>
    <item>
      <title>How Warm-Glow Alters the Usability of Technology</title>
      <link>https://arxiv.org/abs/2506.14720</link>
      <description>arXiv:2506.14720v2 Announce Type: replace-cross 
Abstract: As technology increasingly aligns with users' personal values, traditional models of usability, focused on functionality and specifically effectiveness, efficiency, and satisfaction, may not fully capture how people perceive and evaluate it. This study investigates how the warm-glow phenomenon, the positive feeling associated with doing good, shapes perceived usability. An experimental approach was taken in which participants evaluated a hypothetical technology under conditions designed to evoke either the intrinsic (i.e., personal fulfillment) or extrinsic (i.e., social recognition) dimensions of warm-glow. A Multivariate Analysis of Variance as well as subsequent follow-up analyses revealed that intrinsic warm-glow significantly enhances all dimensions of perceived usability, while extrinsic warm-glow selectively influences perceived effectiveness and satisfaction. These findings suggest that perceptions of usability extend beyond functionality and are shaped by how technology resonates with users' broader sense of purpose. We conclude by proposing that designers consider incorporating warm-glow into technology as a strategic design decision.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14720v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antonios Saravanos (New York University)</dc:creator>
    </item>
    <item>
      <title>DeepShade: Enable Shade Simulation by Text-conditioned Image Generation</title>
      <link>https://arxiv.org/abs/2507.12103</link>
      <description>arXiv:2507.12103v4 Announce Type: replace-cross 
Abstract: Heatwaves pose a significant threat to public health, especially as global warming intensifies. However, current routing systems (e.g., online maps) fail to incorporate shade information due to the difficulty of estimating shades directly from noisy satellite imagery and the limited availability of training data for generative models. In this paper, we address these challenges through two main contributions. First, we build an extensive dataset covering diverse longitude-latitude regions, varying levels of building density, and different urban layouts. Leveraging Blender-based 3D simulations alongside building outlines, we capture building shadows under various solar zenith angles throughout the year and at different times of day. These simulated shadows are aligned with satellite images, providing a rich resource for learning shade patterns. Second, we propose the DeepShade, a diffusion-based model designed to learn and synthesize shade variations over time. It emphasizes the nuance of edge features by jointly considering RGB with the Canny edge layer, and incorporates contrastive learning to capture the temporal change rules of shade. Then, by conditioning on textual descriptions of known conditions (e.g., time of day, solar angles), our framework provides improved performance in generating shade images. We demonstrate the utility of our approach by using our shade predictions to calculate shade ratios for real-world route planning in Tempe, Arizona. We believe this work will benefit society by providing a reference for urban planning in extreme heat weather and its potential practical applications in the environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12103v4</guid>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Longchao Da, Xiangrui Liu, Mithun Shivakoti, Thirulogasankar Pranav Kutralingam, Yezhou Yang, Hua Wei</dc:creator>
    </item>
    <item>
      <title>Uncertainty-Driven Reliability: Selective Prediction and Trustworthy Deployment in Modern Machine Learning</title>
      <link>https://arxiv.org/abs/2508.07556</link>
      <description>arXiv:2508.07556v2 Announce Type: replace-cross 
Abstract: Machine learning (ML) systems are increasingly deployed in high-stakes domains where reliability is paramount. This thesis investigates how uncertainty estimation can enhance the safety and trustworthiness of ML, focusing on selective prediction -- where models abstain when confidence is low.
  We first show that a model's training trajectory contains rich uncertainty signals that can be exploited without altering its architecture or loss. By ensembling predictions from intermediate checkpoints, we propose a lightweight, post-hoc abstention method that works across tasks, avoids the cost of deep ensembles, and achieves state-of-the-art selective prediction performance. Crucially, this approach is fully compatible with differential privacy (DP), allowing us to study how privacy noise affects uncertainty quality. We find that while many methods degrade under DP, our trajectory-based approach remains robust, and we introduce a framework for isolating the privacy-uncertainty trade-off. Next, we then develop a finite-sample decomposition of the selective classification gap -- the deviation from the oracle accuracy-coverage curve -- identifying five interpretable error sources and clarifying which interventions can close the gap. This explains why calibration alone cannot fix ranking errors, motivating methods that improve uncertainty ordering. Finally, we show that uncertainty signals can be adversarially manipulated to hide errors or deny service while maintaining high accuracy, and we design defenses combining calibration audits with verifiable inference.
  Together, these contributions advance reliable ML by improving, evaluating, and safeguarding uncertainty estimation, enabling models that not only make accurate predictions -- but also know when to say "I do not know".</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07556v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>stat.ML</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephan Rabanser</dc:creator>
    </item>
    <item>
      <title>Out of the Box, into the Clinic? Evaluating State-of-the-Art ASR for Clinical Applications for Older Adults</title>
      <link>https://arxiv.org/abs/2508.08684</link>
      <description>arXiv:2508.08684v2 Announce Type: replace-cross 
Abstract: Voice-controlled interfaces can support older adults in clinical contexts, with chatbots being a prime example, but reliable Automatic Speech Recognition (ASR) for underrepresented groups remains a bottleneck. This study evaluates state-of-the-art ASR models on language use of older Dutch adults, who interacted with the \texttt{Welzijn.AI} chatbot designed for geriatric contexts. We benchmark generic multilingual ASR models, and models fine-tuned for Dutch spoken by older adults, while also considering processing speed. Our results show that generic multilingual models outperform fine-tuned models, which suggests recent ASR models can generalise well out of the box to realistic datasets. Furthermore, our results suggest that truncating existing architectures is helpful in balancing the accuracy-speed trade-off, though we also identify some cases with high WER due to hallucinations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08684v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bram van Dijk, Tiberon Kuiper, Sirin Aoulad si Ahmed, Armel Levebvre, Jake Johnson, Jan Duin, Simon Mooijaart, Marco Spruit</dc:creator>
    </item>
    <item>
      <title>Leveraging Large Language Models for Accurate Sign Language Translation in Low-Resource Scenarios</title>
      <link>https://arxiv.org/abs/2508.18183</link>
      <description>arXiv:2508.18183v2 Announce Type: replace-cross 
Abstract: Translating natural languages into sign languages is a highly complex and underexplored task. Despite growing interest in accessibility and inclusivity, the development of robust translation systems remains hindered by the limited availability of parallel corpora which align natural language with sign language data. Existing methods often struggle to generalize in these data-scarce environments, as the few datasets available are typically domain-specific, lack standardization, or fail to capture the full linguistic richness of sign languages. To address this limitation, we propose Advanced Use of LLMs for Sign Language Translation (AulSign), a novel method that leverages Large Language Models via dynamic prompting and in-context learning with sample selection and subsequent sign association. Despite their impressive abilities in processing text, LLMs lack intrinsic knowledge of sign languages; therefore, they are unable to natively perform this kind of translation. To overcome this limitation, we associate the signs with compact descriptions in natural language and instruct the model to use them. We evaluate our method on both English and Italian languages using SignBank+, a recognized benchmark in the field, as well as the Italian LaCAM CNR-ISTC dataset. We demonstrate superior performance compared to state-of-the-art models in low-data scenario. Our findings demonstrate the effectiveness of AulSign, with the potential to enhance accessibility and inclusivity in communication technologies for underrepresented linguistic communities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18183v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Luana Bulla, Gabriele Tuccio, Misael Mongiov\`i, Aldo Gangemi</dc:creator>
    </item>
    <item>
      <title>WeDesign: Generative AI-Facilitated Community Consultations for Urban Public Space Design</title>
      <link>https://arxiv.org/abs/2508.19256</link>
      <description>arXiv:2508.19256v2 Announce Type: replace-cross 
Abstract: Community consultations are integral to urban planning processes intended to incorporate diverse stakeholder perspectives. However, limited resources, visual and spoken language barriers, and uneven power dynamics frequently constrain inclusive decision-making. This paper examines how generative text-to-image methods, specifically Stable Diffusion XL integrated into a custom platform (WeDesign), may support equitable consultations. A half-day workshop in Montreal involved five focus groups, each consisting of architects, urban designers, AI specialists, and residents from varied demographic groups. Additional data was gathered through semi-structured interviews with six urban planning professionals. Participants indicated that immediate visual outputs facilitated creativity and dialogue, yet noted issues in visualizing specific needs of marginalized groups, such as participants with reduced mobility, accurately depicting local architectural elements, and accommodating bilingual prompts. Participants recommended the development of an open-source platform incorporating in-painting tools, multilingual support, image voting functionalities, and preference indicators. The results indicate that generative AI can broaden participation and enable iterative interactions but requires structured facilitation approaches. The findings contribute to discussions on generative AI's role and limitations in participatory urban design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19256v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rashid Mushkani, Hugo Berard, Shin Koseki</dc:creator>
    </item>
    <item>
      <title>Oyster-I: Beyond Refusal -- Constructive Safety Alignment for Responsible Language Models</title>
      <link>https://arxiv.org/abs/2509.01909</link>
      <description>arXiv:2509.01909v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) typically deploy safety mechanisms to prevent harmful content generation. Most current approaches focus narrowly on risks posed by malicious actors, often framing risks as adversarial events and relying on defensive refusals. However, in real-world settings, risks also come from non-malicious users seeking help while under psychological distress (e.g., self-harm intentions). In such cases, the model's response can strongly influence the user's next actions. Simple refusals may lead them to repeat, escalate, or move to unsafe platforms, creating worse outcomes. We introduce Constructive Safety Alignment (CSA), a human-centric paradigm that protects against malicious misuse while actively guiding vulnerable users toward safe and helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic anticipation of user reactions, fine-grained risk boundary discovery, and interpretable reasoning control, turning safety into a trust-building process. Oy1 achieves state-of-the-art safety among open models while retaining high general capabilities. On our Constructive Benchmark, it shows strong constructive engagement, close to GPT-5, and unmatched robustness on the Strata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from refusal-first to guidance-first safety, CSA redefines the model-user relationship, aiming for systems that are not just safe, but meaningfully helpful. We release Oy1, code, and the benchmark to support responsible, user-centered AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01909v3</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.SC</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ranjie Duan, Jiexi Liu, Xiaojun Jia, Shiji Zhao, Ruoxi Cheng, Fengxiang Wang, Cheng Wei, Yong Xie, Chang Liu, Defeng Li, Yinpeng Dong, Yichi Zhang, Yuefeng Chen, Chongwen Wang, Xingjun Ma, Xingxing Wei, Yang Liu, Hang Su, Jun Zhu, Xinfeng Li, Yitong Sun, Jie Zhang, Jinzhao Hu, Sha Xu, Yitong Yang, Jialing Tao, Hui Xue</dc:creator>
    </item>
    <item>
      <title>Gravity Well Echo Chamber Modeling With An LLM-Based Confirmation Bias Model</title>
      <link>https://arxiv.org/abs/2509.03832</link>
      <description>arXiv:2509.03832v2 Announce Type: replace-cross 
Abstract: Social media echo chambers play a central role in the spread of misinformation, yet existing models often overlook the influence of individual confirmation bias. An existing model of echo chambers is the "gravity well" model, which creates an analog between echo chambers and spatial gravity wells. We extend this established model by introducing a dynamic confirmation bias variable that adjusts the strength of pull based on a user's susceptibility to belief-reinforcing content. This variable is calculated for each user through comparisons between their posting history and their responses to posts of a wide range of viewpoints.
  Incorporating this factor produces a confirmation-bias-integrated gravity well model that more accurately identifies echo chambers and reveals community-level markers of information health. We validated the approach on nineteen Reddit communities, demonstrating improved detection of echo chambers.
  Our contribution is a framework for systematically capturing the role of confirmation bias in online group dynamics, enabling more effective identification of echo chambers. By flagging these high-risk environments, the model supports efforts to curb the spread of misinformation at its most common points of amplification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03832v2</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joseph Jackson, Georgiy Lapin, Jeremy E. Thompson</dc:creator>
    </item>
    <item>
      <title>The Good, the Bad and the Constructive: Automatically Measuring Peer Review's Utility for Authors</title>
      <link>https://arxiv.org/abs/2509.04484</link>
      <description>arXiv:2509.04484v2 Announce Type: replace-cross 
Abstract: Providing constructive feedback to paper authors is a core component of peer review. With reviewers increasingly having less time to perform reviews, automated support systems are required to ensure high reviewing quality, thus making the feedback in reviews useful for authors. To this end, we identify four key aspects of review comments (individual points in weakness sections of reviews) that drive the utility for authors: Actionability, Grounding &amp; Specificity, Verifiability, and Helpfulness. To enable evaluation and development of models assessing review comments, we introduce the RevUtil dataset. We collect 1,430 human-labeled review comments and scale our data with 10k synthetically labeled comments for training purposes. The synthetic data additionally contains rationales, i.e., explanations for the aspect score of a review comment. Employing the RevUtil dataset, we benchmark fine-tuned models for assessing review comments on these aspects and generating rationales. Our experiments demonstrate that these fine-tuned models achieve agreement levels with humans comparable to, and in some cases exceeding, those of powerful closed models like GPT-4o. Our analysis further reveals that machine-generated reviews generally underperform human reviews on our four aspects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04484v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Abdelrahman Sadallah, Tim Baumg\"artner, Iryna Gurevych, Ted Briscoe</dc:creator>
    </item>
    <item>
      <title>Emergent Social Dynamics of LLM Agents in the El Farol Bar Problem</title>
      <link>https://arxiv.org/abs/2509.04537</link>
      <description>arXiv:2509.04537v2 Announce Type: replace-cross 
Abstract: We investigate the emergent social dynamics of Large Language Model (LLM) agents in a spatially extended El Farol Bar problem, observing how they autonomously navigate this classic social dilemma. As a result, the LLM agents generated a spontaneous motivation to go to the bar and changed their decision making by becoming a collective. We also observed that the LLM agents did not solve the problem completely, but rather behaved more like humans. These findings reveal a complex interplay between external incentives (prompt-specified constraints such as the 60% threshold) and internal incentives (culturally-encoded social preferences derived from pre-training), demonstrating that LLM agents naturally balance formal game-theoretic rationality with social motivations that characterize human behavior. These findings suggest that a new model of group decision making, which could not be handled in the previous game-theoretic problem setting, can be realized by LLM agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04537v2</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryosuke Takata, Atsushi Masumori, Takashi Ikegami</dc:creator>
    </item>
    <item>
      <title>Transition of car-based human-mobility in the pandemic era: Data insight from a cross-border region in Europe</title>
      <link>https://arxiv.org/abs/2509.05166</link>
      <description>arXiv:2509.05166v2 Announce Type: replace-cross 
Abstract: Many transport authorities are collecting and publishing almost real-time road traffic data to meet the growing trend of massive open data, a vital resource for foresight decision support systems considering deep data insights. We explored the spatio-temporal transitions in the cross-country road traffic volumes in the context of modelling behavioural transitions in car-based human mobility. This study reports on individual car-based daily travel behaviour detected, before (2018) and during the COVID pandemic (2020), between Germany and neighbouring countries. In the case of Luxembourg, the Bridges and Roads Authority has installed a large digital traffic observatory infrastructure through the adoption of sensor-based IoT technologies, like other European member states. Since 2016, they have provided high-performance data processing and published open data on the country's road traffic. The dataset contains an hourly traffic count for different vehicle types, daily for representative observation points, followed by a major road network. The original dataset contains significant missing entries, so comprehensive data harmonization was performed. We observed the decrease in traffic volumes during pandemic factors (e.g. lockdowns and remote work) period by following global trend of reduced personal mobility. The understanding the dynamic adaptive travel behaviours provide a potential opportunity to generate the actionable insight including temporal and spatial implications. This study demonstrates that the national open traffic data products can have adoption potential to address cross-border insights. In relevance to the net-zero carbon transition, further study should shed light on the interpolation and downscaling approaches at the comprehensive road-network level for identifying pollution hot spots, causal link to functional landuse patterns and calculation of spatial influence area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05166v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sujit Kumar Sikder, Jyotirmaya Ijaradar, Hao Li, Hichem Omrani</dc:creator>
    </item>
  </channel>
</rss>
