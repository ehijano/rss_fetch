<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.RO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.RO</link>
    <description>cs.RO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.RO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 18 Sep 2024 04:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>3CSim: CARLA Corner Case Simulation for Control Assessment in Autonomous Driving</title>
      <link>https://arxiv.org/abs/2409.10524</link>
      <description>arXiv:2409.10524v1 Announce Type: new 
Abstract: We present the CARLA corner case simulation (3CSim) for evaluating autonomous driving (AD) systems within the CARLA simulator. This framework is designed to address the limitations of traditional AD model training by focusing on non-standard, rare, and cognitively challenging scenarios. These corner cases are crucial for ensuring vehicle safety and reliability, as they test advanced control capabilities under unusual conditions. Our approach introduces a taxonomy of corner cases categorized into state anomalies, behavior anomalies, and evidence-based anomalies. We implement 32 unique corner cases with adjustable parameters, including 9 predefined weather conditions, timing, and traffic density. The framework enables repeatable and modifiable scenario evaluations, facilitating the creation of a comprehensive dataset for further analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10524v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mat\'u\v{s} \v{C}\'avojsk\'y, Eugen \v{S}lapak, Mat\'u\v{s} Dopiriak, Gabriel Bug\'ar, Juraj Gazda</dc:creator>
    </item>
    <item>
      <title>Slug Mobile: Test-Bench for RL Testing</title>
      <link>https://arxiv.org/abs/2409.10532</link>
      <description>arXiv:2409.10532v1 Announce Type: new 
Abstract: Sim-to real gap in Reinforcement Learning is when a model trained in a simulator does not translate to the real world. This is a problem for Autonomous Vehicles (AVs) as vehicle dynamics can vary from simulation to reality, and also from vehicle to vehicle. Slug Mobile is a one tenth scale autonomous vehicle created to help address the sim-to-real gap for AVs by acting as a test-bench to develop models that can easily scale from one vehicle to another. In addition to traditional sensors found in other one tenth scale AVs, we have also included a Dynamic Vision Sensor so we can train Spiking Neural Networks running on neuromorphic hardware.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10532v1</guid>
      <category>cs.RO</category>
      <category>cs.LG</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jonathan Wellington Morris, Vishrut Shah, Alex Besanceney, Daksh Shah, Leilani H. Gilpin</dc:creator>
    </item>
    <item>
      <title>Safe Interval Motion Planning for Quadrotors in Dynamic Environments</title>
      <link>https://arxiv.org/abs/2409.10647</link>
      <description>arXiv:2409.10647v1 Announce Type: new 
Abstract: Trajectory generation in dynamic environments presents a significant challenge for quadrotors, particularly due to the non-convexity in the spatial-temporal domain. Many existing methods either assume simplified static environments or struggle to produce optimal solutions in real-time. In this work, we propose an efficient safe interval motion planning framework for navigation in dynamic environments. A safe interval refers to a time window during which a specific configuration is safe. Our approach addresses trajectory generation through a two-stage process: a front-end graph search step followed by a back-end gradient-based optimization. We ensure completeness and optimality by constructing a dynamic connected visibility graph and incorporating low-order dynamic bounds within safe intervals and temporal corridors. To avoid local minima, we propose a Uniform Temporal Visibility Deformation (UTVD) for the complete evaluation of spatial-temporal topological equivalence. We represent trajectories with B-Spline curves and apply gradient-based optimization to navigate around static and moving obstacles within spatial-temporal corridors. Through simulation and real-world experiments, we show that our method can achieve a success rate of over 95% in environments with different density levels, exceeding the performance of other approaches, demonstrating its potential for practical deployment in highly dynamic environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10647v1</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Songhao Huang, Yuwei Wu, Yuezhan Tao, Vijay Kumar</dc:creator>
    </item>
    <item>
      <title>Disentangling Uncertainty for Safe Social Navigation using Deep Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2409.10655</link>
      <description>arXiv:2409.10655v1 Announce Type: new 
Abstract: Autonomous mobile robots are increasingly employed in pedestrian-rich environments where safe navigation and appropriate human interaction are crucial. While Deep Reinforcement Learning (DRL) enables socially integrated robot behavior, challenges persist in novel or perturbed scenarios to indicate when and why the policy is uncertain. Unknown uncertainty in decision-making can lead to collisions or human discomfort and is one reason why safe and risk-aware navigation is still an open problem. This work introduces a novel approach that integrates aleatoric, epistemic, and predictive uncertainty estimation into a DRL-based navigation framework for uncertainty estimates in decision-making. We, therefore, incorporate Observation-Dependent Variance (ODV) and dropout into the Proximal Policy Optimization (PPO) algorithm. For different types of perturbations, we compare the ability of Deep Ensembles and Monte-Carlo Dropout (MC-Dropout) to estimate the uncertainties of the policy. In uncertain decision-making situations, we propose to change the robot's social behavior to conservative collision avoidance. The results show that the ODV-PPO algorithm converges faster with better generalization and disentangles the aleatoric and epistemic uncertainties. In addition, the MC-Dropout approach is more sensitive to perturbations and capable to correlate the uncertainty type to the perturbation type better. With the proposed safe action selection scheme, the robot can navigate in perturbed environments with fewer collisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10655v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Fl\"ogel, Marcos G\'omez Villafa\~ne, Joshua Ransiek, S\"oren Hohmann</dc:creator>
    </item>
    <item>
      <title>Online Diffusion-Based 3D Occupancy Prediction at the Frontier with Probabilistic Map Reconciliation</title>
      <link>https://arxiv.org/abs/2409.10681</link>
      <description>arXiv:2409.10681v1 Announce Type: new 
Abstract: Autonomous navigation and exploration in unmapped environments remains a significant challenge in robotics due to the difficulty robots face in making commonsense inference of unobserved geometries. Recent advancements have demonstrated that generative modeling techniques, particularly diffusion models, can enable systems to infer these geometries from partial observation. In this work, we present implementation details and results for real-time, online occupancy prediction using a modified diffusion model. By removing attention-based visual conditioning and visual feature extraction components, we achieve a 73$\%$ reduction in runtime with minimal accuracy reduction. These modifications enable occupancy prediction across the entire map, rather than being limited to the area around the robot where camera data can be collected. We introduce a probabilistic update method for merging predicted occupancy data into running occupancy maps, resulting in a 71$\%$ improvement in predicting occupancy at map frontiers compared to previous methods. Finally, we release our code and a ROS node for on-robot operation &lt;upon publication&gt; at github.com/arpg/sceneSense_ws.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10681v1</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alec Reed, Lorin Achey, Brendan Crowe, Bradley Hayes, Christoffer Heckman</dc:creator>
    </item>
    <item>
      <title>MotIF: Motion Instruction Fine-tuning</title>
      <link>https://arxiv.org/abs/2409.10683</link>
      <description>arXiv:2409.10683v1 Announce Type: new 
Abstract: While success in many robotics tasks can be determined by only observing the final state and how it differs from the initial state - e.g., if an apple is picked up - many tasks require observing the full motion of the robot to correctly determine success. For example, brushing hair requires repeated strokes that correspond to the contours and type of hair. Prior works often use off-the-shelf vision-language models (VLMs) as success detectors; however, when success depends on the full trajectory, VLMs struggle to make correct judgments for two reasons. First, modern VLMs are trained only on single frames, and cannot capture changes over a full trajectory. Second, even if we provide state-of-the-art VLMs with an aggregate input of multiple frames, they still fail to detect success due to a lack of robot data. Our key idea is to fine-tune VLMs using abstract representations that are able to capture trajectory-level information such as the path the robot takes by overlaying keypoint trajectories on the final image. We propose motion instruction fine-tuning (MotIF), a method that fine-tunes VLMs using the aforementioned abstract representations to semantically ground the robot's behavior in the environment. To benchmark and fine-tune VLMs for robotic motion understanding, we introduce the MotIF-1K dataset containing 653 human and 369 robot demonstrations across 13 task categories. MotIF assesses the success of robot motion given the image observation of the trajectory, task instruction, and motion description. Our model significantly outperforms state-of-the-art VLMs by at least twice in precision and 56.1% in recall, generalizing across unseen motions, tasks, and environments. Finally, we demonstrate practical applications of MotIF in refining and terminating robot planning, and ranking trajectories on how they align with task and motion descriptions. Project page: https://motif-1k.github.io</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10683v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Minyoung Hwang, Joey Hejna, Dorsa Sadigh, Yonatan Bisk</dc:creator>
    </item>
    <item>
      <title>Encoding Reusable Multi-Robot Planning Strategies as Abstract Hypergraphs</title>
      <link>https://arxiv.org/abs/2409.10692</link>
      <description>arXiv:2409.10692v1 Announce Type: new 
Abstract: Multi-Robot Task Planning (MR-TP) is the search for a discrete-action plan a team of robots should take to complete a task. The complexity of such problems scales exponentially with the number of robots and task complexity, making them challenging for online solution. To accelerate MR-TP over a system's lifetime, this work looks at combining two recent advances: (i) Decomposable State Space Hypergraph (DaSH), a novel hypergraph-based framework to efficiently model and solve MR-TP problems; and \mbox{(ii) learning-by-abstraction,} a technique that enables automatic extraction of generalizable planning strategies from individual planning experiences for later reuse. Specifically, we wish to extend this strategy-learning technique, originally designed for single-robot planning, to benefit multi-robot planning using hypergraph-based MR-TP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10692v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Khen Elimelech, James Motes, Marco Morales, Nancy M. Amato, Moshe Y. Vardi, Lydia E. Kavraki</dc:creator>
    </item>
    <item>
      <title>Finite Element Modeling of Surface Traveling Wave Friction Driven for Rotary Ultrasonic Motor</title>
      <link>https://arxiv.org/abs/2409.10707</link>
      <description>arXiv:2409.10707v1 Announce Type: new 
Abstract: Finite element modeling (FEM) is a critical tool in the design and analysis of piezoelectric devices, offering detailed numerical simulations that guide various applications. While traditionally applied to eigenfrequency analysis and time-dependent studies for predicting excitation eigenfrequencies and estimating traveling wave amplitudes, FEM's potential extends to more sophisticated tasks. Advanced FEM applications, such as modeling friction-driven dynamic motion and reaction forces, are essential for accurately simulating the complex behaviors of piezoelectric actuators under real-world conditions. This paper presents a comprehensive motor model that encompasses the coupling dynamics between the stator and rotor in a piezoelectric ultrasonic motor (USM). Utilizing contact theory, the model simulates the complex conditions encountered during the USM's initial start-up phase and its transition to steady-state operation. Implemented in COMSOL Multiphysics, the model provides an in-depth analysis of a rotary piezoelectric actuator, capturing the dynamic interactions and reaction forces that influence its performance. The introduction of this FEM-based model represents a significant advancement in the simulation and understanding of piezoelectric actuators. By offering a more complete picture of the motor's behavior from start-up to steady state, this study enables more accurate control and optimization of piezoelectric devices, enhancing their efficiency and reliability in practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10707v1</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zhanyue Zhao, Yang Wang, Charles Bales, Yiwei Jiang, Gregory Fischer</dc:creator>
    </item>
    <item>
      <title>A HeARTfelt Robot: Social Robot-Driven Deep Emotional Art Reflection with Children</title>
      <link>https://arxiv.org/abs/2409.10710</link>
      <description>arXiv:2409.10710v1 Announce Type: new 
Abstract: Social-emotional learning (SEL) skills are essential for children to develop to provide a foundation for future relational and academic success. Using art as a medium for creation or as a topic to provoke conversation is a well-known method of SEL learning. Similarly, social robots have been used to teach SEL competencies like empathy, but the combination of art and social robotics has been minimally explored. In this paper, we present a novel child-robot interaction designed to foster empathy and promote SEL competencies via a conversation about art scaffolded by a social robot. Participants (N=11, age range: 7-11) conversed with a social robot about emotional and neutral art. Analysis of video and speech data demonstrated that this interaction design successfully engaged children in the practice of SEL skills, like emotion recognition and self-awareness, and greater rates of empathetic reasoning were observed when children engaged with the robot about emotional art. This study demonstrated that art-based reflection with a social robot, particularly on emotional art, can foster empathy in children, and interactions with a social robot help alleviate discomfort when sharing deep or vulnerable emotions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10710v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Isabella Pu, Golda Nguyen, Lama Alsultan, Rosalind Picard, Cynthia Breazeal, Sharifa Alghowinem</dc:creator>
    </item>
    <item>
      <title>Trustworthy Conceptual Explanations for Neural Networks in Robot Decision-Making</title>
      <link>https://arxiv.org/abs/2409.10733</link>
      <description>arXiv:2409.10733v1 Announce Type: new 
Abstract: Black box neural networks are an indispensable part of modern robots. Nevertheless, deploying such high-stakes systems in real-world scenarios poses significant challenges when the stakeholders, such as engineers and legislative bodies, lack insights into the neural networks' decision-making process. Presently, explainable AI is primarily tailored to natural language processing and computer vision, falling short in two critical aspects when applied in robots: grounding in decision-making tasks and the ability to assess trustworthiness of their explanations. In this paper, we introduce a trustworthy explainable robotics technique based on human-interpretable, high-level concepts that attribute to the decisions made by the neural network. Our proposed technique provides explanations with associated uncertainty scores by matching neural network's activations with human-interpretable visualizations. To validate our approach, we conducted a series of experiments with various simulated and real-world robot decision-making models, demonstrating the effectiveness of the proposed approach as a post-hoc, human-friendly robot learning diagnostic tool.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10733v1</guid>
      <category>cs.RO</category>
      <category>cs.LG</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Som Sagar, Aditya Taparia, Harsh Mankodiya, Pranav Bidare, Yifan Zhou, Ransalu Senanayake</dc:creator>
    </item>
    <item>
      <title>Uncovering the Secrets of Human-Like Movement: A Fresh Perspective on Motion Planning</title>
      <link>https://arxiv.org/abs/2409.10747</link>
      <description>arXiv:2409.10747v1 Announce Type: new 
Abstract: This article explores human-like movement from a fresh perspective on motion planning. We analyze the coordinated and compliant movement mechanisms of the human body from the perspective of biomechanics. Based on these mechanisms, we propose an optimal control framework that integrates compliant control dynamics, optimizing robotic arm motion through a response time matrix. This matrix sets the timing parameters for joint movements, turning the system into a time-parameterized optimal control problem. The model focuses on the interaction between active and passive joints under external disturbances, improving adaptability and compliance. This method achieves optimal trajectory generation and balances precision and compliance. Experimental results on both a manipulator and a humanoid robot validate the approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10747v1</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lei Shi, Qichao Liu, Cheng Zhou, Wentao Gao, Haotian Wu, Yu Zheng, Xiong Li</dc:creator>
    </item>
    <item>
      <title>A Fairness-Oriented Control Framework for Safety-Critical Multi-Robot Systems: Alternative Authority Control</title>
      <link>https://arxiv.org/abs/2409.10749</link>
      <description>arXiv:2409.10749v1 Announce Type: new 
Abstract: This paper proposes a fair control framework for multi-robot systems, which integrates the newly introduced Alternative Authority Control (AAC) and Flexible Control Barrier Function (F-CBF). Control authority refers to a single robot which can plan its trajectory while considering others as moving obstacles, meaning the other robots do not have authority to plan their own paths. The AAC method dynamically distributes the control authority, enabling fair and coordinated movement across the system. This approach significantly improves computational efficiency, scalability, and robustness in complex environments. The proposed F-CBF extends traditional CBFs by incorporating obstacle shape, velocity, and orientation. F-CBF enhances safety by accurate dynamic obstacle avoidance. The framework is validated through simulations in multi-robot scenarios, demonstrating its safety, robustness and computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10749v1</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lei Shi, Qichao Liu, Cheng Zhou, Xiong Li</dc:creator>
    </item>
    <item>
      <title>Towards the Feasibility Analysis and Additive Manufacturing of a Novel Flexible Pedicle Screw for Spinal Fixation Procedures</title>
      <link>https://arxiv.org/abs/2409.10778</link>
      <description>arXiv:2409.10778v1 Announce Type: new 
Abstract: In this paper, we explore the feasibility of developing a novel flexible pedicle screw (FPS) for enhanced spinal fixation of osteoporotic vertebrae. Vital for spinal fracture treatment, pedicle screws have been around since the early 20th century and have undergone multiple iterations to enhance internal spinal fixation. However, spinal fixation treatments tend to be problematic for osteoporotic patients due to multiple inopportune variables. The inherent rigid nature of the pedicle screw, along with the forced linear trajectory of the screw path, frequently leads to the placement of these screws in highly osteoporotic regions of the bone. This results in eventual screw slippage and causing neurological and respiratory problems for the patient. To address this problem, we focus on developing a novel FPS that is structurally capable of safely bending to fit curved trajectories drilled by a steerable drilling robot and bypass highly osteoporotic regions of the vertebral body. Afterwards, we simulate its morphability capabilities using finite element analysis (FEA). We then additively manufacture the FPS using stainless steel (SS) 316L alloy through direct metal laser sintering (DMLS). Finally, the fabricated FPS is experimentally evaluated for its bending performance and compared with the FEA results for verification. Results demonstrate the feasibility of additive manufacturing of FPS using DMLS approach and agreement of the developed FEA with the experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10778v1</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yash Kulkarni, Susheela Sharma, Jared Allison, Jordan Amadio, Maryam Tilton, Farshid Alambeigi</dc:creator>
    </item>
    <item>
      <title>Benchmarking Sim2Real Gap: High-fidelity Digital Twinning of Agile Manufacturing</title>
      <link>https://arxiv.org/abs/2409.10784</link>
      <description>arXiv:2409.10784v1 Announce Type: new 
Abstract: As the manufacturing industry shifts from mass production to mass customization, there is a growing emphasis on adopting agile, resilient, and human-centric methodologies in line with the directives of Industry 5.0. Central to this transformation is the deployment of digital twins, a technology that digitally replicates manufacturing assets to enable enhanced process optimization, predictive maintenance, synthetic data generation, and accelerated customization and prototyping. This chapter delves into the technologies underpinning the creation of digital twins specifically tailored to agile manufacturing scenarios within the realm of robotic automation. It explores the transfer of trained policies and process optimizations from simulated settings to real-world applications through advanced techniques such as domain randomization, domain adaptation, curriculum learning, and model-based system identification. The chapter also examines various industrial manufacturing automation scenarios, including bin-picking, part inspection, and product assembly, under Sim2Real conditions. The performance of digital twin technologies in these scenarios is evaluated using practical metrics including data latency, adaptation rate, simulation fidelity among others reported, providing a comprehensive assessment of their efficacy and potential impact on modern manufacturing processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10784v1</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sunny Katyara, Suchita Sharma, Praveen Damacharla, Carlos Garcia Santiago, Lubina Dhirani, Bhawani Shankar Chowdhry</dc:creator>
    </item>
    <item>
      <title>Bayesian Optimal Experimental Design for Robot Kinematic Calibration</title>
      <link>https://arxiv.org/abs/2409.10802</link>
      <description>arXiv:2409.10802v1 Announce Type: new 
Abstract: This paper develops a Bayesian optimal experimental design for robot kinematic calibration on ${\mathbb{S}^3 \!\times\! \mathbb{R}^3}$. Our method builds upon a Gaussian process approach that incorporates a geometry-aware kernel based on Riemannian Mat\'ern kernels over ${\mathbb{S}^3}$. To learn the forward kinematics errors via Bayesian optimization with a Gaussian process, we define a geodesic distance-based objective function. Pointwise values of this function are sampled via noisy measurements taken through fiducial markers on the end-effector using a camera and computed pose with the nominal kinematics. The corrected Denavit-Hartenberg parameters using an efficient quadratic program that operates on the obtained data sets. The effectiveness of the proposed method is demonstrated via simulations and calibration experiments on NASA's ocean world lander autonomy testbed (OWLAT).</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10802v1</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ersin Das, Thomas Touma, Joel W. Burdick</dc:creator>
    </item>
    <item>
      <title>Evaluating and Improving the Robustness of LiDAR-based Localization and Mapping</title>
      <link>https://arxiv.org/abs/2409.10824</link>
      <description>arXiv:2409.10824v1 Announce Type: new 
Abstract: LiDAR is one of the most commonly adopted sensors for simultaneous localization and mapping (SLAM) and map-based global localization. SLAM and map-based localization are crucial for the independent operation of autonomous systems, especially when external signals such as GNSS are unavailable or unreliable. While state-of-the-art (SOTA) LiDAR SLAM systems could achieve 0.5% (i.e., 0.5m per 100m) of errors and map-based localization could achieve centimeter-level global localization, it is still unclear how robust they are under various common LiDAR data corruptions. In this work, we extensively evaluated five SOTA LiDAR-based localization systems under 18 common scene-level LiDAR point cloud data (PCD) corruptions. We found that the robustness of LiDAR-based localization varies significantly depending on the category. For SLAM, hand-crafted methods are in general robust against most types of corruption, while being extremely vulnerable (up to +80% errors) to a specific corruption. Learning-based methods are vulnerable to most types of corruptions. For map-based global localization, we found that the SOTA is resistant to all applied corruptions. Finally, we found that simple Bilateral Filter denoising effectively eliminates noise-based corruption but is not helpful in density-based corruption. Re-training is more effective in defending learning-based SLAM against all types of corruption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10824v1</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bo Yang, Tri Minh Triet Pham, Jinqiu Yang</dc:creator>
    </item>
    <item>
      <title>A variational approach to geometric mechanics for undulating robotic locomotion</title>
      <link>https://arxiv.org/abs/2409.10827</link>
      <description>arXiv:2409.10827v1 Announce Type: new 
Abstract: Limbless organisms of all sizes use undulating patterns of self-deformation to locomote. Geometric mechanics, which maps deformations to motions, provides a powerful framework to formalize and investigate the theoretical properties and limitations of such modes of locomotion. However, the inherent level of abstraction poses a challenge when bridging the gap between theory or simulations and laboratory experiments. We investigate the challenges of modeling motion trajectories of an undulating robotic locomotor by comparing experiments and simulations performed with a variational integrator. Despite the extensive simplifications that the model based on a geometric variation principle entails, the simulations show good agreement on average. Notably, our approach merely requires the knowledge of the \emph{dissipation metric} -- a Riemannian metric on the configuration space, which can in practice be approximated by means closely resembling \emph{resistive force theory}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10827v1</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sean Even, Patrick S. Martinez, Cora Keogh, Oliver Gross, Yasemin Ozkan-Aydin, Peter Schr\"oder</dc:creator>
    </item>
    <item>
      <title>DIGIMON: Diagnosis and Mitigation of Sampling Skew for Reinforcement Learning based Meta-Planner in Robot Navigation</title>
      <link>https://arxiv.org/abs/2409.10832</link>
      <description>arXiv:2409.10832v1 Announce Type: new 
Abstract: Robot navigation is increasingly crucial across applications like delivery services and warehouse management. The integration of Reinforcement Learning (RL) with classical planning has given rise to meta-planners that combine the adaptability of RL with the explainable decision-making of classical planners. However, the exploration capabilities of RL-based meta-planners during training are often constrained by the capabilities of the underlying classical planners. This constraint can result in limited exploration, thereby leading to sampling skew issues. To address these issues, our paper introduces a novel framework, DIGIMON, which begins with behavior-guided diagnosis for exploration bottlenecks within the meta-planner and follows up with a mitigation strategy that conducts up-sampling from diagnosed bottleneck data. Our evaluation shows 13.5%+ improvement in navigation performance, greater robustness in out-of-distribution environments, and a 4x boost in training efficiency. DIGIMON is designed as a versatile, plug-and-play solution, allowing seamless integration into various RL-based meta-planners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10832v1</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shiwei Feng, Xuan Chen, Zhiyuan Cheng, Zikang Xiong, Yifei Gao, Siyuan Cheng, Sayali Kate, Xiangyu Zhang</dc:creator>
    </item>
    <item>
      <title>SIFToM: Robust Spoken Instruction Following through Theory of Mind</title>
      <link>https://arxiv.org/abs/2409.10849</link>
      <description>arXiv:2409.10849v1 Announce Type: new 
Abstract: Spoken language instructions are ubiquitous in agent collaboration. However, in human-robot collaboration, recognition accuracy for human speech is often influenced by various speech and environmental factors, such as background noise, the speaker's accents, and mispronunciation. When faced with noisy or unfamiliar auditory inputs, humans use context and prior knowledge to disambiguate the stimulus and take pragmatic actions, a process referred to as top-down processing in cognitive science. We present a cognitively inspired model, Speech Instruction Following through Theory of Mind (SIFToM), to enable robots to pragmatically follow human instructions under diverse speech conditions by inferring the human's goal and joint plan as prior for speech perception and understanding. We test SIFToM in simulated home experiments (VirtualHome 2). Results show that the SIFToM model outperforms state-of-the-art speech and language models, approaching human-level accuracy on challenging speech instruction following tasks. We then demonstrate its ability at the task planning level on a mobile manipulator for breakfast preparation tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10849v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.MA</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lance Ying, Jason Xinyu Liu, Shivam Aarya, Yizirui Fang, Stefanie Tellex, Joshua B. Tenenbaum, Tianmin Shu</dc:creator>
    </item>
    <item>
      <title>LVBA: LiDAR-Visual Bundle Adjustment for RGB Point Cloud Mapping</title>
      <link>https://arxiv.org/abs/2409.10868</link>
      <description>arXiv:2409.10868v1 Announce Type: new 
Abstract: Point cloud maps with accurate color are crucial in robotics and mapping applications. Existing approaches for producing RGB-colorized maps are primarily based on real-time localization using filter-based estimation or sliding window optimization, which may lack accuracy and global consistency. In this work, we introduce a novel global LiDAR-Visual bundle adjustment (BA) named LVBA to improve the quality of RGB point cloud mapping beyond existing baselines. LVBA first optimizes LiDAR poses via a global LiDAR BA, followed by a photometric visual BA incorporating planar features from the LiDAR point cloud for camera pose optimization. Additionally, to address the challenge of map point occlusions in constructing optimization problems, we implement a novel LiDAR-assisted global visibility algorithm in LVBA. To evaluate the effectiveness of LVBA, we conducted extensive experiments by comparing its mapping quality against existing state-of-the-art baselines (i.e., R$^3$LIVE and FAST-LIVO). Our results prove that LVBA can proficiently reconstruct high-fidelity, accurate RGB point cloud maps, outperforming its counterparts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10868v1</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rundong Li, Xiyuan Liu, Haotian Li, Zheng Liu, Jiarong Lin, Yixi Cai, Fu Zhang</dc:creator>
    </item>
    <item>
      <title>P2 Explore: Efficient Exploration in Unknown Clustered Environment with Floor Plan Prediction</title>
      <link>https://arxiv.org/abs/2409.10878</link>
      <description>arXiv:2409.10878v1 Announce Type: new 
Abstract: Robot exploration aims at constructing unknown environments and it is important to achieve it with shorter paths. Traditional methods focus on optimizing the visiting order based on current observations, which may lead to local-minimal results. Recently, by predicting the structure of the unseen environment, the exploration efficiency can be further improved. However, in a cluttered environment, due to the randomness of obstacles, the ability for prediction is limited. Therefore, to solve this problem, we propose a map prediction algorithm that can be efficient in predicting the layout of noisy indoor environments. We focus on the scenario of 2D exploration. First, we perform floor plan extraction by denoising the cluttered map using deep learning. Then, we use a floor plan-based algorithm to improve the prediction accuracy. Additionally, we extract the segmentation of rooms and construct their connectivity based on the predicted map, which can be used for downstream tasks. To validate the effectiveness of the proposed method, it is applied to exploration tasks. Extensive experiments show that even in cluttered scenes, our proposed method can benefit efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10878v1</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kun Song, Gaoming Chen, Masayoshi Tomizuka, Wei Zhan, Zhenhua Xiong, Mingyu Ding</dc:creator>
    </item>
    <item>
      <title>Efficient Computation of Whole-Body Control Utilizing Simplified Whole-Body Dynamics via Centroidal Dynamics</title>
      <link>https://arxiv.org/abs/2409.10903</link>
      <description>arXiv:2409.10903v1 Announce Type: new 
Abstract: In this study, we present a novel method for enhancing the computational efficiency of whole-body control for humanoid robots, a challenge accentuated by their high degrees of freedom. The reduced-dimension rigid body dynamics of a floating base robot is constructed by segmenting its kinematic chain into constrained and unconstrained chains, simplifying the dynamics of the unconstrained chain through the centroidal dynamics. The proposed dynamics model is possible to be applied to whole-body control methods, allowing the problem to be divided into two parts for more efficient computation. The efficiency of the framework is demonstrated by comparative experiments in simulations. The calculation results demonstrate a significant reduction in processing time, highlighting an improvement over the times reported in current methodologies. Additionally, the results also shows the computational efficiency increases as the degrees of freedom of robot model increases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10903v1</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junewhee Ahn, Jaesug Jung, Yisoo Lee, Hokyun Lee, Sami Haddadin, Jaeheung Park</dc:creator>
    </item>
    <item>
      <title>Multi-Floor Zero-Shot Object Navigation Policy</title>
      <link>https://arxiv.org/abs/2409.10906</link>
      <description>arXiv:2409.10906v1 Announce Type: new 
Abstract: Object navigation in multi-floor environments presents a formidable challenge in robotics, requiring sophisticated spatial reasoning and adaptive exploration strategies. Traditional approaches have primarily focused on single-floor scenarios, overlooking the complexities introduced by multi-floor structures. To address these challenges, we first propose a Multi-floor Navigation Policy (MFNP) and implement it in Zero-Shot object navigation tasks. Our framework comprises three key components: (i) Multi-floor Navigation Policy, which enables an agent to explore across multiple floors; (ii) Multi-modal Large Language Models (MLLMs) for reasoning in the navigation process; and (iii) Inter-Floor Navigation, ensuring efficient floor transitions. We evaluate MFNP on the Habitat-Matterport 3D (HM3D) and Matterport 3D (MP3D) datasets, both include multi-floor scenes. Our experiment results demonstrate that MFNP significantly outperforms all the existing methods in Zero-Shot object navigation, achieving higher success rates and improved exploration efficiency. Ablation studies further highlight the effectiveness of each component in addressing the unique challenges of multi-floor navigation. Meanwhile, we conducted real-world experiments to evaluate the feasibility of our policy. Upon deployment of MFNP, the Unitree quadruped robot demonstrated successful multi-floor navigation and found the target object in a completely unseen environment. By introducing MFNP, we offer a new paradigm for tackling complex, multi-floor environments in object navigation tasks, opening avenues for future research in visual-based navigation in realistic, multi-floor settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10906v1</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lingfeng Zhang, Hao Wang, Erjia Xiao, Xinyao Zhang, Qiang Zhang, Zixuan Jiang, Renjing Xu</dc:creator>
    </item>
    <item>
      <title>Agile Continuous Jumping in Discontinuous Terrains</title>
      <link>https://arxiv.org/abs/2409.10923</link>
      <description>arXiv:2409.10923v1 Announce Type: new 
Abstract: We focus on agile, continuous, and terrain-adaptive jumping of quadrupedal robots in discontinuous terrains such as stairs and stepping stones. Unlike single-step jumping, continuous jumping requires accurately executing highly dynamic motions over long horizons, which is challenging for existing approaches. To accomplish this task, we design a hierarchical learning and control framework, which consists of a learned heightmap predictor for robust terrain perception, a reinforcement-learning-based centroidal-level motion policy for versatile and terrain-adaptive planning, and a low-level model-based leg controller for accurate motion tracking. In addition, we minimize the sim-to-real gap by accurately modeling the hardware characteristics. Our framework enables a Unitree Go1 robot to perform agile and continuous jumps on human-sized stairs and sparse stepping stones, for the first time to the best of our knowledge. In particular, the robot can cross two stair steps in each jump and completes a 3.5m long, 2.8m high, 14-step staircase in 4.5 seconds. Moreover, the same policy outperforms baselines in various other parkour tasks, such as jumping over single horizontal or vertical discontinuities. Experiment videos can be found at \url{https://yxyang.github.io/jumping\_cod/}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10923v1</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuxiang Yang, Guanya Shi, Changyi Lin, Xiangyun Meng, Rosario Scalise, Mateo Guaman Castro, Wenhao Yu, Tingnan Zhang, Ding Zhao, Jie Tan, Byron Boots</dc:creator>
    </item>
    <item>
      <title>Frontier Shepherding: A Bio-Mimetic Multi-robot Framework for Large-Scale Exploration</title>
      <link>https://arxiv.org/abs/2409.10931</link>
      <description>arXiv:2409.10931v1 Announce Type: new 
Abstract: Efficient exploration of large-scale environments remains a critical challenge in robotics, with applications ranging from environmental monitoring to search and rescue operations. This article proposes a bio-mimetic multi-robot framework, \textit{Frontier Shepherding (FroShe)}, for large-scale exploration. The presented bio-inspired framework heuristically models frontier exploration similar to the shepherding behavior of herding dogs. This is achieved by modeling frontiers as a sheep swarm reacting to robots modeled as shepherding dogs. The framework is robust across varying environment sizes and obstacle densities and can be easily deployed across multiple agents. Simulation results showcase that the proposed method consistently performed irrespective of the simulated environment's varying sizes and obstacle densities. With the increase in the number of agents, the proposed method outperforms other state-of-the-art exploration methods, with an average improvement of $20\%$ with the next-best approach(for $3$ UAVs). The proposed technique was implemented and tested in a single and dual drone scenario in a real-world forest-like environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10931v1</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>John Lewis, Meysam Basiri, Pedro U. Lima</dc:creator>
    </item>
    <item>
      <title>RoadRunner M&amp;M -- Learning Multi-range Multi-resolution Traversability Maps for Autonomous Off-road Navigation</title>
      <link>https://arxiv.org/abs/2409.10940</link>
      <description>arXiv:2409.10940v1 Announce Type: new 
Abstract: Autonomous robot navigation in off-road environments requires a comprehensive understanding of the terrain geometry and traversability. The degraded perceptual conditions and sparse geometric information at longer ranges make the problem challenging especially when driving at high speeds. Furthermore, the sensing-to-mapping latency and the look-ahead map range can limit the maximum speed of the vehicle. Building on top of the recent work RoadRunner, in this work, we address the challenge of long-range (100 m) traversability estimation. Our RoadRunner (M&amp;M) is an end-to-end learning-based framework that directly predicts the traversability and elevation maps at multiple ranges (50 m, 100 m) and resolutions (0.2 m, 0.8 m) taking as input multiple images and a LiDAR voxel map. Our method is trained in a self-supervised manner by leveraging the dense supervision signal generated by fusing predictions from an existing traversability estimation stack (X-Racer) in hindsight and satellite Digital Elevation Maps. RoadRunner M&amp;M achieves a significant improvement of up to 50% for elevation mapping and 30% for traversability estimation over RoadRunner, and is able to predict in 30% more regions compared to X-Racer while achieving real-time performance. Experiments on various out-of-distribution datasets also demonstrate that our data-driven approach starts to generalize to novel unstructured environments. We integrate our proposed framework in closed-loop with the path planner to demonstrate autonomous high-speed off-road robotic navigation in challenging real-world environments. Project Page: https://leggedrobotics.github.io/roadrunner_mm/</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10940v1</guid>
      <category>cs.RO</category>
      <category>cs.CV</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Manthan Patel, Jonas Frey, Deegan Atha, Patrick Spieler, Marco Hutter, Shehryar Khattak</dc:creator>
    </item>
    <item>
      <title>Robust High-Speed State Estimation for Off-road Navigation using Radar Velocity Factors</title>
      <link>https://arxiv.org/abs/2409.10953</link>
      <description>arXiv:2409.10953v1 Announce Type: new 
Abstract: Enabling robot autonomy in complex environments for mission critical application requires robust state estimation. Particularly under conditions where the exteroceptive sensors, which the navigation depends on, can be degraded by environmental challenges thus, leading to mission failure. It is precisely in such challenges where the potential for FMCW radar sensors is highlighted: as a complementary exteroceptive sensing modality with direct velocity measuring capabilities. In this work we integrate radial speed measurements from a FMCW radar sensor, using a radial speed factor, to provide linear velocity updates into a sliding-window state estimator for fusion with LiDAR pose and IMU measurements. We demonstrate that this augmentation increases the robustness of the state estimator to challenging conditions present in the environment and the negative effects they can pose to vulnerable exteroceptive modalities. The proposed method is extensively evaluated using robotic field experiments conducted using an autonomous, full-scale, off-road vehicle operating at high-speeds (~12 m/s) in complex desert environments. Furthermore, the robustness of the approach is demonstrated for cases of both simulated and real-world degradation of the LiDAR odometry performance along with comparison against state-of-the-art methods for radar-inertial odometry on public datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10953v1</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Morten Nissov, Jeffrey A. Edlund, Patrick Spieler, Curtis Padgett, Kostas Alexis, Shehryar Khattak</dc:creator>
    </item>
    <item>
      <title>GLC-SLAM: Gaussian Splatting SLAM with Efficient Loop Closure</title>
      <link>https://arxiv.org/abs/2409.10982</link>
      <description>arXiv:2409.10982v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) has gained significant attention for its application in dense Simultaneous Localization and Mapping (SLAM), enabling real-time rendering and high-fidelity mapping. However, existing 3DGS-based SLAM methods often suffer from accumulated tracking errors and map drift, particularly in large-scale environments. To address these issues, we introduce GLC-SLAM, a Gaussian Splatting SLAM system that integrates global optimization of camera poses and scene models. Our approach employs frame-to-model tracking and triggers hierarchical loop closure using a global-to-local strategy to minimize drift accumulation. By dividing the scene into 3D Gaussian submaps, we facilitate efficient map updates following loop corrections in large scenes. Additionally, our uncertainty-minimized keyframe selection strategy prioritizes keyframes observing more valuable 3D Gaussians to enhance submap optimization. Experimental results on various datasets demonstrate that GLC-SLAM achieves superior or competitive tracking and mapping performance compared to state-of-the-art dense RGB-D SLAM systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10982v1</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziheng Xu, Qingfeng Li, Chen Chen, Xuefeng Liu, Jianwei Niu</dc:creator>
    </item>
    <item>
      <title>MoDex: Planning High-Dimensional Dexterous Control via Learning Neural Hand Models</title>
      <link>https://arxiv.org/abs/2409.10983</link>
      <description>arXiv:2409.10983v1 Announce Type: new 
Abstract: Controlling hands in the high-dimensional action space has been a longstanding challenge, yet humans naturally perform dexterous tasks with ease. In this paper, we draw inspiration from the human embodied cognition and reconsider dexterous hands as learnable systems. Specifically, we introduce MoDex, a framework which employs a neural hand model to capture the dynamical characteristics of hand movements. Based on the model, a bidirectional planning method is developed, which demonstrates efficiency in both training and inference. The method is further integrated with a large language model to generate various gestures such as ``Scissorshand" and ``Rock\&amp;Roll." Moreover, we show that decomposing the system dynamics into a pretrained hand model and an external model improves data efficiency, as supported by both theoretical analysis and empirical experiments. Additional visualization results are available at https://tongwu19.github.io/MoDex.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10983v1</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tong Wu, Shoujie Li, Chuqiao Lyu, Kit-Wa Sou, Wang-Sing Chan, Wenbo Ding</dc:creator>
    </item>
    <item>
      <title>Stretchable Electrohydraulic Artificial Muscle for Full Motion Ranges in Musculoskeletal Antagonistic Joints</title>
      <link>https://arxiv.org/abs/2409.11017</link>
      <description>arXiv:2409.11017v1 Announce Type: new 
Abstract: Artificial muscles play a crucial role in musculoskeletal robotics and prosthetics to approximate the force-generating functionality of biological muscle. However, current artificial muscle systems are typically limited to either contraction or extension, not both. This limitation hinders the development of fully functional artificial musculoskeletal systems. We address this challenge by introducing an artificial antagonistic muscle system capable of both contraction and extension. Our design integrates non-stretchable electrohydraulic soft actuators (HASELs) with electrostatic clutches within an antagonistic musculoskeletal framework. This configuration enables an antagonistic joint to achieve a full range of motion without displacement loss due to tendon slack. We implement a synchronization method to coordinate muscle and clutch units, ensuring smooth motion profiles and speeds. This approach facilitates seamless transitions between antagonistic muscles at operational frequencies of up to 3.2 Hz. While our prototype utilizes electrohydraulic actuators, this muscle-clutch concept is adaptable to other non-stretchable artificial muscles, such as McKibben actuators, expanding their capability for extension and full range of motion in antagonistic setups. Our design represents a significant advancement in the development of fundamental components for more functional and efficient artificial musculoskeletal systems, bringing their capabilities closer to those of their biological counterparts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11017v1</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amirhossein Kazemipour, Ronan Hinchet, Robert K. Katzschmann</dc:creator>
    </item>
    <item>
      <title>TacDiffusion: Force-domain Diffusion Policy for Precise Tactile Manipulation</title>
      <link>https://arxiv.org/abs/2409.11047</link>
      <description>arXiv:2409.11047v1 Announce Type: new 
Abstract: Assembly is a crucial skill for robots in both modern manufacturing and service robotics. However, mastering transferable insertion skills that can handle a variety of high-precision assembly tasks remains a significant challenge. This paper presents a novel framework that utilizes diffusion models to generate 6D wrench for high-precision tactile robotic insertion tasks. It learns from demonstrations performed on a single task and achieves a zero-shot transfer success rate of 95.7% across various novel high-precision tasks. Our method effectively inherits the self-adaptability demonstrated by our previous work. In this framework, we address the frequency misalignment between the diffusion policy and the real-time control loop with a dynamic system-based filter, significantly improving the task success rate by 9.15%. Furthermore, we provide a practical guideline regarding the trade-off between diffusion models' inference ability and speed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11047v1</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yansong Wu, Zongxie Chen, Fan Wu, Lingyun Chen, Liding Zhang, Zhenshan Bing, Abdalla Swikir, Alois Knoll, Sami Haddadin</dc:creator>
    </item>
    <item>
      <title>Force Myography based Torque Estimation in Human Knee and Ankle Joints</title>
      <link>https://arxiv.org/abs/2409.11061</link>
      <description>arXiv:2409.11061v1 Announce Type: new 
Abstract: Online adaptation of exoskeleton control based on muscle activity sensing is a promising way to personalize exoskeletons based on the user's biosignals. While several electromyography (EMG) based methods have been shown to improve joint torque estimation, EMG sensors require direct skin contact and complex post-processing. In contrast, force myography (FMG) measures normal forces from changes in muscle volume due to muscle activity. We propose an FMG-based method to estimate knee and ankle joint torques by combining joint angles and velocities with muscle activity information. We learn a model for joint torque estimation using Gaussian process regression (GPR). The effectiveness of the proposed FMG-based method is validated on isokinetic motions performed by two subjects. The model is compared to a baseline model using only joint angle and velocity, as well as a model augmented by EMG data. The results show that integrating FMG into exoskeleton control improves the joint torque estimation for the ankle and knee and is therefore a promising way to improve adaptability to different exoskeleton users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11061v1</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charlotte Marquardt, Arne Schulz, Miha Dezman, Gunther Kurz, Thorsten Stein, Tamim Asfour</dc:creator>
    </item>
    <item>
      <title>Co-Designing Tools and Control Policies for Robust Manipulation</title>
      <link>https://arxiv.org/abs/2409.11113</link>
      <description>arXiv:2409.11113v1 Announce Type: new 
Abstract: Inherent robustness in manipulation is prevalent in biological systems and critical for robotic manipulation systems due to real-world uncertainties and disturbances. This robustness relies not only on robust control policies but also on the design characteristics of the end-effectors. This paper introduces a bi-level optimization approach to co-designing tools and control policies to achieve robust manipulation. The approach employs reinforcement learning for lower-level control policy learning and multi-task Bayesian optimization for upper-level design optimization. Diverging from prior approaches, we incorporate caging-based robustness metrics into both levels, ensuring manipulation robustness against disturbances and environmental variations. Our method is evaluated in four non-prehensile manipulation environments, demonstrating improvements in task success rate under disturbances and environment changes. A real-world experiment is also conducted to validate the framework's practical effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11113v1</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yifei Dong, Shaohang Han, Xianyi Cheng, Werner Friedl, Rafael I. Cabral Muchacho, M\'aximo A. Roa, Jana Tumova, Florian T. Pokorny</dc:creator>
    </item>
    <item>
      <title>Multi-UAV Uniform Sweep Coverage in Unknown Environments: A Mergeable Nervous System (MNS)-Based Random Exploration</title>
      <link>https://arxiv.org/abs/2409.11116</link>
      <description>arXiv:2409.11116v1 Announce Type: new 
Abstract: This paper investigates the problem of multi-UAV uniform sweep coverage, where a homogeneous swarm of UAVs must collectively and evenly visit every portion of an unknown environment for a sampling task without having access to their own location and orientation. Random walk-based exploration strategies are practical for such a coverage scenario as they do not rely on localization and are easily implementable in robot swarms. We demonstrate that the Mergeable Nervous System (MNS) framework, which enables a robot swarm to self-organize into a hierarchical ad-hoc communication network using local communication, is a promising control approach for random exploration in unknown environments by UAV swarms. To this end, we propose an MNS-based random walk approach where UAVs self-organize into a line formation using the MNS framework and then follow a random walk strategy to cover the environment while maintaining the formation. Through simulations, we test the efficiency of our approach against several decentralized random walk-based strategies as benchmarks. Our results show that the MNS-based random walk outperforms the benchmarks in terms of the time required to achieve full coverage and the coverage uniformity at that time, assessed across both the entire environment and within local regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11116v1</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aryo Jamshidpey, Hugh H. -T. Liu</dc:creator>
    </item>
    <item>
      <title>ULOC: Learning to Localize in Complex Large-Scale Environments with Ultra-Wideband Ranges</title>
      <link>https://arxiv.org/abs/2409.11122</link>
      <description>arXiv:2409.11122v1 Announce Type: new 
Abstract: While UWB-based methods can achieve high localization accuracy in small-scale areas, their accuracy and reliability are significantly challenged in large-scale environments. In this paper, we propose a learning-based framework named ULOC for Ultra-Wideband (UWB) based localization in such complex large-scale environments. First, anchors are deployed in the environment without knowledge of their actual position. Then, UWB observations are collected when the vehicle travels in the environment. At the same time, map-consistent pose estimates are developed from registering (onboard self-localization) data with the prior map to provide the training labels. We then propose a network based on MAMBA that learns the ranging patterns of UWBs over a complex large-scale environment. The experiment demonstrates that our solution can ensure high localization accuracy on a large scale compared to the state-of-the-art. We release our source code to benefit the community at https://github.com/brytsknguyen/uloc.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11122v1</guid>
      <category>cs.RO</category>
      <category>cs.LG</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thien-Minh Nguyen, Yizhuo Yang, Tien-Dat Nguyen, Shenghai Yuan, Lihua Xie</dc:creator>
    </item>
    <item>
      <title>Use the Force, Bot! -- Force-Aware ProDMP with Event-Based Replanning</title>
      <link>https://arxiv.org/abs/2409.11144</link>
      <description>arXiv:2409.11144v1 Announce Type: new 
Abstract: Movement Primitives (MPs) are a well-established method for representing and generating modular robot trajectories. This work presents FA-ProDMP, a new approach which introduces force awareness to Probabilistic Dynamic Movement Primitives (ProDMP). FA-ProDMP adapts the trajectory during runtime to account for measured and desired forces. It offers smooth trajectories and captures position and force correlations over multiple trajectories, e.g. a set of human demonstrations. FA-ProDMP supports multiple axes of force and is thus agnostic to cartesian or joint space control. This makes FA-ProDMP a valuable tool for learning contact rich manipulation tasks such as polishing, cutting or industrial assembly from demonstration. In order to reliably evaluate FA-ProDMP, this work additionally introduces a modular, 3D printed task suite called POEMPEL, inspired by the popular Lego Technic pins. POEMPEL mimics industrial peg-in-hole assembly tasks with force requirements. It offers multiple parameters of adjustment, such as position, orientation and plug stiffness level, thus varying the direction and amount of required forces. Our experiments show that FA-ProDMP outperforms other MP formulations on the POEMPEL setup and a electrical power plug insertion task, due to its replanning capabilities based on the measured forces. These findings highlight how FA-ProDMP enhances the performance of robotic systems in contact-rich manipulation tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11144v1</guid>
      <category>cs.RO</category>
      <category>cs.LG</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul Werner L\"odige, Maximilian Xiling Li, Rudolf Lioutikov</dc:creator>
    </item>
    <item>
      <title>MI-HGNN: Morphology-Informed Heterogeneous Graph Neural Network for Legged Robot Contact Perception</title>
      <link>https://arxiv.org/abs/2409.11146</link>
      <description>arXiv:2409.11146v1 Announce Type: new 
Abstract: We present a Morphology-Informed Heterogeneous Graph Neural Network (MI-HGNN) for learning-based contact perception. The architecture and connectivity of the MI-HGNN are constructed from the robot morphology, in which nodes and edges are robot joints and links, respectively. By incorporating the morphology-informed constraints into a neural network, we improve a learning-based approach using model-based knowledge. We apply the proposed MI-HGNN to two contact perception problems, and conduct extensive experiments using both real-world and simulated data collected using two quadruped robots. Our experiments demonstrate the superiority of our method in terms of effectiveness, generalization ability, model efficiency, and sample efficiency. Our MI-HGNN improved the performance of a state-of-the-art model that leverages robot morphological symmetry by 8.4% with only 0.21% of its parameters. Although MI-HGNN is applied to contact perception problems for legged robots in this work, it can be seamlessly applied to other types of multi-body dynamical systems and has the potential to improve other robot learning frameworks. Our code is made publicly available at https://github.com/lunarlab-gatech/Morphology-Informed-HGNN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11146v1</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Butterfield, Sandilya Sai Garimella, Nai-Jen Cheng, Lu Gan</dc:creator>
    </item>
    <item>
      <title>The 1st InterAI Workshop: Interactive AI for Human-centered Robotics</title>
      <link>https://arxiv.org/abs/2409.11150</link>
      <description>arXiv:2409.11150v1 Announce Type: new 
Abstract: The workshop is affiliated with 33nd IEEE International Conference on Robot and Human Interactive Communication (RO-MAN 2024) August 26~30, 2023 / Pasadena, CA, USA. It is designed as a half-day event, extending over four hours from 9:00 to 12:30 PST time. It accommodates both in-person and virtual attendees (via Zoom), ensuring a flexible participation mode. The agenda is thoughtfully crafted to include a diverse range of sessions: two keynote speeches that promise to provide insightful perspectives, two dedicated paper presentation sessions, an interactive panel discussion to foster dialogue among experts which facilitates deeper dives into specific topics, and a 15-minute coffee break. The workshop website: https://sites.google.com/view/interaiworkshops/home.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11150v1</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuchong Zhang, Elmira Yadollahi, Yong Ma, Di Fu, Iolanda Leite, Danica Kragic</dc:creator>
    </item>
    <item>
      <title>Annealed Winner-Takes-All for Motion Forecasting</title>
      <link>https://arxiv.org/abs/2409.11172</link>
      <description>arXiv:2409.11172v1 Announce Type: new 
Abstract: In autonomous driving, motion prediction aims at forecasting the future trajectories of nearby agents, helping the ego vehicle to anticipate behaviors and drive safely. A key challenge is generating a diverse set of future predictions, commonly addressed using data-driven models with Multiple Choice Learning (MCL) architectures and Winner-Takes-All (WTA) training objectives. However, these methods face initialization sensitivity and training instabilities. Additionally, to compensate for limited performance, some approaches rely on training with a large set of hypotheses, requiring a post-selection step during inference to significantly reduce the number of predictions. To tackle these issues, we take inspiration from annealed MCL, a recently introduced technique that improves the convergence properties of MCL methods through an annealed Winner-Takes-All loss (aWTA). In this paper, we demonstrate how the aWTA loss can be integrated with state-of-the-art motion forecasting models to enhance their performance using only a minimal set of hypotheses, eliminating the need for the cumbersome post-selection step. Our approach can be easily incorporated into any trajectory prediction model normally trained using WTA and yields significant improvements. To facilitate the application of our approach to future motion forecasting models, the code will be made publicly available upon acceptance: https://github.com/valeoai/MF_aWTA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11172v1</guid>
      <category>cs.RO</category>
      <category>cs.CV</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yihong Xu, Victor Letzelter, Micka\"el Chen, \'Eloi Zablocki, Matthieu Cord</dc:creator>
    </item>
    <item>
      <title>Air-FAR: Fast and Adaptable Routing for Aerial Navigation in Large-scale Complex Unknown Environments</title>
      <link>https://arxiv.org/abs/2409.11188</link>
      <description>arXiv:2409.11188v1 Announce Type: new 
Abstract: This paper presents a novel method for real-time 3D navigation in large-scale, complex environments using a hierarchical 3D visibility graph (V-graph). The proposed algorithm addresses the computational challenges of V-graph construction and shortest path search on the graph simultaneously. By introducing hierarchical 3D V-graph construction with heuristic visibility update, the 3D V-graph is constructed in O(K*n^2logn) time, which guarantees real-time performance. The proposed iterative divide-and-conquer path search method can achieve near-optimal path solutions within the constraints of real-time operations. The algorithm ensures efficient 3D V-graph construction and path search. Extensive simulated and real-world environments validated that our algorithm reduces the travel time by 42%, achieves up to 24.8% higher trajectory efficiency, and runs faster than most benchmarks by orders of magnitude in complex environments. The code and developed simulator have been open-sourced to facilitate future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11188v1</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Botao He, Guofei Chen, Cornelia Fermuller, Yiannis Aloimonos, Ji Zhang</dc:creator>
    </item>
    <item>
      <title>SDP: Spiking Diffusion Policy for Robotic Manipulation with Learnable Channel-Wise Membrane Thresholds</title>
      <link>https://arxiv.org/abs/2409.11195</link>
      <description>arXiv:2409.11195v1 Announce Type: new 
Abstract: This paper introduces a Spiking Diffusion Policy (SDP) learning method for robotic manipulation by integrating Spiking Neurons and Learnable Channel-wise Membrane Thresholds (LCMT) into the diffusion policy model, thereby enhancing computational efficiency and achieving high performance in evaluated tasks. Specifically, the proposed SDP model employs the U-Net architecture as the backbone for diffusion learning within the Spiking Neural Network (SNN). It strategically places residual connections between the spike convolution operations and the Leaky Integrate-and-Fire (LIF) nodes, thereby preventing disruptions to the spiking states. Additionally, we introduce a temporal encoding block and a temporal decoding block to transform static and dynamic data with timestep $T_S$ into each other, enabling the transmission of data within the SNN in spike format. Furthermore, we propose LCMT to enable the adaptive acquisition of membrane potential thresholds, thereby matching the conditions of varying membrane potentials and firing rates across channels and avoiding the cumbersome process of manually setting and tuning hyperparameters. Evaluating the SDP model on seven distinct tasks with SNN timestep $T_S=4$, we achieve results comparable to those of the ANN counterparts, along with faster convergence speeds than the baseline SNN method. This improvement is accompanied by a reduction of 94.3\% in dynamic energy consumption estimated on 45nm hardware.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11195v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhixing Hou, Maoxu Gao, Hang Yu, Mengyu Yang, Chio-In Ieong</dc:creator>
    </item>
    <item>
      <title>Optimization of Rulebooks via Asymptotically Representing Lexicographic Hierarchies for Autonomous Vehicles</title>
      <link>https://arxiv.org/abs/2409.11199</link>
      <description>arXiv:2409.11199v1 Announce Type: new 
Abstract: A key challenge in autonomous driving is that Autonomous Vehicles (AVs) must contend with multiple, often conflicting, planning requirements. These requirements naturally form in a hierarchy -- e.g., avoiding a collision is more important than maintaining lane. While the exact structure of this hierarchy remains unknown, to progress towards ensuring that AVs satisfy pre-determined behavior specifications, it is crucial to develop approaches that systematically account for it. Motivated by lexicographic behavior specification in AVs, this work addresses a lexicographic multi-objective motion planning problem, where each objective is incomparably more important than the next -- consider that avoiding a collision is incomparably more important than a lane change violation. This work ties together two elements. Firstly, a multi-objective candidate function that asymptotically represents lexicographic orders is introduced. Unlike existing multi-objective cost function formulations, this approach assures that returned solutions asymptotically align with the lexicographic behavior specification. Secondly, inspired by continuation methods, we propose two algorithms that asymptotically approach minimum rank decisions -- i.e., decisions that satisfy the highest number of important rules possible. Through a couple practical examples, we showcase that the proposed candidate function asymptotically represents the lexicographic hierarchy, and that both proposed algorithms return minimum rank decisions, even when other approaches do not.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11199v1</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matteo Penlington, Alessandro Zanardi, Emilio Frazzoli</dc:creator>
    </item>
    <item>
      <title>Computational and experimental design of fast and versatile magnetic soft robotic low Re swimmers</title>
      <link>https://arxiv.org/abs/2409.11215</link>
      <description>arXiv:2409.11215v1 Announce Type: new 
Abstract: Miniaturized magnetic soft robots have shown extraordinary capabilities of contactless manipulation, complex path maneuvering, precise localization, and quick actuation, which have equipped them to cater to challenging biomedical applications such as targeted drug delivery, internal wound healing, and laparoscopic surgery. However, despite their successful fabrication by several different research groups, a thorough design strategy encompassing the optimized kinematic performance of the three fundamental biomimetic swimming modes at miniaturized length scales has not been reported till now. Here, we resolve this by designing magnetic soft robotic swimmers (MSRSs) from the class of helical and undulatory low Reynolds number (Re) swimmers using a fully coupled, experimentally calibrated computational fluid dynamics model. We study (and compare) their swimming performance, and report their steady-state swimming speed for different non-dimensional numbers that capture the competition by magnetic loading, non-linear elastic deformation and viscous solid-fluid coupling. We investigate their stability for different initial spatial orientations to ensure robustness during real-life applications. Our results show that the helical 'finger-shaped' swimmer is, by far, the fastest low Re swimmer in terms of body lengths per cycle, but that the undulatory 'carangiform' swimmer proved to be the most versatile, bi-directional swimmer with maximum stability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11215v1</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>R Pramanik, M Park, Z Ren, M Sitti, RWCP Verstappen, PR Onck</dc:creator>
    </item>
    <item>
      <title>Resilient and Adaptive Replanning for Multi-Robot Target Tracking with Sensing and Communication Danger Zones</title>
      <link>https://arxiv.org/abs/2409.11230</link>
      <description>arXiv:2409.11230v1 Announce Type: new 
Abstract: Multi-robot collaboration for target tracking presents significant challenges in hazardous environments, including addressing robot failures, dynamic priority changes, and other unpredictable factors. Moreover, these challenges are increased in adversarial settings if the environment is unknown. In this paper, we propose a resilient and adaptive framework for multi-robot, multi-target tracking in environments with unknown sensing and communication danger zones. The damages posed by these zones are temporary, allowing robots to track targets while accepting the risk of entering dangerous areas. We formulate the problem as an optimization with soft chance constraints, enabling real-time adjustments to robot behavior based on varying types of dangers and failures. An adaptive replanning strategy is introduced, featuring different triggers to improve group performance. This approach allows for dynamic prioritization of target tracking and risk aversion or resilience, depending on evolving resources and real-time conditions. To validate the effectiveness of the proposed method, we benchmark and evaluate it across multiple scenarios in simulation and conduct several real-world experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11230v1</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peihan Li, Yuwei Wu, Jiazhen Liu, Gaurav S. Sukhatme, Vijay Kumar, Lifeng Zhou</dc:creator>
    </item>
    <item>
      <title>Leveraging Symmetry to Accelerate Learning of Trajectory Tracking Controllers for Free-Flying Robotic Systems</title>
      <link>https://arxiv.org/abs/2409.11238</link>
      <description>arXiv:2409.11238v1 Announce Type: new 
Abstract: Tracking controllers enable robotic systems to accurately follow planned reference trajectories. In particular, reinforcement learning (RL) has shown promise in the synthesis of controllers for systems with complex dynamics and modest online compute budgets. However, the poor sample efficiency of RL and the challenges of reward design make training slow and sometimes unstable, especially for high-dimensional systems. In this work, we leverage the inherent Lie group symmetries of robotic systems with a floating base to mitigate these challenges when learning tracking controllers. We model a general tracking problem as a Markov decision process (MDP) that captures the evolution of both the physical and reference states. Next, we prove that symmetry in the underlying dynamics and running costs leads to an MDP homomorphism, a mapping that allows a policy trained on a lower-dimensional "quotient" MDP to be lifted to an optimal tracking controller for the original system. We compare this symmetry-informed approach to an unstructured baseline, using Proximal Policy Optimization (PPO) to learn tracking controllers for three systems: the Particle (a forced point mass), the Astrobee (a fullyactuated space robot), and the Quadrotor (an underactuated system). Results show that a symmetry-aware approach both accelerates training and reduces tracking error after the same number of training steps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11238v1</guid>
      <category>cs.RO</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jake Welde, Nishanth Rao, Pratik Kunapuli, Dinesh Jayaraman, Vijay Kumar</dc:creator>
    </item>
    <item>
      <title>P-RAG: Progressive Retrieval Augmented Generation For Planning on Embodied Everyday Task</title>
      <link>https://arxiv.org/abs/2409.11279</link>
      <description>arXiv:2409.11279v1 Announce Type: new 
Abstract: Embodied Everyday Task is a popular task in the embodied AI community, requiring agents to make a sequence of actions based on natural language instructions and visual observations. Traditional learning-based approaches face two challenges. Firstly, natural language instructions often lack explicit task planning. Secondly, extensive training is required to equip models with knowledge of the task environment. Previous works based on Large Language Model (LLM) either suffer from poor performance due to the lack of task-specific knowledge or rely on ground truth as few-shot samples. To address the above limitations, we propose a novel approach called Progressive Retrieval Augmented Generation (P-RAG), which not only effectively leverages the powerful language processing capabilities of LLMs but also progressively accumulates task-specific knowledge without ground-truth. Compared to the conventional RAG methods, which retrieve relevant information from the database in a one-shot manner to assist generation, P-RAG introduces an iterative approach to progressively update the database. In each iteration, P-RAG retrieves the latest database and obtains historical information from the previous interaction as experiential references for the current interaction. Moreover, we also introduce a more granular retrieval scheme that not only retrieves similar tasks but also incorporates retrieval of similar situations to provide more valuable reference experiences. Extensive experiments reveal that P-RAG achieves competitive results without utilizing ground truth and can even further improve performance through self-iterations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11279v1</guid>
      <category>cs.RO</category>
      <category>cs.CL</category>
      <category>cs.IR</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weiye Xu, Min Wang, Wengang Zhou, Houqiang Li</dc:creator>
    </item>
    <item>
      <title>DroneDiffusion: Robust Quadrotor Dynamics Learning with Diffusion Models</title>
      <link>https://arxiv.org/abs/2409.11292</link>
      <description>arXiv:2409.11292v1 Announce Type: new 
Abstract: An inherent fragility of quadrotor systems stems from model inaccuracies and external disturbances. These factors hinder performance and compromise the stability of the system, making precise control challenging. Existing model-based approaches either make deterministic assumptions, utilize Gaussian-based representations of uncertainty, or rely on nominal models, all of which often fall short in capturing the complex, multimodal nature of real-world dynamics. This work introduces DroneDiffusion, a novel framework that leverages conditional diffusion models to learn quadrotor dynamics, formulated as a sequence generation task. DroneDiffusion achieves superior generalization to unseen, complex scenarios by capturing the temporal nature of uncertainties and mitigating error propagation. We integrate the learned dynamics with an adaptive controller for trajectory tracking with stability guarantees. Extensive experiments in both simulation and real-world flights demonstrate the robustness of the framework across a range of scenarios, including unfamiliar flight paths and varying payloads, velocities, and wind disturbances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11292v1</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Avirup Das, Rishabh Dev Yadav, Sihao Sun, Mingfei Sun, Samuel Kaski, Wei Pan</dc:creator>
    </item>
    <item>
      <title>Autonomous Navigation in Ice-Covered Waters with Learned Predictions on Ship-Ice Interactions</title>
      <link>https://arxiv.org/abs/2409.11326</link>
      <description>arXiv:2409.11326v1 Announce Type: new 
Abstract: Autonomous navigation in ice-covered waters poses significant challenges due to the frequent lack of viable collision-free trajectories. When complete obstacle avoidance is infeasible, it becomes imperative for the navigation strategy to minimize collisions. Additionally, the dynamic nature of ice, which moves in response to ship maneuvers, complicates the path planning process. To address these challenges, we propose a novel deep learning model to estimate the coarse dynamics of ice movements triggered by ship actions through occupancy estimation. To ensure real-time applicability, we propose a novel approach that caches intermediate prediction results and seamlessly integrates the predictive model into a graph search planner. We evaluate the proposed planner both in simulation and in a physical testbed against existing approaches and show that our planner significantly reduces collisions with ice when compared to the state-of-the-art. Codes and demos of this work are available at https://github.com/IvanIZ/predictive-asv-planner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11326v1</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ninghan Zhong, Alessandro Potenza, Stephen L. Smith</dc:creator>
    </item>
    <item>
      <title>PC-SRIF: Preconditioned Cholesky-based Square Root Information Filter for Vision-aided Inertial Navigation</title>
      <link>https://arxiv.org/abs/2409.11372</link>
      <description>arXiv:2409.11372v1 Announce Type: new 
Abstract: In this paper, we introduce a novel estimator for vision-aided inertial navigation systems (VINS), the Preconditioned Cholesky-based Square Root Information Filter (PC-SRIF). When solving linear systems, employing Cholesky decomposition offers superior efficiency but can compromise numerical stability. Due to this, existing VINS utilizing (Square Root) Information Filters often opt for QR decomposition on platforms where single precision is preferred, avoiding the numerical challenges associated with Cholesky decomposition. While these issues are often attributed to the ill-conditioned information matrix in VINS, our analysis reveals that this is not an inherent property of VINS but rather a consequence of specific parameterizations. We identify several factors that contribute to an ill-conditioned information matrix and propose a preconditioning technique to mitigate these conditioning issues. Building on this analysis, we present PC-SRIF, which exhibits remarkable stability in performing Cholesky decomposition in single precision when solving linear systems in VINS. Consequently, PC-SRIF achieves superior theoretical efficiency compared to alternative estimators. To validate the efficiency advantages and numerical stability of PC-SRIF based VINS, we have conducted well controlled experiments, which provide empirical evidence in support of our theoretical findings. Remarkably, in our VINS implementation, PC-SRIF's runtime is 41% faster than QR-based SRIF.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11372v1</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tong Ke, Parth Agrawal, Yun Zhang, Weikun Zhen, Chao X. Guo, Toby Sharp, Ryan C. Dutoit</dc:creator>
    </item>
    <item>
      <title>UniLCD: Unified Local-Cloud Decision-Making via Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2409.11403</link>
      <description>arXiv:2409.11403v1 Announce Type: new 
Abstract: Embodied vision-based real-world systems, such as mobile robots, require a careful balance between energy consumption, compute latency, and safety constraints to optimize operation across dynamic tasks and contexts. As local computation tends to be restricted, offloading the computation, ie, to a remote server, can save local resources while providing access to high-quality predictions from powerful and large models. However, the resulting communication and latency overhead has led to limited usability of cloud models in dynamic, safety-critical, real-time settings. To effectively address this trade-off, we introduce UniLCD, a novel hybrid inference framework for enabling flexible local-cloud collaboration. By efficiently optimizing a flexible routing module via reinforcement learning and a suitable multi-task objective, UniLCD is specifically designed to support the multiple constraints of safety-critical end-to-end mobile systems. We validate the proposed approach using a challenging, crowded navigation task requiring frequent and timely switching between local and cloud operations. UniLCD demonstrates improved overall performance and efficiency, by over 35% compared to state-of-the-art baselines based on various split computing and early exit strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11403v1</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kathakoli Sengupta, Zhongkai Shagguan, Sandesh Bharadwaj, Sanjay Arora, Eshed Ohn-Bar, Renato Mancuso</dc:creator>
    </item>
    <item>
      <title>Motion Forecasting via Model-Based Risk Minimization</title>
      <link>https://arxiv.org/abs/2409.10585</link>
      <description>arXiv:2409.10585v1 Announce Type: cross 
Abstract: Forecasting the future trajectories of surrounding agents is crucial for autonomous vehicles to ensure safe, efficient, and comfortable route planning. While model ensembling has improved prediction accuracy in various fields, its application in trajectory prediction is limited due to the multi-modal nature of predictions. In this paper, we propose a novel sampling method applicable to trajectory prediction based on the predictions of multiple models. We first show that conventional sampling based on predicted probabilities can degrade performance due to missing alignment between models. To address this problem, we introduce a new method that generates optimal trajectories from a set of neural networks, framing it as a risk minimization problem with a variable loss function. By using state-of-the-art models as base learners, our approach constructs diverse and effective ensembles for optimal trajectory sampling. Extensive experiments on the nuScenes prediction dataset demonstrate that our method surpasses current state-of-the-art techniques, achieving top ranks on the leaderboard. We also provide a comprehensive empirical study on ensembling strategies, offering insights into their effectiveness. Our findings highlight the potential of advanced ensembling techniques in trajectory prediction, significantly improving predictive performance and paving the way for more reliable predicted trajectories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10585v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aron Distelzweig, Eitan Kosman, Andreas Look, Faris Janjo\v{s}, Denesh K. Manivannan, Abhinav Valada</dc:creator>
    </item>
    <item>
      <title>Realistic Extreme Behavior Generation for Improved AV Testing</title>
      <link>https://arxiv.org/abs/2409.10669</link>
      <description>arXiv:2409.10669v1 Announce Type: cross 
Abstract: This work introduces a framework to diagnose the strengths and shortcomings of Autonomous Vehicle (AV) collision avoidance technology with synthetic yet realistic potential collision scenarios adapted from real-world, collision-free data. Our framework generates counterfactual collisions with diverse crash properties, e.g., crash angle and velocity, between an adversary and a target vehicle by adding perturbations to the adversary's predicted trajectory from a learned AV behavior model. Our main contribution is to ground these adversarial perturbations in realistic behavior as defined through the lens of data-alignment in the behavior model's parameter space. Then, we cluster these synthetic counterfactuals to identify plausible and representative collision scenarios to form the basis of a test suite for downstream AV system evaluation. We demonstrate our framework using two state-of-the-art behavior prediction models as sources of realistic adversarial perturbations, and show that our scenario clustering evokes interpretable failure modes from a baseline AV policy under evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10669v1</guid>
      <category>math.OC</category>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robert Dyro, Matthew Foutter, Ruolin Li, Luigi Di Lillo, Edward Schmerling, Xilin Zhou, Marco Pavone</dc:creator>
    </item>
    <item>
      <title>Personalized Speech Emotion Recognition in Human-Robot Interaction using Vision Transformers</title>
      <link>https://arxiv.org/abs/2409.10687</link>
      <description>arXiv:2409.10687v1 Announce Type: cross 
Abstract: Emotions are an essential element in verbal communication, so understanding individuals' affect during a human-robot interaction (HRI) becomes imperative. This paper investigates the application of vision transformer models, namely ViT (Vision Transformers) and BEiT (BERT Pre-Training of Image Transformers) pipelines, for Speech Emotion Recognition (SER) in HRI. The focus is to generalize the SER models for individual speech characteristics by fine-tuning these models on benchmark datasets and exploiting ensemble methods. For this purpose, we collected audio data from different human subjects having pseudo-naturalistic conversations with the NAO robot. We then fine-tuned our ViT and BEiT-based models and tested these models on unseen speech samples from the participants. In the results, we show that fine-tuning vision transformers on benchmark datasets and and then using either these already fine-tuned models or ensembling ViT/BEiT models gets us the highest classification accuracies per individual when it comes to identifying four primary emotions from their speech: neutral, happy, sad, and angry, as compared to fine-tuning vanilla-ViTs or BEiTs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10687v1</guid>
      <category>eess.AS</category>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <category>cs.SD</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruchik Mishra, Andrew Frye, Madan Mohan Rayguru, Dan O. Popa</dc:creator>
    </item>
    <item>
      <title>Safety Verification and Navigation for Autonomous Vehicles based on Signal Temporal Logic Constraints</title>
      <link>https://arxiv.org/abs/2409.10689</link>
      <description>arXiv:2409.10689v1 Announce Type: cross 
Abstract: The software architecture behind modern autonomous vehicles (AV) is becoming more complex steadily. Safety verification is now an imminent task prior to the large-scale deployment of such convoluted models. For safety-critical tasks in navigation, it becomes imperative to perform a verification procedure on the trajectories proposed by the planning algorithm prior to deployment. Signal Temporal Logic (STL) constraints can dictate the safety requirements for an AV. A combination of STL constraints is called a specification. A key difference between STL and other logic constraints is that STL allows us to work on continuous signals. We verify the satisfaction of the STL specifications by calculating the robustness value for each signal within the specification. Higher robustness values indicate a safer system. Model Predictive Control (MPC) is one of the most widely used methods to control the navigation of an AV, with an underlying set of state and input constraints. Our research aims to formulate and test an MPC controller, with STL specifications as constraints, that can safely navigate an AV. The primary goal of the cost function is to minimize the control inputs. STL constraints will act as an additional layer of constraints that would change based on the scenario and task on hand. We propose using sTaliro, a MATLAB-based robustness calculator for STL specifications, formulated in a receding horizon control fashion for an AV navigation task. It inputs a simplified AV state space model and a set of STL specifications, for which it constructs a closed-loop controller. We test out our controller for different test cases/scenarios and verify the safe navigation of our AV model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10689v1</guid>
      <category>eess.SY</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aditya Parameshwaran, Yue Wang</dc:creator>
    </item>
    <item>
      <title>A Unifying Action Principle for Classical Mechanical Systems</title>
      <link>https://arxiv.org/abs/2409.11063</link>
      <description>arXiv:2409.11063v1 Announce Type: cross 
Abstract: The modern theory of classical mechanics, developed by Lagrange, Hamilton and Noether, attempts to cast all of classical motion in the form of an optimization problem, based on an energy functional called the classical action. The most important advantage of this formalism is the ability to manifestly incorporate and exploit symmetries and conservation laws. This reformulation succeeded for unconstrained and holonomic systems that at most obey position equality constraints. Non-holonomic systems, which obey velocity dependent constraints or position inequality constraints, are abundant in nature and of central relevance for science, engineering and industry. All attempts so far to solve non-holonomic dynamics as a classical action optimization problem have failed. Here we utilize the classical limit of a quantum field theory action principle to construct a novel classical action for non-holonomic systems. We therefore put to rest the 190 year old question of whether classical mechanics is variational, answering in the affirmative. We illustrate and validate our approach by solving three canonical model problems by direct numerical optimization of our new action. The formalism developed in this work significantly extends the reach of action principles to a large class of relevant mechanical systems, opening new avenues for their analysis and control both analytically and numerically.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11063v1</guid>
      <category>physics.class-ph</category>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. Rothkopf, W. A. Horowitz</dc:creator>
    </item>
    <item>
      <title>HMF: A Hybrid Multi-Factor Framework for Dynamic Intraoperative Hypotension Prediction</title>
      <link>https://arxiv.org/abs/2409.11064</link>
      <description>arXiv:2409.11064v1 Announce Type: cross 
Abstract: Intraoperative hypotension (IOH) prediction using Mean Arterial Pressure (MAP) is a critical research area with significant implications for patient outcomes during surgery. However, existing approaches predominantly employ static modeling paradigms that overlook the dynamic nature of physiological signals. In this paper, we introduce a novel Hybrid Multi-Factor (HMF) framework that reformulates IOH prediction as a blood pressure forecasting task. Our framework leverages a Transformer encoder, specifically designed to effectively capture the temporal evolution of MAP series through a patch-based input representation, which segments the input physiological series into informative patches for accurate analysis. To address the challenges of distribution shift in physiological series, our approach incorporates two key innovations: (1) Symmetric normalization and de-normalization processes help mitigate distributional drift in statistical properties, thereby ensuring the model's robustness across varying conditions, and (2) Sequence decomposition, which disaggregates the input series into trend and seasonal components, allowing for a more precise modeling of inherent sequence dependencies. Extensive experiments conducted on two real-world datasets demonstrate the superior performance of our approach compared to competitive baselines, particularly in capturing the nuanced variations in input series that are crucial for accurate IOH prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11064v1</guid>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingyue Cheng, Jintao Zhang, Zhiding Liu, Chunli Liu, Yanhu Xie</dc:creator>
    </item>
    <item>
      <title>Three Approaches to the Automation of Laser System Alignment and Their Resource Implications: A Case Study</title>
      <link>https://arxiv.org/abs/2409.11090</link>
      <description>arXiv:2409.11090v1 Announce Type: cross 
Abstract: The alignment of optical systems is a critical step in their manufacture. Alignment normally requires considerable knowledge and expertise of skilled operators. The automation of such processes has several potential advantages, but requires additional resource and upfront costs. Through a case study of a simple two mirror system we identify and examine three different automation approaches. They are: artificial neural networks; practice-led, which mimics manual alignment practices; and design-led, modelling from first principles. We find that these approaches make use of three different types of knowledge 1) basic system knowledge (of controls, measurements and goals); 2) behavioural skills and expertise, and 3) fundamental system design knowledge. We demonstrate that the different automation approaches vary significantly in human resources, and measurement sampling budgets. This will have implications for practitioners and management considering the automation of such tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11090v1</guid>
      <category>eess.SY</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David A. Robb, Donald Risbridger, Ben Mills, Ildar Rakhmatulin, Xianwen Kong, Mustafa Erden, M. J. Daniel Esser, Richard M. Carter, Mike J. Chantler</dc:creator>
    </item>
    <item>
      <title>Distributed Perception Aware Safe Leader Follower System via Control Barrier Methods</title>
      <link>https://arxiv.org/abs/2409.11394</link>
      <description>arXiv:2409.11394v1 Announce Type: cross 
Abstract: This paper addresses a distributed leader-follower formation control problem for a group of agents, each using a body-fixed camera with a limited field of view (FOV) for state estimation. The main challenge arises from the need to coordinate the agents' movements with their cameras' FOV to maintain visibility of the leader for accurate and reliable state estimation. To address this challenge, we propose a novel perception-aware distributed leader-follower safe control scheme that incorporates FOV limits as state constraints. A Control Barrier Function (CBF) based quadratic program is employed to ensure the forward invariance of a safety set defined by these constraints. Furthermore, new neural network based and double bounding boxes based estimators, combined with temporal filters, are developed to estimate system states directly from real-time image data, providing consistent performance across various environments. Comparison results in the Gazebo simulator demonstrate the effectiveness and robustness of the proposed framework in two distinct environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11394v1</guid>
      <category>eess.SY</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Richie R. Suganda, Tony Tran, Miao Pan, Lei Fan, Qin Lin, Bin Hu</dc:creator>
    </item>
    <item>
      <title>Detecting and Mitigating System-Level Anomalies of Vision-Based Controllers</title>
      <link>https://arxiv.org/abs/2309.13475</link>
      <description>arXiv:2309.13475v4 Announce Type: replace 
Abstract: Autonomous systems, such as self-driving cars and drones, have made significant strides in recent years by leveraging visual inputs and machine learning for decision-making and control. Despite their impressive performance, these vision-based controllers can make erroneous predictions when faced with novel or out-of-distribution inputs. Such errors can cascade to catastrophic system failures and compromise system safety. In this work, we introduce a run-time anomaly monitor to detect and mitigate such closed-loop, system-level failures. Specifically, we leverage a reachability-based framework to stress-test the vision-based controller offline and mine its system-level failures. This data is then used to train a classifier that is leveraged online to flag inputs that might cause system breakdowns. The anomaly detector highlights issues that transcend individual modules and pertain to the safety of the overall system. We also design a fallback controller that robustly handles these detected anomalies to preserve system safety. We validate the proposed approach on an autonomous aircraft taxiing system that uses a vision-based controller for taxiing. Our results show the efficacy of the proposed approach in identifying and handling system-level anomalies, outperforming methods such as prediction error-based detection, and ensembling, thereby enhancing the overall safety and robustness of autonomous systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.13475v4</guid>
      <category>cs.RO</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>2024/5/13 Conference 2024 IEEE International Conference on Robotics and Automation (ICRA) Pages 9953-9959 Publisher 2024 IEEE International Conference on Robotics and Automation (ICRA)</arxiv:journal_reference>
      <dc:creator>Aryaman Gupta, Kaustav Chakraborty, Somil Bansal</dc:creator>
    </item>
    <item>
      <title>Constraint-Informed Learning for Warm Starting Trajectory Optimization</title>
      <link>https://arxiv.org/abs/2312.14336</link>
      <description>arXiv:2312.14336v2 Announce Type: replace 
Abstract: Future spacecraft and surface robotic missions require increasingly capable autonomy stacks for exploring challenging and unstructured domains, and trajectory optimization will be a cornerstone of such autonomy stacks. However, the nonlinear optimization solvers required remain too slow for use on relatively resource-constrained flight-grade computers. In this work, we turn towards amortized optimization, a learning-based technique for accelerating optimization run times, and present TOAST: Trajectory Optimization with Merit Function Warm Starts. Offline, using data collected from a simulation, we train a neural network to learn a mapping to the full primal and dual solutions given the problem parameters. Crucially, we build upon recent results from decision-focused learning and present a set of decision-focused loss functions using the notion of merit functions for optimization problems. We show that training networks with such constraint-informed losses can better encode the structure of the trajectory optimization problem and jointly learn to reconstruct the primal-dual solution while yielding improved constraint satisfaction. Through numerical experiments on a Lunar rover problem and a 3-degrees-of-freedom Mars powered descent guidance problem, we demonstrate that TOAST outperforms benchmark approaches in terms of both computation times and network prediction constraint satisfaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14336v2</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.13140/RG.2.2.35597.92646</arxiv:DOI>
      <dc:creator>Julia Briden, Changrak Choi, Kyongsik Yun, Richard Linares, Abhishek Cauligi</dc:creator>
    </item>
    <item>
      <title>Fast Long-Term Multi-Scenario Prediction for Maneuver Planning at Unsignalized Intersections</title>
      <link>https://arxiv.org/abs/2401.14879</link>
      <description>arXiv:2401.14879v2 Announce Type: replace 
Abstract: Motion prediction for intelligent vehicles typically focuses on estimating the most probable future evolutions of a traffic scenario. Estimating the gap acceptance, i.e., whether a vehicle merges or crosses before another vehicle with the right of way, is often handled implicitly in the prediction. However, an infrastructure-based maneuver planning can assign artificial priorities between cooperative vehicles, so it needs to evaluate many more potential scenarios. Additionally, the prediction horizon has to be long enough to assess the impact of a maneuver. We, therefore, present a novel long-term prediction approach handling the gap acceptance estimation and the velocity prediction in two separate stages. Thereby, the behavior of regular vehicles as well as priority assignments of cooperative vehicles can be considered. We train both stages on real-world traffic observations to achieve realistic prediction results. Our method has a competitive accuracy and is fast enough to predict a multitude of scenarios in a short time, making it suitable to be used in a maneuver planning framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.14879v2</guid>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.23919/ACC60939.2024.10644176</arxiv:DOI>
      <arxiv:journal_reference>2024 American Control Conference (ACC), Publication Year: 2024, Pages: 111-116</arxiv:journal_reference>
      <dc:creator>Max Bastian Mertens, Jona Ruof, Jan Strohbeck, Michael Buchholz</dc:creator>
    </item>
    <item>
      <title>Detection of Fast-Moving Objects with Neuromorphic Hardware</title>
      <link>https://arxiv.org/abs/2403.10677</link>
      <description>arXiv:2403.10677v2 Announce Type: replace 
Abstract: Neuromorphic Computing (NC) and Spiking Neural Networks (SNNs) in particular are often viewed as the next generation of Neural Networks (NNs). NC is a novel bio-inspired paradigm for energy efficient neural computation, often relying on SNNs in which neurons communicate via spikes in a sparse, event-based manner. This communication via spikes can be exploited by neuromorphic hardware implementations very effectively and results in a drastic reductions of power consumption and latency in contrast to regular GPU-based NNs. In recent years, neuromorphic hardware has become more accessible, and the support of learning frameworks has improved. However, available hardware is partially still experimental, and it is not transparent what these solutions are effectively capable of, how they integrate into real-world robotics applications, and how they realistically benefit energy efficiency and latency. In this work, we provide the robotics research community with an overview of what is possible with SNNs on neuromorphic hardware focusing on real-time processing. We introduce a benchmark of three popular neuromorphic hardware devices for the task of event-based object detection. Moreover, we show that an SNN on a neuromorphic hardware is able to run in a challenging table tennis robot setup in real-time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10677v2</guid>
      <category>cs.RO</category>
      <category>cs.CV</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andreas Ziegler, Karl Vetter, Thomas Gossard, Jonas Tebbe, Sebastian Otte, Andreas Zell</dc:creator>
    </item>
    <item>
      <title>Fully Distributed Cooperative Multi-agent Underwater Obstacle Avoidance</title>
      <link>https://arxiv.org/abs/2403.10759</link>
      <description>arXiv:2403.10759v3 Announce Type: replace 
Abstract: Navigation in cluttered underwater environments is challenging, especially when there are constraints on communication and self-localisation. Part of the fully distributed underwater navigation problem has been resolved by introducing multi-agent robot teams [1], however when the environment becomes cluttered, the problem remains unresolved. In this paper, we first studied the connection between everyday activity of dog walking and the cooperative underwater obstacle avoidance problem. Inspired by this analogy, we propose a novel dog walking paradigm and implement it in a multi-agent underwater system. Simulations were conducted across various scenarios, with performance benchmarked against traditional methods utilising Image-Based Visual Servoing in a multi-agent setup. Results indicate that our dog walking-inspired paradigm significantly enhances cooperative behavior among agents and outperforms the existing approach in navigating through obstacles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10759v3</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kanzhong Yao, Ognjen Marjanovic, Simon Watson</dc:creator>
    </item>
    <item>
      <title>Foundation Models to the Rescue: Deadlock Resolution in Connected Multi-Robot Systems</title>
      <link>https://arxiv.org/abs/2404.06413</link>
      <description>arXiv:2404.06413v2 Announce Type: replace 
Abstract: Connected multi-agent robotic systems (MRS) are prone to deadlocks in an obstacle environment where the robots can get stuck away from their desired locations under a smooth low-level control policy. Without an external intervention, often in terms of a high-level command, a low-level control policy cannot resolve such deadlocks. Utilizing the generalizability and low data requirements of foundation models, this paper explores the possibility of using text-based models, i.e., large language models (LLMs), and text-and-image-based models, i.e., vision-language models (VLMs), as high-level planners for deadlock resolution. We propose a hierarchical control framework where a foundation model-based high-level planner helps to resolve deadlocks by assigning a leader to the MRS along with a set of waypoints for the MRS leader. Then, a low-level distributed control policy based on graph neural networks is executed to safely follow these waypoints, thereby evading the deadlock. We conduct extensive experiments on various MRS environments using the best available pre-trained LLMs and VLMs. We compare their performance with a graph-based planner in terms of effectiveness in helping the MRS reach their target locations and computational time. Our results illustrate that, compared to grid-based planners, the foundation models perform better in terms of the goal-reaching rate and computational time for complex environments, which helps us conclude that foundation models can assist MRS operating in complex obstacle-cluttered environments to resolve deadlocks efficiently.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06413v2</guid>
      <category>cs.RO</category>
      <category>cs.CL</category>
      <category>math.OC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kunal Garg, Songyuan Zhang, Jacob Arkin, Chuchu Fan</dc:creator>
    </item>
    <item>
      <title>BEATLE -- Self-Reconfigurable Aerial Robot: Design, Control and Experimental Validation</title>
      <link>https://arxiv.org/abs/2404.09153</link>
      <description>arXiv:2404.09153v3 Announce Type: replace 
Abstract: Modular self-reconfigurable robots (MSRRs) offer enhanced task flexibility by constructing various structures suitable for each task. However, conventional terrestrial MSRRs equipped with wheels face critical challenges, including limitations in the size of constructible structures and system robustness due to elevated wrench loads applied to each module. In this work, we introduce an Aerial MSRR (A-MSRR) system named BEATLE, capable of merging and separating in-flight. BEATLE can merge without applying wrench loads to adjacent modules, thereby expanding the scalability and robustness of conventional terrestrial MSRRs. In this article, we propose a system configuration for BEATLE, including mechanical design, a control framework for multi-connected flight, and a motion planner for reconfiguration motion. The design of a docking mechanism and housing structure aims to balance the durability of the constructed structure with ease of separation. Furthermore, the proposed flight control framework achieves stable multi-connected flight based on contact wrench control. Moreover, the proposed motion planner based on a finite state machine (FSM) achieves precise and robust reconfiguration motion. We also introduce the actual implementation of the prototype and validate the robustness and scalability of the proposed system design through experiments and simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09153v3</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junichiro Sugihara, Moju Zhao, Takuzumi Nishio, Kei Okada, Masayuki Inaba</dc:creator>
    </item>
    <item>
      <title>Adaptive Reinforcement Learning for Robot Control</title>
      <link>https://arxiv.org/abs/2404.18713</link>
      <description>arXiv:2404.18713v2 Announce Type: replace 
Abstract: Deep reinforcement learning (DRL) has shown remarkable success in simulation domains, yet its application in designing robot controllers remains limited, due to its single-task orientation and insufficient adaptability to environmental changes. To overcome these limitations, we present a novel adaptive agent that leverages transfer learning techniques to dynamically adapt policy in response to different tasks and environmental conditions. The approach is validated through the blimp control challenge, where multitasking capabilities and environmental adaptability are essential. The agent is trained using a custom, highly parallelized simulator built on IsaacGym. We perform zero-shot transfer to fly the blimp in the real world to solve various tasks. We share our code at \url{https://github.com/robot-perception-group/adaptive\_agent/}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18713v2</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yu Tang Liu, Nilaksh Singh, Aamir Ahmad</dc:creator>
    </item>
    <item>
      <title>GISR: Geometric Initialization and Silhouette-based Refinement for Single-View Robot Pose and Configuration Estimation</title>
      <link>https://arxiv.org/abs/2405.04890</link>
      <description>arXiv:2405.04890v3 Announce Type: replace 
Abstract: In autonomous robotics, measurement of the robot's internal state and perception of its environment, including interaction with other agents such as collaborative robots, are essential. Estimating the pose of the robot arm from a single view has the potential to replace classical eye-to-hand calibration approaches and is particularly attractive for online estimation and dynamic environments. In addition to its pose, recovering the robot configuration provides a complete spatial understanding of the observed robot that can be used to anticipate the actions of other agents in advanced robotics use cases. Furthermore, this additional redundancy enables the planning and execution of recovery protocols in case of sensor failures or external disturbances. We introduce GISR - a deep configuration and robot-to-camera pose estimation method that prioritizes execution in real-time. GISR consists of two modules: (i) a geometric initialization module that efficiently computes an approximate robot pose and configuration, and (ii) a deep iterative silhouette-based refinement module that arrives at a final solution in just a few iterations. We evaluate GISR on publicly available data and show that it outperforms existing methods of the same class in terms of both speed and accuracy, and can compete with approaches that rely on ground-truth proprioception and recover only the pose.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04890v3</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ivan Bili\'c, Filip Mari\'c, Fabio Bonsignorio, Ivan Petrovi\'c</dc:creator>
    </item>
    <item>
      <title>RuleFuser: An Evidential Bayes Approach for Rule Injection in Imitation Learned Planners and Predictors for Robustness under Distribution Shifts</title>
      <link>https://arxiv.org/abs/2405.11139</link>
      <description>arXiv:2405.11139v3 Announce Type: replace 
Abstract: Modern motion planners for autonomous driving frequently use imitation learning (IL) to draw from expert driving logs. Although IL benefits from its ability to glean nuanced and multi-modal human driving behaviors from large datasets, the resulting planners often struggle with out-of-distribution (OOD) scenarios and with traffic rule compliance. On the other hand, classical rule-based planners, by design, can generate safe traffic rule compliant behaviors while being robust to OOD scenarios, but these planners fail to capture nuances in agent-to-agent interactions and human drivers' intent. RuleFuser, an evidential framework, combines IL planners with classical rule-based planners to draw on the complementary benefits of both, thereby striking a balance between imitation and safety.
  Our approach, tested on the real-world nuPlan dataset, combines the IL planner's high performance in in-distribution (ID) scenarios with the rule-based planners' enhanced safety in out-of-distribution (OOD) scenarios, achieving a 38.43% average improvement on safety metrics over the IL planner without much detriment to imitation metrics in OOD scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11139v3</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jay Patrikar, Sushant Veer, Apoorva Sharma, Marco Pavone, Sebastian Scherer</dc:creator>
    </item>
    <item>
      <title>Exploiting Chordal Sparsity for Fast Global Optimality with Application to Localization</title>
      <link>https://arxiv.org/abs/2406.02365</link>
      <description>arXiv:2406.02365v4 Announce Type: replace 
Abstract: In recent years, many estimation problems in robotics have been shown to be solvable to global optimality using their semidefinite relaxations. However, the runtime complexity of off-the-shelf semidefinite programming (SDP) solvers is up to cubic in problem size, which inhibits real-time solutions of problems involving large state dimensions. We show that for a large class of problems, namely those with chordal sparsity, we can reduce the complexity of these solvers to linear in problem size. In particular, we show how to replace the large positive-semidefinite variable with a number of smaller interconnected ones using the well-known chordal decomposition. This formulation also allows for the straightforward application of the alternating direction method of multipliers (ADMM), which can exploit parallelism for increased scalability. We show for two example problems in simulation that the chordal solvers provide a significant speed-up over standard SDP solvers, and that global optimality is crucial in the absence of good initializations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02365v4</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Frederike D\"umbgen, Connor Holmes, Timothy D. Barfoot</dc:creator>
    </item>
    <item>
      <title>Navi2Gaze: Leveraging Foundation Models for Navigation and Target Gazing</title>
      <link>https://arxiv.org/abs/2407.09053</link>
      <description>arXiv:2407.09053v2 Announce Type: replace 
Abstract: Task-aware navigation continues to be a challenging area of research, especially in scenarios involving open vocabulary. Previous studies primarily focus on finding suitable locations for task completion, often overlooking the importance of the robot's pose. However, the robot's orientation is crucial for successfully completing tasks because of how objects are arranged (e.g., to open a refrigerator door). Humans intuitively navigate to objects with the right orientation using semantics and common sense. For instance, when opening a refrigerator, we naturally stand in front of it rather than to the side. Recent advances suggest that Vision-Language Models (VLMs) can provide robots with similar common sense. Therefore, we develop a VLM-driven method called Navigation-to-Gaze (Navi2Gaze) for efficient navigation and object gazing based on task descriptions. This method uses the VLM to score and select the best pose from numerous candidates automatically. In evaluations on multiple photorealistic simulation benchmarks, Navi2Gaze significantly outperforms existing approaches by precisely determining the optimal orientation relative to target objects, resulting in a 68.8% reduction in Distance to Goal (DTG). Real-world video demonstrations can be found on the supplementary website</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09053v2</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jun Zhu, Zihao Du, Haotian Xu, Fengbo Lan, Zilong Zheng, Bo Ma, Shengjie Wang, Tao Zhang</dc:creator>
    </item>
    <item>
      <title>Towards Practical Finite Sample Bounds for Motion Planning in TAMP</title>
      <link>https://arxiv.org/abs/2407.17394</link>
      <description>arXiv:2407.17394v2 Announce Type: replace 
Abstract: When using sampling-based motion planners, such as PRMs, in configuration spaces, it is difficult to determine how many samples are required for the PRM to find a solution consistently. This is relevant in Task and Motion Planning (TAMP), where many motion planning problems must be solved in sequence. We attempt to solve this problem by proving an upper bound on the number of samples that are sufficient, with high probability, to find a solution by drawing on prior work in deterministic sampling and sample complexity theory. We also introduce a numerical algorithm to compute a tighter number of samples based on the proof of the sample complexity theorem we apply to derive our bound. Our experiments show that our numerical bounding algorithm is tight within two orders of magnitude on planar planning problems and becomes looser as the problem's dimensionality increases. When deployed as a heuristic to schedule samples in a TAMP planner, we also observe planning time improvements in planar problems. While our experiments show much work remains to tighten our bounds, the ideas presented in this paper are a step towards a practical sample bound.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17394v2</guid>
      <category>cs.RO</category>
      <category>cs.CG</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seiji Shaw, Aidan Curtis, Leslie Pack Kaelbling, Tom\'as Lozano-P\'erez, Nicholas Roy</dc:creator>
    </item>
    <item>
      <title>Learning Multi-Modal Whole-Body Control for Real-World Humanoid Robots</title>
      <link>https://arxiv.org/abs/2408.07295</link>
      <description>arXiv:2408.07295v2 Announce Type: replace 
Abstract: The foundational capabilities of humanoid robots should include robustly standing, walking, and mimicry of whole and partial-body motions. This work introduces the Masked Humanoid Controller (MHC), which supports all of these capabilities by tracking target trajectories over selected subsets of humanoid state variables while ensuring balance and robustness against disturbances. The MHC is trained in simulation using a carefully designed curriculum that imitates partially masked motions from a library of behaviors spanning standing, walking, optimized reference trajectories, re-targeted video clips, and human motion capture data. It also allows for combining joystick-based control with partial-body motion mimicry. We showcase simulation experiments validating the MHC's ability to execute a wide variety of behaviors from partially-specified target motions. Moreover, we demonstrate sim-to-real transfer on the real-world Digit V3 humanoid robot. To our knowledge, this is the first instance of a learned controller that can realize whole-body control of a real-world humanoid for such diverse multi-modal targets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07295v2</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pranay Dugar, Aayam Shrestha, Fangzhou Yu, Bart van Marum, Alan Fern</dc:creator>
    </item>
    <item>
      <title>Robo-GS: A Physics Consistent Spatial-Temporal Model for Robotic Arm with Hybrid Representation</title>
      <link>https://arxiv.org/abs/2408.14873</link>
      <description>arXiv:2408.14873v2 Announce Type: replace 
Abstract: Real2Sim2Real plays a critical role in robotic arm control and reinforcement learning, yet bridging this gap remains a significant challenge due to the complex physical properties of robots and the objects they manipulate. Existing methods lack a comprehensive solution to accurately reconstruct real-world objects with spatial representations and their associated physics attributes.
  We propose a Real2Sim pipeline with a hybrid representation model that integrates mesh geometry, 3D Gaussian kernels, and physics attributes to enhance the digital asset representation of robotic arms.
  This hybrid representation is implemented through a Gaussian-Mesh-Pixel binding technique, which establishes an isomorphic mapping between mesh vertices and Gaussian models. This enables a fully differentiable rendering pipeline that can be optimized through numerical solvers, achieves high-fidelity rendering via Gaussian Splatting, and facilitates physically plausible simulation of the robotic arm's interaction with its environment using mesh-based methods.
  The code,full presentation and datasets will be made publicly available at our website https://robostudioapp.com</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14873v2</guid>
      <category>cs.RO</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Haozhe Lou, Yurong Liu, Yike Pan, Yiran Geng, Jianteng Chen, Wenlong Ma, Chenglong Li, Lin Wang, Hengzhen Feng, Lu Shi, Liyi Luo, Yongliang Shi</dc:creator>
    </item>
    <item>
      <title>CalTag: Robust calibration of mmWave Radar and LiDAR using backscatter tags</title>
      <link>https://arxiv.org/abs/2408.16867</link>
      <description>arXiv:2408.16867v2 Announce Type: replace 
Abstract: The rise of automation in robotics necessitates the use of high-quality perception systems, often through the use of multiple sensors. A crucial aspect of a successfully deployed multi-sensor system is the calibration with a known object typically named fiducial. In this work, we propose a novel fiducial system for millimeter wave radars, termed as CalTag. CalTag addresses the limitations of traditional corner reflector-based calibration methods in extremely cluttered environments. CalTag leverages millimeter wave backscatter technology to achieve more reliable calibration than corner reflectors, enhancing the overall performance of multi-sensor perception systems. We compare the performance in several real-world environments and show the improvement achieved by using CalTag as the radar fiducial over a corner reflector.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16867v2</guid>
      <category>cs.RO</category>
      <category>eess.SP</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junyi Xu, Kshitiz Bansal, Dinesh Bharadia</dc:creator>
    </item>
    <item>
      <title>Agile Decision-Making and Safety-Critical Motion Planning for Emergency Autonomous Vehicles</title>
      <link>https://arxiv.org/abs/2409.08665</link>
      <description>arXiv:2409.08665v2 Announce Type: replace 
Abstract: Efficiency is critical for autonomous vehicles (AVs), especially for emergency AVs. However, most existing methods focus on regular vehicles, overlooking the distinct strategies required by emergency vehicles to address the challenge of maximizing efficiency while ensuring safety. In this paper, we propose an Integrated Agile Decision-Making with Active and Safety-Critical Motion Planning System (IDEAM). IDEAM focuses on enabling emergency AVs, such as ambulances, to actively attain efficiency in dense traffic scenarios with safety in mind. Firstly, the speed-centric decision-making algorithm named the long short-term spatio-temporal graph-centric decision-making (LSGM) is given. LSGM comprises conditional depth-first search (C-DFS) for multiple paths generation as well as methods for speed gains and risk evaluation for path selection, which presents a robust algorithm for high efficiency and safety consideration. Secondly, with an output path from LSGM, the motion planner reconsiders environmental conditions to decide constraints states for the final planning stage, among which the lane-probing state is designed for actively attaining spatial and speed advantage. Thirdly, under the Frenet-based model predictive control (MPC) framework with final constraints state and selected path, the safety-critical motion planner employs decoupled discrete control barrier functions (DCBFs) and linearized discrete-time high-order control barrier functions (DHOCBFs) to model the constraints associated with different driving behaviors, making the optimal optimization problem convex. Finally, we extensively validate our system using scenarios from a randomly synthetic dataset, demonstrating its capability to achieve speed benefits and assure safety simultaneously.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08665v2</guid>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiming Shu, Jingyuan Zhou, Fu Zhang</dc:creator>
    </item>
    <item>
      <title>PIP-Loco: A Proprioceptive Infinite Horizon Planning Framework for Quadrupedal Robot Locomotion</title>
      <link>https://arxiv.org/abs/2409.09441</link>
      <description>arXiv:2409.09441v2 Announce Type: replace 
Abstract: A core strength of Model Predictive Control (MPC) for quadrupedal locomotion has been its ability to enforce constraints and provide interpretability of the sequence of commands over the horizon. However, despite being able to plan, MPC struggles to scale with task complexity, often failing to achieve robust behavior on rapidly changing surfaces. On the other hand, model-free Reinforcement Learning (RL) methods have outperformed MPC on multiple terrains, showing emergent motions but inherently lack any ability to handle constraints or perform planning. To address these limitations, we propose a framework that integrates proprioceptive planning with RL, allowing for agile and safe locomotion behaviors through the horizon. Inspired by MPC, we incorporate an internal model that includes a velocity estimator and a Dreamer module. During training, the framework learns an expert policy and an internal model that are co-dependent, facilitating exploration for improved locomotion behaviors. During deployment, the Dreamer module solves an infinite-horizon MPC problem, adapting actions and velocity commands to respect the constraints. We validate the robustness of our training framework through ablation studies on internal model components and demonstrate improved robustness to training noise. Finally, we evaluate our approach across multi-terrain scenarios in both simulation and hardware.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09441v2</guid>
      <category>cs.RO</category>
      <category>cs.LG</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aditya Shirwatkar, Naman Saxena, Kishore Chandra, Shishir Kolathaya</dc:creator>
    </item>
    <item>
      <title>Robots with Attitude: Singularity-Free Quaternion-Based Model-Predictive Control for Agile Legged Robots</title>
      <link>https://arxiv.org/abs/2409.09940</link>
      <description>arXiv:2409.09940v2 Announce Type: replace 
Abstract: We present a model-predictive control (MPC) framework for legged robots that avoids the singularities associated with common three-parameter attitude representations like Euler angles during large-angle rotations. Our method parameterizes the robot's attitude with singularity-free unit quaternions and makes modifications to the iterative linear-quadratic regulator (iLQR) algorithm to deal with the resulting geometry. The derivation of our algorithm requires only elementary calculus and linear algebra, deliberately avoiding the abstraction and notation of Lie groups. We demonstrate the performance and computational efficiency of quaternion MPC in several experiments on quadruped and humanoid robots.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09940v2</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zixin Zhang, John Z. Zhang, Shuo Yang, Zachary Manchester</dc:creator>
    </item>
    <item>
      <title>IRIS: Interactive Responsive Intelligent Segmentation for 3D Affordance Analysis</title>
      <link>https://arxiv.org/abs/2409.10078</link>
      <description>arXiv:2409.10078v2 Announce Type: replace 
Abstract: Recent advancements in large language and vision-language models have significantly enhanced multimodal understanding, yet translating high-level linguistic instructions into precise robotic actions in 3D space remains challenging. This paper introduces IRIS (Interactive Responsive Intelligent Segmentation), a novel training-free multimodal system for 3D affordance segmentation, alongside a benchmark for evaluating interactive language-guided affordance in everyday environments. IRIS integrates a large multimodal model with a specialized 3D vision network, enabling seamless fusion of 2D and 3D visual understanding with language comprehension. To facilitate evaluation, we present a dataset of 10 typical indoor environments, each with 50 images annotated for object actions and 3D affordance segmentation. Extensive experiments demonstrate IRIS's capability in handling interactive 3D affordance segmentation tasks across diverse settings, showcasing competitive performance across various metrics. Our results highlight IRIS's potential for enhancing human-robot interaction based on affordance understanding in complex indoor environments, advancing the development of more intuitive and efficient robotic systems for real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10078v2</guid>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Meng Chu, Xuan Zhang</dc:creator>
    </item>
    <item>
      <title>SeFlow: A Self-Supervised Scene Flow Method in Autonomous Driving</title>
      <link>https://arxiv.org/abs/2407.01702</link>
      <description>arXiv:2407.01702v2 Announce Type: replace-cross 
Abstract: Scene flow estimation predicts the 3D motion at each point in successive LiDAR scans. This detailed, point-level, information can help autonomous vehicles to accurately predict and understand dynamic changes in their surroundings. Current state-of-the-art methods require annotated data to train scene flow networks and the expense of labeling inherently limits their scalability. Self-supervised approaches can overcome the above limitations, yet face two principal challenges that hinder optimal performance: point distribution imbalance and disregard for object-level motion constraints. In this paper, we propose SeFlow, a self-supervised method that integrates efficient dynamic classification into a learning-based scene flow pipeline. We demonstrate that classifying static and dynamic points helps design targeted objective functions for different motion patterns. We also emphasize the importance of internal cluster consistency and correct object point association to refine the scene flow estimation, in particular on object details. Our real-time capable method achieves state-of-the-art performance on the self-supervised scene flow task on Argoverse 2 and Waymo datasets. The code is open-sourced at https://github.com/KTH-RPL/SeFlow along with trained model weights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01702v2</guid>
      <category>cs.CV</category>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Qingwen Zhang, Yi Yang, Peizheng Li, Olov Andersson, Patric Jensfelt</dc:creator>
    </item>
    <item>
      <title>A Dataset and Benchmark for Shape Completion of Fruits for Agricultural Robotics</title>
      <link>https://arxiv.org/abs/2407.13304</link>
      <description>arXiv:2407.13304v2 Announce Type: replace-cross 
Abstract: As the world population is expected to reach 10 billion by 2050, our agricultural production system needs to double its productivity despite a decline of human workforce in the agricultural sector. Autonomous robotic systems are one promising pathway to increase productivity by taking over labor-intensive manual tasks like fruit picking. To be effective, such systems need to monitor and interact with plants and fruits precisely, which is challenging due to the cluttered nature of agricultural environments causing, for example, strong occlusions. Thus, being able to estimate the complete 3D shapes of objects in presence of occlusions is crucial for automating operations such as fruit harvesting. In this paper, we propose the first publicly available 3D shape completion dataset for agricultural vision systems. We provide an RGB-D dataset for estimating the 3D shape of fruits. Specifically, our dataset contains RGB-D frames of single sweet peppers in lab conditions but also in a commercial greenhouse. For each fruit, we additionally collected high-precision point clouds that we use as ground truth. For acquiring the ground truth shape, we developed a measuring process that allows us to record data of real sweet pepper plants, both in the lab and in the greenhouse with high precision, and determine the shape of the sensed fruits. We release our dataset, consisting of almost 7,000 RGB-D frames belonging to more than 100 different fruits. We provide segmented RGB-D frames, with camera intrinsics to easily obtain colored point clouds, together with the corresponding high-precision, occlusion-free point clouds obtained with a high-precision laser scanner. We additionally enable evaluation of shape completion approaches on a hidden test set through a public challenge on a benchmark server.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13304v2</guid>
      <category>cs.CV</category>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Federico Magistri, Thomas L\"abe, Elias Marks, Sumanth Nagulavancha, Yue Pan, Claus Smitt, Lasse Klingbeil, Michael Halstead, Heiner Kuhlmann, Chris McCool, Jens Behley, Cyrill Stachniss</dc:creator>
    </item>
    <item>
      <title>RING#: PR-by-PE Global Localization with Roto-translation Equivariant Gram Learning</title>
      <link>https://arxiv.org/abs/2409.00206</link>
      <description>arXiv:2409.00206v2 Announce Type: replace-cross 
Abstract: Global localization using onboard perception sensors, such as cameras and LiDARs, is crucial in autonomous driving and robotics applications when GPS signals are unreliable. Most approaches achieve global localization by sequential place recognition (PR) and pose estimation (PE). Some methods train separate models for each task, while others employ a single model with dual heads, trained jointly with separate task-specific losses. However, the accuracy of localization heavily depends on the success of place recognition, which often fails in scenarios with significant changes in viewpoint or environmental appearance. Consequently, this renders the final pose estimation of localization ineffective. To address this, we introduce a new paradigm, PR-by-PE localization, which bypasses the need for separate place recognition by directly deriving it from pose estimation. We propose RING#, an end-to-end PR-by-PE localization network that operates in the bird's-eye-view (BEV) space, compatible with both vision and LiDAR sensors. RING# incorporates a novel design that learns two equivariant representations from BEV features, enabling globally convergent and computationally efficient pose estimation. Comprehensive experiments on the NCLT and Oxford datasets show that RING# outperforms state-of-the-art methods in both vision and LiDAR modalities, validating the effectiveness of the proposed approach. The code will be publicly released.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00206v2</guid>
      <category>cs.CV</category>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sha Lu, Xuecheng Xu, Yuxuan Wu, Haojian Lu, Xieyuanli Chen, Rong Xiong, Yue Wang</dc:creator>
    </item>
    <item>
      <title>Flight Testing of Latch Valve with Lightweight LV-Servo Direct Drive Mechanism</title>
      <link>https://arxiv.org/abs/2409.08825</link>
      <description>arXiv:2409.08825v3 Announce Type: replace-cross 
Abstract: In the field of rocket technology, the latch valve assumes a pivotal role in regulating the flow of fuel gases and liquids to ensure the requisite energy supply. This project endeavors to innovate by replacing the conventional step motor mechanism with a servo motor for latch valve control. The selected servo motor, boasting a more compact form factor and reduced mass, aligns seamlessly with the project's overarching objectives. While servo motors offer myriad advantages, it is imperative to acknowledge and address the constraints of their maximum output torque to guarantee the latch valve's reliable operation. Furthermore, as a rocket ascends, it encounters significant fluctuations in internal temperature and pressure. Consequently, rigorous environmental testing becomes paramount to validate the servo motor's performance under these dynamic conditions, thus ensuring the latch valve's unwavering functionality. The primary focus of this project is the design and testing of the mechanism's performance in simulated rocket environments, achieved through the implementation of the servo motor for latch valve control. The results reveal that the servo motor demonstrated its effectiveness and reliability in controlling the latch valve under the rigorous environmental conditions of rocket flight.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08825v3</guid>
      <category>eess.SY</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hao-Che Huang, Chih-Shin Chang, Jui-Cheng Hsu, Shih-Sin Wei</dc:creator>
    </item>
    <item>
      <title>GlobalMapNet: An Online Framework for Vectorized Global HD Map Construction</title>
      <link>https://arxiv.org/abs/2409.10063</link>
      <description>arXiv:2409.10063v2 Announce Type: replace-cross 
Abstract: High-definition (HD) maps are essential for autonomous driving systems. Traditionally, an expensive and labor-intensive pipeline is implemented to construct HD maps, which is limited in scalability. In recent years, crowdsourcing and online mapping have emerged as two alternative methods, but they have limitations respectively. In this paper, we provide a novel methodology, namely global map construction, to perform direct generation of vectorized global maps, combining the benefits of crowdsourcing and online mapping. We introduce GlobalMapNet, the first online framework for vectorized global HD map construction, which updates and utilizes a global map on the ego vehicle. To generate the global map from scratch, we propose GlobalMapBuilder to match and merge local maps continuously. We design a new algorithm, Map NMS, to remove duplicate map elements and produce a clean map. We also propose GlobalMapFusion to aggregate historical map information, improving consistency of prediction. We examine GlobalMapNet on two widely recognized datasets, Argoverse2 and nuScenes, showing that our framework is capable of generating globally consistent results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10063v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.RO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anqi Shi, Yuze Cai, Xiangyu Chen, Jian Pu, Zeyu Fu, Hong Lu</dc:creator>
    </item>
  </channel>
</rss>
