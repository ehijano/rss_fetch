<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 28 May 2024 04:01:40 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 28 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Development of a Virtual Reality Application for Oculomotor Examination Education Based on Student-Centered Pedagogy</title>
      <link>https://arxiv.org/abs/2405.16392</link>
      <description>arXiv:2405.16392v1 Announce Type: new 
Abstract: This work-in-progress paper discusses the use of student-centered pedagogy to teach clinical oculomotor examination via Virtual Reality (VR). Traditional methods, such as PowerPoint slides and lab activities, are often insufficient for providing hands-on experience due to the high cost of clinical equipment. To address this, a VR-based application was developed using Unity and the HTC Vive Pro headset, offering a cost-effective solution for practical learning. The VR app allows students to engage in oculomotor examinations at their own pace, accommodating diverse backgrounds and learning preferences. This application enables students to collect and analyze data, providing a realistic simulation of clinical practice. The user study results from Doctor of Physical Therapy students indicate a high preference for the flexibility offered by the VR app, suggesting its potential as a valuable educational tool. Additionally, the paper explores the broader implications of using VR in engineering and computing education, highlighting the benefits of immersive, interactive learning environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16392v1</guid>
      <category>cs.CE</category>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/FIE58773.2023.10343209</arxiv:DOI>
      <dc:creator>Austin Finlayson, Rui Wu, Chia-Cheng Lin, Brian Sylcott</dc:creator>
    </item>
    <item>
      <title>Hierarchical Rank-One Sequence Convexification for the Relaxation of Variational Problems with Microstructures</title>
      <link>https://arxiv.org/abs/2405.16866</link>
      <description>arXiv:2405.16866v1 Announce Type: new 
Abstract: This paper presents an efficient algorithm for the approximation of the rank-one convex hull in the context of nonlinear solid mechanics. It is based on hierarchical rank-one sequences and simultaneously provides first and second derivative information essential for the calculation of mechanical stresses and the computational minimization of discretized energies. For materials, whose microstructure can be well approximated in terms of laminates and where each laminate stage achieves energetic optimality with respect to the current stage, the approximate envelope coincides with the rank-one convex envelope. Although the proposed method provides only an upper bound for the rank-one convex hull, a careful examination of the resulting constraints shows a decent applicability in mechanical problems. Various aspects of the algorithm are discussed, including the restoration of rotational invariance, microstructure reconstruction, comparisons with other semi-convexification algorithms, and mesh independency. Overall, this paper demonstrates the efficiency of the algorithm for both, well-established mathematical benchmark problems as well as nonconvex isotropic finite-strain continuum damage models in two and three dimensions. Thereby, for the first time, a feasible concurrent numerical relaxation is established for an incremental, dissipative large-strain model with relevant applications in engineering problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16866v1</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maximilian K\"ohler, Timo Neumeier, Malte. A. Peter, Daniel Peterseim, Daniel Balzani</dc:creator>
    </item>
    <item>
      <title>Structural cohesive element for the modelling of delamination in composite laminates without the cohesive zone limit</title>
      <link>https://arxiv.org/abs/2405.17018</link>
      <description>arXiv:2405.17018v1 Announce Type: new 
Abstract: Delamination is a critical mode of failure that occurs between plies in a composite laminate. The cohesive element, developed based on the cohesive zone model, is widely used for modeling delamination. However, standard cohesive elements suffer from a well-known limit on the mesh density-the element size must be much smaller than the cohesive zone size. This work develops a new set of elements for modelling composite plies and their interfaces in 3D. A triangular Kirchhoff-Love shell element is developed for orthotropic materials to model the plies. A structural cohesive element, conforming to the shell elements of the plies, is developed to model the interface delamination. The proposed method is verified and validated on the classical benchmark problems of Mode I, Mode II, and mixed-mode delamination of unidirectional laminates, as well as on the single-leg bending problem of a multi-directional laminate. All the results show that the element size in the proposed models can be ten times larger than that in the standard cohesive element models, with more than 90% reduction in CPU time, while retaining prediction accuracy. This would then allow more effective and efficient modeling of delamination in composites without worrying about the cohesive zone limit on the mesh density.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17018v1</guid>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaopeng Ai, Boyang Chen, Christos Kassapoglou</dc:creator>
    </item>
    <item>
      <title>AIGB: Generative Auto-bidding via Diffusion Modeling</title>
      <link>https://arxiv.org/abs/2405.16141</link>
      <description>arXiv:2405.16141v1 Announce Type: cross 
Abstract: Auto-bidding plays a crucial role in facilitating online advertising by automatically providing bids for advertisers. Reinforcement learning (RL) has gained popularity for auto-bidding. However, most current RL auto-bidding methods are modeled through the Markovian Decision Process (MDP), which assumes the Markovian state transition. This assumption restricts the ability to perform in long horizon scenarios and makes the model unstable when dealing with highly random online advertising environments. To tackle this issue, this paper introduces AI-Generated Bidding (AIGB), a novel paradigm for auto-bidding through generative modeling. In this paradigm, we propose DiffBid, a conditional diffusion modeling approach for bid generation. DiffBid directly models the correlation between the return and the entire trajectory, effectively avoiding error propagation across time steps in long horizons. Additionally, DiffBid offers a versatile approach for generating trajectories that maximize given targets while adhering to specific constraints. Extensive experiments conducted on the real-world dataset and online A/B test on Alibaba advertising platform demonstrate the effectiveness of DiffBid, achieving 2.81% increase in GMV and 3.36% increase in ROI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16141v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiayan Guo, Yusen Huo, Zhilin Zhang, Tianyu Wang, Chuan Yu, Jian Xu, Yan Zhang, Bo Zheng</dc:creator>
    </item>
    <item>
      <title>Graph Neural PDE Solvers with Conservation and Similarity-Equivariance</title>
      <link>https://arxiv.org/abs/2405.16183</link>
      <description>arXiv:2405.16183v1 Announce Type: cross 
Abstract: Utilizing machine learning to address partial differential equations (PDEs) presents significant challenges due to the diversity of spatial domains and their corresponding state configurations, which complicates the task of encompassing all potential scenarios through data-driven methodologies alone. Moreover, there are legitimate concerns regarding the generalization and reliability of such approaches, as they often overlook inherent physical constraints. In response to these challenges, this study introduces a novel machine-learning architecture that is highly generalizable and adheres to conservation laws and physical symmetries, thereby ensuring greater reliability. The foundation of this architecture is graph neural networks (GNNs), which are adept at accommodating a variety of shapes and forms. Additionally, we explore the parallels between GNNs and traditional numerical solvers, facilitating a seamless integration of conservative principles and symmetries into machine learning models. Our findings from experiments demonstrate that the model's inclusion of physical laws significantly enhances its generalizability, i.e., no significant accuracy degradation for unseen spatial domains while other models degrade. The code is available at https://github.com/yellowshippo/fluxgnn-icml2024.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16183v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masanobu Horie, Naoto Mitsume</dc:creator>
    </item>
    <item>
      <title>DeTEcT: Dynamic and Probabilistic Parameters Extension</title>
      <link>https://arxiv.org/abs/2405.16688</link>
      <description>arXiv:2405.16688v1 Announce Type: cross 
Abstract: This paper presents a theoretical extension of the DeTEcT framework proposed by Sadykhov et al., DeTEcT, where a formal analysis framework was introduced for modelling wealth distribution in token economies. DeTEcT is a framework for analysing economic activity, simulating macroeconomic scenarios, and algorithmically setting policies in token economies. This paper proposes four ways of parametrizing the framework, where dynamic vs static parametrization is considered along with the probabilistic vs non-probabilistic. Using these parametrization techniques, we demonstrate that by adding restrictions to the framework it is possible to derive the existing wealth distribution models from DeTEcT. In addition to exploring parametrization techniques, this paper studies how money supply in DeTEcT framework can be transformed to become dynamic, and how this change will affect the dynamics of wealth distribution. The motivation for studying dynamic money supply is that it enables DeTEcT to be applied to modelling token economies without maximum supply (i.e., Ethereum), and it adds constraints to the framework in the form of symmetries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16688v1</guid>
      <category>q-fin.GN</category>
      <category>cs.CE</category>
      <category>q-fin.CP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rem Sadykhov, Geoffrey Goodell, Philip Treleaven</dc:creator>
    </item>
    <item>
      <title>Sparsity comparison of polytopal finite element methods</title>
      <link>https://arxiv.org/abs/2405.16864</link>
      <description>arXiv:2405.16864v1 Announce Type: cross 
Abstract: In this work we compare crucial parameters for efficiency of different finite element methods for solving partial differential equations (PDEs) on polytopal meshes. We consider the Virtual Element Method (VEM) and different Discontinuous Galerkin (DG) methods, namely the Hybrid DG and Trefftz DG methods. The VEM is a conforming method, that can be seen as a generalization of the classic finite element method to arbitrary polytopal meshes. DG methods are non-conforming methods that offer high flexibility, but also come with high computational costs. Hybridization reduces these costs by introducing additional facet variables, onto which the computational costs can be transfered to. Trefftz DG methods achieve a similar reduction in complexity by selecting a special and smaller set of basis functions on each element. The association of computational costs to different geometrical entities (elements or facets) leads to differences in the performance of these methods on different grid types. This paper aims to compare the dependency of these approaches across different grid configurations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16864v1</guid>
      <category>math.NA</category>
      <category>cs.CC</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christoph Lehrenfeld, Paul Stocker, Maximilian Zienecker</dc:creator>
    </item>
    <item>
      <title>Digitalization in Infrastructure Construction Projects: A PRISMA-Based Review of Benefits and Obstacles</title>
      <link>https://arxiv.org/abs/2405.16875</link>
      <description>arXiv:2405.16875v1 Announce Type: cross 
Abstract: The current study presents a comprehensive review of the benefits and barriers associated with the adoption of Building Information Modeling (BIM) in infrastructure projects, focusing on the period from 2013 to 2023. The research explores the manifold advantages offered by BIM, spanning the entire project life cycle, including planning, design, construction, maintenance, and sustainability. Notably, BIM enhances collaboration, facilitates real-time data-driven decision-making, and leads to substantial cost and time savings. In parallel, a systematic literature review was conducted to identify and categorize the barriers hindering BIM adoption within the infrastructure industry. Eleven studies were selected for in-depth analysis, revealing a total of 74 obstacles. Through synthetic analysis and thematic clustering, seven primary impediments to BIM adoption were identified, encompassing challenges related to education/training, resistance to change, business value clarity, perceived cost, lack of standards and guidelines, lack of mandates, and lack of initiatives. This review explores the benefits and barriers in the industry that are facing BIM adoption in infrastructure projects, giving an important perspective toward improving effective BIM adoption strategies, policies, and standards. Future directions for research and industry development are outlined, including efforts to enhance education and training, promote standardization, advocate for policy and mandates, and integrate BIM with emerging technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16875v1</guid>
      <category>cs.RO</category>
      <category>cs.CE</category>
      <category>cs.ET</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammed Abdulsalam Alsofiani</dc:creator>
    </item>
    <item>
      <title>Functional Programming Paradigm of Python for Scientific Computation Pipeline Integration</title>
      <link>https://arxiv.org/abs/2405.16956</link>
      <description>arXiv:2405.16956v1 Announce Type: cross 
Abstract: The advent of modern data processing has led to an increasing tendency towards interdisciplinarity, which frequently involves the importation of different technical approaches. Consequently, there is an urgent need for a unified data control system to facilitate the integration of varying libraries. This integration is of profound significance in accelerating prototype verification, optimising algorithm performance and minimising maintenance costs. This paper presents a novel functional programming (FP) paradigm based on the Python architecture and associated suites in programming practice, designed for the integration of pipelines of different data mapping operations. In particular, the solution is intended for the integration of scientific computation flows, which affords a robust yet flexible solution for the aforementioned challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16956v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.SE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chen Zhang, Lecheng Jia, Wei Zhang, Ning Wen</dc:creator>
    </item>
    <item>
      <title>Modeling the Evolutionary Trends in Corporate ESG Reporting: A Study based on Knowledge Management Model</title>
      <link>https://arxiv.org/abs/2309.07001</link>
      <description>arXiv:2309.07001v2 Announce Type: replace 
Abstract: Environmental, social, and governance (ESG) reports are globally recognized as a keystone in sustainable enterprise development. However, current literature has not concluded the development of topics and trends in ESG contexts in the twenty-first century. Therefore, We selected 1114 ESG reports from firms in the technology industry to analyze the evolutionary trends of ESG topics by text mining. We discovered the homogenization effect towards low environmental, medium governance, and high social features in the evolution. We also designed a strategic framework to look closer into the dynamic changes of firms' within-industry scores and across-domain importances. We found that companies are gradually converging towards the third quadrant, which indicates that firms contribute less to industrial outstanding and professional distinctiveness in ESG reporting. Firms choose to imitate ESG reports from each other to mitigate uncertainty and enhance behavioral legitimacy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.07001v2</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ziyuan Xia, Anchen Sun, Xiaodong Cai, Saixing Zeng</dc:creator>
    </item>
    <item>
      <title>Enhancing Smart Grids with Internet of Energy: Deep Reinforcement Learning and Convolutional Neural Network</title>
      <link>https://arxiv.org/abs/2405.13831</link>
      <description>arXiv:2405.13831v2 Announce Type: replace 
Abstract: The increasing demand for electricity, coupled with the rise in greenhouse gas emissions, necessitates the integration of Renewable Energy Sources (RESs) into power grids. However, the fluctuating nature of RESs introduces new challenges in energy management. The Internet of Energy (IoE) framework provides a solution by enabling real-time monitoring, dynamic scheduling, and enhanced energy routing. This paper proposes a comprehensive approach to optimizing energy management in smart grids using Deep Reinforcement Learning (DRL) and Convolutional Neural Networks (CNN). The research focuses on three main objectives: optimizing operation scheduling, improving energy routing, and enhancing cyber-physical security. A DRL-based scheduling algorithm is developed to manage energy components effectively, while an optimized energy routing algorithm ensures efficient electricity flow. Additionally, a security framework utilizing Long Short-Term Memory (LSTM) and CNN is proposed to detect False Data Injection (FDI) attacks and electricity theft. The proposed methods aim to improve energy efficiency, reduce costs, and ensure the security of IoE-enabled power systems. This research bridges existing gaps by addressing the dynamic and complex nature of modern energy networks. The integration of these advanced technologies promises significant advancements in the reliability and efficiency of smart grids. Ultimately, this work contributes to the development of a sustainable and secure energy future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13831v2</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ali Mohammadi Ruzbahani</dc:creator>
    </item>
    <item>
      <title>DiffAug: Enhance Unsupervised Contrastive Learning with Domain-Knowledge-Free Diffusion-based Data Augmentation</title>
      <link>https://arxiv.org/abs/2309.07909</link>
      <description>arXiv:2309.07909v2 Announce Type: replace-cross 
Abstract: Unsupervised Contrastive learning has gained prominence in fields such as vision, and biology, leveraging predefined positive/negative samples for representation learning. Data augmentation, categorized into hand-designed and model-based methods, has been identified as a crucial component for enhancing contrastive learning. However, hand-designed methods require human expertise in domain-specific data while sometimes distorting the meaning of the data. In contrast, generative model-based approaches usually require supervised or large-scale external data, which has become a bottleneck constraining model training in many domains. To address the problems presented above, this paper proposes DiffAug, a novel unsupervised contrastive learning technique with diffusion mode-based positive data generation. DiffAug consists of a semantic encoder and a conditional diffusion model; the conditional diffusion model generates new positive samples conditioned on the semantic encoding to serve the training of unsupervised contrast learning. With the help of iterative training of the semantic encoder and diffusion model, DiffAug improves the representation ability in an uninterrupted and unsupervised manner. Experimental evaluations show that DiffAug outperforms hand-designed and SOTA model-based augmentation methods on DNA sequence, visual, and bio-feature datasets. The code for review is released at \url{https://github.com/zangzelin/code_diffaug}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.07909v2</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>cs.CV</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zelin Zang, Hao Luo, Kai Wang, Panpan Zhang, Fan Wang, Stan. Z Li, Yang You</dc:creator>
    </item>
    <item>
      <title>Robust Radiotherapy Planning with Spatially Based Uncertainty Sets</title>
      <link>https://arxiv.org/abs/2402.17040</link>
      <description>arXiv:2402.17040v2 Announce Type: replace-cross 
Abstract: Radiotherapy treatment planning is a challenging large-scale optimization problem plagued by uncertainty. Following the robust optimization methodology, we propose a novel, spatially based uncertainty set for robust modeling of radiotherapy planning, producing solutions that are immune to unexpected changes in biological conditions. Our proposed uncertainty set realistically captures biological radiosensitivity patterns that are observed using recent advances in imaging, while its parameters can be personalized for individual patients. We exploit the structure of this set to devise a compact reformulation of the robust model. We develop a row-generation scheme to solve real, large-scale instances of the robust model. This method is then extended to a relaxation-based scheme for enforcing challenging, yet clinically important, dose-volume cardinality constraints. The computational performance of our algorithms, as well as the quality and robustness of the computed treatment plans, are demonstrated on simulated and real imaging data. Based on accepted performance measures, such as minimal target dose and homogeneity, these examples demonstrate that the spatially robust model achieves almost the same performance as the nominal model in the nominal scenario, and otherwise, the spatial model outperforms both the nominal and the box-uncertainty models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.17040v2</guid>
      <category>math.OC</category>
      <category>cs.CE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Noam Goldberg, Mark P. Langer, Shimrit Shtern</dc:creator>
    </item>
    <item>
      <title>A mechanism-driven reinforcement learning framework for shape optimization of airfoils</title>
      <link>https://arxiv.org/abs/2403.04329</link>
      <description>arXiv:2403.04329v2 Announce Type: replace-cross 
Abstract: In this paper, a novel mechanism-driven reinforcement learning framework is proposed for airfoil shape optimization. To validate the framework, a reward function is designed and analyzed, from which the equivalence between the maximizing the cumulative reward and achieving the optimization objectives is guaranteed theoretically. To establish a quality exploration, and to obtain an accurate reward from the environment, an efficient solver for steady Euler equations is employed in the reinforcement learning method. The solver utilizes the B\'ezier curve to describe the shape of the airfoil, and a Newton-geometric multigrid method for the solution. In particular, a dual-weighted residual-based h-adaptive method is used for efficient calculation of target functional. To effectively streamline the airfoil shape during the deformation process, we introduce the Laplacian smoothing, and propose a B\'ezier fitting strategy, which not only remits mesh tangling but also guarantees a precise manipulation of the geometry. In addition, a neural network architecture is designed based on an attention mechanism to make the learning process more sensitive to the minor change of the airfoil geometry. Numerical experiments demonstrate that our framework can handle the optimization problem with hundreds of design variables. It is worth mentioning that, prior to this work, there are limited works combining such high-fidelity partial differential equatons framework with advanced reinforcement learning algorithms for design problems with such high dimensionality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04329v2</guid>
      <category>math.NA</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingfeng Wang, Guanghui Hu</dc:creator>
    </item>
    <item>
      <title>Design Editing for Offline Model-based Optimization</title>
      <link>https://arxiv.org/abs/2405.13964</link>
      <description>arXiv:2405.13964v2 Announce Type: replace-cross 
Abstract: Offline model-based optimization (MBO) aims to maximize a black-box objective function using only an offline dataset of designs and scores. A prevalent approach involves training a conditional generative model on existing designs and their associated scores, followed by the generation of new designs conditioned on higher target scores. However, these newly generated designs often underperform due to the lack of high-scoring training data. To address this challenge, we introduce a novel method, Design Editing for Offline Model-based Optimization (DEMO), which consists of two phases. In the first phase, termed pseudo-target distribution generation, we apply gradient ascent on the offline dataset using a trained surrogate model, producing a synthetic dataset where the predicted scores serve as new labels. A conditional diffusion model is subsequently trained on this synthetic dataset to capture a pseudo-target distribution, which enhances the accuracy of the conditional diffusion model in generating higher-scoring designs. Nevertheless, the pseudo-target distribution is susceptible to noise stemming from inaccuracies in the surrogate model, consequently predisposing the conditional diffusion model to generate suboptimal designs. We hence propose the second phase, existing design editing, to directly incorporate the high-scoring features from the offline dataset into design generation. In this phase, top designs from the offline dataset are edited by introducing noise, which are subsequently refined using the conditional diffusion model to produce high-scoring designs. Overall, high-scoring designs begin with inheriting high-scoring features from the second phase and are further refined with a more accurate conditional diffusion model in the first phase. Empirical evaluations on 7 offline MBO tasks show that DEMO outperforms various baseline methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13964v2</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ye Yuan, Youyuan Zhang, Can Chen, Haolun Wu, Zixuan Li, Jianmo Li, James J. Clark, Xue Liu</dc:creator>
    </item>
  </channel>
</rss>
