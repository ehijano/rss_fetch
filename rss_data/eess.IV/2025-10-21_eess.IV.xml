<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>eess.IV updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/eess.IV</link>
    <description>eess.IV updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/eess.IV" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 21 Oct 2025 04:00:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Lung Cancer Classification from CT Images Using ResNet</title>
      <link>https://arxiv.org/abs/2510.16310</link>
      <description>arXiv:2510.16310v1 Announce Type: new 
Abstract: Lung cancer, a malignancy originating in lung tissues, is commonly diagnosed and classified using medical imaging techniques, particularly computed tomography (CT). Despite the integration of machine learning and deep learning methods, the predictive efficacy of automated systems for lung cancer classification from CT images remains below the desired threshold for clinical adoption. Existing research predominantly focuses on binary classification, distinguishing between malignant and benign lung nodules. In this study, a novel deep learning-based approach is introduced, aimed at an improved multi-class classification, discerning various subtypes of lung cancer from CT images. Leveraging a pre-trained ResNet model, lung tissue images were classified into three distinct classes, two of which denote malignancy and one benign. Employing a dataset comprising 15,000 lung CT images sourced from the LC25000 histopathological images, the ResNet50 model was trained on 10,200 images, validated on 2,550 images, and tested on the remaining 2,250 images. Through the incorporation of custom layers atop the ResNet architecture and meticulous hyperparameter fine-tuning, a remarkable test accuracy of 98.8% was recorded. This represents a notable enhancement over the performance of prior models on the same dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16310v1</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Olajumoke O. Adekunle, Joseph D. Akinyemi, Khadijat T. Ladoja, Olufade F. W. Onifade</dc:creator>
    </item>
    <item>
      <title>Time-Embedded Algorithm Unrolling for Computational MRI</title>
      <link>https://arxiv.org/abs/2510.16321</link>
      <description>arXiv:2510.16321v1 Announce Type: new 
Abstract: Algorithm unrolling methods have proven powerful for solving the regularized least squares problem in computational magnetic resonance imaging (MRI). These approaches unfold an iterative algorithm with a fixed number of iterations, typically alternating between a neural network-based proximal operator for regularization, a data fidelity operation and auxiliary updates with learnable parameters. While the connection to optimization methods dictate that the proximal operator network should be shared across unrolls, this can introduce artifacts or blurring. Heuristically, practitioners have shown that using distinct networks may be beneficial, but this significantly increases the number of learnable parameters, making it challenging to prevent overfitting. To address these shortcomings, by taking inspirations from proximal operators with varying thresholds in approximate message passing (AMP) and the success of time-embedding in diffusion models, we propose a time-embedded algorithm unrolling scheme for inverse problems. Specifically, we introduce a novel perspective on the iteration-dependent proximal operation in vector AMP (VAMP) and the subsequent Onsager correction in the context of algorithm unrolling, framing them as a time-embedded neural network. Similarly, the scalar weights in the data fidelity operation and its associated Onsager correction are cast as time-dependent learnable parameters. Our extensive experiments on the fastMRI dataset, spanning various acceleration rates and datasets, demonstrate that our method effectively reduces aliasing artifacts and mitigates noise amplification, achieving state-of-the-art performance. Furthermore, we show that our time-embedding strategy extends to existing algorithm unrolling approaches, enhancing reconstruction quality without increasing the computational complexity significantly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16321v1</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>physics.med-ph</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Junno Yun, Ya\c{s}ar Utku Al\c{c}alar, Mehmet Ak\c{c}akaya</dc:creator>
    </item>
    <item>
      <title>Computer Navigated Spinal Surgery Using Magnetic Resonance Imaging and Augmented Reality</title>
      <link>https://arxiv.org/abs/2510.16347</link>
      <description>arXiv:2510.16347v1 Announce Type: new 
Abstract: Current spinal pain management procedures, such as radiofrequency ablation (RFA) and epidural steroid injection (ESI), rely on fluoroscopy for needle placement which exposes patients and physicians to ionizing radiation. In this paper, we investigate a radiation-free surgical navigation system for spinal pain management procedures that combines magnetic resonance imaging (MRI) with fiducial ArUco marker-based augmented reality (AR). High-resolution MRI scans of a lumbar spinal phantom were obtained and assembled as a surface mesh. Laplacian smoothing algorithms were then applied to smoothen the surface and improve the model fidelity. A commercially available stereo camera (ZED2) was used to track single or dual fiducial ArUco markers on the patient to determine the patient's real-time pose. Custom AR software was applied to overlay the MRI image onto the patient, allowing the physician to see not only the outer surface of the patient but also the complete anatomy of the patient below the surface. Needle-insertion trials on a 3D-printed 3-vertebra phantom showed that dual-ArUco marker tracking increased the accuracy of needle insertions and reduced the average needle misplacement distance compared to single-ArUco marker procedures. The average needle misplacement is comparable to the average deviation of 2 mm for conventional epidural techniques using fluoroscopy. Our radiation-free system demonstrates promise to serve as an alternative to fluoroscopy by improving image-guided spinal navigation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16347v1</guid>
      <category>eess.IV</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Songyuan Lu, Jingwen Hui, Jake Weeks, David B. Berry, Fanny Chapelin, Frank Talke</dc:creator>
    </item>
    <item>
      <title>FSAR-Cap: A Fine-Grained Two-Stage Annotated Dataset for SAR Image Captioning</title>
      <link>https://arxiv.org/abs/2510.16394</link>
      <description>arXiv:2510.16394v1 Announce Type: new 
Abstract: Synthetic Aperture Radar (SAR) image captioning enables scene-level semantic understanding and plays a crucial role in applications such as military intelligence and urban planning, but its development is limited by the scarcity of high-quality datasets. To address this, we present FSAR-Cap, a large-scale SAR captioning dataset with 14,480 images and 72,400 image-text pairs. FSAR-Cap is built on the FAIR-CSAR detection dataset and constructed through a two-stage annotation strategy that combines hierarchical template-based representation, manual verification and supplementation, prompt standardization. Compared with existing resources, FSAR-Cap provides richer fine-grained annotations, broader category coverage, and higher annotation quality. Benchmarking with multiple encoder-decoder architectures verifies its effectiveness, establishing a foundation for future research in SAR captioning and intelligent image interpretation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16394v1</guid>
      <category>eess.IV</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinqi Zhang, Lamei Zhang, Bin Zou</dc:creator>
    </item>
    <item>
      <title>Dictionary-Based Deblurring for Unpaired Data</title>
      <link>https://arxiv.org/abs/2510.16428</link>
      <description>arXiv:2510.16428v1 Announce Type: new 
Abstract: Effective image deblurring typically relies on large and fully paired datasets of blurred and corresponding sharp images. However, obtaining such accurately aligned data in the real world poses a number of difficulties, limiting the effectiveness and generalizability of existing deblurring methods. To address this scarcity of data dependency, we present a novel dictionary learning based deblurring approach for jointly estimating a structured blur matrix and a high resolution image dictionary. This framework enables robust image deblurring across different degrees of data supervision. Our method is thoroughly evaluated across three distinct experimental settings: (i) full supervision involving paired data with explicit correspondence, (ii) partial supervision employing unpaired data with implicit relationships, and (iii) unsupervised learning using non-correspondence data where direct pairings are absent. Extensive experimental validation, performed on synthetically blurred subsets of the CMU-Cornell iCoseg dataset and the real-world FocusPath dataset, consistently shows that the proposed framework has superior performance compared to conventional coupled dictionary learning approaches. The results validate that our approach provides an efficient and robust solution for image deblurring in data-constrained scenarios by enabling accurate blur modeling and adaptive dictionary representation with a notably smaller number of training samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16428v1</guid>
      <category>eess.IV</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Alok Panigrahi, Jayaprakash Katual, Satish Mulleti</dc:creator>
    </item>
    <item>
      <title>A Low-Complexity View Synthesis Distortion Estimation Method for 3D Video with Large Baseline Considerations</title>
      <link>https://arxiv.org/abs/2510.17037</link>
      <description>arXiv:2510.17037v1 Announce Type: new 
Abstract: Depth-image-based rendering is a key view synthesis algorithm in 3D video systems, which enables the synthesis of virtual views from texture images and depth maps. An efficient view synthesis distortion estimation model is critical for optimizing resource allocation in real-time applications such as interactive free-viewpoint video and 3D video streaming services. However, existing estimation methods are often computationally intensive, require parameter training, or performance poorly in challenging large baseline configurations. This paper presents a novel, low-complexity, and training-free method to accurately estimate the distortion of synthesized views without performing the actual rendering process. Key contributions include: (1) A joint texture-depth classification method that accurately separates texture image into locally stationary and non-stationary regions, which mitigates misclassifications by using texture-only methods. (2) A novel baseline distance indicator is designed for the compensation scheme for distortions caused by large baseline configurations. (3) A region-based blending estimation strategy that geometrically identifies overlapping, single-view, and mutual disocclusion regions, predicting distortion in synthesized views from two reference views with differing synthesis conditions. Experiments on standard MPEG 3D video sequences validate the proposed method's high accuracy and efficiency, especially in large baseline configurations. This method enables more flexible camera arrangements in 3D content acquisition by accurately predicting synthesis quality under challenging geometric configurations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17037v1</guid>
      <category>eess.IV</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chongyuan Bi, Jie Liang</dc:creator>
    </item>
    <item>
      <title>AV1 Motion Vector Fidelity and Application for Efficient Optical Flow</title>
      <link>https://arxiv.org/abs/2510.17427</link>
      <description>arXiv:2510.17427v1 Announce Type: new 
Abstract: This paper presents a comprehensive analysis of motion vectors extracted from AV1-encoded video streams and their application in accelerating optical flow estimation. We demonstrate that motion vectors from AV1 video codec can serve as a high-quality and computationally efficient substitute for traditional optical flow, a critical but often resource-intensive component in many computer vision pipelines. Our primary contributions are twofold. First, we provide a detailed comparison of motion vectors from both AV1 and HEVC against ground-truth optical flow, establishing their fidelity. In particular we show the impact of encoder settings on motion estimation fidelity and make recommendations about the optimal settings. Second, we show that using these extracted AV1 motion vectors as a "warm-start" for a state-of-the-art deep learning-based optical flow method, RAFT, significantly reduces the time to convergence while achieving comparable accuracy. Specifically, we observe a four-fold speedup in computation time with only a minor trade- off in end-point error. These findings underscore the potential of reusing motion vectors from compressed video as a practical and efficient method for a wide range of motion-aware computer vision applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17427v1</guid>
      <category>eess.IV</category>
      <category>cs.MM</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julien Zouein, Vibhoothi Vibhoothi, Anil Kokaram</dc:creator>
    </item>
    <item>
      <title>Segmenting infant brains across magnetic fields: Domain randomization and annotation curation in ultra-low field MRI</title>
      <link>https://arxiv.org/abs/2510.17436</link>
      <description>arXiv:2510.17436v1 Announce Type: new 
Abstract: Early identification of neurodevelopmental disorders relies on accurate segmentation of brain structures in infancy, a task complicated by rapid brain growth, poor tissue contrast, and motion artifacts in pediatric MRI. These challenges are further exacerbated in ultra-low-field (ULF, 0.064~T) MRI, which, despite its lower image quality, offers an affordable, portable, and sedation-free alternative for use in low-resource settings. In this work, we propose a domain randomization (DR) framework to bridge the domain gap between high-field (HF) and ULF MRI in the context of the hippocampi and basal ganglia segmentation in the LISA challenge. We show that pre-training on whole-brain HF segmentations using DR significantly improves generalization to ULF data, and that careful curation of training labels, by removing misregistered HF-to-ULF annotations from training, further boosts performance. By fusing the predictions of several models through majority voting, we are able to achieve competitive performance. Our results demonstrate that combining robust augmentation with annotation quality control can enable accurate segmentation in ULF data. Our code is available at https://github.com/Medical-Image-Analysis-Laboratory/lisasegm</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17436v1</guid>
      <category>eess.IV</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vladyslav Zalevskyi, Dondu-Busra Bulut, Thomas Sanchez, Meritxell Bach Cuadra</dc:creator>
    </item>
    <item>
      <title>NVM-in-Cache: Repurposing Commodity 6T SRAM Cache into NVM Analog Processing-in-Memory Engine using a Novel Compute-on-Powerline Scheme</title>
      <link>https://arxiv.org/abs/2510.15904</link>
      <description>arXiv:2510.15904v1 Announce Type: cross 
Abstract: The rapid growth of deep neural network (DNN) workloads has significantly increased the demand for large-capacity on-chip SRAM in machine learning (ML) applications, with SRAM arrays now occupying a substantial fraction of the total die area. To address the dual challenges of storage density and computation efficiency, this paper proposes an NVM-in-Cache architecture that integrates resistive RAM (RRAM) devices into a conventional 6T-SRAM cell, forming a compact 6T-2R bit-cell. This hybrid cell enables Processing-in-Memory (PIM) mode, which performs massively parallel multiply-and-accumulate (MAC) operations directly on cache power lines while preserving stored cache data. By exploiting the intrinsic properties of the 6T-2R structure, the architecture achieves additional storage capability, high computational throughput without any bit-cell area overhead. Circuit- and array-level simulations in GlobalFoundries 22nm FDSOI technology demonstrate that the proposed design achieves a throughput of 0.4 TOPS and 491.78 TOPS/W. For 128 row-parallel operations, the CIFAR-10 classification is demonstrated by mapping a Resnet-18 neural network, achieving an accuracy of 91.27%. These results highlight the potential of the NVM-in-Cache approach to serve as a scalable, energy-efficient computing method by re-purposing existing 6T SRAM cache architecture for next-generation AI accelerators and general purpose processors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15904v1</guid>
      <category>cs.AR</category>
      <category>cs.SY</category>
      <category>eess.IV</category>
      <category>eess.SY</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subhradip Chakraborty, Ankur Singh, Xuming Chen, Gourav Datta, Akhilesh R. Jaiswal</dc:creator>
    </item>
    <item>
      <title>Effect of Reporting Mode and Clinical Experience on Radiologists' Gaze and Image Analysis Behavior in Chest Radiography</title>
      <link>https://arxiv.org/abs/2510.16070</link>
      <description>arXiv:2510.16070v1 Announce Type: cross 
Abstract: Structured reporting (SR) and artificial intelligence (AI) may transform how radiologists interact with imaging studies. This prospective study (July to December 2024) evaluated the impact of three reporting modes: free-text (FT), structured reporting (SR), and AI-assisted structured reporting (AI-SR), on image analysis behavior, diagnostic accuracy, efficiency, and user experience. Four novice and four non-novice readers (radiologists and medical students) each analyzed 35 bedside chest radiographs per session using a customized viewer and an eye-tracking system. Outcomes included diagnostic accuracy (compared with expert consensus using Cohen's $\kappa$), reporting time per radiograph, eye-tracking metrics, and questionnaire-based user experience. Statistical analysis used generalized linear mixed models with Bonferroni post-hoc tests with a significance level of ($P \le .01$). Diagnostic accuracy was similar in FT ($\kappa = 0.58$) and SR ($\kappa = 0.60$) but higher in AI-SR ($\kappa = 0.71$, $P &lt; .001$). Reporting times decreased from $88 \pm 38$ s (FT) to $37 \pm 18$ s (SR) and $25 \pm 9$ s (AI-SR) ($P &lt; .001$). Saccade counts for the radiograph field ($205 \pm 135$ (FT), $123 \pm 88$ (SR), $97 \pm 58$ (AI-SR)) and total fixation duration for the report field ($11 \pm 5$ s (FT), $5 \pm 3$ s (SR), $4 \pm 1$ s (AI-SR)) were lower with SR and AI-SR ($P &lt; .001$ each). Novice readers shifted gaze towards the radiograph in SR, while non-novice readers maintained their focus on the radiograph. AI-SR was the preferred mode. In conclusion, SR improves efficiency by guiding visual attention toward the image, and AI-prefilled SR further enhances diagnostic accuracy and user satisfaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16070v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>eess.IV</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mahta Khoobi, Marc Sebastian von der Stueck, Felix Barajas Ordonez, Anca-Maria Iancu, Eric Corban, Julia Nowak, Aleksandar Kargaliev, Valeria Perelygina, Anna-Sophie Schott, Daniel Pinto dos Santos, Christiane Kuhl, Daniel Truhn, Sven Nebelung, Robert Siepmann</dc:creator>
    </item>
    <item>
      <title>Towards Smart Manufacturing Metaverse via Digital Twinning in Extended Reality</title>
      <link>https://arxiv.org/abs/2510.16280</link>
      <description>arXiv:2510.16280v1 Announce Type: cross 
Abstract: The rapid evolution of modern manufacturing systems is driven by the integration of emerging metaverse technologies such as artificial intelligence (AI), digital twin (DT) with different forms of extended reality (XR) like virtual reality (VR), augmented reality (AR), and mixed reality (MR). These advances confront manufacturing workers with complex and evolving environments that demand digital literacy for problem solving in the future workplace. However, manufacturing industry faces a critical shortage of skilled workforce with digital literacy in the world. Further, global pandemic has significantly changed how people work and collaborate digitally and remotely. There is an urgent need to rethink digital platformization and leverage emerging technologies to propel industrial evolution toward human-centered manufacturing metaverse (MfgVerse). This paper presents a forward-looking perspective on the development of smart MfgVerse, highlighting current efforts in learning factory, cognitive digital twinning, and the new sharing economy of manufacturing-as-a-service (MaaS). MfgVerse is converging into multiplex networks, including a social network of human stakeholders, an interconnected network of manufacturing things or agents (e.g., machines, robots, facilities, material handling systems), a network of digital twins of physical things, as well as auxiliary networks of sales, supply chain, logistics, and remanufacturing systems. We also showcase the design and development of a learning factory for workforce training in extended reality. Finally, future directions, challenges, and opportunities are discussed for human-centered manufacturing metaverse. We hope this work helps stimulate more comprehensive studies and in-depth research efforts to advance MfgVerse technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16280v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>eess.IV</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hui Yang, Faisal Aqlan, Richard Zhao</dc:creator>
    </item>
    <item>
      <title>RefAtomNet++: Advancing Referring Atomic Video Action Recognition using Semantic Retrieval based Multi-Trajectory Mamba</title>
      <link>https://arxiv.org/abs/2510.16444</link>
      <description>arXiv:2510.16444v1 Announce Type: cross 
Abstract: Referring Atomic Video Action Recognition (RAVAR) aims to recognize fine-grained, atomic-level actions of a specific person of interest conditioned on natural language descriptions. Distinct from conventional action recognition and detection tasks, RAVAR emphasizes precise language-guided action understanding, which is particularly critical for interactive human action analysis in complex multi-person scenarios. In this work, we extend our previously introduced RefAVA dataset to RefAVA++, which comprises &gt;2.9 million frames and &gt;75.1k annotated persons in total. We benchmark this dataset using baselines from multiple related domains, including atomic action localization, video question answering, and text-video retrieval, as well as our earlier model, RefAtomNet. Although RefAtomNet surpasses other baselines by incorporating agent attention to highlight salient features, its ability to align and retrieve cross-modal information remains limited, leading to suboptimal performance in localizing the target person and predicting fine-grained actions. To overcome the aforementioned limitations, we introduce RefAtomNet++, a novel framework that advances cross-modal token aggregation through a multi-hierarchical semantic-aligned cross-attention mechanism combined with multi-trajectory Mamba modeling at the partial-keyword, scene-attribute, and holistic-sentence levels. In particular, scanning trajectories are constructed by dynamically selecting the nearest visual spatial tokens at each timestep for both partial-keyword and scene-attribute levels. Moreover, we design a multi-hierarchical semantic-aligned cross-attention strategy, enabling more effective aggregation of spatial and temporal tokens across different semantic hierarchies. Experiments show that RefAtomNet++ establishes new state-of-the-art results. The dataset and code are released at https://github.com/KPeng9510/refAVA2.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16444v1</guid>
      <category>cs.CV</category>
      <category>cs.MM</category>
      <category>cs.RO</category>
      <category>eess.IV</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kunyu Peng, Di Wen, Jia Fu, Jiamin Wu, Kailun Yang, Junwei Zheng, Ruiping Liu, Yufan Chen, Yuqian Fu, Danda Pani Paudel, Luc Van Gool, Rainer Stiefelhagen</dc:creator>
    </item>
    <item>
      <title>A Versatile Framework for Designing Group-Sparse Adversarial Attacks</title>
      <link>https://arxiv.org/abs/2510.16637</link>
      <description>arXiv:2510.16637v1 Announce Type: cross 
Abstract: Existing adversarial attacks often neglect perturbation sparsity, limiting their ability to model structural changes and to explain how deep neural networks (DNNs) process meaningful input patterns. We propose ATOS (Attack Through Overlapping Sparsity), a differentiable optimization framework that generates structured, sparse adversarial perturbations in element-wise, pixel-wise, and group-wise forms. For white-box attacks on image classifiers, we introduce the Overlapping Smoothed L0 (OSL0) function, which promotes convergence to a stationary point while encouraging sparse, structured perturbations. By grouping channels and adjacent pixels, ATOS improves interpretability and helps identify robust versus non-robust features. We approximate the L-infinity gradient using the logarithm of the sum of exponential absolute values to tightly control perturbation magnitude. On CIFAR-10 and ImageNet, ATOS achieves a 100% attack success rate while producing significantly sparser and more structurally coherent perturbations than prior methods. The structured group-wise attack highlights critical regions from the network's perspective, providing counterfactual explanations by replacing class-defining regions with robust features from the target class.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16637v1</guid>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alireza Heshmati, Saman Soleimani Roudi, Sajjad Amini, Shahrokh Ghaemmaghami, Farokh Marvasti</dc:creator>
    </item>
    <item>
      <title>Person Re-Identification via Generalized Class Prototypes</title>
      <link>https://arxiv.org/abs/2510.17043</link>
      <description>arXiv:2510.17043v1 Announce Type: cross 
Abstract: Advanced feature extraction methods have significantly contributed to enhancing the task of person re-identification. In addition, modifications to objective functions have been developed to further improve performance. Nonetheless, selecting better class representatives is an underexplored area of research that can also lead to advancements in re-identification performance. Although past works have experimented with using the centroid of a gallery image class during training, only a few have investigated alternative representations during the retrieval stage. In this paper, we demonstrate that these prior techniques yield suboptimal results in terms of re-identification metrics. To address the re-identification problem, we propose a generalized selection method that involves choosing representations that are not limited to class centroids. Our approach strikes a balance between accuracy and mean average precision, leading to improvements beyond the state of the art. For example, the actual number of representations per class can be adjusted to meet specific application requirements. We apply our methodology on top of multiple re-identification embeddings, and in all cases it substantially improves upon contemporary results</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17043v1</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Ahmed Al Muzaddid, William J. Beksi</dc:creator>
    </item>
    <item>
      <title>AutoLungDx: A Hybrid Deep Learning Approach for Early Lung Cancer Diagnosis Using 3D Res-U-Net, YOLOv5, and Vision Transformers</title>
      <link>https://arxiv.org/abs/2305.00046</link>
      <description>arXiv:2305.00046v4 Announce Type: replace 
Abstract: Lung cancer is a leading cause of cancer-related deaths worldwide, and early detection is crucial for improving patient outcomes. Nevertheless, early diagnosis of cancer is a major challenge, particularly in low-resource settings where access to medical resources and trained radiologists is limited. The objective of this study is to propose an automated end-to-end deep learning-based framework for the early detection and classification of lung nodules, specifically for low-resource settings. The proposed framework consists of three stages: lung segmentation using a modified 3D U-Net named 3D Res-U-Net, nodule detection using YOLO-v5, and classification with a Vision Transformer-based architecture. We evaluated the proposed framework on a publicly available dataset, LUNA16. The proposed framework's performance was measured using the respective domain's evaluation matrices. The proposed framework achieved a 98.82% lung segmentation dice score while detecting the lung nodule with 0.76 mAP@50 from the segmented lung, at a low false-positive rate. The performance of both networks of the proposed framework was compared with other studies and found to outperform them regarding segmentation and detection accuracy. Additionally, our proposed Vision transformer network obtained an accuracy of 93.57%, which is 1.21% higher than the state-of-the-art networks. Our proposed end-to-end deep learning-based framework can effectively segment lungs, and detect and classify lung nodules, specifically in low-resource settings with limited access to radiologists. The proposed framework outperforms existing studies regarding all the respective evaluation metrics. The proposed framework can potentially improve the accuracy and efficiency of lung cancer screening in low-resource settings, ultimately leading to better patient outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.00046v4</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samiul Based Shuvo, Tasnia Binte Mamun</dc:creator>
    </item>
    <item>
      <title>Predicting Patient Recovery or Mortality Using Deep Neural Decision Tree and Forest</title>
      <link>https://arxiv.org/abs/2311.13925</link>
      <description>arXiv:2311.13925v4 Announce Type: replace 
Abstract: Objective: Identifying patients at high risk of mortality is crucial for emergency physicians to allocate hospital resources effectively, particularly in regions with limited medical services. This need becomes even more pressing during global health crises that lead to significant morbidity and mortality. This study aimed to present the usability deep neural decision forest and deep neural decision tree to predict mortality among Coronavirus disease 2019 (COVID-19) patients. To this end, We used patient data encompassing Coronavirus disease 2019 diagnosis, demographics, health indicators, and occupational risk factors to analyze disease severity and outcomes. The dataset was partitioned using a stratified sampling method, ensuring that 80% was allocated for training and 20% for testing. Nine machine learning and deep learning methods were employed to build predictive models. The models were evaluated across all stages to determine their effectiveness in predicting patient outcomes. Results: Among the models, the deep neural decision forest consistently outperformed others. Results indicated that using only clinical data yielded an accuracy of 80% by deep neural decision forest, demonstrating it as a reliable predictor of patient mortality. Moreover, the results suggest that clinical data alone may be the most accurate diagnostic tool for predicting mortality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.13925v4</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mohammad Dehghani, Mohadeseh Zarei Ghobadi, Mobin Mohammadi, Diyana Tehrany Dehkordy</dc:creator>
    </item>
    <item>
      <title>Adaptive Convolutional Neural Network for Image Super-resolution</title>
      <link>https://arxiv.org/abs/2402.15704</link>
      <description>arXiv:2402.15704v5 Announce Type: replace 
Abstract: Convolutional neural networks can automatically learn features via deep network architectures and given input samples. However, the robustness of obtained models may face challenges in varying scenes. Bigger differences in network architecture are beneficial to extract more diversified structural information to strengthen the robustness of an obtained super-resolution model. In this paper, we proposed a adaptive convolutional neural network for image super-resolution (ADSRNet). To capture more information, ADSRNet is implemented by a heterogeneous parallel network. The upper network can enhance relation of context information, salient information relation of a kernel mapping and relations of shallow and deep layers to improve performance of image super-resolution. That can strengthen adaptability of an obtained super-resolution model for different scenes. The lower network utilizes a symmetric architecture to enhance relations of different layers to mine more structural information, which is complementary with a upper network for image super-resolution. The relevant experimental results show that the proposed ADSRNet is effective to deal with image resolving. Codes are obtained at https://github.com/hellloxiaotian/ADSRNet.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.15704v5</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziang Wu, Jinwei Xie, Xuanyu Zhang, Tao Wang, Yongjun Zhang, Qi Zhu, Chunwei Tian</dc:creator>
    </item>
    <item>
      <title>Principled Feature Disentanglement for High-Fidelity Unified Brain MRI Synthesis</title>
      <link>https://arxiv.org/abs/2406.14954</link>
      <description>arXiv:2406.14954v2 Announce Type: replace 
Abstract: Multisequence Magnetic Resonance Imaging (MRI) provides a more reliable diagnosis in clinical applications through complementary information across sequences. However, in practice, the absence of certain MR sequences is a common problem that can lead to inconsistent analysis results. In this work, we propose a novel unified framework for synthesizing multisequence MR images, called hybrid-fusion GAN (HF-GAN). The fundamental mechanism of this work is principled feature disentanglement, which aligns the design of the architecture with the complexity of the features. A powerful many-to-one stream is constructed for the extraction of complex complementary features, while utilizing parallel, one-to-one streams to process modality-specific information. These disentangled features are dynamically integrated into a common latent space by a channel attention-based fusion module (CAFF) and then transformed via a modality infuser to generate the target sequence. We validated our framework on public datasets of both healthy and pathological brain MRI. Quantitative and qualitative results show that HF-GAN achieves state-of-the-art performance, with our 2D slice-based framework notably outperforming a leading 3D volumetric model. Furthermore, the utilization of HF-GAN for data imputation substantially improves the performance of the downstream brain tumor segmentation task, demonstrating its clinical relevance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14954v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jihoon Cho, Jonghye Woo, Jinah Park</dc:creator>
    </item>
    <item>
      <title>Accelerating MRI with Longitudinally-informed Latent Posterior Sampling</title>
      <link>https://arxiv.org/abs/2407.00537</link>
      <description>arXiv:2407.00537v2 Announce Type: replace 
Abstract: Purpose: To accelerate MRI acquisition by incorporating the previous scans of a subject during reconstruction. Although longitudinal imaging constitutes much of clinical MRI, leveraging previous scans is challenging due to the complex relationship between scan sessions, potentially involving substantial anatomical or pathological changes, and the lack of open-access datasets with both longitudinal pairs and raw k-space needed for training deep learning-based reconstruction models. Methods: We propose a diffusion-model-based reconstruction framework that eliminates the need for longitudinally paired training data. During training, we treat all scan timepoints as samples from the same distribution, therefore requiring only standalone images. At inference, our framework integrates a subject's prior scan in magnitude DICOM format, which is readily available in clinical workflows, to guide reconstruction of the follow-up. To support future development, we introduce an open-access clinical dataset containing multi-session pairs including prior DICOMs and follow-up k-space. Results: Our method consistently outperforms both longitudinal and non-longitudinal baseline reconstruction methods across various accelerated Cartesian acquisition strategies. In imaging regions highly similar to the prior scan, we observe up to 10\% higher SSIM and 2 dB higher PSNR, without degradation in dissimilar areas. Compared to longitudinal reconstruction baselines, our method demonstrates robustness to varying degrees of anatomical change and misregistration. Conclusion: We demonstrate that prior scans can be effectively integrated with state-of-the-art diffusion-based reconstruction methods to improve image quality and enable greater scan acceleration, without requiring an extensive longitudinally-paired training dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00537v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yonatan Urman, Zachary Shah, Ashwin Kumar, Bruno P. Soares, Kawin Setsompop</dc:creator>
    </item>
    <item>
      <title>Beyond Uncertainty Quantification: Learning Uncertainty for Trust-Informed Neural Network Decisions - A Case Study in COVID-19 Classification</title>
      <link>https://arxiv.org/abs/2410.02805</link>
      <description>arXiv:2410.02805v2 Announce Type: replace 
Abstract: Reliable uncertainty quantification is critical in high-stakes applications, such as medical diagnosis, where confidently incorrect predictions can erode trust in automated decision-making systems. Traditional uncertainty quantification methods rely on a predefined confidence threshold to classify predictions as confident or uncertain. However, this approach assumes that predictions exceeding the threshold are trustworthy, while those below it are uncertain, without explicitly assessing the correctness of high-confidence predictions. As a result, confidently incorrect predictions may still occur, leading to misleading uncertainty assessments. To address this limitation, this study proposed an uncertainty-aware stacked neural network, which extends conventional uncertainty quantification by learning when predictions should be trusted. The framework consists of a two-tier model: the base model generates predictions with uncertainty estimates, while the meta-model learns to assign a trust flag, distinguishing confidently correct cases from those requiring expert review. The proposed approach is evaluated against the traditional threshold-based method across multiple confidence thresholds and pre-trained architectures using the COVIDx CXR-4 dataset. Results demonstrate that the proposed framework significantly reduces confidently incorrect predictions, offering a more trustworthy and efficient decision-support system for high-stakes domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02805v2</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hassan Gharoun, Mohammad Sadegh Khorshidi, Fang Chen, Amir H. Gandomi</dc:creator>
    </item>
    <item>
      <title>Using Randomized Nystr\"om Preconditioners to Accelerate Variational Image Reconstruction</title>
      <link>https://arxiv.org/abs/2411.08178</link>
      <description>arXiv:2411.08178v2 Announce Type: replace 
Abstract: Model-based iterative reconstruction plays a key role in solving inverse problems. However, the associated minimization problems are generally large-scale, nonsmooth, and sometimes even nonconvex, which present challenges in designing efficient iterative solvers. Preconditioning methods can significantly accelerate the convergence of iterative methods. In some applications, computing preconditioners on-the-fly is beneficial. Moreover, forward models in image reconstruction are typically represented as operators, and the corresponding explicit matrices are often unavailable, which brings additional challenges in designing preconditioners. Therefore, for practical use, computing and applying preconditioners should be computationally inexpensive. This paper adapts the randomized Nystr\"om approximation to compute effective preconditioners that accelerate image reconstruction without requiring an explicit matrix for the forward model. We leverage modern GPU computational platforms to compute the preconditioner on-the-fly. Moreover, we propose efficient approaches for applying the preconditioners to problems with classical nonsmooth regularizers, i.e., wavelet, total variation, and Hessian Schatten-norm. Our numerical results on image deblurring, super-resolution with impulsive noise, and 2D computed tomography reconstruction illustrate the efficiency and effectiveness of the proposed preconditioner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08178v2</guid>
      <category>eess.IV</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tao Hong, Zhaoyi Xu, Jason Hu, Jeffrey A. Fessler</dc:creator>
    </item>
    <item>
      <title>A Synthetic Data-Driven Radiology Foundation Model for Pan-tumor Clinical Diagnosis</title>
      <link>https://arxiv.org/abs/2502.06171</link>
      <description>arXiv:2502.06171v2 Announce Type: replace 
Abstract: AI-assisted imaging made substantial advances in tumor diagnosis and management. However, a major barrier to developing robust oncology foundation models is the scarcity of large-scale, high-quality annotated datasets, which are limited by privacy restrictions and the high cost of manual labeling. To address this gap, we present PASTA, a pan-tumor radiology foundation model built on PASTA-Gen, a synthetic data framework that generated 30,000 3D CT scans with pixel-level lesion masks and structured reports of tumors across ten organ systems. Leveraging this resource, PASTA achieves state-of-the-art performance on 45 of 46 oncology tasks, including non-contrast CT tumor screening, lesion segmentation, structured reporting, tumor staging, survival prediction, and MRI-modality transfer. To assess clinical applicability, we developed PASTA-AID, a clinical decision support system, and ran a retrospective simulated clinical trial across two scenarios. For pan-tumor screening on plain CT with fixed reading time, PASTA-AID increased radiologists' throughput by 11.1-25.1% and improved sensitivity by 17.0-31.4% and precision by 10.5-24.9%; additionally, in a diagnosis-aid workflow, it reduced segmentation time by up to 78.2% and reporting time by up to 36.5%. Beyond gains in accuracy and efficiency, PASTA-AID narrowed the expertise gap, enabling less-experienced radiologists to approach expert-level performance. Together, this work establishes an end-to-end, synthetic data-driven pipeline spanning data generation, model development, and clinical validation, thereby demonstrating substantial potential for pan-tumor research and clinical translation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06171v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenhui Lei, Hanyu Chen, Zitian Zhang, Luyang Luo, Qiong Xiao, Yannian Gu, Peng Gao, Yankai Jiang, Ci Wang, Guangtao Wu, Tongjia Xu, Yingjie Zhang, Pranav Rajpurkar, Xiaofan Zhang, Shaoting Zhang, Zhenning Wang</dc:creator>
    </item>
    <item>
      <title>FetalCLIP: A Visual-Language Foundation Model for Fetal Ultrasound Image Analysis</title>
      <link>https://arxiv.org/abs/2502.14807</link>
      <description>arXiv:2502.14807v3 Announce Type: replace 
Abstract: Foundation models are becoming increasingly effective in the medical domain, offering pre-trained models on large datasets that can be readily adapted for downstream tasks. Despite progress, fetal ultrasound images remain a challenging domain for foundation models due to their inherent complexity, often requiring substantial additional training and facing limitations due to the scarcity of paired multimodal data. To overcome these challenges, here we introduce FetalCLIP, a vision-language foundation model capable of generating universal representation of fetal ultrasound images. FetalCLIP was pre-trained using a multimodal learning approach on a diverse dataset of 210,035 fetal ultrasound images paired with text. This represents the largest paired dataset of its kind used for foundation model development to date. This unique training approach allows FetalCLIP to effectively learn the intricate anatomical features present in fetal ultrasound images, resulting in robust representations that can be used for a variety of downstream applications. In extensive benchmarking across a range of key fetal ultrasound applications, including classification, gestational age estimation, congenital heart defect (CHD) detection, and fetal structure segmentation, FetalCLIP outperformed all baselines while demonstrating remarkable generalizability and strong performance even with limited labeled data. We plan to release the FetalCLIP model publicly for the benefit of the broader scientific community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14807v3</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fadillah Maani, Numan Saeed, Tausifa Saleem, Zaid Farooq, Hussain Alasmawi, Werner Diehl, Ameera Mohammad, Gareth Waring, Saudabi Valappi, Leanne Bricker, Mohammad Yaqub</dc:creator>
    </item>
    <item>
      <title>Geodesic Diffusion Models for Efficient Medical Image Enhancement</title>
      <link>https://arxiv.org/abs/2503.00745</link>
      <description>arXiv:2503.00745v2 Announce Type: replace 
Abstract: Diffusion models generate data by learning to reverse a forward process, where samples are progressively perturbed with Gaussian noise according to a predefined noise schedule. From a geometric perspective, each noise schedule corresponds to a unique trajectory in probability space from the data distribution to a Gaussian prior. However, prior diffusion models rely on empirically chosen schedules that may not be optimal. This inefficiency necessitates many intermediate time steps, resulting in high computational costs during both training and sampling. To address this, we derive a family of geodesic noise schedules corresponding to the shortest paths in probability space under the Fisher-Rao metric. Based on these schedules, we propose Geodesic Diffusion Models (GDMs), which significantly improve training and sampling efficiency by minimizing the energy required to transform between probability distributions. This efficiency further enables sampling to start from an intermediate distribution in conditional image generation, achieving state-of-the-art results with as few as 6 steps. We evaluated GDM on two medical image enhancement tasks: CT image denoising and MRI image super-resolution. Experimental results show that GDM achieved state-of-the-art performance while reducing training time by 20- to 30-fold compared to Denoising Diffusion Probabilistic Models (DDPMs) and 4- to 6-fold compared to Fast-DDPM, and accelerating sampling by 160- to 170-fold and 1.6-fold, respectively. These gains support the use of GDM for efficient model development and real-time clinical applications. Our code is publicly available at: https://github.com/mirthAI/GDM-VE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00745v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Teng Zhang, Hongxu Jiang, Kuang Gong, Wei Shao</dc:creator>
    </item>
    <item>
      <title>Convergent Complex Quasi-Newton Proximal Methods for Gradient-Driven Denoisers in Compressed Sensing MRI Reconstruction</title>
      <link>https://arxiv.org/abs/2505.04820</link>
      <description>arXiv:2505.04820v2 Announce Type: replace 
Abstract: In compressed sensing (CS) MRI, model-based methods are pivotal to achieving accurate reconstruction. One of the main challenges in model-based methods is finding an effective prior to describe the statistical distribution of the target image. Plug-and-Play (PnP) and REgularization by Denoising (RED) are two general frameworks that use denoisers as the prior. While PnP/RED methods with convolutional neural networks (CNNs) based denoisers outperform classical hand-crafted priors in CS MRI, their convergence theory relies on assumptions that do not hold for practical CNNs. The recently developed gradient-driven denoisers offer a framework that bridges the gap between practical performance and theoretical guarantees. However, the numerical solvers for the associated minimization problem remain slow for CS MRI reconstruction. This paper proposes a complex quasi-Newton proximal method that achieves faster convergence than existing approaches. To address the complex domain in CS MRI, we propose a modified Hessian estimation method that guarantees Hermitian positive definiteness. Furthermore, we provide a rigorous convergence analysis of the proposed method for nonconvex settings. Numerical experiments on both Cartesian and non-Cartesian sampling trajectories demonstrate the effectiveness and efficiency of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04820v2</guid>
      <category>eess.IV</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tao Hong, Zhaoyi Xu, Se Young Chun, Luis Hernandez-Garcia, Jeffrey A. Fessler</dc:creator>
    </item>
    <item>
      <title>SV-DRR: High-Fidelity Novel View X-Ray Synthesis Using Diffusion Model</title>
      <link>https://arxiv.org/abs/2507.05148</link>
      <description>arXiv:2507.05148v3 Announce Type: replace 
Abstract: X-ray imaging is a rapid and cost-effective tool for visualizing internal human anatomy. While multi-view X-ray imaging provides complementary information that enhances diagnosis, intervention, and education, acquiring images from multiple angles increases radiation exposure and complicates clinical workflows. To address these challenges, we propose a novel view-conditioned diffusion model for synthesizing multi-view X-ray images from a single view. Unlike prior methods, which are limited in angular range, resolution, and image quality, our approach leverages the Diffusion Transformer to preserve fine details and employs a weak-to-strong training strategy for stable high-resolution image generation. Experimental results demonstrate that our method generates higher-resolution outputs with improved control over viewing angles. This capability has significant implications not only for clinical applications but also for medical education and data extension, enabling the creation of diverse, high-quality datasets for training and analysis. Our code is available at https://github.com/xiechun298/SV-DRR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05148v3</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-032-04965-0_54</arxiv:DOI>
      <dc:creator>Chun Xie, Yuichi Yoshii, Itaru Kitahara</dc:creator>
    </item>
    <item>
      <title>Multimodal Fusion at Three Tiers: Physics-Driven Data Generation and Vision-Language Guidance for Brain Tumor Segmentation</title>
      <link>https://arxiv.org/abs/2507.09966</link>
      <description>arXiv:2507.09966v3 Announce Type: replace 
Abstract: Accurate brain tumor segmentation is crucial for neuro-oncology diagnosis and treatment planning. Deep learning methods have made significant progress, but automatic segmentation still faces challenges, including tumor morphological heterogeneity and complex three-dimensional spatial relationships. This paper proposes a three-tier fusion architecture that achieves precise brain tumor segmentation. The method processes information progressively at the pixel, feature, and semantic levels. At the pixel level, physical modeling extends magnetic resonance imaging (MRI) to multimodal data, including simulated ultrasound and synthetic computed tomography (CT). At the feature level, the method performs Transformer-based cross-modal feature fusion through multi-teacher collaborative distillation, integrating three expert teachers (MRI, US, CT). At the semantic level, clinical textual knowledge generated by GPT-4V is transformed into spatial guidance signals using CLIP contrastive learning and Feature-wise Linear Modulation (FiLM). These three tiers together form a complete processing chain from data augmentation to feature extraction to semantic guidance. We validated the method on the Brain Tumor Segmentation (BraTS) 2020, 2021, and 2023 datasets. The model achieves average Dice coefficients of 0.8665, 0.9014, and 0.8912 on the three datasets, respectively, and reduces the 95% Hausdorff Distance (HD95) by an average of 6.57 millimeters compared with the baseline. This method provides a new paradigm for precise tumor segmentation and boundary localization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09966v3</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingda Zhang</dc:creator>
    </item>
    <item>
      <title>SpectraLift: Physics-Guided Spectral-Inversion Network for Self-Supervised Hyperspectral Image Super-Resolution</title>
      <link>https://arxiv.org/abs/2507.13339</link>
      <description>arXiv:2507.13339v2 Announce Type: replace 
Abstract: High-spatial-resolution hyperspectral images (HSI) are essential for applications such as remote sensing and medical imaging, yet HSI sensors inherently trade spatial detail for spectral richness. Fusing high-spatial-resolution multispectral images (HR-MSI) with low-spatial-resolution hyperspectral images (LR-HSI) is a promising route to recover fine spatial structures without sacrificing spectral fidelity. Most state-of-the-art methods for HSI-MSI fusion demand point spread function (PSF) calibration or ground truth high resolution HSI (HR-HSI), both of which are impractical to obtain in real world settings. We present SpectraLift, a fully self-supervised framework that fuses LR-HSI and HR-MSI inputs using only the MSI's Spectral Response Function (SRF). SpectraLift trains a lightweight per-pixel multi-layer perceptron (MLP) network using ($i$)~a synthetic low-spatial-resolution multispectral image (LR-MSI) obtained by applying the SRF to the LR-HSI as input, ($ii$)~the LR-HSI as the output, and ($iii$)~an $\ell_1$ spectral reconstruction loss between the estimated and true LR-HSI as the optimization objective. At inference, SpectraLift uses the trained network to map the HR-MSI pixel-wise into a HR-HSI estimate. SpectraLift converges in minutes, is agnostic to spatial blur and resolution, and outperforms state-of-the-art methods on PSNR, SAM, SSIM, and RMSE benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13339v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ritik Shah, Marco F. Duarte</dc:creator>
    </item>
    <item>
      <title>REACT-KD: Region-Aware Cross-modal Topological Knowledge Distillation for Interpretable Medical Image Classification</title>
      <link>https://arxiv.org/abs/2508.02104</link>
      <description>arXiv:2508.02104v2 Announce Type: replace 
Abstract: Reliable and interpretable tumor classification from clinical imaging remains a core challenge. The main difficulties arise from heterogeneous modality quality, limited annotations, and the absence of structured anatomical guidance. We present REACT-KD, a Region-Aware Cross-modal Topological Knowledge Distillation framework that transfers supervision from high-fidelity multi-modal sources into a lightweight CT-based student model. The framework employs a dual teacher design. One branch captures structure-function relationships through dual-tracer PET/CT, while the other models dose-aware features using synthetically degraded low-dose CT. These branches jointly guide the student model through two complementary objectives. The first achieves semantic alignment through logits distillation, and the second models anatomical topology through region graph distillation. A shared CBAM3D module ensures consistent attention across modalities. To improve reliability in deployment, REACT-KD introduces modality dropout during training, which enables robust inference under partial or noisy inputs. As a case study, we applied REACT-KD to hepatocellular carcinoma staging. The framework achieved an average AUC of 93.5\% on an internal PET/CT cohort and maintained 76.6\% to 81.5\% AUC across varying levels of dose degradation in external CT testing. Decision curve analysis further shows that REACT-KD consistently provides the highest net clinical benefit across all thresholds, confirming its value in real-world diagnostic practice. Code is available at: https://github.com/Kinetics-JOJO/REACT-KD</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02104v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongzhao Chen, Hexiao Ding, Yufeng Jiang, Jing Lan, Ka Chun Li, Gerald W. Y. Cheng, Nga-Chun Ng, Yao Pu, Jing Cai, Liang-ting Lin, Jung Sun Yoo</dc:creator>
    </item>
    <item>
      <title>Robust Pan-Cancer Mitotic Figure Detection with YOLOv12</title>
      <link>https://arxiv.org/abs/2509.02593</link>
      <description>arXiv:2509.02593v4 Announce Type: replace 
Abstract: Mitotic figures represent a key histoprognostic feature in tumor pathology, providing crucial insights into tumor aggressiveness and proliferation. However, their identification remains challenging, subject to significant inter-observer variability, even among experienced pathologists. To address this issue, the MItosis DOmain Generalization (MIDOG) 2025 challenge marks the third edition of an international competition aiming to develop robust mitosis detection algorithms. In this paper, we present a mitotic figure detection approach based on the state-of-the-art YOLOv12 object detection architecture. Our method achieved an F1-score of 0.801 on the preliminary test set (hotspots only) and ranked second on the final test leaderboard with an F1-score of 0.7216 across complex and heterogeneous whole-slide regions, without relying on external data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02593v4</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rapha\"el Bourgade, Guillaume Balezo, Hana Feki, Lily Monier, Matthieu Blons, Alice Blondel, Delphine Loussouarn, Anne Vincent-Salomon, Thomas Walter</dc:creator>
    </item>
    <item>
      <title>JND-Guided Light-Weight Neural Pre-Filter for Perceptual Image Coding</title>
      <link>https://arxiv.org/abs/2510.10648</link>
      <description>arXiv:2510.10648v2 Announce Type: replace 
Abstract: Just Noticeable Distortion (JND)-guided pre-filter is a promising technique for improving the perceptual compression efficiency of image coding. However, existing methods are often computationally expensive, and the field lacks standardized benchmarks for fair comparison. To address these challenges, this paper introduces a twofold contribution. First, we develop and open-source FJNDF-Pytorch, a unified benchmark for frequency-domain JND-Guided pre-filters. Second, leveraging this platform, we propose a complete learning framework for a novel, lightweight Convolutional Neural Network (CNN). Experimental results demonstrate that our proposed method achieves state-of-the-art compression efficiency, consistently outperforming competitors across multiple datasets and encoders. In terms of computational cost, our model is exceptionally lightweight, requiring only 7.15 GFLOPs to process a 1080p image, which is merely 14.1% of the cost of recent lightweight network. Our work presents a robust, state-of-the-art solution that excels in both performance and efficiency, supported by a reproducible research platform. The open-source implementation is available at https://github.com/viplab-fudan/FJNDF-Pytorch.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10648v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.MM</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenlong He, Zhijian Hao, Leilei Huang, Xiaoyang Zeng, Yibo Fan</dc:creator>
    </item>
    <item>
      <title>Limitations of Data-Driven Spectral Reconstruction -- An Optics-Aware Analysis</title>
      <link>https://arxiv.org/abs/2401.03835</link>
      <description>arXiv:2401.03835v4 Announce Type: replace-cross 
Abstract: Hyperspectral imaging empowers machine vision systems with the distinct capability of identifying materials through recording their spectral signatures. Recent efforts in data-driven spectral reconstruction aim at extracting spectral information from RGB images captured by cost-effective RGB cameras, instead of dedicated hardware. Published work reports exceedingly high numerical scores for this reconstruction task, yet real-world performance lags substantially behind. We systematically analyze the performance of such methods. First, we evaluate the overfitting limitations with respect to current datasets by training the networks with less data, validating the trained models with unseen yet slightly modified data and cross-dataset validation. Second, we reveal fundamental limitations in the ability of RGB to spectral methods to deal with metameric or near-metameric conditions, which have so far gone largely unnoticed due to the insufficiencies of existing datasets. We validate the trained models with metamer data generated by metameric black theory and re-training the networks with various forms of metamers. This methodology can also be used for data augmentation as a partial mitigation of the dataset issues, although the RGB to spectral inverse problem remains fundamentally ill-posed. Finally, we analyze the potential for modifying the problem setting to achieve better performance by exploiting optical encoding provided by either optical aberrations or deliberate optical design. Our experiments show such approaches provide improved results under certain circumstances, but their overall performance is limited by the same dataset issues. We conclude that future progress on snapshot spectral imaging will heavily depend on the generation of improved datasets which can then be used to design effective optical encoding strategies. Code: https://github.com/vccimaging/OpticsAwareHSI-Analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.03835v4</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TCI.2025.3622928</arxiv:DOI>
      <arxiv:journal_reference>Fu Q, Souza M, Choi E, Shin S, Baek SH, Heidrich W. Limitations of Data-Driven Spectral Reconstruction--Optics-Aware Analysis. IEEE Transactions on Computational Imaging. 2025</arxiv:journal_reference>
      <dc:creator>Qiang Fu, Matheus Souza, Eunsue Choi, Suhyun Shin, Seung-Hwan Baek, Wolfgang Heidrich</dc:creator>
    </item>
    <item>
      <title>Improvement of Spiking Neural Network with Bit Planes and Color Models</title>
      <link>https://arxiv.org/abs/2410.08229</link>
      <description>arXiv:2410.08229v4 Announce Type: replace-cross 
Abstract: Spiking neural network (SNN) has emerged as a promising paradigm in computational neuroscience and artificial intelligence, offering advantages such as low energy consumption and small memory footprint. However, their practical adoption is constrained by several challenges, prominently among them being performance optimization. In this study, we present a novel approach to enhance the performance of SNN for images through a new coding method that exploits bit plane representation. Our proposed technique is designed to improve the accuracy of SNN without increasing model size. Also, we investigate the impacts of color models of the proposed coding process. Through extensive experimental validation, we demonstrate the effectiveness of our coding strategy in achieving performance gain across multiple datasets. To the best of our knowledge, this is the first research that considers bit planes and color models in the context of SNN. By leveraging the unique characteristics of bit planes, we hope to unlock new potentials in SNNs performance, potentially paving the way for more efficient and effective SNNs models in future researches and applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08229v4</guid>
      <category>cs.CV</category>
      <category>cs.NE</category>
      <category>eess.IV</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nhan T. Luu, Duong T. Luu, Nam N. Pham, Thang C. Truong</dc:creator>
    </item>
    <item>
      <title>Normative Modelling in Neuroimaging: A Practical Guide for Researchers</title>
      <link>https://arxiv.org/abs/2509.07237</link>
      <description>arXiv:2509.07237v2 Announce Type: replace-cross 
Abstract: Normative modelling is an increasingly common statistical technique in neuroimaging that estimates population-level benchmarks in brain structure. It enables the quantification of individual deviations from expected distributions whilst accounting for biological and technical covariates without requiring large, matched control groups. This makes it a powerful alternative to traditional case-control studies for identifying brain structural alterations associated with pathology. Despite the availability of numerous modelling approaches and several toolboxes with pre-trained models, the distinct strengths and limitations of normative modelling make it difficult to determine how and when to implement them appropriately. This review offers practical guidance and outlines statistical considerations for clinical researchers using normative modelling in neuroimaging. Through a worked example using clinical epilepsy data, we outline considerations for responsible implementation of pre-trained normative models, to support their broad and rigorous adoption in neuroimaging research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07237v2</guid>
      <category>q-bio.NC</category>
      <category>eess.IV</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nida Alyas, Jonathan Horsley, Bethany Little, Peter N. Taylor, Yujiang Wang, Karoline Leiberg</dc:creator>
    </item>
  </channel>
</rss>
