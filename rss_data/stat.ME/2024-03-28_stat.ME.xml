<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 29 Mar 2024 04:00:27 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 29 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Robust estimations from distribution structures: V. Non-asymptotic</title>
      <link>https://arxiv.org/abs/2403.18951</link>
      <description>arXiv:2403.18951v1 Announce Type: new 
Abstract: Due to the complexity of order statistics, the finite sample behaviour of robust statistics is generally not analytically solvable. While the Monte Carlo method can provide approximate solutions, its convergence rate is typically very slow, making the computational cost to achieve the desired accuracy unaffordable for ordinary users. In this paper, we propose an approach analogous to the Fourier transformation to decompose the finite sample structure of the uniform distribution. By obtaining sets of sequences that are consistent with parametric distributions for the first four sample moments, we can approximate the finite sample behavior of other estimators with significantly reduced computational costs. This article reveals the underlying structure of randomness and presents a novel approach to integrate multiple assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18951v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.OT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tuobang Li</dc:creator>
    </item>
    <item>
      <title>Efficient global estimation of conditional-value-at-risk through stochastic kriging and extreme value theory</title>
      <link>https://arxiv.org/abs/2403.19018</link>
      <description>arXiv:2403.19018v1 Announce Type: new 
Abstract: We consider the problem of evaluating risk for a system that is modeled by a complex stochastic simulation with many possible input parameter values. Two sources of computational burden can be identified: the effort associated with extensive simulation runs required to accurately represent the tail of the loss distribution for each set of parameter values, and the computational cost of evaluating multiple candidate parameter values. The former concern can be addressed by using Extreme Value Theory (EVT) estimations, which specifically concentrate on the tails. Meta-modeling approaches are often used to tackle the latter concern. In this paper, we propose a framework for constructing a particular meta-modeling framework, stochastic kriging, that is based on EVT-based estimation for a class of coherent measures of risk. The proposed approach requires an efficient estimator of the intrinsic variance, and so we derive an EVT-based expression for it. It then allows us to avoid multiple replications of the risk measure in each design point, which was required in similar previously proposed approaches, resulting in a substantial reduction in computational effort. We then perform a case study, outlining promising use cases, and conditions when the EVT-based approach outperforms simpler empirical estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19018v1</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Armin Khayyer, Alexander Vinel, Joseph J. Kennedy</dc:creator>
    </item>
    <item>
      <title>Imputing missing not-at-random longitudinal marker values in time-to-event analysis: fully conditional specification multiple imputation in joint modeling</title>
      <link>https://arxiv.org/abs/2403.19192</link>
      <description>arXiv:2403.19192v1 Announce Type: new 
Abstract: We propose a procedure for imputing missing values of time-dependent covariates in a survival model using fully conditional specification. Specifically, we focus on imputing missing values of a longitudinal marker in joint modeling of the marker and time-to-event data, but the procedure can be easily applied to a time-varying covariate survival model as well. First, missing marker values are imputed via fully conditional specification multiple imputation, and then joint modeling is applied for estimating the association between the marker and the event. This procedure is recommended since in joint modeling marker measurements that are missing not-at-random can lead to bias (e.g. when patients with higher marker values tend to miss visits). Specifically, in cohort studies such a bias can occur since patients for whom all marker measurements during follow-up are missing are excluded from the analysis. Our procedure enables to include these patients by imputing their missing values using a modified version of fully conditional specification multiple imputation. The imputation model includes a special indicator for the subgroup with missing marker values during follow-up, and can be easily implemented in various software: R, SAS, Stata etc. Using simulations we show that the proposed procedure performs better than standard joint modeling in the missing not-at-random scenario with respect to bias, coverage and Type I error rate of the test, and as good as standard joint modeling in the completely missing at random scenario. Finally we apply the procedure on real data on glucose control and cancer in diabetic patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19192v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Havi Murad, Nirit Agay, Rachel Dankner</dc:creator>
    </item>
    <item>
      <title>Overlap violations in external validity</title>
      <link>https://arxiv.org/abs/2403.19504</link>
      <description>arXiv:2403.19504v1 Announce Type: new 
Abstract: Estimating externally valid causal effects is a foundational problem in the social and biomedical sciences. Generalizing or transporting causal estimates from an experimental sample to a target population of interest relies on an overlap assumption between the experimental sample and the target population--i.e., all units in the target population must have a non-zero probability of being included in the experiment. In practice, having full overlap between an experimental sample and a target population can be implausible. In the following paper, we introduce a framework for considering external validity in the presence of overlap violations. We introduce a novel bias decomposition that parameterizes the bias from an overlap violation into two components: (1) the proportion of units omitted, and (2) the degree to which omitting the units moderates the treatment effect. The bias decomposition offers an intuitive and straightforward approach to conducting sensitivity analysis to assess robustness to overlap violations. Furthermore, we introduce a suite of sensitivity tools in the form of summary measures and benchmarking, which help researchers consider the plausibility of the overlap violations. We apply the proposed framework on an experiment evaluating the impact of a cash transfer program in Northern Uganda.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19504v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Melody Huang</dc:creator>
    </item>
    <item>
      <title>On Bootstrapping Lasso in Generalized Linear Models and the Cross Validation</title>
      <link>https://arxiv.org/abs/2403.19515</link>
      <description>arXiv:2403.19515v1 Announce Type: new 
Abstract: Generalized linear models or GLM constitutes an important set of models which generalizes the ordinary linear regression by connecting the response variable with the covariates through arbitrary link functions. On the other hand, Lasso is a popular and easy to implement penalization method in regression when all the covariates are not relevant. However, Lasso generally has non-tractable asymptotic distribution and hence development of an alternative method of distributional approximation is required for the purpose of statistical inference. In this paper, we develop a Bootstrap method which works as an approximation of the distribution of the Lasso estimator for all the sub-models of GLM. To connect the distributional approximation theory based on the proposed Bootstrap method with the practical implementation of Lasso, we explore the asymptotic properties of K-fold cross validation-based penalty parameter. The results established essentially justifies drawing valid statistical inference regarding the unknown parameters based on the proposed Bootstrap method for any sub model of GLM after selecting the penalty parameter using K-fold cross validation. Good finite sample properties are also shown through a moderately large simulation study. The method is also implemented on a real data set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19515v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mayukh Choudhury, Debraj Das</dc:creator>
    </item>
    <item>
      <title>Data-Adaptive Tradeoffs among Multiple Risks in Distribution-Free Prediction</title>
      <link>https://arxiv.org/abs/2403.19605</link>
      <description>arXiv:2403.19605v1 Announce Type: new 
Abstract: Decision-making pipelines are generally characterized by tradeoffs among various risk functions. It is often desirable to manage such tradeoffs in a data-adaptive manner. As we demonstrate, if this is done naively, state-of-the art uncertainty quantification methods can lead to significant violations of putative risk guarantees.
  To address this issue, we develop methods that permit valid control of risk when threshold and tradeoff parameters are chosen adaptively. Our methodology supports monotone and nearly-monotone risks, but otherwise makes no distributional assumptions.
  To illustrate the benefits of our approach, we carry out numerical experiments on synthetic data and the large-scale vision dataset MS-COCO.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19605v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Drew T. Nguyen, Reese Pathak, Anastasios N. Angelopoulos, Stephen Bates, Michael I. Jordan</dc:creator>
    </item>
    <item>
      <title>Positivity violations in marginal structural survival models with time-dependent confounding: a simulation study on IPTW-estimator performance</title>
      <link>https://arxiv.org/abs/2403.19606</link>
      <description>arXiv:2403.19606v1 Announce Type: new 
Abstract: In longitudinal observational studies, marginal structural models (MSMs) are a class of causal models used to analyze the effect of an exposure on the (survival) outcome of interest while accounting for exposure-affected time-dependent confounding. In the applied literature, inverse probability of treatment weighting (IPTW) has been widely adopted to estimate MSMs. An essential assumption for IPTW-based MSMs is the positivity assumption, which ensures that each individual in the population has a non-zero probability of receiving each exposure level within confounder strata. Positivity, along with consistency, conditional exchangeability, and correct specification of the weighting model, is crucial for valid causal inference through IPTW-based MSMs but is often overlooked compared to confounding bias. Positivity violations can arise from subjects having a zero probability of being exposed/unexposed (strict violations) or near-zero probabilities due to sampling variability (near violations). This article discusses the effect of violations in the positivity assumption on the estimates from IPTW-based MSMs. Building on the algorithms for simulating longitudinal survival data from MSMs by Havercroft and Didelez (2012) and Keogh et al. (2021), systematic simulations under strict/near positivity violations are performed. Various scenarios are explored by varying (i) the size of the confounder interval in which positivity violations arise, (ii) the sample size, (iii) the weight truncation strategy, and (iv) the subject's propensity to follow the protocol violation rule. This study underscores the importance of assessing positivity violations in IPTW-based MSMs to ensure robust and reliable causal inference in survival analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19606v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marta Spreafico</dc:creator>
    </item>
    <item>
      <title>Graph Neural Networks for Treatment Effect Prediction</title>
      <link>https://arxiv.org/abs/2403.19289</link>
      <description>arXiv:2403.19289v1 Announce Type: cross 
Abstract: Estimating causal effects in e-commerce tends to involve costly treatment assignments which can be impractical in large-scale settings. Leveraging machine learning to predict such treatment effects without actual intervention is a standard practice to diminish the risk. However, existing methods for treatment effect prediction tend to rely on training sets of substantial size, which are built from real experiments and are thus inherently risky to create. In this work we propose a graph neural network to diminish the required training set size, relying on graphs that are common in e-commerce data. Specifically, we view the problem as node regression with a restricted number of labeled instances, develop a two-model neural architecture akin to previous causal effect estimators, and test varying message-passing layers for encoding. Furthermore, as an extra step, we combine the model with an acquisition function to guide the creation of the training set in settings with extremely low experimental budget. The framework is flexible since each step can be used separately with other models or policies. The experiments on real large-scale networks indicate a clear advantage of our methodology over the state of the art, which in many cases performs close to random underlining the need for models that can generalize with limited labeled samples to reduce experimental risks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19289v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>George Panagopoulos, Daniele Malitesta, Fragkiskos D. Malliaros, Jun Pang</dc:creator>
    </item>
    <item>
      <title>Simulating Relational Event Histories -- Why and How</title>
      <link>https://arxiv.org/abs/2403.19329</link>
      <description>arXiv:2403.19329v1 Announce Type: cross 
Abstract: Many important social phenomena result from repeated interactions among individuals over time such as email exchanges in an organization, or face-to-face interactions in a classroom. Insights into the mechanisms underlying the dynamics of these interactions can be achieved through simulations of networks on a fine temporal granularity. In this paper, we present statistical frameworks to simulate relational event networks under dyadic and actor-oriented relational event models. These simulators have a broad applicability in temporal social network research such as model fit assessment, theory building, network intervention planning, making predictions, understanding the impact of network structures, to name a few. We show this in three extensive applications. First, it is shown why simulation-based techniques are crucial for relational event model assessment, for example to investigate how past events affect future interactions in the network. Second, we demonstrate how simulation techniques contribute to a better understanding of the longevity of network interventions. Third, we show how simulation techniques are important when building and extending theories about social phenomena such as understanding social identity dynamics using optimal distinctiveness theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19329v1</guid>
      <category>cs.SI</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rumana Lakdawala, Joris Mulder, Roger Leenders</dc:creator>
    </item>
    <item>
      <title>A loss discounting framework for model averaging and selection in time series models</title>
      <link>https://arxiv.org/abs/2201.12045</link>
      <description>arXiv:2201.12045v4 Announce Type: replace 
Abstract: We introduce a Loss Discounting Framework for model and forecast combination which generalises and combines Bayesian model synthesis and generalized Bayes methodologies. We use a loss function to score the performance of different models and introduce a multilevel discounting scheme which allows a flexible specification of the dynamics of the model weights. This novel and simple model combination approach can be easily applied to large scale model averaging/selection, can handle unusual features such as sudden regime changes, and can be tailored to different forecasting problems. We compare our method to both established methodologies and state of the art methods for a number of macroeconomic forecasting examples. We find that the proposed method offers an attractive, computationally efficient alternative to the benchmark methodologies and often outperforms more complex techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.12045v4</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.ijforecast.2024.03.001</arxiv:DOI>
      <dc:creator>Dawid Bernaciak, Jim E. Griffin</dc:creator>
    </item>
    <item>
      <title>Nonstationary Spatial Process Models with Spatially Varying Covariance Kernels</title>
      <link>https://arxiv.org/abs/2203.11873</link>
      <description>arXiv:2203.11873v2 Announce Type: replace 
Abstract: Spatial process models for capturing nonstationary behavior in scientific data present several challenges with regard to statistical inference and uncertainty quantification. While nonstationary spatially-varying kernels are attractive for their flexibility and richness, their practical implementation has been reported to be overwhelmingly cumbersome because of the high-dimensional parameter spaces resulting from the spatially varying process parameters. Matters are considerably exacerbated with the massive numbers of spatial locations over which measurements are available. With limited theoretical tractability offered by nonstationary spatial processes, overcoming such computational bottlenecks require a synergy between model construction and algorithm development. We build a class of scalable nonstationary spatial process models using spatially varying covariance kernels. We present some novel consequences of such representations that befit computationally efficient implementation. More specifically, we operate within a coherent Bayesian modeling framework to achieve full uncertainty quantification using a Hybrid Monte-Carlo with nested interweaving. We carry out experiments on synthetic data sets to explore model selection and parameter identifiability and assess inferential improvements accrued from the nonstationary modeling. We illustrate strengths and pitfalls with a data set on remote sensed normalized difference vegetation index with further analysis of a lead contamination data set in the Supplement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.11873v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>S\'ebastien Coube-Sisqueille, Sudipto Banerjee, Beno\^it Liquet</dc:creator>
    </item>
    <item>
      <title>Causal chain event graphs for remedial maintenance</title>
      <link>https://arxiv.org/abs/2209.06704</link>
      <description>arXiv:2209.06704v2 Announce Type: replace 
Abstract: The analysis of system reliability has often benefited from graphical tools such as fault trees and Bayesian networks. In this article, instead of conventional graphical tools, we apply a probabilistic graphical model called the chain event graph (CEG) to represent the failures and processes of deterioration of a system. The CEG is derived from an event tree and can flexibly represent the unfolding of asymmetric processes. For this application we need to define a new class of formal intervention we call remedial to model causal effects of remedial maintenance. This fixes the root causes of a failure and returns the status of the system to as good as new. We demonstrate that the semantics of the CEG are rich enough to express this novel type of intervention. Furthermore through the bespoke causal algebras the CEG provides a transparent framework with which guide and express the rationale behind predictive inferences about the effects of various different types of remedial intervention. A back-door theorem is adapted to apply to these interventions to help discover when a system is only partially observed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.06704v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuewen Yu, Jim Q. Smith</dc:creator>
    </item>
    <item>
      <title>Heterogeneous Matrix Factorization: When Features Differ by Datasets</title>
      <link>https://arxiv.org/abs/2305.17744</link>
      <description>arXiv:2305.17744v2 Announce Type: replace 
Abstract: In myriad statistical applications, data are collected from related but heterogeneous sources. These sources share some commonalities while containing idiosyncratic characteristics. One of the most fundamental challenges in such scenarios is to recover the shared and source-specific factors. Despite the existence of a few heuristic approaches, a generic algorithm with theoretical guarantees has yet to be established. In this paper, we tackle the problem by proposing a method called Heterogeneous Matrix Factorization to separate the shared and unique factors for a class of problems. HMF maintains the orthogonality between the shared and unique factors by leveraging an invariance property in the objective. The algorithm is easy to implement and intrinsically distributed. On the theoretic side, we show that for the square error loss, HMF will converge into the optimal solutions, which are close to the ground truth. HMF can be integrated auto-encoders to learn nonlinear feature mappings. Through a variety of case studies, we showcase HMF's benefits and applicability in video segmentation, time-series feature extraction, and recommender systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.17744v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Naichen Shi, Raed Al Kontar, Salar Fattahi</dc:creator>
    </item>
    <item>
      <title>Divisive Hierarchical Clustering of Variables Identified by Singular Vectors</title>
      <link>https://arxiv.org/abs/2308.06820</link>
      <description>arXiv:2308.06820v3 Announce Type: replace 
Abstract: In this work, we present a novel method for divisive hierarchical variable clustering. A cluster is a group of elements that exhibit higher similarity among themselves than to elements outside this cluster. The correlation coefficient serves as a natural measure to assess the similarity of variables. This means that in a correlation matrix, a cluster is represented by a block of variables with greater internal than external correlation. Our approach provides a nonparametric solution to identify such block structures in the correlation matrix using singular vectors of the underlying data matrix. When divisively clustering $p$ variables, there are $2^{p-1}$ possible splits. Using the singular vectors for cluster identification, we can effectively reduce these number to at most $p(p-1)$, thereby making it computationally efficient. We elaborate on the methodology and outline the incorporation of dissimilarity measures and linkage functions to assess distances between clusters. Additionally, we demonstrate that these distances are ultrametric, ensuring that the resulting hierarchical cluster structure can be uniquely represented by a dendrogram, with the heights of the dendrogram being interpretable. To validate the efficiency of our method, we perform simulation studies and analyze real world data on personality traits and cognitive abilities. Supplementary materials for this article can be accessed online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.06820v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan O. Bauer</dc:creator>
    </item>
    <item>
      <title>Direct and Indirect Treatment Effects in the Presence of Semi-Competing Risks</title>
      <link>https://arxiv.org/abs/2309.01721</link>
      <description>arXiv:2309.01721v4 Announce Type: replace 
Abstract: Semi-competing risks refer to the phenomenon that the terminal event (such as death) can censor the non-terminal event (such as disease progression) but not vice versa. The treatment effect on the terminal event can be delivered either directly following the treatment or indirectly through the non-terminal event. We consider two strategies to decompose the total effect into a direct effect and an indirect effect under the framework of mediation analysis in completely randomized experiments by adjusting the prevalence and hazard of non-terminal events, respectively. They require slightly different assumptions on cross-world quantities to achieve identifiability. We establish asymptotic properties for the estimated counterfactual cumulative incidences and decomposed treatment effects. We illustrate the subtle difference between these two decompositions through simulation studies and two real-data applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.01721v4</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuhao Deng, Yi Wang, Xiao-Hua Zhou</dc:creator>
    </item>
    <item>
      <title>How to achieve model-robust inference in stepped wedge trials with model-based methods?</title>
      <link>https://arxiv.org/abs/2401.15680</link>
      <description>arXiv:2401.15680v2 Announce Type: replace 
Abstract: A stepped wedge design is a unidirectional crossover design where clusters are randomized to distinct treatment sequences. While model-based analysis of stepped wedge designs -- via linear mixed models or generalized estimating equations -- is standard practice to evaluate treatment effects accounting for clustering and adjusting for baseline covariates, their properties under misspecification have not been systematically explored. In this article, we study when a potentially misspecified multilevel model can offer consistent estimation for treatment effect estimands that are functions of calendar time and/or exposure time. We define nonparametric treatment effect estimands using potential outcomes, and adapt model-based methods via g-computation to achieve estimand-aligned inference. We prove a central result that, as long as the working model includes a correctly specified treatment effect structure, the g-computation is guaranteed to be consistent even if all remaining model components are arbitrarily misspecified. Furthermore, valid inference is obtained via the sandwich variance estimator. The theoretical results are illustrated via several simulation experiments and re-analysis of a completed stepped wedge trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.15680v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bingkai Wang, Xueqi Wang, Fan Li</dc:creator>
    </item>
    <item>
      <title>Bayesian inference for stochastic oscillatory systems using the phase-corrected Linear Noise Approximation</title>
      <link>https://arxiv.org/abs/2205.05955</link>
      <description>arXiv:2205.05955v3 Announce Type: replace-cross 
Abstract: Likelihood-based inference in stochastic non-linear dynamical systems, such as those found in chemical reaction networks and biological clock systems, is inherently complex and has largely been limited to small and unrealistically simple systems. Recent advances in analytically tractable approximations to the underlying conditional probability distributions enable long-term dynamics to be accurately modelled, and make the large number of model evaluations required for exact Bayesian inference much more feasible. We propose a new methodology for inference in stochastic non-linear dynamical systems exhibiting oscillatory behaviour and show the parameters in these models can be realistically estimated from simulated data. Preliminary analyses based on the Fisher Information Matrix of the model can guide the implementation of Bayesian inference. We show that this parameter sensitivity analysis can predict which parameters are practically identifiable. Several Markov chain Monte Carlo algorithms are compared, with our results suggesting a parallel tempering algorithm consistently gives the best approach for these systems, which are shown to frequently exhibit multi-modal posterior distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.05955v3</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ben Swallow, David A. Rand, Giorgos Minas</dc:creator>
    </item>
    <item>
      <title>Toward a Theory of Causation for Interpreting Neural Code Models</title>
      <link>https://arxiv.org/abs/2302.03788</link>
      <description>arXiv:2302.03788v5 Announce Type: replace-cross 
Abstract: Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly progressing from research prototypes to commercial developer tools. As such, understanding the capabilities and limitations of such models is becoming critical. However, the abilities of these models are typically measured using automated metrics that often only reveal a portion of their real-world performance. While, in general, the performance of NCMs appears promising, currently much is unknown about how such models arrive at decisions. To this end, this paper introduces $do_{code}$, a post hoc interpretability method specific to NCMs that is capable of explaining model predictions. $do_{code}$ is based upon causal inference to enable programming language-oriented explanations. While the theoretical underpinnings of $do_{code}$ are extensible to exploring different model properties, we provide a concrete instantiation that aims to mitigate the impact of spurious correlations by grounding explanations of model behavior in properties of programming languages. To demonstrate the practical benefit of $do_{code}$, we illustrate the insights that our framework can provide by performing a case study on two popular deep learning architectures and ten NCMs. The results of this case study illustrate that our studied NCMs are sensitive to changes in code syntax. All our NCMs, except for the BERT-like model, statistically learn to predict tokens related to blocks of code (\eg brackets, parenthesis, semicolon) with less confounding bias as compared to other programming language constructs. These insights demonstrate the potential of $do_{code}$ as a useful method to detect and facilitate the elimination of confounding bias in NCMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.03788v5</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TSE.2024.3379943</arxiv:DOI>
      <dc:creator>David N. Palacio, Alejandro Velasco, Nathan Cooper, Alvaro Rodriguez, Kevin Moran, Denys Poshyvanyk</dc:creator>
    </item>
    <item>
      <title>Assurance Methods for designing a clinical trial with a delayed treatment effect</title>
      <link>https://arxiv.org/abs/2310.06673</link>
      <description>arXiv:2310.06673v2 Announce Type: replace-cross 
Abstract: An assurance calculation is a Bayesian alternative to a power calculation. One may be performed to aid the planning of a clinical trial, specifically setting the sample size or to support decisions about whether or not to perform a study. Immuno-oncology is a rapidly evolving area in the development of anticancer drugs. A common phenomenon that arises in trials of such drugs is one of delayed treatment effects, that is, there is a delay in the separation of the survival curves. To calculate assurance for a trial in which a delayed treatment effect is likely to be present, uncertainty about key parameters needs to be considered. If uncertainty is not considered, the number of patients recruited may not be enough to ensure we have adequate statistical power to detect a clinically relevant treatment effect and the risk of an unsuccessful trial is increased. We present a new elicitation technique for when a delayed treatment effect is likely and show how to compute assurance using these elicited prior distributions. We provide an example to illustrate how this can be used in practice and develop open-source software to implement our methods. Our methodology has the potential to improve the success rate and efficiency of Phase III trials in immuno-oncology and for other treatments where a delayed treatment effect is expected to occur.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.06673v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>James Salsbury, Jeremy Oakley, Steven Julious, Lisa Hampson</dc:creator>
    </item>
    <item>
      <title>Accurate, scalable, and efficient Bayesian Optimal Experimental Design with derivative-informed neural operators</title>
      <link>https://arxiv.org/abs/2312.14810</link>
      <description>arXiv:2312.14810v2 Announce Type: replace-cross 
Abstract: We consider optimal experimental design (OED) problems in selecting the most informative observation sensors to estimate model parameters in a Bayesian framework. Such problems are computationally prohibitive when the parameter-to-observable (PtO) map is expensive to evaluate, the parameters are high-dimensional, and the optimization for sensor selection is combinatorial and high-dimensional. To address these challenges, we develop an accurate, scalable, and efficient computational framework based on derivative-informed neural operators (DINOs). The derivative of the PtO map is essential for accurate evaluation of the optimality criteria of OED in our consideration. We take the key advantage of DINOs, a class of neural operators trained with derivative information, to achieve high approximate accuracy of not only the PtO map but also, more importantly, its derivative. Moreover, we develop scalable and efficient computation of the optimality criteria based on DINOs and propose a modified swapping greedy algorithm for its optimization. We demonstrate that the proposed method is scalable to preserve the accuracy for increasing parameter dimensions and achieves high computational efficiency, with an over 1000x speedup accounting for both offline construction and online evaluation costs, compared to high-fidelity Bayesian OED solutions for a three-dimensional nonlinear convection-diffusion-reaction example with tens of thousands of parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14810v2</guid>
      <category>cs.CE</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinwoo Go, Peng Chen</dc:creator>
    </item>
    <item>
      <title>CAP: A General Algorithm for Online Selective Conformal Prediction with FCR Control</title>
      <link>https://arxiv.org/abs/2403.07728</link>
      <description>arXiv:2403.07728v2 Announce Type: replace-cross 
Abstract: We study the problem of post-selection predictive inference in an online fashion. To avoid devoting resources to unimportant units, a preliminary selection of the current individual before reporting its prediction interval is common and meaningful in online predictive tasks. Since the online selection causes a temporal multiplicity in the selected prediction intervals, it is important to control the real-time false coverage-statement rate (FCR) which measures the overall miscoverage level. We develop a general framework named CAP (Calibration after Adaptive Pick) that performs an adaptive pick rule on historical data to construct a calibration set if the current individual is selected and then outputs a conformal prediction interval for the unobserved label. We provide tractable procedures for constructing the calibration set for popular online selection rules. We proved that CAP can achieve an exact selection-conditional coverage guarantee in the finite-sample and distribution-free regimes. To account for the distribution shift in online data, we also embed CAP into some recent dynamic conformal prediction algorithms and show that the proposed method can deliver long-run FCR control. Numerical results on both synthetic and real data corroborate that CAP can effectively control FCR around the target level and yield more narrowed prediction intervals over existing baselines across various settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07728v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yajie Bao, Yuyang Huo, Haojie Ren, Changliang Zou</dc:creator>
    </item>
  </channel>
</rss>
