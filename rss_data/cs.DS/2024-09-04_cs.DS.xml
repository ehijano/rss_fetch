<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 04 Sep 2024 04:01:27 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Preprocessing to Reduce the Search Space for Odd Cycle Transversal</title>
      <link>https://arxiv.org/abs/2409.00245</link>
      <description>arXiv:2409.00245v1 Announce Type: new 
Abstract: The NP-hard Odd Cycle Transversal problem asks for a minimum vertex set whose removal from an undirected input graph $G$ breaks all odd cycles, and thereby yields a bipartite graph. The problem is well-known to be fixed-parameter tractable when parameterized by the size $k$ of the desired solution. It also admits a randomized kernelization of polynomial size, using the celebrated matroid toolkit by Kratsch and Wahlstr\"{o}m. The kernelization guarantees a reduction in the total $\textit{size}$ of an input graph, but does not guarantee any decrease in the size of the solution to be sought; the latter governs the size of the search space for FPT algorithms parameterized by $k$. We investigate under which conditions an efficient algorithm can detect one or more vertices that belong to an optimal solution to Odd Cycle Transversal. By drawing inspiration from the popular $\textit{crown reduction}$ rule for Vertex Cover, and the notion of $\textit{antler decompositions}$ that was recently proposed for Feedback Vertex Set, we introduce a graph decomposition called $\textit{tight odd cycle cut}$ that can be used to certify that a vertex set is part of an optimal odd cycle transversal. While it is NP-hard to compute such a graph decomposition, we develop parameterized algorithms to find a set of at least $k$ vertices that belong to an optimal odd cycle transversal when the input contains a tight odd cycle cut certifying the membership of $k$ vertices in an optimal solution. The resulting algorithm formalizes when the search space for the solution-size parameterization of Odd Cycle Transversal can be reduced by preprocessing. To obtain our results, we develop a graph reduction step that can be used to simplify the graph to the point that the odd cycle cut can be detected via color coding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00245v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bart M. P. Jansen, Yosuke Mizutani, Blair D. Sullivan, Ruben F. A. Verhaegh</dc:creator>
    </item>
    <item>
      <title>Survey of Results on the ModPath and ModCycle Problems</title>
      <link>https://arxiv.org/abs/2409.00770</link>
      <description>arXiv:2409.00770v1 Announce Type: new 
Abstract: This note summarizes the state of what is known about the tractability of the problem ModPath, which asks if an input undirected graph contains a simple st-path whose length satisfies modulo constraints. We also consider the problem ModCycle, which asks for the existence of a simple cycle subject to such constraints. We also discuss the status of these problems on directed graphs, and on restricted classes of graphs. We explain connections to the problem variant asking for a constant vertex-disjoint number of such paths or cycles, and discuss links to other related work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00770v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antoine Amarilli</dc:creator>
    </item>
    <item>
      <title>Scalable Neighborhood Local Search for Single-Machine Scheduling with Family Setup Times</title>
      <link>https://arxiv.org/abs/2409.00771</link>
      <description>arXiv:2409.00771v1 Announce Type: new 
Abstract: In this work, we study the task of scheduling jobs on a single machine with sequence dependent family setup times under the goal of minimizing the makespan, that is, the completion time of the last job in the schedule. This notoriously NP-hard problem is highly relevant in practical productions and requires heuristics that provide good solutions quickly in order to deal with large instances. In this paper, we present a heuristic based on the approach of parameterized local search. That is, we aim to replace a given solution by a better solution having distance at most $k$ in a pre-defined distance measure. This is done multiple times in a hill-climbing manner, until a locally optimal solution is reached. We analyze the trade-off between the allowed distance $k$ and the algorithm's running time for four natural distance measures. Example of allowed operations for our considered distance measures are: swapping $k$ pairs of jobs in the sequence, or rearranging $k$ consecutive jobs. For two distance measures, we show that finding an improvement for given $k$ can be done in $f(k) \cdot n^{\mathcal{O}(1)}$ time, while such a running time for the other two distance measures is unlikely. We provide a preliminary experimental evaluation of our local search approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00771v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaja Balzereit, Niels Gr\"uttemeier, Nils Morawietz, Dennis Reinhardt, Stefan Windmann, Petra Wolf</dc:creator>
    </item>
    <item>
      <title>ExpoSort: Beating the quasi-polynomial-time barrier for reluctant sorting</title>
      <link>https://arxiv.org/abs/2409.00794</link>
      <description>arXiv:2409.00794v1 Announce Type: new 
Abstract: We introduce the algorithm ExpoSort, a groundbreaking method that sorts an array of $n$ numbers in a spectacularly inefficient $\Theta(2^n)$ time. ExpoSort proudly claims the title of the first reluctant algorithm to decisively surpass the quasi-polynomial running time $\Omega(n^{\log n/(2+\varepsilon)})$ of the notoriously sluggish SlowSort algorithm by Broder and Stolfi [ACM SIGACT News, 1984]. In the ongoing quest for the slowest possible sort, ExpoSort redefines what it means to take one's time.
  Remarkably, ExpoSort achieves this feat with one of the simplest pseudocodes among all known sorting algorithms. However, a slight modification -- merely moving one recursive call inside an if statement -- transforms ExpoSort into an astonishingly well-camouflaged variant of the classic InsertionSort with best- and worst-case running times of $\Theta(n)$ and $\Theta(n^3)$, respectively. This dual nature of ExpoSort serves as a reminder of the utmost care required when crafting pessimal algorithms, where a slight lapse in judgment could result in accidentally producing an embarrassingly practical algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00794v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mikkel Abrahamsen</dc:creator>
    </item>
    <item>
      <title>Fitting trees to $\ell_1$-hyperbolic distances</title>
      <link>https://arxiv.org/abs/2409.01010</link>
      <description>arXiv:2409.01010v1 Announce Type: new 
Abstract: Building trees to represent or to fit distances is a critical component of phylogenetic analysis, metric embeddings, approximation algorithms, geometric graph neural nets, and the analysis of hierarchical data. Much of the previous algorithmic work, however, has focused on generic metric spaces (i.e., those with no a priori constraints). Leveraging several ideas from the mathematical analysis of hyperbolic geometry and geometric group theory, we study the tree fitting problem as finding the relation between the hyperbolicity (ultrametricity) vector and the error of tree (ultrametric) embedding. That is, we define a vector of hyperbolicity (ultrametric) values over all triples of points and compare the $\ell_p$ norms of this vector with the $\ell_q$ norm of the distortion of the best tree fit to the distances. This formulation allows us to define the average hyperbolicity (ultrametricity) in terms of a normalized $\ell_1$ norm of the hyperbolicity vector. Furthermore, we can interpret the classical tree fitting result of Gromov as a $p = q = \infty$ result. We present an algorithm HCCRootedTreeFit such that the $\ell_1$ error of the output embedding is analytically bounded in terms of the $\ell_1$ norm of the hyperbolicity vector (i.e., $p = q = 1$) and that this result is tight. Furthermore, this algorithm has significantly different theoretical and empirical performance as compared to Gromov's result and related algorithms. Finally, we show using HCCRootedTreeFit and related tree fitting algorithms, that supposedly standard data sets for hierarchical data analysis and geometric graph neural networks have radically different tree fits than those of synthetic, truly tree-like data sets, suggesting that a much more refined analysis of these standard data sets is called for.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01010v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.MG</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Advances in Neural Information Processing Systems (2023) 7263-7288</arxiv:journal_reference>
      <dc:creator>Joon-Hyeok Yim, Anna C. Gilbert</dc:creator>
    </item>
    <item>
      <title>Differentially Private Kernel Density Estimation</title>
      <link>https://arxiv.org/abs/2409.01688</link>
      <description>arXiv:2409.01688v1 Announce Type: new 
Abstract: We introduce a refined differentially private (DP) data structure for kernel density estimation (KDE), offering not only improved privacy-utility tradeoff but also better efficiency over prior results. Specifically, we study the mathematical problem: given a similarity function $f$ (or DP KDE) and a private dataset $X \subset \mathbb{R}^d$, our goal is to preprocess $X$ so that for any query $y\in\mathbb{R}^d$, we approximate $\sum_{x \in X} f(x, y)$ in a differentially private fashion. The best previous algorithm for $f(x,y) =\| x - y \|_1$ is the node-contaminated balanced binary tree by [Backurs, Lin, Mahabadi, Silwal, and Tarnawski, ICLR 2024]. Their algorithm requires $O(nd)$ space and time for preprocessing with $n=|X|$. For any query point, the query time is $d \log n$, with an error guarantee of $(1+\alpha)$-approximation and $\epsilon^{-1} \alpha^{-0.5} d^{1.5} R \log^{1.5} n$.
  In this paper, we improve the best previous result [Backurs, Lin, Mahabadi, Silwal, and Tarnawski, ICLR 2024] in three aspects:
  - We reduce query time by a factor of $\alpha^{-1} \log n$.
  - We improve the approximation ratio from $\alpha$ to 1.
  - We reduce the error dependence by a factor of $\alpha^{-0.5}$.
  From a technical perspective, our method of constructing the search tree differs from previous work [Backurs, Lin, Mahabadi, Silwal, and Tarnawski, ICLR 2024]. In prior work, for each query, the answer is split into $\alpha^{-1} \log n$ numbers, each derived from the summation of $\log n$ values in interval tree countings. In contrast, we construct the tree differently, splitting the answer into $\log n$ numbers, where each is a smart combination of two distance values, two counting values, and $y$ itself. We believe our tree structure may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01688v1</guid>
      <category>cs.DS</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Erzhi Liu, Jerry Yao-Chieh Hu, Alex Reneau, Zhao Song, Han Liu</dc:creator>
    </item>
    <item>
      <title>Gradient-Free Method for Heavily Constrained Nonconvex Optimization</title>
      <link>https://arxiv.org/abs/2409.00459</link>
      <description>arXiv:2409.00459v1 Announce Type: cross 
Abstract: Zeroth-order (ZO) method has been shown to be a powerful method for solving the optimization problem where explicit expression of the gradients is difficult or infeasible to obtain. Recently, due to the practical value of the constrained problems, a lot of ZO Frank-Wolfe or projected ZO methods have been proposed. However, in many applications, we may have a very large number of nonconvex white/black-box constraints, which makes the existing zeroth-order methods extremely inefficient (or even not working) since they need to inquire function value of all the constraints and project the solution to the complicated feasible set. In this paper, to solve the nonconvex problem with a large number of white/black-box constraints, we proposed a doubly stochastic zeroth-order gradient method (DSZOG) with momentum method and adaptive step size. Theoretically, we prove DSZOG can converge to the $\epsilon$-stationary point of the constrained problem. Experimental results in two applications demonstrate the superiority of our method in terms of training time and accuracy compared with other ZO methods for the constrained problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00459v1</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>International Conference on Machine Learning. PMLR, 2022: 19935-19955</arxiv:journal_reference>
      <dc:creator>Wanli Shi, Hongchang Gao, Bin Gu</dc:creator>
    </item>
    <item>
      <title>Smarter k-Partitioning of ZX-Diagrams for Improved Quantum Circuit Simulation</title>
      <link>https://arxiv.org/abs/2409.00828</link>
      <description>arXiv:2409.00828v1 Announce Type: cross 
Abstract: We introduce a novel method for strong classical simulation of quantum circuits based on optimally k-partitioning ZX-diagrams, reducing each part individually, and then efficiently cross-referencing their results to conclude the overall probability amplitude of the original circuit. We then analyse how this method fares against the alternatives for circuits of various size, shape, and interconnectedness and demonstrate how it is often liable to outperform those alternatives in speed by orders of magnitude.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00828v1</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Sutcliffe</dc:creator>
    </item>
    <item>
      <title>Rapid GPU-Based Pangenome Graph Layout</title>
      <link>https://arxiv.org/abs/2409.00876</link>
      <description>arXiv:2409.00876v1 Announce Type: cross 
Abstract: Computational Pangenomics is an emerging field that studies genetic variation using a graph structure encompassing multiple genomes. Visualizing pangenome graphs is vital for understanding genome diversity. Yet, handling large graphs can be challenging due to the high computational demands of the graph layout process.
  In this work, we conduct a thorough performance characterization of a state-of-the-art pangenome graph layout algorithm, revealing significant data-level parallelism, which makes GPUs a promising option for compute acceleration. However, irregular data access and the algorithm's memory-bound nature present significant hurdles. To overcome these challenges, we develop a solution implementing three key optimizations: a cache-friendly data layout, coalesced random states, and warp merging. Additionally, we propose a quantitative metric for scalable evaluation of pangenome layout quality.
  Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution achieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline without layout quality loss, reducing execution time from hours to minutes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00876v1</guid>
      <category>cs.DC</category>
      <category>cs.CE</category>
      <category>cs.DS</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jiajie Li, Jan-Niklas Schmelzle, Yixiao Du, Simon Heumos, Andrea Guarracino, Giulia Guidi, Pjotr Prins, Erik Garrison, Zhiru Zhang</dc:creator>
    </item>
    <item>
      <title>A computational transition for detecting correlated stochastic block models by low-degree polynomials</title>
      <link>https://arxiv.org/abs/2409.00966</link>
      <description>arXiv:2409.00966v1 Announce Type: cross 
Abstract: Detection of correlation in a pair of random graphs is a fundamental statistical and computational problem that has been extensively studied in recent years. In this work, we consider a pair of correlated (sparse) stochastic block models $\mathcal{S}(n,\tfrac{\lambda}{n};k,\epsilon;s)$ that are subsampled from a common parent stochastic block model $\mathcal S(n,\tfrac{\lambda}{n};k,\epsilon)$ with $k=O(1)$ symmetric communities, average degree $\lambda=O(1)$, divergence parameter $\epsilon$, and subsampling probability $s$.
  For the detection problem of distinguishing this model from a pair of independent Erd\H{o}s-R\'enyi graphs with the same edge density $\mathcal{G}(n,\tfrac{\lambda s}{n})$, we focus on tests based on \emph{low-degree polynomials} of the entries of the adjacency matrices, and we determine the threshold that separates the easy and hard regimes. More precisely, we show that this class of tests can distinguish these two models if and only if $s&gt; \min \{ \sqrt{\alpha}, \frac{1}{\lambda \epsilon^2} \}$, where $\alpha\approx 0.338$ is the Otter's constant and $\frac{1}{\lambda \epsilon^2}$ is the Kesten-Stigum threshold. Our proof of low-degree hardness is based on a conditional variant of the low-degree likelihood calculation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00966v1</guid>
      <category>math.PR</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guanyi Chen, Jian Ding, Shuyang Gong, Zhangsong Li</dc:creator>
    </item>
    <item>
      <title>The NTU Partitioned Matching Game for International Kidney Exchange Programs</title>
      <link>https://arxiv.org/abs/2409.01452</link>
      <description>arXiv:2409.01452v1 Announce Type: cross 
Abstract: Motivated by the real-world problem of international kidney exchange (IKEP), [Bir\'o et al., Generalized Matching Games for International Kidney Exchange, 2019] introduced a generalized transferable utility matching game featuring a partition of the vertex set of a graph into players, and analyzed its complexity. We explore the non-transferable utility (NTU) variant of the game, where the utility of players is given by the number of their matched vertices. The NTU version is arguably a more natural model of the international kidney exchange program, as the utility of a participating country mostly depends on how many of its patients receive a kidney, which is non-transferable by nature. We study the core of this game, which suitably captures the notion of stability of an IKEP, as it precludes incentives to deviate from the proposed solution for any possible coalition of the players.
  We prove computational complexity results about the weak and strong cores under various assumptions on the players. In particular, we show that if every player has two vertices, which can be considered as an NTU matching game with couples, then the weak core is always non-empty, and the existence of a strong core solution can be decided in polynomial time. In contrast, it is NP-hard to decide whether the strong core is empty when each player has three vertices. We also show that if the number of players is constant, then the non-emptiness of the weak and strong cores is polynomial-time decidable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01452v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gergely Cs\'aji, Tam\'as Kir\'aly, Zsuzsa M\'esz\'aros-Karkus</dc:creator>
    </item>
    <item>
      <title>Weakly Leveled Planarity with Bounded Span</title>
      <link>https://arxiv.org/abs/2409.01889</link>
      <description>arXiv:2409.01889v1 Announce Type: cross 
Abstract: This paper studies planar drawings of graphs in which each vertex is represented as a point along a sequence of horizontal lines, called levels, and each edge is either a horizontal segment or a strictly $y$-monotone curve. A graph is $s$-span weakly leveled planar if it admits such a drawing where the edges have span at most $s$; the span of an edge is the number of levels it touches minus one. We investigate the problem of computing $s$-span weakly leveled planar drawings from both the computational and the combinatorial perspectives. We prove the problem to be para-NP-hard with respect to its natural parameter $s$ and investigate its complexity with respect to widely used structural parameters. We show the existence of a polynomial-size kernel with respect to vertex cover number and prove that the problem is FPT when parameterized by treedepth. We also present upper and lower bounds on the span for various graph classes.
  Notably, we show that cycle trees, a family of $2$-outerplanar graphs generalizing Halin graphs, are $\Theta(\log n)$-span weakly leveled planar and $4$-span weakly leveled planar when $3$-connected. As a byproduct of these combinatorial results, we obtain improved bounds on the edge-length ratio of the graph families under consideration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01889v1</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Bekos, Giordano Da Lozzo, Fabrizio Frati, Siddharth Gupta, Philipp Kindermann, Giuseppe Liotta, Ignaz Rutter, Ioannis G. Tollis</dc:creator>
    </item>
    <item>
      <title>Quantum Algorithms for One-Sided Crossing Minimization</title>
      <link>https://arxiv.org/abs/2409.01942</link>
      <description>arXiv:2409.01942v1 Announce Type: cross 
Abstract: We present singly-exponential quantum algorithms for the One-Sided Crossing Minimization (OSCM) problem. Given an $n$-vertex bipartite graph $G=(U,V,E\subseteq U \times V)$, a $2$-level drawing $(\pi_U,\pi_V)$ of $G$ is described by a linear ordering $\pi_U: U \leftrightarrow \{1,\dots,|U|\}$ of $U$ and linear ordering $\pi_V: V \leftrightarrow \{1,\dots,|V|\}$ of $V$. For a fixed linear ordering $\pi_U$ of $U$, the OSCM problem seeks to find a linear ordering $\pi_V$ of $V$ that yields a $2$-level drawing $(\pi_U,\pi_V)$ of $G$ with the minimum number of edge crossings. We show that OSCM can be viewed as a set problem over $V$ amenable for exact algorithms with a quantum speedup with respect to their classical counterparts. First, we exploit the quantum dynamic programming framework of Ambainis et al. [Quantum Speedups for Exponential-Time Dynamic Programming Algorithms. SODA 2019] to devise a QRAM-based algorithm that solves OSCM in $O^*(1.728^n)$ time and space. Second, we use quantum divide and conquer to obtain an algorithm that solves OSCM without using QRAM in $O^*(2^n)$ time and polynomial space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01942v1</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Susanna Caroppo, Giordano Da Lozzo, Giuseppe Di Battista</dc:creator>
    </item>
    <item>
      <title>Sorting and Ranking of Self-Delimiting Numbers with Applications to Outerplanar Graph Isomorphism</title>
      <link>https://arxiv.org/abs/2002.07287</link>
      <description>arXiv:2002.07287v4 Announce Type: replace 
Abstract: Assume that an $N$-bit sequence $S$ of $k$ numbers encoded as Elias gamma codes is given as input. We present space-efficient algorithms for sorting, dense ranking and competitive ranking on $S$ in the word RAM model with word size $\Omega(\log N)$ bits. Our algorithms run in $O(k + \frac{N}{\log N})$ time and use $O(N)$ bits. The sorting algorithm returns the given numbers in sorted order, stored within a bit-vector of $N$ bits, whereas our ranking algorithms construct data structures that allow us subsequently to return the dense/competitive rank of each number $x$ in $S$ in constant time. For numbers $x \in \mathbb{N}$ with $x &gt; N$ we require the position $p_x$ of $x$ as the input for our dense-/competitive-rank data structure. As an application of our algorithms above we give an algorithm for tree isomorphism, which runs in $O(n)$ time and uses $O(n)$ bits on $n$-node trees. Finally, we generalize our result for tree isomorphism to forests and outerplanar graphs, while maintaining a space-usage of $O(n)$ bits. The previous best linear-time algorithms for trees, forests and outerplanar graph isomorphism all use $\Theta(n \log n)$ bits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2002.07287v4</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Frank Kammer, Johannes Meintrup, Andrej Sajenko</dc:creator>
    </item>
    <item>
      <title>A Refined Laser Method and Faster Matrix Multiplication</title>
      <link>https://arxiv.org/abs/2010.05846</link>
      <description>arXiv:2010.05846v2 Announce Type: replace 
Abstract: The complexity of matrix multiplication is measured in terms of $\omega$, the smallest real number such that two $n\times n$ matrices can be multiplied using $O(n^{\omega+\epsilon})$ field operations for all $\epsilon&gt;0$; the best bound until now is $\omega&lt;2.37287$ [Le Gall'14]. All bounds on $\omega$ since 1986 have been obtained using the so-called laser method, a way to lower-bound the `value' of a tensor in designing matrix multiplication algorithms. The main result of this paper is a refinement of the laser method that improves the resulting value bound for most sufficiently large tensors. Thus, even before computing any specific values, it is clear that we achieve an improved bound on $\omega$, and we indeed obtain the best bound on $\omega$ to date: $$\omega &lt; 2.37286.$$ The improvement is of the same magnitude as the improvement that [Le Gall'14] obtained over the previous bound [Vassilevska W.'12]. Our improvement to the laser method is quite general, and we believe it will have further applications in arithmetic complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2010.05846v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>math.CO</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.46298/theoretics.24.21</arxiv:DOI>
      <arxiv:journal_reference>TheoretiCS, Volume 3 (2024), Article 21, 1-32</arxiv:journal_reference>
      <dc:creator>Josh Alman, Virginia Vassilevska Williams</dc:creator>
    </item>
    <item>
      <title>Dynamic Boundary Time Warping for Sub-sequence Matching with Few Examples</title>
      <link>https://arxiv.org/abs/2010.14464</link>
      <description>arXiv:2010.14464v2 Announce Type: replace 
Abstract: The paper presents a novel method of finding a fragment in a long temporal sequence similar to the set of shorter sequences. We are the first to propose an algorithm for such a search that does not rely on computing the average sequence from query examples. Instead, we use query examples as is, utilizing all of them simultaneously. The introduced method based on the Dynamic Time Warping (DTW) technique is suited explicitly for few-shot query-by-example retrieval tasks. We evaluate it on two different few-shot problems from the field of Natural Language Processing. The results show it either outperforms baselines and previous approaches or achieves comparable results when a low number of examples is available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2010.14464v2</guid>
      <category>cs.DS</category>
      <category>cs.CL</category>
      <category>cs.IR</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>{\L}ukasz Borchmann, Dawid Jurkiewicz, Filip Grali\'nski, Tomasz G\'orecki</dc:creator>
    </item>
    <item>
      <title>On the Advice Complexity of Online Unit Clustering</title>
      <link>https://arxiv.org/abs/2309.14730</link>
      <description>arXiv:2309.14730v2 Announce Type: replace 
Abstract: In online unit clustering a set of n points of a metric space that arrive one by one, partition the points into clusters of diameter at most one, so that number of clusters is minimized. This paper gives linear upper and lower bounds for the advice complexity of 1-competitive online unit clustering algorithms in terms of number of points in $\mathbb{R}^d$ and $\mathbb{Z}^d$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.14730v2</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Judit Nagy-Gy\"orgy</dc:creator>
    </item>
    <item>
      <title>Simpler Optimal Sorting from a Directed Acyclic Graph</title>
      <link>https://arxiv.org/abs/2407.21591</link>
      <description>arXiv:2407.21591v4 Announce Type: replace 
Abstract: Fredman proposed in 1976 the following algorithmic problem: Given are a ground set $X$, some partial order $P$ over $X$, and some comparison oracle $O_L$ that specifies a linear order $L$ over $X$ that extends $P$. A query to $O_L$ has as input distinct $x, x' \in X$ and outputs whether $x &lt;_L x'$ or vice versa. If we denote by $e(P)$ the number of linear extensions of $P$, then $\log e(P)$ is a worst-case lower bound on the number of queries needed to output the sorted order of $X$.
  Fredman did not specify in what form the partial order is given. Haeupler, Hlad\'ik, Iacono, Rozhon, Tarjan, and T\v{e}tek ('24) propose to assume as input a directed acyclic graph, $G$, with $m$ edges and $n=|X|$ vertices. Denote by $P_G$ the partial order induced by $G$. Algorithmic performance is measured in running time and the number of queries used, where they use $\Theta(m + n + \log e(P_G))$ time and $\Theta(\log e(P_G))$ queries to output $X$ in its sorted order. Their algorithm is worst-case optimal in terms of running time and queries, both. Their algorithm combines topological sorting with heapsort. Their analysis relies upon sophisticated counting arguments using entropy, recursively defined sets defined over the run of their algorithm, and vertices in the graph that they identify as bottlenecks for sorting.
  In this paper, we do away with sophistication. We show that when the input is a directed acyclic graph then the problem admits a simple solution using $\Theta(m + n + \log e(P_G))$ time and $\Theta(\log e(P_G))$ queries. Especially our proofs are much simpler as we avoid the usage of advanced charging arguments and data structures, and instead rely upon two brief observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21591v4</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ivor van der Hoog, Eva Rotenberg, Daniel Rutschmann</dc:creator>
    </item>
    <item>
      <title>From Amortized to Worst Case Delay in Enumeration Algorithms</title>
      <link>https://arxiv.org/abs/2108.10208</link>
      <description>arXiv:2108.10208v2 Announce Type: replace-cross 
Abstract: The quality of enumeration algorithms is often measured by their delay, that is, the maximal time spent between the output of two distinct solutions. If the goal is to enumerate $t$ distinct solutions for any given $t$, then another relevant measure is the maximal time needed to output $t$ solutions divided by $t$, a notion we call the amortized delay of the algorithm, since it can be seen as the amortized complexity of the problem of enumerating $t$ elements in the set. In this paper, we study the relation between these two notions of delay, showing different schemes allowing one to transform an algorithm with polynomial amortized delay for which one has a blackbox access into an algorithm with polynomial delay. We complement our results by providing several lower bounds and impossibility theorems in the blackbox model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2108.10208v2</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florent Capelli, Yann Strozecki</dc:creator>
    </item>
    <item>
      <title>Scheduling Servers with Stochastic Bilinear Rewards</title>
      <link>https://arxiv.org/abs/2112.06362</link>
      <description>arXiv:2112.06362v3 Announce Type: replace-cross 
Abstract: We address a control system optimization problem that arises in multi-class, multi-server queueing system scheduling with uncertainty. In this scenario, jobs incur holding costs while awaiting completion, and job-server assignments yield observable stochastic rewards with unknown mean values. The rewards for job-server assignments are assumed to follow a bilinear model with respect to features characterizing jobs and servers. Our objective is regret minimization, aiming to maximize the cumulative reward of job-server assignments over a time horizon while maintaining a bounded total job holding cost, thus ensuring queueing system stability. This problem is motivated by applications in computing services and online platforms. To address this problem, we propose a scheduling algorithm based on weighted proportional fair allocation criteria augmented with marginal costs for reward maximization, incorporating a bandit strategy. Our algorithm achieves sub-linear regret and sub-linear mean holding cost (and queue length bound) with respect to the time horizon, thus guaranteeing queueing system stability. Additionally, we establish stability conditions for distributed iterative algorithms for computing allocations, which are relevant to large-scale system applications. Finally, we validate the efficiency of our algorithm through numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.06362v3</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jung-hun Kim, Milan Vojnovic</dc:creator>
    </item>
    <item>
      <title>A Reduction from Chores Allocation to Job Scheduling</title>
      <link>https://arxiv.org/abs/2302.04581</link>
      <description>arXiv:2302.04581v4 Announce Type: replace-cross 
Abstract: We consider allocating indivisible chores among agents with different cost functions, such that all agents receive a cost of at most a constant factor times their maximin share. The state-of-the-art was presented in In EC 2021 by Huang and Lu. They presented a non-polynomial-time algorithm, called HFFD, that attains an 11/9 approximation, and a polynomial-time algorithm that attains a 5/4 approximation.
  In this paper, we show that HFFD can be reduced to an algorithm called MultiFit, developed by Coffman, Garey and Johnson in 1978 for makespan minimization in job scheduling. Using this reduction, we prove that the approximation ratio of HFFD is in fact equal to that of MultiFit, which is known to be 13/11 in general, 20/17 for n at most 7, and 15/13 for n=3.
  Moreover, we develop an algorithm for (13/11+epsilon)-maximin-share allocation for any epsilon&gt;0, with run-time polynomial in the problem size and 1/epsilon. For n=3, we can improve the algorithm to find a 15/13-maximin-share allocation with run-time polynomial in the problem size. Thus, we have practical algorithms that attain the best known approximation to maximin-share chore allocation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.04581v4</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Huang, Erel Segal-Halevi</dc:creator>
    </item>
    <item>
      <title>Half-space separation in monophonic convexity</title>
      <link>https://arxiv.org/abs/2404.17564</link>
      <description>arXiv:2404.17564v3 Announce Type: replace-cross 
Abstract: We study half-space separation in the convexity of chordless paths of a graph, i.e., monophonic convexity. In this problem, one is given a graph and two (disjoint) subsets of vertices and asks whether these two sets can be separated by complementary convex sets, called half-spaces. While it is known this problem is $\mathbf{NP}$-complete for geodesic convexity -- the convexity of shortest paths -- we show that it can be solved in polynomial time for monophonic convexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17564v3</guid>
      <category>math.CO</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammed Elaroussi, Lhouari Nourine, Simon Vilmin</dc:creator>
    </item>
    <item>
      <title>Fast Estimation of Percolation Centrality</title>
      <link>https://arxiv.org/abs/2408.02389</link>
      <description>arXiv:2408.02389v3 Announce Type: replace-cross 
Abstract: In this work, we present a new algorithm to approximate the percolation centrality of every node in a graph. Such a centrality measure quantifies the importance of the vertices in a network during a contagious process. In this paper, we present a randomized approximation algorithm that can compute probabilistically guaranteed high-quality percolation centrality estimates, generalizing techniques used by Pellegrina and Vandin (TKDD 2024) for the betweenness centrality. The estimation obtained by our algorithm is within $\varepsilon$ of the value with probability at least $1-\delta$, for fixed constants $\varepsilon,\delta \in (0,1)$. We our theoretical results with an extensive experimental analysis on several real-world networks and provide empirical evidence that our algorithm improves the current state of the art in speed, and sample size while maintaining high accuracy of the percolation centrality estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02389v3</guid>
      <category>cs.SI</category>
      <category>cs.DS</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Antonio Cruciani</dc:creator>
    </item>
    <item>
      <title>kendallknight: An R Package for Efficient Implementation of Kendall's Correlation Coefficient Computation</title>
      <link>https://arxiv.org/abs/2408.09618</link>
      <description>arXiv:2408.09618v4 Announce Type: replace-cross 
Abstract: The kendallknight package introduces an efficient implementation of Kendall's correlation coefficient computation, significantly improving the processing time for large datasets without sacrificing accuracy. The kendallknight package, following Knight (1966) and posterior literature, reduces the computational complexity resulting in drastic reductions in computation time, transforming operations that would take minutes or hours into milliseconds or minutes, while maintaining precision and correctly handling edge cases and errors. The package is particularly advantageous in econometric and statistical contexts where rapid and accurate calculation of Kendall's correlation coefficient is desirable. Benchmarks demonstrate substantial performance gains over the base R implementation, especially for large datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09618v4</guid>
      <category>stat.CO</category>
      <category>cs.DS</category>
      <category>econ.EM</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mauricio Vargas Sep\'ulveda</dc:creator>
    </item>
  </channel>
</rss>
