<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SC</link>
    <description>cs.SC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 29 Jul 2025 04:01:47 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Recycling Algebraic Proof Certificates</title>
      <link>https://arxiv.org/abs/2507.20267</link>
      <description>arXiv:2507.20267v1 Announce Type: new 
Abstract: Proof certificates can be used to validate the correctness of algebraic derivations. However, in practice, we frequently observed that the exact same proof steps are repeated for different sets of variables, which leads to unnecessarily large proofs. To overcome this issue we extend the existing Practical Algebraic Calculus with linear combinations (LPAC) with two new proof rules that allow us to capture and reuse parts of the proof to derive a more condensed proof certificate. We integrate these rules into the proof checker Pacheck 2.0. Our experimental results demonstrate that the proposed extension helps to reduce both proof size and verification time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20267v1</guid>
      <category>cs.SC</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniela Kaufmann, Clemens Hofstadler</dc:creator>
    </item>
    <item>
      <title>Smith normal forms of bivariate polynomial matrices</title>
      <link>https://arxiv.org/abs/2507.20889</link>
      <description>arXiv:2507.20889v1 Announce Type: new 
Abstract: In 1978, Frost and Storey asserted that a bivariate polynomial matrix is equivalent to its Smith normal form if and only if the reduced minors of all orders generate the unit ideal. In this paper, we first demonstrate by constructing an example that for any given positive integer s with s &gt;= 2, there exists a square bivariate polynomial matrix M with the degree of det(M) in y equal to s, for which the condition that reduced minors of all orders generate the unit ideal is not a sufficient condition for M to be equivalent to its Smith normal form. Subsequently, we prove that for any square bivariate polynomial matrix M where the degree of det(M) in y is at most 1, Frost and Storey's assertion holds. Using the Quillen-Suslin theorem, we further extend our consideration of M to rank-deficient and non-square cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20889v1</guid>
      <category>cs.SC</category>
      <category>math.RA</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dong Lu, Dingkang Wang, Fanghui Xiao, Xiaopeng Zheng</dc:creator>
    </item>
    <item>
      <title>Speaking in Words, Thinking in Logic: A Dual-Process Framework in QA Systems</title>
      <link>https://arxiv.org/abs/2507.20491</link>
      <description>arXiv:2507.20491v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) have significantly enhanced question-answering (QA) capabilities, particularly in open-domain contexts. However, in closed-domain scenarios such as education, healthcare, and law, users demand not only accurate answers but also transparent reasoning and explainable decision-making processes. While neural-symbolic (NeSy) frameworks have emerged as a promising solution, leveraging LLMs for natural language understanding and symbolic systems for formal reasoning, existing approaches often rely on large-scale models and exhibit inefficiencies in translating natural language into formal logic representations.
  To address these limitations, we introduce Text-JEPA (Text-based Joint-Embedding Predictive Architecture), a lightweight yet effective framework for converting natural language into first-order logic (NL2FOL). Drawing inspiration from dual-system cognitive theory, Text-JEPA emulates System 1 by efficiently generating logic representations, while the Z3 solver operates as System 2, enabling robust logical inference. To rigorously evaluate the NL2FOL-to-reasoning pipeline, we propose a comprehensive evaluation framework comprising three custom metrics: conversion score, reasoning score, and Spearman rho score, which collectively capture the quality of logical translation and its downstream impact on reasoning accuracy.
  Empirical results on domain-specific datasets demonstrate that Text-JEPA achieves competitive performance with significantly lower computational overhead compared to larger LLM-based systems. Our findings highlight the potential of structured, interpretable reasoning frameworks for building efficient and explainable QA systems in specialized domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20491v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.SC</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tuan Bui, Trong Le, Phat Thai, Sang Nguyen, Minh Hua, Ngan Pham, Thang Bui, Tho Quan</dc:creator>
    </item>
    <item>
      <title>A Unified Reduction for Hypergeometric and q-Hypergeometric Creative Telescoping</title>
      <link>https://arxiv.org/abs/2501.03837</link>
      <description>arXiv:2501.03837v2 Announce Type: replace 
Abstract: We adapt the theory of normal and special polynomials from symbolic integration to the summation setting, and then built up a general framework embracing both the usual shift case and the $q$-shift case. In the context of this general framework, we develop a unified reduction algorithm, and subsequently a creative telescoping algorithm, applicable to both hypergeometric terms and their $q$-analogues. Our algorithms allow to split up the usual shift case and the $q$-shift case only when it is really necessary, and thus instantly reveal the intrinsic differences between these two cases. Computational experiments are also provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03837v2</guid>
      <category>cs.SC</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s11139-025-01164-w</arxiv:DOI>
      <dc:creator>Shaoshi Chen, Hao Du, Yiman Gao, Hui Huang, Ziming Li</dc:creator>
    </item>
    <item>
      <title>A Novel Approach to the Initial Value Problem with a complete validated algorithm</title>
      <link>https://arxiv.org/abs/2502.00503</link>
      <description>arXiv:2502.00503v3 Announce Type: replace 
Abstract: We consider the first order autonomous differential equation (ODE) ${\bf x}'={\bf f}({\bf x})$ where ${\bf f}: {\mathbb R}^n\to{\mathbb R}^n$ is locally Lipschitz. For ${\bf x}_0\in{\mathbb R}^n$ and $h&gt;0$, the initial value problem (IVP) for $({\bf f},{\bf x}_0,h)$ is to determine if there is a unique solution, i.e., a function ${\bf x}:[0,h]\to{\mathbb R}^n$ that satisfies the ODE with ${\bf x}(0)={\bf x}_0$. Write ${\bf x} ={\tt IVP}_{\bf f}({\bf x}_0,h)$ for this unique solution.
  We pose a corresponding computational problem, called the End Enclosure Problem: given $({\bf f},B_0,h,\varepsilon_0)$ where $B_0\subseteq{\mathbb R}^n$ is a box and $\varepsilon_0&gt;0$, to compute a pair of non-empty boxes $(\underline{B}_0,B_1)$ such that $\underline{B}_0\subseteq B_0$, width of $B_1$ is $&lt;\varepsilon_0$, and for all ${\bf x}_0\in \underline{B}_0$, ${\bf x}={\tt IVP}_{\bf f}({\bf x}_0,h)$ exists and ${\bf x}(h)\in B_1$. We provide a algorithm for this problem. Under the assumption (promise) that for all ${\bf x}_0\in B_0$, ${\tt IVP}_{\bf f}({\bf x}_0,h)$ exists, we prove the halting of our algorithm. This is the first halting algorithm for IVP problems in such a general setting.
  We also introduce novel techniques for subroutines such as StepA and StepB, and a scaffold datastructure to support our End Enclosure algorithm. Among the techniques are new ways refine full- and end-enclosures based on a {\bf radical transform} combined with logarithm norms. Our preliminary implementation and experiments show considerable promise, and compare well with current algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00503v3</guid>
      <category>cs.SC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bingwei Zhang, Chee Yap</dc:creator>
    </item>
  </channel>
</rss>
