<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 18 Dec 2025 02:34:47 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Codifference as a measure of dispersion and dependence for mixture models</title>
      <link>https://arxiv.org/abs/2512.13928</link>
      <description>arXiv:2512.13928v1 Announce Type: new 
Abstract: Codifference is a commonly used measure of dependence for stable vectors and processes for which covariance is infinite. However, we argue that it can also be used for other heavy-tail distributions and it provides useful information for other non-Gaussian distributions as well, no matter the tails. Motivated by this, we analyse codifference using as little assumptions as possible about the studied model. It leads us to propose its natural domain and three natural variants of it. Using the wide class of variable scale mixture distributions we argue that the codifference can be interpreted as the measure of bulk properties which ignores the tails much more than the covariance. It can also detect forms of non-linear memory which covariance cannot. Finally, we show the asymptotic distribution of its estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13928v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jakub \'Sl\k{e}zak</dc:creator>
    </item>
    <item>
      <title>Extreme Mass Distributions For K-Increasing Quasi-Copulas</title>
      <link>https://arxiv.org/abs/2512.14062</link>
      <description>arXiv:2512.14062v1 Announce Type: new 
Abstract: The rating of quasi-copula problems in the dependence modeling community has recently risen in spite of the lack of probability interpretation of quasi-copulas. The trendsetting paper J.J. Arias-Garcia, R. Mesiar, and B. De Baets, The unwalked path between quasi-copulas and copulas: Stepping stones in higher dimensions, Internat. J. of Approx. Reasoning, 80 (2017) 89--99, proposes the k-increasing property for some k {\le} d as a property of d-variate quasi-copulas that would shed some light on what is in-between. This hierarchy of classes extends the bivariate notion of supermodularity property. The same authors propose a number of open problems in the continuation of this paper (Fuzzy Sets and Systems 393 (2020), 1--28). Their Open problem 5 asks for the extreme values of the mass distributions associated with multivariate quasi-copulas and was recently solved by the authors of this paper (Fuzzy Sets and Systems 527 (2026) 109698). The main goal of the present paper is to solve the maximal-volume problem (in absolute value) within each of the previously mentioned subclasses. By formulating and solving suitably simplified primal and dual linear programs, we derive the exact maximal negative and positive masses together with the corresponding extremal boxes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14062v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matja\v{z} Omladi\v{c}, Martin Vuk, Alja\v{z} Zalar</dc:creator>
    </item>
    <item>
      <title>The Cost of Adaptation under Differential Privacy: Optimal Adaptive Federated Density Estimation</title>
      <link>https://arxiv.org/abs/2512.14337</link>
      <description>arXiv:2512.14337v1 Announce Type: new 
Abstract: Privacy-preserving data analysis has become a central challenge in modern statistics. At the same time, a long-standing goal in statistics is the development of adaptive procedures -- methods that achieve near-optimal performance across diverse function classes without prior knowledge of underlying smoothness or complexity. While adaptation is often achievable at no extra cost in the classical non-private setting, this naturally raises a fundamental question: to what extent is adaptation still possible under privacy constraints?
  We address this question in the context of density estimation under federated differential privacy (FDP), a framework that encompasses both central and local DP models. We establish sharp results that characterize the cost of adaptation under FDP for both global and pointwise estimation, revealing fundamental differences from the non-private case. We then propose an adaptive FDP estimator that achieves explicit performance guarantees by introducing a new noise mechanism, enabling one-shot adaptation via post-processing. This approach strictly improves upon existing adaptive DP methods. Finally, we develop new lower bound techniques that capture the limits of adaptive inference under privacy and may be of independent interest beyond this problem.
  Our findings reveal a sharp contrast between private and non-private settings. For global estimation, where adaptation can be achieved for free in the classical non-private setting, we prove that under FDP an intrinsic adaptation cost is unavoidable. For pointwise estimation, where a logarithmic penalty is already known to arise in the non-private setting, we show that FDP introduces an additional logarithmic factor, thereby compounding the cost of adaptation. Taken together, these results provide the first rigorous characterization of the adaptive privacy-accuracy trade-off.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14337v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>T. Tony Cai, Abhinav Chakraborty, Lasse Vuursteen</dc:creator>
    </item>
    <item>
      <title>Sharp convergence rates for Spectral methods via the feature space decomposition method</title>
      <link>https://arxiv.org/abs/2512.14473</link>
      <description>arXiv:2512.14473v1 Announce Type: new 
Abstract: In this paper, we apply the Feature Space Decomposition (FSD) method developed in [LS24, GLS25, ALSS26] to obtain, under fairly general conditions, matching upper and lower bounds for the population excess risk of spectral methods in linear regression under the squared loss, for every covariance and every signal. This result enables us, for a given linear regression problem, to define a partial order on the set of spectral methods according to their convergence rates, thereby characterizing which spectral algorithm is superior for that specific problem. Furthermore, this allows us to generalize the saturation effect proposed in inverse problems and to provide necessary and sufficient conditions for its occurrence. Our method also shows that, under broad conditions, any spectral algorithm lacks a feature learning property, and therefore cannot overcome the barrier of the information exponent in problems such as single-index learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14473v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guillaume Lecu\'e, Zhifan Li, Zong Shang</dc:creator>
    </item>
    <item>
      <title>Cluster expansion of the log-likelihood ratio: Optimal detection of planted matchings</title>
      <link>https://arxiv.org/abs/2512.14567</link>
      <description>arXiv:2512.14567v1 Announce Type: new 
Abstract: To understand how hidden information can be extracted from statistical networks, planted models in random graphs have been the focus of intensive study in recent years. In this work, we consider the detection of a planted matching, i.e., an independent edge set, hidden in an Erd\H{o}s-R\'enyi random graph, which is formulated as a hypothesis testing problem. We identify the critical regime for this testing problem and prove that the log-likelihood ratio is asymptotically normal. Via analyses of computationally efficient edge or wedge count test statistics that attain the optimal limits of detection, our results also reveal the absence of a statistical-to-computational gap. Our main technical tool is the cluster expansion from statistical physics, which allows us to prove a precise, non-asymptotic characterization of the log-likelihood ratio. Our analyses rely on a careful reorganization and cancellation of terms that occur in the difference between monomer-dimer log partition functions on the complete and Erd\H{o}s-R\'enyi graphs. This combinatorial and statistical physics approach represents a significant departure from the more established methods such as orthogonal decompositions, and positions the cluster expansion as a viable technique in the study of log-likelihood ratios for planted models in general.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14567v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Timothy L. H. Wee, Cheng Mao</dc:creator>
    </item>
    <item>
      <title>Learning the score under shape constraints</title>
      <link>https://arxiv.org/abs/2512.14624</link>
      <description>arXiv:2512.14624v1 Announce Type: new 
Abstract: Score estimation has recently emerged as a key modern statistical challenge, due to its pivotal role in generative modelling via diffusion models. Moreover, it is an essential ingredient in a new approach to linear regression via convex $M$-estimation, where the corresponding error densities are projected onto the log-concave class. Motivated by these applications, we study the minimax risk of score estimation with respect to squared $L^2(P_0)$-loss, where $P_0$ denotes an underlying log-concave distribution on $\mathbb{R}$. Such distributions have decreasing score functions, but on its own, this shape constraint is insufficient to guarantee a finite minimax risk. We therefore define subclasses of log-concave densities that capture two fundamental aspects of the estimation problem. First, we establish the crucial impact of tail behaviour on score estimation by determining the minimax rate over a class of log-concave densities whose score function exhibits controlled growth relative to the quantile levels. Second, we explore the interplay between smoothness and log-concavity by considering the class of log-concave densities with a scale restriction and a $(\beta,L)$-H\"older assumption on the log-density for some $\beta \in [1,2]$. We show that the minimax risk over this latter class is of order $L^{2/(2\beta+1)}n^{-\beta/(2\beta+1)}$ up to poly-logarithmic factors, where $n$ denotes the sample size. When $\beta &lt; 2$, this rate is faster than could be obtained under either the shape constraint or the smoothness assumption alone. Our upper bounds are attained by a locally adaptive, multiscale estimator constructed from a uniform confidence band for the score function. This study highlights intriguing differences between the score estimation and density estimation problems over this shape-constrained class.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14624v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rebecca M. Lewis, Oliver Y. Feng, Henry W. J. Reeve, Min Xu, Richard J. Samworth</dc:creator>
    </item>
    <item>
      <title>Understanding statistics for biomedical research through the lens of replication</title>
      <link>https://arxiv.org/abs/2512.13763</link>
      <description>arXiv:2512.13763v1 Announce Type: cross 
Abstract: Clinicians and scientists have traditionally focussed on whether their findings will be replicated and are very familiar with the concept. The probability that a replication study yields an effect with the same sign, or the same statistical significance as an original study depends on the sum of the variances of the effect estimates. On this basis, when P equals 0.025 one-sided and the replication study has the same sample size and variance as the original study, the probability of achieving a one-sided P is less than or equal to 0.025 a second time is only about 0.283, consistent with currently observed modest replication rates. A higher replication probability would require a larger sample size than that derived from current single variance power calculations. However, if the replication study is based on an infinitely large sample size and thus has negligible variance then the probability that its estimated mean is same sign is 1 - P = 0.975. The reasoning is made clearer by changing continuous distributions to discretised scales and probability masses, thus avoiding ambiguity and improper flat priors. This perspective is consistent with Frequentist and Bayesian interpretations and also requires further reasoning when testing scientific hypotheses and making decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13763v1</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huw Llewelyn</dc:creator>
    </item>
    <item>
      <title>Bayesian Global-Local Regularization</title>
      <link>https://arxiv.org/abs/2512.13992</link>
      <description>arXiv:2512.13992v1 Announce Type: cross 
Abstract: We propose a unified framework for global-local regularization that bridges the gap between classical techniques -- such as ridge regression and the nonnegative garotte -- and modern Bayesian hierarchical modeling. By estimating local regularization strengths via marginal likelihood under order constraints, our approach generalizes Stein's positive-part estimator and provides a principled mechanism for adaptive shrinkage in high-dimensional settings. We establish that this isotonic empirical Bayes estimator achieves near-minimax risk (up to logarithmic factors) over sparse ordered model classes, constituting a significant advance in high-dimensional statistical inference. Applications to orthogonal polynomial regression demonstrate the methodology's flexibility, while our theoretical results clarify the connections between empirical Bayes, shape-constrained estimation, and degrees-of-freedom adjustments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13992v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Jyotishka Datta, Nick Polson, Vadim Sokolov</dc:creator>
    </item>
    <item>
      <title>Maximum Mean Discrepancy with Unequal Sample Sizes via Generalized U-Statistics</title>
      <link>https://arxiv.org/abs/2512.13997</link>
      <description>arXiv:2512.13997v1 Announce Type: cross 
Abstract: Existing two-sample testing techniques, particularly those based on choosing a kernel for the Maximum Mean Discrepancy (MMD), often assume equal sample sizes from the two distributions. Applying these methods in practice can require discarding valuable data, unnecessarily reducing test power. We address this long-standing limitation by extending the theory of generalized U-statistics and applying it to the usual MMD estimator, resulting in new characterization of the asymptotic distributions of the MMD estimator with unequal sample sizes (particularly outside the proportional regimes required by previous partial results). This generalization also provides a new criterion for optimizing the power of an MMD test with unequal sample sizes. Our approach preserves all available data, enhancing test accuracy and applicability in realistic settings. Along the way, we give much cleaner characterizations of the variance of MMD estimators, revealing something that might be surprising to those in the area: while zero MMD implies a degenerate estimator, it is sometimes possible to have a degenerate estimator with nonzero MMD as well; we give a construction and a proof that it does not happen in common situations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13997v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aaron Wei, Milad Jalali, Danica J. Sutherland</dc:creator>
    </item>
    <item>
      <title>Robust estimation of parameters in logistic regression via solving the Cramer-von Mises type L2 optimization problem</title>
      <link>https://arxiv.org/abs/1703.07044</link>
      <description>arXiv:1703.07044v5 Announce Type: replace 
Abstract: This paper proposes a novel method to estimate parameters in a logistic regression model. After obtaining the estimators, their asymptotic properties are rigorously investigated.</description>
      <guid isPermaLink="false">oai:arXiv.org:1703.07044v5</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiwoong Kim</dc:creator>
    </item>
    <item>
      <title>Decision-making with possibilistic inferential models</title>
      <link>https://arxiv.org/abs/2112.13247</link>
      <description>arXiv:2112.13247v2 Announce Type: replace 
Abstract: Inferential models (IMs) are data-dependent, imprecise-probabilistic structures designed to quantify uncertainty about unknowns. As the name suggests, the focus has been on uncertainty quantification for inference and on its reliability properties in that context. Focusing on a likelihood-based possibilistic IM formulation, the present paper develops a corresponding framework for decision making, and investigates the decision-theoretic implications of the IM's reliability guarantees. Here we show that the possibilistic IM's assessment of an action's quality, defined by a simple Choquet integral, tends not be too optimistic compared to that of an oracle. This ensures that the IM tends not to favor actions that the oracle doesn't also favor, hence the IM is also reliable for decision making. We also establish a complementary, large-sample efficiency result that says the IM's reliability isn't achieved by being grossly conservative. In the special case of equivariant statistical models, further connections can be made between the IM's and Bayesian's recommended actions, from which certain optimality conclusions can be drawn.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.13247v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan Martin, Shih-Ni Prim, Jonathan Williams</dc:creator>
    </item>
    <item>
      <title>Minimax Rates of Estimation for Optimal Transport Map between Infinite-Dimensional Spaces</title>
      <link>https://arxiv.org/abs/2505.13570</link>
      <description>arXiv:2505.13570v3 Announce Type: replace 
Abstract: We investigate the estimation of an optimal transport map between probability measures on an infinite-dimensional space and reveal its minimax optimal rate. Optimal transport theory defines distances within a space of probability measures, utilizing an optimal transport map as its key component. Estimating the optimal transport map from samples finds several applications, such as simulating dynamics between probability measures and functional data analysis. However, some transport maps on infinite-dimensional spaces require exponential-order data for estimation, which undermines their applicability. In this paper, we investigate the estimation of an optimal transport map between infinite-dimensional spaces, focusing on optimal transport maps characterized by the notion of $\gamma$-smoothness. Consequently, we show that the order of the minimax risk is polynomial rate in the sample size even in the infinite-dimensional setup. We also develop an estimator whose estimation error matches the minimax optimal rate. With these results, we obtain a class of reasonably estimable optimal transport maps on infinite-dimensional spaces and a method for their estimation. Our experiments validate the theory and practical utility of our approach with application to functional data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13570v3</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Donlapark Ponnoprat, Masaaki Imaizumi</dc:creator>
    </item>
    <item>
      <title>Exclusivity Classes and Partitions of Loss Functions</title>
      <link>https://arxiv.org/abs/2507.12447</link>
      <description>arXiv:2507.12447v3 Announce Type: replace 
Abstract: Loss functions determine what it means for an estimator to be optimal, yet the ways in which different losses impose structurally incompatible optimality requirements are not captured by existing decision-theoretic frameworks. This paper develops a general theory of such incompatibilities by introducing \emph{exclusivity regions}, \emph{exclusivity classes}, and \emph{exclusivity partitions} of the loss space relative to an abstract optimality operator $\mathcal{O}$. An exclusivity region is a subset of losses such that no single estimator can be $\mathcal{O}$-optimal for a loss inside the region and a loss outside it; exclusivity classes additionally require realizability by at least one optimal estimator, and exclusivity partitions provide a global decomposition of a loss family into disjoint exclusivity regions (or classes, if the partition is realizable). We establish basic structural properties of these objects, including the role of conic geometry and invariance of optimality under positive scaling, which allows partitions on normalized representatives to extend along rays in loss cones.
  The framework is illustrated through three fully formal, nontrivial realizable exclusivity partitions under Bayes risk optimality: asymmetric linear (quantile) losses, convex margin-based classification losses and Huber-type robust regression losses. We also formulate an open conjecture on minimax exclusivity for power-type losses and discuss connections to elicitation theory and to topological regularity properties of losses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12447v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stanis{\l}aw M. S. Halkiewicz</dc:creator>
    </item>
    <item>
      <title>Self-Normalized Concentration Inequalities of Marginal Mean with Sample Variance Only</title>
      <link>https://arxiv.org/abs/2512.01817</link>
      <description>arXiv:2512.01817v4 Announce Type: replace 
Abstract: (This is the third version of a working paper.) We develop a family of self-normalized concentration inequalities for marginal mean under martingale-difference structure and $\phi/\tilde{\phi}$-mixing conditions, where the latter includes many processes that are not strongly mixing. The variance term is fully data-observable: naive sample variance in the martingale case and an empirical block long-run variance under mixing conditions. Thus, no predictable variance proxy is required. No specific assumption on the decay of the mixing coefficients (e.g. summability) is needed for the validity. The constants are explicit and the bounds are ready to use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01817v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zihao Yuan</dc:creator>
    </item>
    <item>
      <title>Adaptive almost full recovery in sparse nonparametric models</title>
      <link>https://arxiv.org/abs/2512.10488</link>
      <description>arXiv:2512.10488v2 Announce Type: replace 
Abstract: We observe an unknown function of $d$ variables $f(\boldsymbol{t})$, $\boldsymbol{t} \in[0,1]^d$, in the Gaussian white noise model of intensity $\varepsilon&gt;0$. We assume that the function $f$ is regular and that it is a sum of $k$-variate functions, where $k$ varies from $1$ to $s$ ($1\leq s\leq d$). These functions are unknown to us and only a few of them are nonzero. In this article, we address the problem of identifying the nonzero function components of $f$ almost fully in the case when $d=d_\varepsilon\to \infty$ as $\varepsilon\to 0$ and $s$ is either fixed or $s=s_\varepsilon\to \infty$, $s=o(d)$ as $\varepsilon\to 0$. This may be viewed as a variable selection problem. We derive the conditions when almost full variable selection in the model at hand is possible and provide a selection procedure that achieves this type of selection. The procedure is adaptive to the level of sparsity described by the sparsity index $\beta\in(0,1)$. We also derive conditions that make almost full variable selection in the model of our interest impossible. In view of these conditions, the proposed selector is seen to perform asymptotically optimal. The theoretical findings are illustrated numerically.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.10488v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Natalia Stepanova, Marie Turcicova, Xiang Zhao</dc:creator>
    </item>
    <item>
      <title>Improved Concentration for Mean Estimators via Shrinkage</title>
      <link>https://arxiv.org/abs/2512.12750</link>
      <description>arXiv:2512.12750v2 Announce Type: replace 
Abstract: We study a class of robust mean estimators $\widehat{\mu}$ obtained by adaptively shrinking the weights of sample points far from a base estimator $\widehat{\kappa}$. Given a data-dependent scaling factor $\widehat{\alpha}$ and a weighting function $w:[0, \infty) \to [0,1]$, we let $\widehat{\mu} = \widehat{\kappa} + \frac{1}{n}\sum_{i=1}^n(X_i - \widehat{\kappa})w(\widehat{\alpha}|X_i-\widehat{\kappa}|) $. We prove that, under mild assumptions over $w$, these estimators achieve stronger concentration bounds than the base estimate $\widehat{\kappa}$, including sub-Gaussian guarantees. This framework unifies and extends several existing approaches to robust mean estimation in $\mathbb{R}$. Through numerical experiments, we show that our shrinking approach translates to faster concentration, even for small sample sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12750v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ant\^onio Cat\~ao, Lucas Resende, Paulo Orenstein</dc:creator>
    </item>
    <item>
      <title>Theoretical Guarantees of Learning Ensembling Strategies with Applications to Time Series Forecasting</title>
      <link>https://arxiv.org/abs/2305.15786</link>
      <description>arXiv:2305.15786v4 Announce Type: replace-cross 
Abstract: Ensembling is among the most popular tools in machine learning (ML) due to its effectiveness in minimizing variance and thus improving generalization. Most ensembling methods for black-box base learners fall under the umbrella of "stacked generalization," namely training an ML algorithm that takes the inferences from the base learners as input. While stacking has been widely applied in practice, its theoretical properties are poorly understood. In this paper, we prove a novel result, showing that choosing the best stacked generalization from a (finite or finite-dimensional) family of stacked generalizations based on cross-validated performance does not perform "much worse" than the oracle best. Our result strengthens and significantly extends the results in Van der Laan et al. (2007). Inspired by the theoretical analysis, we further propose a particular family of stacked generalizations in the context of probabilistic forecasting, each one with a different sensitivity for how much the ensemble weights are allowed to vary across items, timestamps in the forecast horizon, and quantiles. Experimental results demonstrate the performance gain of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.15786v4</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hilaf Hasson, Danielle C. Maddix, Yuyang Wang, Gaurav Gupta, Youngsuk Park</dc:creator>
    </item>
    <item>
      <title>Symmetry Lie Algebras of Varieties with Applications to Algebraic Statistics</title>
      <link>https://arxiv.org/abs/2309.10741</link>
      <description>arXiv:2309.10741v5 Announce Type: replace-cross 
Abstract: The motivation for this paper is to detect when an irreducible projective variety V is not toric. We do this by analyzing a Lie group and a Lie algebra associated to V. If the dimension of V is strictly less than the dimension of the above mentioned objects, then V is not a toric variety. We provide an algorithm to compute the Lie algebra of an irreducible variety and use it to provide examples of non-toric statistical models in algebraic statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.10741v5</guid>
      <category>math.AG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Aida Maraj, Arpan Pal</dc:creator>
    </item>
    <item>
      <title>Semiparametric inference for impulse response functions using double/debiased machine learning</title>
      <link>https://arxiv.org/abs/2411.10009</link>
      <description>arXiv:2411.10009v2 Announce Type: replace-cross 
Abstract: We introduce a double/debiased machine learning estimator for the impulse response function in settings where a time series of interest is subjected to multiple discrete treatments, assigned over time, which can have a causal effect on future outcomes. The proposed estimator can rely on fully nonparametric relations between treatment and outcome variables, opening up the possibility to use flexible machine learning approaches to estimate impulse response functions. To this end, we extend the theory of double machine learning from an i.i.d. to a time series setting and show that the proposed estimator is consistent and asymptotically normally distributed at the parametric rate, allowing for semiparametric inference for dynamic effects in a time series setting. The properties of the estimator are validated numerically in finite samples by applying it to learn the impulse response function in the presence of serial dependence in both the confounder and observation innovation processes. We also illustrate the methodology empirically by applying it to the estimation of the effects of macroeconomic shocks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10009v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniele Ballinari, Alexander Wehrli</dc:creator>
    </item>
    <item>
      <title>Toric Multivariate Gaussian Models from Symmetries in a Tree</title>
      <link>https://arxiv.org/abs/2412.00895</link>
      <description>arXiv:2412.00895v2 Announce Type: replace-cross 
Abstract: Given a rooted tree $T$ on $n$ non-root leaves with colored and zeroed nodes, we construct a linear space $L_T$ of $n\times n$ symmetric matrices with constraints determined by the combinatorics of the tree. When $L_T$ represents the covariance matrices of a Gaussian model, it provides natural generalizations of Brownian motion tree (BMT) models in phylogenetics. When $L_T$ represents a space of concentration matrices of a Gaussian model, it gives certain colored Gaussian graphical models, which we refer to as BMT derived models. We investigate conditions under which the reciprocal variety $L_T^{-1}$ is toric. Relying on the birational isomorphism of the inverse matrix map, we show that if the BMT derived graph of $T$ is vertex-regular and a block graph, under the derived Laplacian transformation, $L_T^{-1}$ is the vanishing locus of a toric ideal. This ideal is given by the sum of the toric ideal of the Gaussian graphical model on the block graph, the toric ideal of the original BMT model, and binomial linear conditions coming from vertex-regularity. To this end, we provide monomial parametrizations for these toric models realized through paths among leaves in $T$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00895v2</guid>
      <category>math.AG</category>
      <category>math.CO</category>
      <category>math.ST</category>
      <category>q-bio.PE</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emma Cardwell, Aida Maraj, Alvaro Ribot</dc:creator>
    </item>
    <item>
      <title>Inference on the attractor spaces via functional approximation</title>
      <link>https://arxiv.org/abs/2502.06462</link>
      <description>arXiv:2502.06462v2 Announce Type: replace-cross 
Abstract: This paper discusses semiparametric inference on hypotheses on the cointegration and the attractor spaces for $I(1)$ linear processes with moderately large cross-sectional dimension. The approach is based on empirical canonical correlations and functional approximation of Brownian motions, and it can be applied both to the whole system and or to any set of linear combinations of it. The hypotheses of interest are cast in terms of the number of stochastic trends in specified subsystems, and inference is based either on selection criteria or on sequences of tests. This paper derives the limit distribution of these tests in the special one-dimensional case, and discusses asymptotic properties of the derived inference criteria for hypotheses on the attractor space for sequentially diverging sample size and number of basis elements in the functional approximation. Finite sample properties are analyzed via a Monte Carlo study and an empirical illustration on exchange rates is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06462v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Massimo Franchi, Paolo Paruolo</dc:creator>
    </item>
    <item>
      <title>Generalization performance of narrow one-hidden layer networks in the teacher-student setting</title>
      <link>https://arxiv.org/abs/2507.00629</link>
      <description>arXiv:2507.00629v3 Announce Type: replace-cross 
Abstract: Understanding the generalization abilities of neural networks for simple input-output distributions is crucial to account for their learning performance on real datasets. The classical teacher-student setting, where a network is trained from data obtained thanks to a label-generating teacher model, serves as a perfect theoretical test bed. In this context, a complete theoretical account of the performance of fully connected one-hidden layer networks in the presence of generic activation functions is lacking. In this work, we develop such a general theory for narrow networks, i.e. with a large number of hidden units, yet much smaller than the input dimension. Using methods from statistical physics, we provide closed-form expressions for the typical performance of both finite temperature (Bayesian) and empirical risk minimization estimators, in terms of a small number of summary statistics. In doing so, we highlight the presence of a transition where hidden neurons specialize when the number of samples is sufficiently large and proportional to the number of parameters of the network. Our theory accurately predicts the generalization error of neural networks trained on regression or classification tasks with either noisy full-batch gradient descent (Langevin dynamics) or full-batch gradient descent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00629v3</guid>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jean Barbier, Federica Gerace, Alessandro Ingrosso, Clarissa Lauditi, Enrico M. Malatesta, Gibbs Nwemadji, Rodrigo P\'erez Ortiz</dc:creator>
    </item>
    <item>
      <title>A fine-grained look at causal effects in causal spaces</title>
      <link>https://arxiv.org/abs/2512.11919</link>
      <description>arXiv:2512.11919v2 Announce Type: replace-cross 
Abstract: The notion of causal effect is fundamental across many scientific disciplines. Traditionally, quantitative researchers have studied causal effects at the level of variables; for example, how a certain drug dose (W) causally affects a patient's blood pressure (Y). However, in many modern data domains, the raw variables-such as pixels in an image or tokens in a language model-do not have the semantic structure needed to formulate meaningful causal questions. In this paper, we offer a more fine-grained perspective by studying causal effects at the level of events, drawing inspiration from probability theory, where core notions such as independence are first given for events and sigma-algebras, before random variables enter the picture. Within the measure-theoretic framework of causal spaces, a recently introduced axiomatisation of causality, we first introduce several binary definitions that determine whether a causal effect is present, as well as proving some properties of them linking causal effect to (in)dependence under an intervention measure. Further, we provide quantifying measures that capture the strength and nature of causal effects on events, and show that we can recover the common measures of treatment effect as special cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11919v2</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junhyung Park, Yuqing Zhou</dc:creator>
    </item>
  </channel>
</rss>
