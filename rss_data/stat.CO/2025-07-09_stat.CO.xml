<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 10 Jul 2025 01:33:37 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>hassediagrams:an R package that generates the Hasse diagram of the layout structure and the restricted layout structure</title>
      <link>https://arxiv.org/abs/2507.05949</link>
      <description>arXiv:2507.05949v1 Announce Type: new 
Abstract: With the advent of modern statistical software, complex experimental designs are now routinely employed in many areas of research. Failing to correctly identify the structure of the experimental design can lead to incorrect model selection and misleading inferences. This paper describes the hassediagrams package in R that determines the structure of the design, summarised by the layout structure, and generates a Hasse diagram of the layout structure. By considering the randomisation performed, in conjunction with the layout structure, a set of randomisation objects can be defined that form the restricted layout structure. This structure can also be visualised using a generalisation of the Hasse diagram. Objects in the restricted layout structure can be used to identify the terms to include in the statistical model. The use of the procedure thus ensures consistency of model selection due to the systematic approach taken to generate the model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05949v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Damianos Michaelides, Simon T. Bate, Marion J. Chatfield</dc:creator>
    </item>
    <item>
      <title>Permutations accelerate Approximate Bayesian Computation</title>
      <link>https://arxiv.org/abs/2507.06037</link>
      <description>arXiv:2507.06037v1 Announce Type: cross 
Abstract: Approximate Bayesian Computation (ABC) methods have become essential tools for performing inference when likelihood functions are intractable or computationally prohibitive. However, their scalability remains a major challenge in hierarchical or high-dimensional models. In this paper, we introduce permABC, a new ABC framework designed for settings with both global and local parameters, where observations are grouped into exchangeable compartments.
  Building upon the Sequential Monte Carlo ABC (ABC-SMC) framework, permABC exploits the exchangeability of compartments through permutation-based matching, significantly improving computational efficiency.
  We then develop two further, complementary sequential strategies: Over Sampling, which facilitates early-stage acceptance by temporarily increasing the number of simulated compartments, and Under Matching, which relaxes the acceptance condition by matching only subsets of the data.
  These techniques allow for robust and scalable inference even in high-dimensional regimes. Through synthetic and real-world experiments -- including a hierarchical Susceptible-Infectious-Recover model of the early COVID-19 epidemic across 94 French departments -- we demonstrate the practical gains in accuracy and efficiency achieved by our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06037v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antoine Luciano, Charly Andral, Christian P. Robert, Robin J. Ryder</dc:creator>
    </item>
    <item>
      <title>seMCD: Sequentially implemented Monte Carlo depth computation with statistical guarantees</title>
      <link>https://arxiv.org/abs/2507.06227</link>
      <description>arXiv:2507.06227v1 Announce Type: cross 
Abstract: Statistical depth functions provide center-outward orderings in spaces of dimension larger than one, where a natural ordering does not exist. The numerical evaluation of such depth functions can be computationally prohibitive, even for relatively low dimensions. We present a novel sequentially implemented Monte Carlo methodology for the computation of, theoretical and empirical, depth functions and related quantities (seMCD), that outputs an interval, a so-called seMCD-bucket, to which the quantity of interest belongs with a high probability prespecified by the user. For specific classes of depth functions, we adapt algorithms from sequential testing, providing finite-sample guarantees. For depth functions dependent on unknown distributions, we offer asymptotic guarantees using non-parametric statistical methods. In contrast to plain-vanilla Monte Carlo methodology the number of samples required in the algorithm is random but typically much smaller than standard choices suggested in the literature. The seMCD method can be applied to various depth functions, covering multivariate and functional spaces. We demonstrate the efficiency and reliability of our approach through empirical studies, highlighting its applicability in outlier or anomaly detection, classification, and depth region computation. In conclusion, the seMCD-algorithm can achieve accurate depth approximations with few Monte Carlo samples while maintaining rigorous statistical guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06227v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Felix Gnettner, Claudia Kirch, Alicia Nieto-Reyes</dc:creator>
    </item>
    <item>
      <title>Computationally efficient variational-like approximations of possibilistic inferential models</title>
      <link>https://arxiv.org/abs/2404.19224</link>
      <description>arXiv:2404.19224v3 Announce Type: replace 
Abstract: Inferential models (IMs) offer provably reliable, data-driven, possibilistic statistical inference. But despite the IM framework's theoretical and foundational advantages, efficient computation is a challenge. This paper presents a simple yet powerful numerical strategy for approximating the IM's possibility contour, or at least its $\alpha$-cut for a specified $\alpha \in (0,1)$. Our proposal starts with the specification of a parametric family that, in a certain sense, approximately covers the credal set associated with the IM's possibility measure. Akin to variational inference, we then propose to tune the parameters of that parametric family so that its $100(1-\alpha)\%$ credible set roughly matches the IM contour's $\alpha$-cut. This parametric $\alpha$-cut matching strategy implies a full approximation to the IM's possibility contour at a fraction of the computational cost associated with previous strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19224v3</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.ijar.2025.109506</arxiv:DOI>
      <arxiv:journal_reference>International Journal of Approximate Reasoning, volume 186, paper 109506, 2025</arxiv:journal_reference>
      <dc:creator>Leonardo Cella, Ryan Martin</dc:creator>
    </item>
    <item>
      <title>An efficient Monte Carlo method for valid prior-free possibilistic statistical inference</title>
      <link>https://arxiv.org/abs/2501.10585</link>
      <description>arXiv:2501.10585v3 Announce Type: replace 
Abstract: Inferential models (IMs) offer prior-free, Bayesian-like posterior degrees of belief designed for statistical inference, which feature a frequentist-like calibration property that ensures reliability of said inferences. The catch is that IMs' degrees of belief are possibilistic rather than probabilistic and, since the familiar Monte Carlo methods approximate probabilistic quantities, there are significant computational challenges associated with putting this framework into practice. The present paper overcomes these challenges by developing a new Monte Carlo method designed specifically to approximate the IM's possibilistic output. The proposal is based on a characterization of the possibilistic IM's credal set, which identifies the "best probabilistic approximation" of the IM as a mixture distribution that can be readily approximated and sampled from. These samples can then be transformed into an approximation of the possibilistic IM. Numerical results are presented highlighting the proposed approximation's accuracy and computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10585v3</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan Martin</dc:creator>
    </item>
    <item>
      <title>Static and Dynamic BART for Rank-Order Data</title>
      <link>https://arxiv.org/abs/2308.10231</link>
      <description>arXiv:2308.10231v4 Announce Type: replace-cross 
Abstract: Ranking lists are often provided at regular time intervals in a range of applications, including economics, sports, marketing, and politics. Most popular methods for rank-order data postulate a linear specification for the latent scores, which determine the observed ranks, and ignore the temporal dependence of the ranking lists. To address these issues, novel nonparametric static (ROBART) and autoregressive (ARROBART) models are developed, with latent scores defined as nonlinear Bayesian additive regression tree functions of covariates. To make inferences in the dynamic ARROBART model, closed-form filtering, predictive, and smoothing distributions for the latent time-varying scores are derived. These results are applied in a Gibbs sampler with data augmentation for posterior inference. The proposed methods are shown to outperform existing competitors in simulation studies, static data applications to electoral data, stated preferences for sushi and movies, and dynamic data applications to economic complexity rankings of countries and weekly pollster rankings of NCAA football teams.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.10231v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matteo Iacopini, Eoghan O'Neill, Luca Rossini</dc:creator>
    </item>
  </channel>
</rss>
