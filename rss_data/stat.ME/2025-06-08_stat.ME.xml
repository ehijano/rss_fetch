<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 09 Jun 2025 04:01:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Adaptive stable distribution and Hurst exponent by method of moments moving estimator for nonstationary time series</title>
      <link>https://arxiv.org/abs/2506.05354</link>
      <description>arXiv:2506.05354v1 Announce Type: new 
Abstract: Nonstationarity of real-life time series requires model adaptation. In classical approaches like ARMA-ARCH there is assumed some arbitrarily chosen dependence type. To avoid their bias, we will focus on novel more agnostic approach: moving estimator, which estimates parameters separately for every time $t$: optimizing $F_t=\sum_{\tau&lt;t} (1-\eta)^{t-\tau} \ln(\rho_\theta (x_\tau))$ local log-likelihood with exponentially weakening weights of the old values. In practice such moving estimates can be found by EMA (exponential moving average) of some parameters, like $m_p=E[|x-\mu|^p]$ absolute central moments, updated by $m_{p,t+1} = m_{p,t} + \eta (|x_t-\mu_t|^p-m_{p,t})$. We will focus here on its applications for alpha-Stable distribution, which also influences Hurst exponent, hence can be used for its adaptive estimation. Its application will be shown on financial data as DJIA time series - beside standard estimation of evolution of center $\mu$ and scale parameter $\sigma$, there is also estimated evolution of $\alpha$ parameter allowing to continuously evaluate market stability - tails having $\rho(x) \sim 1/|x|^{\alpha+1}$ behavior, controlling probability of potentially dangerous extreme events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05354v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jarek Duda</dc:creator>
    </item>
    <item>
      <title>Testing Hypotheses of Covariate Effects on Topics of Discourse</title>
      <link>https://arxiv.org/abs/2506.05570</link>
      <description>arXiv:2506.05570v1 Announce Type: new 
Abstract: We introduce an approach to topic modelling with document-level covariates that remains tractable in the face of large text corpora. This is achieved by de-emphasizing the role of parameter estimation in an underlying probabilistic model, assuming instead that the data come from a fixed but unknown distribution whose statistical functionals are of interest. We propose combining a convex formulation of non-negative matrix factorization with standard regression techniques as a fast-to-compute and useful estimate of such a functional. Uncertainty quantification can then be achieved by reposing non-parametric resampling methods on top of this scheme. This is in contrast to popular topic modelling paradigms, which posit a complex and often hard-to-fit generative model of the data. We argue that the simple, non-parametric approach advocated here is faster, more interpretable, and enjoys better inferential justification than said generative models. Finally, our methods are demonstrated with an application analysing covariate effects on discourse of flavours attributed to Canadian beers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05570v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriel Phelan, David A. Campbell</dc:creator>
    </item>
    <item>
      <title>Fusion of heterogeneous data for robust degradation prognostics</title>
      <link>https://arxiv.org/abs/2506.05882</link>
      <description>arXiv:2506.05882v1 Announce Type: new 
Abstract: Assessing the degradation state of an industrial asset first requires evaluating its current condition and then to project the forecast model trajectory to a predefined prognostic threshold, thereby estimating its remaining useful life (RUL). Depending on the available information, two primary categories of forecasting models may be used: physics-based simulation codes and datadriven (machine learning) approaches. Combining both modelling approaches may enhance prediction robustness, especially with respect to their individual uncertainties. This paper introduces a methodology for fusion of heterogeneous data in degradation prognostics. The proposed approach acts iteratively on a computer model's uncertain input variables by combining kernel-based sensitivity analysis for variable ranking with a Bayesian framework to inform the priors with the heterogeneous data. Additionally, we propose an integration of an aggregate surrogate modeling strategy for computationally expensive degradation simulation codes. The methodology updates the knowledge of the computer code input probabilistic model and reduces the output uncertainty. As an application, we illustrate this methodology on a toy model from crack propagation based on Paris law as well as a complex industrial clogging simulation model for nuclear power plant steam generators, where data is intermittently available over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05882v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Edgar Jaber (EDF R\&amp;D PRISME, CB, DATAFLOT), Emmanuel Remy (EDF R\&amp;D PRISME), Vincent Chabridon (EDF R\&amp;D PRISME), Mathilde Mougeot (ENSIIE, CB), Didier Lucor (DATAFLOT)</dc:creator>
    </item>
    <item>
      <title>Sequential Monte Carlo approximations of Wasserstein--Fisher--Rao gradient flows</title>
      <link>https://arxiv.org/abs/2506.05905</link>
      <description>arXiv:2506.05905v1 Announce Type: new 
Abstract: We consider the problem of sampling from a probability distribution $\pi$. It is well known that this can be written as an optimisation problem over the space of probability distribution in which we aim to minimise the Kullback--Leibler divergence from $\pi$. We consider several partial differential equations (PDEs) whose solution is a minimiser of the Kullback--Leibler divergence from $\pi$ and connect them to well-known Monte Carlo algorithms. We focus in particular on PDEs obtained by considering the Wasserstein--Fisher--Rao geometry over the space of probabilities and show that these lead to a natural implementation using importance sampling and sequential Monte Carlo. We propose a novel algorithm to approximate the Wasserstein--Fisher--Rao flow of the Kullback--Leibler divergence which empirically outperforms the current state-of-the-art.
  We study tempered versions of these PDEs obtained by replacing the target distribution with a geometric mixture of initial and target distribution and show that these do not lead to a convergence speed up.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05905v1</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesca R. Crucinio, Sahani Pathiraja</dc:creator>
    </item>
    <item>
      <title>Optimal designs for identifying effective doses in drug combination studies</title>
      <link>https://arxiv.org/abs/2506.05913</link>
      <description>arXiv:2506.05913v1 Announce Type: new 
Abstract: We consider the optimal design problem for identifying effective dose combinations within drug combination studies where the effect of the combination of two drugs is investigated. Drug combination studies are becoming increasingly important as they investigate potential interaction effects rather than the individual impacts of the drugs. In this situation, identifying effective dose combinations that yield a prespecified effect is of special interest. If nonlinear surface models are used to describe the dose combination-response relationship, these effective dose combinations result in specific contour lines of the fitted response model.
  We propose a novel design criterion that targets the precise estimation of these effective dose combinations. In particular, an optimal design minimizes the width of the confidence band of the contour lines of interest. Optimal design theory is developed for this problem, including equivalence theorems and efficiency bounds. The performance of the optimal design is illustrated in several examples modeling dose combination data by various nonlinear surface models. It is demonstrated that the proposed optimal design for identifying effective dose combinations yields a more precise estimation of the effective dose combinations than commonly used ray or factorial designs. This particularly holds true for a case study motivated by data from an oncological dose combination study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05913v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leonie Sch\"urmeyer, Ludger Sandig, Leonie Theresa Hezler, Bernd-Wolfgang Igl, Kirsten Schorning</dc:creator>
    </item>
    <item>
      <title>Yule-Walker Estimation for Functional Time Series in Hilbert Space</title>
      <link>https://arxiv.org/abs/2506.05922</link>
      <description>arXiv:2506.05922v1 Announce Type: new 
Abstract: Recent advances in data collection technologies have led to the widespread availability of functional data observed over time, often exhibiting strong temporal dependence. However, existing methodologies typically assume independence across functions or impose restrictive low-order dependence structures, limiting their ability to capture the full dynamics of functional time series. To address this gap, we investigate higher-order functional autoregressive (FAR) models in Hilbert spaces, focusing on the statistical challenges introduced by infinite dimensionality. A fundamental challenge arises from the ill-posedness of estimating autoregressive operators, which stems from the compactness of the autocovariance operator and the consequent unboundedness of its inverse. We propose a regularized Yule-Walker-type estimation procedure, grounded in Tikhonov regularization, to stabilize the estimation. Specializing to $L^2$ spaces, we derive explicit and computationally feasible estimators that parallel classical finite-dimensional methods. Within a unified theoretical framework, we study the asymptotic properties of the proposed estimators and predictors. Notably, while the regularized predictors attain asymptotic normality, the corresponding estimators of the autoregressive operators fail to converge weakly in distribution under the operator norm topology, due to the compactness of the autocovariance operator. We further analyze the mean squared prediction error (MSPE), decomposing it into components attributable to regularization bias, truncation, and estimation variance. This decomposition reveals the advantages of our approach over traditional linear truncation schemes. Extensive simulations and an application to high-frequency wearable sensor data demonstrate the practical utility and robustness of the proposed methodology in capturing complex temporal structures in functional time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05922v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ying Niu, Yuwei Zhao, Zhao Chen, Christina Dan Wang</dc:creator>
    </item>
    <item>
      <title>Permutation-Free High-Order Interaction Tests</title>
      <link>https://arxiv.org/abs/2506.05963</link>
      <description>arXiv:2506.05963v1 Announce Type: new 
Abstract: Kernel-based hypothesis tests offer a flexible, non-parametric tool to detect high-order interactions in multivariate data, beyond pairwise relationships. Yet the scalability of such tests is limited by the computationally demanding permutation schemes used to generate null approximations. Here we introduce a family of permutation-free high-order tests for joint independence and partial factorisations of $d$ variables. Our tests eliminate the need for permutation-based approximations by leveraging V-statistics and a novel cross-centring technique to yield test statistics with a standard normal limiting distribution under the null. We present implementations of the tests and showcase their efficacy and scalability through synthetic datasets. We also show applications inspired by causal discovery and feature selection, which highlight both the importance of high-order interactions in data and the need for efficient computational methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05963v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhaolu Liu, Robert L. Peach, Mauricio Barahona</dc:creator>
    </item>
    <item>
      <title>Bayesian variable selection in a Cox proportional hazards model with the "Sum of Single Effects" prior</title>
      <link>https://arxiv.org/abs/2506.06233</link>
      <description>arXiv:2506.06233v1 Announce Type: new 
Abstract: Motivated by genetic fine-mapping applications, we introduce a new approach to Bayesian variable selection regression (BVSR) for time-to-event (TTE) outcomes. This new approach is designed to deal with the specific challenges that arise in genetic fine-mapping, including: the presence of very strong correlations among the covariates, often exceeding 0.99; very large data sets containing potentially thousands of covariates and hundreds of thousands of samples. We accomplish this by extending the "Sum of Single Effects" (SuSiE) method to the Cox proportional hazards (CoxPH) model. We demonstrate the benefits of the new method, "CoxPH-SuSiE", over existing BVSR methods for TTE outcomes in simulated fine-mapping data sets. We also illustrate CoxPH-SuSiE on real data by fine-mapping asthma loci using data from UK Biobank. This fine-mapping identified 14 asthma risk SNPs in 8 asthma risk loci, among which 6 had strong evidence for being causal (posterior inclusion probability greater than 50%). Two of the 6 putatively causal variants are known to be pathogenic, and others lie within a genomic sequence that is known to regulate the expression of GATA3.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06233v1</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunqi Yang, Karl Tayeb, Peter Carbonetto, Xiaoyuan Zhong, Carole Ober, Matthew Stephens</dc:creator>
    </item>
    <item>
      <title>When Measurement Mediates the Effect of Interest</title>
      <link>https://arxiv.org/abs/2506.06267</link>
      <description>arXiv:2506.06267v1 Announce Type: new 
Abstract: Many health promotion strategies aim to improve reach into the target population and outcomes among those reached. For example, an HIV prevention strategy could expand the reach of risk screening and the delivery of biomedical prevention to persons with HIV risk. This setting creates a complex missing data problem: the strategy improves health outcomes directly and indirectly through expanded reach, while outcomes are only measured among those reached. To formally define the total causal effect in such settings, we use Counterfactual Strata Effects: causal estimands where the outcome is only relevant for a group whose membership is subject to missingness and/or impacted by the exposure. To identify and estimate the corresponding statistical estimand, we propose a novel extension of Two-Stage targeted minimum loss-based estimation (TMLE). Simulations demonstrate the practical performance of our approach as well as the limitations of existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06267v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Joy Zora Nakato, Janice Litunya, Brian Beesiga, Jane Kabami, James Ayieko, Moses R. Kamya, Gabriel Chamie, Laura B. Balzer</dc:creator>
    </item>
    <item>
      <title>Non-Heuristic Selection via Hybrid Regularized and Machine Learning Models for Insurance</title>
      <link>https://arxiv.org/abs/2506.05609</link>
      <description>arXiv:2506.05609v1 Announce Type: cross 
Abstract: In this study, machine learning models were tested to predict whether or not a customer of an insurance company would purchase a travel insurance product. For this purpose, secondary data provided by an open-source website that compiles databases from statistical modeling competitions were used. The dataset used presents approximately 2,700 records from an unidentified company in the tourism insurance sector. Initially, the feature engineering stage was carried out, which were selected through regularized models: Ridge, Lasso and Elastic-Net. In this phase, gains were observed not only in relation to dimensionality, but also in the maintenance of interpretative capacity, through the coefficients obtained. After this process, five classification models were evaluated (Random Forests, XGBoost, H2O GBM, LightGBM and CatBoost) separately and in a hybrid way with the previous regularized models, all these stages using the k-fold stratified cross-validation technique. The evaluations were conducted by traditional metrics, including AUC, precision, recall and F1 score. A very competitive hybrid model was obtained using CatBoost combined with Lasso feature selection, achieving an AUC of 0.861 and an F1 score of 0.808. These findings motivate us to present the effectiveness of using hybrid models as a way to obtain high predictive power and maintain the interpretability of the estimation process</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05609v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luciano Ribeiro Galv\~ao, Rafael de Andrade Moral</dc:creator>
    </item>
    <item>
      <title>Revisiting Whittaker-Henderson Smoothing</title>
      <link>https://arxiv.org/abs/2306.06932</link>
      <description>arXiv:2306.06932v4 Announce Type: replace 
Abstract: Introduced over a century ago, Whittaker-Henderson smoothing remains widely used by actuaries in constructing one-dimensional and two-dimensional experience tables for mortality, disability and other life insurance risks. In this paper, we reinterpret this smoothing technique within a modern statistical framework and address six practically relevant questions about its use. First, we adopt a Bayesian perspective on this method to construct credible intervals. Second, in the context of survival analysis, we clarify how to choose the observation and weight vectors by linking the smoothing technique to a maximum likelihood estimator. Third, we improve accuracy by relaxing the method's reliance on an implicit normal approximation. Fourth, we select the smoothing parameters by maximizing a marginal likelihood function. Fifth, we improve computational efficiency when dealing with numerous observation points and consequently parameters. Finally, we develop an extrapolation procedure that ensures consistency between estimated and predicted values through constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.06932v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guillaume Biessy (LPSM)</dc:creator>
    </item>
    <item>
      <title>The Exchangeability Assumption for Permutation Tests of Multiple Regression Models: Implications for Statistics and Data Science Educators</title>
      <link>https://arxiv.org/abs/2406.07756</link>
      <description>arXiv:2406.07756v2 Announce Type: replace 
Abstract: Permutation tests are a powerful and flexible approach to inference via resampling. As computational methods become more ubiquitous in the statistics curriculum, use of permutation tests has become more tractable. At the heart of the permutation approach is the exchangeability assumption, which determines the appropriate null sampling distribution. We explore the exchangeability assumption in the context of permutation tests for multiple linear regression models, including settings where the assumption is not tenable. Various permutation schemes for the multiple linear regression setting have been proposed and assessed in the literature. As has been demonstrated previously, in most settings, the choice of how to permute a multiple linear regression model does not materially change inferential conclusions with respect to Type I errors. However, some violations (e.g., when clustering is not appropriately accounted for) lead to issues with Type I error rates. Regardless, we believe that understanding (1) exchangeability in the multiple linear regression setting and also (2) how it relates to the null hypothesis of interest is valuable. We close with pedagogical recommendations for instructors who want to bring multiple linear regression permutation inference into their classroom as a way to deepen student understanding of resampling-based inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07756v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Johanna Hardin, Lauren Quesada, Julie Ye, Nicholas J. Horton</dc:creator>
    </item>
    <item>
      <title>Medical Knowledge Integration into Reinforcement Learning Algorithms for Dynamic Treatment Regimes</title>
      <link>https://arxiv.org/abs/2407.00364</link>
      <description>arXiv:2407.00364v2 Announce Type: replace 
Abstract: The goal of precision medicine is to provide individualized treatment at each stage of chronic diseases, a concept formalized by Dynamic Treatment Regimes (DTR). These regimes adapt treatment strategies based on decision rules learned from clinical data to enhance therapeutic effectiveness. Reinforcement Learning (RL) algorithms allow to determine these decision rules conditioned by individual patient data and their medical history. The integration of medical expertise into these models makes possible to increase confidence in treatment recommendations and facilitate the adoption of this approach by healthcare professionals and patients. In this work, we examine the mathematical foundations of RL, contextualize its application in the field of DTR, and present an overview of methods to improve its effectiveness by integrating medical expertise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00364v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1111/insr.12617</arxiv:DOI>
      <dc:creator>Sophia Yazzourh, Nicolas Savy, Philippe Saint-Pierre, Michael R. Kosorok</dc:creator>
    </item>
    <item>
      <title>Searching for local associations while controlling the false discovery rate</title>
      <link>https://arxiv.org/abs/2412.02182</link>
      <description>arXiv:2412.02182v2 Announce Type: replace 
Abstract: We introduce local conditional hypotheses that express how the relation between explanatory variables and outcomes changes across different contexts, described by covariates. By expanding upon the model-X knockoff filter, we show how to adaptively discover these local associations, all while controlling the false discovery rate. Our enhanced inferences can help explain sample heterogeneity and uncover interactions, making better use of the capabilities offered by modern machine learning models. Specifically, our method is able to leverage any model for the identification of data-driven hypotheses pertaining to different contexts. Then, it rigorously test these hypotheses without succumbing to selection bias. Importantly, our approach is efficient and does not require sample splitting. We demonstrate the effectiveness of our method through numerical experiments and by studying the genetic architecture of Waist-Hip-Ratio across different sexes in the UKBiobank.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02182v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Paula Gablenz, Matteo Sesia, Tianshu Sun, Chiara Sabatti</dc:creator>
    </item>
    <item>
      <title>A Bayesian Record Linkage Approach to Applications in Tree Demography Using Overlapping LiDAR Scans</title>
      <link>https://arxiv.org/abs/2501.13285</link>
      <description>arXiv:2501.13285v2 Announce Type: replace 
Abstract: In the information age, it has become increasingly common for data containing records about overlapping individuals to be distributed across multiple sources, making it necessary to identify which records refer to the same individual. The goal of record linkage is to estimate this unknown structure in the absence of a unique identifiable attribute. We introduce a Bayesian hierarchical record linkage model for spatial location data motivated by the estimation of individual specific growth-size curves for conifer species using data derived from overlapping LiDAR scans. Annual tree growth may be estimated dependent upon correctly identifying unique individuals across scans in the presence of noise. We formalize a two-stage modeling framework, connecting the record linkage model and a flexible downstream individual tree growth model, that provides robust uncertainty quantification and propagation through both stages of the modeling pipeline via an extension of the linkage-averaging approach of Sadinle (2018). In this paper, we discuss the two-stage model formulation, outline the computational strategies required to achieve scalability, assess the model performance on simulated data, and fit the model to a bi-temporal dataset derived from LiDAR scans of the Upper Gunnison Watershed provided by the Rocky Mountain Biological Laboratory to assess the impact of key topographic covariates on the growth behavior of conifer species in the Southern Rocky Mountains (USA).</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13285v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>L. Drew, A. Kaplan, I. Breckheimer</dc:creator>
    </item>
    <item>
      <title>Finding network effect of randomized treatment under weak assumptions for any outcome and any effect heterogeneity</title>
      <link>https://arxiv.org/abs/2501.15814</link>
      <description>arXiv:2501.15814v2 Announce Type: replace 
Abstract: In estimating the effects of a treatment/policy with a network, an unit is subject to two types of treatment: one is the direct treatment on the unit itself, and the other is the indirect treatment (i.e., network/spillover influence) through the treated units among the friends/neighbors of the unit. In the literature, linear models are widely used where either the number of the treated neighbors or the proportion of them among the neighbors represents the intensity of the indirect treatment. In this paper, we obtain a nonparametric network-based "causal reduced form (CRF)" that allows any outcome variable (binary, count, continuous, ...) and any effect heterogeneity. Then we assess those popular linear models through the lens of the CRF. This reveals what kind of restrictive assumptions are embedded in those models, and how the restrictions can result in biases. With the CRF, we conduct almost model-free estimation and inference for network effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15814v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bora Kim, Myoung-jae Lee</dc:creator>
    </item>
    <item>
      <title>Orthogonal Arrays: A Review</title>
      <link>https://arxiv.org/abs/2505.15032</link>
      <description>arXiv:2505.15032v2 Announce Type: replace 
Abstract: Orthogonal arrays are arguably one of the most fascinating and important statistical tools for efficient data collection. They have a simple, natural definition, desirable properties when used as fractional factorials, and a rich and beautiful mathematical theory. Their connections with combinatorics, finite fields, geometry, and error-correcting codes are profound. Orthogonal arrays have been widely used in agriculture, engineering, manufacturing, and high-technology industries for quality and productivity improvement experiments. In recent years, they have drawn rapidly growing interest from various fields such as computer experiments, integration, visualization, optimization, big data, machine learning/artificial intelligence through successful applications in those fields. We review the fundamental concepts and statistical properties and report recent developments. Discussions of recent applications and connections with various fields are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15032v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1002/wics.70029</arxiv:DOI>
      <dc:creator>C. Devon Lin, John Stufken</dc:creator>
    </item>
    <item>
      <title>Testing for Replicated Signals Across Multiple Studies with Side Information</title>
      <link>https://arxiv.org/abs/2505.15328</link>
      <description>arXiv:2505.15328v2 Announce Type: replace 
Abstract: Partial conjunction (PC) $p$-values and side information provided by covariates can be used to detect signals that replicate across multiple studies investigating the same set of features, all while controlling the false discovery rate (FDR). However, when many features are present, the extent of multiplicity correction required for FDR control, along with the inherently limited power of PC $p$-values$\unicode{x2013}$especially when replication across all studies is demanded$\unicode{x2013}$often inhibits the number of discoveries made. To address this problem, we develop a $p$-value-based covariate-adaptive methodology that revolves around partitioning studies into smaller groups and borrowing information between them to filter out unpromising features. This filtering strategy: 1) reduces the multiplicity correction required for FDR control, and 2) allows independent hypothesis weights to be trained on data from filtered-out features to enhance the power of the PC $p$-values in the rejection rule. Our methodology has finite-sample FDR control under minimal distributional assumptions, and we demonstrate its competitive performance through simulation studies and a real-world case study on gene expression and the immune system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15328v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ninh Tran, Dennis Leung</dc:creator>
    </item>
    <item>
      <title>A Novel Criterion for Interpreting Acoustic Emission Damage Signals Based on Cluster Onset Distribution</title>
      <link>https://arxiv.org/abs/2312.13416</link>
      <description>arXiv:2312.13416v3 Announce Type: replace-cross 
Abstract: Structural health monitoring (SHM) relies on non-destructive techniques such as acoustic emission (AE) that generate large amounts of data over the lifespan of systems. Clustering methods are used to interpret these data and gain insights into damage progression and mechanisms. Conventional methods for evaluating clustering results utilise clustering validity indices (CVI) that prioritise compact and separable clusters. This paper introduces a novel approach based on the temporal sequence of cluster onsets, indicating the initial appearance of potential damage and allowing for early detection of defect initiation. The proposed CVI is based on the Kullback-Leibler divergence and can incorporate prior information about damage onsets when available. Three experiments on real-world datasets validate the effectiveness of the proposed method. The first benchmark focuses on detecting the loosening of bolted plates under vibration, where the onset-based CVI outperforms the conventional approach in both cluster quality and the accuracy of bolt loosening detection. The results demonstrate not only superior cluster quality but also unmatched precision in identifying cluster onsets, whether during uniform or accelerated damage growth. The two additional applications stem from industrial contexts. The first focuses on micro-drilling of hard materials using electrical discharge machining, demonstrating, for the first time, that the proposed criterion can effectively retrieve electrode progression to the reference depth, thus validating the setting of the machine to ensure structural integrity. The final application involves damage understanding in a composite/metal hybrid joint structure, where the cluster timeline is used to establish a scenario leading to critical failure due to slippage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.13416v3</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emmanuel Ramasso, Martin Mbarga Nkogo, Neha Chandarana, Gilles Bourbon, Patrice Le Moal, Quentin Lefebvre, Martial Personeni, Constantinos Soutis, Matthieu Gresil, S\'ebastien Thibaud</dc:creator>
    </item>
    <item>
      <title>Longitudinal Targeted Minimum Loss-based Estimation with Temporal-Difference Heterogeneous Transformer</title>
      <link>https://arxiv.org/abs/2404.04399</link>
      <description>arXiv:2404.04399v2 Announce Type: replace-cross 
Abstract: We propose Deep Longitudinal Targeted Minimum Loss-based Estimation (Deep LTMLE), a novel approach to estimate the counterfactual mean of outcome under dynamic treatment policies in longitudinal problem settings. Our approach utilizes a transformer architecture with heterogeneous type embedding trained using temporal-difference learning. After obtaining an initial estimate using the transformer, following the targeted minimum loss-based likelihood estimation (TMLE) framework, we statistically corrected for the bias commonly associated with machine learning algorithms. Furthermore, our method also facilitates statistical inference by enabling the provision of 95% confidence intervals grounded in asymptotic statistical theory. Simulation results demonstrate our method's superior performance over existing approaches, particularly in complex, long time-horizon scenarios. It remains effective in small-sample, short-duration contexts, matching the performance of asymptotically efficient estimators. To demonstrate our method in practice, we applied our method to estimate counterfactual mean outcomes for standard versus intensive blood pressure management strategies in a real-world cardiovascular epidemiology cohort study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04399v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the 41st International Conference on Machine Learning, PMLR 235:45097-45113, 2024</arxiv:journal_reference>
      <dc:creator>Toru Shirakawa, Yi Li, Yulun Wu, Sky Qiu, Yuxuan Li, Mingduo Zhao, Hiroyasu Iso, Mark van der Laan</dc:creator>
    </item>
    <item>
      <title>Likelihood distortion and Bayesian local robustness</title>
      <link>https://arxiv.org/abs/2405.15141</link>
      <description>arXiv:2405.15141v3 Announce Type: replace-cross 
Abstract: Robust Bayesian analysis has been mainly devoted to detecting and measuring robustness w.r.t. the prior distribution. Many contributions in the literature aim to define suitable classes of priors which allow the computation of variations of quantities of interest while the prior changes within those classes. The literature has devoted much less attention to the robustness of Bayesian methods w.r.t. the likelihood function due to mathematical and computational complexity, and because it is often arguably considered a more objective choice compared to the prior. In this contribution, we propose a new approach to Bayesian local robustness, mainly focusing on robustness w.r.t. the likelihood function. Successively, we extend it to account for robustness w.r.t. the prior, as well as the prior and the likelihood jointly. This approach is based on the notion of distortion function introduced in the literature on risk theory. The novel robustness measure is a local sensitivity measure that turns out to be very tractable and easy to compute for several classes of distortion functions. Asymptotic properties are derived, and numerical experiments illustrate the theory and its applicability for modelling purposes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15141v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antonio Di Noia, Fabrizio Ruggeri, Antonietta Mira</dc:creator>
    </item>
    <item>
      <title>Distributional Matrix Completion via Nearest Neighbors in the Wasserstein Space</title>
      <link>https://arxiv.org/abs/2410.13112</link>
      <description>arXiv:2410.13112v2 Announce Type: replace-cross 
Abstract: We study the problem of distributional matrix completion: Given a sparsely observed matrix of empirical distributions, we seek to impute the true distributions associated with both observed and unobserved matrix entries. This is a generalization of traditional matrix completion, where the observations per matrix entry are scalar-valued. To do so, we utilize tools from optimal transport to generalize the nearest neighbors method to the distributional setting. Under a suitable latent factor model on probability distributions, we establish that our method recovers the distributions in the Wasserstein metric. We demonstrate through simulations that our method (i) provides better distributional estimates for an entry compared to using observed samples for that entry alone, (ii) yields accurate estimates of distributional quantities such as standard deviation and value-at-risk, and (iii) inherently supports heteroscedastic distributions. In addition, we demonstrate our method on a real-world dataset of quarterly earnings prediction distributions. We also prove novel asymptotic results for Wasserstein barycenters over one-dimensional distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13112v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jacob Feitelberg, Kyuseong Choi, Anish Agarwal, Raaz Dwivedi</dc:creator>
    </item>
    <item>
      <title>Ab Initio Nonparametric Variable Selection for Scalable Symbolic Regression with Large $p$</title>
      <link>https://arxiv.org/abs/2410.13681</link>
      <description>arXiv:2410.13681v2 Announce Type: replace-cross 
Abstract: Symbolic regression (SR) is a powerful technique for discovering symbolic expressions that characterize nonlinear relationships in data, gaining increasing attention for its interpretability, compactness, and robustness. However, existing SR methods do not scale to datasets with a large number of input variables (referred to as extreme-scale SR), which is common in modern scientific applications. This ``large $p$'' setting, often accompanied by measurement error, leads to slow performance of SR methods and overly complex expressions that are difficult to interpret. To address this scalability challenge, we propose a method called PAN+SR, which combines a key idea of ab initio nonparametric variable selection with SR to efficiently pre-screen large input spaces and reduce search complexity while maintaining accuracy. The use of nonparametric methods eliminates model misspecification, supporting a strategy called parametric-assisted nonparametric (PAN). We also extend SRBench, an open-source benchmarking platform, by incorporating high-dimensional regression problems with various signal-to-noise ratios. Our results demonstrate that PAN+SR consistently enhances the performance of 19 contemporary SR methods, enabling several to achieve state-of-the-art performance on these challenging datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13681v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shengbin Ye, Meng Li</dc:creator>
    </item>
    <item>
      <title>Causal Effect Identification in lvLiNGAM from Higher-Order Cumulants</title>
      <link>https://arxiv.org/abs/2506.05202</link>
      <description>arXiv:2506.05202v2 Announce Type: replace-cross 
Abstract: This paper investigates causal effect identification in latent variable Linear Non-Gaussian Acyclic Models (lvLiNGAM) using higher-order cumulants, addressing two prominent setups that are challenging in the presence of latent confounding: (1) a single proxy variable that may causally influence the treatment and (2) underspecified instrumental variable cases where fewer instruments exist than treatments. We prove that causal effects are identifiable with a single proxy or instrument and provide corresponding estimation methods. Experimental results demonstrate the accuracy and robustness of our approaches compared to existing methods, advancing the theoretical and practical understanding of causal inference in linear systems with latent confounders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05202v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniele Tramontano, Yaroslav Kivva, Saber Salehkaleybar, Mathias Drton, Negar Kiyavash</dc:creator>
    </item>
  </channel>
</rss>
