<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 17 Oct 2024 01:57:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>rquest: An R package for hypothesis tests and confidence intervals for quantiles and summary measures based on quantiles</title>
      <link>https://arxiv.org/abs/2410.11093</link>
      <description>arXiv:2410.11093v1 Announce Type: new 
Abstract: Sample quantiles, such as the median, are often better suited than the sample mean for summarising location characteristics of a data set. Similarly, linear combinations of sample quantiles and ratios of such linear combinations, e.g. the interquartile range and quantile-based skewness measures, are often used to quantify characteristics such as spread and skew. While often reported, it is uncommon to accompany quantile estimates with confidence intervals or standard errors. The rquest package provides a simple way to conduct hypothesis tests and derive confidence intervals for quantiles, linear combinations of quantiles, ratios of dependent linear combinations (e.g., Bowley's measure of skewness) and differences and ratios of all of the above for comparisons between independent samples. Many commonly used measures based on quantiles are included, although it is also very simple for users to define their own. Additionally, quantile-based measures of inequality are also considered. The methods are based on recent research showing that reliable distribution-free confidence intervals can be obtained, even for moderate sample sizes. Several examples are provided herein.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11093v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luke A. Prendergast, Shenal Dedduwakumara, Robert G. Staudte</dc:creator>
    </item>
    <item>
      <title>Models for spatiotemporal data with some missing locations and application to emergency calls models calibration</title>
      <link>https://arxiv.org/abs/2410.11103</link>
      <description>arXiv:2410.11103v1 Announce Type: new 
Abstract: We consider two classes of models for spatiotemporal data: one without covariates and one with covariates. If $\mathcal{T}$ is a partition of time and $\mathcal{I}$ a partition of the studied area into zones and if $\mathcal{C}$ is the set of arrival types, we assume that the process of arrivals for time interval $t \in \mathcal{T}$, zone $i \in \mathcal{I}$, and arrival type $c \in \mathcal{C}$ is Poisson with some intensity $\lambda_{c,i,t}$. We discussed the calibration and implementation of such models in \cite{laspatedpaper, laspatedmanual} with corresponding software LASPATED (Library for the Analysis of SPAtioTEmporal Discrete data) available on GitHub at https://github.com/vguigues/LASPATED. In this paper, we discuss the extension of these models when some of the locations are missing in the historical data. We propose three models to deal with missing locations and implemented them both in Matlab and C++. The corresponding code is available on GitHub as an extension of LASPATED at https://github.com/vguigues/LASPATED/Missing_Data. We tested our implementation using the process of emergency calls to an Emergency Health Service where many calls come with missing locations and show the importance and benefit of using models that consider missing locations, rather than discarding the calls with missing locations for the calibration of statistical models for such calls.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11103v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vincent Guigues, Anton Kleywegt, Victor Hugo Nascimento, Lucas Lucas Rafael de Andrade</dc:creator>
    </item>
    <item>
      <title>Discovering the critical number of respondents to validate an item in a questionnaire: The Binomial Cut-level Content Validity proposal</title>
      <link>https://arxiv.org/abs/2410.11151</link>
      <description>arXiv:2410.11151v1 Announce Type: new 
Abstract: The question that drives this research is: "How to discover the number of respondents that are necessary to validate items of a questionnaire as actually essential to reach the questionnaire's proposal?" Among the efforts in this subject, \cite{Lawshe1975, Wilson2012, Ayre_CVR_2014} approached this issue by proposing and refining the Content Validation Ratio (CVR) that looks to identify items that are actually essentials. Despite their contribution, these studies do not check if an item validated as "essential" should be also validated as "not essential" by the same sample, which should be a paradox. Another issue is the assignment a probability equal a 50\% to a item be randomly checked by a respondent as essential, despite an evaluator has three options to choose. Our proposal faces these issues, making it possible to verify if a paradoxical situation occurs, and being more precise in recommending whether an item should either be retained or discarded from a questionnaire.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11151v1</guid>
      <category>stat.ME</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Helder Gomes Costa, Eduardo Shimoda, Jos\'e Fabiano da Serra Costa, Aldo Shimoya, Edilvando Pereira Eufrazio</dc:creator>
    </item>
    <item>
      <title>Multi-scale tests of independence powerful for detecting explicit or implicit functional relationship</title>
      <link>https://arxiv.org/abs/2410.11192</link>
      <description>arXiv:2410.11192v1 Announce Type: new 
Abstract: In this article, we consider the problem of testing the independence between two random variables. Our primary objective is to develop tests that are highly effective at detecting associations arising from explicit or implicit functional relationship between two variables. We adopt a multi-scale approach by analyzing neighborhoods of varying sizes within the dataset and aggregating the results. We introduce a general testing framework designed to enhance the power of existing independence tests to achieve our objective. Additionally, we propose a novel test method that is powerful as well as computationally efficient. The performance of these tests is compared with existing methods using various simulated datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11192v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Angshuman Roy, Sagnik Das</dc:creator>
    </item>
    <item>
      <title>Regularized Estimation of High-Dimensional Matrix-Variate Autoregressive Models</title>
      <link>https://arxiv.org/abs/2410.11320</link>
      <description>arXiv:2410.11320v1 Announce Type: new 
Abstract: Matrix-variate time series data are increasingly popular in economics, statistics, and environmental studies, among other fields. This paper develops regularized estimation methods for analyzing high-dimensional matrix-variate time series using bilinear matrix-variate autoregressive models. The bilinear autoregressive structure is widely used for matrix-variate time series, as it reduces model complexity while capturing interactions between rows and columns. However, when dealing with large dimensions, the commonly used iterated least-squares method results in numerous estimated parameters, making interpretation difficult. To address this, we propose two regularized estimation methods to further reduce model dimensionality. The first assumes banded autoregressive coefficient matrices, where each data point interacts only with nearby points. A two-step estimation method is used: first, traditional iterated least-squares is applied for initial estimates, followed by a banded iterated least-squares approach. A Bayesian Information Criterion (BIC) is introduced to estimate the bandwidth of the coefficient matrices. The second method assumes sparse autoregressive matrices, applying the LASSO technique for regularization. We derive asymptotic properties for both methods as the dimensions diverge and the sample size $T\rightarrow\infty$. Simulations and real data examples demonstrate the effectiveness of our methods, comparing their forecasting performance against common autoregressive models in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11320v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hangjin Jiang, Baining Shen, Yuzhou Li, Zhaoxing Gao</dc:creator>
    </item>
    <item>
      <title>Effect modification and non-collapsibility leads to conflicting treatment decisions: a review of marginal and conditional estimands and recommendations for decision-making</title>
      <link>https://arxiv.org/abs/2410.11438</link>
      <description>arXiv:2410.11438v1 Announce Type: new 
Abstract: Effect modification occurs when a covariate alters the relative effectiveness of treatment compared to control. It is widely understood that, when effect modification is present, treatment recommendations may vary by population and by subgroups within the population. Population-adjustment methods are increasingly used to adjust for differences in effect modifiers between study populations and to produce population-adjusted estimates in a relevant target population for decision-making. It is also widely understood that marginal and conditional estimands for non-collapsible effect measures, such as odds ratios or hazard ratios, do not in general coincide even without effect modification. However, the consequences of both non-collapsibility and effect modification together are little-discussed in the literature.
  In this paper, we set out the definitions of conditional and marginal estimands, illustrate their properties when effect modification is present, and discuss the implications for decision-making. In particular, we show that effect modification can result in conflicting treatment rankings between conditional and marginal estimates. This is because conditional and marginal estimands correspond to different decision questions that are no longer aligned when effect modification is present. For time-to-event outcomes, the presence of covariates implies that marginal hazard ratios are time-varying, and effect modification can cause marginal hazard curves to cross. We conclude with practical recommendations for decision-making in the presence of effect modification, based on pragmatic comparisons of both conditional and marginal estimates in the decision target population. Currently, multilevel network meta-regression is the only population-adjustment method capable of producing both conditional and marginal estimates, in any decision target population.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11438v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David M. Phillippo (Population Health Sciences, Bristol Medical School, University of Bristol, Bristol, United Kingdom), Antonio Remiro-Az\'ocar (Methods and Outreach, Novo Nordisk Pharma, Madrid, Spain), Anna Heath (Child Health Evaluative Sciences, The Hospital for Sick Children, Toronto, Canada), Gianluca Baio (Department of Statistical Science, University College London, London, United Kingdom), Sofia Dias (Centre for Reviews and Dissemination, University of York, York, United Kingdom), A. E. Ades (Population Health Sciences, Bristol Medical School, University of Bristol, Bristol, United Kingdom), Nicky J. Welton (Population Health Sciences, Bristol Medical School, University of Bristol, Bristol, United Kingdom)</dc:creator>
    </item>
    <item>
      <title>Scalable likelihood-based estimation and variable selection for the Cox model with incomplete covariates</title>
      <link>https://arxiv.org/abs/2410.11482</link>
      <description>arXiv:2410.11482v1 Announce Type: new 
Abstract: Regression analysis with missing data is a long-standing and challenging problem, particularly when there are many missing variables with arbitrary missing patterns. Likelihood-based methods, although theoretically appealing, are often computationally inefficient or even infeasible when dealing with a large number of missing variables. In this paper, we consider the Cox regression model with incomplete covariates that are missing at random. We develop an expectation-maximization (EM) algorithm for nonparametric maximum likelihood estimation, employing a transformation technique in the E-step so that it involves only a one-dimensional integration. This innovation makes our methods scalable with respect to the dimension of the missing variables. In addition, for variable selection, we extend the proposed EM algorithm to accommodate a LASSO penalty in the likelihood. We demonstrate the feasibility and advantages of the proposed methods over existing methods by large-scale simulation studies and apply the proposed methods to a cancer genomic study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11482v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ngok Sang Kwok, Kin Yau Wong</dc:creator>
    </item>
    <item>
      <title>Functional Data Analysis on Wearable Sensor Data: A Systematic Review</title>
      <link>https://arxiv.org/abs/2410.11562</link>
      <description>arXiv:2410.11562v1 Announce Type: new 
Abstract: Wearable devices and sensors have recently become a popular way to collect data, especially in the health sciences. The use of sensors allows patients to be monitored over a period of time with a high observation frequency. Due to the continuous-on-time structure of the data, novel statistical methods are recommended for the analysis of sensor data. One of the popular approaches in the analysis of wearable sensor data is functional data analysis. The main objective of this paper is to review functional data analysis methods applied to wearable device data according to the type of sensor. In addition, we introduce several freely available software packages and open databases of wearable device data to facilitate access to sensor data in different fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11562v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nihan Acar-Denizli, Pedro Delicado</dc:creator>
    </item>
    <item>
      <title>Prediction-Centric Uncertainty Quantification via MMD</title>
      <link>https://arxiv.org/abs/2410.11637</link>
      <description>arXiv:2410.11637v1 Announce Type: new 
Abstract: Deterministic mathematical models, such as those specified via differential equations, are a powerful tool to communicate scientific insight. However, such models are necessarily simplified descriptions of the real world. Generalised Bayesian methodologies have been proposed for inference with misspecified models, but these are typically associated with vanishing parameter uncertainty as more data are observed. In the context of a misspecified deterministic mathematical model, this has the undesirable consequence that posterior predictions become deterministic and certain, while being incorrect. Taking this observation as a starting point, we propose Prediction-Centric Uncertainty Quantification, where a mixture distribution based on the deterministic model confers improved uncertainty quantification in the predictive context. Computation of the mixing distribution is cast as a (regularised) gradient flow of the maximum mean discrepancy (MMD), enabling consistent numerical approximations to be obtained. Results are reported on both a toy model from population ecology and a real model of protein signalling in cell biology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11637v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zheyang Shen, Jeremias Knoblauch, Sam Power, Chris. J. Oates</dc:creator>
    </item>
    <item>
      <title>Enhancing Statistical Validity and Power in Hybrid Controlled Trials: A Randomization Inference Approach with Conformal Selective Borrowing</title>
      <link>https://arxiv.org/abs/2410.11713</link>
      <description>arXiv:2410.11713v1 Announce Type: new 
Abstract: Randomized controlled trials (RCTs) are the gold standard for causal inference on treatment effects. However, they can be underpowered due to small population sizes in rare diseases and limited number of patients willing to participate due to questions regarding equipoise among treatment groups in common diseases. Hybrid controlled trials use external controls (ECs) from historical studies or large observational databases to enhance statistical efficiency, and as such, are considered for drug evaluation in rare diseases or indications associated with low trial participation. However, non-randomized ECs can introduce biases that compromise validity and inflate type I errors for treatment discovery, particularly in small samples. To address this, we extend the Fisher randomization test (FRT) to hybrid controlled trials. Our approach involves a test statistic combining RCT and EC data and is based solely on randomization in the RCT. This method strictly controls the type I error rate, even with biased ECs, and improves power by incorporating unbiased ECs. To mitigate the power loss caused by biased ECs, we introduce conformal selective borrowing, which uses conformal p-values to individually select unbiased ECs, offering the flexibility to use either computationally efficient parametric models or off-the-shelf machine learning models to construct the conformal score function, along with model-agnostic reliability. We identify a risk-benefit trade-off in the power of FRT associated with different selection thresholds for conformal p-values. We offer both fixed and adaptive selection threshold options, enabling robust performance across varying levels of hidden bias. The advantages of our method are demonstrated through simulations and an application to a small lung cancer RCT with ECs from the National Cancer Database. Our method is available in the R package intFRT on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11713v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ke Zhu, Shu Yang, Xiaofei Wang</dc:creator>
    </item>
    <item>
      <title>Randomization-based Inference for MCP-Mod</title>
      <link>https://arxiv.org/abs/2410.11716</link>
      <description>arXiv:2410.11716v1 Announce Type: new 
Abstract: Dose selection is critical in pharmaceutical drug development, as it directly impacts therapeutic efficacy and patient safety of a drug. The Generalized Multiple Comparison Procedures and Modeling (MCP-Mod) approach is commonly used in Phase II trials for testing and estimation of dose-response relationships. However, its effectiveness in small sample sizes, particularly with binary endpoints, is hindered by issues like complete separation in logistic regression, leading to non-existence of estimates. Motivated by an actual clinical trial using the MCP-Mod approach, this paper introduces penalized maximum likelihood estimation (MLE) and randomization-based inference techniques to address these challenges. Randomization-based inference allows for exact finite sample inference, while population-based inference for MCP-Mod typically relies on asymptotic approximations. Simulation studies demonstrate that randomization-based tests can enhance statistical power in small to medium-sized samples while maintaining control over type-I error rates, even in the presence of time trends. Our results show that residual-based randomization tests using penalized MLEs not only improve computational efficiency but also outperform standard randomization-based methods, making them an adequate choice for dose-finding analyses within the MCP-Mod framework. Additionally, we apply these methods to pharmacometric settings, demonstrating their effectiveness in such scenarios. The results in this paper underscore the potential of randomization-based inference for the analysis of dose-finding trials, particularly in small sample contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11716v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lukas Pin, Oleksandr Sverdlov, Frank Bretz, Bj\"orn Bornkamp</dc:creator>
    </item>
    <item>
      <title>Addressing the Null Paradox in Epidemic Models: Correcting for Collider Bias in Causal Inference</title>
      <link>https://arxiv.org/abs/2410.11743</link>
      <description>arXiv:2410.11743v1 Announce Type: new 
Abstract: We address the null paradox in epidemic models, where standard methods estimate a non-zero treatment effect despite the true effect being zero. This occurs when epidemic models mis-specify how causal effects propagate over time, especially when covariates act as colliders between past interventions and latent variables, leading to spurious correlations. Standard approaches like maximum likelihood and Bayesian methods can misinterpret these biases, inferring false causal relationships. While semi-parametric models and inverse propensity weighting offer potential solutions, they often limit the ability of domain experts to incorporate epidemic-specific knowledge. To resolve this, we propose an alternative estimating equation that corrects for collider bias while allowing for statistical inference with frequentist guarantees, previously unavailable for complex models like SEIR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11743v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heejong Bong, Val\'erie Ventura, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Unraveling Heterogeneous Treatment Effects in Networks: A Non-Parametric Approach Based on Node Connectivity</title>
      <link>https://arxiv.org/abs/2410.11797</link>
      <description>arXiv:2410.11797v1 Announce Type: new 
Abstract: In network settings, interference between units makes causal inference more challenging as outcomes may depend on the treatments received by others in the network. Typical estimands in network settings focus on treatment effects aggregated across individuals in the population. We propose a framework for estimating node-wise counterfactual means, allowing for more granular insights into the impact of network structure on treatment effect heterogeneity. We develop a doubly robust and non-parametric estimation procedure, KECENI (Kernel Estimation of Causal Effect under Network Interference), which offers consistency and asymptotic normality under network dependence. The utility of this method is demonstrated through an application to microfinance data, revealing the impact of network characteristics on treatment effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11797v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heejong Bong, Colin B. Fogarty, Liza Levina, Ji Zhu</dc:creator>
    </item>
    <item>
      <title>Impact of existence and nonexistence of pivot on the coverage of empirical best linear prediction intervals for small areas</title>
      <link>https://arxiv.org/abs/2410.11238</link>
      <description>arXiv:2410.11238v1 Announce Type: cross 
Abstract: We advance the theory of parametric bootstrap in constructing highly efficient empirical best (EB) prediction intervals of small area means. The coverage error of such a prediction interval is of the order $O(m^{-3/2})$, where $m$ is the number of small areas to be pooled using a linear mixed normal model. In the context of an area level model where the random effects follow a non-normal known distribution except possibly for unknown hyperparameters, we analytically show that the order of coverage error of empirical best linear (EBL) prediction interval remains the same even if we relax the normality of the random effects by the existence of pivot for a suitably standardized random effects when hyperpameters are known. Recognizing the challenge of showing existence of a pivot, we develop a simple moment-based method to claim non-existence of pivot. We show that existing parametric bootstrap EBL prediction interval fails to achieve the desired order of the coverage error, i.e. $O(m^{-3/2})$, in absence of a pivot. We obtain a surprising result that the order $O(m^{-1})$ term is always positive under certain conditions indicating possible overcoverage of the existing parametric bootstrap EBL prediction interval. In general, we analytically show for the first time that the coverage problem can be corrected by adopting a suitably devised double parametric bootstrap. Our Monte Carlo simulations show that our proposed single bootstrap method performs reasonably well when compared to rival methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11238v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuting Chen, Masayo Y. Hirose, Partha Lahiri</dc:creator>
    </item>
    <item>
      <title>Filtering coupled Wright-Fisher diffusions</title>
      <link>https://arxiv.org/abs/2410.11429</link>
      <description>arXiv:2410.11429v1 Announce Type: cross 
Abstract: Coupled Wright-Fisher diffusions have been recently introduced to model the temporal evolution of finitely-many allele frequencies at several loci. These are vectors of multidimensional diffusions whose dynamics are weakly coupled among loci through interaction coefficients, which make the reproductive rates for each allele depend on its frequencies at several loci. Here we consider the problem of filtering a coupled Wright-Fisher diffusion with parent-independent mutation, when this is seen as an unobserved signal in a hidden Markov model. We assume individuals are sampled multinomially at discrete times from the underlying population, whose type configuration at the loci is described by the diffusion states, and adapt recently introduced duality methods to derive the filtering and smoothing distributions. These respectively provide the conditional distribution of the diffusion states given past data, and that conditional on the entire dataset, and are key to be able to perform parameter inference on models of this type. We show that for this model these distributions are countable mixtures of tilted products of Dirichlet kernels, and describe their mixing weights and how these can be updated sequentially. The evaluation of the weights involves the transition probabilities of the dual process, which are not available in closed form. We lay out pseudo codes for the implementation of the algorithms, discuss how to handle the unavailable quantities, and briefly illustrate the procedure with synthetic data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11429v1</guid>
      <category>math.PR</category>
      <category>q-bio.PE</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chiara Boetti, Matteo Ruggiero</dc:creator>
    </item>
    <item>
      <title>A Flexible Quasi-Copula Distribution for Statistical Modeling</title>
      <link>https://arxiv.org/abs/2205.03505</link>
      <description>arXiv:2205.03505v3 Announce Type: replace 
Abstract: Copulas, generalized estimating equations, and generalized linear mixed models promote the analysis of grouped data where non-normal responses are correlated. Unfortunately, parameter estimation remains challenging in these three frameworks. Based on prior work of Tonda, we derive a new class of probability density functions that allow explicit calculation of moments, marginal and conditional distributions, and the score and observed information needed in maximum likelihood estimation. We also illustrate how the new distribution flexibly models longitudinal data following a non-Gaussian distribution. Finally, we conduct a tri-variate genome-wide association analysis on dichotomized systolic and diastolic blood pressure and body mass index data from the UK-Biobank, showcasing the modeling prowess and computational scalability of the new distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.03505v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sarah S. Ji, Benjamin B. Chu, Hua Zhou, Kenneth Lange</dc:creator>
    </item>
    <item>
      <title>Semiparametric Efficient Fusion of Individual Data and Summary Statistics</title>
      <link>https://arxiv.org/abs/2210.00200</link>
      <description>arXiv:2210.00200v4 Announce Type: replace 
Abstract: Suppose we have individual data from an internal study and various summary statistics from relevant external studies. External summary statistics have the potential to improve statistical inference for the internal population; however, it may lead to efficiency loss or bias if not used properly. We study the fusion of individual data and summary statistics in a semiparametric framework to investigate the efficient use of external summary statistics. Under a weak transportability assumption, we establish the semiparametric efficiency bound for estimating a general functional of the internal data distribution, which is no larger than that using only internal data and underpins the potential efficiency gain of integrating individual data and summary statistics. We propose a data-fused efficient estimator that achieves this efficiency bound. In addition, an adaptive fusion estimator is proposed to eliminate the bias of the original data-fused estimator when the transportability assumption fails. We establish the asymptotic oracle property of the adaptive fusion estimator. Simulations and application to a Helicobacter pylori infection dataset demonstrate the promising numerical performance of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.00200v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenjie Hu, Ruoyu Wang, Wei Li, Wang Miao</dc:creator>
    </item>
    <item>
      <title>Harnessing The Collective Wisdom: Fusion Learning Using Decision Sequences From Diverse Sources</title>
      <link>https://arxiv.org/abs/2308.11026</link>
      <description>arXiv:2308.11026v2 Announce Type: replace 
Abstract: Learning from the collective wisdom of crowds is related to the statistical notion of fusion learning from multiple data sources or studies. However, fusing inferences from diverse sources is challenging since cross-source heterogeneity and potential data-sharing complicate statistical inference. Moreover, studies may rely on disparate designs, employ myriad modeling techniques, and prevailing data privacy norms may forbid sharing even summary statistics across the studies for an overall analysis. We propose an Integrative Ranking and Thresholding (IRT) framework for fusion learning in multiple testing. IRT operates under the setting where from each study a triplet is available: the vector of binary accept-reject decisions on the tested hypotheses, its False Discovery Rate (FDR) level and the hypotheses tested by it. Under this setting, IRT constructs an aggregated and nonparametric measure of evidence against each null hypotheses, which facilitates ranking the hypotheses in the order of their likelihood of being rejected. We show that IRT guarantees an overall FDR control if the studies control their respective FDR at the desired levels. IRT is extremely flexible, and a comprehensive numerical study demonstrates its practical relevance for pooling inferences. A real data illustration and extensions to alternative forms of Type I error control are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.11026v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Trambak Banerjee, Bowen Gang, Jianliang He</dc:creator>
    </item>
    <item>
      <title>Frequentist Inference for Semi-mechanistic Epidemic Models with Interventions</title>
      <link>https://arxiv.org/abs/2309.10792</link>
      <description>arXiv:2309.10792v2 Announce Type: replace 
Abstract: The effect of public health interventions on an epidemic are often estimated by adding the intervention to epidemic models. During the Covid-19 epidemic, numerous papers used such methods for making scenario predictions. The majority of these papers use Bayesian methods to estimate the parameters of the model. In this paper we show how to use frequentist methods for estimating these effects which avoids having to specify prior distributions. We also use model-free shrinkage methods to improve estimation when there are many different geographic regions. This allows us to borrow strength from different regions while still getting confidence intervals with correct coverage and without having to specify a hierarchical model. Throughout, we focus on a semi-mechanistic model which provides a simple, tractable alternative to compartmental methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.10792v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heejong Bong, Val\'erie Ventura, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>An Experimental Design for Anytime-Valid Causal Inference on Multi-Armed Bandits</title>
      <link>https://arxiv.org/abs/2311.05794</link>
      <description>arXiv:2311.05794v4 Announce Type: replace 
Abstract: Experimentation is crucial for managers to rigorously quantify the value of a change and determine if it leads to a statistically significant improvement over the status quo. As companies increasingly mandate that all changes undergo experimentation before widespread release, two challenges arise: (1) minimizing the proportion of customers assigned to the inferior treatment and (2) increasing experimentation velocity by enabling data-dependent stopping. This paper addresses both challenges by introducing the Mixture Adaptive Design (MAD), a new experimental design for multi-armed bandit (MAB) algorithms that enables anytime-valid inference on the Average Treatment Effect (ATE) for \emph{any} MAB algorithm. Intuitively, MAD "mixes" any bandit algorithm with a Bernoulli design, where at each time step, the probability of assigning a unit via the Bernoulli design is determined by a user-specified deterministic sequence that can converge to zero. This sequence lets managers directly control the trade-off between regret minimization and inferential precision. Under mild conditions on the rate the sequence converges to zero, we provide a confidence sequence that is asymptotically anytime-valid and guaranteed to shrink around the true ATE. Hence, when the true ATE converges to a non-zero value, the MAD confidence sequence is guaranteed to exclude zero in finite time. Therefore, the MAD enables managers to stop experiments early while ensuring valid inference, enhancing both the efficiency and reliability of adaptive experiments. Empirically, we demonstrate that the MAD achieves finite-sample anytime-validity while accurately and precisely estimating the ATE, all without incurring significant losses in reward compared to standard bandit designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.05794v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Biyonka Liang, Iavor Bojinov</dc:creator>
    </item>
    <item>
      <title>Expressing and visualizing model uncertainty in Bayesian variable selection using Cartesian credible sets</title>
      <link>https://arxiv.org/abs/2402.12323</link>
      <description>arXiv:2402.12323v3 Announce Type: replace 
Abstract: Modern regression applications can involve hundreds or thousands of variables which motivates the use of variable selection methods. Bayesian variable selection defines a posterior distribution on the possible subsets of the variables (which are usually termed models) to express uncertainty about which variables are strongly linked to the response. This can be used to provide Bayesian model averaged predictions or inference, and to understand the relative importance of different variables. However, there has been little work on meaningful representations of this uncertainty beyond first order summaries. We introduce Cartesian credible sets to address this gap. The elements of these sets are formed by concatenating sub-models defined on each block of a partition of the variables. Investigating these sub-models allow us to understand whether the models in the Cartesian credible set always/never/sometimes include a particular variable or group of variables and provide a useful summary of model uncertainty. We introduce a method to find these sets that emphasizes ease of understanding and can be easily computed from Markov chain Monte Carlo output. The potential of the method is illustrated on regression problems with both small and large numbers of variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.12323v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>J. E. Griffin</dc:creator>
    </item>
    <item>
      <title>Semi-parametric goodness-of-fit testing for INAR models</title>
      <link>https://arxiv.org/abs/2402.17425</link>
      <description>arXiv:2402.17425v2 Announce Type: replace 
Abstract: Among the various models designed for dependent count data, integer-valued autoregressive (INAR) processes enjoy great popularity. Typically, statistical inference for INAR models uses asymptotic theory that relies on rather stringent (parametric) assumptions on the innovations such as Poisson or negative binomial distributions. In this paper, we present a novel semi-parametric goodness-of-fit test tailored for the INAR model class. Relying on the INAR-specific shape of the joint probability generating function, our approach allows for model validation of INAR models without specifying the (family of the) innovation distribution. We derive the limiting null distribution of our proposed test statistic, prove consistency under fixed alternatives and discuss its asymptotic behavior under local alternatives. By manifold Monte Carlo simulations, we illustrate the overall good performance of our testing procedure in terms of power and size properties. In particular, it turns out that the power can be considerably improved by using higher-order test statistics. We conclude the article with the application on three real-world economic data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.17425v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maxime Faymonville, Carsten Jentsch, Christian H. Wei{\ss}</dc:creator>
    </item>
    <item>
      <title>Identifying sparse treatment effects in high-dimensional outcome spaces</title>
      <link>https://arxiv.org/abs/2404.14644</link>
      <description>arXiv:2404.14644v2 Announce Type: replace 
Abstract: Based on technological advances in sensing modalities, randomized trials with primary outcomes represented as high-dimensional vectors have become increasingly prevalent. For example, these outcomes could be week-long time-series data from wearable devices or high-dimensional neuroimaging data, such as from functional magnetic resonance imaging. This paper focuses on randomized treatment studies with such high-dimensional outcomes characterized by sparse treatment effects, where interventions may influence a small number of dimensions, e.g., small temporal windows or specific brain regions. Conventional practices, such as using fixed, low-dimensional summaries of the outcomes, result in significantly reduced power for detecting treatment effects. To address this limitation, we propose a procedure that involves subset selection followed by inference. Specifically, given a potentially large set of outcome summaries, we identify the subset that captures treatment effects, which requires only one call to the Lasso, and subsequently conduct inference on the selected subset. Via theoretical analysis as well as simulations, we demonstrate that our method asymptotically selects the correct subset and increases statistical power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14644v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yujin Jeong, Emily Fox, Ramesh Johari</dc:creator>
    </item>
    <item>
      <title>Informativeness of Weighted Conformal Prediction</title>
      <link>https://arxiv.org/abs/2405.06479</link>
      <description>arXiv:2405.06479v2 Announce Type: replace 
Abstract: Weighted conformal prediction (WCP), a recently proposed framework, provides uncertainty quantification with the flexibility to accommodate different covariate distributions between training and test data. However, it is pointed out in this paper that the effectiveness of WCP heavily relies on the overlap between covariate distributions; insufficient overlap can lead to uninformative prediction intervals. To enhance the informativeness of WCP, we propose two methods for scenarios involving multiple sources with varied covariate distributions. We establish theoretical guarantees for our proposed methods and demonstrate their efficacy through simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06479v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mufang Ying, Wenge Guo, Koulik Khamaru, Ying Hung</dc:creator>
    </item>
    <item>
      <title>Sparse Multivariate Linear Regression with Strongly Associated Response Variables</title>
      <link>https://arxiv.org/abs/2410.10025</link>
      <description>arXiv:2410.10025v2 Announce Type: replace 
Abstract: We propose new methods for multivariate linear regression when the regression coefficient matrix is sparse and the error covariance matrix is dense. We assume that the error covariance matrix has equicorrelation across the response variables. Two procedures are proposed: one is based on constant marginal response variance (compound symmetry), and the other is based on general varying marginal response variance. Two approximate procedures are also developed for high dimensions. We propose an approximation to the Gaussian validation likelihood for tuning parameter selection. Extensive numerical experiments illustrate when our procedures outperform relevant competitors as well as their robustness to model misspecification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10025v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daeyoung Ham, Bradley S. Price, Adam J. Rothman</dc:creator>
    </item>
    <item>
      <title>COBRA: Comparison-Optimal Betting for Risk-limiting Audits</title>
      <link>https://arxiv.org/abs/2304.01010</link>
      <description>arXiv:2304.01010v2 Announce Type: replace-cross 
Abstract: Risk-limiting audits (RLAs) can provide routine, affirmative evidence that reported election outcomes are correct by checking a random sample of cast ballots. An efficient RLA requires checking relatively few ballots. Here we construct highly efficient RLAs by optimizing supermartingale tuning parameters--$\textit{bets}$--for ballot-level comparison audits. The exactly optimal bets depend on the true rate of errors in cast-vote records (CVRs)--digital receipts detailing how machines tabulated each ballot. We evaluate theoretical and simulated workloads for audits of contests with a range of diluted margins and CVR error rates. Compared to bets recommended in past work, using these optimal bets can dramatically reduce expected workloads--by 93% on average over our simulated audits. Because the exactly optimal bets are unknown in practice, we offer some strategies for approximating them. As with the ballot-polling RLAs described in ALPHA and RiLACs, adapting bets to previously sampled data or diversifying them over a range of suspected error rates can lead to substantially more efficient audits than fixing bets to $\textit{a priori}$ values, especially when those values are far from correct. We sketch extensions to other designs and social choice functions, and conclude with some recommendations for real-world comparison audits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.01010v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jacob Spertus</dc:creator>
    </item>
  </channel>
</rss>
