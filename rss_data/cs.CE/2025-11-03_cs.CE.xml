<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 04 Nov 2025 03:48:37 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>FMint-SDE: A Multimodal Foundation Model for Accelerating Numerical Simulation of SDEs via Error Correction</title>
      <link>https://arxiv.org/abs/2510.27173</link>
      <description>arXiv:2510.27173v1 Announce Type: new 
Abstract: Fast and accurate simulation of dynamical systems is a fundamental challenge across scientific and engineering domains. Traditional numerical integrators often face a trade-off between accuracy and computational efficiency, while existing neural network-based approaches typically require training a separate model for each case. To overcome these limitations, we introduce a novel multi-modal foundation model for large-scale simulations of differential equations: FMint-SDE (Foundation Model based on Initialization for stochastic differential equations). Based on a decoder-only transformer with in-context learning, FMint-SDE leverages numerical and textual modalities to learn a universal error-correction scheme. It is trained using prompted sequences of coarse solutions generated by conventional solvers, enabling broad generalization across diverse systems. We evaluate our models on a suite of challenging SDE benchmarks spanning applications in molecular dynamics, mechanical systems, finance, and biology. Experimental results show that our approach achieves a superior accuracy-efficiency tradeoff compared to classical solvers, underscoring the potential of FMint-SDE as a general-purpose simulation tool for dynamical systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27173v1</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.DS</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiaxin Yuan, Haizhao Yang, Maria Cameron</dc:creator>
    </item>
    <item>
      <title>Adaptive Control for a Physics-Informed Model of a Thermal Energy Distribution System: Qualitative Analysis</title>
      <link>https://arxiv.org/abs/2510.26959</link>
      <description>arXiv:2510.26959v1 Announce Type: cross 
Abstract: Integrated energy systems (IES) are complex heterogeneous architectures that typically encompass power sources, hydrogen electrolyzers, energy storage, and heat exchangers. This integration is achieved through operating control strategy optimization. However, the lack of physical understanding as to how these systems evolve over time introduces uncertainties that hinder reliable application thereof. Techniques that can accommodate such uncertainties are fundamental for ensuring proper operation of these systems. Unfortunately, no unifying methodology exists for accommodating uncertainties in this regard. That being said, adaptive control (AC) is a discipline that may allow for accommodating such uncertainties in real-time. In the present work, we derive an AC formulation for linear systems in which all states are observable and apply it to the control of a glycol heat exchanger (GHX) in an IES. Based on prior research in which we quantified the uncertainties of the GHXs system dynamics, we introduced an error of 50% on four terms of the nominal model. In the case where a linear quadratic regulator is used as the nominal control for the reference system, we found that employing AC can reduce the mean absolute error and integral time absolute error by a factor of 30%-75%. This reduction is achieved with minimal computing overhead and control infrastructure, thus underscoring the strength of AC. However, the control effort induced is significant, therefore warranting further study in order to estimate its impact on a physical system. To address further challenges, including partially observable and non-linear dynamics, enhancements of the linear formulation are currently being developed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26959v1</guid>
      <category>eess.SY</category>
      <category>cs.CE</category>
      <category>cs.SY</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Paul Seurin, Auradha Annaswamy, Linyu Lin</dc:creator>
    </item>
    <item>
      <title>Black-Scholes Model, comparison between Analytical Solution and Numerical Analysis</title>
      <link>https://arxiv.org/abs/2510.27277</link>
      <description>arXiv:2510.27277v1 Announce Type: cross 
Abstract: The main purpose of this article is to give a general overview and understanding of the first widely used option-pricing model, the Black-Scholes model. The history and context are presented, with the usefulness and implications in the economics world. A brief review of fundamental calculus concepts is introduced to derive and solve the model. The equation is then resolved using both an analytical (variable separation) and a numerical method (finite differences). Conclusions are drawn in order to understand how Black-Scholes is employed nowadays. At the end a handy appendix (A) is written with some economics notions to ease the reader's comprehension of the paper; furthermore a second appendix (B) is given with some code scripts, to allow the reader to put in practice some concepts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27277v1</guid>
      <category>q-fin.PR</category>
      <category>cs.CE</category>
      <category>q-fin.CP</category>
      <category>q-fin.RM</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Francesco Romaggi</dc:creator>
    </item>
    <item>
      <title>Large Language Models for Combinatorial Optimization of Design Structure Matrix</title>
      <link>https://arxiv.org/abs/2506.09749</link>
      <description>arXiv:2506.09749v2 Announce Type: replace 
Abstract: In complex engineering systems, the dependencies among components or development activities are often modeled and analyzed using Design Structure Matrix (DSM). Reorganizing elements within a DSM to minimize feedback loops and enhance modularity or process efficiency constitutes a challenging combinatorial optimization (CO) problem in engineering design and operations. As problem sizes increase and dependency networks become more intricate, traditional optimization methods that rely solely on mathematical heuristics often fail to capture the contextual nuances and struggle to deliver effective solutions. In this study, we explore the potential of Large Language Models (LLMs) to address such CO problems by leveraging their capabilities for advanced reasoning and contextual understanding. We propose a novel LLM-based framework that integrates network topology with contextual domain knowledge for iterative optimization of DSM sequencing-a common CO problem. Experiments on various DSM cases demonstrate that our method consistently achieves faster convergence and superior solution quality compared to both stochastic and deterministic baselines. Notably, incorporating contextual domain knowledge significantly enhances optimization performance regardless of the chosen LLM backbone. These findings highlight the potential of LLMs to solve complex engineering CO problems by combining semantic and mathematical reasoning. This approach paves the way towards a new paradigm in LLM-based engineering design optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09749v2</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shuo Jiang, Min Xie, Jianxi Luo</dc:creator>
    </item>
    <item>
      <title>Superstructure Optimization with Embedded Neural Networks for Sustainable Aviation Fuel Production</title>
      <link>https://arxiv.org/abs/2509.09796</link>
      <description>arXiv:2509.09796v2 Announce Type: replace 
Abstract: This study presents a multi-objective optimization framework for sustainable aviation fuel (SAF) production, integrating artificial neural networks (ANNs) within a mixed-integer quadratically constrained programming (MIQCP) formulation. By embedding data-driven surrogate models into the mathematical optimization structure, the proposed methodology addresses key limitations of conventional superstructure-based approaches, enabling simultaneous optimization of discrete process choices and continuous operating parameters. The framework captures variable input and output stream compositions, facilitating the joint optimization of target product composition and system design. Application to Fischer-Tropsch (FT) kerosene production demonstrates that cost-minimizing configurations under unconstrained CO2 emissions are dominated by the fossil-based autothermal reforming (ATR) route. Imposing carbon emission constraints necessitates the integration of biomass gasification and direct air capture coupled with carbon sequestration (DAC-CS), resulting in substantially reduced net emissions but higher production costs. At the zero-emission limit, hybrid configurations combining ATR and biomass gasification achieve the lowest costs (~2.38 \$/kg-kerosene), followed closely by biomass gasification-only (~2.43 \$/kg), both of which outperform the ATR-only pathway with DAC-CS (~2.65 \$/kg). In contrast, DAC-only systems relying exclusively on atmospheric CO2 and water electrolysis are prohibitively expensive (~10.8 \$/kg). The results highlight the critical role of the embedded ANNs: optimal process conditions, such as FT reactor pressure and gasification temperature, adapt to changing circumstances, consistently outperforming fixed setups and achieving up to 20% cost savings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09796v2</guid>
      <category>cs.CE</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Klimek, Christoph Plate, Sebastian Sager, Kai Sundmacher, Caroline Ganzer</dc:creator>
    </item>
    <item>
      <title>Common Task Framework For a Critical Evaluation of Scientific Machine Learning Algorithms</title>
      <link>https://arxiv.org/abs/2510.23166</link>
      <description>arXiv:2510.23166v2 Announce Type: replace 
Abstract: Machine learning (ML) is transforming modeling and control in the physical, engineering, and biological sciences. However, rapid development has outpaced the creation of standardized, objective benchmarks - leading to weak baselines, reporting bias, and inconsistent evaluations across methods. This undermines reproducibility, misguides resource allocation, and obscures scientific progress. To address this, we propose a Common Task Framework (CTF) for scientific machine learning. The CTF features a curated set of datasets and task-specific metrics spanning forecasting, state reconstruction, and generalization under realistic constraints, including noise and limited data. Inspired by the success of CTFs in fields like natural language processing and computer vision, our framework provides a structured, rigorous foundation for head-to-head evaluation of diverse algorithms. As a first step, we benchmark methods on two canonical nonlinear systems: Kuramoto-Sivashinsky and Lorenz. These results illustrate the utility of the CTF in revealing method strengths, limitations, and suitability for specific classes of problems and diverse objectives. Next, we are launching a competition around a global real world sea surface temperature dataset with a true holdout dataset to foster community engagement. Our long-term vision is to replace ad hoc comparisons with standardized evaluations on hidden test sets that raise the bar for rigor and reproducibility in scientific ML.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23166v2</guid>
      <category>cs.CE</category>
      <category>physics.comp-ph</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philippe Martin Wyder, Judah Goldfeder, Alexey Yermakov, Yue Zhao, Stefano Riva, Jan P. Williams, David Zoro, Amy Sara Rude, Matteo Tomasetto, Joe Germany, Joseph Bakarji, Georg Maierhofer, Miles Cranmer, J. Nathan Kutz</dc:creator>
    </item>
    <item>
      <title>PPDiff: Diffusing in Hybrid Sequence-Structure Space for Protein-Protein Complex Design</title>
      <link>https://arxiv.org/abs/2506.11420</link>
      <description>arXiv:2506.11420v2 Announce Type: replace-cross 
Abstract: Designing protein-binding proteins with high affinity is critical in biomedical research and biotechnology. Despite recent advancements targeting specific proteins, the ability to create high-affinity binders for arbitrary protein targets on demand, without extensive rounds of wet-lab testing, remains a significant challenge. Here, we introduce PPDiff, a diffusion model to jointly design the sequence and structure of binders for arbitrary protein targets in a non-autoregressive manner. PPDiffbuilds upon our developed Sequence Structure Interleaving Network with Causal attention layers (SSINC), which integrates interleaved self-attention layers to capture global amino acid correlations, k-nearest neighbor (kNN) equivariant graph layers to model local interactions in three-dimensional (3D) space, and causal attention layers to simplify the intricate interdependencies within the protein sequence. To assess PPDiff, we curate PPBench, a general protein-protein complex dataset comprising 706,360 complexes from the Protein Data Bank (PDB). The model is pretrained on PPBenchand finetuned on two real-world applications: target-protein mini-binder complex design and antigen-antibody complex design. PPDiffconsistently surpasses baseline methods, achieving success rates of 50.00%, 23.16%, and 16.89% for the pretraining task and the two downstream applications, respectively. The code, data and models are available at https://github.com/JocelynSong/PPDiff.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11420v2</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenqiao Song, Tiaoxiao Li, Lei Li, Martin Renqiang Min</dc:creator>
    </item>
    <item>
      <title>Algorithmic Collusion of Pricing and Advertising on E-commerce Platforms</title>
      <link>https://arxiv.org/abs/2508.08325</link>
      <description>arXiv:2508.08325v2 Announce Type: replace-cross 
Abstract: When online sellers use AI learning algorithms to automatically compete on e-commerce platforms, there is concern that they will learn to coordinate on higher than competitive prices. However, this concern was primarily raised in single-dimension price competition. We investigate whether this prediction holds when sellers make pricing and advertising decisions together, i.e., two-dimensional decisions. We analyze competition in multi-agent reinforcement learning, and use a large-scale dataset from Amazon.com to provide empirical evidence. We show that when consumers have high search costs, learning algorithms can coordinate on prices lower than competitive prices, facilitating a win-win-win for consumers, sellers, and platforms. This occurs because algorithms learn to coordinate on lower advertising bids, which lower advertising costs, leading to lower prices and enlarging demand on the platform. We also show that our results generalize to any learning algorithm that uses exploration of price and advertising bids. Consistent with our predictions, an empirical analysis shows that price levels exhibit a negative interaction between estimated consumer search costs and algorithm usage index. We analyze the platform's strategic response and find that reserve price adjustments will not increase platform profits, but commission adjustments will, while maintaining the beneficial outcomes for both sellers and consumers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08325v2</guid>
      <category>econ.GN</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hangcheng Zhao, Ron Berman</dc:creator>
    </item>
    <item>
      <title>Towards a Generalizable AI for Materials Discovery: Validation through Immersion Coolant Screening</title>
      <link>https://arxiv.org/abs/2510.23371</link>
      <description>arXiv:2510.23371v2 Announce Type: replace-cross 
Abstract: Artificial intelligence (AI) has emerged as a powerful accelerator of materials discovery, yet most existing models remain problem-specific, requiring additional data collection and retraining for each new property. Here we introduce and validate GATE (Geometrically Aligned Transfer Encoder) -- a generalizable AI framework that jointly learns 34 physicochemical properties spanning thermal, electrical, mechanical, and optical domains. By aligning these properties within a shared geometric space, GATE captures cross-property correlations that reduce disjoint-property bias -- a key factor causing false positives in multi-criteria screening. To demonstrate its generalizable utility, GATE -- without any problem-specific model reconfiguration -- applied to the discovery of immersion cooling fluids for data centers, a stringent real-world challenge defined by the Open Compute Project (OCP). Screening billions of candidates, GATE identified 92,861 molecules as promising for practical deployment. Four were experimentally or literarily validated, showing strong agreement with wet-lab measurements and performance comparable to or exceeding a commercial coolant. These results establish GATE as a generalizable AI platform readily applicable across diverse materials discovery tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23371v2</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hyunseung Kim (LG AI Research, Republic of Korea), Dae-Woong Jeong (LG AI Research, Republic of Korea), Changyoung Park (LG AI Research, Republic of Korea), Won-Ji Lee (Kyonggi University, Republic of Korea), Ha-Eun Lee (Kyonggi University, Republic of Korea), Ji-Hye Lee (Kyonggi University, Republic of Korea), Rodrigo Hormazabal (LG AI Research, Republic of Korea), Sung Moon Ko (LG AI Research, Republic of Korea), Sumin Lee (LG AI Research, Republic of Korea), Soorin Yim (LG AI Research, Republic of Korea), Chanhui Lee (LG AI Research, Republic of Korea), Sehui Han (LG AI Research, Republic of Korea), Sang-Ho Cha (Kyonggi University, Republic of Korea), Woohyung Lim (LG AI Research, Republic of Korea)</dc:creator>
    </item>
  </channel>
</rss>
