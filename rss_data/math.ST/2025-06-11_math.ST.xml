<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Jun 2025 01:39:30 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>On the Existence of Unbiased Hypothesis Tests: An Algebraic Approach</title>
      <link>https://arxiv.org/abs/2506.08259</link>
      <description>arXiv:2506.08259v1 Announce Type: new 
Abstract: In hypothesis testing problems the property of strict unbiasedness describes whether a test is able to discriminate, in the sense of a difference in power, between any distribution in the null hypothesis space and any distribution in the alternative hypothesis space. In this work we examine conditions under which unbiased tests exist for discrete statistical models. It is shown that the existence of an unbiased test can be reduced to an algebraic criterion; an unbiased test exists if and only if there exists a polynomial that separates the null and alternative hypothesis sets. This places a strong, semialgebraic restriction on the classes of null hypotheses that have unbiased tests. The minimum degree of a separating polynomial coincides with the minimum sample size that is needed for an unbiased test to exist, termed the unbiasedness threshold. It is demonstrated that Gr\"obner basis techniques can be used to provide upper bounds for, and in many cases exactly find, the unbiasedness threshold. Existence questions for uniformly most powerful unbiased tests are also addressed, where it is shown that whether such a test exists can depend subtly on the specified level of the test and the sample size. Numerous examples, concerning tests in contingency tables, linear, log-linear, and mixture models are provided. All of the machinery developed in this work is constructive in the sense that when a test with a certain property is shown to exist it is possible to explicitly construct this test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08259v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew McCormack</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Model Averaging via Cross-Validation</title>
      <link>https://arxiv.org/abs/2506.08451</link>
      <description>arXiv:2506.08451v1 Announce Type: new 
Abstract: Model averaging is an important alternative to model selection with attractive prediction accuracy. However, its application to high-dimensional data remains under-explored. We propose a high-dimensional model averaging method via cross-validation under a general framework and systematically establish its theoretical properties. Each candidate model is fitted using a flexible loss function paired with a general regularizer, and the optimal weights are determined by minimizing a cross-validation criterion. When all candidate models are misspecified, we establish a non-asymptotic upper bound and a minimax lower bound for our weight estimator. The asymptotic optimality is also derived, showing that the proposed weight estimator achieves the lowest possible prediction risk asymptotically. When the correct models are included in the candidate model set, the proposed method asymptotically assigns all weights to the correct models, and the model averaging estimator achieves a nearly-oracle convergence rate. Further, we introduce a post-averaging debiased estimator and establish Gaussian and bootstrap approximation to construct simultaneous confidence intervals. A fast greedy model averaging (FGMA) algorithm is proposed to solve the simplex-constrained optimization problem, which has a descent property empirically and a faster convergence rate, compared to the original greedy model averaging algorithm. Empirical results demonstrate the strong competitiveness of the proposed method in prediction and inference, compared to other existing model averaging and selection methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08451v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhengyan Wan, Fang Fang, Binyan Jiang</dc:creator>
    </item>
    <item>
      <title>Generalizing while preserving monotonicity in comparison-based preference learning models</title>
      <link>https://arxiv.org/abs/2506.08616</link>
      <description>arXiv:2506.08616v1 Announce Type: new 
Abstract: If you tell a learning model that you prefer an alternative $a$ over another alternative $b$, then you probably expect the model to be monotone, that is, the valuation of $a$ increases, and that of $b$ decreases. Yet, perhaps surprisingly, many widely deployed comparison-based preference learning models, including large language models, fail to have this guarantee. Until now, the only comparison-based preference learning algorithms that were proved to be monotone are the Generalized Bradley-Terry models. Yet, these models are unable to generalize to uncompared data. In this paper, we advance the understanding of the set of models with generalization ability that are monotone. Namely, we propose a new class of Linear Generalized Bradley-Terry models with Diffusion Priors, and identify sufficient conditions on alternatives' embeddings that guarantee monotonicity. Our experiments show that this monotonicity is far from being a general guarantee, and that our new class of generalizing models improves accuracy, especially when the dataset is limited.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08616v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julien Fageot, Peva Blanchard, Gilles Bareilles, L\^e-Nguy\^en Hoang</dc:creator>
    </item>
    <item>
      <title>Wasserstein and Convex Gaussian Approximations for Non-stationary Time Series of Diverging Dimensionality</title>
      <link>https://arxiv.org/abs/2506.08723</link>
      <description>arXiv:2506.08723v1 Announce Type: new 
Abstract: In high-dimensional time series analysis, Gaussian approximation (GA) schemes under various distance measures or on various collections of subsets of the Euclidean space play a fundamental role in a wide range of statistical inference problems. To date, most GA results for high-dimensional time series are established on hyper-rectangles and their equivalence. In this paper, by considering the 2-Wasserstein distance and the collection of all convex sets, we establish a general GA theory for a broad class of high-dimensional non-stationary (HDNS) time series, extending the scope of problems that can be addressed in HDNS time series analysis. For HDNS time series of sufficiently weak dependence and light tail, the GA rates established in this paper are either nearly optimal with respect to the dimensionality and time series length, or they are nearly identical to the corresponding best-known GA rates established for independent data. A multiplier bootstrap procedure is utilized and theoretically justified to implement our GA theory. We demonstrate by two previously undiscussed time series applications the use of the GA theory and the bootstrap procedure as unified tools for a wide range of statistical inference problems in HDNS time series analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08723v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miaoshiqi Liu, Jun Yang, Zhou Zhou</dc:creator>
    </item>
    <item>
      <title>Dirichlet kernel density estimation for strongly mixing sequences on the simplex</title>
      <link>https://arxiv.org/abs/2506.08816</link>
      <description>arXiv:2506.08816v1 Announce Type: new 
Abstract: This paper investigates the theoretical properties of Dirichlet kernel density estimators for compositional data supported on simplices, for the first time addressing scenarios involving time-dependent observations characterized by strong mixing conditions. We establish rigorous results for the asymptotic normality and mean squared error of these estimators, extending previous findings from the independent and identically distributed (iid) context to the more general setting of strongly mixing processes. To demonstrate its practical utility, the estimator is applied to monthly market-share compositions of several Renault vehicle classes over a twelve-year period, with bandwidth selection performed via leave-one-out least squares cross-validation. Our findings underscore the reliability and strength of Dirichlet kernel techniques when applied to temporally dependent compositional data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08816v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hanen Daayeb, Salah Khardani, Fr\'ed\'eric Ouimet</dc:creator>
    </item>
    <item>
      <title>On the Bernstein-smoothed lower-tail Spearman's rho estimator</title>
      <link>https://arxiv.org/abs/2506.08857</link>
      <description>arXiv:2506.08857v1 Announce Type: new 
Abstract: This note develops a Bernstein estimator for lower-tail Spearman's rho and establishes its strong consistency and asymptotic normality under mild regularity conditions. Smoothing the empirical copula yields a strictly smaller mean squared error (MSE) in tail regions by lowering sampling variance relative to the classical Spearman's rho estimator. A Monte Carlo simulation experiment with the Farlie--Gumbel--Morgenstern copula demonstrates variance reductions that translate into lower MSE estimates (up to $\sim 70\%$ lower) at deep-tail thresholds under weak to moderate dependence and small sample sizes. To facilitate reproducibility of the findings, the R code that generated all simulation results is readily accessible online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08857v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fr\'ed\'eric Ouimet, Selim Orhun Susam</dc:creator>
    </item>
    <item>
      <title>On Monotonicity in AI Alignment</title>
      <link>https://arxiv.org/abs/2506.08998</link>
      <description>arXiv:2506.08998v1 Announce Type: new 
Abstract: Comparison-based preference learning has become central to the alignment of AI models with human preferences. However, these methods may behave counterintuitively. After empirically observing that, when accounting for a preference for response $y$ over $z$, the model may actually decrease the probability (and reward) of generating $y$ (an observation also made by others), this paper investigates the root causes of (non) monotonicity, for a general comparison-based preference learning framework that subsumes Direct Preference Optimization (DPO), Generalized Preference Optimization (GPO) and Generalized Bradley-Terry (GBT). Under mild assumptions, we prove that such methods still satisfy what we call local pairwise monotonicity. We also provide a bouquet of formalizations of monotonicity, and identify sufficient conditions for their guarantee, thereby providing a toolbox to evaluate how prone learning models are to monotonicity violations. These results clarify the limitations of current methods and provide guidance for developing more trustworthy preference learning algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08998v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gilles Bareilles, Julien Fageot, L\^e-Nguy\^en Hoang, Peva Blanchard, Wassim Bouaziz, S\'ebastien Rouault, El-Mahdi El-Mhamdi</dc:creator>
    </item>
    <item>
      <title>Standard LSParameter Estimators Ensure Finite Convergence Time for Linear Regression Equations Under an Interval Excitation Assumption</title>
      <link>https://arxiv.org/abs/2506.08211</link>
      <description>arXiv:2506.08211v1 Announce Type: cross 
Abstract: In this brief note we recall the little-known fact that, for linear regression equations (LRE) with intervally excited (IE) regressors, standard Least Square (LS) parameter estimators ensure finite convergence time (FCT) of the estimated parameters. The convergence time being equal to the time length needed to comply with the IE assumption. As is well-known, IE is necessary and sufficient for the identifiability of the LRE-hence, it is the weakest assumption for the on-or off-line solution of the parameter estimation problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08211v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Romeo Ortega, Jose Guadalupe Romero, Stanislav Aranovskiy, Gang Tao</dc:creator>
    </item>
    <item>
      <title>Private Evolution Converges</title>
      <link>https://arxiv.org/abs/2506.08312</link>
      <description>arXiv:2506.08312v1 Announce Type: cross 
Abstract: Private Evolution (PE) is a promising training-free method for differentially private (DP) synthetic data generation. While it achieves strong performance in some domains (e.g., images and text), its behavior in others (e.g., tabular data) is less consistent. To date, the only theoretical analysis of the convergence of PE depends on unrealistic assumptions about both the algorithm's behavior and the structure of the sensitive dataset. In this work, we develop a new theoretical framework to explain PE's practical behavior and identify sufficient conditions for its convergence. For $d$-dimensional sensitive datasets with $n$ data points from a bounded domain, we prove that PE produces an $(\epsilon, \delta)$-DP synthetic dataset with expected 1-Wasserstein distance of order $\tilde{O}(d(n\epsilon)^{-1/d})$ from the original, establishing worst-case convergence of the algorithm as $n \to \infty$. Our analysis extends to general Banach spaces as well. We also connect PE to the Private Signed Measure Mechanism, a method for DP synthetic data generation that has thus far not seen much practical adoption. We demonstrate the practical relevance of our theoretical findings in simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08312v1</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tom\'as Gonz\'alez, Giulia Fanti, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Improved Scaling Laws in Linear Regression via Data Reuse</title>
      <link>https://arxiv.org/abs/2506.08415</link>
      <description>arXiv:2506.08415v1 Announce Type: cross 
Abstract: Neural scaling laws suggest that the test error of large language models trained online decreases polynomially as the model size and data size increase. However, such scaling can be unsustainable when running out of new data. In this work, we show that data reuse can improve existing scaling laws in linear regression. Specifically, we derive sharp test error bounds on $M$-dimensional linear models trained by multi-pass stochastic gradient descent (multi-pass SGD) on $N$ data with sketched features. Assuming that the data covariance has a power-law spectrum of degree $a$, and that the true parameter follows a prior with an aligned power-law spectrum of degree $b-a$ (with $a &gt; b &gt; 1$), we show that multi-pass SGD achieves a test error of $\Theta(M^{1-b} + L^{(1-b)/a})$, where $L \lesssim N^{a/b}$ is the number of iterations. In the same setting, one-pass SGD only attains a test error of $\Theta(M^{1-b} + N^{(1-b)/a})$ (see e.g., Lin et al., 2024). This suggests an improved scaling law via data reuse (i.e., choosing $L&gt;N$) in data-constrained regimes. Numerical simulations are also provided to verify our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08415v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Licong Lin, Jingfeng Wu, Peter L. Bartlett</dc:creator>
    </item>
    <item>
      <title>Transforming the Erd\H{o}s-Kac theorem</title>
      <link>https://arxiv.org/abs/2506.08503</link>
      <description>arXiv:2506.08503v1 Announce Type: cross 
Abstract: Transforming the Erd\H{o}s-Kac theorem provides more flexibility in how the theorem can be utilized as an interval estimate for the prime omega function, which counts the number of distinct prime divisors. Here, we consider a direct transformation by the delta method. Then, we demonstrate that the square-root and three-quarters power asymptotically achieve variance stabilization and an optimal width, respectively. Furthermore, by adjusting the denominator of the theorem, we derive the score interval estimate. To make these interval estimates reliable for small positive integers, we examine performances of various interval estimates for the prime omega function using fuzzy coverage probabilities. The results indicate that the score interval estimate performs well even for small positive integers after training the mean and standard deviation using the prime omega function. Moreover, the Poisson interval estimate is relatively reliable with or without training. Additional theoretical results on the Erd\H{o}s-Pomerance theorem are also provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08503v1</guid>
      <category>math.NT</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kimihiro Noguchi</dc:creator>
    </item>
    <item>
      <title>On Limiting Probability Distributions of Higher Order Markov Chains</title>
      <link>https://arxiv.org/abs/2506.08874</link>
      <description>arXiv:2506.08874v1 Announce Type: cross 
Abstract: The limiting (or stationary) probability distribution is one of the key characteristics of a Markov chain since it shows its long-term behavior. In this paper, for a higher order Markov chain, we establish a sufficient condition for the existence of its limiting probability distribution. This condition is built upon the regularity of its transition tensor. Our results extend the corresponding conclusions for first order chains. Besides, they complement the existing results concerning higher order chains that rely on approximation schemes or two-phase power iterations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08874v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lixing Han, Jianhong Xu</dc:creator>
    </item>
    <item>
      <title>Enabling stratified sampling in high dimensions via nonlinear dimensionality reduction</title>
      <link>https://arxiv.org/abs/2506.08921</link>
      <description>arXiv:2506.08921v1 Announce Type: cross 
Abstract: We consider the problem of propagating the uncertainty from a possibly large number of random inputs through a computationally expensive model. Stratified sampling is a well-known variance reduction strategy, but its application, thus far, has focused on models with a limited number of inputs due to the challenges of creating uniform partitions in high dimensions. To overcome these challenges, we perform stratification with respect to the uniform distribution defined over the unit interval, and then derive the corresponding strata in the original space using nonlinear dimensionality reduction. We show that our approach is effective in high dimensions and can be used to further reduce the variance of multifidelity Monte Carlo estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08921v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gianluca Geraci, Daniele E. Schiavazzi, Andrea Zanoni</dc:creator>
    </item>
    <item>
      <title>Estimates of MM type for the multivariate linear model</title>
      <link>https://arxiv.org/abs/1004.4883</link>
      <description>arXiv:1004.4883v5 Announce Type: replace 
Abstract: We propose a class of robust estimates for multivariate linear models. Based on the approach of MM estimation (Yohai 1987), we estimate the regression coefficients and the covariance matrix of the errors simultaneously. These estimates have both high breakdown point and high asymptotic efficiency under Gaussian errors. We prove consistency and asymptotic normality assuming errors with an elliptical distribution. We describe an iterative algorithm for the numerical calculation of these estimates. The advantages of the proposed estimates over their competitors are demonstrated through both simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:1004.4883v5</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jmva.2011.04.011</arxiv:DOI>
      <arxiv:journal_reference>Journal of Multivariate Analysis, Volume 102, Issue 9, October 2011, Pages 1280-1292</arxiv:journal_reference>
      <dc:creator>Nadia L. Kudraszow, Ricardo A. Maronna</dc:creator>
    </item>
    <item>
      <title>Nonsmooth Nonparametric Regression via Fractional Laplacian Eigenmaps</title>
      <link>https://arxiv.org/abs/2402.14985</link>
      <description>arXiv:2402.14985v2 Announce Type: replace 
Abstract: We develop nonparametric regression methods for the case when the true regression function is not necessarily smooth. More specifically, our approach is using the fractional Laplacian and is designed to handle the case when the true regression function lies in an $L_2$-fractional Sobolev space with order $s\in (0,1)$. This function class is a Hilbert space lying between the space of square-integrable functions and the first-order Sobolev space consisting of differentiable functions. It contains fractional power functions, piecewise constant or polynomial functions and bump function as canonical examples. For the proposed approach, we prove upper bounds on the in-sample mean-squared estimation error of order $n^{-\frac{2s}{2s+d}}$, where $d$ is the dimension, $s$ is the aforementioned order parameter and $n$ is the number of observations. We also provide preliminary empirical results validating the practical performance of the developed estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.14985v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaoyang Shi, Krishnakumar Balasubramanian, Wolfgang Polonik</dc:creator>
    </item>
    <item>
      <title>Subsampling for Big Data Linear Models with Measurement Errors</title>
      <link>https://arxiv.org/abs/2403.04361</link>
      <description>arXiv:2403.04361v3 Announce Type: replace 
Abstract: Subsampling algorithms for various parametric regression models with massive data have been extensively investigated in recent years. However, all existing studies on subsampling heavily rely on clean massive data. In practical applications, the observed covariates may suffer from inaccuracies due to measurement errors. To address the challenge of large datasets with measurement errors, this study explores two subsampling algorithms based on the corrected likelihood approach: the optimal subsampling algorithm utilizing inverse probability weighting and the perturbation subsampling algorithm employing random weighting assuming a perfectly known distribution. Theoretical properties for both algorithms are provided. Numerical simulations and two real-world examples demonstrate the effectiveness of these proposed methods compared to other uncorrected algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04361v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jiangshan Ju, Mingqiu Wang, Shengli Zhao</dc:creator>
    </item>
    <item>
      <title>Minimax optimal seriation in polynomial time</title>
      <link>https://arxiv.org/abs/2405.08747</link>
      <description>arXiv:2405.08747v2 Announce Type: replace 
Abstract: We consider the seriation problem, where the statistician seeks to recover a hidden ordering from a noisy observation of a permuted Robinson matrix. We tightly characterize the minimax rate of this problem on a general class of matrices which satisfy some local assumptions, and we provide a polynomial time algorithm achieving this rate. Our general results cover the special case of bi-Lipschitz matrices, thereby answering two open questions of [Giraud et al., 2021]. Our analysis further extends to broader classes of matrices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08747v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yann Issartel, Christophe Giraud, Nicolas Verzelen</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Cumulative INAR($\infty$) Processes via Least-Squares</title>
      <link>https://arxiv.org/abs/2412.01569</link>
      <description>arXiv:2412.01569v4 Announce Type: replace 
Abstract: This paper investigates the cumulative Integer-Valued Autoregressive model of infinite order, denoted as INAR($\infty$), a class of processes crucial for modeling count time series and equivalent to discrete-time Hawkes processes. We propose a computationally efficient conditional least-squares (CLS) estimator to address the challenge of parameter inference in this infinite-dimensional setting. We establish the key theoretical properties of the estimator, including its consistency and asymptotic normality. A central contribution is the rigorous treatment of its large-sample distribution in a framework where the parameter dimension grows with the sample size, for which we derive the corresponding sandwich-form covariance matrix. The theoretical results are substantiated through comprehensive Monte Carlo simulations. These experiments demonstrate that the estimator's accuracy and stability systematically improve as the sample size increases, confirming its consistency. Furthermore, we show that the estimator's finite-sample distribution is well-approximated by a normal distribution, and this approximation becomes more robust with larger samples. Our work provides a complete and practical framework for statistical inference in cumulative INAR($\infty$) models. The code to reproduce the numerical experiments is publicly available at https://github.com/gagawjbytw/INAR_estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01569v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yingli Wang, Xiaohong Duan, Ping He</dc:creator>
    </item>
    <item>
      <title>Scaling Laws in Linear Regression: Compute, Parameters, and Data</title>
      <link>https://arxiv.org/abs/2406.08466</link>
      <description>arXiv:2406.08466v3 Announce Type: replace-cross 
Abstract: Empirically, large-scale deep learning models often satisfy a neural scaling law: the test error of the trained model improves polynomially as the model size and data size grow. However, conventional wisdom suggests the test error consists of approximation, bias, and variance errors, where the variance error increases with model size. This disagrees with the general form of neural scaling laws, which predict that increasing model size monotonically improves performance.
  We study the theory of scaling laws in an infinite dimensional linear regression setup. Specifically, we consider a model with $M$ parameters as a linear function of sketched covariates. The model is trained by one-pass stochastic gradient descent (SGD) using $N$ data. Assuming the optimal parameter satisfies a Gaussian prior and the data covariance matrix has a power-law spectrum of degree $a&gt;1$, we show that the reducible part of the test error is $\Theta(M^{-(a-1)} + N^{-(a-1)/a})$. The variance error, which increases with $M$, is dominated by the other errors due to the implicit regularization of SGD, thus disappearing from the bound. Our theory is consistent with the empirical neural scaling laws and verified by numerical simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08466v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Licong Lin, Jingfeng Wu, Sham M. Kakade, Peter L. Bartlett, Jason D. Lee</dc:creator>
    </item>
    <item>
      <title>Spectral Estimators for Multi-Index Models: Precise Asymptotics and Optimal Weak Recovery</title>
      <link>https://arxiv.org/abs/2502.01583</link>
      <description>arXiv:2502.01583v2 Announce Type: replace-cross 
Abstract: Multi-index models provide a popular framework to investigate the learnability of functions with low-dimensional structure and, also due to their connections with neural networks, they have been object of recent intensive study. In this paper, we focus on recovering the subspace spanned by the signals via spectral estimators -- a family of methods routinely used in practice, often as a warm-start for iterative algorithms. Our main technical contribution is a precise asymptotic characterization of the performance of spectral methods, when sample size and input dimension grow proportionally and the dimension $p$ of the space to recover is fixed. Specifically, we locate the top-$p$ eigenvalues of the spectral matrix and establish the overlaps between the corresponding eigenvectors (which give the spectral estimators) and a basis of the signal subspace. Our analysis unveils a phase transition phenomenon in which, as the sample complexity grows, eigenvalues escape from the bulk of the spectrum and, when that happens, eigenvectors recover directions of the desired subspace. The precise characterization we put forward enables the optimization of the data preprocessing, thus allowing to identify the spectral estimator that requires the minimal sample size for weak recovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01583v2</guid>
      <category>stat.ML</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Filip Kova\v{c}evi\'c, Yihan Zhang, Marco Mondelli</dc:creator>
    </item>
  </channel>
</rss>
