<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Jun 2025 04:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The Case for a Horizontal Federated AI operating System for Telcos</title>
      <link>https://arxiv.org/abs/2506.17259</link>
      <description>arXiv:2506.17259v1 Announce Type: new 
Abstract: As artificial intelligence capabilities rapidly advance, Telco operators face a growing need to unify fragmented AI efforts across customer experience, network operations, and service orchestration. This paper proposes the design and deployment of a horizontal federated AI operating system tailored for the telecommunications domain. Unlike vertical vendor-driven platforms, this system acts as a common execution and coordination layer, enabling Telcos to deploy AI agents at scale while preserving data locality, regulatory compliance, and architectural heterogeneity. We argue that such an operating system must expose tightly scoped abstractions for telemetry ingestion, agent execution, and model lifecycle management. It should support federated training across sovereign operators, offer integration hooks into existing OSS and BSS systems, and comply with TM Forum and O-RAN standards. Importantly, the platform must be governed through a neutral foundation model to ensure portability, compatibility, and multi-vendor extensibility. This architecture offers a path to break the current silos, unlock ecosystem-level intelligence, and provide a foundation for agent-based automation across the Telco stack. The case for this horizontal layer is not only technical but structural, redefining how intelligence is deployed and composed in a distributed network environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17259v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Sebastian Barros</dc:creator>
    </item>
    <item>
      <title>Solving the Problem of Poor Internet Connectivity in Dhaka: Innovative Solutions Using Advanced WebRTC and Adaptive Streaming Technologies</title>
      <link>https://arxiv.org/abs/2506.17343</link>
      <description>arXiv:2506.17343v1 Announce Type: new 
Abstract: Dhaka, Bangladesh, one of the world's most densely populated cities, faces severe challenges in maintaining reliable, high-speed internet connectivity. This paper presents an innovative framework that addresses poor mobile data connections through the integration of advanced WebRTC technology with adaptive streaming and server-side recording solutions. Focusing on the unique network conditions in Dhaka in 2025, our approach combines dynamic transcoding, real-time error correction, and optimized interface selection to enhance connectivity. We analyze empirical data on connection speeds, mobile tower density, district-level population statistics, and social media usage. Extensive mathematical formulations, including novel models for bitrate estimation, round-trip time optimization, and reliability analysis, are provided alongside detailed diagrams and multiple examples of code in both Python and C++. Experimental results demonstrate significant improvements in throughput, latency reduction, and overall service quality, offering a scalable blueprint for next-generation communication systems in hyper-dense urban environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17343v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.56726/IRJMETS68451</arxiv:DOI>
      <arxiv:journal_reference>Int. Res. J. Mod. Eng. Technol. Sci. 7(3) (2025) 1766-1777</arxiv:journal_reference>
      <dc:creator>Pavel Malinovskiy</dc:creator>
    </item>
    <item>
      <title>VReaves: Eavesdropping on Virtual Reality App Identity and Activity via Electromagnetic Side Channels</title>
      <link>https://arxiv.org/abs/2506.17570</link>
      <description>arXiv:2506.17570v1 Announce Type: new 
Abstract: Virtual reality (VR) has recently proliferated significantly, consisting of headsets or head-mounted displays (HMDs) and hand controllers for an embodied and immersive experience. The VR device is usually embedded with different kinds of IoT sensors, such as cameras, microphones, communication sensors, etc. However, VR security has not been scrutinized from a physical hardware point of view, especially electromagnetic emanations (EM) that are automatically and unintentionally emitted from the VR headset. This paper presents VReaves, a system that can eavesdrop on the electromagnetic emanation side channel of a VR headset for VR app identification and activity recognition. To do so, we first characterize the electromagnetic emanations from the embedded IoT sensors (e.g., cameras and microphones) in the VR headset through a signal processing pipeline and further propose machine learning models to identify the VR app and recognize the VR app activities. Our experimental evaluation with commercial off-the-shelf VR devices demonstrates the efficiency of VR app identification and activity recognition via electromagnetic emanation side channel.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17570v1</guid>
      <category>cs.NI</category>
      <category>cs.CR</category>
      <category>cs.HC</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sun Wei, Fang Minghong, Li Mengyuan</dc:creator>
    </item>
    <item>
      <title>Non-Intrusive MLOps-Driven Performance Intelligence in Software Data Planes</title>
      <link>https://arxiv.org/abs/2506.17658</link>
      <description>arXiv:2506.17658v1 Announce Type: new 
Abstract: The last decade has witnessed the proliferation of network function virtualization (NFV) in the telco industry, thanks to its unparalleled flexibility, scalability, and cost-effectiveness. However, as the NFV infrastructure is shared by virtual network functions (VNFs), sporadic resource contentions are inevitable. Such contention makes it extremely challenging to guarantee the performance of the provisioned network services, especially in high-speed regimes (e.g., Gigabit Ethernet). Existing solutions typically rely on direct traffic analysis (e.g., packet- or flow-level measurements) to detect performance degradation and identify bottlenecks, which is not always applicable due to significant integration overhead and system-level constraints.
  This paper complements existing solutions with a lightweight, non-intrusive framework for online performance inference and adaptation. Instead of direct data-plane collection, we reuse hardware features in the underlying NFV infrastructure, introducing negligible interference in the data plane. This framework can be integrated into existing NFV systems with minimal engineering effort and operates without the need for predefined traffic models or VNF-specific customization. Through comprehensive evaluation across diverse NFV scenarios, our Drift-Resilient and Self-Tuning (DRST) framework delivers accurate performance inference, runtime bottleneck diagnose, and automated adaptation under runtime drift, via a lightweight MLOps pipeline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17658v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiong Liu, Jianke Lin, Tianzhu Zhang, Leonardo Linguaglossa</dc:creator>
    </item>
    <item>
      <title>Location Information Sharing Using Software Defined Radio in Multi-UAV Systems</title>
      <link>https://arxiv.org/abs/2506.17678</link>
      <description>arXiv:2506.17678v1 Announce Type: new 
Abstract: SDR (Software Defined Radio) provides flexible, reproducible, and longer-lasting radio tools for military and civilian wireless communications infrastructure. SDR is a radio communication system whose components are implemented as software. This study aims to establish multi-channel wireless communication with FANET between two SDRs to share location information and examine it in a realistic test environment. We used multi-channel token circulation as a channel access protocol and GNU Radio platform for SDR software development. The structures of the communication layer, including the protocols, communication systems, and network structures suggested in the studies in the literature, are generally tested in the simulation environment. The simulation environment provides researchers with fast and easy development and testing, but disadvantages exist. These cause a product to be isolated from hardware, software, and cost effects encountered while developing and environmental factors affecting the communication channel while testing. Another contribution of the study is to present the developed block diagrams and codes as clear and reproducible. The developed software and block diagrams are available at github.com/knrl/uav-in-802.11-gnuradio.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17678v1</guid>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mehmet Kaan Erol, Eyup Emre Ulku</dc:creator>
    </item>
    <item>
      <title>The Blind Spot of BGP Anomaly Detection: Why LSTM Autoencoders Fail on Real-World Outages</title>
      <link>https://arxiv.org/abs/2506.17821</link>
      <description>arXiv:2506.17821v1 Announce Type: new 
Abstract: Deep learning has significant potential to make the Internet's Border Gateway Protocol (BGP) secure by detecting anomalous routing activity. However, all but a few of these approaches rely on the implicit assumption that anomalies manifest as noisy, high-complexity outliers from some normal baseline. This work challenges this assumption by investigating if a best-in-class detection model built on this assumption can effectively deal with real-world security events' diverse signatures. We employ an LSTM-based autoencoder, a classical example of a reconstruction-based anomaly detector, as our test vehicle. We then contrast this model with a representative sampling of historical BGP anomalies, including the Slammer worm and the Moscow blackout, and with a simulated 'BGP storm' designed as a positive control. Our experience unveils a blind spot of our model: the model easily identifies the synthetic anomaly of high complexity but invariably fails to identify real-world events that manifest in the form of a "signal loss" (e.g., Slammer, Moscow Blackout) or "low-deviation" (e.g., WannaCry) signature. We demonstrate that the model mistakenly recognizes the abrupt cut-off of BGP updates during catastrophic failures as a signal of extreme stability, leading to reconstruction errors of virtually zero and total failure to detect. We conclude that the characterization of BGP anomalies as high-reconstruction-error events alone is a weak and dangerous oversimplification. Our research provides the data-driven case for why hybrid, multi-modal detection systems capable of identifying both high-complexity and signal-loss signatures are required to enable end-to-end BGP security.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17821v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel Oluwafemi Adebayo</dc:creator>
    </item>
    <item>
      <title>Supporting Deterministic Traffic on Standard NICs</title>
      <link>https://arxiv.org/abs/2506.17877</link>
      <description>arXiv:2506.17877v1 Announce Type: new 
Abstract: Networked mission-critical applications (e.g., avionic control and industrial automation systems) require deterministic packet transmissions to support a range of sensing and control tasks with stringent timing constraints. While specialized network infrastructure (e.g., time-sensitive networking (TSN) switches) provides deterministic data transport across the network, achieving strict end-to-end timing guarantees requires equally capable end devices to support deterministic traffic. These end devices, however, often employ general-purpose computing platforms like standard PCs, which lack native support for deterministic traffic and suffer from unpredictable delays introduced by their software stack and system architecture. Although specialized NICs with hardware scheduling offload can mitigate this problem, the limited compatibility hinders their widespread adoption, particularly for cost-sensitive applications or in legacy devices.
  To fill this gap, this paper proposes a novel software-based driver model, namely KeepON, to enable the support of deterministic packet transmissions on end devices equipped with standard NICs. The key idea of KeepON is to have the NIC keep on transmitting fixed-size data chunks as placeholders, thereby maintaining a predictable temporal transmission pattern. The real-time packets generated by the mission-critical application(s) will then be precisely inserted into this stream by replacing placeholders at the designated position to ensure their accurate transmission time. We implement and evaluate KeepON by modifying the network driver on a Raspberry Pi using its standard NIC. Our experiments demonstrate that KeepON can achieve x162 times scheduling accuracy comparable to its default driver, and x2.6 times compared to hardware-based solution, thus enabling precise timing control on standard commodity hardware.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17877v1</guid>
      <category>cs.NI</category>
      <category>cs.OS</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chuanyu Xue, Tianyu Zhang, Andrew Loveless, Song Han</dc:creator>
    </item>
    <item>
      <title>LiSec-RTF: Reinforcing RPL Resilience Against Routing Table Falsification Attack in 6LoWPAN</title>
      <link>https://arxiv.org/abs/2506.17911</link>
      <description>arXiv:2506.17911v1 Announce Type: new 
Abstract: Routing Protocol for Low-Power and Lossy Networks (RPL) is an energy-efficient routing solution for IPv6 over Low-Power Wireless Personal Area Networks (6LoWPAN), recommended for resource-constrained devices. While RPL offers significant benefits, its security vulnerabilities pose challenges, particularly due to unauthenticated control messages used to establish and maintain routing information. These messages are susceptible to manipulation, enabling malicious nodes to inject false routing data. A notable security concern is the Routing Table Falsification (RTF) attack, where attackers forge Destination Advertisement Object (DAO) messages to promote fake routes via a parent nodes routing table. Experimental results indicate that RTF attacks significantly reduce packet delivery ratio, increase end-to-end delay, and leverage power consumption. Currently, no effective countermeasures exist in the literature, reinforcing the need for a security solution to prevent network disruption and protect user applications. This paper introduces a Lightweight Security Solution against Routing Table Falsification Attack (LiSec-RTF), leveraging Physical Unclonable Functions (PUFs) to generate unique authentication codes, termed Licenses. LiSec-RTF mitigates RTF attack impact while considering the resource limitations of 6LoWPAN devices in both static and mobile scenarios. Our testbed experiments indicate that LiSec-RTF significantly improves network performance compared to standard RPL under RTF attacks, thereby ensuring reliable and efficient operation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17911v1</guid>
      <category>cs.NI</category>
      <category>cs.CR</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TMC.2025.3581561</arxiv:DOI>
      <dc:creator>Shefali Goel, Vinod Kumar Verma, Abhishek Verma</dc:creator>
    </item>
    <item>
      <title>Mapping The Invisible Internet: Framework and Dataset</title>
      <link>https://arxiv.org/abs/2506.18159</link>
      <description>arXiv:2506.18159v1 Announce Type: new 
Abstract: This article presents a novel dataset focusing on the network layer of the Invisible Internet Project (I2P), where prior research has predominantly examined application layers like the dark web. Data was collected through the SWARM- I2P framework, deploying I2P routers as mapping agents, utilizing dynamic port mapping (30000-50000 range). The dataset documents over 50,000 nodes, including 2,077 FastSet nodes and 2,331 high-capacity nodes characterized by bandwidth, latency (mean 121.21ms +- 48.50), and uptime metrics. It contains 1,997 traffic records (1,003,032 packets/bytes) and 4,222,793 records (2,147,585,625 packets/bytes), with geographic distributions for 3,444 peers showing capacity metrics (mean 8.57 +- 1.20). Collection methods included router console queries (127.0.0.1:port/tunnels), netDb analysis, and passive monitoring, with anonymized identifiers. Data is structured in CSV/TXT formats (Zenodo) with collection scripts (GitHub). Potential applications include tunnel peer selection analysis, anonymity network resilience studies, and adversarial modelling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18159v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Siddique Abubakr Muntaka, Jacques Bou Abdo, Kemi Akanbi, Sunkanmi Oluwadare, Faiza Hussein, Oliver Konyo, Michael Asante</dc:creator>
    </item>
    <item>
      <title>Consistent Channel Hopping Algorithms for the Multichannel Rendezvous Problem with Heterogeneous Available Channel Sets</title>
      <link>https://arxiv.org/abs/2506.18381</link>
      <description>arXiv:2506.18381v1 Announce Type: new 
Abstract: We propose a theoretical framework for consistent channel hopping algorithms to address the multichannel rendezvous problem (MRP) in wireless networks with heterogeneous available channel sets. A channel selection function is called consistent if the selected channel remains unchanged when the available channel set shrinks, provided the selected channel is still available. We show that all consistent channel selection functions are equivalent to the function that always selects the smallest-index channel under appropriate channel relabeling. This leads to a natural representation of a consistent channel hopping algorithm as a sequence of permutations. For the two-user MRP, we characterize rendezvous time slots using a fictitious user and derive tight bounds on the maximum time-to-rendezvous (MTTR) and expected time-to-rendezvous (ETTR). Notably, the ETTR is shown to be the inverse of the Jaccard index when permutations are randomly selected. We also prove that consistent channel hopping algorithms maximize the rendezvous probability. To reduce implementation complexity, we propose the modulo algorithm, which uses modular arithmetic with one-cycle permutations and achieves performance comparable to locality-sensitive hashing (LSH)-based algorithms. The framework is extended to multiple users, with novel strategies such as stick-together, spread-out, and a hybrid method that accelerates rendezvous in both synchronous and asynchronous settings. Simulation results confirm the effectiveness and scalability of the proposed algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18381v1</guid>
      <category>cs.NI</category>
      <category>cs.PF</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiwei Liu, Yi-Chia Cheng, Cheng-Shang Chang</dc:creator>
    </item>
    <item>
      <title>XR Offloading Across Multiple Time Scales: The Roles of Power, Temperature, and Energy</title>
      <link>https://arxiv.org/abs/2506.18584</link>
      <description>arXiv:2506.18584v1 Announce Type: new 
Abstract: Extended reality (XR) devices, commonly known as wearables, must handle significant computational loads under tight latency constraints. To meet these demands, they rely on a combination of on-device processing and edge offloading. This letter focuses on offloading strategies for wearables by considering their impact across three time scales: instantaneous power consumption, short-term temperature fluctuations, and long-term battery duration. We introduce a comprehensive system model that captures these temporal dynamics, and propose a stochastic and stationary offloading strategy, called TAO (for temperature-aware offloading), designed to minimize the offloading cost while adhering to power, thermal, and energy constraints. Our performance evaluation, leveraging COMSOL models of real-world wearables, confirms that TAO reduces offloading cost by over 35% compared to state-of-the-art approaches, without violating the wearable operational limits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18584v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francesco Malandrino, Olga Chukhno, Alessandro Catania, Antonella Molinaro, Carla Fabiana Chiasserini</dc:creator>
    </item>
    <item>
      <title>RL-Driven Semantic Compression Model Selection and Resource Allocation in Semantic Communication Systems</title>
      <link>https://arxiv.org/abs/2506.18660</link>
      <description>arXiv:2506.18660v1 Announce Type: new 
Abstract: Semantic communication (SemCom) is an emerging paradigm that leverages semantic-level understanding to improve communication efficiency, particularly in resource-constrained scenarios. However, existing SemCom systems often overlook diverse computational and communication capabilities and requirements among different users. Motivated by the need to adaptively balance semantic accuracy, latency, and energy consumption, this paper presents a reinforcement learning (RL)-driven framework for semantic compression model (SCM) selection and resource allocation in multi-user SemCom systems. To address the challenges of balancing image reconstruction quality and communication performance, a system-level optimization metric called Rate-Distortion Efficiency (RDE) has been defined. The framework considers multiple SCMs with varying complexity and resource requirements. A proximal policy optimization (PPO)-based RL approach is developed to dynamically select SCMs and allocate bandwidth and power under non-convex constraints. Simulations demonstrate that the proposed method outperforms several baseline strategies. This paper also discusses the generalization ability, computational complexity, scalability, and practical implications of the framework for real-world SemCom systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18660v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyi Lin, Peizheng Li, Adnan Aijaz</dc:creator>
    </item>
    <item>
      <title>Adaptive Social Metaverse Streaming based on Federated Multi-Agent Deep Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2506.17342</link>
      <description>arXiv:2506.17342v1 Announce Type: cross 
Abstract: The social metaverse is a growing digital ecosystem that blends virtual and physical worlds. It allows users to interact socially, work, shop, and enjoy entertainment. However, privacy remains a major challenge, as immersive interactions require continuous collection of biometric and behavioral data. At the same time, ensuring high-quality, low-latency streaming is difficult due to the demands of real-time interaction, immersive rendering, and bandwidth optimization. To address these issues, we propose ASMS (Adaptive Social Metaverse Streaming), a novel streaming system based on Federated Multi-Agent Proximal Policy Optimization (F-MAPPO). ASMS leverages F-MAPPO, which integrates federated learning (FL) and deep reinforcement learning (DRL) to dynamically adjust streaming bit rates while preserving user privacy. Experimental results show that ASMS improves user experience by at least 14% compared to existing streaming methods across various network conditions. Therefore, ASMS enhances the social metaverse experience by providing seamless and immersive streaming, even in dynamic and resource-constrained networks, while ensuring that sensitive user data remains on local devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17342v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.MM</category>
      <category>cs.NI</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TCSS.2025.3555419</arxiv:DOI>
      <dc:creator>Zijian Long, Haopeng Wang, Haiwei Dong, Abdulmotaleb El Saddik</dc:creator>
    </item>
    <item>
      <title>Secret Sharing in 5G-MEC: Applicability for joint Security and Dependability</title>
      <link>https://arxiv.org/abs/2506.17371</link>
      <description>arXiv:2506.17371v1 Announce Type: cross 
Abstract: Multi-access Edge Computing (MEC), an enhancement of 5G, processes data closer to its generation point, reducing latency and network load. However, the distributed and edge-based nature of 5G-MEC presents privacy and security challenges, including data exposure risks. Ensuring efficient manipulation and security of sensitive data at the edge is crucial. To address these challenges, we investigate the usage of threshold secret sharing in 5G-MEC storage, an approach that enhances both security and dependability. A (k,n) threshold secret sharing scheme splits and stores sensitive data among n nodes, requiring at least k nodes for reconstruction. The solution ensures confidentiality by protecting data against fewer than k colluding nodes and enhances availability by tolerating up to n-k failing nodes. This approach mitigates threats such as unauthorized access and node failures, whether accidental or intentional. We further discuss a method for selecting the convenient MEHs to store the shares, considering the MEHs' trustworthiness level as a main criterion. Although we define our proposal in the context of secret-shared data storage, it can be seen as an independent, standalone selection process for 5G-MEC trustworthy node selection in other scenarios too.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17371v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thilina Pathirana, Ruxandra F. Olimid</dc:creator>
    </item>
    <item>
      <title>Crowdsourcing Ubiquitous Indoor Localization with Non-Cooperative Wi-Fi Ranging</title>
      <link>https://arxiv.org/abs/2506.18317</link>
      <description>arXiv:2506.18317v1 Announce Type: cross 
Abstract: Indoor localization opens the path to potentially transformative applications. Although many indoor localization methods have been proposed over the years, they remain too impractical for widespread deployment in the real world. In this paper, we introduce PeepLoc, a deployable and scalable Wi-Fi-based solution for indoor localization that relies only on pre-existing devices and infrastructure. Specifically, PeepLoc works on any mobile device with an unmodified Wi-Fi transceiver and in any indoor environment with a sufficient number of Wi-Fi access points (APs) and pedestrian traffic. At the core of PeepLoc is (a) a mechanism which allows any Wi-Fi device to obtain non-cooperative time-of-flight (ToF) to any Wi-Fi AP and (b) a novel bootstrapping mechanism that relies on pedestrian dead reckoning (PDR) and crowdsourcing to opportunistically initialize pre-existing APs as anchor points within an environment. We implement PeepLoc using commodity hardware and evaluate it extensively across 4 campus buildings. We show PeepLoc leads to a mean and median positional error of 3.41 m and 3.06 m respectively, which is superior to existing deployed indoor localization systems and is competitive with commodity GPS in outdoor environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18317v1</guid>
      <category>cs.HC</category>
      <category>cs.NI</category>
      <category>cs.RO</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emerson Sie, Enguang Fan, Federico Cifuentes-Urtubey, Deepak Vasisht</dc:creator>
    </item>
    <item>
      <title>Enhancing Network Failure Mitigation with Performance-Aware Ranking</title>
      <link>https://arxiv.org/abs/2305.13792</link>
      <description>arXiv:2305.13792v2 Announce Type: replace 
Abstract: Cloud providers install mitigations to reduce the impact of network failures within their datacenters. Existing network mitigation systems rely on simple local criteria or global proxy metrics to determine the best action. In this paper, we show that we can support a broader range of actions and select more effective mitigations by directly optimizing end-to-end flow-level metrics and analyzing actions holistically. To achieve this, we develop novel techniques to quickly estimate the impact of different mitigations and rank them with high fidelity. Our results on incidents from a large cloud provider show orders of magnitude improvements in flow completion time and throughput. We also show our approach scales to large datacenters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.13792v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pooria Namyar, Arvin Ghavidel, Daniel Crankshaw, Daniel S. Berger, Kevin Hsieh, Srikanth Kandula, Ramesh Govindan, Behnaz Arzani</dc:creator>
    </item>
    <item>
      <title>LayerZero</title>
      <link>https://arxiv.org/abs/2312.09118</link>
      <description>arXiv:2312.09118v3 Announce Type: replace 
Abstract: In this paper, we present the first intrinsically secure and semantically universal omnichain interoperability protocol: LayerZero. Utilizing an immutable endpoint, append-only verification modules, and fully-configurable verification infrastructure, LayerZero provides the security, configurability, and extensibility necessary to achieve omnichain interoperability. LayerZero enforces strict application-exclusive ownership of protocol security and cost through its novel trust-minimized modular security framework which is designed to universally support all blockchains and use cases. Omnichain applications (OApps) built on the LayerZero protocol achieve frictionless blockchain-agnostic interoperation through LayerZero's universal network semantics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.09118v3</guid>
      <category>cs.NI</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryan Zarick, Bryan Pellegrino, Isaac Zhang, Thomas Kim, Caleb Banister</dc:creator>
    </item>
    <item>
      <title>Wireless-Friendly Window Position Optimization for RIS-Aided Outdoor-to-Indoor Networks based on Multi-Modal Large Language Model</title>
      <link>https://arxiv.org/abs/2410.20691</link>
      <description>arXiv:2410.20691v2 Announce Type: replace 
Abstract: This paper aims to simultaneously optimize indoor wireless and daylight performance by adjusting the positions of windows and the beam directions of window-deployed reconfigurable intelligent surfaces (RISs) for RIS-aided outdoor-to-indoor (O2I) networks utilizing large language models (LLM) as optimizers. Firstly, we illustrate the wireless and daylight system models of RIS-aided O2I networks and formulate a joint optimization problem to enhance both wireless traffic sum rate and daylight illumination performance. Then, we present a multi-modal LLM-based window optimization (LMWO) framework, accompanied by a prompt construction template to optimize the overall performance in a zero-shot fashion, functioning as both an architect and a wireless network planner. Finally, we analyze the optimization performance of the LMWO framework and the impact of the number of windows, room size, number of RIS units, and daylight factor. Numerical results demonstrate that our proposed LMWO framework can achieve outstanding optimization performance in terms of initial performance, convergence speed, final outcomes, and time complexity, compared with classic optimization methods. The building's wireless performance can be significantly enhanced while ensuring indoor daylight performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20691v2</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinbo Hou, Kehai Qiu, Zitian Zhang, Yong Yu, Kezhi Wang, Stefano Capolongo, Jiliang Zhang, Zeyang Li, Jie Zhang</dc:creator>
    </item>
    <item>
      <title>Quark: Implementing Convolutional Neural Networks Entirely on Programmable Data Plane</title>
      <link>https://arxiv.org/abs/2501.15100</link>
      <description>arXiv:2501.15100v2 Announce Type: replace 
Abstract: The rapid development of programmable network devices and the widespread use of machine learning (ML) in networking have facilitated efficient research into intelligent data plane (IDP). Offloading ML to programmable data plane (PDP) enables quick analysis and responses to network traffic dynamics, and efficient management of network links. However, PDP hardware pipeline has significant resource limitations. For instance, Intel Tofino ASIC has only 10Mb SRAM in each stage, and lacks support for multiplication, division and floating-point operations. These constraints significantly hinder the development of IDP. This paper presents \quark, a framework that fully offloads convolutional neural network (CNN) inference onto PDP. \quark employs model pruning to simplify the CNN model, and uses quantization to support floating-point operations. Additionally, \quark divides the CNN into smaller units to improve resource utilization on the PDP. We have implemented a testbed prototype of \quark on both P4 hardware switch (Intel Tofino ASIC) and software switch (i.e., BMv2). Extensive evaluation results demonstrate that \quark achieves 97.3\% accuracy in anomaly detection task while using only 22.7\% of the SRAM resources on the Intel Tofino ASIC switch, completing inference tasks at line rate with an average latency of 42.66$\mu s$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15100v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mai Zhang, Lin Cui, Xiaoquan Zhang, Fung Po Tso, Zhen Zhang, Yuhui Deng, Zhetao Li</dc:creator>
    </item>
    <item>
      <title>Large Language Models powered Malicious Traffic Detection: Architecture, Opportunities and Case Study</title>
      <link>https://arxiv.org/abs/2503.18487</link>
      <description>arXiv:2503.18487v2 Announce Type: replace 
Abstract: Malicious traffic detection is a pivotal technology for network security to identify abnormal network traffic and detect network attacks. Large Language Models (LLMs) are trained on a vast corpus of text, have amassed remarkable capabilities of context-understanding and commonsense knowledge. This has opened up a new door for network attacks detection. Researchers have already initiated discussions regarding the application of LLMs on specific cyber-security tasks. Unfortunately, there remains a lack of comprehensive analysis on harnessing LLMs for traffic detection, as well as the opportunities and challenges. In this paper, we focus on unleashing the full potential of Large Language Models (LLMs) in malicious traffic detection. We present a holistic view of the architecture of LLM-powered malicious traffic detection, including the procedures of Pre-training, Fine-tuning, and Detection. Especially, by exploring the knowledge and capabilities of LLM, we identify three distinct roles LLM can act in traffic classification: Classifier, Encoder, and Predictor. For each of them, the modeling paradigm, opportunities and challenges are elaborated. Finally, we present our design on LLM-powered DDoS detection as a case study. The proposed framework attains accurate detection on carpet bombing DDoS by exploiting LLMs' capabilities in contextual mining. The evaluation shows its efficacy, exhibiting a nearly 35% improvement compared to existing systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18487v2</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinggong Zhang, Haotian Meng, Qingyang Li, Yunpeng Tan, Lei Zhang</dc:creator>
    </item>
    <item>
      <title>Small noise limits of Markov chains and the PageRank</title>
      <link>https://arxiv.org/abs/2503.05137</link>
      <description>arXiv:2503.05137v2 Announce Type: replace-cross 
Abstract: We recall the classical formulation of PageRank as the stationary distribution of a singularly perturbed irreducible Markov chain that is not irreducible when the perturbation parameter goes to zero. Specifically, we use the Markov chain tree theorem to derive explicit expressions for the PageRank. This analysis leads to some surprising results. These results are then extended to a much more general class of perturbations that subsume personalized PageRank. We also give examples where even simpler formulas for PageRank are possible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05137v2</guid>
      <category>math.PR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vivek S Borkar, S Sowmya, Raghavendra Tripathi</dc:creator>
    </item>
    <item>
      <title>SLED: A Speculative LLM Decoding Framework for Efficient Edge Serving</title>
      <link>https://arxiv.org/abs/2506.09397</link>
      <description>arXiv:2506.09397v2 Announce Type: replace-cross 
Abstract: Regardless of the advancements in device capabilities, efficient inferencing advanced large language models (LLMs) at the edge remains challenging due to limited device memory and power constraints. Existing strategies, such as aggressive quantization, pruning, or remote inference, trade accuracy for efficiency or lead to substantial cost burdens. This position paper introduces a new approach that leverages speculative decoding, previously viewed primarily as a decoding acceleration technique for autoregressive generation of LLMs, as a promising approach specifically adapted for edge computing by orchestrating computation across heterogeneous devices. We propose \acronym, a method that allows lightweight edge devices to draft multiple candidate tokens locally using diverse draft models, while a single, shared edge server efficiently batches and verifies the tokens utilizing a more precise target model. This approach supports device heterogeneity and reduces server-side memory footprint by avoiding the need to deploy multiple target models. Our initial experiments with Jetson Orin Nano, Raspberry Pi 4B/5, and an edge server equipped with 4 Nvidia A100 GPUs indicate substantial benefits: significantly increased system throughput, capacity, and better cost efficiency, all without sacrificing model accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09397v2</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiangchen Li, Dimitrios Spatharakis, Saeid Ghafouri, Jiakun Fan, Hans Vandierendonck, Deepu John, Bo Ji, Dimitrios Nikolopoulos</dc:creator>
    </item>
  </channel>
</rss>
