<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 13 May 2025 02:49:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Optimal Regret of Bernoulli Bandits under Global Differential Privacy</title>
      <link>https://arxiv.org/abs/2505.05613</link>
      <description>arXiv:2505.05613v1 Announce Type: cross 
Abstract: As sequential learning algorithms are increasingly applied to real life, ensuring data privacy while maintaining their utilities emerges as a timely question. In this context, regret minimisation in stochastic bandits under $\epsilon$-global Differential Privacy (DP) has been widely studied. Unlike bandits without DP, there is a significant gap between the best-known regret lower and upper bound in this setting, though they "match" in order. Thus, we revisit the regret lower and upper bounds of $\epsilon$-global DP algorithms for Bernoulli bandits and improve both. First, we prove a tighter regret lower bound involving a novel information-theoretic quantity characterising the hardness of $\epsilon$-global DP in stochastic bandits. Our lower bound strictly improves on the existing ones across all $\epsilon$ values. Then, we choose two asymptotically optimal bandit algorithms, i.e. DP-KLUCB and DP-IMED, and propose their DP versions using a unified blueprint, i.e., (a) running in arm-dependent phases, and (b) adding Laplace noise to achieve privacy. For Bernoulli bandits, we analyse the regrets of these algorithms and show that their regrets asymptotically match our lower bound up to a constant arbitrary close to 1. This refutes the conjecture that forgetting past rewards is necessary to design optimal bandit algorithms under global DP. At the core of our algorithms lies a new concentration inequality for sums of Bernoulli variables under Laplace mechanism, which is a new DP version of the Chernoff bound. This result is universally useful as the DP literature commonly treats the concentrations of Laplace noise and random variables separately, while we couple them to yield a tighter bound.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05613v1</guid>
      <category>stat.ML</category>
      <category>cs.CR</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Achraf Azize, Yulian Wu, Junya Honda, Francesco Orabona, Shinji Ito, Debabrota Basu</dc:creator>
    </item>
    <item>
      <title>Estimation and Inference in Boundary Discontinuity Designs</title>
      <link>https://arxiv.org/abs/2505.05670</link>
      <description>arXiv:2505.05670v1 Announce Type: cross 
Abstract: Boundary Discontinuity Designs are used to learn about treatment effects along a continuous boundary that splits units into control and treatment groups according to a bivariate score variable. These research designs are also called Multi-Score Regression Discontinuity Designs, a leading special case being Geographic Regression Discontinuity Designs. We study the statistical properties of commonly used local polynomial treatment effects estimators along the continuous treatment assignment boundary. We consider two distinct approaches: one based explicitly on the bivariate score variable for each unit, and the other based on their univariate distance to the boundary. For each approach, we present pointwise and uniform estimation and inference methods for the treatment effect function over the assignment boundary. Notably, we show that methods based on univariate distance to the boundary exhibit an irreducible large misspecification bias when the assignment boundary has kinks or other irregularities, making the distance-based approach unsuitable for empirical work in those settings. In contrast, methods based on the bivariate score variable do not suffer from that drawback. We illustrate our methods with an empirical application. Companion general-purpose software is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05670v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Rocio Titiunik, Ruiqi Rae Yu</dc:creator>
    </item>
    <item>
      <title>Approximations for the number of maxima and near-maxima in independent data</title>
      <link>https://arxiv.org/abs/2505.06088</link>
      <description>arXiv:2505.06088v1 Announce Type: cross 
Abstract: In the setting where we have $n$ independent observations of a random variable $X$, we derive explicit error bounds in total variation distance when approximating the number of observations equal to the maximum of the sample (in the case where $X$ is discrete) or the number of observations within a given distance of an order statistic of the sample (in the case where $X$ is absolutely continuous). The logarithmic and Poisson distributions are used as approximations in the discrete case, with proofs which include the development of Stein's method for a logarithmic target distribution. In the absolutely continuous case our approximations are by the negative binomial distribution, and are established by considering negative binomial approximation for mixed binomials. The cases where $X$ is geometric, Gumbel and uniform are used as illustrative examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06088v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fraser Daly</dc:creator>
    </item>
    <item>
      <title>Beyond the Mean: Limit Theory and Tests for Infinite-Mean Autoregressive Conditional Durations</title>
      <link>https://arxiv.org/abs/2505.06190</link>
      <description>arXiv:2505.06190v1 Announce Type: cross 
Abstract: Integrated autoregressive conditional duration (ACD) models serve as natural counterparts to the well-known integrated GARCH models used for financial returns. However, despite their resemblance, asymptotic theory for ACD is challenging and also not complete, in particular for integrated ACD. Central challenges arise from the facts that (i) integrated ACD processes imply durations with infinite expectation, and (ii) even in the non-integrated case, conventional asymptotic approaches break down due to the randomness in the number of durations within a fixed observation period. Addressing these challenges, we provide here unified asymptotic theory for the (quasi-) maximum likelihood estimator for ACD models; a unified theory which includes integrated ACD models. Based on the new results, we also provide a novel framework for hypothesis testing in duration models, enabling inference on a key empirical question: whether durations possess a finite or infinite expectation. We apply our results to high-frequency cryptocurrency ETF trading data. Motivated by parameter estimates near the integrated ACD boundary, we assess whether durations between trades in these markets have finite expectation, an assumption often made implicitly in the literature on point process models. Our empirical findings indicate infinite-mean durations for all the five cryptocurrencies examined, with the integrated ACD hypothesis rejected -- against alternatives with tail index less than one -- for four out of the five cryptocurrencies considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06190v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>q-fin.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giuseppe Cavaliere, Thomas Mikosch, Anders Rahbek, Frederik Vilandt</dc:creator>
    </item>
    <item>
      <title>Is speckle noise more challenging to mitigate than additive noise?</title>
      <link>https://arxiv.org/abs/2409.16585</link>
      <description>arXiv:2409.16585v2 Announce Type: replace 
Abstract: We study the problem of estimating a function in the presence of both speckle and additive noises, commonly referred to as the de-speckling problem. Although additive noise has been thoroughly explored in nonparametric estimation, speckle noise, prevalent in applications such as synthetic aperture radar, ultrasound imaging, and digital holography, has not received as much attention. Consequently, there is a lack of theoretical investigations into the fundamental limits of mitigating the speckle noise.This paper is the first step in filling this gap.
  Our focus is on investigating the minimax estimation error for estimating a $\beta$-H\"older continuous function and determining the rate of the minimax risk. Specifically, if $n$ represents the number of data points, $f$ denotes the underlying function to be estimated, $\hat{\nu}_n$ is an estimate of $f$, and $\sigma_n$ is the standard deviation of the additive Gaussian noise, then $\inf_{\hat{\nu}_n} \sup_f \mathbb{E}_f\| \hat{\nu}_n - f \|^2_2$ decays at the rate $(\max(1,\sigma_n^4)/n)^{\frac{2\beta}{2\beta+1}}$. Note that the rate achieved under purely additive noise is $({\sigma_n^2/n})^{\frac{2\beta}{2\beta+1}}$. We will provide a detailed comparison of this rate with the one obtained in the presence of both noise types across different regimes of their relative magnitudes, and discuss the insights that emerge from these comparisons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16585v2</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <category>stat.TH</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Reihaneh Malekian, Hao Xing, Arian Maleki</dc:creator>
    </item>
    <item>
      <title>Nonparametric Regression in Dirichlet Spaces: A Random Obstacle Approach</title>
      <link>https://arxiv.org/abs/2412.14357</link>
      <description>arXiv:2412.14357v4 Announce Type: replace 
Abstract: In this paper, we consider nonparametric estimation over general Dirichlet metric measure spaces. Unlike the more commonly studied reproducing kernel Hilbert space, whose elements may be defined pointwise, a Dirichlet space typically only contain equivalence classes, i.e. its elements are only unique almost everywhere. This lack of pointwise definition presents significant challenges in the context of nonparametric estimation, for example the classical ridge regression problem is ill-posed. In this paper, we develop a new technique for renormalizing the ridge loss by replacing pointwise evaluations with certain \textit{local means} around the boundaries of obstacles centered at each data point. The resulting renormalized empirical risk functional is well-posed and even admits a representer theorem in terms of certain equilibrium potentials, which are truncated versions of the associated Green function, cut-off at a data-driven threshold. We demonstrate that the renormalized ridge estimator is rate-optimal, and derive an adaptive upper bound on its convergence rate that highlights the interplay between the analytic, geometric, and probabilistic properties of the Dirichlet form. Our framework notably does not require the smoothness of the underlying space, and is applicable to both manifold and fractal settings. To the best of our knowledge, this is the first paper to obtain optimal, out-of-sample convergence guarantees in the framework of general metric measure Dirichlet spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14357v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Prem Talwai, David Simchi-Levi</dc:creator>
    </item>
    <item>
      <title>Outlier-Bias Removal with Alpha Divergence: A Robust Non-Convex Estimator for Linear Regression</title>
      <link>https://arxiv.org/abs/2412.19183</link>
      <description>arXiv:2412.19183v2 Announce Type: replace 
Abstract: Convex and penalized robust methods often suffer from bias induced by large outliers, limiting their effectiveness in adversarial or heavy-tailed settings. In this study, we propose a novel approach that eliminates this bias (when possible) by leveraging a non-convex $M$-estimator based on the alpha divergence. We address the problem of estimating the parameters vector in high dimensional linear regression, even when a subset of the data has been deliberately corrupted by an adversary with full knowledge of the dataset and its underlying distribution.
  Our primary contribution is to demonstrate that the objective function, although non-convex, exhibits convexity within a carefully chosen basin of attraction, enabling robust and unbiased estimation. Additionally, we establish three key theoretical guarantees for the estimator: (a) a deviation bound that is minimax optimal up to a logarithmic factor, (b) an improved unbiased bound when the outliers are large and (c) asymptotic normality as the sample size increases. Finally, we validate the theoretical findings through empirical comparisons with state-of-the-art estimators on both synthetic and real-world datasets, highlighting the proposed method's superior robustness, efficiency, and ability to mitigate outlier-induced bias.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19183v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilyes Hammouda, Mohamed Ndaoud, Abd-Krim Seghouane</dc:creator>
    </item>
    <item>
      <title>Geometric Ergodicity of Gibbs Algorithms for a Normal Model With a Global-Local Shrinkage Prior</title>
      <link>https://arxiv.org/abs/2503.00538</link>
      <description>arXiv:2503.00538v3 Announce Type: replace 
Abstract: In this paper, we consider Gibbs samplers for a normal linear regression model with a global-local shrinkage prior. We show that they produce geometrically ergodic Markov chains under some assumptions. In the first half of the paper, we prove geometric ergodicity under the horseshoe local prior and a three-parameter beta global prior which does not have a finite $(p / 5)$-th negative moment, where $p$ is the number of regression coefficients. This is in contrast to the case of a known general result which is applicable if the global parameter has a finite approximately $(p / 2)$-th negative moment. In the second half of the paper, we consider a more general class of global-local shrinkage priors. Geometric ergodicity is proved for two-stage and three-stage Gibbs samplers based on rejection sampling without assuming the negative moment condition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00538v3</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yasuyuki Hamura</dc:creator>
    </item>
    <item>
      <title>The Poisson tensor completion non-parametric differential entropy estimator</title>
      <link>https://arxiv.org/abs/2505.04957</link>
      <description>arXiv:2505.04957v2 Announce Type: replace 
Abstract: We introduce the Poisson tensor completion (PTC) estimator, a non-parametric differential entropy estimator. The PTC estimator leverages inter-sample relationships to compute a low-rank Poisson tensor decomposition of the frequency histogram. Our crucial observation is that the histogram bins are an instance of a space partitioning of counts and thus can be identified with a spatial Poisson process. The Poisson tensor decomposition leads to a completion of the intensity measure over all bins -- including those containing few to no samples -- and leads to our proposed PTC differential entropy estimator. A Poisson tensor decomposition models the underlying distribution of the count data and guarantees non-negative estimated values and so can be safely used directly in entropy estimation. We believe our estimator is the first tensor-based estimator that exploits the underlying spatial Poisson process related to the histogram explicitly when estimating the probability density with low-rank tensor decompositions or tensor completion. Furthermore, we demonstrate that our PTC estimator is a substantial improvement over standard histogram-based estimators for sub-Gaussian probability distributions because of the concentration of norm phenomenon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04957v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel M. Dunlavy, Richard B. Lehoucq, Carolyn D. Mayer, Arvind Prasadan</dc:creator>
    </item>
    <item>
      <title>Doublethink: simultaneous Bayesian-frequentist model-averaged hypothesis testing</title>
      <link>https://arxiv.org/abs/2312.17566</link>
      <description>arXiv:2312.17566v3 Announce Type: replace-cross 
Abstract: Establishing the frequentist properties of Bayesian approaches widens their appeal and offers new understanding. In hypothesis testing, Bayesian model averaging addresses the problem that conclusions are sensitive to variable selection. But Bayesian false discovery rate (FDR) guarantees are sensitive to subjective prior assumptions. Here we show that Bayesian model-averaged hypothesis testing is a closed testing procedure that controls the frequentist familywise error rate (FWER) in the strong sense. To quantify the FWER, we use the theory of regular variation and likelihood asymptotics to derive a chi-squared tail approximation for the model-averaged posterior odds. Convergence is pointwise as the sample size grows and, in a simplified setting subject to a minimum effect size assumption, uniform. The 'Doublethink' method computes simultaneous posterior odds and asymptotic p-values for model-averaged hypothesis testing. We explore Doublethink through a Mendelian randomization study and simulations, comparing to approaches like LASSO, stepwise regression, the Benjamini-Hochberg procedure, the harmonic mean p-value and e-values. We consider the limitations of the approach, including finite-sample inflation, and mitigations, like testing groups of correlated variables. We discuss the benefits of Doublethink, including post-hoc variable selection, and its wider implications for the theory and practice of hypothesis testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.17566v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Helen R. Fryer, Nicolas Arning, Daniel J. Wilson</dc:creator>
    </item>
    <item>
      <title>Inference on Dynamic Spatial Autoregressive Models with Change Point Detection</title>
      <link>https://arxiv.org/abs/2411.18773</link>
      <description>arXiv:2411.18773v3 Announce Type: replace-cross 
Abstract: We analyze a varying-coefficient dynamic spatial autoregressive model with spatial fixed effects. One salient feature of the model is the incorporation of multiple spatial weight matrices through their linear combinations with varying coefficients, which help solve the problem of choosing the most ``correct'' one for applied econometricians who often face the availability of multiple expert spatial weight matrices. We estimate and make inferences on the model coefficients and coefficients in basis expansions of the varying coefficients through penalized estimations, establishing the oracle properties of the estimators and the consistency of the overall estimated spatial weight matrix, which can be time-dependent. We further consider two applications of our model in change point detections in dynamic spatial autoregressive models, providing theoretical justifications in consistent change point locations estimation and practical implementations. Simulation experiments demonstrate the performance of our proposed methodology, and real data analyses are also carried out.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18773v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zetai Cen, Yudong Chen, Clifford Lam</dc:creator>
    </item>
    <item>
      <title>A New Proof of Sub-Gaussian Norm Concentration Inequality</title>
      <link>https://arxiv.org/abs/2503.14347</link>
      <description>arXiv:2503.14347v2 Announce Type: replace-cross 
Abstract: We present a new method for proving the norm concentration inequality of sub-Gaussian variables. Our proof is based on an averaged version of the moment generating function, termed the averaged moment generating function. Our method applies to both vector cases to bound the vector norm and matrix cases to bound the operator norm. Compared with the widely adopted $\varepsilon$-net technique-based proof of the sub-Gaussian norm concentration inequality, our method does not rely on the union bound and promises a tighter concentration bound.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14347v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zishun Liu, Sam Power, Yongxin Chen</dc:creator>
    </item>
  </channel>
</rss>
