<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 21 Feb 2025 05:11:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Gradients can train reward models: An Empirical Risk Minimization Approach for Offline Inverse RL and Dynamic Discrete Choice Model</title>
      <link>https://arxiv.org/abs/2502.14131</link>
      <description>arXiv:2502.14131v1 Announce Type: cross 
Abstract: We study the problem of estimating Dynamic Discrete Choice (DDC) models, also known as offline Maximum Entropy-Regularized Inverse Reinforcement Learning (offline MaxEnt-IRL) in machine learning. The objective is to recover reward or $Q^*$ functions that govern agent behavior from offline behavior data. In this paper, we propose a globally convergent gradient-based method for solving these problems without the restrictive assumption of linearly parameterized rewards. The novelty of our approach lies in introducing the Empirical Risk Minimization (ERM) based IRL/DDC framework, which circumvents the need for explicit state transition probability estimation in the Bellman equation. Furthermore, our method is compatible with non-parametric estimation techniques such as neural networks. Therefore, the proposed method has the potential to be scaled to high-dimensional, infinite state spaces. A key theoretical insight underlying our approach is that the Bellman residual satisfies the Polyak-Lojasiewicz (PL) condition -- a property that, while weaker than strong convexity, is sufficient to ensure fast global convergence guarantees. Through a series of synthetic experiments, we demonstrate that our approach consistently outperforms benchmark methods and state-of-the-art alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14131v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>econ.EM</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Enoch H. Kang, Hema Yoganarasimhan, Lalit Jain</dc:creator>
    </item>
    <item>
      <title>Consider or Choose? The Role and Power of Consideration Sets</title>
      <link>https://arxiv.org/abs/2302.04354</link>
      <description>arXiv:2302.04354v4 Announce Type: replace 
Abstract: Consideration sets play a crucial role in discrete choice modeling, where customers often form consideration sets in the first stage and then use a second-stage choice mechanism to select the product with the highest utility. While many recent studies aim to improve choice models by incorporating more sophisticated second-stage choice mechanisms, this paper takes a step back and goes into the opposite extreme. We simplify the second-stage choice mechanism to its most basic form and instead focus on modeling customer choice by emphasizing the role and power of the first-stage consideration set formation. To this end, we study a model that is parameterized solely by a distribution over consideration sets with a bounded rationality interpretation. Intriguingly, we show that this model is characterized by the axiom of symmetric demand cannibalization, enabling complete statistical identification. The latter finding highlights the critical role of consideration sets in the identifiability of two-stage choice models. We also examine the model's implications for assortment planning, proving that the optimal assortment is revenue-ordered within each partition block created by consideration sets. Despite this compelling structure, we establish that the assortment problem under this model is NP-hard even to approximate, highlighting how consideration sets contribute to nontractability, even under the simplest uniform second-stage choice mechanism. Finally, using real-world data, we show that the model achieves prediction performance comparable to other advanced choice models. Given the simplicity of the model's second-stage phase, this result showcases the enormous power of first-stage consideration set formation in capturing customers' decision-making processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.04354v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yi-Chun Akchen, Dmitry Mitrofanov</dc:creator>
    </item>
    <item>
      <title>Axiomatic modeling of fixed proportion technologies</title>
      <link>https://arxiv.org/abs/2404.12462</link>
      <description>arXiv:2404.12462v2 Announce Type: replace 
Abstract: Understanding input substitution and output transformation possibilities is critical for efficient resource allocation and firm strategy. There are important examples of fixed proportion technologies where certain inputs are non-substitutable and/or certain outputs are non-transformable. However, there is widespread confusion about the appropriate modeling of fixed proportion technologies in data envelopment analysis. We point out and rectify several misconceptions in the existing literature, and show how fixed proportion technologies can be correctly incorporated into the axiomatic framework. A Monte Carlo study is performed to demonstrate the proposed solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12462v2</guid>
      <category>econ.EM</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xun Zhou, Timo Kuosmanen</dc:creator>
    </item>
    <item>
      <title>The Nonstationary Newsvendor with (and without) Predictions</title>
      <link>https://arxiv.org/abs/2305.07993</link>
      <description>arXiv:2305.07993v4 Announce Type: replace-cross 
Abstract: The classic newsvendor model yields an optimal decision for a ``newsvendor'' selecting a quantity of inventory, under the assumption that the demand is drawn from a known distribution. Motivated by applications such as cloud provisioning and staffing, we consider a setting in which newsvendor-type decisions must be made sequentially, in the face of demand drawn from a stochastic process that is both unknown and nonstationary. All prior work on this problem either (a) assumes that the level of nonstationarity is known, or (b) imposes additional statistical assumptions that enable accurate predictions of the unknown demand. Our research tackles the Nonstationary Newsvendor without these assumptions, both with and without predictions.
  We first, in the setting without predictions, design a policy which we prove achieves order-optimal regret -- ours is the first policy to accomplish this without being given the level of nonstationarity of the underlying demand. We then, for the first time, introduce a model for generic (i.e. with no statistical assumptions) predictions with arbitrary accuracy, and propose a policy that incorporates these predictions without being given their accuracy. We upper bound the regret of this policy, and show that it matches the best achievable regret had the accuracy of the predictions been known.
  Our findings provide valuable insights on inventory management. Managers can make more informed and effective decisions in dynamic environments, reducing costs and enhancing service levels despite uncertain demand patterns. We empirically validate our new policy with experiments based on three real-world datasets containing thousands of time-series, showing that it succeeds in closing approximately 74% of the gap between the best approaches based on nonstationarity and predictions alone.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.07993v4</guid>
      <category>math.OC</category>
      <category>econ.EM</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin An, Andrew A. Li, Benjamin Moseley, R. Ravi</dc:creator>
    </item>
    <item>
      <title>Measuring the Quality of Answers in Political Q&amp;As with Large Language Models</title>
      <link>https://arxiv.org/abs/2404.08816</link>
      <description>arXiv:2404.08816v5 Announce Type: replace-cross 
Abstract: This article proposes a new approach for assessing the quality of answers in political question-and-answer sessions. We measure the quality of an answer based on how easily and accurately it can be recognized in a random set of candidate answers given the question's text. This measure reflects the answer's relevance and depth of engagement with the question. Like semantic search, we can implement this approach by training a language model on the corpus of observed questions and answers without additional human-labeled data. We showcase and validate our methodology within the context of the Question Period in the Canadian House of Commons. Our analysis reveals that while some answers have a weak semantic connection to questions, hinting at some evasion or obfuscation, they are generally at least moderately relevant, far exceeding what we would expect from random replies. We also find a meaningful correlation between answer quality and the party affiliation of the members of Parliament asking the questions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08816v5</guid>
      <category>cs.CL</category>
      <category>econ.EM</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>R. Michael Alvarez, Jacob Morrier</dc:creator>
    </item>
    <item>
      <title>Global Ease of Living Index: a machine learning framework for longitudinal analysis of major economies</title>
      <link>https://arxiv.org/abs/2502.06866</link>
      <description>arXiv:2502.06866v2 Announce Type: replace-cross 
Abstract: The drastic changes in the global economy, geopolitical conditions, and disruptions such as the COVID-19 pandemic have impacted the cost of living and quality of life. It is important to understand the long-term nature of the cost of living and quality of life in major economies. A transparent and comprehensive living index must include multiple dimensions of living conditions. In this study, we present an approach to quantifying the quality of life through the Global Ease of Living Index that combines various socio-economic and infrastructural factors into a single composite score. Our index utilises economic indicators that define living standards, which could help in targeted interventions to improve specific areas. We present a machine learning framework for addressing the problem of missing data for some of the economic indicators for specific countries. We then curate and update the data and use a dimensionality reduction approach (principal component analysis) to create the Ease of Living Index for major economies since 1970. Our work significantly adds to the literature by offering a practical tool for policymakers to identify areas needing improvement, such as healthcare systems, employment opportunities, and public safety. Our approach with open data and code can be easily reproduced and applied to various contexts. This transparency and accessibility make our work a valuable resource for ongoing research and policy development in quality-of-life assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06866v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Fri, 21 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tanay Panat, Rohitash Chandra</dc:creator>
    </item>
  </channel>
</rss>
