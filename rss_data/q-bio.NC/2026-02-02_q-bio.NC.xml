<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 03 Feb 2026 04:28:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The Where and How of Touch: A Review of Tactile Localization Research</title>
      <link>https://arxiv.org/abs/2601.23023</link>
      <description>arXiv:2601.23023v1 Announce Type: new 
Abstract: Tactile localization is the seemingly simple ability to 'tell' where a touch has occurred. However, how this ability is assessed, and what conclusions are drawn from experiments, depends on the theoretical ideas that inspire the research. Here, we review both theoretical frameworks and methodological approaches based on a systematic web-based literature search on tactile localization. After presenting current theories of tactile localization, we discuss task characteristics that differentiate current methodology for tactile localization into at least 8 distinct types of experimental tasks. We describe these tasks, discuss their, often implicit, underlying assumptions and cognitive requirements, and relate them to the theoretical approaches. We then compare, in an exemplary manner, the tactile localization results reported by a subset of studies and demonstrate how some methods are associated with specific biases, illustrating that the choice of experimental method significantly affects the conclusions drawn from the results. Our review suggests that the field currently lacks a clear concept of the specific processes induced by the various experimental tasks and, thus, calls for concerted efforts to clarify and unify currently diverse, fragmented, and partly inconsistent theoretical underpinnings of tactile spatial processing, flanked by dedicated data sharing to allow across-study analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.23023v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xaver Fuchs, Jason A. M. Khoury, Sergiu Tcaci Popescu, Tobias Heed, Matej Hoffmann</dc:creator>
    </item>
    <item>
      <title>SLIM-Brain: A Data- and Training-Efficient Foundation Model for fMRI Data Analysis</title>
      <link>https://arxiv.org/abs/2512.21881</link>
      <description>arXiv:2512.21881v3 Announce Type: replace-cross 
Abstract: Foundation models are emerging as a powerful paradigm for fMRI analysis, but current approaches face a dual bottleneck of data- and training-efficiency. Atlas-based methods aggregate voxel signals into fixed regions of interest, reducing data dimensionality but discarding fine-grained spatial details, and requiring extremely large cohorts to train effectively as general-purpose foundation models. Atlas-free methods, on the other hand, operate directly on voxel-level information - preserving spatial fidelity but are prohibitively memory- and compute-intensive, making large-scale pre-training infeasible. We introduce SLIM-Brain (Sample-efficient, Low-memory fMRI Foundation Model for Human Brain), a new atlas-free foundation model that simultaneously improves both data- and training-efficiency. SLIM-Brain adopts a two-stage adaptive design: (i) a lightweight temporal extractor captures global context across full sequences and ranks data windows by saliency, and (ii) a 4D hierarchical encoder (Hiera-JEPA) learns fine-grained voxel-level representations only from the top-$k$ selected windows, while deleting about 70% masked patches. Extensive experiments across seven public benchmarks show that SLIM-Brain establishes new state-of-the-art performance on diverse tasks, while requiring only 4 thousand pre-training sessions and approximately 30% of GPU memory comparing to traditional voxel-level methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21881v3</guid>
      <category>cs.CV</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mo Wang, Junfeng Xia, Wenhao Ye, Enyu Liu, Kaining Peng, Jianfeng Feng, Quanying Liu, Hongkai Wen</dc:creator>
    </item>
  </channel>
</rss>
