<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 08 Jul 2025 04:04:43 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Nonparametric regression for cost-effectiveness analyses with observational data -- a tutorial</title>
      <link>https://arxiv.org/abs/2507.03511</link>
      <description>arXiv:2507.03511v1 Announce Type: new 
Abstract: Healthcare decision-making often requires selecting among treatment options under budget constraints, particularly when one option is more effective but also more costly. Cost-effectiveness analysis (CEA) provides a framework for evaluating whether the health benefits of a treatment justify its additional costs. A key component of CEA is the estimation of treatment effects on both health outcomes and costs, which becomes challenging when using observational data, due to potential confounding. While advanced causal inference methods exist for use in such circumstances, their adoption in CEAs remains limited, with many studies relying on overly simplistic methods such as linear regression or propensity score matching. We believe that this is mainly due to health economists being generally unfamiliar with superior methodology. In this paper, we address this gap by introducing cost-effectiveness researchers to modern nonparametric regression models, with a particular focus on Bayesian Additive Regression Trees (BART). We provide practical guidance on how to implement BART in CEAs, including code examples, and discuss its advantages in producing more robust and credible estimates from observational data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03511v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas Esser, Mateus Maia, Judith Bosmans, Johanna van Dongen</dc:creator>
    </item>
    <item>
      <title>A General Class of Model-Free Dense Precision Matrix Estimators</title>
      <link>https://arxiv.org/abs/2507.04663</link>
      <description>arXiv:2507.04663v1 Announce Type: new 
Abstract: We introduce prototype consistent model-free, dense precision matrix estimators that have broad application in economics. Using quadratic form concentration inequalities and novel algebraic characterizations of confounding dimension reductions, we are able to: (i) obtain non-asymptotic bounds for precision matrix estimation errors and also (ii) consistency in high dimensions; (iii) uncover the existence of an intrinsic signal-to-noise -- underlying dimensions tradeoff; and (iv) avoid exact population sparsity assumptions. In addition to its desirable theoretical properties, a thorough empirical study of the S&amp;P 500 index shows that a tuning parameter-free special case of our general estimator exhibits a doubly ascending Sharpe Ratio pattern, thereby establishing a link with the famous double descent phenomenon dominantly present in recent statistical and machine learning literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04663v1</guid>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Mehmet Caner Agostino Capponi Mihailo Stojnic</dc:creator>
    </item>
    <item>
      <title>Identification of Causal Effects with a Bunching Design</title>
      <link>https://arxiv.org/abs/2507.05210</link>
      <description>arXiv:2507.05210v1 Announce Type: new 
Abstract: We show that causal effects can be identified when there is bunching in the distribution of a continuous treatment variable, without imposing any parametric assumptions. This yields a new nonparametric method for overcoming selection bias in the absence of instrumental variables, panel data, or other popular research designs for causal inference. The method leverages the change of variables theorem from integration theory, relating the selection bias to the ratio of the density of the treatment and the density of the part of the outcome that varies with confounders. At the bunching point, the treatment level is constant, so the variation in the outcomes is due entirely to unobservables, allowing us to identify the denominator. Our main result identifies the average causal response to the treatment among individuals who marginally select into the bunching point. We further show that under additional smoothness assumptions on the selection bias, treatment effects away from the bunching point may also be identified. We propose estimators based on standard software packages and apply the method to estimate the effect of maternal smoking during pregnancy on birth weight.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05210v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carolina Caetano, Gregorio Caetano, Leonard Goff, Eric Nielsen</dc:creator>
    </item>
    <item>
      <title>A New and Efficient Debiased Estimation of General Treatment Models by Balanced Neural Networks Weighting</title>
      <link>https://arxiv.org/abs/2507.04044</link>
      <description>arXiv:2507.04044v1 Announce Type: cross 
Abstract: Estimation and inference of treatment effects under unconfounded treatment assignments often suffer from bias and the `curse of dimensionality' due to the nonparametric estimation of nuisance parameters for high-dimensional confounders. Although debiased state-of-the-art methods have been proposed for binary treatments under particular treatment models, they can be unstable for small sample sizes. Moreover, directly extending them to general treatment models can lead to computational complexity. We propose a balanced neural networks weighting method for general treatment models, which leverages deep neural networks to alleviate the curse of dimensionality while retaining optimal covariate balance through calibration, thereby achieving debiased and robust estimation. Our method accommodates a wide range of treatment models, including average, quantile, distributional, and asymmetric least squares treatment effects, for discrete, continuous, and mixed treatments. Under regularity conditions, we show that our estimator achieves rate double robustness and $\sqrt{N}$-asymptotic normality, and its asymptotic variance achieves the semiparametric efficiency bound. We further develop a statistical inference procedure based on weighted bootstrap, which avoids estimating the efficient influence/score functions. Simulation results reveal that the proposed method consistently outperforms existing alternatives, especially when the sample size is small. Applications to the 401(k) dataset and the Mother's Significant Features dataset further illustrate the practical value of the method for estimating both average and quantile treatment effects under binary and continuous treatments, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04044v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zeqi Wu, Meilin Wang, Wei Huang, Zheng Zhang</dc:creator>
    </item>
    <item>
      <title>A Test for Jumps in Metric-Space Conditional Means</title>
      <link>https://arxiv.org/abs/2507.04560</link>
      <description>arXiv:2507.04560v1 Announce Type: cross 
Abstract: Standard methods for detecting discontinuities in conditional means are not applicable to outcomes that are complex, non-Euclidean objects like distributions, networks, or covariance matrices. This article develops a nonparametric test for jumps in conditional means when outcomes lie in a non-Euclidean metric space. Using local Fr\'echet regression$\unicode{x2014}$which generalizes standard regression to metric-space valued data$\unicode{x2014}$the method estimates a mean path on either side of a candidate cutoff, extending existing k-sample tests to a flexible regression setting. Key theoretical contributions include a central limit theorem for the local estimator of the conditional Fr\'echet variance and the asymptotic validity and consistency of the proposed test. Simulations confirm nominal size control and robust power in finite samples. Two applications demonstrate the method's value by revealing effects invisible to scalar-based tests. First, I detect a sharp change in work-from-home compositions at Washington State's income threshold for non-compete enforceability during COVID-19, highlighting remote work's role as a bargaining margin. Second, I find that countries restructure their input-output networks after losing preferential US trade access. These findings underscore that analyzing regression functions within their native metric spaces can reveal structural discontinuities that scalar summaries would miss.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04560v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Van Dijcke</dc:creator>
    </item>
    <item>
      <title>Forward Variable Selection in Ultra-High Dimensional Linear Regression Using Gram-Schmidt Orthogonalization</title>
      <link>https://arxiv.org/abs/2507.04668</link>
      <description>arXiv:2507.04668v1 Announce Type: cross 
Abstract: We investigate forward variable selection for ultra-high dimensional linear regression using a Gram-Schmidt orthogonalization procedure. Unlike the commonly used Forward Regression (FR) method, which computes regression residuals using an increasing number of selected features, or the Orthogonal Greedy Algorithm (OGA), which selects variables based on their marginal correlations with the residuals, our proposed Gram-Schmidt Forward Regression (GSFR) simplifies the selection process by evaluating marginal correlations between the residuals and the orthogonalized new variables. Moreover, we introduce a new model size selection criterion that determines the number of selected variables by detecting the most significant change in their unique contributions, effectively filtering out redundant predictors along the selection path. While GSFR is theoretically equivalent to FR except for the stopping rule, our refinement and the newly proposed stopping rule significantly improve computational efficiency. In ultra-high dimensional settings, where the dimensionality far exceeds the sample size and predictors exhibit strong correlations, we establish that GSFR achieves a convergence rate comparable to OGA and ensures variable selection consistency under mild conditions. We demonstrate the proposed method {using} simulations and real data examples. Extensive numerical studies show that GSFR outperforms commonly used methods in ultra-high dimensional variable selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04668v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jialuo Chen, Zhaoxing Gao, Ruey S. Tsay</dc:creator>
    </item>
    <item>
      <title>Blind Targeting: Personalization under Third-Party Privacy Constraints</title>
      <link>https://arxiv.org/abs/2507.05175</link>
      <description>arXiv:2507.05175v1 Announce Type: cross 
Abstract: Major advertising platforms recently increased privacy protections by limiting advertisers' access to individual-level data. Instead of providing access to granular raw data, the platforms only allow a limited number of aggregate queries to a dataset, which is further protected by adding differentially private noise. This paper studies whether and how advertisers can design effective targeting policies within these restrictive privacy preserving data environments. To achieve this, I develop a probabilistic machine learning method based on Bayesian optimization, which facilitates dynamic data exploration. Since Bayesian optimization was designed to sample points from a function to find its maximum, it is not applicable to aggregate queries and to targeting. Therefore, I introduce two innovations: (i) integral updating of posteriors which allows to select the best regions of the data to query rather than individual points and (ii) a targeting-aware acquisition function that dynamically selects the most informative regions for the targeting task. I identify the conditions of the dataset and privacy environment that necessitate the use of such a "smart" querying strategy. I apply the strategic querying method to the Criteo AI Labs dataset for uplift modeling (Diemert et al., 2018) that contains visit and conversion data from 14M users. I show that an intuitive benchmark strategy only achieves 33% of the non-privacy-preserving targeting potential in some cases, while my strategic querying method achieves 97-101% of that potential, and is statistically indistinguishable from Causal Forest (Athey et al., 2019): a state-of-the-art non-privacy-preserving machine learning targeting method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05175v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anya Shchetkina</dc:creator>
    </item>
    <item>
      <title>Realized Stochastic Volatility Model with Skew-t Distributions for Improved Volatility and Quantile Forecasting</title>
      <link>https://arxiv.org/abs/2401.13179</link>
      <description>arXiv:2401.13179v3 Announce Type: replace 
Abstract: Accurate forecasting of volatility and return quantiles is essential for evaluating financial tail risks such as value-at-risk and expected shortfall. This study proposes an extension of the traditional stochastic volatility model, termed the realized stochastic volatility model, that incorporates realized volatility as an efficient proxy for latent volatility. To better capture the stylized features of financial return distributions, particularly skewness and heavy tails, we introduce three variants of skewed t-distributions, two of which incorporate skew-normal components to flexibly model asymmetry. The models are estimated using a Bayesian Markov chain Monte Carlo approach and applied to daily returns and realized volatilities from major U.S. and Japanese stock indices. Empirical results demonstrate that incorporating both realized volatility and flexible return distributions substantially improves the accuracy of volatility and tail risk forecasts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.13179v3</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Makoto Takahashi, Yuta Yamauchi, Toshiaki Watanabe, Yasuhiro Omori</dc:creator>
    </item>
    <item>
      <title>Femicide Laws, Unilateral Divorce, and Abortion Decriminalization Fail to Stop Women from Being Killed in Mexico</title>
      <link>https://arxiv.org/abs/2407.06722</link>
      <description>arXiv:2407.06722v2 Announce Type: replace 
Abstract: This paper evaluates the effectiveness of femicide laws in combating gender-based killings of women, a major cause of premature female mortality. Focusing on Mexico, a pioneer in adopting such legislation, the paper exploits variations in the enactment of femicide laws and prison sentences across states. Using the difference-in-differences estimator, the analysis reveals femicide laws have not impacted femicides, homicides, disappearances, or suicides of women. Results remain robust when considering differences in prison sentencing, states introducing unilateral divorce, equitable divorce asset compensation, or decriminalizing abortion. Findings also hold with synthetic matching, suggesting laws are insufficient to combat gender-based violence in contexts of impunity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06722v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Roxana Guti\'errez-Romero</dc:creator>
    </item>
    <item>
      <title>Democratizing Strategic Planning in Master-Planned Communities</title>
      <link>https://arxiv.org/abs/2410.04676</link>
      <description>arXiv:2410.04676v2 Announce Type: replace 
Abstract: This paper introduces a strategic planning tool for master-planned communities designed specifically to quantify residents' subjective preferences about large investments in amenities and infrastructure projects. Drawing on data obtained from brief online surveys, the tool ranks alternative plans by considering the aggregate anticipated utilization of each proposed amenity and cost sensitivity to it (or risk sensitivity for infrastructure plans). In addition, the tool estimates the percentage of households that favor the preferred plan and predicts whether residents would actually be willing to fund the project. The mathematical underpinnings of the tool are borrowed from utility theory, incorporating exponential functions to model diminishing marginal returns on quality, cost, and risk mitigation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04676v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Christopher K. Allsup, Irene S. Gabashvili</dc:creator>
    </item>
    <item>
      <title>Treatment Effect Heterogeneity in Regression Discontinuity Designs</title>
      <link>https://arxiv.org/abs/2503.13696</link>
      <description>arXiv:2503.13696v3 Announce Type: replace 
Abstract: Empirical studies using Regression Discontinuity (RD) designs often explore heterogeneous treatment effects based on pretreatment covariates, even though no formal statistical methods exist for such analyses. This has led to the widespread use of ad hoc approaches in applications. Motivated by common empirical practice, we develop a unified, theoretically grounded framework for RD heterogeneity analysis. We show that a fully interacted local linear (in functional parameters) model effectively captures heterogeneity while still being tractable and interpretable in applications. The model structure holds without loss of generality for discrete covariates. Although our proposed model is potentially restrictive for continuous covariates, it naturally aligns with standard empirical practice and offers a causal interpretation for RD applications. We establish principled bandwidth selection and robust bias-corrected inference methods to analyze heterogeneous treatment effects and test group differences. We provide companion software to facilitate implementation of our results. An empirical application illustrates the practical relevance of our methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13696v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sebastian Calonico, Matias D. Cattaneo, Max H. Farrell, Filippo Palomba, Rocio Titiunik</dc:creator>
    </item>
    <item>
      <title>Minimax and Bayes Optimal Best-arm Identification: Adaptive Experimental Design for Treatment Choice</title>
      <link>https://arxiv.org/abs/2506.24007</link>
      <description>arXiv:2506.24007v2 Announce Type: replace 
Abstract: This study investigates adaptive experimental design for treatment choice, also known as fixed-budget best-arm identification. We consider an adaptive procedure consisting of a treatment-allocation phase followed by a treatment-choice phase, and we design an adaptive experiment for this setup to efficiently identify the best treatment arm, defined as the one with the highest expected outcome. In our designed experiment, the treatment-allocation phase consists of two stages. The first stage is a pilot phase, where we allocate each treatment arm uniformly with equal proportions to eliminate clearly suboptimal arms and estimate outcome variances. In the second stage, we allocate treatment arms in proportion to the variances estimated in the first stage. After the treatment-allocation phase, the procedure enters the treatment-choice phase, where we choose the treatment arm with the highest sample mean as our estimate of the best treatment arm. We prove that this single design is simultaneously asymptotically minimax and Bayes optimal for the simple regret, with upper bounds that match our lower bounds up to exact constants. Therefore, our designed experiment achieves the sharp efficiency limits without requiring separate tuning for minimax and Bayesian objectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.24007v2</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Learning in Finance</title>
      <link>https://arxiv.org/abs/2506.03780</link>
      <description>arXiv:2506.03780v3 Announce Type: replace-cross 
Abstract: Recent advances in machine learning have shown promising results for financial prediction using large, over-parameterized models. This paper provides theoretical foundations and empirical validation for understanding when and how these methods achieve predictive success. I examine two key aspects of high-dimensional learning in finance. First, I prove that within-sample standardization in Random Fourier Features implementations fundamentally alters the underlying Gaussian kernel approximation, replacing shift-invariant kernels with training-set dependent alternatives. Second, I establish information-theoretic lower bounds that identify when reliable learning is impossible no matter how sophisticated the estimator. A detailed quantitative calibration of the polynomial lower bound shows that with typical parameter choices, e.g., 12,000 features, 12 monthly observations, and R-square 2-3%, the required sample size to escape the bound exceeds 25-30 years of data--well beyond any rolling-window actually used. Thus, observed out-of-sample success must originate from lower-complexity artefacts rather than from the intended high-dimensional mechanism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03780v3</guid>
      <category>q-fin.ST</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hasan Fallahgoul</dc:creator>
    </item>
  </channel>
</rss>
