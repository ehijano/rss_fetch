<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 09 Jul 2024 04:00:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 09 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Convexified Matching Approach to Imputation and Individualized Inference</title>
      <link>https://arxiv.org/abs/2407.05372</link>
      <description>arXiv:2407.05372v1 Announce Type: new 
Abstract: We introduce a new convexified matching method for missing value imputation and individualized inference inspired by computational optimal transport. Our method integrates favorable features from mainstream imputation approaches: optimal matching, regression imputation, and synthetic control. We impute counterfactual outcomes based on convex combinations of observed outcomes, defined based on an optimal coupling between the treated and control data sets. The optimal coupling problem is considered a convex relaxation to the combinatorial optimal matching problem. We estimate granular-level individual treatment effects while maintaining a desirable aggregate-level summary by properly constraining the coupling. We construct transparent, individual confidence intervals for the estimated counterfactual outcomes. We devise fast iterative entropic-regularized algorithms to solve the optimal coupling problem that scales favorably when the number of units to match is large. Entropic regularization plays a crucial role in both inference and computation; it helps control the width of the individual confidence intervals and design fast optimization algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05372v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>YoonHaeng Hur, Tengyuan Liang</dc:creator>
    </item>
    <item>
      <title>Methodology for Calculating CO2 Absorption by Tree Planting for Greening Projects</title>
      <link>https://arxiv.org/abs/2407.05596</link>
      <description>arXiv:2407.05596v1 Announce Type: new 
Abstract: In order to explore the possibility of carbon credits for greening projects, which play an important role in climate change mitigation, this paper examines a formula for estimating the amount of carbon fixation for greening activities in urban areas through tree planting. The usefulness of the formula studied was examined by conducting calculations based on actual data through measurements made by on-site surveys of a greening companie. A series of calculation results suggest that this formula may be useful. Recognizing carbon credits for green businesses for the carbon sequestration of their projects is an important incentive not only as part of environmental improvement and climate change action, but also to improve the health and well-being of local communities and to generate economic benefits. This study is a pioneering exploration of the methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05596v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Kento Ichii, Toshiki Muraoka, Nobumichi Shinohara, Shunsuke Managi, Shutaro Takeda</dc:creator>
    </item>
    <item>
      <title>Dynamic Matrix Factor Models for High Dimensional Time Series</title>
      <link>https://arxiv.org/abs/2407.05624</link>
      <description>arXiv:2407.05624v1 Announce Type: cross 
Abstract: Matrix time series, which consist of matrix-valued data observed over time, are prevalent in various fields such as economics, finance, and engineering. Such matrix time series data are often observed in high dimensions. Matrix factor models are employed to reduce the dimensionality of such data, but they lack the capability to make predictions without specified dynamics in the latent factor process. To address this issue, we propose a two-component dynamic matrix factor model that extends the standard matrix factor model by incorporating a matrix autoregressive structure for the low-dimensional latent factor process. This two-component model injects prediction capability to the matrix factor model and provides deeper insights into the dynamics of high-dimensional matrix time series. We present the estimation procedures of the model and their theoretical properties, as well as empirical analysis of the estimation procedures via simulations, and a case study of New York city taxi data, demonstrating the performance and usefulness of the model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05624v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ruofan Yu, Rong Chen, Han Xiao, Yuefeng Han</dc:creator>
    </item>
    <item>
      <title>Bayesian and Frequentist Inference for Synthetic Controls</title>
      <link>https://arxiv.org/abs/2206.01779</link>
      <description>arXiv:2206.01779v3 Announce Type: replace-cross 
Abstract: The synthetic control method has become a widely popular tool to estimate causal effects with observational data. Despite this, inference for synthetic control methods remains challenging. Often, inferential results rely on linear factor model data generating processes. In this paper, we characterize the conditions on the factor model primitives (the factor loadings) for which the statistical risk minimizers are synthetic controls (in the simplex). Then, we propose a Bayesian alternative to the synthetic control method that preserves the main features of the standard method and provides a new way of doing valid inference. We explore a Bernstein-von Mises style result to link our Bayesian inference to the frequentist inference. For linear factor model frameworks we show that a maximum likelihood estimator (MLE) of the synthetic control weights can consistently estimate the predictive function of the potential outcomes for the treated unit and that our Bayes estimator is asymptotically close to the MLE in the total variation sense. Through simulations, we show that there is convergence between the Bayes and frequentist approach even in sparse settings. Finally, we apply the method to re-visit the study of the economic costs of the German re-unification and the Catalan secession movement. The Bayesian synthetic control method is available in the bsynth R-package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.01779v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ignacio Martinez, Jaume Vives-i-Bastida</dc:creator>
    </item>
    <item>
      <title>The Nonstationary Newsvendor with (and without) Predictions</title>
      <link>https://arxiv.org/abs/2305.07993</link>
      <description>arXiv:2305.07993v3 Announce Type: replace-cross 
Abstract: The classic newsvendor model yields an optimal decision for a "newsvendor" selecting a quantity of inventory, under the assumption that the demand is drawn from a known distribution. Motivated by applications such as cloud provisioning and staffing, we consider a setting in which newsvendor-type decisions must be made sequentially, in the face of demand drawn from a stochastic process that is both unknown and nonstationary. All prior work on this problem either (a) assumes that the level of nonstationarity is known, or (b) imposes additional statistical assumptions that enable accurate predictions of the unknown demand.
  We study the Nonstationary Newsvendor, with and without predictions. We first, in the setting without predictions, design a policy which we prove (via matching upper and lower bounds) achieves order-optimal regret -- ours is the first policy to accomplish this without being given the level of nonstationarity of the underlying demand. We then, for the first time, introduce a model for generic (i.e. with no statistical assumptions) predictions with arbitrary accuracy, and propose a policy that incorporates these predictions without being given their accuracy. We upper bound the regret of this policy, and show that it matches the best achievable regret had the accuracy of the predictions been known. Finally, we empirically validate our new policy with experiments based on three real-world datasets containing thousands of time-series, showing that it succeeds in closing approximately 74% of the gap between the best approaches based on nonstationarity and predictions alone.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.07993v3</guid>
      <category>math.OC</category>
      <category>econ.EM</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin An, Andrew A. Li, Benjamin Moseley, R. Ravi</dc:creator>
    </item>
    <item>
      <title>Correlations versus noise in the NFT market</title>
      <link>https://arxiv.org/abs/2404.15495</link>
      <description>arXiv:2404.15495v2 Announce Type: replace-cross 
Abstract: The non-fungible token (NFT) market emerges as a recent trading innovation leveraging blockchain technology, mirroring the dynamics of the cryptocurrency market. The current study is based on the capitalization changes and transaction volumes across a large number of token collections on the Ethereum platform. In order to deepen the understanding of the market dynamics, the collection-collection dependencies are examined by using the multivariate formalism of detrended correlation coefficient and correlation matrix. It appears that correlation strength is lower here than that observed in previously studied markets. Consequently, the eigenvalue spectra of the correlation matrix more closely follow the Marchenko-Pastur distribution, still, some departures indicating the existence of correlations remain. The comparison of results obtained from the correlation matrix built from the Pearson coefficients and, independently, from the detrended cross-correlation coefficients suggests that the global correlations in the NFT market arise from higher frequency fluctuations. Corresponding minimal spanning trees (MSTs) for capitalization variability exhibit a scale-free character while, for the number of transactions, they are somewhat more decentralized.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15495v2</guid>
      <category>q-fin.ST</category>
      <category>cs.CE</category>
      <category>econ.EM</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1063/5.0214399</arxiv:DOI>
      <arxiv:journal_reference>Chaos 34, 073112 (2024)</arxiv:journal_reference>
      <dc:creator>Marcin W\k{a}torek, Pawe{\l} Szyd{\l}o, Jaros{\l}aw Kwapie\'n, Stanis{\l}aw Dro\.zd\.z</dc:creator>
    </item>
  </channel>
</rss>
