<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.MS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.MS</link>
    <description>cs.MS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.MS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 02 May 2025 04:00:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>NApy: Efficient Statistics in Python for Large-Scale Heterogeneous Data with Enhanced Support for Missing Data</title>
      <link>https://arxiv.org/abs/2505.00448</link>
      <description>arXiv:2505.00448v1 Announce Type: new 
Abstract: Existing Python libraries and tools lack the ability to efficiently compute statistical test results for large datasets in the presence of missing values. This presents an issue as soon as constraints on runtime and memory availability become essential considerations for a particular usecase. Relevant research areas where such limitations arise include interactive tools and databases for exploratory analysis of biomedical data. To address this problem, we present the Python package NApy, which relies on a Numba and C++ backend with OpenMP parallelization to enable scalable statistical testing for mixed-type datasets in the presence of missing values. Both with respect to runtime and memory consumption, NApy outperforms competitor tools and baseline implementations with naive Python-based parallelization by orders of magnitude, thereby enabling on-the-fly analyses in interactive applications. NApy is publicly available at https://github.com/DyHealthNet/NApy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00448v1</guid>
      <category>cs.MS</category>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fabian Woller, Lis Arend, Christian Fuchsberger, Markus List, David B. Blumenthal</dc:creator>
    </item>
    <item>
      <title>PDCS: A Primal-Dual Large-Scale Conic Programming Solver with GPU Enhancements</title>
      <link>https://arxiv.org/abs/2505.00311</link>
      <description>arXiv:2505.00311v1 Announce Type: cross 
Abstract: In this paper, we introduce the "Primal-Dual Conic Programming Solver" (PDCS), a large-scale conic programming solver with GPU enhancements. Problems that PDCS currently supports include linear programs, second-order cone programs, convex quadratic programs, and exponential cone programs. PDCS achieves scalability to large-scale problems by leveraging sparse matrix-vector multiplication as its core computational operation, which is both memory-efficient and well-suited for GPU acceleration. The solver is based on the restarted primal-dual hybrid gradient method but further incorporates several enhancements, including adaptive reflected Halpern restarts, adaptive step-size selection, adaptive weight adjustment, and diagonal rescaling. Additionally, PDCS employs a bijection-based method to compute projections onto rescaled cones. Furthermore, cuPDCS is a GPU implementation of PDCS and it implements customized computational schemes that utilize different levels of GPU architecture to handle cones of different types and sizes. Numerical experiments demonstrate that cuPDCS is generally more efficient than state-of-the-art commercial solvers and other first-order methods on large-scale conic program applications, including Fisher market equilibrium problems, Lasso regression, and multi-period portfolio optimization. Furthermore, cuPDCS also exhibits better scalability, efficiency, and robustness compared to other first-order methods on the conic program benchmark dataset CBLIB. These advantages are more pronounced in large-scale, lower-accuracy settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00311v1</guid>
      <category>math.OC</category>
      <category>cs.MS</category>
      <pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenwei Lin, Zikai Xiong, Dongdong Ge, Yinyu Ye</dc:creator>
    </item>
    <item>
      <title>TumorTwin: A python framework for patient-specific digital twins in oncology</title>
      <link>https://arxiv.org/abs/2505.00670</link>
      <description>arXiv:2505.00670v1 Announce Type: cross 
Abstract: Background: Advances in the theory and methods of computational oncology have enabled accurate characterization and prediction of tumor growth and treatment response on a patient-specific basis. This capability can be integrated into a digital twin framework in which bi-directional data-flow between the physical tumor and the digital tumor facilitate dynamic model re-calibration, uncertainty quantification, and clinical decision-support via recommendation of optimal therapeutic interventions. However, many digital twin frameworks rely on bespoke implementations tailored to each disease site, modeling choice, and algorithmic implementation.
  Findings: We present TumorTwin, a modular software framework for initializing, updating, and leveraging patient-specific cancer tumor digital twins. TumorTwin is publicly available as a Python package, with associated documentation, datasets, and tutorials. Novel contributions include the development of a patient-data structure adaptable to different disease sites, a modular architecture to enable the composition of different data, model, solver, and optimization objects, and CPU- or GPU-parallelized implementations of forward model solves and gradient computations. We demonstrate the functionality of TumorTwin via an in silico dataset of high-grade glioma growth and response to radiation therapy.
  Conclusions: The TumorTwin framework enables rapid prototyping and testing of image-guided oncology digital twins. This allows researchers to systematically investigate different models, algorithms, disease sites, or treatment decisions while leveraging robust numerical and computational infrastructure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00670v1</guid>
      <category>physics.med-ph</category>
      <category>cs.MS</category>
      <pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Kapteyn, Anirban Chaudhuri, Ernesto A. B. F. Lima, Graham Pash, Rafael Bravo, Karen Willcox, Thomas E. Yankeelov, David A. Hormuth II</dc:creator>
    </item>
  </channel>
</rss>
