<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 06 Mar 2025 05:00:37 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Adaptive monotonicity testing in sublinear time</title>
      <link>https://arxiv.org/abs/2503.03020</link>
      <description>arXiv:2503.03020v1 Announce Type: new 
Abstract: Modern large-scale data analysis increasingly faces the challenge of achieving computational efficiency as well as statistical accuracy, as classical statistically efficient methods often fall short in the first regard. In the context of testing monotonicity of a regression function, we propose FOMT (Fast and Optimal Monotonicity Test), a novel methodology tailored to meet these dual demands. FOMT employs a sparse collection of local tests, strategically generated at random, to detect violations of monotonicity scattered throughout the domain of the regression function. This sparsity enables significant computational efficiency, achieving sublinear runtime in most cases, and quasilinear runtime (i.e., linear up to a log factor) in the worst case. In contrast, existing statistically optimal tests typically require at least quadratic runtime. FOMT's statistical accuracy is achieved through the precise calibration of these local tests and their effective combination, ensuring both sensitivity to violations and control over false positives. More precisely, we show that FOMT separates the null and alternative hypotheses at minimax optimal rates over H\"older function classes of smoothness order in $(0,2]$. Further, when the smoothness is unknown, we introduce an adaptive version of FOMT, based on a modified Lepskii principle, which attains statistical optimality and meanwhile maintains the same computational complexity as if the intrinsic smoothness were known. Extensive simulations confirm the competitiveness and effectiveness of both FOMT and its adaptive variant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03020v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Housen Li, Zhi Liu, Axel Munk</dc:creator>
    </item>
    <item>
      <title>Estimating weak Markov-switching AR(1) models</title>
      <link>https://arxiv.org/abs/2503.03316</link>
      <description>arXiv:2503.03316v1 Announce Type: new 
Abstract: In this paper, we present the asymptotic properties of the moment estimator for autoregressive (AR for short) models subject to Markovian changes in regime under the assumption that the errors are uncorrelated but not necessarily independent. We relax the standard independence assumption on the innovation process to extend considerably the range of application of the Markov-switching AR models. We provide necessary conditions to prove the consistency and asymptotic normality of the moment estimator in a specific case. Particular attention is paid to the estimation of the asymptotic covariance matrix. Finally, some simulation studies and an application to the hourly meteorological data are presented to corroborate theoretical work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03316v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yacouba Boubacar Mainassara (LMB, UFC, UPHF, INSA Hauts-De-France, CERAMATHS), Landy Rabehasaina (LMB), Armel Bra (LMB)</dc:creator>
    </item>
    <item>
      <title>Drift estimation for rough processes under small noise asymptotic: trajectory fitting method</title>
      <link>https://arxiv.org/abs/2503.03347</link>
      <description>arXiv:2503.03347v1 Announce Type: new 
Abstract: We consider a process $X^\varepsilon$ solution of a stochastic Volterra equation with an unknown parameter $\theta^\star$ in the drift function. The Volterra kernel is singular and given by $K(u)=c u^{\alpha-1/2} \mathbb{1}_{u&gt;0}$ with $\alpha \in (0,1/2)$. It is assumed that the diffusion coefficient is proportional to $\varepsilon \to 0$. From an observation of the path $(X^\varepsilon_s)_{s\in[0,T]}$, we construct a Trajectory Fitting Estimator, which is shown to be consistent and asymptotically normal. We also specify identifiability conditions insuring the $L^p$ convergence of the estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03347v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arnaud Gloter (LaMME), Nakahiro Yoshida</dc:creator>
    </item>
    <item>
      <title>Statistical Limits in Random Tensors with Multiple Correlated Spikes</title>
      <link>https://arxiv.org/abs/2503.03356</link>
      <description>arXiv:2503.03356v1 Announce Type: new 
Abstract: We use tools from random matrix theory to study the multi-spiked tensor model, i.e., a rank-$r$ deformation of a symmetric random Gaussian tensor. In particular, thanks to the nature of local optimization methods used to find the maximum likelihood estimator of this model, we propose to study the phase transition phenomenon for finding critical points of the corresponding optimization problem, i.e., those points defined by the Karush-Kuhn-Tucker (KKT) conditions. Moreover, we characterize the limiting alignments between the estimated signals corresponding to a critical point of the likelihood and the ground truth signals. With the help of these results, we propose a new estimator of the rank-$r$ tensor weights by solving a system of polynomial equations, which is asymptotically unbiased contrary the maximum likelihood estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03356v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>math.SP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Qi, Alexis Decurninge</dc:creator>
    </item>
    <item>
      <title>Visual tests using several safe confidence intervals</title>
      <link>https://arxiv.org/abs/2503.03567</link>
      <description>arXiv:2503.03567v1 Announce Type: new 
Abstract: We propose a new statistical hypothesis testing framework which decides visually, using confidence intervals, whether the means of two samples are equal or if one is larger than the other. With our method, the user can at the same time visualize the confidence region of the means and do a test to decide if the means of the two populations are significantly different or not by looking whether the two confidence intervals overlap. To design this test we use confidence intervals constructed using e-variables, which provide a measure of evidence in hypothesis testing. We propose both a sequential test and a non-sequential test based on the overlap of confidence intervals and for each of these tests we give finite-time error bounds on the probabilities of error. We also illustrate the practicality of our method by applying it to the comparison of sequential learning algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03567v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Timoth\'ee Mathieu</dc:creator>
    </item>
    <item>
      <title>Asymmetric Cross-Correlation in Multivariate Spatial Stochastic Processes: A Primer</title>
      <link>https://arxiv.org/abs/2503.02903</link>
      <description>arXiv:2503.02903v1 Announce Type: cross 
Abstract: Multivariate spatial phenomena are ubiquitous, spanning domains such as climate, pandemics, air quality, and social economy. Cross-correlation between different quantities of interest at different locations is asymmetric in general. This paper provides the visualization, structure, and properties of asymmetric cross-correlation as well as symmetric auto-correlation. It reviews mainstream multivariate spatial models and analyzes their capability to accommodate asymmetric cross-correlation. It also illustrates the difference in model accuracy with and without asymmetric accommodation using a 1D simulated example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02903v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xiaoqing Chen</dc:creator>
    </item>
    <item>
      <title>Stochastic block models with many communities and the Kesten--Stigum bound</title>
      <link>https://arxiv.org/abs/2503.03047</link>
      <description>arXiv:2503.03047v1 Announce Type: cross 
Abstract: We study the inference of communities in stochastic block models with a growing number of communities. For block models with $n$ vertices and a fixed number of communities $q$, it was predicted in Decelle et al. (2011) that there are computationally efficient algorithms for recovering the communities above the Kesten--Stigum (KS) bound and that efficient recovery is impossible below the KS bound. This conjecture has since stimulated a lot of interest, with the achievability side proven in a line of research that culminated in the work of Abbe and Sandon (2018). Conversely, recent work by Sohn and Wein (2025) provides evidence for the hardness part using the low-degree paradigm.
  In this paper we investigate community recovery in the regime $q=q_n \to \infty$ as $n\to\infty$ where no such predictions exist. We show that efficient inference of communities remains possible above the KS bound. Furthermore, we show that recovery of block models is low-degree hard below the KS bound when the number of communities satisfies $q\ll \sqrt{n}$. Perhaps surprisingly, we find that when $q \gg \sqrt{n}$, there is an efficient algorithm based on non-backtracking walks for recovery even below the KS bound. We identify a new threshold and ask if it is the threshold for efficient recovery in this regime. Finally, we show that detection is easy and identify (up to a constant) the information-theoretic threshold for community recovery as the number of communities $q$ diverges.
  Our low-degree hardness results also naturally have consequences for graphon estimation, improving results of Luo and Gao (2024).</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03047v1</guid>
      <category>math.PR</category>
      <category>cs.SI</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Byron Chin, Elchanan Mossel, Youngtak Sohn, Alexander S. Wein</dc:creator>
    </item>
    <item>
      <title>Generating Networks to Target Assortativity via Archimedean Copula Graphons</title>
      <link>https://arxiv.org/abs/2503.03061</link>
      <description>arXiv:2503.03061v1 Announce Type: cross 
Abstract: We develop an approach to generate random graphs to a target level of assortativity by using copula structures in graphons. Unlike existing random graph generators, we do not use rewiring or binning approaches to generate the desired random graph. Instead, we connect Archimedean bivariate copulas to graphons in order to produce flexible models that can generate random graphs to target assortativity. We propose three models that use the copula distribution function, copula density function and their mixed tensor product to produce networks. We express the assortativity coefficient in terms of homomorphism densities. Establishing this relationship forges a connection between the parameter of the copula and the frequency of subgraphs in the generated network. Therefore, our method attains a desired the subgraph distribution as well as the target assortativity. We establish the homomorphism densities and assortativity coefficient for each of the models. Numerical examples demonstrate the ability of the proposed models to produce graphs with different levels of assortativity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03061v1</guid>
      <category>cs.SI</category>
      <category>math.ST</category>
      <category>physics.data-an</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Victory Idowu</dc:creator>
    </item>
    <item>
      <title>An Analytical Theory of Power Law Spectral Bias in the Learning Dynamics of Diffusion Models</title>
      <link>https://arxiv.org/abs/2503.03206</link>
      <description>arXiv:2503.03206v1 Announce Type: cross 
Abstract: We developed an analytical framework for understanding how the learned distribution evolves during diffusion model training. Leveraging the Gaussian equivalence principle, we derived exact solutions for the gradient-flow dynamics of weights in one- or two-layer linear denoiser settings with arbitrary data. Remarkably, these solutions allowed us to derive the generated distribution in closed form and its KL divergence through training. These analytical results expose a pronounced power-law spectral bias, i.e., for weights and distributions, the convergence time of a mode follows an inverse power law of its variance. Empirical experiments on both Gaussian and image datasets demonstrate that the power-law spectral bias remains robust even when using deeper or convolutional architectures. Our results underscore the importance of the data covariance in dictating the order and rate at which diffusion models learn different modes of the data, providing potential explanations for why earlier stopping could lead to incorrect details in image generative models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03206v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Binxu Wang</dc:creator>
    </item>
    <item>
      <title>Safety Verification of Nonlinear Stochastic Systems via Probabilistic Tube</title>
      <link>https://arxiv.org/abs/2503.03328</link>
      <description>arXiv:2503.03328v1 Announce Type: cross 
Abstract: We address the problem of safety verification for nonlinear stochastic systems, specifically the task of certifying that system trajectories remain within a safe set with high probability. To tackle this challenge, we adopt a set-erosion strategy, which decouples the effects of stochastic disturbances from deterministic dynamics. This approach converts the stochastic safety verification problem on a safe set into a deterministic safety verification problem on an eroded subset of the safe set. The success of this strategy hinges on the depth of erosion, which is determined by a probabilistic tube that bounds the deviation of stochastic trajectories from their corresponding deterministic trajectories. Our main contribution is the establishment of a tight bound for the probabilistic tube of nonlinear stochastic systems. To obtain a probabilistic bound for stochastic trajectories, we adopt a martingale-based approach. The core innovation lies in the design of a novel energy function associated with the averaged moment generating function, which forms an affine martingale, a generalization of the traditional c-martingale. Using this energy function, we derive a precise bound for the probabilistic tube. Furthermore, we enhance this bound by incorporating the union-bound inequality for strictly contractive dynamics. By integrating the derived probabilistic tubes into the set-erosion strategy, we demonstrate that the safety verification problem for nonlinear stochastic systems can be reduced to a deterministic safety verification problem. Our theoretical results are validated through applications in reachability-based safety verification and safe controller synthesis, accompanied by several numerical examples that illustrate their effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03328v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zishun Liu, Saber Jafarpour, Yongxin Chen</dc:creator>
    </item>
    <item>
      <title>Early-Stopped Mirror Descent for Linear Regression over Convex Bodies</title>
      <link>https://arxiv.org/abs/2503.03426</link>
      <description>arXiv:2503.03426v1 Announce Type: cross 
Abstract: Early-stopped iterative optimization methods are widely used as alternatives to explicit regularization, and direct comparisons between early-stopping and explicit regularization have been established for many optimization geometries. However, most analyses depend heavily on the specific properties of the optimization geometry or strong convexity of the empirical objective, and it remains unclear whether early-stopping could ever be less statistically efficient than explicit regularization for some particular shape constraint, especially in the overparameterized regime. To address this question, we study the setting of high-dimensional linear regression under additive Gaussian noise when the ground truth is assumed to lie in a known convex body and the task is to minimize the in-sample mean squared error. Our main result shows that for any convex body and any design matrix, up to an absolute constant factor, the worst-case risk of unconstrained early-stopped mirror descent with an appropriate potential is at most that of the least squares estimator constrained to the convex body. We achieve this by constructing algorithmic regularizers based on the Minkowski functional of the convex body.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03426v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tobias Wegel, Gil Kur, Patrick Rebeschini</dc:creator>
    </item>
    <item>
      <title>Asymptotics for M-type smoothing splines with non-smooth objective functions</title>
      <link>https://arxiv.org/abs/2002.04898</link>
      <description>arXiv:2002.04898v4 Announce Type: replace 
Abstract: M-type smoothing splines are a broad class of spline estimators that include the popular least-squares smoothing spline but also spline estimators that are less susceptible to outlying observations and model-misspecification. However, available asymptotic theory only covers smoothing spline estimators based on smooth objective functions and consequently leaves out frequently used resistant estimators such as quantile and Huber-type smoothing splines. We provide a general treatment in this paper and, assuming only the convexity of the objective function, show that the least-squares (super-)convergence rates can be extended to M-type estimators whose asymptotic properties have not been hitherto described. We further show that auxiliary scale estimates may be handled under significantly weaker assumptions than those found in the literature and we establish optimal rates of convergence for the derivatives, which have not been obtained outside the least-squares framework. A simulation study and a real-data example illustrate the competitive performance of non-smooth M-type splines in relation to the least-squares spline on regular data and their superior performance on data that contain anomalies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2002.04898v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s11749-021-00782-y</arxiv:DOI>
      <arxiv:journal_reference>Test 2021</arxiv:journal_reference>
      <dc:creator>Ioannis Kalogridis</dc:creator>
    </item>
    <item>
      <title>Efficient Estimation of a Gaussian Mean with Local Differential Privacy</title>
      <link>https://arxiv.org/abs/2402.04840</link>
      <description>arXiv:2402.04840v3 Announce Type: replace 
Abstract: In this paper we study the problem of estimating the unknown mean $\theta$ of a unit variance Gaussian distribution in a locally differentially private (LDP) way. In the high-privacy regime ($\epsilon\le 1$), we identify an optimal privacy mechanism that minimizes the variance of the estimator asymptotically. Our main technical contribution is the maximization of the Fisher-Information of the sanitized data with respect to the local privacy mechanism $Q$. We find that the exact solution $Q_{\theta,\epsilon}$ of this maximization is the sign mechanism that applies randomized response to the sign of $X_i-\theta$, where $X_1,\dots, X_n$ are the confidential iid original samples. However, since this optimal local mechanism depends on the unknown mean $\theta$, we employ a two-stage LDP parameter estimation procedure which requires splitting agents into two groups. The first $n_1$ observations are used to consistently but not necessarily efficiently estimate the parameter $\theta$ by $\tilde{\theta}_{n_1}$. Then this estimate is updated by applying the sign mechanism with $\tilde{\theta}_{n_1}$ instead of $\theta$ to the remaining $n-n_1$ observations, to obtain an LDP and efficient estimator of the unknown mean.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04840v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nikita P. Kalinin, Lukas Steinberger</dc:creator>
    </item>
    <item>
      <title>Learning of deep convolutional network image classifiers via stochastic gradient descent and over-parametrization</title>
      <link>https://arxiv.org/abs/2404.07128</link>
      <description>arXiv:2404.07128v3 Announce Type: replace 
Abstract: Image classification from independent and identically distributed random variables is considered. Image classifiers are defined which are based on a linear combination of deep convolutional networks with max-pooling layer. Here all the weights are learned by stochastic gradient descent. A general result is presented which shows that the image classifiers are able to approximate the best possible deep convolutional network. In case that the a posteriori probability satisfies a suitable hierarchical composition model it is shown that the corresponding deep convolutional neural network image classifier achieves a rate of convergence which is independent of the dimension of the images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07128v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Kohler, Adam Krzyzak, Alisha S\"anger</dc:creator>
    </item>
    <item>
      <title>Fast exact recovery of noisy matrix from few entries: the infinity norm approach</title>
      <link>https://arxiv.org/abs/2501.19224</link>
      <description>arXiv:2501.19224v2 Announce Type: replace 
Abstract: The matrix recovery (completion) problem, a central problem in data science and theoretical computer science, is to recover a matrix $A$ from a relatively small sample of entries.
  While such a task is impossible in general, it has been shown that one can recover $A$ exactly in polynomial time, with high probability, from a random subset of entries, under three (basic and necessary) assumptions: (1) the rank of $A$ is very small compared to its dimensions (low rank), (2) $A$ has delocalized singular vectors (incoherence), and (3) the sample size is sufficiently large.
  There are many different algorithms for the task, including convex optimization by Candes, Tao and Recht (2009), alternating projection by Hardt and Wooters (2014) and low rank approximation with gradient descent by Keshavan, Montanari and Oh (2009, 2010).
  In applications, it is more realistic to assume that data is noisy. In this case, these approaches provide an approximate recovery with small root mean square error. However, it is hard to transform such an approximate recovery to an exact one.
  Recently, results by Abbe et al. (2017) and Bhardwaj et al. (2023) concerning approximation in the infinity norm showed that we can achieve exact recovery even in the noisy case, given that the ground matrix has bounded precision. Beyond the three basic assumptions above, they required either the condition number of $A$ is small (Abbe et al.) or the gap between consecutive singular values is large (Bhardwaj et al.).
  In this paper, we remove these extra spectral assumptions. As a result, we obtain a simple algorithm for exact recovery in the noisy case, under only the three basic assumptions. This is the first such algorithm. To analyse this algorithm, we introduce a contour integration argument which is totally different from all previous methods and may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19224v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math.CO</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>BaoLinh Tran, Van Vu</dc:creator>
    </item>
  </channel>
</rss>
