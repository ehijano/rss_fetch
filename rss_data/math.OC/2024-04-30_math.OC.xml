<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 30 Apr 2024 04:00:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 30 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Subdifferential of fuzzy n-cell number valued functions and its applications in optimization problems</title>
      <link>https://arxiv.org/abs/2404.17600</link>
      <description>arXiv:2404.17600v1 Announce Type: new 
Abstract: In this paper, we present the concept of subdifferential for fuzzy n-cell number valued functions. Then we state some theorems related to subdifferentiability based on the new definition. Finally, we present some applications emphasized on optimization problems, including the Lagrangian dual problem and minimizing the composite problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17600v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Samira Fatemi, Ildar Sadeqi, Fridoun Moradlou</dc:creator>
    </item>
    <item>
      <title>Energy Storage Arbitrage in Two-settlement Markets: A Transformer-Based Approach</title>
      <link>https://arxiv.org/abs/2404.17683</link>
      <description>arXiv:2404.17683v1 Announce Type: new 
Abstract: This paper presents an integrated model for bidding energy storage in day-ahead and real-time markets to maximize profits. We show that in integrated two-stage bidding, the real-time bids are independent of day-ahead settlements, while the day-ahead bids should be based on predicted real-time prices. We utilize a transformer-based model for real-time price prediction, which captures complex dynamical patterns of real-time prices, and use the result for day-ahead bidding design. For real-time bidding, we utilize a long short-term memory-dynamic programming hybrid real-time bidding model. We train and test our model with historical data from New York State, and our results showed that the integrated system achieved promising results of almost a 20\% increase in profit compared to only bidding in real-time markets, and at the same time reducing the risk in terms of the number of days with negative profits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17683v1</guid>
      <category>math.OC</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Saud Alghumayjan, Jiajun Han, Ningkun Zheng, Ming Yi, Bolun Xu</dc:creator>
    </item>
    <item>
      <title>Inexact FPPA for the $\ell_0$ Sparse Regularization Problem</title>
      <link>https://arxiv.org/abs/2404.17689</link>
      <description>arXiv:2404.17689v1 Announce Type: new 
Abstract: We study inexact fixed-point proximity algorithms for solving a class of sparse regularization problems involving the $\ell_0$ norm. Specifically, the $\ell_0$ model has an objective function that is the sum of a convex fidelity term and a Moreau envelope of the $\ell_0$ norm regularization term. Such an $\ell_0$ model is non-convex. Existing exact algorithms for solving the problems require the availability of closed-form formulas for the proximity operator of convex functions involved in the objective function. When such formulas are not available, numerical computation of the proximity operator becomes inevitable. This leads to inexact iteration algorithms. We investigate in this paper how the numerical error for every step of the iteration should be controlled to ensure global convergence of the resulting inexact algorithms. We establish a theoretical result that guarantees the sequence generated by the proposed inexact algorithm converges to a local minimizer of the optimization problem. We implement the proposed algorithms for three applications of practical importance in machine learning and image science, which include regression, classification, and image deblurring. The numerical results demonstrate the convergence of the proposed algorithm and confirm that local minimizers of the $\ell_0$ models found by the proposed inexact algorithm outperform global minimizers of the corresponding $\ell_1$ models, in terms of approximation accuracy and sparsity of the solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17689v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ronglong Fang, Yuesheng Xu, Mingsong Yan</dc:creator>
    </item>
    <item>
      <title>New second-order optimality conditions for directional optimality of a general set-constrained optimization problem</title>
      <link>https://arxiv.org/abs/2404.17696</link>
      <description>arXiv:2404.17696v1 Announce Type: new 
Abstract: In this paper we derive new second-order optimality conditions for a very general set-constrained optimization problem where the underlying set may be nononvex. We consider local optimality in specific directions (i.e., optimal in a directional neighborhood) in pursuit of developing these new optimality conditions. First-order necessary conditions for local optimality in given directions are provided by virtue of the corresponding directional normal cones. Utilizing the classical and/or the lower generalized support function, we obtain new second-order necessary and sufficient conditions for local optimality of general nonconvex constrained optimization problem in given directions via both the corresponding asymptotic second-order tangent cone and outer second-order tangent set. Our results do not require convexity and/or nonemptyness of the outer second-order tangent set. This is an important improvement to other results in the literature since the outer second-order tangent set can be nonconvex and empty even when the set is convex.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17696v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wei Ouyang, Jane Ye, Binbin Zhang</dc:creator>
    </item>
    <item>
      <title>Polynomial Optimization Over Unions of Sets</title>
      <link>https://arxiv.org/abs/2404.17717</link>
      <description>arXiv:2404.17717v1 Announce Type: new 
Abstract: This paper studies the polynomial optimization problem whose feasible set is a union of several basic closed semialgebraic sets. We propose a unified hierarchy of Moment-SOS relaxations to solve it globally. Under some assumptions, we prove the asymptotic or finite convergence of the unified hierarchy. Special properties for the univariate case are discussed.The application for computing $(p,q)$-norms of matrices is also presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17717v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiawang Nie, Linghao Zhang</dc:creator>
    </item>
    <item>
      <title>Control randomisation approach for policy gradient and application to reinforcement learning in optimal switching</title>
      <link>https://arxiv.org/abs/2404.17939</link>
      <description>arXiv:2404.17939v1 Announce Type: new 
Abstract: We propose a comprehensive framework for policy gradient methods tailored to continuous time reinforcement learning. This is based on the connection between stochastic control problems and randomised problems, enabling applications across various classes of Markovian continuous time control problems, beyond diffusion models, including e.g. regular, impulse and optimal stopping/switching problems. By utilizing change of measure in the control randomisation technique, we derive a new policy gradient representation for these randomised problems, featuring parametrised intensity policies. We further develop actor-critic algorithms specifically designed to address general Markovian stochastic control issues. Our framework is demonstrated through its application to optimal switching problems, with two numerical case studies in the energy sector focusing on real options.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17939v1</guid>
      <category>math.OC</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robert Denkert, Huy\^en Pham, Xavier Warin</dc:creator>
    </item>
    <item>
      <title>On a Stochastic Differential Equation with Correction Term Governed by a Monotone and Lipschitz Continuous Operator</title>
      <link>https://arxiv.org/abs/2404.17986</link>
      <description>arXiv:2404.17986v1 Announce Type: new 
Abstract: In our pursuit of finding a zero for a monotone and Lipschitz continuous operator $M : \R^n \rightarrow \R^n$ amidst noisy evaluations, we explore an associated differential equation within a stochastic framework, incorporating a correction term. We present a result establishing the existence and uniqueness of solutions for the stochastic differential equations under examination. Additionally, assuming that the diffusion term is square-integrable, we demonstrate the almost sure convergence of the trajectory process $X(t)$ to a zero of $M$ and of $\|M(X(t))\|$ to $0$ as $t \rightarrow +\infty$. Furthermore, we provide ergodic upper bounds and ergodic convergence rates in expectation for $\|M(X(t))\|^2$ and $\langle M(X(t), X(t)-x^*\rangle$, where $x^*$ is an arbitrary zero of the monotone operator. Subsequently, we apply these findings to a minimax problem. Finally, we analyze two temporal discretizations of the continuous-time models, resulting in stochastic variants of the Optimistic Gradient Descent Ascent and Extragradient methods, respectively, and assess their convergence properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17986v1</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Radu Ioan Bot, Chiara Schindler</dc:creator>
    </item>
    <item>
      <title>Swarm-based gradient descent meets simulated annealing</title>
      <link>https://arxiv.org/abs/2404.18015</link>
      <description>arXiv:2404.18015v1 Announce Type: new 
Abstract: We introduce a novel method for non-convex optimization which is at the interface between the swarm-based gradient-descent (SBGD) [J. Lu et. al., ArXiv:2211.17157; E.Tadmor and A. Zenginoglu, Acta Applicandae Math., 190, 2024] and Simulated Annealing (SA) [V. Cerny, J. optimization theory and appl., 45:41-51, 1985; S.Kirkpatrick et. al., Science, 220(4598):671-680, 1983; S. Geman and C.-R. Hwang, SIAM J. Control and Optimization, 24(5):1031-1043, 1986]. We follow the methodology of SBGD in which a swarm of agents, each identified with a position, ${\mathbf x}$ and mass $m$, explores the ambient space. The agents proceed in gradient descent direction, and are subject to Brownian motion with annealing-rate dictated by a decreasing function of their mass. Thus, instead of the SA protocol for time-decreasing temperature, we let the swarm decide how to `cool down' agents, depending on their accumulated mass over time. The dynamics of masses is coupled with the dynamics of positions: agents at higher ground transfer (part of) their mass to those at lower ground. Consequently, the swarm is dynamically divided between heavier, cooler agents viewed as `leaders' and lighter, warmer agents viewed as `explorers'. Mean-field convergence analysis and benchmark optimizations demonstrate the effectiveness of the swarm-based method as a multi-dimensional global optimizer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18015v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiyan Ding, Martin Guerra, Qin Li, Eitan Tadmor</dc:creator>
    </item>
    <item>
      <title>Fast Monte Carlo Analysis for 6-DoF Powered-Descent Guidance via GPU-Accelerated Sequential Convex Programming</title>
      <link>https://arxiv.org/abs/2404.18034</link>
      <description>arXiv:2404.18034v1 Announce Type: new 
Abstract: We introduce a GPU-accelerated Monte Carlo framework for nonconvex, free-final-time trajectory optimization problems. This framework makes use of the prox-linear method, which belongs to the larger family of sequential convex programming (SCP) algorithms, in conjunction with a constraint reformulation that guarantees inter-sample constraint satisfaction. Key features of this framework are: (1) continuous-time constraint satisfaction; (2) a matrix-inverse-free solution method; (3) the use of the proportional-integral projected gradient (PIPG) method, a first-order convex optimization solver, customized to the convex subproblem at hand; and, (4) an end-to-end, library-free implementation of the algorithm. We demonstrate this GPU-based framework on the 6-DoF powered-descent guidance problem, and show that it is faster than an equivalent serial CPU implementation for Monte Carlo simulations with over 1000 runs. To the best of our knowledge, this is the first GPU-based implementation of a general-purpose nonconvex trajectory optimization solver.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18034v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Govind M. Chari, Abhinav G. Kamath, Purnanand Elango, Beh\c{c}et A\c{c}{\i}kme\c{s}e</dc:creator>
    </item>
    <item>
      <title>An open-source solver for finding global solutions to constrained derivative-free optimization problems</title>
      <link>https://arxiv.org/abs/2404.18080</link>
      <description>arXiv:2404.18080v1 Announce Type: new 
Abstract: In this work, we propose a heuristic based open source solver for finding global solution to constrained derivative-free optimization (DFO) problems. Our solver named Global optimization using Surrogates for Derivative-free Optimization (GSDO) relies on surrogate approximation to the original problem. In the proposed algorithm, an initial feasible point is first generated. This point is subsequently used to generate well spaced feasible points for formulating better radial basis function based surrogate approximations to original objective and constraint functions. Finally, these surrogates are used to solve the derivative-free global optimization problems. The proposed solver is capable of handling quantifiable and nonquantifiable as well as relaxable and unrelaxable constraints. We compared the performance of proposed solver with state of the art solvers like Nonlinear Optimization by Mesh Adaptive Direct Search (NOMAD), differential evolution (DE) and Simplicial Homology Global Optimization (SHGO) on standard test problems. The numerical results clearly demonstrate that the performance of our method is competitive with respect to other solvers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18080v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gannavarapu Chandramouli, Vishnu Narayanan</dc:creator>
    </item>
    <item>
      <title>Contaminated Online Convex Optimization</title>
      <link>https://arxiv.org/abs/2404.18093</link>
      <description>arXiv:2404.18093v1 Announce Type: new 
Abstract: In the field of online convex optimization, some efficient algorithms have been designed for each of the individual classes of objective functions, e.g., convex, strongly convex, and exp-concave. However, existing regret analyses, including those of universal algorithms, are limited to cases in which the objective functions in all rounds belong to the same class, and cannot be applied to cases in which the property of objective functions may change in each time step. To address such cases, this paper proposes a new regime which we refer to as \textit{contaminated} online convex optimization. In the contaminated case, regret is bounded by $O(\log T+\sqrt{k\log T})$ when some universal algorithms are used, and bounded by $O(\log T+\sqrt{k})$ when our proposed algorithms are used, where $k$ represents how contaminated objective functions are. We also present a matching lower bound of $\Omega(\log T + \sqrt{k})$. These are intermediate bounds between a convex case and a strongly convex or exp-concave case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18093v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomoya Kamijima, Shinji Ito</dc:creator>
    </item>
    <item>
      <title>Approximations of Rockafellians, Lagrangians, and Dual Functions</title>
      <link>https://arxiv.org/abs/2404.18097</link>
      <description>arXiv:2404.18097v1 Announce Type: new 
Abstract: Solutions of an optimization problem are sensitive to changes caused by approximations or parametric perturbations, especially in the nonconvex setting. This paper investigates the ability of substitute problems, constructed from Rockafellian functions, to provide robustness against such perturbations. Unlike classical stability analysis focused on local changes around (local) minimizers, we employ epi-convergence to examine whether the approximating problems suitably approach the actual one globally. We show that under natural assumptions the substitute problems can be well-behaved in the sense of epi-convergence even though the actual one is not. We further quantify the rates of convergence that often lead to Lipschitz-kind stability properties for the substitute problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18097v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julio Deride, Johannes O. Royset</dc:creator>
    </item>
    <item>
      <title>Research on the Evaluation Index System of Enterprise Production Efficiency</title>
      <link>https://arxiv.org/abs/2404.18121</link>
      <description>arXiv:2404.18121v1 Announce Type: new 
Abstract: This paper focuses on studying the evaluation index system for the production efficiency of tobacco enterprises. Considering the limitations of existing evaluation methods in accurately assessing the production quality of cigarette enterprises, a mathematical model based on the Analytic Hierarchy Process (AHP) is established. This model constructs an evaluation framework for the production efficiency of cigarette enterprises and subsequently analyzes the significance of each index within this framework. To comprehensively analyze the multi-index and feasibility aspects of the selected projects, the AHP method is employed to establish a comprehensive feasibility research and evaluation structure model. The result of this feasibility study provides the conclusion that the construction of an evaluation index system for the production efficiency of cigarette enterprises can indeed promote the enhancement of their production efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18121v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>W. Li, J. Cai, C. Wang, Y. Chen, J. Xu, J. Zhao, Y. Chen</dc:creator>
    </item>
    <item>
      <title>Moment-SOS relaxations for moment and tensor recovery problems</title>
      <link>https://arxiv.org/abs/2404.18332</link>
      <description>arXiv:2404.18332v1 Announce Type: new 
Abstract: This paper studies moment and tensor recovery problems whose decomposing vectors are contained in some given semialgebraic sets. We propose Moment-SOS relaxations with generic objectives for recovering moments and tensors, whose decomposition lengths are expected to be low. This kind of problems have broad applications in various tensor decomposition questions. Numerical experiments are provided to demonstrate the efficiency of this approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18332v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lei Huang, Jiawang Nie, Jiajia Wang</dc:creator>
    </item>
    <item>
      <title>Dynamic Global Feedback Stabilization: why do the twist?</title>
      <link>https://arxiv.org/abs/2404.18380</link>
      <description>arXiv:2404.18380v1 Announce Type: new 
Abstract: We investigate global dynamic feedback stabilization from a topological viewpoint. In particular, we consider the general case of dynamic feedback systems, whereby the total space (which includes the state space of the system and of the controller) is a fibre bundle, and derive conditions on the topology of the bundle that are necessary for various notions of global stabilization to hold. This point of view highlight the importance of distinguishing trivial bundles and twisted bundles in the study of global dynamic feedback stabilization, as we show that dynamic feedback defined on a twisted bundle can stabilize systems that dynamic feedback on trivial bundles cannot.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18380v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohamed-Ali Belabbas, Jehyung Ko</dc:creator>
    </item>
    <item>
      <title>Random Reshuffling with Momentum for Nonconvex Problems: Iteration Complexity and Last Iterate Convergence</title>
      <link>https://arxiv.org/abs/2404.18452</link>
      <description>arXiv:2404.18452v1 Announce Type: new 
Abstract: Random reshuffling with momentum (RRM) corresponds to the SGD optimizer with momentum option enabled, as found in popular machine learning libraries like PyTorch and TensorFlow. Despite its widespread use in practical applications, the understanding of its convergence properties in nonconvex scenarios remains limited. Under a Lipschitz smoothness assumption, this paper provides one of the first iteration complexities for RRM. Specifically, we prove that RRM achieves the iteration complexity $O(n^{-1/3}((1-\beta^n)T)^{-2/3})$ where $n$ denotes the number of component functions $f(\cdot;i)$ and $\beta \in [0,1)$ is the momentum parameter. Furthermore, every accumulation point of a sequence of iterates $\{x^k\}_k$ generated by RRM is shown to be a stationary point of the problem. In addition, under the Kurdyka-Lojasiewicz inequality - a local geometric property - the iterates $\{x^k\}_k$ provably converge to a unique stationary point $x^*$ of the objective function. Importantly, in our analysis, this last iterate convergence is obtained without requiring convexity nor a priori boundedness of the iterates. Finally, for polynomial step size schemes, convergence rates of the form $\|x^k - x^*\| = O(k^{-p})$, $\|\nabla f(x^k)\|^2 = O(k^{-q})$, and $|f(x^k) - f(x^*)| = O(k^{-q})$, $p \in (0,1]$, $q \in (0,2]$ are derived.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18452v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junwen Qiu, Andre Milzarek</dc:creator>
    </item>
    <item>
      <title>Adaptive (re)operations facilitate environmental flow maintenance downstream of multi-purpose reservoirs</title>
      <link>https://arxiv.org/abs/2404.18535</link>
      <description>arXiv:2404.18535v1 Announce Type: new 
Abstract: Multi-purpose reservoirs support socioeconomic development by providing irrigation, domestic water supply, hydropower, and other services. However, impoundment of water impacts instream aquatic ecosystems. Thus, the concept of minimum environmental flows (MEFs) was established to restore the benefits of naturally flowing rivers by specifying minimum flow rates to be maintained downstream of dams.But varying legislative contexts under which multi-purpose reservoirs operate may not always necessitate MEF releases. To what extent the release of MEF affects other sectoral benefits remains an open-ended and possibly a site-specific inquiry. A related issue is - how does the order in which releases are prioritized influences sectoral performances? We analyse these issues for the Nagarjuna Sagar reservoir, one of the largest multipurpose reservoirs in southern India. We formulate two versions of a multi-objective decision problem. PF_MEF formulation prioritizes MEF releases over releases for water demand satisfaction, followed by hydropower releases. PF_nMEF formulation follows the regional legislative rule releasing first for demand satisfaction, followed by hydropower and MEF releases. Results thus indicate that prioritizing MEF releases improves can meet MEF requirements without significant compromises in other objectives. We hypothesize that similar investigations may reveal how simple modification of release order may improve ability of other reservoirs to meet environmental goals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18535v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akshay Sunil, Riddhi Singh, Manvitha Molakala</dc:creator>
    </item>
    <item>
      <title>Non-convex Pose Graph Optimization in SLAM via Proximal Linearized Riemannian ADMM</title>
      <link>https://arxiv.org/abs/2404.18560</link>
      <description>arXiv:2404.18560v1 Announce Type: new 
Abstract: Pose graph optimization (PGO) is a well-known technique for solving the pose-based simultaneous localization and mapping (SLAM) problem. In this paper, we represent the rotation and translation by a unit quaternion and a three-dimensional vector, and propose a new PGO model based on the von Mises-Fisher distribution. The constraints derived from the unit quaternions are spherical manifolds, and the projection onto the constraints can be calculated by normalization. Then a proximal linearized Riemannian alternating direction method of multipliers (PieADMM) is developed to solve the proposed model, which not only has low memory requirements, but also can update the poses in parallel. Furthermore, we establish the iteration complexity of $O(1/\epsilon^{2})$ of PieADMM for finding an $\epsilon$-stationary solution of our model. The efficiency of our proposed algorithm is demonstrated by numerical experiments on two synthetic and four 3D SLAM benchmark datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18560v1</guid>
      <category>math.OC</category>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Chen, Chunfeng Cui, Deren Han, Liqun Qi</dc:creator>
    </item>
    <item>
      <title>Social Optima of Linear Forward-Backward Stochastic System</title>
      <link>https://arxiv.org/abs/2404.18561</link>
      <description>arXiv:2404.18561v1 Announce Type: new 
Abstract: A linear quadratic (LQ) stochastic optimization system involving large population, which is driven by forward-backward stochastic differential equation (FBSDE), is investigated in this paper. Agents cooperate with each other to minimize the so-called social objective, which is rather different from mean field (MF) game. Employing forward-backward person-by-person optimality principle, we derive an auxiliary LQ control problem by decentralized information. A decentralized strategy is obtained by virtue of an MF-type forward-backward stochastic differential equation consistency condition. Applying Riccati equation decoupling method, we solve the consistency condition system. We also verify the asymptotic social optimality in this framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18561v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guangchen Wang, Shujun Wang, Jie Xiong</dc:creator>
    </item>
    <item>
      <title>Towards topology optimization of a hybrid-excited machine using recursive material interpolation</title>
      <link>https://arxiv.org/abs/2404.18625</link>
      <description>arXiv:2404.18625v1 Announce Type: new 
Abstract: Hybrid-excited electrical machines aim to combine the advantages of permanent magnet machines (high efficiency and torque density) with those of separately excited machines (ease of flux-weakening at high speed). These machines are of interest to electric vehicles, and only parametric approaches are available in the literature for their optimization. This work proposes a more general topology optimization methodology by extending the formalism of density methods. The difficulty lies in integrating the numerous natures of materials (conductors, permanent magnets, ferromagnetic material, air...) without strongly deconvexifying the optimization problem, which leads to non-physical results with unsatisfactory performance. To address this issue, a recursive material interpolation is introduced. The hybrid-excited rotors optimized by this approach are compared with those of existing techniques, demonstrating a clear superiority of the recursive interpolation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18625v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Th\'eodore Cherri\`ere</dc:creator>
    </item>
    <item>
      <title>Energy Efficiency Optimization of Multi-unit System with Different Devices</title>
      <link>https://arxiv.org/abs/2404.18652</link>
      <description>arXiv:2404.18652v1 Announce Type: new 
Abstract: The energy efficiency optimization of the power generation system and the energy efficiency optimization of the energy consumption system are unified into the same optimization problem, and a simple method to achieve energy efficiency optimization without establishing an accurate mathematical model of the system is proposed. For systems with similar energy efficiency, it is proved that the best load distribution method between equipment is to keep the operating energy efficiency of each operating device equal, Yao's theorem 1. It is proved that the optimal switching method for the number of operating units between equipment with different energy efficiency is to keep the energy efficiency of the switching point equal, or at the maximum load point of the equipment, Yao's Theorem 2. This article gives two cases, a system composed of equipment with similar efficiency and a system composed of equipment with different efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18652v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fulai Yao</dc:creator>
    </item>
    <item>
      <title>A Scoping Review on Simulation-based Design Optimization in Marine Engineering: Trends, Best Practices, and Gaps</title>
      <link>https://arxiv.org/abs/2404.18654</link>
      <description>arXiv:2404.18654v1 Announce Type: new 
Abstract: This scoping review assesses the current use of simulation-based design optimization (SBDO) in marine engineering, focusing on identifying research trends, methodologies, and application areas. Analyzing 277 studies from Scopus and Web of Science, the review finds that SBDO is predominantly applied to optimizing marine vessel hulls, including both surface and underwater types, and extends to key components like bows, sterns, propellers, and fins. It also covers marine structures and renewable energy systems. A notable trend is the preference for deterministic single-objective optimization methods, indicating potential growth areas in multi-objective and stochastic approaches. The review points out the necessity of integrating more comprehensive multidisciplinary optimization methods to address the complex challenges in marine environments. Despite the extensive application of SBDO in marine engineering, there remains a need for enhancing the methodologies' efficiency and robustness. This review offers a critical overview of SBDO's role in marine engineering and highlights opportunities for future research to advance the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18654v1</guid>
      <category>math.OC</category>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrea Serani, Thomas Scholcz, Valentina Vanzi</dc:creator>
    </item>
    <item>
      <title>Barrier Algorithms for Constrained Non-Convex Optimization</title>
      <link>https://arxiv.org/abs/2404.18724</link>
      <description>arXiv:2404.18724v1 Announce Type: new 
Abstract: In this paper we theoretically show that interior-point methods based on self-concordant barriers possess favorable global complexity beyond their standard application area of convex optimization. To do that we propose first- and second-order methods for non-convex optimization problems with general convex set constraints and linear constraints. Our methods attain a suitably defined class of approximate first- or second-order KKT points with the worst-case iteration complexity similar to unconstrained problems, namely $O(\varepsilon^{-2})$ (first-order) and $O(\varepsilon^{-3/2})$ (second-order), respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18724v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pavel Dvurechensky, Mathias Staudigl</dc:creator>
    </item>
    <item>
      <title>Generalizing Space Logistics Network Optimization with Integrated Machine Learning and Mathematical Programming</title>
      <link>https://arxiv.org/abs/2404.18770</link>
      <description>arXiv:2404.18770v1 Announce Type: new 
Abstract: Recent growing complexity in space missions has led to an active research field of space logistics and mission design. This research field leverages the key ideas and methods used to handle complex terrestrial logistics to tackle space logistics design problems. A typical goal in space logistics is to optimize the commodity flow to satisfy some mission objectives with the lowest cost. One of the successful space logistics approaches is network flow modeling and optimization using mixed-integer linear programming (MILP). A caveat of the conventional MILP-based network approach for space logistics is its incapability of handling nonlinearity. For example, in the MILP formulation, the spacecraft structure mass and fuel/payload capacity are approximated by a linear relationship. However, this oversimplified relationship cannot characterize a realistic spacecraft design. Other types of nonlinearity can appear when a nonlinear time-dependent trajectory model is considered in an event-driven network, where the time step of each event itself is a variable. In response to this challenge, this Note develops a new systematic general framework to handle nonlinearity in the MILP-based space logistics formulation using machine learning (ML). Specifically, we replace the nonlinear constraints in the space logistics formulation with trained ML models that are compatible with MILP. The MILP-compatible ML model includes linear regression, PWL approximations, neural networks (NN) with Rectified Linear Unit (ReLU) activations, decision tree regression, and random forest regression, among others; these models can be translated into MILP formulations with a definition of additional variables and constraints while maintaining the linearity. This Note provides the first demonstration of using such trained ML models directly in a MILP-based space logistics optimization formulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18770v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Koki Ho, Yuri Shimane, Masafumi Isaji</dc:creator>
    </item>
    <item>
      <title>Interpolating between Optimal Transport and KL regularized Optimal Transport using R\'enyi Divergences</title>
      <link>https://arxiv.org/abs/2404.18834</link>
      <description>arXiv:2404.18834v1 Announce Type: new 
Abstract: Regularized optimal transport (OT) has received much attention in recent years starting from Cuturi's paper with Kullback-Leibler (KL) divergence regularized OT. In this paper, we propose to regularize the OT problem using the family of $\alpha$-R\'enyi divergences for $\alpha \in (0, 1)$. R\'enyi divergences are neither $f$-divergences nor Bregman distances, but they recover the KL divergence in the limit $\alpha \nearrow 1$. The advantage of introducing the additional parameter $\alpha$ is that for $\alpha \searrow 0$ we obtain convergence to the unregularized OT problem. For the KL regularized OT problem, this was achieved by letting the regularization parameter tend to zero, which causes numerical instabilities. We present two different ways to obtain premetrics on probability measures, namely by R\'enyi divergence constraints and penalization. The latter premetric interpolates between the unregularized and KL regularized OT problem with weak convergence of the minimizer, generalizing the interpolating property of KL regularized OT. We use a nested mirror descent algorithm for solving the primal formulation. Both on real and synthetic data sets R\'enyi regularized OT plans outperform their KL and Tsallis counterparts in terms of being closer to the unregularized transport plans and recovering the ground truth in inference tasks better.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18834v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.FA</category>
      <category>math.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonas Bresch, Viktor Stein</dc:creator>
    </item>
    <item>
      <title>Optimization of District Heating Network Parameters in Steady-State Operation</title>
      <link>https://arxiv.org/abs/2404.18868</link>
      <description>arXiv:2404.18868v1 Announce Type: new 
Abstract: We examine the modeling, simulation, and optimization of district heating systems, which are widely used for thermal transport using steam or hot water as a carrier. We propose a generalizable framework to specify network models and scenario parameters, and develop an optimization method for evaluating system states including pressures, fluid flow rates, and temperatures throughout the network. The network modeling includes pipes, thermal plants, pumps, and passive or controllable loads as system components. We propose basic models for thermodynamic fluid transport and enforce the balance of physical quantities in steady-state flow over co-located outgoing and return networks. We formulate an optimization problem with steam and hot water as the outgoing and return carriers, as in legacy 20th century systems. The physical laws and engineering limitations are specified for each component type, and the thermal network flow optimization (TNFO) problem is formulated and solved for a realistic test network under several scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18868v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sai Krishna K. Hari, Anatoly Zlotnik, Shriram Srinivasan, Kaarthik Sundar, Mary Ewers</dc:creator>
    </item>
    <item>
      <title>An optimal lower bound for smooth convex functions</title>
      <link>https://arxiv.org/abs/2404.18889</link>
      <description>arXiv:2404.18889v1 Announce Type: new 
Abstract: First order methods endowed with global convergence guarantees operate using global lower bounds on the objective. The tightening of the bounds has been shown to increase both the theoretical guarantees and the practical performance. In this work, we define a global lower bound for smooth differentiable objectives that is optimal with respect to the collected oracle information. The bound can be readily employed by the Gradient Method with Memory to improve its performance. Further using the machinery underlying the optimal bounds, we introduce a modified version of the estimate sequence that we use to construct an Optimized Gradient Method with Memory possessing the best known convergence guarantees for its class of algorithms, even in terms of the proportionality constant. We additionally equip the method with an adaptive convergence guarantee adjustment procedure that is an effective replacement for line-search. Simulation results on synthetic but otherwise difficult smooth problems validate the theoretical properties of the bound and proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18889v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mihai I. Florea, Yurii Nesterov</dc:creator>
    </item>
    <item>
      <title>BiLO: Bilevel Local Operator Learning for PDE inverse problems</title>
      <link>https://arxiv.org/abs/2404.17789</link>
      <description>arXiv:2404.17789v1 Announce Type: cross 
Abstract: We propose a new neural network based method for solving inverse problems for partial differential equations (PDEs) by formulating the PDE inverse problem as a bilevel optimization problem. At the upper level, we minimize the data loss with respect to the PDE parameters. At the lower level, we train a neural network to locally approximate the PDE solution operator in the neighborhood of a given set of PDE parameters, which enables an accurate approximation of the descent direction for the upper level optimization problem. The lower level loss function includes the L2 norms of both the residual and its derivative with respect to the PDE parameters. We apply gradient descent simultaneously on both the upper and lower level optimization problems, leading to an effective and fast algorithm. The method, which we refer to as BiLO (Bilevel Local Operator learning), is also able to efficiently infer unknown functions in the PDEs through the introduction of an auxiliary variable. We demonstrate that our method enforces strong PDE constraints, is robust to sparse and noisy data, and eliminates the need to balance the residual and the data loss, which is inherent to soft PDE constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17789v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ray Zirui Zhang, Xiaohui Xie, John Lowengrub</dc:creator>
    </item>
    <item>
      <title>Variational Optimization for Quantum Problems using Deep Generative Networks</title>
      <link>https://arxiv.org/abs/2404.18041</link>
      <description>arXiv:2404.18041v1 Announce Type: cross 
Abstract: Optimization is one of the keystones of modern science and engineering. Its applications in quantum technology and machine learning helped nurture variational quantum algorithms and generative AI respectively. We propose a general approach to design variational optimization algorithms based on generative models: the Variational Generative Optimization Network (VGON). To demonstrate its broad applicability, we apply VGON to three quantum tasks: finding the best state in an entanglement-detection protocol, finding the ground state of a 1D quantum spin model with variational quantum circuits, and generating degenerate ground states of many-body quantum Hamiltonians. For the first task, VGON greatly reduces the optimization time compared to stochastic gradient descent while generating nearly optimal quantum states. For the second task, VGON alleviates the barren plateau problem in variational quantum circuits. For the final task, VGON can identify the degenerate ground state spaces after a single stage of training and generate a variety of states therein.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18041v1</guid>
      <category>quant-ph</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lingxia Zhang, Xiaodie Lin, Peidong Wang, Kaiyan Yang, Xiao Zeng, Zhaohui Wei, Zizhu Wang</dc:creator>
    </item>
    <item>
      <title>Exploring the Robustness of In-Context Learning with Noisy Labels</title>
      <link>https://arxiv.org/abs/2404.18191</link>
      <description>arXiv:2404.18191v1 Announce Type: cross 
Abstract: Recently, the mysterious In-Context Learning (ICL) ability exhibited by Transformer architectures, especially in large language models (LLMs), has sparked significant research interest. However, the resilience of Transformers' in-context learning capabilities in the presence of noisy samples, prevalent in both training corpora and prompt demonstrations, remains underexplored. In this paper, inspired by prior research that studies ICL ability using simple function classes, we take a closer look at this problem by investigating the robustness of Transformers against noisy labels. Specifically, we first conduct a thorough evaluation and analysis of the robustness of Transformers against noisy labels during in-context learning and show that they exhibit notable resilience against diverse types of noise in demonstration labels. Furthermore, we delve deeper into this problem by exploring whether introducing noise into the training set, akin to a form of data augmentation, enhances such robustness during inference, and find that such noise can indeed improve the robustness of ICL. Overall, our fruitful analysis and findings provide a comprehensive understanding of the resilience of Transformer models against label noises during ICL and provide valuable insights into the research on Transformers in natural language processing. Our code is available at https://github.com/InezYu0928/in-context-learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18191v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chen Cheng, Xinzhi Yu, Haodong Wen, Jinsong Sun, Guanzhang Yue, Yihao Zhang, Zeming Wei</dc:creator>
    </item>
    <item>
      <title>Bilinear optimal control for the Stokes-Brinkman equations: a priori and a posteriori error analyses</title>
      <link>https://arxiv.org/abs/2404.18348</link>
      <description>arXiv:2404.18348v1 Announce Type: cross 
Abstract: We analyze a bilinear optimal control problem for the Stokes--Brinkman equations: the control variable enters the state equations as a coefficient. In two- and three-dimensional Lipschitz domains, we perform a complete continuous analysis that includes the existence of solutions and first- and second-order optimality conditions. We also develop two finite element methods that differ fundamentally in whether the admissible control set is discretized or not. For each of the proposed methods, we perform a convergence analysis and derive a priori error estimates; the latter under the assumption that the domain is convex. Finally, assuming that the domain is Lipschitz, we develop an a posteriori error estimator for each discretization scheme and obtain a global reliability bound.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18348v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alejandro Allendes, Gilberto Campa\~na, Enrique Otarola</dc:creator>
    </item>
    <item>
      <title>Uncertainty relation and the constrained quadratic programming</title>
      <link>https://arxiv.org/abs/2404.18671</link>
      <description>arXiv:2404.18671v1 Announce Type: cross 
Abstract: The uncertainty relation is a fundamental concept in quantum theory, plays a pivotal role in various quantum information processing tasks. In this study, we explore the additive uncertainty relation pertaining to two or more observables, in terms of their variance,by utilizing the generalized Gell-Mann representation in qudit systems. We find that the tight state-independent lower bound of the variance sum can be characterized as a quadratic programming problem with nonlinear constraints in optimization theory. As illustrative examples, we derive analytical solutions for these quadratic programming problems in lower-dimensional systems, which align with the state-independent lower bounds. Additionally, we introduce a numerical algorithm tailored for solving these quadratic programming instances, highlighting its efficiency and accuracy. The advantage of our approach lies in its potential ability to simultaneously achieve the optimal value of the quadratic programming problem with nonlinear constraints but also precisely identify the extremal state where this optimal value is attained. This enables us to establish a tight state-independent lower bound for the sum of variances, and further identify the extremal state at which this lower bound is realized.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18671v1</guid>
      <category>quant-ph</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1088/1402-4896/ad3f86</arxiv:DOI>
      <arxiv:journal_reference>Physica Scripta 99, 065103 (2024)</arxiv:journal_reference>
      <dc:creator>Lin Zhang, Dade Wu, Ming-Jing Zhao, Hua Nan</dc:creator>
    </item>
    <item>
      <title>Work Smarter...Not Harder: Efficient Minimization of Dependency Length in SOV Languages</title>
      <link>https://arxiv.org/abs/2404.18684</link>
      <description>arXiv:2404.18684v1 Announce Type: cross 
Abstract: Dependency length minimization is a universally observed quantitative property of natural languages. However, the extent of dependency length minimization, and the cognitive mechanisms through which the language processor achieves this minimization remain unclear. This research offers mechanistic insights by postulating that moving a short preverbal constituent next to the main verb explains preverbal constituent ordering decisions better than global minimization of dependency length in SOV languages. This approach constitutes a least-effort strategy because it's just one operation but simultaneously reduces the length of all preverbal dependencies linked to the main verb. We corroborate this strategy using large-scale corpus evidence across all seven SOV languages that are prominently represented in the Universal Dependency Treebank. These findings align with the concept of bounded rationality, where decision-making is influenced by 'quick-yet-economical' heuristics rather than exhaustive searches for optimal solutions. Overall, this work sheds light on the role of bounded rationality in linguistic decision-making and language evolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18684v1</guid>
      <category>cs.CL</category>
      <category>econ.TH</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sidharth Ranjan, Titus von der Malsburg</dc:creator>
    </item>
    <item>
      <title>Differential Inclusions Involving the Curl Operator</title>
      <link>https://arxiv.org/abs/2404.18744</link>
      <description>arXiv:2404.18744v1 Announce Type: cross 
Abstract: In this article, we study the existence of $\eta\in W_0^{1,\infty}(\Omega;\mathbb R^n)$ satisfying $$\textrm{curl} \ \eta\in E \textrm{ a.e. in }\Omega,$$ where $n\in \mathbb N, \Omega\subseteq \mathbb R^n$ is open, bounded and $E\subseteq \Lambda^2.$</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18744v1</guid>
      <category>math.AP</category>
      <category>math.CA</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nurun Nesha</dc:creator>
    </item>
    <item>
      <title>Optimality and uniqueness of the D_4 root system</title>
      <link>https://arxiv.org/abs/2404.18794</link>
      <description>arXiv:2404.18794v1 Announce Type: cross 
Abstract: We prove that the $D_4$ root system (the set of vertices of the regular $24$-cell) is the unique optimal kissing configuration in $\mathbb R^4$, and is an optimal spherical code. For this, we use semidefinite programming to compute an exact optimal solution to the second level of the Lasserre hierarchy. We also improve the upper bound for the kissing number problem in $\mathbb R^6$ to $77$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18794v1</guid>
      <category>math.MG</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David de Laat, Nando M. Leijenhorst, Willem H. H. de Muinck Keizer</dc:creator>
    </item>
    <item>
      <title>PlanNetX: Learning an Efficient Neural Network Planner from MPC for Longitudinal Control</title>
      <link>https://arxiv.org/abs/2404.18863</link>
      <description>arXiv:2404.18863v1 Announce Type: cross 
Abstract: Model predictive control (MPC) is a powerful, optimization-based approach for controlling dynamical systems. However, the computational complexity of online optimization can be problematic on embedded devices. Especially, when we need to guarantee fixed control frequencies. Thus, previous work proposed to reduce the computational burden using imitation learning (IL) approximating the MPC policy by a neural network. In this work, we instead learn the whole planned trajectory of the MPC. We introduce a combination of a novel neural network architecture PlanNetX and a simple loss function based on the state trajectory that leverages the parameterized optimal control structure of the MPC. We validate our approach in the context of autonomous driving by learning a longitudinal planner and benchmarking it extensively in the CommonRoad simulator using synthetic scenarios and scenarios derived from real data. Our experimental results show that we can learn the open-loop MPC trajectory with high accuracy while improving the closed-loop performance of the learned control policy over other baselines like behavior cloning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18863v1</guid>
      <category>cs.RO</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jasper Hoffmann, Diego Fernandez, Julien Brosseit, Julian Bernhard, Klemens Esterle, Moritz Werling, Michael Karg, Joschka Boedecker</dc:creator>
    </item>
    <item>
      <title>Automated Tour Design in the Saturnian System</title>
      <link>https://arxiv.org/abs/2210.14996</link>
      <description>arXiv:2210.14996v2 Announce Type: replace 
Abstract: Future missions to Enceladus would benefit from multi-moon tours that leverage V-infinity on resonant orbits to progressively transfer between moons. Such "resonance family hopping" trajectories present a vast search space for global optimization due to the different combinations of available resonances and flyby speeds. The proposed multi-objective tour design algorithm optimizes entire moon tours from Titan to Enceladus via grid-based dynamic programming, in which the computation time is significantly reduced by utilizing a database of V-infinity-leveraging transfers. The result unveils a complete trade space of the moon tour design to Enceladus in a tractable computation time and global optimality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.14996v2</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s10569-023-10179-8</arxiv:DOI>
      <dc:creator>Yuji Takubo, Damon Landau, Brian Anderson</dc:creator>
    </item>
    <item>
      <title>Primal Dual Alternating Proximal Gradient Algorithms for Nonsmooth Nonconvex Minimax Problems with Coupled Linear Constraints</title>
      <link>https://arxiv.org/abs/2212.04672</link>
      <description>arXiv:2212.04672v4 Announce Type: replace 
Abstract: Nonconvex minimax problems have attracted wide attention in machine learning, signal processing and many other fields in recent years. In this paper, we propose a primal-dual alternating proximal gradient (PDAPG) algorithm for solving nonsmooth nonconvex-(strongly) concave minimax problems with coupled linear constraints, respectively. The iteration complexity of the two algorithms are proved to be $\mathcal{O}\left( \varepsilon ^{-2} \right)$ (resp. $\mathcal{O}\left( \varepsilon ^{-4} \right)$) under nonconvex-strongly concave (resp. nonconvex-concave) setting to reach an $\varepsilon$-stationary point. To our knowledge, it is the first algorithm with iteration complexity guarantees for solving the nonconvex minimax problems with coupled linear constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.04672v4</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huiling Zhang, Junlin Wang, Zi Xu, Yu-Hong Dai</dc:creator>
    </item>
    <item>
      <title>A Stochastic Benders Decomposition Scheme for Large-Scale Stochastic Network Design</title>
      <link>https://arxiv.org/abs/2303.07695</link>
      <description>arXiv:2303.07695v2 Announce Type: replace 
Abstract: Network design problems involve constructing edges in a transportation or supply chain network to minimize construction and daily operational costs. We study a stochastic version where operational costs are uncertain due to fluctuating demand and estimated as a sample average from historical data. This problem is computationally challenging, and instances with as few as 100 nodes often cannot be solved to optimality using current decomposition techniques. We propose a stochastic variant of Benders decomposition that mitigates the high computational cost of generating each cut by sampling a subset of the data at each iteration and nonetheless generates deterministically valid cuts, rather than the probabilistically valid cuts frequently proposed in the stochastic optimization literature, via a dual averaging technique. We implement both single-cut and multi-cut variants of this Benders decomposition, as well as a variant that uses clustering of the historical scenarios. To our knowledge, this is the first single-tree implementation of Benders decomposition that facilitates sampling. On instances with 100-200 nodes and relatively complete recourse, our algorithm achieves 5-7% optimality gaps, compared with 16-27% for deterministic Benders schemes, and scales to instances with 700 nodes and 50 commodities within hours. Beyond network design, our strategy could be adapted to generic two-stage stochastic mixed-integer optimization problems where second-stage costs are estimated via a sample average.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.07695v2</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Dimitris Bertsimas, Ryan Cory-Wright, Jean Pauphilet, Periklis Petridis</dc:creator>
    </item>
    <item>
      <title>Policy Iteration Reinforcement Learning Method for Continuous-time Mean-Field Linear-Quadratic Optimal Problem</title>
      <link>https://arxiv.org/abs/2305.00424</link>
      <description>arXiv:2305.00424v2 Announce Type: replace 
Abstract: This paper employs a policy iteration reinforcement learning (RL) method to study continuous-time linear quadratic mean-field control problems in the infinite horizon. The drift and diffusion terms in the dynamics involve the state as well as the control. We investigate the stability and convergence of the RL algorithm using a Lyapunov Recursion. Instead of solving a pair of coupled Riccati equations, the RL technique focuses on strengthening an auxiliary function and the cost functional as the objective functions and updating the new policy to compute the optimal control via state trajectories. A numerical example sheds light on the established theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.00424v2</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Na Li, Xun Li, Zuo Quan Xu</dc:creator>
    </item>
    <item>
      <title>Reliable optimal controls for SEIR models in epidemiology</title>
      <link>https://arxiv.org/abs/2307.05415</link>
      <description>arXiv:2307.05415v3 Announce Type: replace 
Abstract: We present and compare two different optimal control approaches applied to SEIR models in epidemiology, which allow us to obtain some policies for controlling the spread of an epidemic. The first approach uses Dynamic Programming to characterise the value function of the problem as the solution of a partial differential equation, the Hamilton-Jacobi-Bellman equation, and derive the optimal policy in feedback form. The second is based on Pontryagin's maximum principle and directly gives open-loop controls, via the solution of an optimality system of ordinary differential equations. This method, however, may not converge to the optimal solution. We propose a combination of the two methods in order to obtain high-quality and reliable solutions. Several simulations are presented and discussed, also checking first and second order necessary optimality conditions for the corresponding numerical solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.05415v3</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simone Cacace, Alessio Oliviero</dc:creator>
    </item>
    <item>
      <title>Robust Combinatorial Optimization Problems Under Budgeted Interdiction Uncertainty</title>
      <link>https://arxiv.org/abs/2307.08525</link>
      <description>arXiv:2307.08525v2 Announce Type: replace 
Abstract: In robust combinatorial optimization, we would like to find a solution that performs well under all realizations of an uncertainty set of possible parameter values. How we model this uncertainty set has a decisive influence on the complexity of the corresponding robust problem. For this reason, budgeted uncertainty sets are often studied, as they enable us to decompose the robust problem into easier subproblems. We propose a variant of discrete budgeted uncertainty for cardinality-based constraints or objectives, where a weight vector is applied to the budget constraint. We show that while the adversarial problem can be solved in linear time, the robust problem becomes NP-hard and not approximable. We discuss different possibilities to model the robust problem and show experimentally that despite the hardness result, some models scale relatively well in the problem size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.08525v2</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marc Goerigk, Mohammad Khosravi</dc:creator>
    </item>
    <item>
      <title>Lagrangian Duality in Quantum Optimization: Overcoming QUBO Limitations for Constrained Problems</title>
      <link>https://arxiv.org/abs/2310.04542</link>
      <description>arXiv:2310.04542v2 Announce Type: replace 
Abstract: We propose an approach to solving constrained combinatorial optimization problems based on embedding the concept of Lagrangian duality into the framework of adiabatic quantum computation. Within the setting of circuit-model fault-tolerant quantum computation, we demonstrate that this approach achieves a quadratic improvement in circuit depth and maintains a constraint-independent circuit width in contrast to the prevalent approach of solving constrained problems via reformulations based on the quadratic unconstrained binary optimization (QUBO) framework. Our study includes a detailed review of the limitations encountered when using QUBO for constrained optimization. We show that the proposed method overcomes these limitations by encoding the optimal solution at an energetically elevated level of a simpler problem Hamiltonian, which results in substantially more resource-efficient quantum circuits. We consolidate our strategy with a detailed analysis on how the concepts of Lagrangian duality such as duality gap and complementary slackness relate to the success probability of sampling the optimal solution. Our findings are illustrated by benchmarking the Lagrangian dual approach against the QUBO approach using the NP-complete binary knapsack problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.04542v2</guid>
      <category>math.OC</category>
      <category>quant-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Einar Gabbassov, Gili Rosenberg, Artur Scherer</dc:creator>
    </item>
    <item>
      <title>A mathematical model of the visual MacKay effect</title>
      <link>https://arxiv.org/abs/2311.07338</link>
      <description>arXiv:2311.07338v2 Announce Type: replace 
Abstract: This paper investigates the intricate connection between visual perception and the mathematical modelling of neural activity in the primary visual cortex (V1). The focus is on modelling the visual MacKay effect [Mackay, Nature 1957]. While bifurcation theory has been a prominent mathematical approach for addressing issues in neuroscience, especially in describing spontaneous pattern formations in V1 due to parameter changes, it faces challenges in scenarios with localized sensory inputs. This is evident, for instance, in Mackay's psychophysical experiments, where the redundancy of visual stimuli information results in irregular shapes, making bifurcation theory and multi-scale analysis less effective. To address this, we follow a mathematical viewpoint based on the input-output controllability of an Amari-type neural fields model. In this framework, we consider sensory input as a control function, a cortical representation via the retino-cortical map of the visual stimulus that captures its distinct features. This includes highly localized information in the center of MacKay's funnel pattern "MacKay rays". From a control theory point of view, the Amari-type equation's exact controllability property is discussed for linear and nonlinear response functions. For the visual MacKay effect modelling, we adjust the parameter representing intra-neuron connectivity to ensure that cortical activity exponentially stabilizes to the stationary state in the absence of sensory input. Then, we perform quantitative and qualitative studies to demonstrate that they capture all the essential features of the induced after-image reported by MacKay.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.07338v2</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.NA</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cyprien Tamekue, Dario Prandi, Yacine Chitour</dc:creator>
    </item>
    <item>
      <title>Alignments of Geophysical Fields: a differential geometry perspective</title>
      <link>https://arxiv.org/abs/2312.01341</link>
      <description>arXiv:2312.01341v3 Announce Type: replace 
Abstract: To estimate the displacements of physical state variables, the physics principles that govern the state variables must be considered. Technically, for a certain class of state variables, each state variable is associated to a tensor field. Ways displacement maps act on different state variables will then differ according to their associated different tensor field definitions. Displacement procedures can then explicitly ensure the conservation of certain physical quantities (total mass, total vorticity, total kinetic energy, etc.), and a differential-geometry-based optimisation formulated. Morphing with the correct physics, it is reasonable to apply the estimated displacement map to unobserved state variables, as long as the displacement maps are strongly correlated. This leads to a new nudging strategy using all-available observations to infer displacements of both observed and unobserved state variables. Using the proposed nudging method before applying ensemble data assimilation, numerical results show improved preservation of the intrinsic structure of underline physical processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.01341v3</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yicun Zhen, Valentin Resseguier, Bertrand Chapron</dc:creator>
    </item>
    <item>
      <title>Understanding the Influence of Digraphs on Decentralized Optimization: Effective Metrics, Lower Bound, and Optimal Algorithm</title>
      <link>https://arxiv.org/abs/2312.04928</link>
      <description>arXiv:2312.04928v2 Announce Type: replace 
Abstract: This paper investigates the influence of directed networks on decentralized stochastic non-convex optimization associated with column-stochastic mixing matrices. Surprisingly, we find that the canonical spectral gap, a widely used metric in undirected networks, is insufficient to characterize the impact of directed topology on decentralized algorithms. To overcome this limitation, we introduce a novel metric termed equilibrium skewness. This metric, together with the spectral gap, accurately and comprehensively captures the influence of column-stochastic mixing matrices on decentralized stochastic algorithms. With these two metrics, we clarify, for the first time, how the directed network topology influences the performance of prevalent algorithms such as Push-Sum and Push-Diging. Furthermore, we establish the first lower bound of the convergence rate for decentralized stochastic non-convex algorithms over directed networks. Since existing algorithms cannot match our lower bound, we further propose the MG-Push-Diging algorithm, which integrates Push-Diging with a multi-round gossip technique. MG-Push-Diging attains our lower bound up to logarithmic factors, demonstrating its near-optimal performance and the tightness of the lower bound. Numerical experiments verify our theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.04928v2</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liyuan Liang, Xinmeng Huang, Ran Xin, Kun Yuan</dc:creator>
    </item>
    <item>
      <title>Oracle complexities of augmented Lagrangian methods for nonsmooth manifold optimization</title>
      <link>https://arxiv.org/abs/2404.05121</link>
      <description>arXiv:2404.05121v2 Announce Type: replace 
Abstract: In this paper, we present two novel manifold inexact augmented Lagrangian methods, \textbf{ManIAL} for deterministic settings and \textbf{StoManIAL} for stochastic settings, solving nonsmooth manifold optimization problems. By using the Riemannian gradient method as a subroutine, we establish an $\mathcal{O}(\epsilon^{-3})$ oracle complexity result of \textbf{ManIAL}, matching the best-known complexity result. Our algorithm relies on the careful selection of penalty parameters and the precise control of termination criteria for subproblems. Moreover, for cases where the smooth term follows an expectation form, our proposed \textbf{StoManIAL} utilizes a Riemannian recursive momentum method as a subroutine, and achieves an oracle complexity of $\tilde{\mathcal{O}}(\epsilon^{-3.5})$, which surpasses the best-known $\mathcal{O}(\epsilon^{-4})$ result. Numerical experiments conducted on sparse principal component analysis and sparse canonical correlation analysis demonstrate that our proposed methods outperform an existing method with the previously best-known complexity result. To the best of our knowledge, these are the first complexity results of the augmented Lagrangian methods for solving nonsmooth manifold optimization problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05121v2</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kangkang Deng, Jiang Hu, Jiayuan Wu, Zaiwen Wen</dc:creator>
    </item>
    <item>
      <title>Semi-Infinite Programs for Robust Control and Optimization: Efficient Solutions and Extensions to Existence Constraints</title>
      <link>https://arxiv.org/abs/2404.05635</link>
      <description>arXiv:2404.05635v2 Announce Type: replace 
Abstract: Discrete-time robust optimal control problems generally take a min-max structure over continuous variable spaces, which can be difficult to solve in practice. In this paper, we extend the class of such problems that can be solved through a previously proposed local reduction method to consider those with existence constraints on the uncountable variables. We also consider the possibility of non-unique trajectories that satisfy equality and inequality constraints. Crucially, we show that the problems of interest can be cast into a standard semi-infinite program and demonstrate how to generate optimal uncertainty scenario sets in order to obtain numerical solutions. We also include examples on model predictive control for obstacle avoidance with logical conditions, control with input saturation affected by uncertainty, and optimal parameter estimation to highlight the need for the proposed extension. Our method solves each of the examples considered, producing violation-free and locally optimal solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05635v2</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jad Wehbeh (Department of Electrical and Electronic Engineering, Imperial College London), Eric C. Kerrigan (Department of Electrical and Electronic Engineering, Imperial College London, Department of Aeronautics, Imperial College London)</dc:creator>
    </item>
    <item>
      <title>A New Two-Sided Sketching Algorithm for Large-Scale Tensor Decomposition Based on Discrete Cosine Transformation</title>
      <link>https://arxiv.org/abs/2404.16580</link>
      <description>arXiv:2404.16580v2 Announce Type: replace 
Abstract: Large tensors are frequently encountered in various fields such as computer vision, scientific simulations, sensor networks, and data mining. However, these tensors are often too large for convenient processing, transfer, or storage. Fortunately, they typically exhibit a low-rank structure that can be leveraged through tensor decomposition. Despite this, performing large-scale tensor decomposition can be time-consuming. Sketching is a useful technique to reduce the dimensionality of the data. In this study, we introduce a novel two-sided sketching method based on the $t$-product decomposition and the discrete cosine transformation. We conduct a thorough theoretical analysis to assess the approximation error of the proposed method. Specifically, we enhance the algorithm with power iteration to achieve more precise approximate solutions. Extensive numerical experiments and comparisons on low-rank approximation of color images and grayscale videos illustrate the efficiency and effectiveness of the proposed approach in terms of both CPU time and approximation accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16580v2</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiguang Cheng, Gaohang Yu, Xiaohao Cai, Liqun Qi</dc:creator>
    </item>
    <item>
      <title>Calibrar: an R package for fitting complex ecological models</title>
      <link>https://arxiv.org/abs/1603.03141</link>
      <description>arXiv:1603.03141v2 Announce Type: replace-cross 
Abstract: The fitting or parameter estimation of complex ecological models is a challenging optimisation task, with a notable lack of tools for fitting complex, long runtime or stochastic models. calibrar is an R package that is dedicated to the fitting of complex models to data. It is a generic tool that can be used for any type of model, especially those with non-differentiable objective functions and long runtime, including Individual Based Models. calibrar supports multiple phases and constrained optimisation, includes 18 optimisation algorithms, including derivative-based and heuristic ones. It supports any type of parallelization, the restart of interrupted optimisations for long runtime models and the combination of different optimisation methods during the multiple phases of the calibration. User-level expertise in R is necessary to handle calibration experiments with calibrar, but there is no need to modify the model's code, which can be programmed in any language. It implements maximum likelihood estimation methods and automated construction of the objective function from simulated model outputs. For more experienced users, calibrar allows the implementation of user-defined objective functions. The package source code is fully accessible and can be installed directly from CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:1603.03141v2</guid>
      <category>q-bio.QM</category>
      <category>math.OC</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ricardo Oliveros-Ramos, Yunne-Jai Shin</dc:creator>
    </item>
    <item>
      <title>Size and depth of monotone neural networks: interpolation and approximation</title>
      <link>https://arxiv.org/abs/2207.05275</link>
      <description>arXiv:2207.05275v2 Announce Type: replace-cross 
Abstract: We study monotone neural networks with threshold gates where all the weights (other than the biases) are non-negative. We focus on the expressive power and efficiency of representation of such networks. Our first result establishes that every monotone function over $[0,1]^d$ can be approximated within arbitrarily small additive error by a depth-4 monotone network. When $d &gt; 3$, we improve upon the previous best-known construction which has depth $d+1$. Our proof goes by solving the monotone interpolation problem for monotone datasets using a depth-4 monotone threshold network. In our second main result we compare size bounds between monotone and arbitrary neural networks with threshold gates. We find that there are monotone real functions that can be computed efficiently by networks with no restriction on the gates whereas monotone networks approximating these functions need exponential size in the dimension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.05275v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dan Mikulincer, Daniel Reichman</dc:creator>
    </item>
    <item>
      <title>Heart Disease Detection using Quantum Computing and Partitioned Random Forest Methods</title>
      <link>https://arxiv.org/abs/2208.08882</link>
      <description>arXiv:2208.08882v3 Announce Type: replace-cross 
Abstract: Heart disease morbidity and mortality rates are increasing, which has a negative impact on public health and the global economy. Early detection of heart disease reduces the incidence of heart mortality and morbidity. Recent research has utilized quantum computing methods to predict heart disease with more than 5 qubits and are computationally intensive. Despite the higher number of qubits, earlier work reports a lower accuracy in predicting heart disease, have not considered the outlier effects, and requires more computation time and memory for heart disease prediction. To overcome these limitations, we propose hybrid random forest quantum neural network (HQRF) using a few qubits (two to four) and considered the effects of outlier in the dataset. Two open-source datasets, Cleveland and Statlog, are used in this study to apply quantum networks. The proposed algorithm has been applied on two open-source datasets and utilized two different types of testing strategies such as 10-fold cross validation and 70-30 train/test ratio. We compared the performance of our proposed methodology with our earlier algorithm called hybrid quantum neural network (HQNN) proposed in the literature for heart disease prediction. HQNN and HQRF outperform in 10-fold cross validation and 70/30 train/test split ratio, respectively. The results show that HQNN requires a large training dataset while HQRF is more appropriate for both large and small training dataset. According to the experimental results, the proposed HQRF is not sensitive to the outlier data compared to HQNN. Compared to earlier works, the proposed HQRF achieved a maximum area under the curve (AUC) of 96.43% and 97.78% in predicting heart diseases using Cleveland and Statlog datasets, respectively with HQNN. The proposed HQRF is highly efficient in detecting heart disease at an early stage and will speed up clinical diagnosis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.08882v3</guid>
      <category>quant-ph</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hanif Heidari, Gerhard Hellstern, Murugappan Murugappan</dc:creator>
    </item>
    <item>
      <title>Feedback Stability Analysis via Dissipativity with Dynamic Supply Rates</title>
      <link>https://arxiv.org/abs/2209.08322</link>
      <description>arXiv:2209.08322v2 Announce Type: replace-cross 
Abstract: We propose a general notion of dissipativity with dynamic supply rates for nonlinear systems. This extends classical dissipativity with static supply rates and dynamic supply rates of miscellaneous quadratic forms. The main results of this paper concern Lyapunov and asymptotic stability analysis for nonlinear feedback dissipative systems that are characterised by dissipation inequalities with respect to compatible dynamic supply rates but involving possibly different and independent auxiliary systems. Importantly, dissipativity conditions guaranteeing stability of the state of the feedback systems, without concerns on the stability of the state of the auxiliary systems, are provided. The key results also specialise to a simple coupling test for the interconnection of two nonlinear systems described by dynamic (Psi, Pi, Upsilon, Omega)-dissipativity, and are shown to recover several existing results in the literature, including small-gain, passivity indices, static (Q, S, R)-dissipativity, dissipativity with terminal costs, etc. Comparison with the input-output approach to feedback stability analysis based on integral quadratic constraints is also made.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.08322v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sei Zhen Khong, Chao Chen, Alexander Lanzon</dc:creator>
    </item>
    <item>
      <title>Examining Policy Entropy of Reinforcement Learning Agents for Personalization Tasks</title>
      <link>https://arxiv.org/abs/2211.11869</link>
      <description>arXiv:2211.11869v4 Announce Type: replace-cross 
Abstract: This effort is focused on examining the behavior of reinforcement learning systems in personalization environments and detailing the differences in policy entropy associated with the type of learning algorithm utilized. We demonstrate that Policy Optimization agents often possess low-entropy policies during training, which in practice results in agents prioritizing certain actions and avoiding others. Conversely, we also show that Q-Learning agents are far less susceptible to such behavior and generally maintain high-entropy policies throughout training, which is often preferable in real-world applications. We provide a wide range of numerical experiments as well as theoretical justification to show that these differences in entropy are due to the type of learning being employed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.11869v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anton Dereventsov, Andrew Starnes, Clayton G. Webster</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Linear Functionals of Online SGD in High-dimensional Linear Regression</title>
      <link>https://arxiv.org/abs/2302.09727</link>
      <description>arXiv:2302.09727v2 Announce Type: replace-cross 
Abstract: Stochastic gradient descent (SGD) has emerged as the quintessential method in a data scientist's toolbox. Using SGD for high-stakes applications requires, however, careful quantification of the associated uncertainty. Towards that end, in this work, we establish a high-dimensional Central Limit Theorem (CLT) for linear functionals of online SGD iterates for overparametrized least-squares regression with non-isotropic Gaussian inputs. Our result shows that a CLT holds even when the dimensionality is of order exponential in the number of iterations of the online SGD, which, to the best of our knowledge, is the first such result. In order to use the developed result in practice, we further develop an online approach for estimating the expectation and the variance terms appearing in the CLT, and establish high-probability bounds for the developed online estimator. Furthermore, we propose a two-step fully online bias-correction methodology which together with the CLT result and the variance estimation result, provides a fully online and data-driven way to numerically construct confidence intervals, thereby enabling practical high-dimensional algorithmic inference with SGD. We also extend our results to a class of single-index models, based on the Gaussian Stein's identity. We also provide numerical simulations to verify our theoretical findings in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.09727v2</guid>
      <category>math.ST</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bhavya Agrawalla, Krishnakumar Balasubramanian, Promit Ghosal</dc:creator>
    </item>
    <item>
      <title>TAMUNA: Doubly Accelerated Distributed Optimization with Local Training, Compression, and Partial Participation</title>
      <link>https://arxiv.org/abs/2302.09832</link>
      <description>arXiv:2302.09832v3 Announce Type: replace-cross 
Abstract: In distributed optimization and learning, several machines alternate between local computations in parallel and communication with a distant server. Communication is usually slow and costly and forms the main bottleneck. This is particularly true in federated learning, where a large number of users collaborate toward a global training task. In addition, it is desirable for a robust algorithm to allow for partial participation, since it is often the case that some clients are not able to participate to the entire process and are idle at certain times. Two strategies are popular to reduce the communication burden: 1) local training, which consists in communicating less frequently, or equivalently performing more local computations between the communication rounds; and 2) compression, whereby compressed information instead of full-dimensional vectors is communicated. We propose TAMUNA, the first algorithm for distributed optimization that leveraged the two strategies of local training and compression jointly and allows for partial participation. In the strongly convex setting, TAMUNA converges linearly to the exact solution and provably benefits from the two mechanisms: it exhibits a doubly-accelerated convergence rate, with respect to the condition number of the functions and the model dimension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.09832v3</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Laurent Condat, Ivan Agarsk\'y, Grigory Malinovsky, Peter Richt\'arik</dc:creator>
    </item>
    <item>
      <title>Smoothing the Edges: Smooth Optimization for Sparse Regularization using Hadamard Overparametrization</title>
      <link>https://arxiv.org/abs/2307.03571</link>
      <description>arXiv:2307.03571v3 Announce Type: replace-cross 
Abstract: We present a framework for smooth optimization of explicitly regularized objectives for (structured) sparsity. These non-smooth and possibly non-convex problems typically rely on solvers tailored to specific models and regularizers. In contrast, our method enables fully differentiable and approximation-free optimization and is thus compatible with the ubiquitous gradient descent paradigm in deep learning. The proposed optimization transfer comprises an overparameterization of selected parameters and a change of penalties. In the overparametrized problem, smooth surrogate regularization induces non-smooth, sparse regularization in the base parametrization. We prove that the surrogate objective is equivalent in the sense that it not only has identical global minima but also matching local minima, thereby avoiding the introduction of spurious solutions. Additionally, our theory establishes results of independent interest regarding matching local minima for arbitrary, potentially unregularized, objectives. We comprehensively review sparsity-inducing parametrizations across different fields that are covered by our general theory, extend their scope, and propose improvements in several aspects. Numerical experiments further demonstrate the correctness and effectiveness of our approach on several sparse learning problems ranging from high-dimensional regression to sparse neural network training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.03571v3</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chris Kolb, Christian L. M\"uller, Bernd Bischl, David R\"ugamer</dc:creator>
    </item>
    <item>
      <title>Evolving Scientific Discovery by Unifying Data and Background Knowledge with AI Hilbert</title>
      <link>https://arxiv.org/abs/2308.09474</link>
      <description>arXiv:2308.09474v3 Announce Type: replace-cross 
Abstract: The discovery of scientific formulae that parsimoniously explain natural phenomena and align with existing background theory is a key goal in science. Historically, scientists have derived natural laws by manipulating equations based on existing knowledge, forming new equations, and verifying them experimentally. In recent years, data-driven scientific discovery has emerged as a viable competitor in settings with large amounts of experimental data. Unfortunately, data-driven methods often fail to discover valid laws when data is noisy or scarce. Accordingly, recent works combine regression and reasoning to eliminate formulae inconsistent with background theory. However, the problem of searching over the space of formulae consistent with background theory to find one that best fits the data is not well-solved. We propose a solution to this problem when all axioms and scientific laws are expressible via polynomial equalities and inequalities and argue that our approach is widely applicable. We model notions of minimal complexity using binary variables and logical constraints, solve polynomial optimization problems via mixed-integer linear or semidefinite optimization, and prove the validity of our scientific discoveries in a principled manner using Positivstellensatz certificates. The optimization techniques leveraged in this paper allow our approach to run in polynomial time with fully correct background theory under an assumption that the complexity of our derivation is bounded), or non-deterministic polynomial (NP) time with partially correct background theory. We demonstrate that some famous scientific laws, including Kepler's Third Law of Planetary Motion, the Hagen-Poiseuille Equation, and the Radiated Gravitational Wave Power equation, can be derived in a principled manner from axioms and experimental data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.09474v3</guid>
      <category>cs.AI</category>
      <category>cs.SC</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryan Cory-Wright, Cristina Cornelio, Sanjeeb Dash, Bachir El Khadir, Lior Horesh</dc:creator>
    </item>
    <item>
      <title>Self2Seg: Single-Image Self-Supervised Joint Segmentation and Denoising</title>
      <link>https://arxiv.org/abs/2309.10511</link>
      <description>arXiv:2309.10511v2 Announce Type: replace-cross 
Abstract: We develop Self2Seg, a self-supervised method for the joint segmentation and denoising of a single image. To this end, we combine the advantages of variational segmentation with self-supervised deep learning. One major benefit of our method lies in the fact, that in contrast to data-driven methods, where huge amounts of labeled samples are necessary, Self2Seg segments an image into meaningful regions without any training database. Moreover, we demonstrate that self-supervised denoising itself is significantly improved through the region-specific learning of Self2Seg. Therefore, we introduce a novel self-supervised energy functional in which denoising and segmentation are coupled in a way that both tasks benefit from each other. We propose a unified optimisation strategy and numerically show that for noisy microscopy images our proposed joint approach outperforms its sequential counterpart as well as alternative methods focused purely on denoising or segmentation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.10511v2</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nadja Gruber, Johannes Schwab, No\'emie Debroux, Nicolas Papadakis, Markus Haltmeier</dc:creator>
    </item>
    <item>
      <title>Variational Quantum Eigensolver with Constraints (VQEC): Solving Constrained Optimization Problems via VQE</title>
      <link>https://arxiv.org/abs/2311.08502</link>
      <description>arXiv:2311.08502v3 Announce Type: replace-cross 
Abstract: Variational quantum approaches have shown great promise in finding near-optimal solutions to computationally challenging tasks. Nonetheless, enforcing constraints in a disciplined fashion has been largely unexplored. To address this gap, this work proposes a hybrid quantum-classical algorithmic paradigm termed VQEC that extends the celebrated VQE to handle optimization with constraints. As with the standard VQE, the vector of optimization variables is captured by the state of a variational quantum circuit (VQC). To deal with constraints, VQEC optimizes a Lagrangian function classically over both the VQC parameters as well as the dual variables associated with constraints. To comply with the quantum setup, variables are updated via a perturbed primal-dual method leveraging the parameter shift rule. Among a wide gamut of potential applications, we showcase how VQEC can approximately solve quadratically-constrained binary optimization (QCBO) problems, find stochastic binary policies satisfying quadratic constraints on the average and in probability, and solve large-scale linear programs (LP) over the probability simplex. Under an assumption on the error for the VQC to approximate an arbitrary probability mass function (PMF), we provide bounds on the optimality gap attained by a VQC. Numerical tests on a quantum simulator investigate the effect of various parameters and corroborate that VQEC can generate high-quality solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.08502v3</guid>
      <category>quant-ph</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thinh Viet Le, Vassilis Kekatos</dc:creator>
    </item>
    <item>
      <title>Self-Supervised Learning for Large-Scale Preventive Security Constrained DC Optimal Power Flow</title>
      <link>https://arxiv.org/abs/2311.18072</link>
      <description>arXiv:2311.18072v2 Announce Type: replace-cross 
Abstract: Security-Constrained Optimal Power Flow (SCOPF) plays a crucial role in power grid stability but becomes increasingly complex as systems grow. This paper introduces PDL-SCOPF, a self-supervised end-to-end primal-dual learning framework for producing near-optimal solutions to large-scale SCOPF problems in milliseconds. Indeed, PDL-SCOPF remedies the limitations of supervised counterparts that rely on training instances with their optimal solutions, which becomes impractical for large-scale SCOPF problems. PDL-SCOPF mimics an Augmented Lagrangian Method (ALM) for training primal and dual networks that learn the primal solutions and the Lagrangian multipliers, respectively, to the unconstrained optimizations. In addition, PDL-SCOPF incorporates a repair layer to ensure the feasibility of the power balance in the nominal case, and a binary search layer to compute, using the Automatic Primary Response (APR), the generator dispatches in the contingencies. The resulting differentiable program can then be trained end-to-end using the objective function of the SCOPF and the power balance constraints of the contingencies. Experimental results demonstrate that the PDL-SCOPF delivers accurate feasible solutions with minimal optimality gaps. The framework underlying PDL-SCOPF aims at bridging the gap between traditional optimization methods and machine learning, highlighting the potential of self-supervised end-to-end primal-dual learning for large-scale optimization tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.18072v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Seonho Park, Pascal Van Hentenryck</dc:creator>
    </item>
    <item>
      <title>Predictive stability filters for nonlinear dynamical systems affected by disturbances</title>
      <link>https://arxiv.org/abs/2401.11183</link>
      <description>arXiv:2401.11183v2 Announce Type: replace-cross 
Abstract: Predictive safety filters provide a way of projecting potentially unsafe inputs, proposed, e.g. by a human or learning-based controller, onto the set of inputs that guarantee recursive state and input constraint satisfaction by leveraging model predictive control techniques. In this paper, we extend this framework such that in addition, robust asymptotic stability of the closed-loop system can be guaranteed by enforcing a decrease of an implicit Lyapunov function which is constructed using a predicted system trajectory. Differently from previous results, we show robust asymptotic stability with respect to a predefined disturbance set on an extended state consisting of the system state and a warmstart input sequence. The proposed strategy is applied to an automotive lane keeping example in simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.11183v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexandre Didier, Andrea Zanelli, Kim P. Wabersich, Melanie N. Zeilinger</dc:creator>
    </item>
    <item>
      <title>Who Plays First? Optimizing the Order of Play in Stackelberg Games with Many Robots</title>
      <link>https://arxiv.org/abs/2402.09246</link>
      <description>arXiv:2402.09246v2 Announce Type: replace-cross 
Abstract: We consider the multi-agent spatial navigation problem of computing the socially optimal order of play, i.e., the sequence in which the agents commit to their decisions, and its associated equilibrium in an N-player Stackelberg trajectory game. We model this problem as a mixed-integer optimization problem over the space of all possible Stackelberg games associated with the order of play's permutations. To solve the problem, we introduce Branch and Play (B&amp;P), an efficient and exact algorithm that provably converges to a socially optimal order of play and its Stackelberg equilibrium. As a subroutine for B&amp;P, we employ and extend sequential trajectory planning, i.e., a popular multi-agent control approach, to scalably compute valid local Stackelberg equilibria for any given order of play. We demonstrate the practical utility of B&amp;P to coordinate air traffic control, swarm formation, and delivery vehicle fleets. We find that B&amp;P consistently outperforms various baselines, and computes the socially optimal equilibrium.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.09246v2</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haimin Hu, Gabriele Dragotto, Zixu Zhang, Kaiqu Liang, Bartolomeo Stellato, Jaime F. Fisac</dc:creator>
    </item>
    <item>
      <title>A note on weak compactness of occupation measures for an absorbing Markov decision process</title>
      <link>https://arxiv.org/abs/2402.10672</link>
      <description>arXiv:2402.10672v3 Announce Type: replace-cross 
Abstract: We consider an absorbing Markov decision process with Borel state and action spaces. We study conditions under which the MDP is uniformly absorbing and the set of occupation measures of the MDP is compact in the usual weak topology. These include suitable continuity requirements on the transition kernel and conditions on the dynamics of the system at the boundary of the absorbing set. We generalize previously known results and give an answer to some conjectures that have been mentioned in the related literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.10672v3</guid>
      <category>math.PR</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fran\c{c}ois Dufour, Tom\'as Prieto-Rumeau</dc:creator>
    </item>
    <item>
      <title>A Tunable Universal Formula for Safety-Critical Control</title>
      <link>https://arxiv.org/abs/2403.06285</link>
      <description>arXiv:2403.06285v2 Announce Type: replace-cross 
Abstract: Sontag's universal formula is a widely-used technique for stabilizing control through control Lyapunov functions, and it has been extended to address safety-critical control in recent years by incorporating control barrier functions (CBFs). However, how to derive a universal formula that satisfies requirements on essential properties, including safety, robustness, and smoothness, is still an open problem. To address this challenge, this paper introduces a novel solution - a tunable universal formula - by incorporating a (state-dependent) tunable scaling term into Sontag's universal formula. This tunable scaling term enables the regulation of safety control performances, allowing the attainment of desired properties through a proper selection. Furthermore, we extend this tunable universal formula to address safety-critical control problems with norm-bounded input constraints, showcasing its applicability across diverse control scenarios. Finally, we demonstrate the efficacy of our method through a collision avoidance example, investigating the essential properties including safety, robustness, and smoothness under various tunable scaling terms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06285v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ming Li, Zhiyong Sun, Patrick J. W. Koelewijn, Siep Weiland</dc:creator>
    </item>
    <item>
      <title>Unified Projection-Free Algorithms for Adversarial DR-Submodular Optimization</title>
      <link>https://arxiv.org/abs/2403.10063</link>
      <description>arXiv:2403.10063v2 Announce Type: replace-cross 
Abstract: This paper introduces unified projection-free Frank-Wolfe type algorithms for adversarial continuous DR-submodular optimization, spanning scenarios such as full information and (semi-)bandit feedback, monotone and non-monotone functions, different constraints, and types of stochastic queries. For every problem considered in the non-monotone setting, the proposed algorithms are either the first with proven sub-linear $\alpha$-regret bounds or have better $\alpha$-regret bounds than the state of the art, where $\alpha$ is a corresponding approximation bound in the offline setting. In the monotone setting, the proposed approach gives state-of-the-art sub-linear $\alpha$-regret bounds among projection-free algorithms in 7 of the 8 considered cases while matching the result of the remaining case. Additionally, this paper addresses semi-bandit and bandit feedback for adversarial DR-submodular optimization, advancing the understanding of this optimization area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10063v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CC</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad Pedramfar, Yididiya Y. Nadew, Christopher J. Quinn, Vaneet Aggarwal</dc:creator>
    </item>
    <item>
      <title>Towards Stable Machine Learning Model Retraining via Slowly Varying Sequences</title>
      <link>https://arxiv.org/abs/2403.19871</link>
      <description>arXiv:2403.19871v3 Announce Type: replace-cross 
Abstract: Retraining machine learning models (ML) when new batches of data become available is an important task in real-world pipelines. Existing methods focus largely on greedy approaches to find the best-performing model for each batch, without considering the stability of the model's structure across retraining iterations. In this study, we propose a methodology for finding sequences of ML models that are stable across retraining iterations. We develop a mixed-integer optimization algorithm that is guaranteed to recover Pareto optimal models (in terms of the predictive power-stability trade-off) and an efficient polynomial-time algorithm that performs well in practice. Our method focuses on retaining consistent analytical insights -- which is important to model interpretability, ease of implementation, and fostering trust with users -- by using custom-defined distance metrics that can be directly incorporated into the optimization problem. Importantly, our method shows stronger stability than greedily trained models with a small, controllable sacrifice in model performance in a real-world case study. Using SHAP feature importance, we show that analytical insights are consistent across retraining iterations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19871v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dimitris Bertsimas, Vassilis Digalakis Jr, Yu Ma, Phevos Paschalidis</dc:creator>
    </item>
    <item>
      <title>Collaborative Pareto Set Learning in Multiple Multi-Objective Optimization Problems</title>
      <link>https://arxiv.org/abs/2404.01224</link>
      <description>arXiv:2404.01224v2 Announce Type: replace-cross 
Abstract: Pareto Set Learning (PSL) is an emerging research area in multi-objective optimization, focusing on training neural networks to learn the mapping from preference vectors to Pareto optimal solutions. However, existing PSL methods are limited to addressing a single Multi-objective Optimization Problem (MOP) at a time. When faced with multiple MOPs, this limitation results in significant inefficiencies and hinders the ability to exploit potential synergies across varying MOPs. In this paper, we propose a Collaborative Pareto Set Learning (CoPSL) framework, which learns the Pareto sets of multiple MOPs simultaneously in a collaborative manner. CoPSL particularly employs an architecture consisting of shared and MOP-specific layers. The shared layers are designed to capture commonalities among MOPs collaboratively, while the MOP-specific layers tailor these general insights to generate solution sets for individual MOPs. This collaborative approach enables CoPSL to efficiently learn the Pareto sets of multiple MOPs in a single execution while leveraging the potential relationships among various MOPs. To further understand these relationships, we experimentally demonstrate that shareable representations exist among MOPs. Leveraging these shared representations effectively improves the capability to approximate Pareto sets. Extensive experiments underscore the superior efficiency and robustness of CoPSL in approximating Pareto sets compared to state-of-the-art approaches on a variety of synthetic and real-world MOPs. Code is available at https://github.com/ckshang/CoPSL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01224v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chikai Shang, Rongguang Ye, Jiaqi Jiang, Fangqing Gu</dc:creator>
    </item>
    <item>
      <title>Learning epidemic trajectories through Kernel Operator Learning: from modelling to optimal control</title>
      <link>https://arxiv.org/abs/2404.11130</link>
      <description>arXiv:2404.11130v2 Announce Type: replace-cross 
Abstract: Since infectious pathogens start spreading into a susceptible population, mathematical models can provide policy makers with reliable forecasts and scenario analyses, which can be concretely implemented or solely consulted. In these complex epidemiological scenarios, machine learning architectures can play an important role, since they directly reconstruct data-driven models circumventing the specific modelling choices and the parameter calibration, typical of classical compartmental models. In this work, we discuss the efficacy of Kernel Operator Learning (KOL) to reconstruct population dynamics during epidemic outbreaks, where the transmission rate is ruled by an input strategy. In particular, we introduce two surrogate models, named KOL-m and KOL-$\partial$, which reconstruct in two different ways the evolution of the epidemics. Moreover, we evaluate the generalization performances of the two approaches with different kernels, including the Neural Tangent Kernels, and compare them with a classical neural network model learning method. Employing synthetic but semi-realistic data, we show how the two introduced approaches are suitable for realizing fast and robust forecasts and scenario analyses, and how these approaches are competitive for determining optimal intervention strategies with respect to specific performance measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11130v2</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <category>q-bio.PE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giovanni Ziarelli, Nicola Parolini, Marco Verani</dc:creator>
    </item>
    <item>
      <title>End-to-End Mesh Optimization of a Hybrid Deep Learning Black-Box PDE Solver</title>
      <link>https://arxiv.org/abs/2404.11766</link>
      <description>arXiv:2404.11766v2 Announce Type: replace-cross 
Abstract: Deep learning has been widely applied to solve partial differential equations (PDEs) in computational fluid dynamics. Recent research proposed a PDE correction framework that leverages deep learning to correct the solution obtained by a PDE solver on a coarse mesh. However, end-to-end training of such a PDE correction model over both solver-dependent parameters such as mesh parameters and neural network parameters requires the PDE solver to support automatic differentiation through the iterative numerical process. Such a feature is not readily available in many existing solvers. In this study, we explore the feasibility of end-to-end training of a hybrid model with a black-box PDE solver and a deep learning model for fluid flow prediction. Specifically, we investigate a hybrid model that integrates a black-box PDE solver into a differentiable deep graph neural network. To train this model, we use a zeroth-order gradient estimator to differentiate the PDE solver via forward propagation. Although experiments show that the proposed approach based on zeroth-order gradient estimation underperforms the baseline that computes exact derivatives using automatic differentiation, our proposed method outperforms the baseline trained with a frozen input mesh to the solver. Moreover, with a simple warm-start on the neural network parameters, we show that models trained by these zeroth-order algorithms achieve an accelerated convergence and improved generalization performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11766v2</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shaocong Ma, James Diffenderfer, Bhavya Kailkhura, Yi Zhou</dc:creator>
    </item>
  </channel>
</rss>
