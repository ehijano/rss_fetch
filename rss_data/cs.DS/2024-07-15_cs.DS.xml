<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 16 Jul 2024 02:45:51 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 15 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The CDAWG Index and Pattern Matching on Grammar-Compressed Strings</title>
      <link>https://arxiv.org/abs/2407.08826</link>
      <description>arXiv:2407.08826v1 Announce Type: new 
Abstract: The compact directed acyclic word graph (CDAWG) is the minimal compact automaton that recognizes all the suffixes of a string. Classically the CDAWG has been implemented as an index of the string it recognizes, requiring $o(n)$ space for a copy of the string $T$ being indexed, where $n=|T|$. In this work, we propose using the CDAWG as an index for grammar-compressed strings. While this enables all analyses supported by the CDAWG on any grammar-compressed string, in this work we specifically consider pattern matching. Using the CDAWG index, pattern matching can be performed on any grammar-compressed string in $\mathcal{O}(\text{ra}(m)+\text{occ})$ time while requiring only $\mathcal{O}(\text{er}(T))$ additional space, where $m$ is the length of the pattern, $\text{ra}(m)$ is the grammar random access time, $\text{occ}$ is the number of occurrences of the pattern in $T$, and $\text{er}(T)$ is the number of right-extensions of the maximal repeats in $T$. Our experiments show that even when using a na\"ive random access algorithm, the CDAWG index achieves state of the art run-time performance for pattern matching on grammar-compressed strings. Additionally, we find that all of the grammars computed for our experiments are smaller than the number of right-extensions in the string they produce and, thus, their CDAWGs are within the best known $\mathcal{O}(\text{er}(T))$ space asymptotic bound.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08826v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alan M. Cleary, Joseph Winjum, Jordan Dood, Shunsuke Inenaga</dc:creator>
    </item>
    <item>
      <title>Optimal Protocols for 2-Party Contention Resolution</title>
      <link>https://arxiv.org/abs/2407.08845</link>
      <description>arXiv:2407.08845v1 Announce Type: new 
Abstract: \emph{Contention Resolution} is a fundamental symmetry-breaking problem in which $n$ devices must acquire temporary and exclusive access to some \emph{shared resource}, without the assistance of a mediating authority. For example, the $n$ devices may be sensors that each need to transmit a single packet of data over a broadcast channel. In each time step, devices can (probabilistically) choose to acquire the resource or remain idle; if exactly one device attempts to acquire it, it succeeds, and if two or more devices make an attempt, none succeeds. The complexity of the problem depends heavily on what types of \emph{collision detection} are available. In this paper we consider \emph{acknowledgement-based protocols}, in which devices \underline{only} learn whether their own attempt succeeded or failed; they receive no other feedback from the environment whatsoever, i.e., whether other devices attempted to acquire the resource, succeeded, or failed.
  Nearly all work on the Contention Resolution problem evaluated the performance of algorithms \emph{asymptotically}, as $n\rightarrow \infty$. In this work we focus on the simplest case of $n=2$ devices, but look for \underline{\emph{precisely}} optimal algorithms. We design provably optimal algorithms under three natural cost metrics: minimizing the expected average of the waiting times ({\sc avg}), the expected waiting time until the first device acquires the resource ({\sc min}), and the expected time until the last device acquires the resource ({\sc max}).</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08845v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dingyu Wang</dc:creator>
    </item>
    <item>
      <title>Bipartizing (Pseudo-)Disk Graphs: Approximation with a Ratio Better than 3</title>
      <link>https://arxiv.org/abs/2407.09356</link>
      <description>arXiv:2407.09356v1 Announce Type: new 
Abstract: In a disk graph, every vertex corresponds to a disk in $\mathbb{R}^2$ and two vertices are connected by an edge whenever the two corresponding disks intersect. Disk graphs form an important class of geometric intersection graphs, which generalizes both planar graphs and unit-disk graphs. We study a fundamental optimization problem in algorithmic graph theory, Bipartization (also known as Odd Cycle Transversal), on the class of disk graphs. The goal of Bipartization is to delete a minimum number of vertices from the input graph such that the resulting graph is bipartite. A folklore (polynomial-time) $3$-approximation algorithm for Bipartization on disk graphs follows from the classical framework of Goemans and Williamson [Combinatorica'98] for cycle-hitting problems. For over two decades, this result has remained the best known approximation for the problem (in fact, even for Bipartization on unit-disk graphs). In this paper, we achieve the first improvement upon this result, by giving a $(3-\alpha)$-approximation algorithm for Bipartization on disk graphs, for some constant $\alpha&gt;0$. Our algorithm directly generalizes to the broader class of pseudo-disk graphs. Furthermore, our algorithm is robust in the sense that it does not require a geometric realization of the input graph to be given.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09356v1</guid>
      <category>cs.DS</category>
      <category>cs.CG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Lokshtanov, Fahad Panolan, Saket Saurabh, Jie Xue, Meirav Zehavi</dc:creator>
    </item>
    <item>
      <title>Maximum Unique Coverage on Streams: Improved FPT Approximation Scheme and Tighter Space Lower Bound</title>
      <link>https://arxiv.org/abs/2407.09368</link>
      <description>arXiv:2407.09368v1 Announce Type: new 
Abstract: We consider the Max Unique Coverage problem, including applications to the data stream model. The input is a universe of $n$ elements, a collection of $m$ subsets of this universe, and a cardinality constraint, $k$. The goal is to select a subcollection of at most $k$ sets that maximizes unique coverage, i.e, the number of elements contained in exactly one of the selected sets. The Max Unique Coverage problem has applications in wireless networks, radio broadcast, and envy-free pricing.
  Our first main result is a fixed-parameter tractable approximation scheme (FPT-AS) for Max Unique Coverage, parameterized by $k$ and the maximum element frequency, $r$, which can be implemented on a data stream. Our FPT-AS finds a $(1-\epsilon)$-approximation while maintaining a kernel of size $\tilde{O}(k r/\epsilon)$, which can be combined with subsampling to use $\tilde{O}(k^2 r / \epsilon^3)$ space overall. This significantly improves on the previous-best FPT-AS with the same approximation, but a kernel of size $\tilde{O}(k^2 r / \epsilon^2)$. In order to achieve our result, we show upper bounds on the ratio of a collection's coverage to the unique coverage of a maximizing subcollection; this is by constructing explicit algorithms that find a subcollection with unique coverage at least a logarithmic ratio of the collection's coverage. We complement our algorithms with our second main result, showing that $\Omega(m / k^2)$ space is necessary to achieve a $(1.5 + o(1))/(\ln k - 1)$-approximation in the data stream. This dramatically improves the previous-best lower bound showing that $\Omega(m / k^2)$ is necessary to achieve better than a $e^{-1+1/k}$-approximation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09368v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philip Cervenjak, Junhao Gan, Seeun William Umboh, Anthony Wirth</dc:creator>
    </item>
    <item>
      <title>Nearly-Tight Bounds for Flow Sparsifiers in Quasi-Bipartite Graphs</title>
      <link>https://arxiv.org/abs/2407.09433</link>
      <description>arXiv:2407.09433v1 Announce Type: new 
Abstract: Flow sparsification is a classic graph compression technique which, given a capacitated graph $G$ on $k$ terminals, aims to construct another capacitated graph $H$, called a \emph{flow sparsifier}, that preserves, either exactly or approximately, every \emph{multicommodity flow} between terminals (ideally, with size as a small function of $k$). Cut sparsifiers are a restricted variant of flow sparsifiers which are only required to preserve maximum flows between bipartitions of the terminal set. It is known that exact cut sparsifiers require $2^{\Omega(k)}$ many vertices [Krauthgamer and Rika, SODA 2013], with the hard instances being \emph{quasi-bipartite} graphs, {where there are no edges between non-terminals}. On the other hand, it has been shown recently that exact (or even $(1+\varepsilon)$-approximate) flow sparsifiers on networks with just 6 terminals require unbounded size [Krauthgamer and Mosenzon, SODA 2023, Chen and Tan, SODA 2024].
  In this paper, we construct exact flow sparsifiers of size $3^{k^{3}}$ and exact cut sparsifiers of size $2^{k^2}$ for quasi-bipartite graphs. In particular, the flow sparsifiers are contraction-based, that is, they are obtained from the input graph by (vertex) contraction operations. Our main contribution is a new technique to construct sparsifiers that exploits connections to polyhedral geometry, and that can be generalized to graphs with a small separator that separates the graph into small components. We also give an improved reduction theorem for graphs of bounded treewidth~[Andoni et al., SODA 2011], implying a flow sparsifier of size $O(k\cdot w)$ and quality $O\bigl(\frac{\log w}{\log \log w}\bigr)$, where $w$ is the treewidth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09433v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Syamantak Das, Nikhil Kumar, Daniel Vaz</dc:creator>
    </item>
    <item>
      <title>A Distance for Geometric Graphs via the Labeled Merge Tree Interleaving Distance</title>
      <link>https://arxiv.org/abs/2407.09442</link>
      <description>arXiv:2407.09442v1 Announce Type: new 
Abstract: Geometric graphs appear in many real-world data sets, such as road networks, sensor networks, and molecules. We investigate the notion of distance between embedded graphs and present a metric to measure the distance between two geometric graphs via merge trees. In order to preserve as much useful information as possible from the original data, we introduce a way of rotating the sublevel set to obtain the merge trees via the idea of the directional transform. We represent the merge trees using a surjective multi-labeling scheme and then compute the distance between two representative matrices. We show some theoretically desirable qualities and present two methods of computation: approximation via sampling and exact distance using a kinetic data structure, both in polynomial time. We illustrate its utility by implementing it on two data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09442v1</guid>
      <category>cs.DS</category>
      <category>cs.CG</category>
      <category>math.GN</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erin Wolf Chambers, Elizabeth Munch, Sarah Percival, Xinyi Wang</dc:creator>
    </item>
    <item>
      <title>Interactive Coding with Unbounded Noise</title>
      <link>https://arxiv.org/abs/2407.09463</link>
      <description>arXiv:2407.09463v1 Announce Type: new 
Abstract: Interactive coding allows two parties to conduct a distributed computation despite noise corrupting a certain fraction of their communication. Dani et al.\@ (Inf.\@ and Comp., 2018) suggested a novel setting in which the amount of noise is unbounded and can significantly exceed the length of the (noise-free) computation. While no solution is possible in the worst case, under the restriction of oblivious noise, Dani et al.\@ designed a coding scheme that succeeds with a polynomially small failure probability.
  We revisit the question of conducting computations under this harsh type of noise and devise a computationally-efficient coding scheme that guarantees the success of the computation, except with an exponentially small probability. This higher degree of correctness matches the case of coding schemes with a bounded fraction of noise.
  Our simulation of an $N$-bit noise-free computation in the presence of $T$ corruptions, communicates an optimal number of $O(N+T)$ bits and succeeds with probability $1-2^{-\Omega(N)}$. We design this coding scheme by introducing an intermediary noise model, where an oblivious adversary can choose the locations of corruptions in a worst-case manner, but the effect of each corruption is random: the noise either flips the transmission with some probability or otherwise erases it. This randomized abstraction turns out to be instrumental in achieving an optimal coding scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09463v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eden Fargion, Ran Gelles, Meghal Gupta</dc:creator>
    </item>
    <item>
      <title>On Equivalence of Parameterized Inapproximability of k-Median, k-Max-Coverage, and 2-CSP</title>
      <link>https://arxiv.org/abs/2407.08917</link>
      <description>arXiv:2407.08917v1 Announce Type: cross 
Abstract: Parameterized Inapproximability Hypothesis (PIH) is a central question in the field of parameterized complexity. PIH asserts that given as input a 2-CSP on $k$ variables and alphabet size $n$, it is W[1]-hard parameterized by $k$ to distinguish if the input is perfectly satisfiable or if every assignment to the input violates 1% of the constraints.
  An important implication of PIH is that it yields the tight parameterized inapproximability of the $k$-maxcoverage problem. In the $k$-maxcoverage problem, we are given as input a set system, a threshold $\tau&gt;0$, and a parameter $k$ and the goal is to determine if there exist $k$ sets in the input whose union is at least $\tau$ fraction of the entire universe. PIH is known to imply that it is W[1]-hard parameterized by $k$ to distinguish if there are $k$ input sets whose union is at least $\tau$ fraction of the universe or if the union of every $k$ input sets is not much larger than $\tau\cdot (1-\frac{1}{e})$ fraction of the universe.
  In this work we present a gap preserving FPT reduction (in the reverse direction) from the $k$-maxcoverage problem to the aforementioned 2-CSP problem, thus showing that the assertion that approximating the $k$-maxcoverage problem to some constant factor is W[1]-hard implies PIH. In addition, we present a gap preserving FPT reduction from the $k$-median problem (in general metrics) to the $k$-maxcoverage problem, further highlighting the power of gap preserving FPT reductions over classical gap preserving polynomial time reductions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08917v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Karthik C. S., Euiwoong Lee, Pasin Manurangsi</dc:creator>
    </item>
    <item>
      <title>Tree Independence Number IV. Even-hole-free Graphs</title>
      <link>https://arxiv.org/abs/2407.08927</link>
      <description>arXiv:2407.08927v1 Announce Type: cross 
Abstract: We prove that the tree independence number of every even-hole-free graph is at most polylogarithmic in its number of vertices. More explicitly, we prove that there exists a constant c&gt;0 such that for every integer n&gt;1 every n-vertex even-hole-free graph has a tree decomposition where each bag has stability (independence) number at most c log^10 n. This implies that the Maximum Weight Independent Set problem, as well as several other natural algorithmic problems that are known to be NP-hard in general, can be solved in quasi-polynomial time if the input graph is even-hole-free.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08927v1</guid>
      <category>math.CO</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maria Chudnovsky, Peter Gartland, Sepehr Hajebi, Daniel Lokshtanov, Sophie Spirkl</dc:creator>
    </item>
    <item>
      <title>Resource-aware scheduling of multiple quantum circuits on a hardware device</title>
      <link>https://arxiv.org/abs/2407.08930</link>
      <description>arXiv:2407.08930v1 Announce Type: cross 
Abstract: Recent quantum technologies and quantum error-correcting codes emphasize the requirement for arranging interacting qubits in a nearest-neighbor (NN) configuration while mapping a quantum circuit onto a given hardware device, in order to avoid undesirable noise. It is equally important to minimize the wastage of qubits in a quantum hardware device with m qubits while running circuits of n qubits in total, with n &lt; m. In order to prevent cross-talk between two circuits, a buffer distance between their layouts is needed. Furthermore, not all the qubits and all the two-qubit interactions are at the same noise-level. Scheduling multiple circuits on the same hardware may create a possibility that some circuits are executed on a noisier layout than the others. In this paper, we consider an optimization problem which schedules as many circuits as possible for execution in parallel on the hardware, while maintaining a pre-defined layout quality for each. An integer linear programming formulation to ensure maximum fidelity while preserving the nearest neighbor arrangement among interacting qubits is presented. Our assertion is supported by comprehensive investigations involving various well-known quantum circuit benchmarks. As this scheduling problem is shown to be NP Hard, we also propose a greedy heuristic method which provides 2x and 3x better utilization for 27-qubit and 127-qubit hardware devices respectively in terms of qubits and time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08930v1</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <category>cs.ET</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Debasmita Bhoumik, Ritajit Majumdar, Susmita Sur-Kolay</dc:creator>
    </item>
    <item>
      <title>Structure and Independence in Hyperbolic Uniform Disk Graphs</title>
      <link>https://arxiv.org/abs/2407.09362</link>
      <description>arXiv:2407.09362v1 Announce Type: cross 
Abstract: We consider intersection graphs of disks of radius $r$ in the hyperbolic plane. Unlike the Euclidean setting, these graph classes are different for different values of $r$, where very small $r$ corresponds to an almost-Euclidean setting and $r \in \Omega(\log n)$ corresponds to a firmly hyperbolic setting. We observe that larger values of $r$ create simpler graph classes, at least in terms of separators and the computational complexity of the \textsc{Independent Set} problem.
  First, we show that intersection graphs of disks of radius $r$ in the hyperbolic plane can be separated with $\mathcal{O}((1+1/r)\log n)$ cliques in a balanced manner. Our second structural insight concerns Delaunay complexes in the hyperbolic plane and may be of independent interest. We show that for any set $S$ of $n$ points with pairwise distance at least $2r$ in the hyperbolic plane the corresponding Delaunay complex has outerplanarity $1+\mathcal{O}(\frac{\log n}{r})$, which implies a similar bound on the balanced separators and treewidth of such Delaunay complexes.
  Using this outerplanarity (and treewidth) bound we prove that \textsc{Independent Set} can be solved in $n^{\mathcal{O}(1+\frac{\log n}{r})}$ time. The algorithm is based on dynamic programming on some unknown sphere cut decomposition that is based on the solution. The resulting algorithm is a far-reaching generalization of a result of Kisfaludi-Bak (SODA 2020), and it is tight under the Exponential Time Hypothesis. In particular, \textsc{Independent Set} is polynomial-time solvable in the firmly hyperbolic setting of $r\in \Omega(\log n)$. Finally, in the case when the disks have ply (depth) at most $\ell$, we give a PTAS for \textsc{Maximum Independent Set} that has only quasi-polynomial dependence on $1/\varepsilon$ and $\ell$. Our PTAS is a further generalization of our exact algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09362v1</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Thomas Bl\"asius, Jean-Pierre von der Heydt, S\'andor Kisfaludi-Bak, Marcus Wilhelm, Geert van Wordragen</dc:creator>
    </item>
    <item>
      <title>Integer programs with nearly totally unimodular matrices: the cographic case</title>
      <link>https://arxiv.org/abs/2407.09477</link>
      <description>arXiv:2407.09477v1 Announce Type: cross 
Abstract: It is a notorious open question whether integer programs (IPs), with an integer coefficient matrix $M$ whose subdeterminants are all bounded by a constant $\Delta$ in absolute value, can be solved in polynomial time. We answer this question in the affirmative if we further require that, by removing a constant number of rows and columns from $M$, one obtains a submatrix $A$ that is the transpose of a network matrix.
  Our approach focuses on the case where $A$ arises from $M$ after removing $k$ rows only, where $k$ is a constant. We achieve our result in two main steps, the first related to the theory of IPs and the second related to graph minor theory.
  First, we derive a strong proximity result for the case where $A$ is a general totally unimodular matrix: Given an optimal solution of the linear programming relaxation, an optimal solution to the IP can be obtained by finding a constant number of augmentations by circuits of $[A\; I]$.
  Second, for the case where $A$ is transpose of a network matrix, we reformulate the problem as a maximum constrained integer potential problem on a graph $G$. We observe that if $G$ is $2$-connected, then it has no rooted $K_{2,t}$-minor for $t = \Omega(k \Delta)$. We leverage this to obtain a tree-decomposition of $G$ into highly structured graphs for which we can solve the problem locally. This allows us to solve the global problem via dynamic programming.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09477v1</guid>
      <category>math.CO</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manuel Aprile, Samuel Fiorini, Gwena\"el Joret, Stefan Kober, Micha{\l} T. Seweryn, Stefan Weltge, Yelena Yuditsky</dc:creator>
    </item>
    <item>
      <title>Calibrated Recommendations for Users with Decaying Attention</title>
      <link>https://arxiv.org/abs/2302.03239</link>
      <description>arXiv:2302.03239v2 Announce Type: replace 
Abstract: Recommendation systems capable of providing diverse sets of results are a focus of increasing importance, with motivations ranging from fairness to novelty and other aspects of optimizing user experience. One form of diversity of recent interest is calibration, the notion that personalized recommendations should reflect the full distribution of a user's interests, rather than a single predominant category -- for instance, a user who mainly reads entertainment news but also wants to keep up with news on the environment and the economy would prefer to see a mixture of these genres, not solely entertainment news. Existing work has formulated calibration as a subset selection problem; this line of work observes that the formulation requires the unrealistic assumption that all recommended items receive equal consideration from the user, but leaves as an open question the more realistic setting in which user attention decays as they move down the list of results.
  In this paper, we consider calibration with decaying user attention under two different models. In both models, there is a set of underlying genres that items can belong to. In the first setting, where items are represented by fine-grained mixtures of genre percentages, we provide a $(1-1/e)$-approximation algorithm by extending techniques for constrained submodular optimization. In the second setting, where items are coarsely binned into a single genre each, we surpass the $(1-1/e)$ barrier imposed by submodular maximization and give a $2/3$-approximate greedy algorithm. Our work thus addresses the problem of capturing ordering effects due to decaying attention, allowing for the extension of near-optimal calibration from recommendation sets to recommendation lists.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.03239v2</guid>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <category>cs.SI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jon Kleinberg, Emily Ryu, \'Eva Tardos</dc:creator>
    </item>
    <item>
      <title>Structural Parameterizations of the Biclique-Free Vertex Deletion Problem</title>
      <link>https://arxiv.org/abs/2308.00501</link>
      <description>arXiv:2308.00501v2 Announce Type: replace 
Abstract: In this work, we study the Biclique-Free Vertex Deletion problem: Given a graph $G$ and integers $k$ and $i \le j$, find a set of at most $k$ vertices that intersects every (not necessarily induced) biclique $K_{i, j}$ in $G$. This is a natural generalization of the Bounded-Degree Deletion problem, wherein one asks whether there is a set of at most $k$ vertices whose deletion results in a graph of a given maximum degree $r$. The two problems coincide when $i = 1$ and $j = r + 1$. We show that Biclique-Free Vertex Deletion is fixed-parameter tractable with respect to $k + d$ for the degeneracy $d$ by developing a $2^{O(d k^2)} \cdot n^{O(1)}$-time algorithm. We also show that it can be solved in $2^{O(f k)} \cdot n^{O(1)}$ time for the feedback vertex number $f$ when $i \ge 2$. In contrast, we find that it is W[1]-hard for the treedepth for any integer $i \ge 1$. Finally, we show that Biclique-Free Vertex Deletion has a polynomial kernel for every $i \ge 1$ when parameterized by the feedback edge number. Previously, for this parameter, its fixed-parameter tractability for $i = 1$ was known [Betzler et al., DAM '12] but the existence of polynomial kernel was open.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.00501v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lito Goldmann, Leon Kellerhals, Tomohiro Koana</dc:creator>
    </item>
    <item>
      <title>A Combinatorial Algorithm for Weighted Correlation Clustering</title>
      <link>https://arxiv.org/abs/2310.09638</link>
      <description>arXiv:2310.09638v3 Announce Type: replace 
Abstract: This article introduces a quick and simple combinatorial approximation algorithm for the weighted correlation clustering problem. In this problem, we have a set of vertices and two weight values for each pair of vertices denoting their difference and similarity. The goal is to cluster the vertices with minimum total intra-cluster difference weights plus inter-cluster similarity weights. Our algorithm is a randomized approximation algorithm with $O(n^2)$ running time where $n$ is the number of vertices. Its approximation factor is 3 when the instance satisfies probability constraints. If the instance satisfies triangle inequality in addition to probability constraints, the approximation factor is 1.6. Both algorithms are superior to the best known results in terms of running time and the second one is also superior in terms of the approximation factor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.09638v3</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mojtaba Ostovari, Alireza Zarei</dc:creator>
    </item>
    <item>
      <title>$k$-times bin packing and its application to fair electricity distribution</title>
      <link>https://arxiv.org/abs/2311.16742</link>
      <description>arXiv:2311.16742v2 Announce Type: replace 
Abstract: Given items of different sizes and a fixed bin capacity, the bin-packing problem is to pack these items into a minimum number of bins such that the sum of item sizes in a bin does not exceed the capacity. We define a new variant called \emph{$k$-times bin-packing ($k$BP)}, where the goal is to pack the items such that each item appears exactly $k$ times, in $k$ different bins. We generalize some existing approximation algorithms for bin-packing to solve $k$BP, and analyze their performance ratio.
  The study of $k$BP is motivated by the problem of \emph{fair electricity distribution}. In many developing countries, the total electricity demand is higher than the supply capacity. We prove that every electricity division problem can be solved by $k$-times bin-packing for some finite $k$. We also show that $k$-times bin-packing can be used to distribute the electricity in a fair and efficient way. Particularly, we implement generalizations of the First-Fit and First-Fit Decreasing bin-packing algorithms to solve $k$BP, and apply the generalizations to real electricity demand data. We show that our generalizations outperform existing heuristic solutions to the same problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.16742v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dinesh Kumar Baghel, Alex Ravsky, Erel Segal-Halevi</dc:creator>
    </item>
    <item>
      <title>Chasing Convex Functions with Long-term Constraints</title>
      <link>https://arxiv.org/abs/2402.14012</link>
      <description>arXiv:2402.14012v2 Announce Type: replace 
Abstract: We introduce and study a family of online metric problems with long-term constraints. In these problems, an online player makes decisions $\mathbf{x}_t$ in a metric space $(X,d)$ to simultaneously minimize their hitting cost $f_t(\mathbf{x}_t)$ and switching cost as determined by the metric. Over the time horizon $T$, the player must satisfy a long-term demand constraint $\sum_{t} c(\mathbf{x}_t) \geq 1$, where $c(\mathbf{x}_t)$ denotes the fraction of demand satisfied at time $t$. Such problems can find a wide array of applications to online resource allocation in sustainable energy/computing systems. We devise optimal competitive and learning-augmented algorithms for the case of bounded hitting cost gradients and weighted $\ell_1$ metrics, and further show that our proposed algorithms perform well in numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.14012v2</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adam Lechowicz, Nicolas Christianson, Bo Sun, Noman Bashir, Mohammad Hajiesmaili, Adam Wierman, Prashant Shenoy</dc:creator>
    </item>
    <item>
      <title>Dynamic Suffix Array in Optimal Compressed Space</title>
      <link>https://arxiv.org/abs/2404.07510</link>
      <description>arXiv:2404.07510v2 Announce Type: replace 
Abstract: Big data, encompassing extensive datasets, has seen rapid expansion, notably with a considerable portion being textual data, including strings and texts. Simple compression methods and standard data structures prove inadequate for processing these datasets, as they require decompression for usage or consume extensive memory resources. Consequently, this motivation has led to the development of compressed data structures that support various queries for a given string, typically operating in polylogarithmic time and utilizing compressed space proportional to the string's length. Notably, the suffix array (SA) query is a critical component in implementing a suffix tree, which has a broad spectrum of applications.
  A line of research has been conducted on (especially, static) compressed data structures that support the SA query. A common finding from most of the studies is the suboptimal space efficiency of existing compressed data structures. Kociumaka, Navarro, and Prezza [IEEE Trans. Inf. Theory 2023] have made a significant contribution by introducing an asymptotically minimal space requirement, $O\left(\delta \log\frac{n\log\sigma}{\delta\log n} \log n \right)$ bits ($\delta$-optimal space), sufficient to represent any string of length $n$, with an alphabet size of $\sigma$, and substring complexity $\delta$, serving as a measure of repetitiveness. More recently, Kempa and Kociumaka [FOCS 2023] presented $\delta$-SA, a compressed data structure supporting SA queries in $\delta$-optimal space. However, the data structures introduced thus far are static.
  We present the first dynamic compressed data structure that supports the SA query and update in polylogarithmic time and $\delta$-optimal space. More precisely, it can answer SA queries and perform updates in $O(\log^7 n)$ and expected $O(\log^8 n)$ time, respectively, using an expected $\delta$-optimal space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07510v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takaaki Nishimoto, Yasuo Tabei</dc:creator>
    </item>
    <item>
      <title>Finding Most Shattering Minimum Vertex Cuts of Polylogarithmic Size in Near-Linear Time</title>
      <link>https://arxiv.org/abs/2405.03801</link>
      <description>arXiv:2405.03801v2 Announce Type: replace 
Abstract: We show the first near-linear time randomized algorithms for listing all minimum vertex cuts of polylogarithmic size that separate the graph into at least three connected components (also known as shredders) and for finding the most shattering one, i.e., the one maximizing the number of connected components. Our algorithms break the quadratic time bound by Cheriyan and Thurimella (STOC'96) for both problems that has stood for more than two decades. Our work also removes a bottleneck to near-linear time algorithms for the vertex connectivity augmentation problem (Jordan '95). Note that it is necessary to list only minimum vertex cuts that separate the graph into at least three components because there can be an exponential number of minimum vertex cuts in general.
  To obtain near-linear time algorithms, we have extended techniques in local flow algorithms developed by Forster et al. (SODA'20) to list shredders on a local scale. We also exploit fast queries to a pairwise vertex connectivity oracle subject to vertex failures (Long and Saranurak FOCS'22, Kosinas ESA'23). This is the first application of connectivity oracles subject to vertex failures to speed up a static graph algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03801v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4230/LIPIcs.ICALP.2024.87</arxiv:DOI>
      <dc:creator>Kevin Hua, Daniel Li, Jaewoo Park, Thatchaphol Saranurak</dc:creator>
    </item>
    <item>
      <title>Complexity of Digital Quantum Simulation in the Low-Energy Subspace: Applications and a Lower Bound</title>
      <link>https://arxiv.org/abs/2312.08867</link>
      <description>arXiv:2312.08867v3 Announce Type: replace-cross 
Abstract: Digital quantum simulation has broad applications in approximating unitary evolution of Hamiltonians. In practice, many simulation tasks for quantum systems focus on quantum states in the low-energy subspace instead of the entire Hilbert space. In this paper, we systematically investigate the complexity of digital quantum simulation based on product formulas in the low-energy subspace. We show that the simulation error depends on the effective low-energy norm of the Hamiltonian for a variety of digital quantum simulation algorithms and quantum systems, allowing improvements over the previous complexities for full unitary simulations even for imperfect state preparations due to thermalization. In particular, for simulating spin models in the low-energy subspace, we prove that randomized product formulas such as qDRIFT and random permutation require smaller Trotter numbers. Such improvement also persists in symmetry-protected digital quantum simulations. We prove a similar improvement in simulating the dynamics of power-law quantum interactions. We also provide a query lower bound for general digital quantum simulations in the low-energy subspace.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.08867v3</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.22331/q-2024-07-15-1409</arxiv:DOI>
      <arxiv:journal_reference>Quantum 8, 1409 (2024)</arxiv:journal_reference>
      <dc:creator>Weiyuan Gong, Shuo Zhou, Tongyang Li</dc:creator>
    </item>
    <item>
      <title>Data organization limits the predictability of binary classification</title>
      <link>https://arxiv.org/abs/2401.17036</link>
      <description>arXiv:2401.17036v2 Announce Type: replace-cross 
Abstract: The structure of data organization is widely recognized as having a substantial influence on the efficacy of machine learning algorithms, particularly in binary classification tasks. Our research provides a theoretical framework suggesting that the maximum potential of binary classifiers on a given dataset is primarily constrained by the inherent qualities of the data. Through both theoretical reasoning and empirical examination, we employed standard objective functions, evaluative metrics, and binary classifiers to arrive at two principal conclusions. Firstly, we show that the theoretical upper bound of binary classification performance on actual datasets can be theoretically attained. This upper boundary represents a calculable equilibrium between the learning loss and the metric of evaluation. Secondly, we have computed the precise upper bounds for three commonly used evaluation metrics, uncovering a fundamental uniformity with our overarching thesis: the upper bound is intricately linked to the dataset's characteristics, independent of the classifier in use. Additionally, our subsequent analysis uncovers a detailed relationship between the upper limit of performance and the level of class overlap within the binary classification data. This relationship is instrumental for pinpointing the most effective feature subsets for use in feature engineering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.17036v2</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fei Jing, Zi-Ke Zhang, Yi-Cheng Zhang, Qingpeng Zhang</dc:creator>
    </item>
    <item>
      <title>Budget Recycling Differential Privacy</title>
      <link>https://arxiv.org/abs/2403.11445</link>
      <description>arXiv:2403.11445v4 Announce Type: replace-cross 
Abstract: Differential Privacy (DP) mechanisms usually {force} reduction in data utility by producing "out-of-bound" noisy results for a tight privacy budget. We introduce the Budget Recycling Differential Privacy (BR-DP) framework, designed to provide soft-bounded noisy outputs for a broad range of existing DP mechanisms. By "soft-bounded," we refer to the mechanism's ability to release most outputs within a predefined error boundary, thereby improving utility and maintaining privacy simultaneously. The core of BR-DP consists of two components: a DP kernel responsible for generating a noisy answer per iteration, and a recycler that probabilistically recycles/regenerates or releases the noisy answer. We delve into the privacy accounting of BR-DP, culminating in the development of a budgeting principle that optimally sub-allocates the available budget between the DP kernel and the recycler. Furthermore, we introduce algorithms for tight BR-DP accounting in composition scenarios, and our findings indicate that BR-DP achieves reduced privacy leakage post-composition compared to DP. Additionally, we explore the concept of privacy amplification via subsampling within the BR-DP framework and propose optimal sampling rates for BR-DP across various queries. We experiment with real data, and the results demonstrate BR-DP's effectiveness in lifting the utility-privacy tradeoff provided by DP mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11445v4</guid>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>eess.SP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bo Jiang, Jian Du, Sagar Sharma, Qiang Yan</dc:creator>
    </item>
    <item>
      <title>SAT Encoding of Partial Ordering Models for Graph Coloring Problems</title>
      <link>https://arxiv.org/abs/2403.15961</link>
      <description>arXiv:2403.15961v2 Announce Type: replace-cross 
Abstract: In this paper, we suggest new SAT encodings of the partial-ordering based ILP model for the graph coloring problem (GCP) and the bandwidth coloring problem (BCP). The GCP asks for the minimum number of colors that can be assigned to the vertices of a given graph such that each two adjacent vertices get different colors. The BCP is a generalization, where each edge has a weight that enforces a minimal "distance" between the assigned colors, and the goal is to minimize the "largest" color used. For the widely studied GCP, we experimentally compare our new SAT encoding to the state-of-the-art approaches on the DIMACS benchmark set. Our evaluation confirms that this SAT encoding is effective for sparse graphs and even outperforms the state-of-the-art on some DIMACS instances. For the BCP, our theoretical analysis shows that the partial-ordering based SAT and ILP formulations have an asymptotically smaller size than that of the classical assignment-based model. Our practical evaluation confirms not only a dominance compared to the assignment-based encodings but also to the state-of-the-art approaches on a set of benchmark instances. Up to our knowledge, we have solved several open instances of the BCP from the literature for the first time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15961v2</guid>
      <category>cs.AI</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>cs.LO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Faber, Adalat Jabrayilov, Petra Mutzel</dc:creator>
    </item>
  </channel>
</rss>
