<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 30 Apr 2024 04:00:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 30 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Recent Increments in Incremental View Maintenance</title>
      <link>https://arxiv.org/abs/2404.17679</link>
      <description>arXiv:2404.17679v1 Announce Type: new 
Abstract: We overview recent progress on the longstanding problem of incremental view maintenance (IVM), with a focus on the fine-grained complexity and optimality of IVM for classes of conjunctive queries. This theoretical progress guided the development of IVM engines that reported practical benefits in academic papers and industrial settings. When taken in isolation, each of the reported advancements is but a small increment. Yet when taken together, they may well pave the way to a deeper understanding of the IVM problem.
  This paper accompanies the invited Gems of PODS 2024 talk with the same title. Some of the works highlighted in this paper are based on prior or on-going collaborations with: Ahmet Kara, Milos Nikolic, and Haozhe Zhang in the F-IVM project; and Mahmoud Abo Khamis, Niko G\"obel, Hung Ngo, and Dan Suciu at RelationalAI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17679v1</guid>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dan Olteanu</dc:creator>
    </item>
    <item>
      <title>Geospatial Big Data: Survey and Challenges</title>
      <link>https://arxiv.org/abs/2404.18428</link>
      <description>arXiv:2404.18428v1 Announce Type: new 
Abstract: In recent years, geospatial big data (GBD) has obtained attention across various disciplines, categorized into big earth observation data and big human behavior data. Identifying geospatial patterns from GBD has been a vital research focus in the fields of urban management and environmental sustainability. This paper reviews the evolution of GBD mining and its integration with advanced artificial intelligence (AI) techniques. GBD consists of data generated by satellites, sensors, mobile devices, and geographical information systems, and we categorize geospatial data based on different perspectives. We outline the process of GBD mining and demonstrate how it can be incorporated into a unified framework. Additionally, we explore new technologies like large language models (LLM), the Metaverse, and knowledge graphs, and how they could make GBD even more useful. We also share examples of GBD helping with city management and protecting the environment. Finally, we discuss the real challenges that come up when working with GBD, such as issues with data retrieval and security. Our goal is to give readers a clear view of where GBD mining stands today and where it might go next.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18428v1</guid>
      <category>cs.DB</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiayang Wu, Wensheng Gan, Han-Chieh Chao, Philip S. Yu</dc:creator>
    </item>
    <item>
      <title>Open-Source Drift Detection Tools in Action: Insights from Two Use Cases</title>
      <link>https://arxiv.org/abs/2404.18673</link>
      <description>arXiv:2404.18673v1 Announce Type: new 
Abstract: Data drifts pose a critical challenge in the lifecycle of machine learning (ML) models, affecting their performance and reliability. In response to this challenge, we present a microbenchmark study, called D3Bench, which evaluates the efficacy of open-source drift detection tools. D3Bench examines the capabilities of Evidently AI, NannyML, and Alibi-Detect, leveraging real-world data from two smart building use cases.We prioritize assessing the functional suitability of these tools to identify and analyze data drifts. Furthermore, we consider a comprehensive set of non-functional criteria, such as the integrability with ML pipelines, the adaptability to diverse data types, user-friendliness, computational efficiency, and resource demands. Our findings reveal that Evidently AI stands out for its general data drift detection, whereas NannyML excels at pinpointing the precise timing of shifts and evaluating their consequent effects on predictive accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18673v1</guid>
      <category>cs.DB</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rieke M\"uller, Mohamed Abdelaal, Davor Stjelja</dc:creator>
    </item>
    <item>
      <title>LLMClean: Context-Aware Tabular Data Cleaning via LLM-Generated OFDs</title>
      <link>https://arxiv.org/abs/2404.18681</link>
      <description>arXiv:2404.18681v1 Announce Type: new 
Abstract: Machine learning's influence is expanding rapidly, now integral to decision-making processes from corporate strategy to the advancements in Industry 4.0. The efficacy of Artificial Intelligence broadly hinges on the caliber of data used during its training phase; optimal performance is tied to exceptional data quality. Data cleaning tools, particularly those that exploit functional dependencies within ontological frameworks or context models, are instrumental in augmenting data quality. Nevertheless, crafting these context models is a demanding task, both in terms of resources and expertise, often necessitating specialized knowledge from domain experts.
  In light of these challenges, this paper introduces an innovative approach, called LLMClean, for the automated generation of context models, utilizing Large Language Models to analyze and understand various datasets. LLMClean encompasses a sequence of actions, starting with categorizing the dataset, extracting or mapping relevant models, and ultimately synthesizing the context model. To demonstrate its potential, we have developed and tested a prototype that applies our approach to three distinct datasets from the Internet of Things, healthcare, and Industry 4.0 sectors. The results of our evaluation indicate that our automated approach can achieve data cleaning efficacy comparable with that of context models crafted by human experts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18681v1</guid>
      <category>cs.DB</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fabian Biester, Mohamed Abdelaal, Daniel Del Gaudio</dc:creator>
    </item>
    <item>
      <title>Middle Architecture Criteria</title>
      <link>https://arxiv.org/abs/2404.17757</link>
      <description>arXiv:2404.17757v1 Announce Type: cross 
Abstract: Mid-level ontologies are used to integrate terminologies and data across disparate domains. There are, however, no clear, defensible criteria for determining whether a given ontology should count as mid-level, because we lack a rigorous characterization of what the middle level of generality is supposed to contain. Attempts to provide such a characterization have failed, we believe, because they have focused on the goal of specifying what is characteristic of those single ontologies that have been advanced as mid-level ontologies. Unfortunately, single ontologies of this sort are generally a mixture of top- and mid-level, and sometimes even of domain-level terms. To gain clarity, we aim to specify the necessary and sufficient conditions for a collection of one or more ontologies to inhabit what we call a mid-level architecture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17757v1</guid>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <category>cs.LO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John Beverley, Giacomo De Colle, Mark Jensen, Carter Benson, Barry Smith</dc:creator>
    </item>
    <item>
      <title>The Common Core Ontologies</title>
      <link>https://arxiv.org/abs/2404.17758</link>
      <description>arXiv:2404.17758v1 Announce Type: cross 
Abstract: The Common Core Ontologies (CCO) are designed as a mid-level ontology suite that extends the Basic Formal Ontology. CCO has since been increasingly adopted by a broad group of users and applications and is proposed as the first standard mid-level ontology. Despite these successes, documentation of the contents and design patterns of the CCO has been comparatively minimal. This paper is a step toward providing enhanced documentation for the mid-level ontology suite through a discussion of the contents of the eleven ontologies that collectively comprise the Common Core Ontology suite.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17758v1</guid>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <category>cs.LO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark Jensen, Giacomo De Colle, Sean Kindya, Cameron More, Alexander P. Cox, John Beverley</dc:creator>
    </item>
    <item>
      <title>4DBInfer: A 4D Benchmarking Toolbox for Graph-Centric Predictive Modeling on Relational DBs</title>
      <link>https://arxiv.org/abs/2404.18209</link>
      <description>arXiv:2404.18209v1 Announce Type: cross 
Abstract: Although RDBs store vast amounts of rich, informative data spread across interconnected tables, the progress of predictive machine learning models as applied to such tasks arguably falls well behind advances in other domains such as computer vision or natural language processing. This deficit stems, at least in part, from the lack of established/public RDB benchmarks as needed for training and evaluation purposes. As a result, related model development thus far often defaults to tabular approaches trained on ubiquitous single-table benchmarks, or on the relational side, graph-based alternatives such as GNNs applied to a completely different set of graph datasets devoid of tabular characteristics. To more precisely target RDBs lying at the nexus of these two complementary regimes, we explore a broad class of baseline models predicated on: (i) converting multi-table datasets into graphs using various strategies equipped with efficient subsampling, while preserving tabular characteristics; and (ii) trainable models with well-matched inductive biases that output predictions based on these input subgraphs. Then, to address the dearth of suitable public benchmarks and reduce siloed comparisons, we assemble a diverse collection of (i) large-scale RDB datasets and (ii) coincident predictive tasks. From a delivery standpoint, we operationalize the above four dimensions (4D) of exploration within a unified, scalable open-source toolbox called 4DBInfer. We conclude by presenting evaluations using 4DBInfer, the results of which highlight the importance of considering each such dimension in the design of RDB predictive models, as well as the limitations of more naive approaches such as simply joining adjacent tables. Our source code is released at https://github.com/awslabs/multi-table-benchmark .</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18209v1</guid>
      <category>cs.LG</category>
      <category>cs.DB</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Minjie Wang, Quan Gan, David Wipf, Zhenkun Cai, Ning Li, Jianheng Tang, Yanlin Zhang, Zizhao Zhang, Zunyao Mao, Yakun Song, Yanbo Wang, Jiahang Li, Han Zhang, Guang Yang, Xiao Qin, Chuan Lei, Muhan Zhang, Weinan Zhang, Christos Faloutsos, Zheng Zhang</dc:creator>
    </item>
    <item>
      <title>SPECIAL: Synopsis Assisted Secure Collaborative Analytics</title>
      <link>https://arxiv.org/abs/2404.18388</link>
      <description>arXiv:2404.18388v1 Announce Type: cross 
Abstract: Secure collaborative analytics (SCA) enable the processing of analytical SQL queries across multiple owners' data, even when direct data sharing is not feasible. Although essential for strong privacy, the large overhead from data-oblivious primitives in traditional SCA has hindered its practical adoption. Recent SCA variants that permit controlled leakages under differential privacy (DP) show a better balance between privacy and efficiency. However, they still face significant challenges, such as potentially unbounded privacy loss, suboptimal query planning, and lossy processing. To address these challenges, we introduce SPECIAL, the first SCA system that simultaneously ensures bounded privacy loss, advanced query planning, and lossless processing. SPECIAL employs a novel synopsis-assisted secure processing model, where a one-time privacy cost is spent to acquire private synopses (table statistics) from owner data. These synopses then allow SPECIAL to estimate (compaction) sizes for secure operations (e.g., filter, join) and index encrypted data without extra privacy loss. Crucially, these estimates and indexes can be prepared before runtime, thereby facilitating efficient query planning and accurate cost estimations. Moreover, by using one-sided noise mechanisms and private upper bound techniques, SPECIAL ensures strict lossless processing for complex queries (e.g., multi-join). Through a comprehensive benchmark, we show that SPECIAL significantly outperforms cutting-edge SCAs, with up to 80X faster query times and over 900X smaller memory for complex queries. Moreover, it also achieves up to an 89X reduction in privacy loss under continual processing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18388v1</guid>
      <category>cs.CR</category>
      <category>cs.DB</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenghong Wang, Lina Qiu, Johes Bater, Yukui Luo</dc:creator>
    </item>
    <item>
      <title>ProMoAI: Process Modeling with Generative AI</title>
      <link>https://arxiv.org/abs/2403.04327</link>
      <description>arXiv:2403.04327v2 Announce Type: replace 
Abstract: ProMoAI is a novel tool that leverages Large Language Models (LLMs) to automatically generate process models from textual descriptions, incorporating advanced prompt engineering, error handling, and code generation techniques. Beyond automating the generation of complex process models, ProMoAI also supports process model optimization. Users can interact with the tool by providing feedback on the generated model, which is then used for refining the process model. ProMoAI utilizes the capabilities LLMs to offer a novel, AI-driven approach to process modeling, significantly reducing the barrier to entry for users without deep technical knowledge in process modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04327v2</guid>
      <category>cs.DB</category>
      <category>cs.CL</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Humam Kourani, Alessandro Berti, Daniel Schuster, Wil M. P. van der Aalst</dc:creator>
    </item>
    <item>
      <title>Towards declarative comparabilities: application to functional dependencies</title>
      <link>https://arxiv.org/abs/1909.12656</link>
      <description>arXiv:1909.12656v4 Announce Type: replace-cross 
Abstract: In real life, data are often of poor quality as a result, for instance, of uncertainty, mismeasurements, missing values or bad inputs. This issue hampers an implicit yet crucial operation of every database management system: equality testing. Indeed, equality is, in the end, a context-dependent operation with a plethora of interpretations. In practice, the treatment of different types of equality is left to programmers, who have to struggle with those interpretations in their code. We propose a new lattice-based declarative framework to address this problem. It allows specification of numerous semantics for equality at a high level of abstraction. To go beyond tuple equality, we study functional dependencies (FDs) in the light of our framework. First, we define abstract FDs, generalizing classical FDs. These lead to the consideration of particular interpretations of equality: realities. Building upon realities and possible/certain answers, we introduce possible/certain FDs and give some related complexity results.</description>
      <guid isPermaLink="false">oai:arXiv.org:1909.12656v4</guid>
      <category>cs.LO</category>
      <category>cs.DB</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lhouari Nourine, Jean Marc Petit, Simon Vilmin</dc:creator>
    </item>
  </channel>
</rss>
