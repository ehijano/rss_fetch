<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 24 Mar 2025 04:01:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Numerical Investigation of Preferential Flow Paths in Enzymatically Induced Calcite Precipitation supported by Bayesian Model Analysis</title>
      <link>https://arxiv.org/abs/2503.17314</link>
      <description>arXiv:2503.17314v1 Announce Type: cross 
Abstract: The usability of enzymatically induced calcium carbonate precipitation (EICP) as a method for altering porous-media properties, soil stabilization, or biocementation depends on our ability to predict the spatial distribution of the precipitated calcium carbonate in porous media. While current REV-scale models are able to reproduce the main features of laboratory experiments, they neglect effects like the formation of preferential flow paths and the appearance of multiple polymorphs of calcium carbonate with differing properties. We show that extending an existing EICP model by the conceptual assumption of a mobile precipitate, amorphous calcium carbonate (ACC), allows for the formation of preferential flow paths when the initial porosity is heterogeneous. We apply sensitivity analysis and Bayesian inference to gain an understanding of the influence of characteristic parameters of ACC that are uncertain or unknown and compare two variations of the model based on different formulations of the ACC detachment term to analyse the plausibility of our hypothesis. An arbitrary Polynomial Chaos (aPC) surrogate model is trained based on the full model and used to reduce the computational cost of this study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17314v1</guid>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rebecca Kohlhaas, Johannes Hommel, Felix Weinhardt, Holger Class, Sergey Oladyshkin, Bernd Flemisch</dc:creator>
    </item>
    <item>
      <title>Network reconstruction via the minimum description length principle</title>
      <link>https://arxiv.org/abs/2405.01015</link>
      <description>arXiv:2405.01015v3 Announce Type: replace-cross 
Abstract: A fundamental problem associated with the task of network reconstruction from dynamical or behavioral data consists in determining the most appropriate model complexity in a manner that prevents overfitting, and produces an inferred network with a statistically justifiable number of edges. The status quo in this context is based on $L_{1}$ regularization combined with cross-validation. However, besides its high computational cost, this commonplace approach unnecessarily ties the promotion of sparsity with weight "shrinkage". This combination forces a trade-off between the bias introduced by shrinkage and the network sparsity, which often results in substantial overfitting even after cross-validation. In this work, we propose an alternative nonparametric regularization scheme based on hierarchical Bayesian inference and weight quantization, which does not rely on weight shrinkage to promote sparsity. Our approach follows the minimum description length (MDL) principle, and uncovers the weight distribution that allows for the most compression of the data, thus avoiding overfitting without requiring cross-validation. The latter property renders our approach substantially faster to employ, as it requires a single fit to the complete data. As a result, we have a principled and efficient inference scheme that can be used with a large variety of generative models, without requiring the number of edges to be known in advance. We also demonstrate that our scheme yields systematically increased accuracy in the reconstruction of both artificial and empirical networks. We highlight the use of our method with the reconstruction of interaction networks between microbial communities from large-scale abundance samples involving in the order of $10^{4}$ to $10^{5}$ species, and demonstrate how the inferred model can be used to predict the outcome of interventions in the system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01015v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <category>physics.data-an</category>
      <category>q-bio.PE</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevX.15.011065</arxiv:DOI>
      <arxiv:journal_reference>Phys. Rev. X 15, 011065 (2025)</arxiv:journal_reference>
      <dc:creator>Tiago P. Peixoto</dc:creator>
    </item>
    <item>
      <title>Schr\"odinger Bridges for Systems of Interacting Particles</title>
      <link>https://arxiv.org/abs/2503.09328</link>
      <description>arXiv:2503.09328v2 Announce Type: replace-cross 
Abstract: A Schr\"odinger bridge is the most probable time-dependent probability distribution that connects an initial probability distribution $w_{i}$ to a final one $w_{f}$. The problem has been solved and widely used for the case of simple Brownian evolution (non-interacting particles). It is related to the problem of entropy regularized Wasserstein optimal transport. In this article, we generalize Brownian bridges to systems of interacting particles. We derive some equations for the forward and backward single particle ``wave-functions'' which allow to compute the most probable evolution of the single-particle probability between the initial and final distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09328v2</guid>
      <category>cond-mat.stat-mech</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Henri Orland</dc:creator>
    </item>
    <item>
      <title>Hyperspectral Unmixing using Iterative, Sparse and Ensambling Approaches for Large Spectral Libraries Applied to Soils and Minerals</title>
      <link>https://arxiv.org/abs/2503.16298</link>
      <description>arXiv:2503.16298v2 Announce Type: replace-cross 
Abstract: Unmixing is a fundamental process in hyperspectral image processing in which the materials present in a mixed pixel are determined based on the spectra of candidate materials and the pixel spectrum. Practical and general utility requires a large spectral library with sample measurements covering the full variation in each candidate material as well as a sufficiently varied collection of potential materials. However, any spectral library with more spectra than bands will lead to an ill-posed inversion problem when using classical least-squares regression-based unmixing methods. Moreover, for numerical and dimensionality reasons, libraries with over 10 or 20 spectra behave computationally as though they are ill-posed. In current practice, unmixing is often applied to imagery using manually-selected materials or image endmembers. General unmixing of a spectrum from an unknown material with a large spectral library requires some form of sparse regression; regression where only a small number of coefficients are nonzero. This requires a trade-off between goodness-of-fit and model size. In this study we compare variations of two sparse regression techniques, focusing on the relationship between structure and chemistry of materials and the accuracy of the various models for identifying the correct mixture of materials present. Specifically, we examine LASSO regression and ElasticNet in contrast with variations of iterative feature selection, Bayesian Model Averaging (BMA), and quadratic BMA (BMA-Q) -- incorporating LASSO regression and ElasticNet as their base model. To evaluate the the effectiveness of these methods, we consider the molecular composition similarities and differences of substances selected in the models compared to the ground truth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16298v2</guid>
      <category>eess.IV</category>
      <category>physics.data-an</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jade Preston, William Basener</dc:creator>
    </item>
  </channel>
</rss>
