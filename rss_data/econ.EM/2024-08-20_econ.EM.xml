<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 20 Aug 2024 04:00:45 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 20 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Panel Data Unit Root testing: Overview</title>
      <link>https://arxiv.org/abs/2408.08908</link>
      <description>arXiv:2408.08908v1 Announce Type: new 
Abstract: This review discusses methods of testing for a panel unit root. Modern approaches to testing in cross-sectionally correlated panels are discussed, preceding the analysis with an analysis of independent panels. In addition, methods for testing in the case of non-linearity in the data (for example, in the case of structural breaks) are presented, as well as methods for testing in short panels, when the time dimension is small and finite. In conclusion, links to existing packages that allow implementing some of the described methods are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08908v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anton Skrobotov</dc:creator>
    </item>
    <item>
      <title>Externally Valid Selection of Experimental Sites via the k-Median Problem</title>
      <link>https://arxiv.org/abs/2408.09187</link>
      <description>arXiv:2408.09187v1 Announce Type: new 
Abstract: We present a decision-theoretic justification for viewing the question of how to best choose where to experiment in order to optimize external validity as a k-median (clustering) problem, a popular problem in computer science and operations research. We present conditions under which minimizing the worst-case, welfare-based regret among all nonrandom schemes that select k sites to experiment is approximately equal - and sometimes exactly equal - to finding the k most central vectors of baseline site-level covariates. The k-median problem can be formulated as a linear integer program. Two empirical applications illustrate the theoretical and computational benefits of the suggested procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09187v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jos\'e Luis Montiel Olea, Brenda Prallon, Chen Qiu, J\"org Stoye, Yiwei Sun</dc:creator>
    </item>
    <item>
      <title>Counterfactual and Synthetic Control Method: Causal Inference with Instrumented Principal Component Analysis</title>
      <link>https://arxiv.org/abs/2408.09271</link>
      <description>arXiv:2408.09271v1 Announce Type: new 
Abstract: The fundamental problem of causal inference lies in the absence of counterfactuals. Traditional methodologies impute the missing counterfactuals implicitly or explicitly based on untestable or overly stringent assumptions. Synthetic control method (SCM) utilizes a weighted average of control units to impute the missing counterfactual for the treated unit. Although SCM relaxes some strict assumptions, it still requires the treated unit to be inside the convex hull formed by the controls, avoiding extrapolation. In recent advances, researchers have modeled the entire data generating process (DGP) to explicitly impute the missing counterfactual. This paper expands the interactive fixed effect (IFE) model by instrumenting covariates into factor loadings, adding additional robustness. This methodology offers multiple benefits: firstly, it incorporates the strengths of previous SCM approaches, such as the relaxation of the untestable parallel trends assumption (PTA). Secondly, it does not require the targeted outcomes to be inside the convex hull formed by the controls. Thirdly, it eliminates the need for correct model specification required by the IFE model. Finally, it inherits the ability of principal component analysis (PCA) to effectively handle high-dimensional data and enhances the value extracted from numerous covariates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09271v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Cong Wang</dc:creator>
    </item>
    <item>
      <title>Deep Learning for the Estimation of Heterogeneous Parameters in Discrete Choice Models</title>
      <link>https://arxiv.org/abs/2408.09560</link>
      <description>arXiv:2408.09560v1 Announce Type: new 
Abstract: This paper studies the finite sample performance of the flexible estimation approach of Farrell, Liang, and Misra (2021a), who propose to use deep learning for the estimation of heterogeneous parameters in economic models, in the context of discrete choice models. The approach combines the structure imposed by economic models with the flexibility of deep learning, which assures the interpretebility of results on the one hand, and allows estimating flexible functional forms of observed heterogeneity on the other hand. For inference after the estimation with deep learning, Farrell et al. (2021a) derive an influence function that can be applied to many quantities of interest. We conduct a series of Monte Carlo experiments that investigate the impact of regularization on the proposed estimation and inference procedure in the context of discrete choice models. The results show that the deep learning approach generally leads to precise estimates of the true average parameters and that regular robust standard errors lead to invalid inference results, showing the need for the influence function approach for inference. Without regularization, the influence function approach can lead to substantial bias and large estimated standard errors caused by extreme outliers. Regularization reduces this property and stabilizes the estimation procedure, but at the expense of inducing an additional bias. The bias in combination with decreasing variance associated with increasing regularization leads to the construction of invalid inferential statements in our experiments. Repeated sample splitting, unlike regularization, stabilizes the estimation approach without introducing an additional bias, thereby allowing for the construction of valid inferential statements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09560v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stephan Hetzenecker, Maximilian Osterhaus</dc:creator>
    </item>
    <item>
      <title>Method of Moments Estimation for Affine Stochastic Volatility Models</title>
      <link>https://arxiv.org/abs/2408.09185</link>
      <description>arXiv:2408.09185v1 Announce Type: cross 
Abstract: We develop moment estimators for the parameters of affine stochastic volatility models. We first address the challenge of calculating moments for the models by introducing a recursive equation for deriving closed-form expressions for moments of any order. Consequently, we propose our moment estimators. We then establish a central limit theorem for our estimators and derive the explicit formulas for the asymptotic covariance matrix. Finally, we provide numerical results to validate our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09185v1</guid>
      <category>q-fin.ST</category>
      <category>econ.EM</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yan-Feng Wu, Xiangyu Yang, Jian-Qiang Hu</dc:creator>
    </item>
    <item>
      <title>Anytime-Valid Inference for Double/Debiased Machine Learning of Causal Parameters</title>
      <link>https://arxiv.org/abs/2408.09598</link>
      <description>arXiv:2408.09598v1 Announce Type: cross 
Abstract: Double (debiased) machine learning (DML) has seen widespread use in recent years for learning causal/structural parameters, in part due to its flexibility and adaptability to high-dimensional nuisance functions as well as its ability to avoid bias from regularization or overfitting. However, the classic double-debiased framework is only valid asymptotically for a predetermined sample size, thus lacking the flexibility of collecting more data if sharper inference is needed, or stopping data collection early if useful inferences can be made earlier than expected. This can be of particular concern in large scale experimental studies with huge financial costs or human lives at stake, as well as in observational studies where the length of confidence of intervals do not shrink to zero even with increasing sample size due to partial identifiability of a structural parameter. In this paper, we present time-uniform counterparts to the asymptotic DML results, enabling valid inference and confidence intervals for structural parameters to be constructed at any arbitrary (possibly data-dependent) stopping time. We provide conditions which are only slightly stronger than the standard DML conditions, but offer the stronger guarantee for anytime-valid inference. This facilitates the transformation of any existing DML method to provide anytime-valid guarantees with minimal modifications, making it highly adaptable and easy to use. We illustrate our procedure using two instances: a) local average treatment effect in online experiments with non-compliance, and b) partial identification of average treatment effect in observational studies with potential unmeasured confounding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09598v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhinandan Dalal, Patrick Bl\"obaum, Shiva Kasiviswanathan, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Experimental Design For Causal Inference Through An Optimization Lens</title>
      <link>https://arxiv.org/abs/2408.09607</link>
      <description>arXiv:2408.09607v1 Announce Type: cross 
Abstract: The study of experimental design offers tremendous benefits for answering causal questions across a wide range of applications, including agricultural experiments, clinical trials, industrial experiments, social experiments, and digital experiments. Although valuable in such applications, the costs of experiments often drive experimenters to seek more efficient designs. Recently, experimenters have started to examine such efficiency questions from an optimization perspective, as experimental design problems are fundamentally decision-making problems. This perspective offers a lot of flexibility in leveraging various existing optimization tools to study experimental design problems. This manuscript thus aims to examine the foundations of experimental design problems in the context of causal inference as viewed through an optimization lens.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09607v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinglong Zhao</dc:creator>
    </item>
    <item>
      <title>kendallknight: Efficient Implementation of Kendall's Correlation Coefficient Computation</title>
      <link>https://arxiv.org/abs/2408.09618</link>
      <description>arXiv:2408.09618v1 Announce Type: cross 
Abstract: The kendallknight package introduces an efficient implementation of Kendall's correlation coefficient computation, significantly improving the processing time for large datasets without sacrificing accuracy. The kendallknight package, following Knight (1966) and posterior literature, reduces the computational complexity resulting in drastic reductions in computation time, transforming operations that would take minutes or hours into milliseconds or minutes, while maintaining precision and correctly handling edge cases and errors. The package is particularly advantageous in econometric and statistical contexts where rapid and accurate calculation of Kendall's correlation coefficient is desirable. Benchmarks demonstrate substantial performance gains over the base R implementation, especially for large datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09618v1</guid>
      <category>stat.CO</category>
      <category>cs.DS</category>
      <category>econ.EM</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mauricio Vargas Sep\'ulveda</dc:creator>
    </item>
    <item>
      <title>Robustness, Heterogeneous Treatment Effects and Covariate Shifts</title>
      <link>https://arxiv.org/abs/2112.09259</link>
      <description>arXiv:2112.09259v2 Announce Type: replace 
Abstract: This paper studies the robustness of estimated policy effects to changes in the distribution of covariates. Robustness to covariate shifts is important, for example, when evaluating the external validity of quasi-experimental results, which are often used as a benchmark for evidence-based policy-making. I propose a novel scalar robustness metric. This metric measures the magnitude of the smallest covariate shift needed to invalidate a claim on the policy effect (for example, $ATE \geq 0$) supported by the quasi-experimental evidence. My metric links the heterogeneity of policy effects and robustness in a flexible, nonparametric way and does not require functional form assumptions. I cast the estimation of the robustness metric as a de-biased GMM problem. This approach guarantees a parametric convergence rate for the robustness metric while allowing for machine learning-based estimators of policy effect heterogeneity (for example, lasso, random forest, boosting, neural nets). I apply my procedure to the Oregon Health Insurance experiment. I study the robustness of policy effects estimates of health-care utilization and financial strain outcomes, relative to a shift in the distribution of context-specific covariates. Such covariates are likely to differ across US states, making quantification of robustness an important exercise for adoption of the insurance policy in states other than Oregon. I find that the effect on outpatient visits is the most robust among the metrics of health-care utilization considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.09259v2</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pietro Emilio Spini</dc:creator>
    </item>
    <item>
      <title>Identifying Causal Effects of Nonbinary, Ordered Treatments using Multiple Instrumental Variables</title>
      <link>https://arxiv.org/abs/2311.17575</link>
      <description>arXiv:2311.17575v2 Announce Type: replace 
Abstract: This paper introduces a novel method for identifying causal effects of ordered, nonbinary treatments using multiple binary instruments. Extending the two-stage least squares (TSLS) framework, the approach accommodates ordered treatments under any monotonicity assumption. The key contribution is the identification of a new causal parameter that simplifies the interpretation of causal effects and is broadly applicable due to a mild monotonicity assumption, offering a compelling alternative to TSLS. The paper builds upon recent causal machine learning methodology for estimation and demonstrates how causal forests can detect local violations of the underlying monotonicity assumption. The methodology is applied to estimate the returns to education using the seminal dataset of Card (1995) and to evaluate the impact of an additional child on female labor market outcomes using the data from Angrist and Evans (1998).</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.17575v2</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nadja van 't Hoff</dc:creator>
    </item>
    <item>
      <title>Julia as a universal platform for statistical software development</title>
      <link>https://arxiv.org/abs/2404.09309</link>
      <description>arXiv:2404.09309v3 Announce Type: replace 
Abstract: The julia package integrates the Julia programming language into Stata. Users can transfer data between Stata and Julia, issue Julia commands to analyze and plot, and pass results back to Stata. Julia's econometric ecosystem is not as mature as Stata's or R's or Python's. But Julia is an excellent environment for developing high-performance numerical applications, which can then be called from many platforms. For example, the boottest program for wild bootstrap-based inference (Roodman et al. 2019) and fwildclusterboot for R (Fischer and Roodman 2021) can both call the same Julia back end. And the program reghdfejl mimics reghdfe (Correia 2016) in fitting linear models with high-dimensional fixed effects but calls a Julia package for tenfold acceleration on hard problems. reghdfejl also supports nonlinear fixed-effect models that cannot otherwise be fit in Stata--though preliminarily, as the Julia package for that purpose is immature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09309v3</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>David Roodman</dc:creator>
    </item>
    <item>
      <title>Measuring Diagnostic Test Performance Using Imperfect Reference Tests: A Partial Identification Approach</title>
      <link>https://arxiv.org/abs/2204.00180</link>
      <description>arXiv:2204.00180v4 Announce Type: replace-cross 
Abstract: Diagnostic tests are almost never perfect. Studies quantifying their performance use knowledge of the true health status, measured with a reference diagnostic test. Researchers commonly assume that the reference test is perfect, which is often not the case in practice. When the assumption fails, conventional studies identify "apparent" performance or performance with respect to the reference, but not true performance. This paper provides the smallest possible bounds on the measures of true performance - sensitivity (true positive rate) and specificity (true negative rate), or equivalently false positive and negative rates, in standard settings. Implied bounds on policy-relevant parameters are derived: 1) Prevalence in screened populations; 2) Predictive values. Methods for inference based on moment inequalities are used to construct uniformly consistent confidence sets in level over a relevant family of data distributions. Emergency Use Authorization (EUA) and independent study data for the BinaxNOW COVID-19 antigen test demonstrate that the bounds can be very informative. Analysis reveals that the estimated false negative rates for symptomatic and asymptomatic patients are up to 3.17 and 4.59 times higher than the frequently cited "apparent" false negative rate. Further applicability of the results in the context of imperfect proxies such as survey responses and imputed protected classes is indicated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.00180v4</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Filip Obradovi\'c</dc:creator>
    </item>
    <item>
      <title>Revisiting Day-ahead Electricity Price: Simple Model Save Millions</title>
      <link>https://arxiv.org/abs/2405.14893</link>
      <description>arXiv:2405.14893v2 Announce Type: replace-cross 
Abstract: Accurate day-ahead electricity price forecasting is essential for residential welfare, yet current methods often fall short in forecast accuracy. We observe that commonly used time series models struggle to utilize the prior correlation between price and demand-supply, which, we found, can contribute a lot to a reliable electricity price forecaster. Leveraging this prior, we propose a simple piecewise linear model that significantly enhances forecast accuracy by directly deriving prices from readily forecastable demand-supply values. Experiments in the day-ahead electricity markets of Shanxi province and ISO New England reveal that such forecasts could potentially save residents millions of dollars a year compared to existing methods. Our findings underscore the value of suitably integrating time series modeling with economic prior for enhanced electricity price forecasting accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14893v2</guid>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Linian Wang, Jianghong Liu, Huibin Zhang, Leye Wang</dc:creator>
    </item>
  </channel>
</rss>
