<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 23 Jun 2025 04:00:40 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The Optimality of a Nested Generalized Pairwise Group Testing Procedure</title>
      <link>https://arxiv.org/abs/2506.15797</link>
      <description>arXiv:2506.15797v1 Announce Type: new 
Abstract: We study the problem of identifying defective units in a finite population of \( n \) units, where each unit \( i \) is independently defective with known probability \( p_i \). This setting is referred to as the \emph{Generalized Group Testing Problem}. A testing procedure is called optimal if it minimizes the expected number of tests. It has been conjectured that, when all probabilities \( p_i \) lie within the interval \( \left[1 - \frac{1}{\sqrt{2}},\, \frac{3 - \sqrt{5}}{2} \right] \), the \emph{generalized pairwise testing {algorithm}}, applied to the \( p_i \) arranged in nondecreasing order, constitutes the optimal nested testing strategy among all such order-preserving nested strategies. In this work, we confirm this conjecture and establish the optimality of the procedure within the specified regime. Additionally, we provide a complete structural characterization of the procedure and derive a closed-form expression for its expected number of tests. These results offer new insights into the theory of optimal nested strategies in generalized group testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15797v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yaakov Malinovsky, Viktor Skorniakov</dc:creator>
    </item>
    <item>
      <title>The exact region and an inequality between Chatterjee's and Spearman's rank correlations</title>
      <link>https://arxiv.org/abs/2506.15897</link>
      <description>arXiv:2506.15897v1 Announce Type: new 
Abstract: The rank correlation xi(X,Y), recently introduced by Chatterjee [12] and already popular in the statistics literature, takes values in [0,1], where 0 characterizes independence of X and Y, and 1 characterizes perfect dependence of Y on X. Unlike classical concordance measures such as Spearman's rho, which capture the degree of positive or negative dependence, xi quantifies the strength of functional dependence. In this paper, we determine, for each fixed value of xi, the full range of possible values of rho. The resulting xi-rho-region is a convex set whose boundary is characterized by a novel class of absolutely continuous, asymmetric copulas that are piecewise linear in the conditioning variable. Moreover, we prove that xi(X,Y) &lt;= |rho(X,Y)| whenever Y is stochastically increasing or decreasing in X, and we identify the maximal difference rho(X,Y) - xi(X,Y) as exactly 0.4. Our proofs rely on a convex optimization problem under various equality and inequality constraints, as well as on a rearrangement-based dependence order underlying xi. Our results contribute to a better understanding of Chatterjee's rank correlation which typically yields substantially smaller values than Spearman's rho when quantifying the same positive dependence structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15897v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan Ansari, Marcus Rockel</dc:creator>
    </item>
    <item>
      <title>Estimating Multiple Weighted Networks with Node-Sparse Differences and Shared Low-Rank Structure</title>
      <link>https://arxiv.org/abs/2506.15915</link>
      <description>arXiv:2506.15915v1 Announce Type: new 
Abstract: We study the problem of modeling multiple symmetric, weighted networks defined on a common set of nodes, where networks arise from different groups or conditions. We propose a model in which each network is expressed as the sum of a shared low-rank structure and a node-sparse matrix that captures the differences between conditions. This formulation is motivated by practical scenarios, such as in connectomics, where most nodes share a global connectivity structure while only a few exhibit condition-specific deviations. We develop a multi-stage estimation procedure that combines a spectral initialization step, semidefinite programming for support recovery, and a debiased refinement step for low-rank estimation. We establish minimax-optimal guarantees for recovering the shared low-rank component under the row-wise $\ell_{2,\infty}$ norm and elementwise $\ell_{\infty}$ norm, as well as for detecting node-level perturbations under various signal-to-noise regimes. We demonstrate that the availability of multiple networks can significantly enhance estimation accuracy compared to single-network settings. Additionally, we show that commonly-used methods such as group Lasso may provably fail to recover the sparse structure in this setting, a result which might be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15915v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hao Yan, Keith Levin</dc:creator>
    </item>
    <item>
      <title>Conditional Dirichlet Processes and Functional Condition Models</title>
      <link>https://arxiv.org/abs/2506.15932</link>
      <description>arXiv:2506.15932v1 Announce Type: new 
Abstract: In this paper, we study the conditional Dirichlet process (cDP) when a functional of a random distribution is specified. Specifically, we apply the cDP to the functional condition model, a nonparametric model in which a finite-dimensional parameter of interest is defined as the solution to a functional equation of the distribution. We derive both the posterior distribution of the parameter of interest and the posterior distribution of the underlying distribution itself. We establish two general limiting theorems for the posterior: one as the total mass of the Dirichlet process parameter tends to zero, and another as the sample size tends to infinity. We consider two specific models, the quantile model and the moment model, and propose algorithms for posterior computation, accompanied by illustrative data analysis examples. As a byproduct, we show that the Jeffreys substitute likelihood emerges as the limit of the marginal posterior in the functional condition model with a cDP prior, thereby providing a theoretical justification that has so far been lacking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15932v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jaeyong Lee, Kwangmin Lee, Jaegui Lee, Seongil Jo</dc:creator>
    </item>
    <item>
      <title>Regularized Learning for Fractional Brownian Motion via Path Signatures</title>
      <link>https://arxiv.org/abs/2506.16156</link>
      <description>arXiv:2506.16156v1 Announce Type: new 
Abstract: Fractional Brownian motion (fBm) extends classical Brownian motion by introducing dependence between increments, governed by the Hurst parameter $H\in (0,1)$. Unlike traditional Brownian motion, the increments of an fBm are not independent. Paths generated by fractional Brownian motions can exhibit significant irregularity, particularly when the Hurst parameter is small. As a result, classical regression methods may not perform effectively. Signatures, defined as iterated path integrals of continuous and discrete-time processes, offer a universal nonlinearity property that simplifies the challenge of feature selection in time series data analysis by effectively linearizing it. Consequently, we employ Lasso regression techniques for regularization when handling irregular data. To evaluate the performance of signature Lasso on fractional Brownian motion (fBM), we study its consistency when the Hurst parameter $ H \ne \frac{1}{2} $. This involves deriving bounds on the first and second moments of the signature. For the case $ H &gt; \frac{1}{2} $, we use the signature defined in the Young sense, while for $ H &lt; \frac{1}{2} $, we use the Stratonovich interpretation. Simulation results indicate that signature Lasso can outperform traditional regression methods for synthetic data as well as for real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16156v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ali Mohaddes, Francesco Iafrate, Johannes Lederer</dc:creator>
    </item>
    <item>
      <title>Multi-Armed Bandits With Machine Learning-Generated Surrogate Rewards</title>
      <link>https://arxiv.org/abs/2506.16658</link>
      <description>arXiv:2506.16658v1 Announce Type: new 
Abstract: Multi-armed bandit (MAB) is a widely adopted framework for sequential decision-making under uncertainty. Traditional bandit algorithms rely solely on online data, which tends to be scarce as it must be gathered during the online phase when the arms are actively pulled. However, in many practical settings, rich auxiliary data, such as covariates of past users, is available prior to deploying any arms. We introduce a new setting for MAB where pre-trained machine learning (ML) models are applied to convert side information and historical data into \emph{surrogate rewards}. A prominent feature of this setting is that the surrogate rewards may exhibit substantial bias, as true reward data is typically unavailable in the offline phase, forcing ML predictions to heavily rely on extrapolation. To address the issue, we propose the Machine Learning-Assisted Upper Confidence Bound (MLA-UCB) algorithm, which can be applied to any reward prediction model and any form of auxiliary data. When the predicted and true rewards are jointly Gaussian, it provably improves the cumulative regret, provided that the correlation is non-zero -- even in cases where the mean surrogate reward completely misaligns with the true mean rewards. Notably, our method requires no prior knowledge of the covariance matrix between true and surrogate rewards. We compare MLA-UCB with the standard UCB on a range of numerical studies and show a sizable efficiency gain even when the size of the offline data and the correlation between predicted and true rewards are moderate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16658v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenlong Ji, Yihan Pan, Ruihao Zhu, Lihua Lei</dc:creator>
    </item>
    <item>
      <title>Inference for SDEs driven by Hermite processes</title>
      <link>https://arxiv.org/abs/2506.16916</link>
      <description>arXiv:2506.16916v1 Announce Type: new 
Abstract: In the paper, we address parametric and non-parametric estimation for nonlinear stochastic differential equations with additive Hermite noise with possibly nonlinear scaling. We assume that a single trajectory of the solution is observed discretely and we propose estimators of the Hurst parameter and the Hermite order of the driving process as well as of the average noise intensity and noise intensity function. The estimators are based on the weighted quadratic variation whose properties are used, in particular, to prove weak consistency of the proposed estimators under in-fill asymptotics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16916v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Petr Coupek, Pavel Kriz</dc:creator>
    </item>
    <item>
      <title>Large Average Subtensor Problem: Ground-State, Algorithms, and Algorithmic Barriers</title>
      <link>https://arxiv.org/abs/2506.17118</link>
      <description>arXiv:2506.17118v1 Announce Type: new 
Abstract: We introduce the large average subtensor problem: given an order-$p$ tensor over $\mathbb{R}^{N\times \cdots \times N}$ with i.i.d. standard normal entries and a $k\in\mathbb{N}$, algorithmically find a $k\times \cdots \times k$ subtensor with a large average entry. This generalizes the large average submatrix problem, a key model closely related to biclustering and high-dimensional data analysis, to tensors. For the submatrix case, Bhamidi, Dey, and Nobel~\cite{bhamidi2017energy} explicitly highlight the regime $k=\Theta(N)$ as an intriguing open question.
  Addressing the regime $k=\Theta(N)$ for tensors, we establish that the largest average entry concentrates around an explicit value $E_{\mathrm{max}}$, provided that the tensor order $p$ is sufficiently large. Furthermore, we prove that for any $\gamma&gt;0$ and large $p$, this model exhibits multi Overlap Gap Property ($m$-OGP) above the threshold $\gamma E_{\mathrm{max}}$. The $m$-OGP serves as a rigorous barrier for a broad class of algorithms exhibiting input stability. These results hold for both $k=\Theta(N)$ and $k=o(N)$. Moreover, for small $k$, specifically $k=o(\log^{1.5}N)$, we show that a certain polynomial-time algorithm identifies a subtensor with average entry $\frac{2\sqrt{p}}{p+1}E_{\mathrm{max}}$. In particular, the $m$-OGP is asymptotically sharp: onset of the $m$-OGP and the algorithmic threshold match as $p$ grows.
  Our results show that while the case $k=\Theta(N)$ remains open for submatrices, it can be rigorously analyzed for tensors in the large $p$ regime. This is achieved by interpreting the model as a Boolean spin glass and drawing on insights from recent advances in the Ising $p$-spin glass model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17118v1</guid>
      <category>math.ST</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abhishek Hegade K. R., Eren C. K{\i}z{\i}lda\u{g}</dc:creator>
    </item>
    <item>
      <title>Data analysis using discrete cubical homology</title>
      <link>https://arxiv.org/abs/2506.15020</link>
      <description>arXiv:2506.15020v1 Announce Type: cross 
Abstract: We present a new tool for data analysis: persistence discrete homology, which is well-suited to analyze filtrations of graphs. In particular, we provide a novel way of representing high-dimensional data as a filtration of graphs using pairwise correlations. We discuss several applications of these tools, e.g., in weather and financial data, comparing them to the standard methods used in the respective fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15020v1</guid>
      <category>math.AT</category>
      <category>cs.LG</category>
      <category>math.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chris Kapulkin, Nathan Kershaw</dc:creator>
    </item>
    <item>
      <title>Sampling conditioned diffusions via Pathspace Projected Monte Carlo</title>
      <link>https://arxiv.org/abs/2506.15743</link>
      <description>arXiv:2506.15743v1 Announce Type: cross 
Abstract: We present an algorithm to sample stochastic differential equations conditioned on rather general constraints, including integral constraints, endpoint constraints, and stochastic integral constraints. The algorithm is a pathspace Metropolis-adjusted manifold sampling scheme, which samples stochastic paths on the submanifold of realizations that adhere to the conditioning constraint. We demonstrate the effectiveness of the algorithm by sampling a dynamical condensation phase transition, conditioning a random walk on a fixed Levy stochastic area, conditioning a stochastic nonlinear wave equation on high amplitude waves, and sampling a stochastic partial differential equation model of turbulent pipe flow conditioned on relaminarization events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15743v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tobias Grafke</dc:creator>
    </item>
    <item>
      <title>A Fast Iterative Robust Principal Component Analysis Method</title>
      <link>https://arxiv.org/abs/2506.16013</link>
      <description>arXiv:2506.16013v1 Announce Type: cross 
Abstract: Principal Component Analysis (PCA) is widely used for dimensionality reduction and data analysis. However, PCA results are adversely affected by outliers often observed in real-world data. Existing robust PCA methods are often computationally expensive or exhibit limited robustness. In this work, we introduce a Fast Iterative Robust (FIR) PCA method by efficiently estimating the inliers center location and covariance. Our approach leverages Incremental PCA (IPCA) to iteratively construct a subset of data points that ensures improved location and covariance estimation that effectively mitigates the influence of outliers on PCA projection. We demonstrate that our method achieves competitive accuracy and performance compared to existing robust location and covariance methods while offering improved robustness to outlier contamination. We utilize simulated and real-world datasets to evaluate and demonstrate the efficacy of our approach in identifying and preserving underlying data structures in the presence of contamination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16013v1</guid>
      <category>cs.CE</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Timbwaoga Aime Judicael Ouermi, Jixian Li, Chris R. Johnson</dc:creator>
    </item>
    <item>
      <title>Leveraging Optimal Transport for Distributed Two-Sample Testing: An Integrated Transportation Distance-based Framework</title>
      <link>https://arxiv.org/abs/2506.16047</link>
      <description>arXiv:2506.16047v1 Announce Type: cross 
Abstract: This paper introduces a novel framework for distributed two-sample testing using the Integrated Transportation Distance (ITD), an extension of the Optimal Transport distance. The approach addresses the challenges of detecting distributional changes in decentralized learning or federated learning environments, where data privacy and heterogeneity are significant concerns. We provide theoretical foundations for the ITD, including convergence properties and asymptotic behavior. A permutation test procedure is proposed for practical implementation in distributed settings, allowing for efficient computation while preserving data privacy. The framework's performance is demonstrated through theoretical power analysis and extensive simulations, showing robust Type I error control and high power across various distributions and dimensions. The results indicate that ITD effectively aggregates information across distributed clients, detecting subtle distributional shifts that might be missed when examining individual clients. This work contributes to the growing field of distributed statistical inference, offering a powerful tool for two-sample testing in modern, decentralized data environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16047v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhengqi Lin, Yan Chen</dc:creator>
    </item>
    <item>
      <title>An introduction to Causal Modelling</title>
      <link>https://arxiv.org/abs/2506.16486</link>
      <description>arXiv:2506.16486v1 Announce Type: cross 
Abstract: This tutorial provides a concise introduction to modern causal modeling by integrating potential outcomes and graphical methods. We motivate causal questions such as counterfactual reasoning under interventions and define binary treatments and potential outcomes. We discuss causal effect measures-including average treatment effects on the treated and on the untreated-and choices of effect scales for binary outcomes. We derive identification in randomized experiments under exchangeability and consistency, and extend to stratification and blocking designs. We present inverse probability weighting with propensity score estimation and robust inference via sandwich estimators. Finally, we introduce causal graphs, d-separation, the backdoor criterion, single-world intervention graphs, and structural equation models, showing how graphical and potential-outcome approaches complement each other. Emphasis is placed on clear notation, intuitive explanations, and practical examples for applied researchers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16486v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gauranga Kumar Baishya</dc:creator>
    </item>
    <item>
      <title>Profile monitoring of random functions with Gaussian process basis expansions</title>
      <link>https://arxiv.org/abs/2506.17153</link>
      <description>arXiv:2506.17153v1 Announce Type: cross 
Abstract: We consider the problem of online profile monitoring of random functions that admit basis expansions possessing random coefficients for the purpose of out-of-control state detection. Our approach is applicable to a broad class of random functions which feature two sources of variation: additive error and random fluctuations through random coefficients in the basis representation of functions. We focus on a two-phase monitoring problem with a first stage consisting of learning the in-control process and the second stage leveraging the learned process for out-of-control state detection. The foundations of our method are derived under the assumption that the coefficients in the basis expansion are Gaussian random variables, which facilitates the development of scalable and effective monitoring methodology for the observed processes that makes weak functional assumptions on the underlying process. We demonstrate the potential of our method through simulation studies that highlight some of the nuances that emerge in profile monitoring problems with random functions, and through an application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17153v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takayuki Iguchi, Jonathan R. Stewart, Eric Chicken</dc:creator>
    </item>
    <item>
      <title>Scaling limits for sample autocovariance operators of Hilbert space-valued linear processes</title>
      <link>https://arxiv.org/abs/2506.17168</link>
      <description>arXiv:2506.17168v1 Announce Type: cross 
Abstract: This article considers linear processes with values in a separable Hilbert space exhibiting long-range dependence. The scaling limits for the sample autocovariance operators at different time lags are investigated in the topology of their respective Hilbert spaces. Distinguishing two different regimes of long-range dependence, the limiting object is either a Hilbert space-valued Gaussian or a Hilbert space-valued non-Gaussian random variable. The latter can be represented as a unitary transformation of double Wiener-It\^o integrals with sample paths in a function space. This work is the first to show weak convergence to such double stochastic integrals with sample paths in infinite dimensions. The result generalizes the well known convergence to a Hermite process in finite dimensions, introducing a new domain of attraction for probability measures in Hilbert spaces. The key technical contributions include the introduction of double Wiener-It\^o integrals with values in a function space and with dependent integrators, as well as establishing sufficient conditions for their existence as limits of sample autocovariance operators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17168v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marie-Christine D\"uker, Pavlos Zoubouloglou</dc:creator>
    </item>
    <item>
      <title>Reversible Markov chains: variational representations and ordering</title>
      <link>https://arxiv.org/abs/1809.01903</link>
      <description>arXiv:1809.01903v2 Announce Type: replace 
Abstract: This pedagogical document explains three variational representations that are useful when comparing the efficiencies of reversible Markov chains: (i) the Dirichlet form and the associated variational representations of the spectral gaps; (ii) a variational representation of the asymptotic variance of an ergodic average; and (iii) the conductance, and the equivalence of a non-zero conductance to a non-zero right spectral gap.</description>
      <guid isPermaLink="false">oai:arXiv.org:1809.01903v2</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chris Sherlock</dc:creator>
    </item>
    <item>
      <title>Dimension free ridge regression</title>
      <link>https://arxiv.org/abs/2210.08571</link>
      <description>arXiv:2210.08571v3 Announce Type: replace 
Abstract: Random matrix theory has become a widely useful tool in high-dimensional statistics and theoretical machine learning. However, random matrix theory is largely focused on the proportional asymptotics in which the number of columns grows proportionally to the number of rows of the data matrix. This is not always the most natural setting in statistics where columns correspond to covariates and rows to samples. With the objective to move beyond the proportional asymptotics, we revisit ridge regression ($\ell_2$-penalized least squares) on i.i.d. data $(x_i, y_i)$, $i\le n$, where $x_i$ is a feature vector and $y_i = \beta^\top x_i +\epsilon_i \in\mathbb{R}$ is a response. We allow the feature vector to be high-dimensional, or even infinite-dimensional, in which case it belongs to a separable Hilbert space, and assume either $z_i := \Sigma^{-1/2}x_i$ to have i.i.d. entries, or to satisfy a certain convex concentration property. Within this setting, we establish non-asymptotic bounds that approximate the bias and variance of ridge regression in terms of the bias and variance of an `equivalent' sequence model (a regression model with diagonal design matrix). The approximation is up to multiplicative factors bounded by $(1\pm \Delta)$ for some explicitly small $\Delta$. Previously, such an approximation result was known only in the proportional regime and only up to additive errors: in particular, it did not allow to characterize the behavior of the excess risk when this converges to $0$. Our general theory recovers earlier results in the proportional regime (with better error rates). As a new application, we obtain a completely explicit and sharp characterization of ridge regression for Hilbert covariates with regularly varying spectrum. Finally, we analyze the overparametrized near-interpolation setting and obtain sharp `benign overfitting' guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.08571v3</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>The Annals of Statistics 52.6 (2024): 2879-2912</arxiv:journal_reference>
      <dc:creator>Chen Cheng, Andrea Montanari</dc:creator>
    </item>
    <item>
      <title>Measuring Evidence against Exchangeability and Group Invariance with E-values</title>
      <link>https://arxiv.org/abs/2310.01153</link>
      <description>arXiv:2310.01153v4 Announce Type: replace 
Abstract: We study e-values for quantifying evidence against exchangeability and general invariance of a random variable under a compact group. We start by characterizing such e-values, and explaining how they nest traditional group invariance tests as a special case. We show they can be easily designed for an arbitrary test statistic, and computed through Monte Carlo sampling. We prove a result that characterizes optimal e-values for group invariance against optimality targets that satisfy a mild orbit-wise decomposition property. We apply this to design expected-utility-optimal e-values for group invariance, which include both Neyman-Pearson-optimal tests and log-optimal e-values. Moreover, we generalize the notion of rank- and sign-based testing to compact groups, by using a representative inversion kernel. In addition, we characterize e-processes for group invariance for arbitrary filtrations, and provide tools to construct them. We also describe test martingales under a natural filtration, which are simpler to construct. Peeking beyond compact groups, we encounter e-values and e-processes based on ergodic theorems. These nest e-processes based on de Finetti's theorem for testing exchangeability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.01153v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nick W. Koning</dc:creator>
    </item>
    <item>
      <title>Statistical properties of matrix decomposition factor analysis</title>
      <link>https://arxiv.org/abs/2403.06968</link>
      <description>arXiv:2403.06968v5 Announce Type: replace 
Abstract: Numerous estimators have been proposed for factor analysis, and their statistical properties have been extensively studied. In the early 2000s, a novel matrix factorization-based approach, known as Matrix Decomposition Factor Analysis (MDFA), was introduced and has been actively developed in computational statistics. The MDFA estimator offers several advantages, including the guarantee of proper solutions (i.e., no Heywood cases) and the extensibility to $\ell_0$-sparse estimation. However, the MDFA estimator does not appear to be formulated as a classical M-estimator or a minimum discrepancy function (MDF) estimator, and the statistical properties of the MDFA estimator have remained largely unexplored. Although the MDFA estimator minimizes a loss function resembling that of principal component analysis (PCA), it empirically behaves more like consistent estimators used in factor analysis than like PCA itself. This raises a fundamental question: can matrix decomposition factor analysis truly be regarded as "factor analysis"? To address this issue, we establish consistency and asymptotic normality of the MDFA estimator. Recognizing that the MDFA estimator can be formulated as a semiparametric maximum likelihood estimator, we surprisingly find that the profile likelihood is given by the squared Bures-Wasserstein distance between the sample covariance matrix and the modeled covariance matrix. As a consequence, the MDFA estimator is ultimately an MDF estimator for factor analysis. Beyond MDFA, the same representation holds for a broad class of component analysis methods, including PCA, thereby offering a unified perspective on component analysis. Numerical experiments demonstrate that MDFA performs competitively with other established estimators, suggesting that it is a theoretically grounded and computationally appealing alternative for factor analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06968v5</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yoshikazu Terada</dc:creator>
    </item>
    <item>
      <title>Minimax rates of convergence for the nonparametric estimation of the diffusion coefficient from time-homogeneous SDE paths</title>
      <link>https://arxiv.org/abs/2501.15933</link>
      <description>arXiv:2501.15933v3 Announce Type: replace 
Abstract: Consider a diffusion process X, solution of a time-homogeneous stochastic differential equation. We assume that the diffusion process X is observed at discrete times, at high frequency, which means that the time step tends toward zero. In addition, the drift and diffusion coefficients of the process X are assumed to be unknown. In this paper, we study the minimax rates of convergence of the nonparametric estimators of the square of the diffusion coefficient. Two observation schemes are considered depending on the estimation interval. The square of the diffusion coefficient is estimated on the real line from repeated observations of the process X, where the number of diffusion paths tends to infinity. For the case of a compact estimation interval, we study the nonparametric estimation of the square of the diffusion coefficient constructed from a single diffusion path on one side and from repeated observations on the other side, where the number of trajectories tends to infinity. In each of these cases, we establish minimax convergence rates of the risk of estimation of the diffusion coefficient over a space of Holder functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15933v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eddy Michel Ella Mintsa</dc:creator>
    </item>
    <item>
      <title>Central limit theorems under non-stationarity via relative weak convergence</title>
      <link>https://arxiv.org/abs/2505.02197</link>
      <description>arXiv:2505.02197v2 Announce Type: replace 
Abstract: Statistical inference for non-stationary data is hindered by the failure of classical central limit theorems (CLTs), not least because there is no fixed Gaussian limit to converge to. To resolve this, we introduce relative weak convergence, an extension of weak convergence that compares a statistic or process to a sequence of evolving processes. Relative weak convergence retains the essential consequences of classical weak convergence and coincides with it under stationarity. Crucially, it applies in general non-stationary settings where classical weak convergence fails. We establish concrete relative CLTs for random vectors and empirical processes, along with sequential, weighted, and bootstrap variants, that parallel the state-of-the-art in stationary settings. Our framework and results offer simple, plug-in replacements for classical CLTs whenever stationarity is untenable, as illustrated by applications in nonparametric trend estimation and hypothesis testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02197v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolai Palm, Thomas Nagler</dc:creator>
    </item>
    <item>
      <title>A new measure of dependence: Integrated $R^2$</title>
      <link>https://arxiv.org/abs/2505.18146</link>
      <description>arXiv:2505.18146v2 Announce Type: replace 
Abstract: We propose a new measure of dependence that quantifies the degree to which a random variable $Y$ depends on a random vector $X$. This measure is zero if and only if $Y$ and $X$ are independent, and equals one if and only if $Y$ is a measurable function of $X$. We introduce a simple and interpretable estimator that is comparable in ease of computation to classical correlation coefficients such as Pearson's, Spearman's, or Chatterjee's. Building on this coefficient, we develop a model-free variable selection algorithm, feature ordering by dependence (FORD), inspired by FOCI. FORD requires no tuning parameters and is provably consistent under suitable sparsity assumptions. We demonstrate its effectiveness and improvements over FOCI through experiments on both synthetic and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18146v2</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mona Azadkia, Pouya Roudaki</dc:creator>
    </item>
    <item>
      <title>Statistical microlocal analysis in two-dimensional X-ray CT</title>
      <link>https://arxiv.org/abs/2506.05113</link>
      <description>arXiv:2506.05113v3 Announce Type: replace 
Abstract: In many imaging applications it is important to assess how well the edges of the original object, $f$, are resolved in an image, $f^\text{rec}$, reconstructed from the measured data, $g$. In this paper we consider the case of image reconstruction in 2D X-ray Computed Tomography (CT). Let $f$ be a function describing the object being scanned, and $g=Rf + \eta$ be the Radon transform data in $\mathbb{R}^2$ corrupted by noise, $\eta$, and sampled with step size $\sim\epsilon$. Conventional microlocal analysis provides conditions for edge detectability based on the scanner geometry in the case of continuous, noiseless data (when $\eta = 0$), but does not account for noise and finite sampling step size. We develop a novel technique called Statistical Microlocal Analysis (SMA), which uses a statistical hypothesis testing framework to determine if an image edge (singularity) of $f$ is detectable from $f^\text{rec}$, and we quantify edge detectability using the statistical power of the test. Our approach is based on the theory we developed in previous work, which provides a characterization of $f^\text{rec}$ in local $O(\epsilon)$-size neighborhoods when $\eta \neq 0$. We derive a statistical test for the presence and direction of an edge microlocally given the magnitude of $\eta$ and data sampling step size. Using the properties of the null distribution of the test, we quantify the uncertainty of the edge magnitude and direction. We validate our theory using simulations, which show strong agreement between our predictions and experimental observations. Our work is not only of practical value, but of theoretical value as well. SMA is a natural extension of classical microlocal analysis theory which accounts for practical measurement imperfections, such as noise and finite step size, at the highest possible resolution compatible with the data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05113v3</guid>
      <category>math.ST</category>
      <category>math.FA</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anuj Abhishek, Alexander Katsevich, James W. Webber</dc:creator>
    </item>
    <item>
      <title>Karhunen-Lo\`eve expansion of Random Measures</title>
      <link>https://arxiv.org/abs/2203.14202</link>
      <description>arXiv:2203.14202v2 Announce Type: replace-cross 
Abstract: We present an orthogonal expansion for real, function-regulated, second-order random measures over $\mathbb{R}^{d}$ with measure covariance. Such a expansion, which can be seen as a Karhunen-Lo\`eve decomposition, consists in a series of deterministic real measures weighted by uncorrelated real random variables with the variances forming a convergent series. The convergence of the series is in a mean-square sense stochastically and against measurable bounded test functions (with compact support if the random measure is not finite) in the measure sense, which implies set-wise convergence. This is proven taking advantage of the extra requirement of having a covariance measure over $\mathbb{R}^{d}\times\mathbb{R}^{d}$ describing the covariance structure of the random measure, for which we also provide a series expansion. These results cover for instance the cases of Gaussian White Noise, Poisson and Cox point processes, and can be used to obtain expansions for trawl processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.14202v2</guid>
      <category>math.PR</category>
      <category>math.FA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Ricardo Carrizo Vergara</dc:creator>
    </item>
    <item>
      <title>A Bayesian approach to functional regression: theory and computation</title>
      <link>https://arxiv.org/abs/2312.14086</link>
      <description>arXiv:2312.14086v3 Announce Type: replace-cross 
Abstract: We propose a novel Bayesian methodology for inference in functional linear and logistic regression models based on the theory of reproducing kernel Hilbert spaces (RKHS's). We introduce general models that build upon the RKHS generated by the covariance function of the underlying stochastic process, and whose formulation includes as particular cases all finite-dimensional models based on linear combinations of marginals of the process, which can collectively be seen as a dense subspace made of simple approximations. By imposing a suitable prior distribution on this dense functional space we can perform data-driven inference via standard Bayes methodology, estimating the posterior distribution through reversible jump Markov chain Monte Carlo methods. In this context, our contribution is two-fold. First, we derive theoretical results that guarantee strong posterior consistency and contraction at an optimal rate under mild conditions. Second, we show that several prediction strategies stemming from our Bayesian procedure are competitive against other usual alternatives in both simulations and real data sets, including a Bayesian-motivated variable selection method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14086v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jos\'e R. Berrendero, Antonio Co\'in, Antonio Cuevas</dc:creator>
    </item>
    <item>
      <title>Belted and Ensembled Neural Network for Linear and Nonlinear Sufficient Dimension Reduction</title>
      <link>https://arxiv.org/abs/2412.08961</link>
      <description>arXiv:2412.08961v2 Announce Type: replace-cross 
Abstract: We introduce a unified, flexible, and easy-to-implement framework of sufficient dimension reduction that can accommodate both linear and nonlinear dimension reduction, and both the conditional distribution and the conditional mean as the targets of estimation. This unified framework is achieved by a specially structured neural network -- the Belted and Ensembled Neural Network (BENN) -- that consists of a narrow latent layer, which we call the belt, and a family of transformations of the response, which we call the ensemble. By strategically placing the belt at different layers of the neural network, we can achieve linear or nonlinear sufficient dimension reduction, and by choosing the appropriate transformation families, we can achieve dimension reduction for the conditional distribution or the conditional mean. Moreover, thanks to the advantage of the neural network, the method is very fast to compute, overcoming a computation bottleneck of the traditional sufficient dimension reduction estimators, which involves the inversion of a matrix of dimension either p or n. We develop the algorithm and convergence rate of our method, compare it with existing sufficient dimension reduction methods, and apply it to two data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08961v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yin Tang, Bing Li</dc:creator>
    </item>
    <item>
      <title>Fast data inversion for high-dimensional dynamical systems from noisy measurements</title>
      <link>https://arxiv.org/abs/2501.01324</link>
      <description>arXiv:2501.01324v3 Announce Type: replace-cross 
Abstract: In this work, we develop a scalable approach for a flexible latent factor model for high-dimensional dynamical systems. Each latent factor process has its own correlation and variance parameters, and the orthogonal factor loading matrix can be either fixed or estimated. We utilize an orthogonal factor loading matrix that avoids computing the inversion of the posterior covariance matrix at each time of the Kalman filter, and derive closed-form expressions in an expectation-maximization algorithm for parameter estimation, which substantially reduces the computational complexity without approximation. Our study is motivated by inversely estimating slow slip events from geodetic data, such as continuous GPS measurements. Extensive simulated studies illustrate higher accuracy and scalability of our approach compared to alternatives. By applying our method to geodetic measurements in the Cascadia region, our estimated slip better agrees with independently measured seismic data of tremor events. The substantial acceleration from our method enables the use of massive noisy data for geological hazard quantification and other applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01324v3</guid>
      <category>stat.CO</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yizi Lin, Xubo Liu, Paul Segall, Mengyang Gu</dc:creator>
    </item>
    <item>
      <title>Statistical Inference under Performativity</title>
      <link>https://arxiv.org/abs/2505.18493</link>
      <description>arXiv:2505.18493v2 Announce Type: replace-cross 
Abstract: Performativity of predictions refers to the phenomena that prediction-informed decisions may influence the target they aim to predict, which is widely observed in policy-making in social sciences and economics. In this paper, we initiate the study of statistical inference under performativity. Our contribution is two-fold. First, we build a central limit theorem for estimation and inference under performativity, which enables inferential purposes in policy-making such as constructing confidence intervals or testing hypotheses. Second, we further leverage the derived central limit theorem to investigate prediction-powered inference (PPI) under performativity, which is based on a small labeled dataset and a much larger dataset of machine-learning predictions. This enables us to obtain more precise estimation and improved confidence regions for the model parameter (i.e., policy) of interest in performative prediction. We demonstrate the power of our framework by numerical experiments. To the best of our knowledge, this paper is the first one to establish statistical inference under performativity, which brings up new challenges and inference settings that we believe will add significant values to policy-making, statistics, and machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18493v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiang Li, Yunai Li, Huiying Zhong, Lihua Lei, Zhun Deng</dc:creator>
    </item>
  </channel>
</rss>
