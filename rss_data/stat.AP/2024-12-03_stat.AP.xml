<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 03 Dec 2024 05:01:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The ecological forecast horizon revisited: Potential, actual and relative system predictability</title>
      <link>https://arxiv.org/abs/2412.00753</link>
      <description>arXiv:2412.00753v1 Announce Type: new 
Abstract: Ecological forecasts are model-based statements about currently unknown ecosystem states in time or space. For a model forecast to be useful to inform decision-makers, model validation and verification determine adequateness. The measure of forecast goodness that can be translated into a limit up to which a forecast is acceptable is known as the `forecast horizon'. While verification of meteorological models follows strict criteria with established metrics and forecast horizons, assessments of ecological forecasting models still remain experiment-specific and forecast horizons are rarely reported. As such, users of ecological forecasts remain uninformed of how far into the future statements can be trusted. In this work, we synthesise existing approaches, define empirical forecast horizons in a unified framework for assessing ecological predictability and offer recipes on their computation. We distinguish upper and lower boundary estimates of predictability limits, reflecting the model's potential and actual forecast horizon, and show how a benchmark model can help determine its relative forecast horizon. The approaches are demonstrated with four case studies from population, ecosystem, and earth system research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00753v1</guid>
      <category>stat.AP</category>
      <category>physics.data-an</category>
      <category>q-bio.PE</category>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marieke Wesselkamp, Jakob Albrecht, Ewan Pinnington, William J. Castillo, Florian Pappenberger, Carsten F. Dormann</dc:creator>
    </item>
    <item>
      <title>A Bayesian Model of Underreporting for Sexual Assault on College Campuses</title>
      <link>https://arxiv.org/abs/2412.00823</link>
      <description>arXiv:2412.00823v1 Announce Type: new 
Abstract: In an effort to quantify and combat sexual assault, US colleges and universities are required to disclose the number of reported sexual assaults on their campuses each year. However, many instances of sexual assault are never reported to authorities, and consequently the number of reported assaults does not fully reflect the true total number of assaults that occurred; the reported values could arise from many combinations of reporting rate and true incidence. In this paper we estimate these underlying quantities via a hierarchical Bayesian model of the reported number of assaults. We use informative priors, based on national crime statistics, to act as a tiebreaker to help distinguish between reporting rates and incidence. We outline a Hamiltonian Monte Carlo (HMC) sampling scheme for posterior inference regarding reporting rates and assault incidence at each school, and apply this method to campus sexual assault data from 2014-2019. Results suggest an increasing trend in reporting rates for the overall college population during this time. However, the extent of underreporting varies widely across schools. That variation has implications for how individual schools should interpret their reported crime statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00823v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>The Annals of Applied Statistics 18.4 (2024): 3146-3164</arxiv:journal_reference>
      <dc:creator>Casey Bradshaw, David M. Blei</dc:creator>
    </item>
    <item>
      <title>A Bayesian Hierarchical Framework for Capturing Preference Heterogeneity in Migration Flows</title>
      <link>https://arxiv.org/abs/2412.01242</link>
      <description>arXiv:2412.01242v1 Announce Type: new 
Abstract: Understanding and predicting human migration patterns is a central challenge in population dynamics research. Traditional physics-inspired gravity and radiation models represent migration flows as functions of attractiveness using socio-economic features as proxies. They assume that the relationship between features and migration is spatially invariant, regardless of the origin and destination locations of migrants. We use Bayesian hierarchical models to demonstrate that migrant preferences likely vary based on geographical context, specifically the origin-destination pair. By applying these models to U.S. interstate migration data, we show that incorporating heterogeneity in a single latent migration parameter significantly improves the ability to explain variations in migrant flows. Accounting for such heterogeneity enables it to outperform classical methods and recent machine-learning approaches. A clustering analysis of spatially varying parameters reveals two distinct groups of migration paths. Individuals migrating along low-flow paths (typically between smaller populations or over larger distances) exhibit more nuanced decision-making. Their choices are less directly influenced by specific destination characteristics such as housing costs, land area, and climate-related disaster costs. High-flow path migrants appear to respond more directly to these destination attributes. Our results challenge assumptions of uniform preferences and underscore the value of capturing heterogeneity in migration models and policymaking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01242v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aric Cutuli, Upmanu Lall, Michael J. Puma, \'Emile Esmaili, Rachata Muneepeerakul</dc:creator>
    </item>
    <item>
      <title>Navigating Challenges in Spatio-temporal Modelling of Antarctic Krill Abundance: Addressing Zero-inflated Data and Misaligned Covariates</title>
      <link>https://arxiv.org/abs/2412.01399</link>
      <description>arXiv:2412.01399v1 Announce Type: new 
Abstract: Antarctic krill (Euphausia superba) are among the most abundant species on our planet and serve as a vital food source for many marine predators in the Southern Ocean. In this paper, we utilise statistical spatio-temporal methods to combine data from various sources and resolutions, aiming to accurately model krill abundance. Our focus lies in fitting the model to a dataset comprising acoustic measurements of krill biomass. To achieve this, we integrate climate covariates obtained from satellite imagery and from drifting surface buoys (also known as drifters). Additionally, we use sparsely collected krill biomass data obtained from net fishing efforts (KRILLBASE) for validation. However, integrating these multiple heterogeneous data sources presents significant modelling challenges, including spatio-temporal misalignment and inflated zeros in the observed data. To address these challenges, we fit a Hurdle-Gamma model to jointly describe the occurrence of zeros and the krill biomass for the non-zero observations, while also accounting for misaligned and heterogeneous data sources, including drifters. Therefore, our work presents a comprehensive framework for analysing and predicting krill abundance in the Southern Ocean, leveraging information from various sources and formats. This is crucial due to the impact of krill fishing, as understanding their distribution is essential for informed management decisions and fishing regulations aimed at protecting the species.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01399v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andr\'e Victor Ribeiro Amaral, Adam M. Sykulski, Emma Cavan, Sophie Fielding</dc:creator>
    </item>
    <item>
      <title>Visual Error Patterns in Multi-Modal AI: A Statistical Approach</title>
      <link>https://arxiv.org/abs/2412.00083</link>
      <description>arXiv:2412.00083v1 Announce Type: cross 
Abstract: Artificial Intelligence (AI) has achieved transformative success across a wide range of domains, revolutionizing fields such as healthcare, education, and human-computer interaction. However, the mechanisms driving AI's performance often remain opaque, particularly in the context of large language models (LLMs), which have advanced at an unprecedented pace in recent years. Multi-modal large language models (MLLMs) like GPT-4o exemplify this evolution, integrating text, audio, and visual inputs to enable interaction across diverse domains. Despite their remarkable capabilities, these models remain largely "black boxes," offering limited insight into how they process multi-modal information internally. This lack of transparency poses significant challenges, including systematic biases, flawed associations, and unintended behaviors, which require careful investigation. Understanding the decision-making processes of MLLMs is both beneficial and essential for mitigating these challenges and ensuring their reliable deployment in critical applications. GPT-4o was chosen as the focus of this study for its advanced multi-modal capabilities, which allow simultaneous processing of textual and visual information. These capabilities make it an ideal model for investigating the parallels and distinctions between machine-driven and human-driven visual perception. While GPT-4o performs effectively in tasks involving structured and complete data, its reliance on bottom-up processing, which involves a feature-by-feature analysis of sensory inputs, presents challenges when interpreting complex or ambiguous stimuli. This limitation contrasts with human vision, which is remarkably adept at resolving ambiguity and reconstructing incomplete information through high-level cognitive processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00083v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ching-Yi Wang</dc:creator>
    </item>
    <item>
      <title>How reproducible are data-driven subtypes of Alzheimer's disease atrophy?</title>
      <link>https://arxiv.org/abs/2412.00160</link>
      <description>arXiv:2412.00160v1 Announce Type: cross 
Abstract: Alzheimer's disease (AD) exhibits substantial clinical and biological heterogeneity, complicating efforts in treatment and intervention development. While new computational methods offer insights into AD progression, the reproducibility of these subtypes across datasets remains understudied, particularly concerning the robustness of subtype definitions when validated on diverse databases. This study evaluates the consistency of AD progression subtypes identified by the Subtype and Stage Inference (SuStaIn) algorithm using T1-weighted MRI data across 5,444 subjects from ANMerge, OASIS, and ADNI datasets, forming four independent cohorts. Each cohort was analyzed under two conditions: one using the full cohort, including cognitively normal controls, and another excluding controls to test subtype robustness. Results confirm the three primary atrophy subtypes identified in earlier studies: Typical, Cortical, and Subcortical, as well as the emergence of rare and atypical AD variants such as posterior cortical atrophy (PCA). Notably, each subtype displayed varying robustness to the inclusion of controls, with certain subtypes, like Subcortical, more influenced by cohort composition. This investigation underscores SuStaIn's reliability for defining stable AD subtypes and suggests its utility in clinical stratification for trials and diagnosis. However, our findings also highlight the need for improved dataset diversity, particularly in terms of ethnic representation, to enhance generalizability and support broader clinical application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00160v1</guid>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emma Prevot, Cameron Shand, Neil Oxtoby, for Alzheimer's Disease Neuroimaging Initiative</dc:creator>
    </item>
    <item>
      <title>Benchmarking covariates balancing methods, a simulation study</title>
      <link>https://arxiv.org/abs/2412.00280</link>
      <description>arXiv:2412.00280v1 Announce Type: cross 
Abstract: Causal inference in observational studies has advanced significantly since Rosenbaum and Rubin introduced propensity score methods. Inverse probability of treatment weighting (IPTW) is widely used to handle confounding bias. However, newer methods, such as energy balancing (EB), kernel optimal matching (KOM), and covariate balancing propensity score by tailored loss function (TLF), offer model-free or non-parametric alternatives. Despite these developments, guidance remains limited in selecting the most suitable method for treatment effect estimation in practical applications. This study compares IPTW with EB, KOM, and TLF, focusing on their ability to estimate treatment effects since this is the primary objective in many applications. Monte Carlo simulations are used to assess the ability of these balancing methods combined with different estimators to evaluate average treatment effect. We compare these methods across a range of scenarios varying sample size, level of confusion, and proportion of treated. In our simulation, we observe no significant advantages in using EB, KOM, or TLF methods over IPTW. Moreover, these recent methods make obtaining confidence intervals with nominal coverage difficult. We also compare the methods on the PROBITsim dataset and get results similar to those of our simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00280v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Etienne Peyrot, Rapha\"el Porcher, Francois Petit</dc:creator>
    </item>
    <item>
      <title>Disentangling The Effects of Air Pollution on Social Mobility: A Bayesian Principal Stratification Approach</title>
      <link>https://arxiv.org/abs/2412.00311</link>
      <description>arXiv:2412.00311v1 Announce Type: cross 
Abstract: Principal stratification provides a robust framework for causal inference, enabling the investigation of the causal link between air pollution exposure and social mobility, mediated by the education level. Studying the causal mechanisms through which air pollution affects social mobility is crucial to highlight the role of education as a mediator, and offering evidence that can inform policies aimed at reducing both environmental and educational inequalities for more equitable social outcomes. In this paper, we introduce a novel Bayesian nonparametric model for principal stratification, leveraging the dependent Dirichlet process to flexibly model the distribution of potential outcomes. By incorporating confounders and potential outcomes for the post-treatment variable in the Bayesian mixture model for the final outcome, our approach improves the accuracy of missing data imputation and allows for the characterization of treatment effects. We assess the performance of our method through a simulation study and demonstrate its application in evaluating the principal causal effects of air pollution on social mobility in the United States.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00311v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dafne Zorzetto, Paolo Dalla Torre, Sonia Petrone, Francesca Dominici, Falco J. Bargagli-Stoffi</dc:creator>
    </item>
    <item>
      <title>Prognostic Framework for Robotic Manipulators Operating Under Dynamic Task Severities</title>
      <link>https://arxiv.org/abs/2412.00538</link>
      <description>arXiv:2412.00538v1 Announce Type: cross 
Abstract: Robotic manipulators are critical in many applications but are known to degrade over time. This degradation is influenced by the nature of the tasks performed by the robot. Tasks with higher severity, such as handling heavy payloads, can accelerate the degradation process. One way this degradation is reflected is in the position accuracy of the robot's end-effector. In this paper, we present a prognostic modeling framework that predicts a robotic manipulator's Remaining Useful Life (RUL) while accounting for the effects of task severity. Our framework represents the robot's position accuracy as a Brownian motion process with a random drift parameter that is influenced by task severity. The dynamic nature of task severity is modeled using a continuous-time Markov chain (CTMC). To evaluate RUL, we discuss two approaches -- (1) a novel closed-form expression for Remaining Lifetime Distribution (RLD), and (2) Monte Carlo simulations, commonly used in prognostics literature. Theoretical results establish the equivalence between these RUL computation approaches. We validate our framework through experiments using two distinct physics-based simulators for planar and spatial robot fleets. Our findings show that robots in both fleets experience shorter RUL when handling a higher proportion of high-severity tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00538v1</guid>
      <category>cs.RO</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.AP</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ayush Mohanty, Jason Dekarske, Stephen K. Robinson, Sanjay Joshi, Nagi Gebraeel</dc:creator>
    </item>
    <item>
      <title>The Advancement of Personalized Learning Potentially Accelerated by Generative AI</title>
      <link>https://arxiv.org/abs/2412.00691</link>
      <description>arXiv:2412.00691v1 Announce Type: cross 
Abstract: The rapid development of Generative AI (GAI) has sparked revolutionary changes across various aspects of education. Personalized learning, a focal point and challenge in educational research, has also been influenced by the development of GAI. To explore GAI's extensive impact on personalized learning, this study investigates its potential to enhance various facets of personalized learning through a thorough analysis of existing research. The research comprehensively examines GAI's influence on personalized learning by analyzing its application across different methodologies and contexts, including learning strategies, paths, materials, environments, and specific analyses within the teaching and learning processes. Through this in-depth investigation, we find that GAI demonstrates exceptional capabilities in providing adaptive learning experiences tailored to individual preferences and needs. Utilizing different forms of GAI across various subjects yields superior learning outcomes. The article concludes by summarizing scenarios where GAI is applicable in educational processes and discussing strategies for leveraging GAI to enhance personalized learning, aiming to guide educators and learners in effectively utilizing GAI to achieve superior learning objectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00691v1</guid>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuang Wei, Yuan-Hao Jiang, Jiayi Liu, Changyong Qi, Rui Jia</dc:creator>
    </item>
    <item>
      <title>Stochastic Search Variable Selection for Bayesian Generalized Linear Mixed Effect Models</title>
      <link>https://arxiv.org/abs/2412.01084</link>
      <description>arXiv:2412.01084v1 Announce Type: cross 
Abstract: Variable selection remains a difficult problem, especially for generalized linear mixed models (GLMMs). While some frequentist approaches to simultaneously select joint fixed and random effects exist, primarily through the use of penalization, existing approaches for Bayesian GLMMs exist only for special cases, like that of logistic regression. In this work, we apply the Stochastic Search Variable Selection (SSVS) approach for the joint selection of fixed and random effects proposed in Yang et al. (2020) for linear mixed models to Bayesian GLMMs. We show that while computational issues remain, SSVS serves as a feasible and effective approach to jointly select fixed and random effects. We demonstrate the effectiveness of the proposed methodology to both simulated and real data. Furthermore, we study the role hyperparameters play in the model selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01084v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Feng Ding, Ian Laga</dc:creator>
    </item>
    <item>
      <title>Generative AI-based data augmentation for improved bioacoustic classification in noisy environments</title>
      <link>https://arxiv.org/abs/2412.01530</link>
      <description>arXiv:2412.01530v1 Announce Type: cross 
Abstract: 1. Obtaining data to train robust artificial intelligence (AI)-based models for species classification can be challenging, particularly for rare species. Data augmentation can boost classification accuracy by increasing the diversity of training data and is cheaper to obtain than expert-labelled data. However, many classic image-based augmentation techniques are not suitable for audio spectrograms. 2. We investigate two generative AI models as data augmentation tools to synthesise spectrograms and supplement audio data: Auxiliary Classifier Generative Adversarial Networks (ACGAN) and Denoising Diffusion Probabilistic Models (DDPMs). The latter performed particularly well in terms of both realism of generated spectrograms and accuracy in a resulting classification task. 3. Alongside these new approaches, we present a new audio data set of 640 hours of bird calls from wind farm sites in Ireland, approximately 800 samples of which have been labelled by experts. Wind farm data are particularly challenging for classification models given the background wind and turbine noise. 4. Training an ensemble of classification models on real and synthetic data combined gave 92.6% accuracy (and 90.5% with just the real data) when compared with highly confident BirdNET predictions. 5. Our approach can be used to augment acoustic signals for more species and other land-use types, and has the potential to bring about a step-change in our capacity to develop reliable AI-based detection of rare species. Our code is available at https://github.com/gibbona1/ SpectrogramGenAI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01530v1</guid>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <category>stat.AP</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anthony Gibbons, Emma King, Ian Donohue, Andrew Parnell</dc:creator>
    </item>
    <item>
      <title>Predicting Stock Price of Construction Companies using Weighted Ensemble Learning</title>
      <link>https://arxiv.org/abs/2311.06397</link>
      <description>arXiv:2311.06397v3 Announce Type: replace 
Abstract: Modeling the behavior of stock price data has always been one of the challengeous applications of Artificial Intelligence (AI) and Machine Learning (ML) due to its high complexity and dependence on various conditions. Recent studies show that this will be difficult to do with just one learning model. The problem can be more complex for companies of construction section, due to the dependency of their behavior on more conditions. This study aims to provide a hybrid model for improving the accuracy of prediction for stock price index of companies in construction section. The contribution of this paper can be considered as follows: First, a combination of several prediction models is used to predict stock price, so that learning models can cover each other's error. In this research, an ensemble model based on Artificial Neural Network (ANN), Gaussian Process Regression (GPR) and Classification and Regression Tree (CART) is presented for predicting stock price index. Second, the optimization technique is used to determine the effect of each learning model on the prediction result. For this purpose, first all three mentioned algorithms process the data simultaneously and perform the prediction operation. Then, using the Cuckoo Search (CS) algorithm, the output weight of each algorithm is determined as a coefficient. Finally, using the ensemble technique, these results are combined and the final output is generated through weighted averaging on optimal coefficients. The results showed that using CS optimization in the proposed ensemble system is highly effective in reducing prediction error. Comparing the evaluation results of the proposed system with similar algorithms, indicates that our model is more accurate and can be useful for predicting stock price index in real-world scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.06397v3</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyuan Song</dc:creator>
    </item>
    <item>
      <title>Career Modeling with Missing Data and Traces</title>
      <link>https://arxiv.org/abs/2311.15257</link>
      <description>arXiv:2311.15257v2 Announce Type: replace 
Abstract: Many social scientists study the career trajectories of populations of interest, such as economic and administrative elites. However, data to document such processes are rarely completely available, which motivates the adoption of inference tools that can account for large numbers of missing values. Taking the example of public-private paths of elite civil servants in France, we introduce binary Markov switching models to perform Bayesian data augmentation. Our procedure relies on two data sources: (1) detailed observations of a small number of individual trajectories, and (2) less informative ``traces'' left by all individuals, which we model for imputation of missing data. An advantage of this model class is that it maintains the properties of hidden Markov models and enables a tailored sampler to target the posterior, while allowing for varying parameters across individuals and time. We provide two applied studies which demonstrate this can be used to properly test substantive hypotheses, and expand the social scientific literature in various ways. We notably show that the rate at which ENA graduates exit the French public sector has not increased since 1990, but that the rate at which they come back has increased.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.15257v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Th\'eo Voldoire, Robin J. Ryder, Ryan Lahfa</dc:creator>
    </item>
    <item>
      <title>Properties and maximum likelihood estimation of the gamma-normal and related probability distributions</title>
      <link>https://arxiv.org/abs/2402.11088</link>
      <description>arXiv:2402.11088v3 Announce Type: replace 
Abstract: This paper presents likelihood-based inference methods for the family of univariate gamma-normal distributions GN({\alpha}, r, {\mu}, {\sigma}^2 ) that result from summing independent gamma({\alpha}, r) and N({\mu}, {\sigma}^2 ) random variables. First, the probability density function of a gamma-normal variable is provided in compact form with the use of parabolic cylinder functions, along with key properties. We then provide analytic expressions for the maximum-likelihood score equations and the Fisher information matrix, and discuss inferential methods for the gamma-normal distribution. Given the widespread use of the two constituting distributions, the gamma-normal distribution is a general purpose tool for a variety of applications. In particular, we discuss two distributions that are obtained as special cases and that are featured in a variety of statistical applications: the exponential-normal distribution and the chi-squared-normal (or overdispersed chi-squared) distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.11088v3</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Massimiliano Bonamente, Dale Zimmerman</dc:creator>
    </item>
    <item>
      <title>State-Space Modeling of Shape-constrained Functional Time Series</title>
      <link>https://arxiv.org/abs/2404.07586</link>
      <description>arXiv:2404.07586v2 Announce Type: replace 
Abstract: Functional time series data frequently appears in econometric analyses, where the functions of interest are subject to some shape constraints, including monotonicity and convexity, as typical of the estimation of the Lorenz curve. This paper proposes a state-space model for time-varying functions to extract trends and serial dependence from functional time series while imposing the shape constraints on the estimated functions. The function of interest is modeled by a convex combination of selected basis functions to satisfy the shape constraints, where the time-varying convex weights on simplex follow the dynamic multi-logit models. To enable posterior computation by an efficient Markov chain Monte Carlo method, a novel data augmentation technique is devised for the complicated likelihood of this model. The proposed method is applied to the estimation of time-varying Lorenz curves, and its utility is illustrated through numerical experiments and analysis of panel data of household incomes in Japan.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07586v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daichi Hiraki, Yasuyuki Hamura, Kaoru Irie, Shonosuke Sugasawa</dc:creator>
    </item>
    <item>
      <title>Development of a Statistical Predictive Model for Daily Water Table Depth and Important Variables Selection for Inference</title>
      <link>https://arxiv.org/abs/2410.01001</link>
      <description>arXiv:2410.01001v2 Announce Type: replace 
Abstract: Accurately predicting water table dynamics is vital for sustaining groundwater resources that support ecological functions and anthropogenic activities. This study evaluates a statistical model (BigVAR) that handles three major flexibilities: (a) prediction under a sparsity assumption in coefficients, (b) consideration of a time series autoregression framework, and (c) allowance for lags in both dependent and independent variables for estimating water table depth using daily hydroclimatic data from the USDA Forest Service Santee Experimental Forest (SC) and a site in NC. Data from 2006--2019 (SC) and 1988--2008 (NC) were used, with key predictors including soil and air temperature, precipitation, wind, and radiation. For WS80, RMSE during the dormant season was 10.09 cm, with a daily testing phase RMSE of 14.94 cm. The model achieved an R^2 of 0.93 for 2019 (a dry year) and 0.96 for 2016 (a wet year). Solar radiation, rainfall, and wind direction were among the most influential variables. This predictive model aids in managing wetland hydrology and supports decision-making for forest managers and hydrologists.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01001v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alokesh Manna, Sushant Mehan, Devendra M. Amatya</dc:creator>
    </item>
    <item>
      <title>Pedestrian crash causation analysis near bus stops: Insights from random parameters NB-Lindley models</title>
      <link>https://arxiv.org/abs/2410.22253</link>
      <description>arXiv:2410.22253v2 Announce Type: replace 
Abstract: Pedestrian activity near bus stops is typically high, especially in urban areas with bus transit corridors. These locations are critical nodes in the urban network, where frequent interactions between pedestrians and vehicles occur. Consequently, pedestrian safety near bus stops has become a significant concern due to increasing crashes. However, research often needs more comprehensive analysis, specifically examining exposure (e.g., vehicle, pedestrian volumes) characteristics, roadway environment, and bus stop design features contributing to pedestrian-vehicle crashes. This study contributes by exploring influencing factors related to these crashes by the development of statistical models. Key factors identified include bus stop design, traffic conditions, passenger activity, and the roadway environment near bus stops in Fort Worth, Texas. The analysis incorporates data from 596 bus stop locations with non and crash history and integrates crash records from 2018 to 2022. The result demonstrates that the Random Parameters Negative Binomial-Lindley (RPNB-L) model provides a detailed understanding of factors influencing crash frequency. Significant factors associated with increased crash frequency include average annual daily traffic (AADT), bus passenger boarding activity, and the absence of safety features such as medians, crosswalks, and lighting. Additional factors, including school numbers and lower speed limits (e.g., 35 mph or less), were also linked to positively associated higher crash rates. Conversely, signalized intersections, far-side bus stops, locations in mixed-use areas, and sidewalk availability were found to be negatively associated with crash frequency. The study further applies the full Bayes (FB) based Potential for Safety Improvement (PSI) metric to identify high-risk stops and hazardous corridors, helping prioritize safety interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22253v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad Anis, Srinivas R. Geedipally, Dominique Lord</dc:creator>
    </item>
    <item>
      <title>Targeting mediating mechanisms of social disparities with an interventional effects framework, applied to the gender pay gap in West Germany</title>
      <link>https://arxiv.org/abs/2411.07368</link>
      <description>arXiv:2411.07368v2 Announce Type: replace 
Abstract: The Oaxaca-Blinder decomposition is a widely used method to explain social disparities. However, assigning causal meaning to its estimated components requires strong assumptions that often lack explicit justification. This paper emphasizes the importance of clearly defined estimands and their identification when targeting mediating mechanisms of social disparities. Three approaches are distinguished based on their scientific questions and assumptions: a mediation approach and two interventional approaches. The Oaxaca-Blinder decomposition and Monte Carlo simulation-based g-computation are discussed for estimation in relation to these approaches. The latter method is used in an interventional effects analysis of the observed gender pay gap in West Germany, using data from the 2017 German Socio-Economic Panel. Ten mediators, including indicators of human capital and job characteristics, are considered. Key findings indicate that the gender pay gap in log hourly wages could be reduced by up to 86% if these mediators were evenly distributed between women and men. Substantial reductions could be achieved by aligning full-time employment and work experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07368v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christiane Didden</dc:creator>
    </item>
    <item>
      <title>Methods for generating and evaluating synthetic longitudinal patient data: a systematic review</title>
      <link>https://arxiv.org/abs/2309.12380</link>
      <description>arXiv:2309.12380v3 Announce Type: replace-cross 
Abstract: The rapid growth in data availability has facilitated research and development, yet not all industries have benefited equally due to legal and privacy constraints. The healthcare sector faces significant challenges in utilizing patient data because of concerns about data security and confidentiality. To address this, various privacy-preserving methods, including synthetic data generation, have been proposed. Synthetic data replicate existing data as closely as possible, acting as a proxy for sensitive information. While patient data are often longitudinal, this aspect remains underrepresented in existing reviews of synthetic data generation in healthcare. This paper maps and describes methods for generating and evaluating synthetic longitudinal patient data in real-life settings through a systematic literature review, conducted following the PRISMA guidelines and incorporating data from five databases up to May 2024. Thirty-nine methods were identified, with four addressing all challenges of longitudinal data generation, though none included privacy-preserving mechanisms. Resemblance was evaluated in most studies, utility in the majority, and privacy in just over half. Only a small fraction of studies assessed all three aspects. Our findings highlight the need for further research in this area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.12380v3</guid>
      <category>stat.ME</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Katariina Perkonoja, Kari Auranen, Joni Virta</dc:creator>
    </item>
    <item>
      <title>Causal Panel Analysis under Parallel Trends: Lessons from A Large Reanalysis Study</title>
      <link>https://arxiv.org/abs/2309.15983</link>
      <description>arXiv:2309.15983v4 Announce Type: replace-cross 
Abstract: Two-way fixed effects (TWFE) models are widely used in political science to establish causality, but recent methodological discussions highlight their limitations under heterogeneous treatment effects (HTE) and violations of the parallel trends (PT) assumption. This growing literature has introduced new estimators and diagnostics, causing confusion among researchers about the reliability of existing results and best practices. To address these concerns, we replicated and reanalyzed 49 articles from leading journals using TWFE models for observational panel data with binary treatments. Using six HTE-robust estimators, diagnostic tests, and sensitivity analyses, we find: (i) HTE-robust estimators yield qualitatively similar but highly variable results; (ii) while a few studies show clear signs of PT violations, many lack evidence to support this assumption; and (iii) many studies are underpowered when accounting for HTE and potential PT violations. We emphasize the importance of strong research designs and rigorous validation of key identifying assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.15983v4</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Albert Chiu, Xingchen Lan, Ziyi Liu, Yiqing Xu</dc:creator>
    </item>
    <item>
      <title>Ranking by Lifts: A Cost-Benefit Approach to Large-Scale A/B Tests</title>
      <link>https://arxiv.org/abs/2407.01036</link>
      <description>arXiv:2407.01036v2 Announce Type: replace-cross 
Abstract: A/B testers that conduct large-scale tests often prioritize lifts as the main outcome metric and want to be able to control costs resulting from false rejections of the null. This work develops a decision-theoretic framework for maximizing profits subject to false discovery rate (FDR) control. We build an empirical Bayes solution for the problem via a greedy knapsack approach. We derive an oracle rule based on ranking the ratio of expected lifts and the cost of wrong rejections using the local false discovery rate (lfdr) statistic. Our oracle decision rule is valid and optimal for large-scale tests. Further, we establish asymptotic validity for the data-driven procedure and demonstrate finite-sample validity in experimental studies. We also demonstrate the merit of the proposed method over other FDR control methods. Finally, we discuss an application to data collected by experiments on the Optimizely platform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01036v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pallavi Basu, Ron Berman</dc:creator>
    </item>
    <item>
      <title>Ensemble data assimilation to diagnose AI-based weather prediction model: A case with ClimaX version 0.3.1</title>
      <link>https://arxiv.org/abs/2407.17781</link>
      <description>arXiv:2407.17781v4 Announce Type: replace-cross 
Abstract: Artificial intelligence (AI)-based weather prediction research is growing rapidly and has shown to be competitive with the advanced dynamic numerical weather prediction models. However, research combining AI-based weather prediction models with data assimilation remains limited partially because long-term sequential data assimilation cycles are required to evaluate data assimilation systems. This study proposes using ensemble data assimilation for diagnosing AI-based weather prediction models, and marked the first successful implementation of ensemble Kalman filter with AI-based weather prediction models. Our experiments with an AI-based model ClimaX demonstrated that the ensemble data assimilation cycled stably for the AI-based weather prediction model using covariance inflation and localization techniques within the ensemble Kalman filter. While ClimaX showed some limitations in capturing flow-dependent error covariance compared to dynamical models, the AI-based ensemble forecasts provided reasonable and beneficial error covariance in sparsely observed regions. In addition, ensemble data assimilation revealed that error growth based on ensemble ClimaX predictions was weaker than that of dynamical NWP models, leading to higher inflation factors. A series of experiments demonstrated that ensemble data assimilation can be used to diagnose properties of AI weather prediction models such as physical consistency and accurate error growth representation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17781v4</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shunji Kotsuki, Kenta Shiraishi, Atsushi Okazaki</dc:creator>
    </item>
    <item>
      <title>Dynamic Estimation of Learning Rates Using a Non-Linear Autoregressive Model</title>
      <link>https://arxiv.org/abs/2410.09943</link>
      <description>arXiv:2410.09943v2 Announce Type: replace-cross 
Abstract: We introduce a new class of adaptive non-linear autoregressive (Nlar) models incorporating the concept of momentum, which dynamically estimate both the learning rates and momentum as the number of iterations increases. In our method, the growth of the gradients is controlled using a scaling (clipping) function, leading to stable convergence. Within this framework, we propose three distinct estimators for learning rates and provide theoretical proof of their convergence. We further demonstrate how these estimators underpin the development of effective Nlar optimizers. The performance of the proposed estimators and optimizers is rigorously evaluated through extensive experiments across several datasets and a reinforcement learning environment. The results highlight two key features of the Nlar optimizers: robust convergence despite variations in underlying parameters, including large initial learning rates, and strong adaptability with rapid convergence during the initial epochs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09943v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ramin Okhrati</dc:creator>
    </item>
  </channel>
</rss>
