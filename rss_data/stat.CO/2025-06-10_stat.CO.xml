<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 10 Jun 2025 04:00:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Efficient Mirror-type Kernels for the Metropolis-Hastings Algorithm</title>
      <link>https://arxiv.org/abs/2506.06660</link>
      <description>arXiv:2506.06660v1 Announce Type: new 
Abstract: We propose a new Metropolis-Hastings (MH) kernel by introducing the Mirror move into the Metropolis adjusted Langevin algorithm (MALA). This new kernel uses the strength of one kernel to overcome the shortcoming of the other, and generates proposals that are distant from the current position, but still within the high-density region of the target distribution. The resulting algorithm can be much more efficient than both Mirror and MALA, while stays comparable in terms of computational cost. We demonstrate the advantages of the MirrorMALA kernel using a variety of one-dimensional and multi-dimensional examples. The Mirror and MirrorMALA are both special cases of the Mirror-type kernels, a new suite of efficient MH proposals. We use the Mirror-type kernels, together with a novel method of doing the whitening transformation on high-dimensional random variables, which was inspired by Tan and Nott, to analyse the Bayesian generalized linear mixed models (GLMMs), and obtain the per-time-unit efficiency that is 2--20 times higher than the HMC or NUTS algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06660v1</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nuo Guan, Xiyun Jiao</dc:creator>
    </item>
    <item>
      <title>Linear Discriminant Analysis with Gradient Optimization on Covariance Inverse</title>
      <link>https://arxiv.org/abs/2506.06845</link>
      <description>arXiv:2506.06845v1 Announce Type: new 
Abstract: Linear discriminant analysis (LDA) is a fundamental method in statistical pattern recognition and classification, achieving Bayes optimality under Gaussian assumptions. However, it is well-known that classical LDA may struggle in high-dimensional settings due to instability in covariance estimation. In this work, we propose LDA with gradient optimization (LDA-GO), a new approach that directly optimizes the inverse covariance matrix via gradient descent. The algorithm parametrizes the inverse covariance matrix through Cholesky factorization, incorporates a low-rank extension to reduce computational complexity, and considers a multiple-initialization strategy, including identity initialization and warm-starting from the classical LDA estimates. The effectiveness of LDA-GO is demonstrated through extensive multivariate simulations and real-data experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06845v1</guid>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cencheng Shen, Yuexiao Dong</dc:creator>
    </item>
    <item>
      <title>The Lasso Distribution: Properties, Sampling Methods, and Applications in Bayesian Lasso Regression</title>
      <link>https://arxiv.org/abs/2506.07394</link>
      <description>arXiv:2506.07394v1 Announce Type: new 
Abstract: In this paper, we introduce a new probability distribution, the Lasso distribution. We derive several fundamental properties of the distribution, including closed-form expressions for its moments and moment-generating function. Additionally, we present an efficient and numerically stable algorithm for generating random samples from the distribution, facilitating its use in both theoretical and applied settings. We establish that the Lasso distribution belongs to the exponential family. A direct application of the Lasso distribution arises in the context of an existing Gibbs sampler, where the full conditional distribution of each regression coefficient follows this distribution. This leads to a more computationally efficient and theoretically grounded sampling scheme. To facilitate the adoption of our methodology, we provide an R package implementing the proposed methods. Our findings offer new insights into the probabilistic structure underlying the Lasso penalty and provide practical improvements in Bayesian inference for high-dimensional regression problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07394v1</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Javad Davoudabadi, Jonathon Tidswell, Samuel Muller, Garth Tarr, John T. Ormerod</dc:creator>
    </item>
    <item>
      <title>Evaluating Large Language Model Capabilities in Assessing Spatial Econometrics Research</title>
      <link>https://arxiv.org/abs/2506.06377</link>
      <description>arXiv:2506.06377v1 Announce Type: cross 
Abstract: This paper investigates Large Language Models (LLMs) ability to assess the economic soundness and theoretical consistency of empirical findings in spatial econometrics. We created original and deliberately altered "counterfactual" summaries from 28 published papers (2005-2024), which were evaluated by a diverse set of LLMs. The LLMs provided qualitative assessments and structured binary classifications on variable choice, coefficient plausibility, and publication suitability. The results indicate that while LLMs can expertly assess the coherence of variable choices (with top models like GPT-4o achieving an overall F1 score of 0.87), their performance varies significantly when evaluating deeper aspects such as coefficient plausibility and overall publication suitability. The results further revealed that the choice of LLM, the specific characteristics of the paper and the interaction between these two factors significantly influence the accuracy of the assessment, particularly for nuanced judgments. These findings highlight LLMs' current strengths in assisting with initial, more surface-level checks and their limitations in performing comprehensive, deep economic reasoning, suggesting a potential assistive role in peer review that still necessitates robust human oversight.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06377v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.CO</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giuseppe Arbia, Luca Morandini, Vincenzo Nardelli</dc:creator>
    </item>
    <item>
      <title>A Neuronal Model at the Edge of Criticality: An Ising-Inspired Approach to Brain Dynamics</title>
      <link>https://arxiv.org/abs/2506.07027</link>
      <description>arXiv:2506.07027v1 Announce Type: cross 
Abstract: We present a neuronal network model inspired by the Ising model, where each neuron is a binary spin ($s_i = \pm1$) interacting with its neighbors on a 2D lattice. Updates are asynchronous and follow Metropolis dynamics, with a temperature-like parameter $T$ introducing stochasticity.
  To incorporate physiological realism, each neuron includes fixed on/off durations, mimicking the refractory period found in real neurons. These counters prevent immediate reactivation, adding biologically grounded timing constraints to the model.
  As $T$ varies, the network transitions from asynchronous to synchronised activity. Near a critical point $T_c$, we observe hallmarks of criticality: heightened fluctuations, long-range correlations, and increased sensitivity. These features resemble patterns found in cortical recordings, supporting the hypothesis that the brain operates near criticality for optimal information processing.
  This simplified model demonstrates how basic spin interactions and physiological constraints can yield complex, emergent behavior, offering a useful tool for studying criticality in neural systems through statistical physics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07027v1</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.soft</category>
      <category>stat.CO</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sajedeh Sarmastani, Maliheh Ghodrat, Yousef Jamali</dc:creator>
    </item>
    <item>
      <title>One-dimensional quantile-stratified sampling and its application in statistical simulations</title>
      <link>https://arxiv.org/abs/2506.07437</link>
      <description>arXiv:2506.07437v1 Announce Type: cross 
Abstract: In this paper we examine quantile-stratified samples from a known univariate probability distribution, with stratification occurring over a partition of the quantile regions in the distribution. We examine some general properties of this sampling method and we contrast it with standard IID sampling to highlight its similarities and differences. We examine the applications of this sampling method to various statistical simulations including importance sampling. We conduct simulation analysis to compare the performance of standard importance sampling against the quantile-stratified importance sampling to see how they each perform on a range of functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07437v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ben O'Neill</dc:creator>
    </item>
    <item>
      <title>Simulation thinning algorithm for a CARMA(p,q)-Hawkes model</title>
      <link>https://arxiv.org/abs/2402.03275</link>
      <description>arXiv:2402.03275v2 Announce Type: replace 
Abstract: This paper presents an algorithm for the simulation of Hawkes-type processes where the intensity is expressed in terms of a continuous-time autoregressive moving average model. We identify upper bounds for both the univariate and the multivariate intensity functions that are used to develop simulation algorithms based on the thinning technique.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.03275v2</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Mercuri, Andrea Perchiazzo, Edit Rroji</dc:creator>
    </item>
    <item>
      <title>Digesting Gibbs Sampling Using R</title>
      <link>https://arxiv.org/abs/2410.14073</link>
      <description>arXiv:2410.14073v3 Announce Type: replace 
Abstract: In general, the statistical simulation approaches are referred to as the Monte Carlo methods as a whole. The broad class of the Monte Carlo methods involves the Markov chain Monte Carlo (MCMC) techniques that attract the attention of researchers from a wide variety of study fields. The main focus of this report is to provide a framework for all users who are interested in implementing the MCMC approaches in their investigations, especially the Gibbs sampling. I have tried, if possible, to eliminate the proofs, but reader is expected to know some topics in elementary calculus (including mathematical function, limit, derivative, partial derivative, simple integral) and statistics (including random variables, expected value and variance, moment generating function, multivariate distribution, distribution of a functions of random variable, and the central limit theorem).</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14073v3</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mahdi Teimouri</dc:creator>
    </item>
    <item>
      <title>SMRS: advocating a unified reporting standard for surrogate models in the artificial intelligence era</title>
      <link>https://arxiv.org/abs/2502.06753</link>
      <description>arXiv:2502.06753v2 Announce Type: replace 
Abstract: Surrogate models are widely used to approximate complex systems across science and engineering to reduce computational costs. Despite their widespread adoption, the field lacks standardisation across key stages of the modelling pipeline, including data sampling, model selection, evaluation, and downstream analysis. This fragmentation limits reproducibility and cross-domain utility -- a challenge further exacerbated by the rapid proliferation of AI-driven surrogate models. We argue for the urgent need to establish a structured reporting standard, the Surrogate Model Reporting Specification (SMRS), that systematically captures essential design and evaluation choices while remaining agnostic to implementation specifics. By promoting a standardised yet flexible framework, we aim to improve the reliability of surrogate modelling, foster interdisciplinary knowledge transfer, and, as a result, accelerate scientific progress in the AI era.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06753v2</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Elizaveta Semenova, Alisa Sheinkman, Timothy James Hitge, Siobhan Mackenzie Hall, Jon Cockayne</dc:creator>
    </item>
    <item>
      <title>The inverse Kalman filter</title>
      <link>https://arxiv.org/abs/2407.10089</link>
      <description>arXiv:2407.10089v4 Announce Type: replace-cross 
Abstract: We introduce the inverse Kalman filter, which enables exact matrix-vector multiplication between a covariance matrix from a dynamic linear model and any real-valued vector with linear computational cost. We integrate the inverse Kalman filter with the conjugate gradient algorithm, which substantially accelerates the computation of matrix inversion for a general form of covariance matrix, where other approximation approaches may not be directly applicable. We demonstrate the scalability and efficiency of the proposed approach through applications in nonparametric estimation of particle interaction functions, using both simulations and cell trajectories from microscopy data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10089v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyi Fang, Mengyang Gu</dc:creator>
    </item>
    <item>
      <title>Fitting Multilevel Factor Models</title>
      <link>https://arxiv.org/abs/2409.12067</link>
      <description>arXiv:2409.12067v3 Announce Type: replace-cross 
Abstract: We examine a special case of the multilevel factor model, with covariance given by multilevel low rank (MLR) matrix~\cite{parshakova2023factor}. We develop a novel, fast implementation of the expectation-maximization algorithm, tailored for multilevel factor models, to maximize the likelihood of the observed data. This method accommodates any hierarchical structure and maintains linear time and storage complexities per iteration. This is achieved through a new efficient technique for computing the inverse of the positive definite MLR matrix. We show that the inverse of positive definite MLR matrix is also an MLR matrix with the same sparsity in factors, and we use the recursive Sherman-Morrison-Woodbury matrix identity to obtain the factors of the inverse. Additionally, we present an algorithm that computes the Cholesky factorization of an expanded matrix with linear time and space complexities, yielding the covariance matrix as its Schur complement. This paper is accompanied by an open-source package that implements the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12067v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.MS</category>
      <category>stat.CO</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tetiana Parshakova, Trevor Hastie, Stephen Boyd</dc:creator>
    </item>
    <item>
      <title>Improving the convergence of Markov chains via permutations and projections</title>
      <link>https://arxiv.org/abs/2411.08295</link>
      <description>arXiv:2411.08295v3 Announce Type: replace-cross 
Abstract: This paper aims at improving the convergence to equilibrium of finite ergodic Markov chains via permutations and projections. First, we prove that a specific mixture of permuted Markov chains arises naturally as a projection under the KL divergence or the squared-Frobenius norm. We then compare various mixing properties of the mixture with other competing Markov chain samplers and demonstrate that it enjoys improved convergence. This geometric perspective motivates us to propose samplers based on alternating projections to combine different permutations and to analyze their rate of convergence. We give necessary, and under some additional assumptions also sufficient, conditions for the projection to achieve stationarity in the limit in terms of the trace of the transition matrix. We proceed to discuss tuning strategies of the projection samplers when these permutations are viewed as parameters. Along the way, we reveal connections between the mixture and a Markov chain Sylvester's equation as well as assignment problems, and highlight how these can be used to understand and improve Markov chain mixing. We provide two examples as illustrations. In the first example, the projection sampler (with a suitable choice of the permutation) improves upon Metropolis-Hastings in a discrete bimodal distribution with a reduced relaxation time from exponential to polynomial in the system size, while in the second example, the mixture of permuted Markov chain yields a mixing time that is logarithmic in system size (with high probability under random permutation), compared to a linear mixing time in the Diaconis-Holmes-Neal sampler. Finally, we provide numerical experiments on simple statistical physics models to illustrate the improved mixing performance of the proposed projection samplers over standard Metropolis-Hastings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08295v3</guid>
      <category>math.PR</category>
      <category>math.OC</category>
      <category>stat.CO</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael C. H. Choi, Max Hird, Youjia Wang</dc:creator>
    </item>
    <item>
      <title>Mechanistic models for panel data: Analysis of ecological experiments with four interacting species</title>
      <link>https://arxiv.org/abs/2506.04508</link>
      <description>arXiv:2506.04508v2 Announce Type: replace-cross 
Abstract: In an ecological context, panel data arise when time series measurements are made on a collection of ecological processes. Each process may correspond to a spatial location for field data, or to an experimental ecosystem in a designed experiment. Statistical models for ecological panel data should capture the high levels of nonlinearity, stochasticity, and measurement uncertainty inherent in ecological systems. Furthermore, the system dynamics may depend on unobservable variables. This study applies iterated particle filtering techniques to explore new possibilities for likelihood-based statistical analysis of these complex systems. We analyze data from a mesocosm experiment in which two species of the freshwater planktonic crustacean genus, Daphnia, coexist with an alga and a fungal parasite. Time series data were collected on replicated mesocosms under six treatment conditions. Iterated filtering enables maximization of the likelihood for scientifically motivated nonlinear partially observed Markov process models, providing access to standard likelihood-based methods for parameter estimation, confidence intervals, hypothesis testing, model selection and diagnostics. This toolbox allows scientists to propose and evaluate scientifically motivated stochastic dynamic models for panel data, constrained only by the requirement to write code to simulate from the model and to specify a measurement distribution describing how the system state is observed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04508v2</guid>
      <category>stat.AP</category>
      <category>q-bio.PE</category>
      <category>stat.CO</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bo Yang, Jesse Wheeler, Meghan A. Duffy, Aaron A. King, Edward L. Ionides</dc:creator>
    </item>
  </channel>
</rss>
