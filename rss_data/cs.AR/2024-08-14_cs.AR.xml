<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 15 Aug 2024 04:02:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 15 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Bitwise Logic Using Phase Change Memory Devices Based on the Pinatubo Architecture</title>
      <link>https://arxiv.org/abs/2408.07228</link>
      <description>arXiv:2408.07228v1 Announce Type: new 
Abstract: This paper experimentally demonstrates a near-crossbar memory logic technique called Pinatubo. Pinatubo, an acronym for Processing In Non-volatile memory ArchiTecture for bUlk Bitwise Operations, facilitates the concurrent activation of two or more rows, enabling bitwise operations such as OR, AND, XOR, and NOT on the activated rows. We implement Pinatubo using phase change memory (PCM) and compare our experimental results with the simulated data from the original Pinatubo study. Our findings highlight a significant four-orders of magnitude difference between resistance states, suggesting the robustness of the Pinatubo architecture with PCM technology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07228v1</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1109/VLSID60093.2024.00103</arxiv:DOI>
      <dc:creator>Noa Aflalo, Eilam Yalon, Shahar Kvatinsky</dc:creator>
    </item>
    <item>
      <title>Interactive and Automatic Generation of Primitive Custom Circuit Layout Using LLMs</title>
      <link>https://arxiv.org/abs/2408.07279</link>
      <description>arXiv:2408.07279v1 Announce Type: new 
Abstract: In this study, we investigate the use of Large Language Models (LLMs) for the interactive and automated production of customs circuit layouts described in natural language. Our proposed layout automation process leverages a template-and-grid-based layout generation framework to create process-portable layout generators tailored for various custom circuits, including standard cells and high-speed mixed-signal circuits. However, rather than directly describing the layout generators in traditional programming language, we utilize natural language using LLMs to make the layout generation process more intuitive and efficient. This approach also supports interactive modifications of the layout generator code, enhancing customization capabilities. We demonstrate the effectiveness of our LLM-based layout generation method across several custom circuit examples, such as logic standard cells, a serializer and a strong arm latch, including their completeness in terms of Design Rule Check (DRC), Layout Versus Schematic (LVS) test, and post-layout performance for high-speed circuits. Our experimental results indicate that LLMs can generate a diverse range of circuit layouts with substantial customization options.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07279v1</guid>
      <category>cs.AR</category>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Geunyoung You, Youjin Byun, Sojin Lim, Jaeduk Han</dc:creator>
    </item>
    <item>
      <title>LPU: A Latency-Optimized and Highly Scalable Processor for Large Language Model Inference</title>
      <link>https://arxiv.org/abs/2408.07326</link>
      <description>arXiv:2408.07326v1 Announce Type: new 
Abstract: The explosive arrival of OpenAI's ChatGPT has fueled the globalization of large language model (LLM), which consists of billions of pretrained parameters that embodies the aspects of syntax and semantics. HyperAccel introduces latency processing unit (LPU), a latency-optimized and highly scalable processor architecture for the acceleration of LLM inference. LPU perfectly balances the memory bandwidth and compute logic with streamlined dataflow to maximize performance and efficiency. LPU is equipped with expandable synchronization link (ESL) that hides data synchronization latency between multiple LPUs. HyperDex complements LPU as an intuitive software framework to run LLM applications. LPU achieves 1.25 ms/token and 20.9 ms/token for 1.3B and 66B model, respectively, which is 2.09x and 1.37x faster than the GPU. LPU, synthesized using Samsung 4nm process, has total area of 0.824 mm2 and power consumption of 284.31 mW. LPU-based servers achieve 1.33x and 1.32x energy efficiency over NVIDIA H100 and L4 servers, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07326v1</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Seungjae Moon, Jung-Hoon Kim, Junsoo Kim, Seongmin Hong, Junseo Cha, Minsu Kim, Sukbin Lim, Gyubin Choi, Dongjin Seo, Jongho Kim, Hunjong Lee, Hyunjun Park, Ryeowook Ko, Soongyu Choi, Jongse Park, Jinwon Lee, Joo-Young Kim</dc:creator>
    </item>
    <item>
      <title>Efficient Edge AI: Deploying Convolutional Neural Networks on FPGA with the Gemmini Accelerator</title>
      <link>https://arxiv.org/abs/2408.07404</link>
      <description>arXiv:2408.07404v1 Announce Type: new 
Abstract: The growing concerns regarding energy consumption and privacy have prompted the development of AI solutions deployable on the edge, circumventing the substantial CO2 emissions associated with cloud servers and mitigating risks related to sharing sensitive data. But deploying Convolutional Neural Networks (CNNs) on non-off-the-shelf edge devices remains a complex and labor-intensive task. In this paper, we present and end-to-end workflow for deployment of CNNs on Field Programmable Gate Arrays (FPGAs) using the Gemmini accelerator, which we modified for efficient implementation on FPGAs. We describe how we leverage the use of open source software on each optimization step of the deployment process, the customizations we added to them and its impact on the final system's performance. We were able to achieve real-time performance by deploying a YOLOv7 model on a Xilinx ZCU102 FPGA with an energy efficiency of 36.5 GOP/s/W. Our FPGA-based solution demonstrates superior power efficiency compared with other embedded hardware devices, and even outperforms other FPGA reference implementations. Finally, we present how this kind of solution can be integrated into a wider system, by testing our proposed platform in a traffic monitoring scenario.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07404v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Federico Nicolas Peccia, Svetlana Pavlitska, Tobias Fleck, Oliver Bringmann</dc:creator>
    </item>
    <item>
      <title>Development of simulation model for Single Carrier Transceiver for Nanosatellite</title>
      <link>https://arxiv.org/abs/2408.07655</link>
      <description>arXiv:2408.07655v1 Announce Type: new 
Abstract: CubeSat is a nanosatellite concept emerged from a paper published by Stanford University and with their low cost nature and extreme feasibility , more started researching on nano satellites. New technology emerged , paving path to many academics and small vendors to create their own CubeSat models .
  This nanosatellite requires a transceiver to maintain its communication between it's systems and the ground station, which helps it navigate and collects data gained from its programmed functions. This transceiver system consists mainly of a transmitter and a receiver. The transmitter manages sending data from the satellite to ground station while the receiver captures the data and instruction sent from the ground station to the satellite. These systems were built using separate digital communication devices in the beginning, with many critical limitations with respect to the space and scalability of the modules to be attached and the programmability of hardware materials and the concept of system-on -board emerged. Meanwhile, As the size of electronic devices minimized with the research conducted, FPGA (Filed Programmable Logic Array) was introduced as an architecture to be used in various applications and research needs. The reason FPGA was mention was for the fact that it provides flexibility in the designing of transceiver ed with design and prototypes implementation at a low cost competitional electronic ware and the system -on chip Concept was introduced . This research describes the development a system-on-chip transceiver model for nanosatellites which contains a single carrier . Keywords-Single Carrier, transceiver, system C, FPGA</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07655v1</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pallewela R. C. K Dr. Rohana Thilakuamra Mr. Prabath Buddhika</dc:creator>
    </item>
    <item>
      <title>Natural Language to Verilog: Design of a Recurrent Spiking Neural Network using Large Language Models and ChatGPT</title>
      <link>https://arxiv.org/abs/2405.01419</link>
      <description>arXiv:2405.01419v2 Announce Type: replace 
Abstract: This paper investigates the use of Large Language Models (LLMs) for automating the generation of hardware description code, aiming to explore their potential in supporting and enhancing the development of efficient neuromorphic computing architectures. Building on our prior work, we employ OpenAI's ChatGPT4 and natural language prompts to synthesize a RTL Verilog module of a programmable recurrent spiking neural network, while also generating test benches to assess the system's correctness. The resultant design was validated in three case studies, the exclusive OR,the IRIS flower classification and the MNIST hand-written digit classification, achieving accuracies of up to 96.6%. To verify its synthesizability and implementability, the design was prototyped on a field-programmable gate array and implemented on SkyWater 130 nm technology by using an open-source electronic design automation flow. Additionally, we have submitted it to Tiny Tapeout 6 chip fabrication program to further evaluate the system on-chip performance in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01419v2</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paola Vitolo, George Psaltakis, Michael Tomlinson, Gian Domenico Licciardo, Andreas G. Andreou</dc:creator>
    </item>
    <item>
      <title>The Bicameral Cache: a split cache for vector architectures</title>
      <link>https://arxiv.org/abs/2407.15440</link>
      <description>arXiv:2407.15440v2 Announce Type: replace 
Abstract: The Bicameral Cache is a cache organization proposal for a vector architecture that segregates data according to their access type, distinguishing scalar from vector references. Its aim is to avoid both types of references from interfering in each other's data locality, with a special focus on prioritizing the performance on vector references. The proposed system incorporates an additional, non-polluting prefetching mechanism to help populate the long vector cache lines in advance to increase the hit rate by further exploiting the spatial locality on vector data. Its evaluation was conducted on the Cavatools simulator, comparing the performance to a standard conventional cache, over different typical vector benchmarks for several vector lengths. The results proved the proposed cache speeds up performance on stride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition, the prefetching feature consistently provided an additional value.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15440v2</guid>
      <category>cs.AR</category>
      <category>cs.PF</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Susana Rebolledo, Borja Perez, Jose Luis Bosque, Peter Hsu</dc:creator>
    </item>
    <item>
      <title>Control-Flow Attestation: Concepts, Solutions, and Open Challenges</title>
      <link>https://arxiv.org/abs/2408.06304</link>
      <description>arXiv:2408.06304v2 Announce Type: replace-cross 
Abstract: Control-flow attestation unifies the worlds of control-flow integrity and platform attestation by measuring and reporting a target's run-time behaviour to a verifier. Trust assurances in the target are provided by testing whether its execution follows an authorised control-flow path. The problem has been explored in various settings, such as assessing the trustworthiness of cyber-physical systems, Internet of Things devices, cloud platforms, and many others. Despite a significant number of proposals being made in recent years, the area remains fragmented, addressing different adversarial behaviours, verification paradigms, and deployment challenges. In this paper, we present the first survey of control-flow attestation, examining the core ideas and solutions in state-of-the-art schemes. In total, we survey over 30 papers published between 2016-2024, consolidate and compare their key features, and pose several challenges and recommendations for future research in the area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06304v2</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhanyu Sha, Carlton Shepherd, Amir Rafi, Konstantinos Markantonakis</dc:creator>
    </item>
  </channel>
</rss>
