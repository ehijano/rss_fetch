<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 22 Oct 2025 01:47:58 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Population-Based Search Method Using Uncertainty-related Pareto Front for Robust Multi-objective Optimization</title>
      <link>https://arxiv.org/abs/2510.16386</link>
      <description>arXiv:2510.16386v1 Announce Type: new 
Abstract: Traditional robust multi-objective optimization methods typically prioritize convergence while treating robustness as a secondary consideration. This approach can yield solutions that are not genuinely robust optimal under noise-affected scenarios. Furthermore, compared to population-based search methods, determining the robust optimal solution by evaluating the robustness of a single convergence-optimal solution is also inefficient. To address these two limitations,we propose a novel Uncertainty-related Pareto Front (UPF) framework that balances robustness and convergence as equal priorities. Unlike traditional Pareto Front, the UPF explicitly accounts for decision variable with noise perturbation by quantifying their effects on both convergence guarantees and robustness preservation equally within a theoretically grounded and general framework. Building upon UPF, we propose RMOEA-UPF--a population-based search robust multi-objective optimization algorithm. This method enables efficient search optimization by calculating and optimizing the UPF during the evolutionary process.Experiments on nine benchmark problems and a real-world application demonstrate that RMOEA-UPF consistently delivers high-quality results. Our method's consistent top-ranking performance indicates a more general and reliable approach for solving complex, uncertain multi-objective optimization problems. Code is available at: https://github.com/WenxiangJiang-me/RMOEA-UPF.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16386v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lihong Xu, Wenxiang Jiang</dc:creator>
    </item>
    <item>
      <title>ViT-Transformer: Self-attention mechanism based constitutive modeling for nonlinear heterogeneous materials</title>
      <link>https://arxiv.org/abs/2510.16575</link>
      <description>arXiv:2510.16575v1 Announce Type: new 
Abstract: Multi-scale simulations of nonlinear heterogeneous materials and composites are challenging due to the prohibitive computational costs of high-fidelity simulations. Recently, machine learning (ML) based approaches have emerged as promising alternatives to traditional multiscale methods. However, existing ML surrogate constitutive models struggle in capturing long-range dependencies and generalization across microstructures. The recent advancements in attention-based Transformer architectures open the door to a more powerful class of surrogate models. Attention mechanism has demonstrated remarkable capabilities in natural language processing and computer vision. In this work, we introduce a surrogate (meta) model, namely ViT-Transformer, using a Vision Transformer (ViT) encoder and a Transformer-based decoder which are both driven by the self-attention mechanism. The ViT encoder extracts microstructural features from material images, while the decoder is a masked Transformer encoder that combines the latent geometrical features with the macroscopic strain input sequence to predict the corresponding stress response. To enhance training, we propose a random extract training algorithm that improves robustness to sequences of variable length. We design and construct a compact yet diverse dataset via data augmentation, and validate the surrogate model using various composite material images and loading scenarios. Several numerical examples are provided to show the effectiveness and accuracy of the ViT-Transformer model and the training algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16575v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yijing Zhou, Shabnam J. Semnani</dc:creator>
    </item>
    <item>
      <title>Chem-R: Learning to Reason as a Chemist</title>
      <link>https://arxiv.org/abs/2510.16880</link>
      <description>arXiv:2510.16880v1 Announce Type: new 
Abstract: Although large language models (LLMs) have significant potential to advance chemical discovery, current LLMs lack core chemical knowledge, produce unreliable reasoning trajectories, and exhibit suboptimal performance across diverse chemical tasks. To address these challenges, we propose Chem-R, a generalizable Chemical Reasoning model designed to emulate the deliberative processes of chemists. Chem-R is trained through a three-phase framework that progressively builds advanced reasoning capabilities, including: 1) Chemical Foundation Training, which establishes core chemical knowledge. 2) Chemical Reasoning Protocol Distillation, incorporating structured, expert-like reasoning traces to guide systematic and reliable problem solving. 3) Multi-task Group Relative Policy Optimization that optimizes the model for balanced performance across diverse molecular- and reaction-level tasks. This structured pipeline enables Chem-R to achieve state-of-the-art performance on comprehensive benchmarks, surpassing leading large language models, including Gemini-2.5-Pro and DeepSeek-R1, by up to 46% on molecular tasks and 66% on reaction tasks. Meanwhile, Chem-R also consistently outperforms the existing chemical foundation models across both molecular and reaction level tasks. These results highlight Chem-R's robust generalization, interpretability, and potential as a foundation for next-generation AI-driven chemical discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16880v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weida Wang, Benteng Chen, Di Zhang, Wanhao Liu, Shuchen Pu, Ben Gao, Jin Zeng, Lei Bai, Wanli Ouyang, Xiaoyong Wei, Tianshu Yu, Tianfan Fu, Shuzhou Sun, Jiatong Li, Zifu Wang, Yuqiang Li, Shufei Zhang</dc:creator>
    </item>
    <item>
      <title>Addressing data scarcity in structural health monitoring through generative augmentation</title>
      <link>https://arxiv.org/abs/2510.16889</link>
      <description>arXiv:2510.16889v1 Announce Type: new 
Abstract: Structural Health Monitoring plays a crucial role in ensuring the safety, reliability, and longevity of bridge infrastructures through early damage detection. Although recent advances in deep learning-based models have enabled automated event detection, their performance is often limited by data scarcity, environmental noise, and class imbalance. To address these challenges, this study introduces a customized Generative Adversarial Network model, STFTSynth, designed particularly for generating short-time Fourier transform spectrograms derived from acoustic event signals. In contrast to augmentation techniques such as MixUp, generative adversarial networks can synthesize high-quality spectrograms that mimic real-world events, enhancing dataset diversity and robustness. The proposed model integrates dense residual blocks for spatial consistency with bidirectional gated recurrent units for temporal dependency modeling. Model performance is evaluated against three baseline generative models using qualitative inspection and quantitative metrics, including Structural Similarity Index Measure, Peak Signal-to-Noise Ratio, and Fr\'echet Inception Distance. Results show that STFTSynth outperforms baseline models, producing high-resolution, temporally consistent spectrograms that align closely with real-world data. These findings indicate the potential of generative-based data augmentation as a scalable and cost-effective solution for bridge monitoring scenarios where rare events, such as prestressing wire breakage, suffer from data scarcity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16889v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sasan Farhadi, Mariateresa Iavarone, Mauro Corrado, Eleni Chatzi, Giulio Ventura</dc:creator>
    </item>
    <item>
      <title>Trading with the Devil: Risk and Return in Foundation Model Strategies</title>
      <link>https://arxiv.org/abs/2510.17165</link>
      <description>arXiv:2510.17165v1 Announce Type: new 
Abstract: Foundation models - already transformative in domains such as natural language processing - are now starting to emerge for time-series tasks in finance. While these pretrained architectures promise versatile predictive signals, little is known about how they shape the risk profiles of the trading strategies built atop them, leaving practitioners reluctant to commit serious capital. In this paper, we propose an extension to the Capital Asset Pricing Model (CAPM) that disentangles the systematic risk introduced by a shared foundation model - potentially capable of generating alpha if the underlying model is genuinely predictive - from the idiosyncratic risk attributable to custom fine-tuning, which typically accrues no systematic premium. To enable a practical estimation of these separate risks, we align this decomposition with the concepts of uncertainty disentanglement, casting systematic risk as epistemic uncertainty (rooted in the pretrained model) and idiosyncratic risk as aleatory uncertainty (introduced during custom adaptations). Under the Aleatory Collapse Assumption, we illustrate how Monte Carlo dropout - among other methods in the uncertainty-quantization toolkit - can directly measure the epistemic risk, thereby mapping trading strategies to a more transparent risk-return plane. Our experiments show that isolating these distinct risk factors yields deeper insights into the performance limits of foundation-model-based strategies, their model degradation over time, and potential avenues for targeted refinements. Taken together, our results highlight both the promise and the pitfalls of deploying large pretrained models in competitive financial markets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17165v1</guid>
      <category>cs.CE</category>
      <category>q-fin.TR</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinrui Zhang</dc:creator>
    </item>
    <item>
      <title>StrengthLawExtractor: A Fiji plugin for 3D morphological feature extraction from X-ray micro-CT data</title>
      <link>https://arxiv.org/abs/2510.17279</link>
      <description>arXiv:2510.17279v1 Announce Type: new 
Abstract: Non-destructive methods are essential for linking the microstructural geometry of porous materials to their mechanical behavior, as destructive testing is often infeasible due to limited material availability or irreproducible conditions. Micro-computed tomography (micro-CT) provides high resolution three dimensional reconstructions of porous microstructures, enabling direct quantification of geometric descriptors. Recent advances in morphometric theory have demonstrated that four independent morphometric measures (porosity, surface area, mean curvature, and Euler characteristic) are required to capture the relationship between microstructure and strength, thereby forming the basis of generalized strength laws. To facilitate practical application of this framework, a Fiji plugin was developed to extract the four morphometric measures (porosity, surface area, mean curvature, Euler characteristic) from micro-CT datasets automatically. The plugin integrates within the Fiji platform to provide reproducible, accessible, and user friendly analysis. The application of the tool demonstrates that the extracted descriptors can be readily incorporated into constitutive models and machine learning workflows, enabling the forward prediction of stress-strain behavior as well as the inverse design of microstructures. This approach supports non-destructive evaluation, accelerates materials selection, and advances the integration of imaging with predictive modeling in porous media research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17279v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qinyi Tian, Laura E. Dalton</dc:creator>
    </item>
    <item>
      <title>Modelling complexity in system safety: generalizing the D2T2 methodology</title>
      <link>https://arxiv.org/abs/2510.17351</link>
      <description>arXiv:2510.17351v1 Announce Type: new 
Abstract: Although Fault Tree and Event Tree analysis are still today the standard approach to system safety analysis for many engineering sectors, these techniques lack the capabilities of fully capturing the realistic, dynamic behaviour of complex systems, which results in a dense network of dependencies at any level, i.e. between components, trains of components or subsystems. While these limitations are well recognised across both industry and academia, the shortage of alternative tools able to tackle such challenges while retaining the computational feasibility of the analysis keeps fuelling the long-lived success of Fault Tree and Event Tree modelling. Analysts and regulators often rely on the use of conservative assumptions to mitigate the effect of oversimplifications associated with the use of such techniques. However, this results in the analysis output to be characterised by an unknown level of conservatism, with potential consequences on market competitiveness (i.e., over-conservatism) or safety (i.e., under-conservatism). This study proposes a generalization of the Dynamic and Dependent Tree Theory, which offers theoretical tools for the systematic integration of dependency modelling within the traditional Fault and Event Tree analysis framework. This is achieved by marrying the traditional combinatorial nature of failure analysis, formalised by the Fault and Event Tree language, with more flexible modelling solutions, which provide the flexibility required to capture complex system features. The main advantage of the proposed approach in comparison to existent solutions is the ability to take into account, under the same modelling framework, any type of dependency regardless of its nature and location, while retaining the familiarity and effectiveness of traditional safety modelling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17351v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Silvia Tolo, John Andrews</dc:creator>
    </item>
    <item>
      <title>Volumetric Non-Invasive Cardiac Mapping for Accessible Global Arrhythmia Characterization</title>
      <link>https://arxiv.org/abs/2510.17539</link>
      <description>arXiv:2510.17539v1 Announce Type: new 
Abstract: Cardiac arrhythmias are a major cause of morbidity and mortality increasing the risk of stroke, heart failure, and sudden cardiac death. Imageless electrocardiographic imaging (ECGI) provides a non invasive alternative to electrical mapping from body surface potentials, but conventional ECGI is confined to epicardial reconstructions and can miss arrhythmias originating in deeper myocardium. We address this by reconstructing three dimensional cardiac activity with a volumetric formulation that solves an inverse source problem via Green's functions, enabling full volume activation mapping and improved localization in anatomically complex regions. We evaluate the approach on simulated premature ventricular beats and on four challenging patient cases, a right ventricular outflow tract premature ventricular contraction, a left bundle branch block, a ventricular tachycardia, and Wolff Parkinson White, and additionally assess performance on an open source myocardial infarction dataset. Results show that volumetric ECGI recovers 3D activation and sharpens arrhythmia origin localization, achieving a 59.3% reduction in geodesic error between estimated and simulated origins relative to surface only methods; in patient cases, activation patterns align with clinical diagnoses. Overall, imageless volumetric ECGI offers accessible, non invasive 3D activation mapping that overcomes a core limitation of surface restricted techniques and may improve preprocedural planning, ablation target guidance, and selection or optimization of cardiac resynchronization therapy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17539v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jorge Vicente-Puig (Universitat Aut\`onoma de Barcelona, Spain, Corify Care S.L, Madrid, Spain), Judit Chamorro-Servent (Universitat Aut\`onoma de Barcelona, Spain), Ernesto Zacur (Corify Care S.L, Madrid, Spain), In\'es Llorente-Lipe (ITACA Institute, Universitat Polit\`ecnica de Val\`encia, Valencia, Spain), Marta Mart\'inez (ITACA Institute, Universitat Polit\`ecnica de Val\`encia, Valencia, Spain), Jorge Sanchez (Centro de Investigaci\'on e Innovaci\'on en Bioingenier\'ia, Universidad Politecnica de Valencia, Valencia, Spain, Institute of biomedical engineering, Karlsruhe Institute of Technology, Karlsruhe, Germany, Universidad Internacional de Valencia, Valencia, Spain), Jana Revent\'os (Corify Care S.L, Madrid, Spain, ITACA Institute, Universitat Polit\`ecnica de Val\`encia, Valencia, Spain, Arrhythmia Section, Cardiology Department, Hospital Cl\'inic, Universitat de Barcelona, Barcelona, Catalonia, Spain), Ivo Roca-Luque (Arrhythmia Section, Cardiology Department, Hospital Cl\'inic, Universitat de Barcelona, Barcelona, Catalonia, Spain), Lluis Mont (Arrhythmia Section, Cardiology Department, Hospital Cl\'inic, Universitat de Barcelona, Barcelona, Catalonia, Spain), Felipe Atienza (Department of Cardiology, Hospital General Universitario Gregorio Mara\~n\'on, Instituto de Investigaci\'on Sanitaria Gregorio Mara\~n\'on, Centro de Investigaci\'on Biom\'edica en Red de Enfermedades Cardiovasculares), Andreu M. Climent (Corify Care S.L, Madrid, Spain, ITACA Institute, Universitat Polit\`ecnica de Val\`encia, Valencia, Spain), Maria S. Guillem (Corify Care S.L, Madrid, Spain, ITACA Institute, Universitat Polit\`ecnica de Val\`encia, Valencia, Spain), Ismael Hern\'andez-Romero (Corify Care S.L, Madrid, Spain, ITACA Institute, Universitat Polit\`ecnica de Val\`encia, Valencia, Spain)</dc:creator>
    </item>
    <item>
      <title>Fire-EnSF: Wildfire Spread Data Assimilation using Ensemble Score Filter</title>
      <link>https://arxiv.org/abs/2510.15954</link>
      <description>arXiv:2510.15954v1 Announce Type: cross 
Abstract: As wildfires become increasingly destructive and expensive to control, effective management of active wildfires requires accurate, real-time fire spread predictions. To enhance the forecasting accuracy of active fires, data assimilation plays a vital role by integrating observations (such as remote-sensing data) and fire predictions generated from numerical models. This paper provides a comprehensive investigation on the application of a recently proposed diffusion-model-based filtering algorithm -- the Ensemble Score Filter (EnSF) -- to the data assimilation problem for real-time active wildfire spread predictions. Leveraging a score-based generative diffusion model, EnSF has been shown to have superior accuracy for high-dimensional nonlinear filtering problems, making it an ideal candidate for the filtering problems of wildfire spread models. Technical details are provided, and our numerical investigations demonstrate that EnSF provides superior accuracy, stability, and computational efficiency, establishing it as a robust and practical method for wildfire data assimilation. Our code has been made publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15954v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>stat.AP</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hongzheng Shi, Yuhang Wang, Xiao Liu</dc:creator>
    </item>
    <item>
      <title>Cash Flow Underwriting with Bank Transaction Data: Advancing MSME Financial Inclusion in Malaysia</title>
      <link>https://arxiv.org/abs/2510.16066</link>
      <description>arXiv:2510.16066v1 Announce Type: cross 
Abstract: Despite accounting for 96.1% of all businesses in Malaysia, access to financing remains one of the most persistent challenges faced by Micro, Small, and Medium Enterprises (MSMEs). Newly established or young businesses are often excluded from formal credit markets as traditional underwriting approaches rely heavily on credit bureau data. This study investigates the potential of bank statement data as an alternative data source for credit assessment to promote financial inclusion in emerging markets. Firstly, we propose a cash flow-based underwriting pipeline where we utilise bank statement data for end to end data extraction and machine learning credit scoring. Secondly, we introduce a novel dataset of 611 loan applicants from a Malaysian lending institution. Thirdly, we develop and evaluate credit scoring models based on application information and bank transaction-derived features. Empirical results show that the use of such data boosts the performance of all models on our dataset, which can improve credit scoring for new-to-lending MSMEs. Lastly, we intend to release the anonymised bank transaction dataset to facilitate further research on MSMEs financial inclusion within Malaysia's emerging economy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16066v1</guid>
      <category>q-fin.ST</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>q-fin.RM</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chun Chet Ng, Wei Zeng Low, Yin Yin Boon</dc:creator>
    </item>
    <item>
      <title>A hierarchical Bayesian approach for population-based structural health monitoring in ship hull structures</title>
      <link>https://arxiv.org/abs/2510.16316</link>
      <description>arXiv:2510.16316v1 Announce Type: cross 
Abstract: Structural health monitoring (SHM) strategies involve the processing of structural response data to indirectly assess an asset's condition. These strategies can be enhanced for a group of structures, especially when they are similar, since mutual underlying physics are expected to exist. The concept behind population-based SHM exploits the sharing of data among individuals, so that data-rich members can support data-scarce ones. One approach to population-level modeling is the hierarchical Bayesian method, where the model is structured hierarchically in terms of its parameters, and correlation among learning tasks is enabled by conditioning on shared latent variables.
  This work investigates the application of a hierarchical Bayesian model to infer expected distributions of deflection amplitudes at both the population and domain levels, with the aim of detecting excessive initial deflections in a population of plate elements. Although these damages are typically localized, they can trigger unexpected events, if not properly monitored. The work is conducted in a numerical setting using a Finite Element model to generate strain response data, which serve as the monitoring data. Bayesian inference was conducted using Markov Chain Monte Carlo (MCMC), with a surrogate model employed to calculate the likelihood function. The hierarchical approach was compared to an independent model for a plate component with few data. The results revealed that, under data sparsity conditions, the hierarchical model can offer more robust results in terms of uncertainty, which is essential for decision-making tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16316v1</guid>
      <category>stat.AP</category>
      <category>cs.CE</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Georgios Aravanis (Politecnico di Milano, Italy), Nicholas Silionis (National Technical University of Athens, Greece), Jacopo Bardiani (Politecnico di Milano, Italy), Marco Giglio (Politecnico di Milano, Italy), Konstantinos Anyfantis (National Technical University of Athens, Greece), Claudio Sbarufatti (Politecnico di Milano, Italy)</dc:creator>
    </item>
    <item>
      <title>Foundation and Large-Scale AI Models in Neuroscience: A Comprehensive Review</title>
      <link>https://arxiv.org/abs/2510.16658</link>
      <description>arXiv:2510.16658v1 Announce Type: cross 
Abstract: The advent of large-scale artificial intelligence (AI) models has a transformative effect on neuroscience research, which represents a paradigm shift from the traditional computational methods through the facilitation of end-to-end learning from raw brain signals and neural data. In this paper, we explore the transformative effects of large-scale AI models on five major neuroscience domains: neuroimaging and data processing, brain-computer interfaces and neural decoding, molecular neuroscience and genomic modeling, clinical assistance and translational frameworks, and disease-specific applications across neurological and psychiatric disorders. These models are demonstrated to address major computational neuroscience challenges, including multimodal neural data integration, spatiotemporal pattern interpretation, and the derivation of translational frameworks for clinical deployment. Moreover, the interaction between neuroscience and AI has become increasingly reciprocal, as biologically informed architectural constraints are now incorporated to develop more interpretable and computationally efficient models. This review highlights both the notable promise of such technologies and key implementation considerations, with particular emphasis on rigorous evaluation frameworks, effective domain knowledge integration, and comprehensive ethical guidelines for clinical use. Finally, a systematic listing of critical neuroscience datasets used to derive and validate large-scale AI models across diverse research applications is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16658v1</guid>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shihao Yang, Xiying Huang, Danilo Bernardo, Jun-En Ding, Andrew Michael, Jingmei Yang, Patrick Kwan, Ashish Raj, Feng Liu</dc:creator>
    </item>
    <item>
      <title>FinSight: Towards Real-World Financial Deep Research</title>
      <link>https://arxiv.org/abs/2510.16844</link>
      <description>arXiv:2510.16844v1 Announce Type: cross 
Abstract: Generating professional financial reports is a labor-intensive and intellectually demanding process that current AI systems struggle to fully automate. To address this challenge, we introduce FinSight (Financial InSight), a novel multi agent framework for producing high-quality, multimodal financial reports. The foundation of FinSight is the Code Agent with Variable Memory (CAVM) architecture, which unifies external data, designed tools, and agents into a programmable variable space, enabling flexible data collection, analysis and report generation through executable code. To ensure professional-grade visualization, we propose an Iterative Vision-Enhanced Mechanism that progressively refines raw visual outputs into polished financial charts. Furthermore, a two stage Writing Framework expands concise Chain-of-Analysis segments into coherent, citation-aware, and multimodal reports, ensuring both analytical depth and structural consistency. Experiments on various company and industry-level tasks demonstrate that FinSight significantly outperforms all baselines, including leading deep research systems in terms of factual accuracy, analytical depth, and presentation quality, demonstrating a clear path toward generating reports that approach human-expert quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16844v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jiajie Jin, Yuyao Zhang, Yimeng Xu, Hongjin Qian, Yutao Zhu, Zhicheng Dou</dc:creator>
    </item>
    <item>
      <title>Explainable Heterogeneous Anomaly Detection in Financial Networks via Adaptive Expert Routing</title>
      <link>https://arxiv.org/abs/2510.17088</link>
      <description>arXiv:2510.17088v1 Announce Type: cross 
Abstract: Financial anomalies exhibit heterogeneous mechanisms (price shocks, liquidity freezes, contagion cascades, regime shifts), but existing detectors treat all anomalies uniformly, producing scalar scores without revealing which mechanism is failing, where risks concentrate, or how to intervene. This opacity prevents targeted regulatory responses. Three unsolved challenges persist: (1) static graph structures cannot adapt when market correlations shift during regime changes; (2) uniform detection mechanisms miss type-specific signatures across multiple temporal scales while failing to integrate individual behaviors with network contagion; (3) black-box outputs provide no actionable guidance on anomaly mechanisms or their temporal evolution.
  We address these via adaptive graph learning with specialized expert networks that provide built-in interpretability. Our framework captures multi-scale temporal dependencies through BiLSTM with self-attention, fuses temporal and spatial information via cross-modal attention, learns dynamic graphs through neural multi-source interpolation, adaptively balances learned dynamics with structural priors via stress-modulated fusion, routes anomalies to four mechanism-specific experts, and produces dual-level interpretable attributions. Critically, interpretability is embedded architecturally rather than applied post-hoc.
  On 100 US equities (2017-2024), we achieve 92.3% detection of 13 major events with 3.8-day lead time, outperforming best baseline by 30.8pp. Silicon Valley Bank case study demonstrates anomaly evolution tracking: Price-Shock expert weight rose to 0.39 (33% above baseline 0.29) during closure, peaking at 0.48 (66% above baseline) one week later, revealing automatic temporal mechanism identification without labeled supervision.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17088v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zan Li, Rui Fan</dc:creator>
    </item>
    <item>
      <title>Physics-Informed Large Language Models for HVAC Anomaly Detection with Autonomous Rule Generation</title>
      <link>https://arxiv.org/abs/2510.17146</link>
      <description>arXiv:2510.17146v1 Announce Type: cross 
Abstract: Heating, Ventilation, and Air-Conditioning (HVAC) systems account for a substantial share of global building energy use, making reliable anomaly detection essential for improving efficiency and reducing emissions. Classical rule-based approaches offer explainability but lack adaptability, while deep learning methods provide predictive power at the cost of transparency, efficiency, and physical plausibility. Recent attempts to use Large Language Models (LLMs) for anomaly detection improve interpretability but largely ignore the physical principles that govern HVAC operations. We present PILLM, a Physics-Informed LLM framework that operates within an evolutionary loop to automatically generate, evaluate, and refine anomaly detection rules. Our approach introduces physics-informed reflection and crossover operators that embed thermodynamic and control-theoretic constraints, enabling rules that are both adaptive and physically grounded. Experiments on the public Building Fault Detection dataset show that PILLM achieves state-of-the-art performance while producing diagnostic rules that are interpretable and actionable, advancing trustworthy and deployable AI for smart building systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17146v1</guid>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Subin Lin, Chuanbo Hua</dc:creator>
    </item>
    <item>
      <title>TwinMarket: A Scalable Behavioral and Social Simulation for Financial Markets</title>
      <link>https://arxiv.org/abs/2502.01506</link>
      <description>arXiv:2502.01506v5 Announce Type: replace 
Abstract: The study of social emergence has long been a central focus in social science. Traditional modeling approaches, such as rule-based Agent-Based Models (ABMs), struggle to capture the diversity and complexity of human behavior, particularly the irrational factors emphasized in behavioral economics. Recently, large language model (LLM) agents have gained traction as simulation tools for modeling human behavior in social science and role-playing applications. Studies suggest that LLMs can account for cognitive biases, emotional fluctuations, and other non-rational influences, enabling more realistic simulations of socio-economic dynamics. In this work, we introduce TwinMarket, a novel multi-agent framework that leverages LLMs to simulate socio-economic systems. Specifically, we examine how individual behaviors, through interactions and feedback mechanisms, give rise to collective dynamics and emergent phenomena. Through experiments in a simulated stock market environment, we demonstrate how individual actions can trigger group behaviors, leading to emergent outcomes such as financial bubbles and recessions. Our approach provides valuable insights into the complex interplay between individual decision-making and collective socio-economic patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01506v5</guid>
      <category>cs.CE</category>
      <category>cs.CY</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuzhe Yang, Yifei Zhang, Minghao Wu, Kaidi Zhang, Yunmiao Zhang, Honghai Yu, Yan Hu, Benyou Wang</dc:creator>
    </item>
    <item>
      <title>LRQ-Solver: A Transformer-Based Neural Operator for Fast and Accurate Solving of Large-scale 3D PDEs</title>
      <link>https://arxiv.org/abs/2510.11636</link>
      <description>arXiv:2510.11636v2 Announce Type: replace 
Abstract: Solving large-scale Partial Differential Equations (PDEs) on complex three-dimensional geometries represents a central challenge in scientific and engineering computing, often impeded by expensive pre-processing stages and substantial computational overhead. We introduce Low-Rank Query-based PDE Solver (LRQ-Solver), a physics-integrated framework engineered for rapid, accurate, and highly scalable simulations of industrial-grade models. This framework is built upon two primary technical innovations. First, our Parameter Conditioned Lagrangian Modeling (PCLM) approach explicitly couples local physical states with global design parameters, enabling robust predictions across varied simulation configurations. By embedding physical consistency directly into the learning architecture, PCLM ensures that predictions remain physically meaningful even under unseen design conditions, significantly enhancing generalization and reliability. Second, the Low-Rank Query Attention (LR-QA) module leverages the second-order statistics of physical fields to construct a global coherence kernel, reducing the computational complexity of attention from O(N2) to O(NC2 + C3). By replacing point-wise clustering with covariance decomposition, LRQ-Solver achieves exceptional scalability efficiently processing up to 2 million points on a single GPU. Validated on standard benchmarks, LRQ-Solver achieves a 38.9% error reduction on the DrivAerNet++ dataset and 28.76% on the 3D Beam dataset, alongside a training speedup of up to 50 times. Our results establish that LRQ-Solver offers a powerful paradigm for multi-configuration physics simulations, delivering a SOTA combination of accuracy, scalability, and efficiency. Code to reproduce the experiments is available at https://github.com/LilaKen/LRQ-Solver.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11636v2</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peijian Zeng, Guan Wang, Haohao Gu, Xiaoguang Hu, Tiezhu Gao, Zhuowei Wang, Aimin Yang, Xiaoyu Song</dc:creator>
    </item>
    <item>
      <title>Multiscale Neural PDE Surrogates for Prediction and Downscaling: Application to Ocean Currents</title>
      <link>https://arxiv.org/abs/2507.18067</link>
      <description>arXiv:2507.18067v2 Announce Type: replace-cross 
Abstract: Accurate modeling of physical systems governed by partial differential equations is a central challenge in scientific computing. In oceanography, high-resolution current data are critical for coastal management, environmental monitoring, and maritime safety. However, available satellite products, such as Copernicus data for sea water velocity at ~0.08 degrees spatial resolution and global ocean models, often lack the spatial granularity required for detailed local analyses. In this work, we (a) introduce a supervised deep learning framework based on neural operators for solving PDEs and providing arbitrary resolution solutions, and (b) propose downscaling models with an application to Copernicus ocean current data. Additionally, our method can model surrogate PDEs and predict solutions at arbitrary resolution, regardless of the input resolution. We evaluated our model on real-world Copernicus ocean current data and synthetic Navier-Stokes simulation datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18067v2</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdessamad El-Kabid, Loubna Benabbou, Redouane Lguensat, Alex Hern\'andez-Garc\'ia</dc:creator>
    </item>
    <item>
      <title>Epistemic Trade-Off: An Analysis of the Operational Breakdown and Ontological Limits of "Certainty-Scope" in AI</title>
      <link>https://arxiv.org/abs/2508.19304</link>
      <description>arXiv:2508.19304v2 Announce Type: replace-cross 
Abstract: The recently published "certainty-scope" conjecture offers a compelling insight into the inherent trade-off present within artificial intelligence (AI) systems. As general research, this investigation remains vital as a philosophical undertaking and a potential guide for directing AI investments, design, and deployment, especially in safety-critical and mission-critical domains where risk levels are substantially elevated. While maintaining intellectual coherence, its formalization ultimately consolidates this insight into a suspended epistemic truth, which resists operational implementation within practical systems. This paper argues that the conjecture's objective to furnish insights for engineering design and regulatory decision-making is limited by two fundamental factors: first, its dependence on incomputable constructs and its failure to capture the generality factors of AI, rendering it practically unimplementable and unverifiable; second, its foundational ontological assumption of AI systems as self-contained epistemic entities, distancing it from the complex and dynamic socio-technical environments where knowledge is co-constructed. We conclude that this dual breakdown - an epistemic closure deficit and an embeddedness bypass - hinders the conjecture's transition to a practical and actionable framework suitable for informing and guiding AI deployments. In response, we point towards a possible framing of the epistemic challenge, emphasizing the inherent epistemic burdens of AI within complex human-centric domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19304v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Generoso Immediato</dc:creator>
    </item>
    <item>
      <title>Programmable Cognitive Bias in Social Agents</title>
      <link>https://arxiv.org/abs/2509.13588</link>
      <description>arXiv:2509.13588v2 Announce Type: replace-cross 
Abstract: This paper introduces CoBRA, a novel toolkit for systematically specifying agent behavior in LLM-based social simulation. We found that conventional approaches that specify agent behaviors through implicit natural language descriptions cannot yield consistent behaviors across models, and the produced agent behaviors do not capture the nuances of the descriptions. In contrast, CoBRA presents a new approach to program agents' cognitive biases explicitly, by grounding agents' expected behaviors using classic social science experiments. CoBRA has two components: (1) Cognitive Bias Index that measures the cognitive bias of a social agent, by quantifying the agent's reactions in a set of validated classical social science experiments; (2) Behavioral Regulation Engine that aligns the agent's behavior to demonstrate controlled cognitive bias. We evaluated CoBRA as an HCI toolkit through demonstration and technical benchmarks. Our results suggest that CoBRA can precisely program the cognitive bias demonstrated in a social agent in a model-agnostic manner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13588v2</guid>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.CY</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuan Liu, Haoyang Shang, Haojian Jin</dc:creator>
    </item>
  </channel>
</rss>
