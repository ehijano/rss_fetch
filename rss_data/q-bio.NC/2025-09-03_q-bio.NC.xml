<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 04 Sep 2025 01:30:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Meta-learning ecological priors from large language models explains human learning and decision making</title>
      <link>https://arxiv.org/abs/2509.00116</link>
      <description>arXiv:2509.00116v2 Announce Type: new 
Abstract: Human cognition is profoundly shaped by the environments in which it unfolds. Yet, it remains an open question whether learning and decision making can be explained as a principled adaptation to the statistical structure of real-world tasks. We introduce ecologically rational analysis, a computational framework that unifies the normative foundations of rational analysis with ecological grounding. Leveraging large language models to generate ecologically valid cognitive tasks at scale, and using meta-learning to derive rational models optimized for these environments, we develop a new class of learning algorithms: Ecologically Rational Meta-learned Inference (ERMI). ERMI internalizes the statistical regularities of naturalistic problem spaces and adapts flexibly to novel situations, without requiring hand-crafted heuristics or explicit parameter updates. We show that ERMI captures human behavior across 15 experiments spanning function learning, category learning, and decision making, outperforming several established cognitive models in trial-by-trial prediction. Our results suggest that much of human cognition may reflect adaptive alignment to the ecological structure of the problems we encounter in everyday life.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00116v2</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akshay K. Jagadish, Mirko Thalmann, Julian Coda-Forno, Marcel Binz, Eric Schulz</dc:creator>
    </item>
    <item>
      <title>Sleep Disorder Diagnosis Using EEG Signals and LSTM Deep Learning Method</title>
      <link>https://arxiv.org/abs/2509.00208</link>
      <description>arXiv:2509.00208v1 Announce Type: new 
Abstract: Diagnosing sleep disorders is an important focus in neuroscience and engineering, as these conditions involve issues such as insufficient sleep, frequent awakenings, and difficulty reaching deep sleep. Accurate detection based on brain signals, particularly electroencephalography (EEG), enables development of personalized treatments. While statistical pattern recognition was once the standard for analyzing EEG, deep learning has become the dominant approach. In this study, we analyzed a public database of 197 full night sleep recordings from participants aged 25-101 years. After preprocessing and feature extraction, Long Short-Term Memory (LSTM) neural networks achieved 93.3% accuracy in distinguishing healthy from disordered sleep, which improved to 95% with fusion techniques. The models were also computationally efficient, suggesting strong clinical potential for rapid and precise sleep disorder diagnosis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00208v1</guid>
      <category>q-bio.NC</category>
      <category>q-bio.QM</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Reza Yousefi, Reza Rahimi</dc:creator>
    </item>
    <item>
      <title>Integrated information and predictive processing theories of consciousness: An adversarial collaborative review</title>
      <link>https://arxiv.org/abs/2509.00555</link>
      <description>arXiv:2509.00555v1 Announce Type: new 
Abstract: As neuroscientific theories of consciousness continue to proliferate, the need to assess their similarities and differences -- as well as their predictive and explanatory power -- becomes ever more pressing. Recently, a number of structured adversarial collaborations have been devised to test the competing predictions of several candidate theories of consciousness. In this review, we compare and contrast three theories being investigated in one such adversarial collaboration: Integrated Information Theory, Neurorepresentationalism, and Active Inference. We begin by presenting the core claims of each theory, before comparing them in terms of (1) the phenomena they seek to explain, (2) the sorts of explanations they avail, and (3) the methodological strategies they endorse. We then consider some of the inherent challenges of theory testing, and how adversarial collaboration addresses some of these difficulties. More specifically, we outline the key hypotheses that will be tested in this adversarial collaboration, and exemplify how contrasting empirical predictions may pertain to core and auxiliary components of each theory. Finally, we discuss how the data harvested across disparate experiments (and their replicates) may be formally integrated to provide a quantitative measure of the evidential support accrued under each theory. We suggest this approach to theory comparison may afford a useful metric for tracking the amount of scientific progress being made in consciousness research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00555v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Andrew W. Corcoran, Andrew M. Haun, Reinder Dorman, Giulio Tononi, Karl J. Friston, Cyriel M. A. Pennartz,  TWCF,  :, INTREPID Consortium</dc:creator>
    </item>
    <item>
      <title>Response function as a quantitative measure of consciousness in brain dynamics</title>
      <link>https://arxiv.org/abs/2509.00730</link>
      <description>arXiv:2509.00730v1 Announce Type: new 
Abstract: Understanding the neural correlates of consciousness remains a central challenge in neuroscience. In this study, we investigate the relationship between consciousness and neural responsiveness by analyzing intracranial ECoG recordings from non-human primates across three distinct states: wakefulness, anesthesia, and recovery. Using a nonequilibrium recurrent neural network (RNN) model, we fit state-dependent cortical dynamics to extract the neural response function as a dynamics complexity indicator. Our findings demonstrate that the amplitude of the neural response function serves as a robust dynamical indicator of conscious state, consistent with the role of a linear response function in statistical physics. Notably, this aligns with our previous theoretical results showing that the response function in RNNs peaks near the transition between ordered and chaotic regimes -- highlighting criticality as a potential principle for sustaining flexible and responsive cortical dynamics. Empirically, we find that during wakefulness, neural responsiveness is strong, widely distributed, and consistent with rich nonequilibrium fluctuations. Under anesthesia, response amplitudes are significantly suppressed, and the network dynamics become more chaotic, indicating a loss of dynamical sensitivity. During recovery, the neural response function is elevated, supporting the gradual re-establishment of flexible and responsive activity that parallels the restoration of conscious processing. Our work suggests that a robust, brain-state-dependent neural response function may be a necessary dynamical condition for consciousness, providing a principled framework for quantifying levels of consciousness in terms of nonequilibrium responsiveness in the brain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00730v1</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.stat-mech</category>
      <category>nlin.CD</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenkang Du, Haiping Huang</dc:creator>
    </item>
    <item>
      <title>Retinal processing of natural scenes : challenges ahead</title>
      <link>https://arxiv.org/abs/2509.00939</link>
      <description>arXiv:2509.00939v1 Announce Type: new 
Abstract: While a great deal is known about the way the retina processes simple stimuli, our understanding of how the retina processes natural stimuli is still limited. Here we highlight some of the challenges that remain to be addressed to understand retinal processing of natural stimuli and describe emerging research avenues to overcome them.
  A key issue is model complexity. When complexifying the probing stimuli towards natural stimuli, the number of parameters required in models of retinal computations increases, raising issues of overfitting, generalization, and interpretability. This increase in complexity is also a challenge for normative approaches as it makes it difficult to derive non-linear retinal computations from simple principles.
  We describe two types of approaches that may help circumvent this issue in the future. First, we propose that a new form of reductionism is emerging: instead of breaking down natural stimuli into sums of simpler stimuli, it becomes possible to 'divide and conquer' natural scenes into different visual inputs corresponding to different visual tasks, in order to study retinal computations separately for each of these visual tasks. Second, several studies suggest that it will soon be possible to mitigate the issue of complexity, by 'embodying' the models with more biological constraints, in particularly those derived from connectomic studies. Together, these approaches may offer a powerful strategy to move beyond current limitations and advance our understanding of how the retina processes natural visual environments, and suggest approaches that could be used beyond, for other sensory areas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00939v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuele Virgili, Olivier Marre</dc:creator>
    </item>
    <item>
      <title>Computational Modeling for Personalized Transcranial Electrical Stimulation: Theory, Tools, and Applications</title>
      <link>https://arxiv.org/abs/2509.01192</link>
      <description>arXiv:2509.01192v1 Announce Type: new 
Abstract: Objective. Personalized transcranial electrical stimulation (tES) has gained growing attention due to the substantial inter-individual variability in brain anatomy and physiology. While previous reviews have discussed the physiological mechanisms and clinical applications of tES, there remains a critical gap in up-to-date syntheses focused on the computational modeling frameworks that enable individualized stimulation optimization. Approach. This review presents a comprehensive overview of recent advances in computational techniques supporting personalized tES. We systematically examine developments in forward modeling for simulating individualized electric fields, as well as inverse modeling approaches for optimizing stimulation parameters. We critically evaluate progress in head modeling pipelines, optimization algorithms, and the integration of multimodal brain data. Main results. Recent advances have substantially accelerated the construction of subject-specific head conductor models and expanded the landscape of optimization methods, including multi-objective optimization and brain network-informed optimization. These advances allow for dynamic and individualized stimulation planning, moving beyond empirical trial-and-error approaches.Significance. By integrating the latest developments in computational modeling for personalized tES, this review highlights current challenges, emerging opportunities, and future directions for achieving precision neuromodulation in both research and clinical contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01192v1</guid>
      <category>q-bio.NC</category>
      <category>cs.CE</category>
      <category>cs.NE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mo Wang, Kexin Zheng, Yiling Liu, Huichun Luo, Tifei Yuan, Hongkai Wen, Pengfei Wei, Quanying Liu</dc:creator>
    </item>
    <item>
      <title>Automatic Screening of Parkinson's Disease from Visual Explorations</title>
      <link>https://arxiv.org/abs/2509.01326</link>
      <description>arXiv:2509.01326v1 Announce Type: new 
Abstract: Eye movements can reveal early signs of neurodegeneration, including those associated with Parkinson's Disease (PD). This work investigates the utility of a set of gaze-based features for the automatic screening of PD from different visual exploration tasks. For this purpose, a novel methodology is introduced, combining classic fixation/saccade oculomotor features (e.g., saccade count, fixation duration, scanned area) with features derived from gaze clusters (i.e., regions with a considerable accumulation of fixations). These features are automatically extracted from six exploration tests and evaluated using different machine learning classifiers. A Mixture of Experts ensemble is used to integrate outputs across tests and both eyes. Results show that ensemble models outperform individual classifiers, achieving an Area Under the Receiving Operating Characteristic Curve (AUC) of 0.95 on a held-out test set. The findings support visual exploration as a non-invasive tool for early automatic screening of PD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01326v1</guid>
      <category>q-bio.NC</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maria F. Alcala-Durand, J. Camilo Puerta-Acevedo, Juli\'an D. Arias-Londo\~no, Juan I. Godino-Llorente</dc:creator>
    </item>
    <item>
      <title>DCA: Graph-Guided Deep Embedding Clustering for Brain Atlases</title>
      <link>https://arxiv.org/abs/2509.01426</link>
      <description>arXiv:2509.01426v1 Announce Type: new 
Abstract: Brain atlases are essential for reducing the dimensionality of neuroimaging data and enabling interpretable analysis. However, most existing atlases are predefined, group-level templates with limited flexibility and resolution. We present Deep Cluster Atlas (DCA), a graph-guided deep embedding clustering framework for generating individualized, voxel-wise brain parcellations. DCA combines a pretrained autoencoder with spatially regularized deep clustering to produce functionally coherent and spatially contiguous regions. Our method supports flexible control over resolution and anatomical scope, and generalizes to arbitrary brain structures. We further introduce a standardized benchmarking platform for atlas evaluation, using multiple large-scale fMRI datasets. Across multiple datasets and scales, DCA outperforms state-of-the-art atlases, improving functional homogeneity by 98.8\% and silhouette coefficient by 29\%, and achieves superior performance in downstream tasks such as autism diagnosis and cognitive decoding. Codes and models will be released soon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01426v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mo Wang, Kaining Peng, Jingsheng Tang, Hongkai Wen, Quanying Liu</dc:creator>
    </item>
    <item>
      <title>Context dependent adaptation in a neural computation</title>
      <link>https://arxiv.org/abs/2509.01760</link>
      <description>arXiv:2509.01760v1 Announce Type: new 
Abstract: Brains adapt to the statistical structure of their input. In the visual system, local light intensities change rapidly, the variance of the intensity changes more slowly, and the dynamic range of contrast itself changes more slowly still. We use a motion-sensitive neuron in the fly visual system to probe this hierarchy of adaptation phenomena, delivering naturalistic stimuli that have been simplified to have a clear separation of time scales. We show that the neural response to visual motion depends on contrast, and this dependence itself varies with context. Using the spike-triggered average velocity trajectory as a response measure, we find that context dependence is confined to a low-dimensional space, with a single dominant dimension. Across a wide range of conditions this adaptation serves to match the integration time to the mean interval between spikes, reducing redundancy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01760v1</guid>
      <category>q-bio.NC</category>
      <category>physics.bio-ph</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charles J. Edelson, Sima Setayeshgar, William Bialek, Rob R. de Ruyter van Steveninck</dc:creator>
    </item>
    <item>
      <title>On sources to variabilities of simple cells in the primary visual cortex: A principled theory for the interaction between geometric image transformations and receptive field responses</title>
      <link>https://arxiv.org/abs/2509.02139</link>
      <description>arXiv:2509.02139v2 Announce Type: new 
Abstract: This paper gives an overview of a theory for modelling the interaction between geometric image transformations and receptive field responses for a visual observer that views objects and spatio-temporal events in the environment. This treatment is developed over combinations of (i) uniform spatial scaling transformations, (ii) spatial affine transformations, (iii) Galilean transformations and (iv) temporal scaling transformations.
  By postulating that the family of receptive fields should be covariant under these classes of geometric image transformations, it follows that the receptive field shapes should be expanded over the degrees of freedom of the corresponding image transformations, to enable a formal matching between the receptive field responses computed under different viewing conditions for the same scene or for a structurally similar spatio-temporal event.
  We conclude the treatment by discussing and providing potential support for a working hypothesis that the receptive fields of simple cells in the primary visual cortex ought to be covariant under these classes of geometric image transformations, and thus have the shapes of their receptive fields expanded over the degrees of freedom of the corresponding geometric image transformations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02139v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tony Lindeberg</dc:creator>
    </item>
    <item>
      <title>Improving Electroencephalogram-Based Deception Detection in Concealed Information Test under Low Stimulus Heterogeneity</title>
      <link>https://arxiv.org/abs/2509.02234</link>
      <description>arXiv:2509.02234v1 Announce Type: new 
Abstract: The concealed information test (CIT) is widely used for detecting deception in criminal investigations, primarily leveraging the P300 component of electroencephalogram (EEG) signals. However, the traditional bootstrapped amplitude difference (BAD) method struggles to accurately differentiate deceptive individuals from innocent ones when irrelevant stimuli carry familiarity or inherent meaning, thus limiting its practical applicability in real-world investigations. This study aimed to enhance the deception detection capability of the P300-based CIT, particularly under conditions of low stimulus heterogeneity. To closely simulate realistic investigative scenarios, we designed a realistic mock-crime setup in which participants were familiarized with all CIT stimuli except the target stimulus. EEG data acquired during CIT sessions were analyzed using the BAD method, machine learning algorithms, and deep learning (DL) methods (ShallowNet and EEGNet). Among these techniques, EEGNet demonstrated the highest deception detection accuracy at 86.67%, when employing our proposed data augmentation approach. Overall, DL methods could significantly improve the accuracy of deception detection under challenging conditions of low stimulus heterogeneity by effectively capturing subtle cognitive responses not accessible through handcrafted features. To the best of our knowledge, this is the first study that employed DL approaches for subject-independent deception classification using the CIT paradigm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02234v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Suhye Kim, Jaehoon Cheon, Taehee Kim, Seok Chan Kim, Chang-Hwan Im</dc:creator>
    </item>
    <item>
      <title>Self-organized learning emerges from coherent coupling of critical neurons</title>
      <link>https://arxiv.org/abs/2509.00107</link>
      <description>arXiv:2509.00107v1 Announce Type: cross 
Abstract: Deep artificial neural networks have surpassed human-level performance across a diverse array of complex learning tasks, establishing themselves as indispensable tools in both social applications and scientific research.
  Despite these advances, the underlying mechanisms of training in artificial neural networks remain elusive.
  Here, we propose that artificial neural networks function as adaptive, self-organizing information processing systems in which training is mediated by the coherent coupling of strongly activated, task-specific critical neurons.
  We demonstrate that such neuronal coupling gives rise to Hebbian-like neural correlation graphs, which undergo a dynamic, second-order connectivity phase transition during the initial stages of training.
  Concurrently, the connection weights among critical neurons are consistently reinforced while being simultaneously redistributed in a stochastic manner.
  As a result, a precise balance of neuronal contributions is established, inducing a local concentration within the random loss landscape which provides theoretical explanation for generalization capacity.
  We further identify a later on convergence phase transition characterized by a phase boundary in hyperparameter space, driven by the nonequilibrium probability flux through weight space.
  The critical computational graphs resulting from coherent coupling also decode the predictive rules learned by artificial neural networks, drawing analogies to avalanche-like dynamics observed in biological neural circuits.
  Our findings suggest that the coherent coupling of critical neurons and the ensuing local concentration within the loss landscapes may represent universal learning mechanisms shared by both artificial and biological neural computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00107v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chuanbo Liu, Jin Wang</dc:creator>
    </item>
    <item>
      <title>PyNoetic: A modular python framework for no-code development of EEG brain-computer interfaces</title>
      <link>https://arxiv.org/abs/2509.00670</link>
      <description>arXiv:2509.00670v1 Announce Type: cross 
Abstract: Electroencephalography (EEG)-based Brain-Computer Interfaces (BCIs) have emerged as a transformative technology with applications spanning robotics, virtual reality, medicine, and rehabilitation. However, existing BCI frameworks face several limitations, including a lack of stage-wise flexibility essential for experimental research, steep learning curves for researchers without programming expertise, elevated costs due to reliance on proprietary software, and a lack of all-inclusive features leading to the use of multiple external tools affecting research outcomes. To address these challenges, we present PyNoetic, a modular BCI framework designed to cater to the diverse needs of BCI research. PyNoetic is one of the very few frameworks in Python that encompasses the entire BCI design pipeline, from stimulus presentation and data acquisition to channel selection, filtering, feature extraction, artifact removal, and finally simulation and visualization. Notably, PyNoetic introduces an intuitive and end-to-end GUI coupled with a unique pick-and-place configurable flowchart for no-code BCI design, making it accessible to researchers with minimal programming experience. For advanced users, it facilitates the seamless integration of custom functionalities and novel algorithms with minimal coding, ensuring adaptability at each design stage. PyNoetic also includes a rich array of analytical tools such as machine learning models, brain-connectivity indices, systematic testing functionalities via simulation, and evaluation methods of novel paradigms. PyNoetic's strengths lie in its versatility for both offline and real-time BCI development, which streamlines the design process, allowing researchers to focus on more intricate aspects of BCI development and thus accelerate their research endeavors. Project Website: https://neurodiag.github.io/PyNoetic</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00670v1</guid>
      <category>eess.SP</category>
      <category>cs.HC</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1371/journal.pone.0327791</arxiv:DOI>
      <arxiv:journal_reference>PLoS One 20.8 (2025): e0327791</arxiv:journal_reference>
      <dc:creator>Gursimran Singh, Aviral Chharia, Rahul Upadhyay, Vinay Kumar, Luca Longo</dc:creator>
    </item>
    <item>
      <title>IMU-Enhanced EEG Motion Artifact Removal with Fine-Tuned Large Brain Models</title>
      <link>https://arxiv.org/abs/2509.01073</link>
      <description>arXiv:2509.01073v1 Announce Type: cross 
Abstract: Electroencephalography (EEG) is a non-invasive method for measuring brain activity with high temporal resolution; however, EEG signals often exhibit low signal-to-noise ratios because of contamination from physiological and environmental artifacts. One of the major challenges hindering the real-world deployment of brain-computer interfaces (BCIs) involves the frequent occurrence of motion-related EEG artifacts. Most prior studies on EEG motion artifact removal rely on single-modality approaches, such as Artifact Subspace Reconstruction (ASR) and Independent Component Analysis (ICA), without incorporating simultaneously recorded modalities like inertial measurement units (IMUs), which directly capture the extent and dynamics of motion. This work proposes a fine-tuned large brain model (LaBraM)-based correlation attention mapping method that leverages spatial channel relationships in IMU data to identify motion-related artifacts in EEG signals. The fine-tuned model contains approximately 9.2 million parameters and uses 5.9 hours of EEG and IMU recordings for training, just 0.2346\% of the 2500 hours used to train the base model. We compare our results against the established ASR-ICA benchmark across varying time scales and motion activities, showing that incorporating IMU reference signals significantly improves robustness under diverse motion scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01073v1</guid>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuhong Zhang, Xusheng Zhu, Yuchen Xu, ChiaEn Lu, Hsinyu Shih, Gert Cauwenberghs, Tzyy-Ping Jung</dc:creator>
    </item>
    <item>
      <title>Intermittent localization and fast spatial learning by non-Markov random walks with decaying memory</title>
      <link>https://arxiv.org/abs/2509.01806</link>
      <description>arXiv:2509.01806v2 Announce Type: cross 
Abstract: Random walks on lattices with preferential relocation to previously visited sites provide a simple modeling of the displacements of animals and humans. When the lattice contains a single impurity or resource site where the walker spends more time on average at each visit than on the other sites, the long range memory can suppress diffusion and induce by reinforcement a steady state localized around the resource. This phenomenon can be identified with a spatial learning process by the walker. Here we study theoretically and numerically how the decay of memory impacts learning in these models. If memory decays as $1/\tau$ or slower, where $\tau$ is the time backward into the past, the localized solutions are the same as with perfect, non-decaying memory and they are linearly stable. If forgetting is faster than $1/\tau$, for instance exponential, an unusual regime of intermittent localization is observed, where well localized periods of exponentially distributed duration are interspersed with intervals of diffusive motion. At the transition between the two regimes, for a kernel in $1/\tau$, the approach to the stable localized state is the fastest, opposite to the expected critical slowing down effect. Hence forgetting can allow the walker to save memory without compromising learning and to achieve a faster learning. These findings agree with biological evidence on the benefits of forgetting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01806v2</guid>
      <category>cond-mat.stat-mech</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paulina R. Mart\'in-Cornejo, Denis Boyer</dc:creator>
    </item>
    <item>
      <title>Control of Functional Connectivity in Cerebral Cortex by Basal Ganglia Mediated Synchronization</title>
      <link>https://arxiv.org/abs/1708.00779</link>
      <description>arXiv:1708.00779v3 Announce Type: replace 
Abstract: Since the earliest electroencephalography experiments, large scale oscillations have been observed in the mammalian brain. More recently, episodes of oscillation and bursting have been identified not only in the cerebral cortex and thalamus, but pervasively in the healthy basal ganglia. The basal ganglia mediated synchronization model, introduced here, implicates these episodes in the integration of stimulus-response and reinforcement mechanisms in the basal ganglia with cortical association mechanisms. In so doing, the model helps explain how oscillations and synchrony are functionally central, and in particular, how they organize neural activity to exploit the selectivity of coincidence detectors in cortex and beyond. In the core mechanism of the model, salient spatiotemporal activity patterns in cortex are selectively focused by and routed through the basal ganglia to the thalamus. Coherent thalamocortical activity patterns then project back to widely separated areas of cortex, where they establish and facilitate contextually appropriate functional connections, while disconnecting and inhibiting competing ones. Corticostriatal, striatopallidal, and striatonigral conduction delays are crucial to this mechanism. These delays are unusually long, and unusually varied, in arrangements that facilitate learning of useful time alignments and associated resonant frequencies. Other structural arrangements in the basal ganglia show further specialization for this role, with convergence in the inputs from cortex, and divergence in many of the return paths to cortex, that systematically reflect corticocortical anatomical connectivity. The basal ganglia also target the dopaminergic, cholinergic, and serotonergic centers of the brainstem and basal forebrain, and the intralaminar and reticular nuclei of the thalamus, structures broadly implicated in the modulation of network activity and [...]</description>
      <guid isPermaLink="false">oai:arXiv.org:1708.00779v3</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Pouzzner</dc:creator>
    </item>
    <item>
      <title>Net2Brain: A Toolbox to compare artificial vision models with human brain responses</title>
      <link>https://arxiv.org/abs/2208.09677</link>
      <description>arXiv:2208.09677v3 Announce Type: replace-cross 
Abstract: We introduce Net2Brain, a graphical and command-line user interface toolbox for comparing the representational spaces of artificial deep neural networks (DNNs) and human brain recordings. While different toolboxes facilitate only single functionalities or only focus on a small subset of supervised image classification models, Net2Brain allows the extraction of activations of more than 600 DNNs trained to perform a diverse range of vision-related tasks (e.g semantic segmentation, depth estimation, action recognition, etc.), over both image and video datasets. The toolbox computes the representational dissimilarity matrices (RDMs) over those activations and compares them to brain recordings using representational similarity analysis (RSA), weighted RSA, both in specific ROIs and with searchlight search. In addition, it is possible to add a new data set of stimuli and brain recordings to the toolbox for evaluation. We demonstrate the functionality and advantages of Net2Brain with an example showcasing how it can be used to test hypotheses of cognitive computational neuroscience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.09677v3</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.3389/fninf.2025.1515873</arxiv:DOI>
      <arxiv:journal_reference>Front. Neuroinform. 19 (2025) 1515873</arxiv:journal_reference>
      <dc:creator>Domenic Bersch, Kshitij Dwivedi, Martina Vilas, Radoslaw M. Cichy, Gemma Roig</dc:creator>
    </item>
    <item>
      <title>Full-Head Segmentation of MRI with Abnormal Brain Anatomy: Model and Data Release</title>
      <link>https://arxiv.org/abs/2501.18716</link>
      <description>arXiv:2501.18716v2 Announce Type: replace-cross 
Abstract: Purpose: The goal of this work was to develop a deep network for whole-head segmentation including clinical MRIs with abnormal anatomy, and compile the first public benchmark dataset for this purpose. We collected 98 MRIs with volumetric segmentation labels for a diverse set of human subjects including normal, as well as abnormal anatomy in clinical cases of stroke and disorders of consciousness. Approach: Training labels were generated by manually correcting initial automated segmentations for skin/scalp, skull, CSF, gray matter, white matter, air cavity and extracephalic air. We developed a MultiAxial network consisting of three 2D U-Net that operate independently in sagittal, axial and coronal planes and are then combined to produce a single 3D segmentation. Results: The MultiAxial network achieved a test-set Dice scores of 0.88+-0.04 (median +- interquartile range) on whole head segmentation including gray and white matter. This compared to 0.86 +- 0.04 for Multipriors and 0.79 +- 0.10 for SPM12, two standard tools currently available for this task. The MultiAxial network gains in robustness by avoiding the need for coregistration with an atlas. It performed well in regions with abnormal anatomy and on images that have been de-identified. It enables more accurate and robust current flow modeling when incorporated into ROAST, a widely-used modeling toolbox for transcranial electric stimulation.Conclusions: We are releasing a new state-of-the-art tool for whole-head MRI segmentation in abnormal anatomy, along with the largest volume of labeled clinical head MRIs including labels for non-brain structures. Together the model and data may serve as a benchmark for future efforts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18716v2</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Andrew M Birnbaum, Adam Buchwald, Peter Turkeltaub, Adam Jacks, George Carra, Shreya Kannana, Yu Huang, Abhisheck Datta, Lucas C Parra, Lukas A Hirsch</dc:creator>
    </item>
    <item>
      <title>Toward Efficient Spiking Transformers: Synapse Pruning Meets Synergistic Learning-Based Compensation</title>
      <link>https://arxiv.org/abs/2508.01992</link>
      <description>arXiv:2508.01992v2 Announce Type: replace-cross 
Abstract: As a foundational architecture of artificial intelligence models, Transformer has been recently adapted to spiking neural networks with promising performance across various tasks. However, existing spiking Transformer (ST)-based models require a substantial number of parameters and incur high computational costs, thus limiting their deployment in resource-constrained environments. To address these challenges, we propose combining synapse pruning with a synergistic learning-based compensation strategy to derive lightweight ST-based models. Specifically, two types of tailored pruning strategies are introduced to reduce redundancy in the weight matrices of ST blocks: an unstructured $\mathrm{L_{1}P}$ method to induce sparse representations, and a structured DSP method to induce low-rank representations. In addition, we propose an enhanced spiking neuron model, termed the synergistic leaky integrate-and-fire (sLIF) neuron, to effectively compensate for model pruning through synergistic learning between synaptic and intrinsic plasticity mechanisms. Extensive experiments on benchmark datasets demonstrate that the proposed methods significantly reduce model size and computational overhead while maintaining competitive performance. These results validate the effectiveness of the proposed pruning and compensation strategies in constructing efficient and high-performing ST-based models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01992v2</guid>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongze Sun, Wuque Cai, Duo Chen, Shifeng Mao, Jiayi He, Zhenxing Wang, Dezhong Yao, Daqing Guo</dc:creator>
    </item>
  </channel>
</rss>
