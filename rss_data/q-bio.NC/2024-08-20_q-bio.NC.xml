<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 20 Aug 2024 04:00:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 20 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Comparison of Large Language Model and Human Performance on Random Number Generation Tasks</title>
      <link>https://arxiv.org/abs/2408.09656</link>
      <description>arXiv:2408.09656v1 Announce Type: cross 
Abstract: Random Number Generation Tasks (RNGTs) are used in psychology for examining how humans generate sequences devoid of predictable patterns. By adapting an existing human RNGT for an LLM-compatible environment, this preliminary study tests whether ChatGPT-3.5, a large language model (LLM) trained on human-generated text, exhibits human-like cognitive biases when generating random number sequences. Initial findings indicate that ChatGPT-3.5 more effectively avoids repetitive and sequential patterns compared to humans, with notably lower repeat frequencies and adjacent number frequencies. Continued research into different models, parameters, and prompting methodologies will deepen our understanding of how LLMs can more closely mimic human random generation behaviors, while also broadening their applications in cognitive and behavioral science research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09656v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rachel M. Harrison</dc:creator>
    </item>
    <item>
      <title>Prospective and retrospective coding in cortical neurons</title>
      <link>https://arxiv.org/abs/2405.14810</link>
      <description>arXiv:2405.14810v2 Announce Type: replace 
Abstract: Brains can process sensory information from different modalities at astonishing speed; this is surprising as the integration of inputs through the membrane of each individual neuron already causes a delayed response. Neuronal recordings {\em in vitro} reveal a possible explanation for this fast processing, in terms of individual neurons advancing their output firing rates with respect to the input, a concept which we refer to as prospective coding. The underlying mechanisms of prospective coding, however, are not completely understood. We propose a mechanistic explanation for individual neurons advancing their output on the level of single action potentials and instantaneous firing rates. We show that the spike generation mechanism can be the source for prospective (advanced) or retrospective (delayed) responses. A simplified Hodgkin-Huxley model identifies sodium inactivation as a source for prospective firing, controlling the timing of the neuron's output as a function of the voltage and its temporal derivative. We further show that slow adaptation processes, such as spike-frequency adaptation or deactivating dendritic currents, represent mechanisms generating prospective firing for inputs that undergo slow temporal modulations. In general, we show that adaptation processes at different time scales can cause advanced neuronal responses to time-varying inputs that are modulated on the corresponding time scales.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14810v2</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Simon Brandt, Mihai Alexandru Petrovici, Walter Senn, Katharina Anna Wilmes, Federico Benitez</dc:creator>
    </item>
    <item>
      <title>An Explicit Retinal Model of Color Vision Produces Center-Surround Phenomena and Implies That Center-Surround and Mutually Exclusive Colors Are Two Different Kinds of Color Opponency</title>
      <link>https://arxiv.org/abs/2406.16788</link>
      <description>arXiv:2406.16788v2 Announce Type: replace 
Abstract: Center-surround may be the best-known fact of retinal processing of information. Apparently, however, no explicit model has been proposed that shows how neurons can be connected to produce the center-surround phenomena. Here it is shown that a previous retinal model of color vision can produce the center-surround on-off phenomena of opponent colors. The model accomplishes this because the networks for the two ganglion cells of each opponent color pair are identical except that the cells' inputs from bipolar cells are reversed.
  The model was previously shown to generate several phenomena central to color vision. The model is only one of 16 similar models that produce all of the model's phenomena that were found before the discovery of the model's on-off color properties of center-surround architecture. It is shown here that the original model is the only one of the 16 that produces these phenomena. This supports the original model as the correct one for retinal architecture.
  The 16 similar models produce properties of local color vision by identifying the ordering of the strengths of the three cone classes' responses to a photostimulus. All 16 models produce the mutually exclusive color pairs as a by-product of identifying the ordering. Red or green is perceived when the L or M cone has the greatest absorption of photons, respectively. They cannot both be greatest. Blue or yellow is perceived when the L or S cone has the least absorption, respectively. They cannot both be least.
  The model's explanations of the opponent properties of mutually exclusive colors and center-surround on-off have nothing in common. The different explanations, and only one model that produces center-surround out of 16 that produce mutually exclusive colors, imply that mutually exclusive colors and center-surround on-off properties are two different kinds of color opponency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16788v2</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lane Yoder</dc:creator>
    </item>
    <item>
      <title>Decoding the human brain tissue response to radiofrequency excitation using a biophysical-model-free deep MRI on a chip framework</title>
      <link>https://arxiv.org/abs/2408.08376</link>
      <description>arXiv:2408.08376v2 Announce Type: replace-cross 
Abstract: Magnetic resonance imaging (MRI) relies on radiofrequency (RF) excitation of proton spin. Clinical diagnosis requires a comprehensive collation of biophysical data via multiple MRI contrasts, acquired using a series of RF sequences that lead to lengthy examinations. Here, we developed a vision transformer-based framework that captures the spatiotemporal magnetic signal evolution and decodes the brain tissue response to RF excitation, constituting an MRI on a chip. Following a per-subject rapid calibration scan (28.2 s), a wide variety of image contrasts including fully quantitative molecular, water relaxation, and magnetic field maps can be generated automatically. The method was validated across healthy subjects and a cancer patient in two different imaging sites, and proved to be 94% faster than alternative protocols. The deep MRI on a chip (DeepMonC) framework may reveal the molecular composition of the human brain tissue in a wide range of pathologies, while offering clinically attractive scan times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08376v2</guid>
      <category>physics.med-ph</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dinor Nagar (School of Electrical Engineering, Tel Aviv University, Tel Aviv, Israel), Moritz Zaiss (Institute of Neuroradiology, Friedrich-Alexander Universitat Erlangen-Nurnberg, Department of Artificial Intelligence in Biomedical Engineering, Friedrich-Alexander Universitat Erlangen-Nurnberg, Erlangen, Germany), Or Perlman (Department of Biomedical Engineering, Tel Aviv University, Tel Aviv, Israel, Sagol School of Neuroscience, Tel Aviv University, Tel Aviv, Israel)</dc:creator>
    </item>
  </channel>
</rss>
