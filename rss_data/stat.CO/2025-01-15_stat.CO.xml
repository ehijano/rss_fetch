<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 16 Jan 2025 02:28:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>fastrerandomize: An R Package for Fast Rerandomization Using Accelerated Computing</title>
      <link>https://arxiv.org/abs/2501.07642</link>
      <description>arXiv:2501.07642v1 Announce Type: new 
Abstract: The fastrerandomize R package provides hardware-accelerated tools for performing rerandomization and randomization testing in experimental research. Using a JAX backend, the package enables exact rerandomization inference even for large experiments with hundreds of billions of possible randomizations. Key functionalities include generating pools of acceptable rerandomizations based on covariate balance, conducting exact randomization tests, and performing pre-analysis evaluations to determine optimal rerandomization acceptance thresholds. Through batched processing and GPU acceleration, fastrerandomize achieves substantial performance gains compared to existing implementations, making previously intractable designs computationally feasible. The package therefore extends the randomization-based inference toolkit in R, allowing researchers to efficiently implement more stringent rerandomization designs and conduct valid inference even with large sample sizes or in high-dimensional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07642v1</guid>
      <category>stat.CO</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rebecca Goldstein, Connor T. Jerzak, Aniket Kamat, Fucheng Warren Zhu</dc:creator>
    </item>
    <item>
      <title>Fast sampling and model selection for Bayesian mixture models</title>
      <link>https://arxiv.org/abs/2501.07668</link>
      <description>arXiv:2501.07668v1 Announce Type: new 
Abstract: We describe two Monte Carlo algorithms for sampling from the integrated posterior distributions of a range of Bayesian mixture models. Both algorithms allow us to directly sample not only the assignment of observations to components but also the number of components, thereby fitting the model and performing model selection over the number of components in a single computation. The first algorithm is a traditional collapsed Gibbs sampler, albeit with an unusual move-set; the second builds on the first, adding rejection-free sampling from the prior over component assignments, to create an algorithm that has excellent mixing time in typical applications and outperforms current state-of-the-art methods, in some cases by a wide margin. We demonstrate our methods with a selection of applications to latent class analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07668v1</guid>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M. E. J. Newman</dc:creator>
    </item>
    <item>
      <title>Adaptive sequential Monte Carlo for automated cross validation in structural Bayesian hierarchical models</title>
      <link>https://arxiv.org/abs/2501.07685</link>
      <description>arXiv:2501.07685v1 Announce Type: new 
Abstract: Importance sampling (IS) is widely used for approximate Bayesian cross validation (CV) due to its efficiency, requiring only the re-weighting of a single set of posterior draws. With structural Bayesian hierarchical models, vanilla IS can produce unreliable results, as out-of-sample replication may involve non-standard case-deletion schemes which significantly alter the posterior geometry. This inevitably necessitates computationally expensive re-runs of Markov chain Monte Carlo (MCMC), making structural CV impracticable. To address this challenge, we consider sampling from a sequence of posteriors leading to the case-deleted posterior(s) via adaptive sequential Monte Carlo (SMC). We design the sampler to (a) support a broad range of structural CV schemes, (b) enhance efficiency by adaptively selecting Markov kernels, intervening in parallelizable MCMC re-runs only when necessary, and (c) streamline the workflow by automating the design of intermediate bridging distributions. Its practical utility is demonstrated through three real-world applications involving three types of predictive model assessments: leave-group-out CV, group $K$-fold CV, and sequential one-step-ahead validation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07685v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Geonhee Han, Andrew Gelman</dc:creator>
    </item>
    <item>
      <title>Black-box Optimization with Simultaneous Statistical Inference for Optimal Performance</title>
      <link>https://arxiv.org/abs/2501.07795</link>
      <description>arXiv:2501.07795v1 Announce Type: new 
Abstract: Black-box optimization is often encountered for decision-making in complex systems management, where the knowledge of system is limited. Under these circumstances, it is essential to balance the utilization of new information with computational efficiency. In practice, decision-makers often face the dual tasks of optimization and statistical inference for the optimal performance, in order to achieve it with a high reliability. Our goal is to address the dual tasks in an online fashion. Wu et al (2022) [arXiv preprint: 2210.06737] point out that the sample average of performance estimates generated by the optimization algorithm needs not to admit a central limit theorem. We propose an algorithm that not only tackles this issue, but also provides an online consistent estimator for the variance of the performance. Furthermore, we characterize the convergence rate of the coverage probabilities of the asymptotic confidence intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07795v1</guid>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Teng Lian, Jian-Qiang Hu, Yuhang Wu, Zeyu Zheng</dc:creator>
    </item>
    <item>
      <title>Fast and Cheap Covariance Smoothing</title>
      <link>https://arxiv.org/abs/2501.08265</link>
      <description>arXiv:2501.08265v1 Announce Type: new 
Abstract: We introduce the Tensorized-and-Restricted Krylov (TReK) method, a simple and efficient algorithm for estimating covariance tensors with large observational sizes. TReK extends the conjugate gradient method to incorporate range restrictions, enabling its use in a variety of covariance smoothing applications. By leveraging matrix-level operations, it achieves significant improvements in both computational speed and memory cost, improving over existing methods by an order of magnitude. TReK ensures finite-step convergence in the absence of rounding errors and converges fast in practice, making it well-suited for large-scale problems. The algorithm is also highly flexible, supporting a wide range of forward and projection tensors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08265v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ho Yun, Victor M. Panaretos</dc:creator>
    </item>
    <item>
      <title>COMBO and COMMA: R packages for regression modeling and inference in the presence of misclassified binary mediator or outcome variables</title>
      <link>https://arxiv.org/abs/2501.08320</link>
      <description>arXiv:2501.08320v1 Announce Type: new 
Abstract: Misclassified binary outcome or mediator variables can cause unpredictable bias in resulting parameter estimates. As more datasets that were not originally collected for research purposes are being used for studies in the social and health sciences, the need for methods that address data quality concerns is growing. In this paper, we describe two R packages, COMBO and COMMA, that implement bias-correction methods for misclassified binary outcome and mediator variables, respectively. These likelihood-based approaches do not require gold standard measures and allow for estimation of sensitivity and specificity rates for the misclassified variable(s). In addition, these R packages automatically apply crucial label switching corrections, allowing researchers to circumvent the inherent permutation invariance of the misclassification model likelihood. We demonstrate COMBO for single-outcome cases using a study of bar exam passage. We develop and evaluate a risk prediction model based on noisy indicators in a pretrial risk assessment study to demonstrate COMBO for multi-outcome cases. In addition, we use COMMA to evaluate the mediating effect of potentially misdiagnosed gestational hypertension on the maternal ethnicity-birthweight relationship.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08320v1</guid>
      <category>stat.CO</category>
      <category>stat.OT</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kimberly A. Hochstedler Webb, Martin T. Wells</dc:creator>
    </item>
    <item>
      <title>Optimal Scaling for the Proximal Langevin Algorithm in High Dimensions</title>
      <link>https://arxiv.org/abs/2204.10793</link>
      <description>arXiv:2204.10793v2 Announce Type: replace 
Abstract: The Metropolis-adjusted Langevin (MALA) algorithm is a sampling algorithm that incorporates the gradient of the logarithm of the target density in its proposal distribution. In an earlier joint work \citet{pill:stu:12}, the author had extended the seminal work of \cite{Robe:Rose:98} and showed that in stationarity, MALA applied to an $N-$dimensional approximation of the target will take ${\cal O}(N^{\frac13})$ steps to explore its target measure. It was also shown that the MALA algorithm is optimized at an average acceptance probability of $0.574$. In \citet{pere:16}, the author introduced the proximal MALA algorithm where the gradient of the log target density is replaced by the proximal function. In this paper, we show that for a wide class of twice differentiable target densities, the proximal MALA enjoys the same optimal scaling as that of MALA in high dimensions and also has an average optimal acceptance probability of $0.574$. The results of this paper thus give the following practically useful guideline: for smooth target densities where it is expensive to compute the gradient while implementing MALA, users may replace the gradient with the corresponding proximal function (that can be often computed relatively cheaply via convex optimization) \emph{without} losing any efficiency gains from optimal scaling. This confirms some of the empirical observations made in \cite{pere:16}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.10793v2</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Journal of Machine Learning Research, 2025</arxiv:journal_reference>
      <dc:creator>Natesh S. Pillai</dc:creator>
    </item>
    <item>
      <title>Bind Recovery of Sparse Factor Structures by Signal Cancellation</title>
      <link>https://arxiv.org/abs/2404.03781</link>
      <description>arXiv:2404.03781v2 Announce Type: replace-cross 
Abstract: Blind factor recovery follows from the principle that the signal of variables exclusive to a factor can be combined in a contrast (weighted sum) that cancels their factor contributions, leaving only a compound of the variables unique variances. Successful contrasts, uncorrelated with any remaining variable, become the signature of factors with at least two unique indicator variables. Pairwise signal cancellation, usually incomplete for variables affected by different factors, nevertheless succeeds for variables with proportional loadings on two factors, which places three cancelling clusters in the plane of two factors. This is recognized by successful cancellation among variable triplets representing the three clusters. The Signal Cancellation Recovery of Factors (SCRoF) algorithm implements these principles, only requiring that each factor has at least two unique indicators, not even requiring having pre-estimated the number of factors. Alternate sparse factor solutions are obtained through a two significance-threshold strategy. The individually estimated factor loadings and factor correlations of each potential solution are globally optimized for maximum likelihood, yielding a chi-square indication of compatibility with observed data. SCRoF is illustrated with synthetic data from a complex six-factor structure. Actual data then document that SCRoF can even benefit confirmatory factor analysis when the initial model appears inadequate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03781v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andr\'e Achim</dc:creator>
    </item>
    <item>
      <title>Optimal Sampling for Generalized Linear Model under Measurement Constraint with Surrogate Variables</title>
      <link>https://arxiv.org/abs/2501.00972</link>
      <description>arXiv:2501.00972v2 Announce Type: replace-cross 
Abstract: Measurement-constrained datasets, often encountered in semi-supervised learning, arise when data labeling is costly, time-intensive, or hindered by confidentiality or ethical concerns, resulting in a scarcity of labeled data. In certain cases, surrogate variables are accessible across the entire dataset and can serve as approximations to the true response variable; however, these surrogates often contain measurement errors and thus cannot be directly used for accurate prediction. We propose an optimal sampling strategy that effectively harnesses the available information from surrogate variables. This approach provides consistent estimators under the assumption of a generalized linear model, achieving theoretically lower asymptotic variance than existing optimal sampling algorithms that do not use surrogate data information. By employing the A-optimality criterion from optimal experimental design, our strategy maximizes statistical efficiency. Numerical studies demonstrate that our approach surpasses existing optimal sampling methods, exhibiting reduced empirical mean squared error and enhanced robustness in algorithmic performance. These findings highlight the practical advantages of our strategy in scenarios where measurement constraints exist and surrogates are available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00972v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yixin Shen, Yang Ning</dc:creator>
    </item>
  </channel>
</rss>
