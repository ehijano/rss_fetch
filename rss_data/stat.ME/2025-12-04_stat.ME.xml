<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 05 Dec 2025 02:33:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Assessing Extrapolation of Peaks Over Thresholds with Martingale Testing</title>
      <link>https://arxiv.org/abs/2512.03116</link>
      <description>arXiv:2512.03116v1 Announce Type: new 
Abstract: We present the winning strategy for the EVA2025 Data Challenge, which aimed to estimate the probability of extreme precipitation events. These events occurred at most once in the dataset making the challenge fundamentally one of extrapolating extreme values. Given the scarcity of extreme events, we argue that a simple, robust modeling approach is essential. We adopt univariate models instead of multivariate ones and model Peaks Over Thresholds using Extreme Value Theory. Specifically, we fit an exponential distribution to model exceedances of the target variable above a high quantile (after seasonal adjustment). The novelty of our approach lies in using martingale testing to evaluate the extrapolation power of the procedure and to agnostically select the level of the high quantile. While this method has several limitations, we believe that framing extrapolation as a game opens the door to other agnostic approaches in Extreme Value Analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03116v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joseph de Vilmarest (LPSM), Olivier Wintenberger (LPSM)</dc:creator>
    </item>
    <item>
      <title>Estimation of Semiparametric Factor Models with Missing Data</title>
      <link>https://arxiv.org/abs/2512.03235</link>
      <description>arXiv:2512.03235v1 Announce Type: new 
Abstract: We study semiparametric factor models in high-dimensional panels where the factor loadings consist of a nonparametric component explained by observed covariates and an idiosyncratic component capturing unobserved heterogeneity. A key challenge in empirical applications is the presence of missing observations, which can distort both factor recovery and loading estimation. To address this issue, we develop a projected principal component analysis (PPCA) procedure that accommodates general missing-at-random mechanisms through inverse-probability weighting. We establish consistency and derive the asymptotic distributions of the estimated factors and loading functions, allowing the sieve dimension to diverge and permitting the time dimension to be either fixed or growing. Unlike classical PCA, PPCA achieves consistent factor estimation even when T is fixed, and the limiting distributions under missing data exhibit mixture normality with enlarged asymptotic variances. Theoretical results are supported by simulations and an empirical application. Our findings demonstrate that PPCA provides an effective and robust framework for estimating semiparametric factor models in the presence of missing data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03235v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sijie Zheng</dc:creator>
    </item>
    <item>
      <title>Assumption-Lean Differential Variance Inference for Heterogeneous Treatment Effect Detection</title>
      <link>https://arxiv.org/abs/2512.03254</link>
      <description>arXiv:2512.03254v1 Announce Type: new 
Abstract: The conditional average treatment effect (CATE) is frequently estimated to refute the homogeneous treatment effect assumption. Under this assumption, all units making up the population under study experience identical benefit from a given treatment. Uncovering heterogeneous treatment effects through inference about the CATE, however, requires that covariates truly modifying the treatment effect be reliably collected at baseline. CATE-based techniques will necessarily fail to detect violations when effect modifiers are omitted from the data due to, for example, resource constraints. Severe measurement error has a similar impact. To address these limitations, we prove that the homogeneous treatment effect assumption can be gauged through inference about contrasts of the potential outcomes' variances. We derive causal machine learning estimators of these contrasts and study their asymptotic properties. We establish that these estimators are doubly robust and asymptotically linear under mild conditions, permitting formal hypothesis testing about the homogeneous treatment effect assumption even when effect modifiers are missing or mismeasured. Numerical experiments demonstrate that these estimators' asymptotic guarantees are approximately achieved in experimental and observational data alike. These inference procedures are then used to detect heterogeneous treatment effects in the re-analysis of randomized controlled trials investigating targeted temperature management in cardiac arrest patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03254v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philippe A. Boileau, Hani Zaki, Gabriele Lileikyte, Niklas Nielsen, Patrick R. Lawler, Mireille E. Schnitzer</dc:creator>
    </item>
    <item>
      <title>Change Point Detection for Functional Autoregressive Processes on the Sphere</title>
      <link>https://arxiv.org/abs/2512.03255</link>
      <description>arXiv:2512.03255v1 Announce Type: new 
Abstract: We introduce a novel framework for change point detection in spherical functional autoregressive (SPHAR) processes, enabling the identification of structural breaks in spatio-temporal random fields on the sphere. Our LASSO-regularized estimator, based on penalized dynamic programming in the harmonic domain, operates without knowledge of the number or locations of change points and offers non-asymptotic theoretical guarantees. This approach provides a new tool for analyzing nonstationary phenomena on the sphere, relevant to climate science, cosmology, and beyond.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03255v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Federica Spoto, Alessia Caponera, Pierpaolo Brutti</dc:creator>
    </item>
    <item>
      <title>Invited Discussion of "Model Uncertainty and Missing Data: An Objective Bayesian Perspective" by Gonzalo Garc\'ia-Donato , Mar\'ia Eugenia Castellanos , Stefano Cabras Alicia Quir\'os , and Anabel Forte</title>
      <link>https://arxiv.org/abs/2512.03266</link>
      <description>arXiv:2512.03266v1 Announce Type: new 
Abstract: The article by Garc{\'i}a-Donato and co-authors addresses the dual challenges of accounting for model uncertainty and missing data within the Gaussian regression frameworks from an objective Bayesian perspective. Thru the use of an imputation $g$-prior that replaces $X_\gamma^TX_\gamma$ for model $\gamma$ in the covariance of $\beta_\gamma$ with $\Sigma_{X_\gamma}$, the authors develop a coherent approach to addressing the missing data problem and model uncertainty simultaneously with random $X_\gamma$ in the missing at random (MAR) or missing completely at random (MCAR) settings, while still being computationally tractable. I discuss the connection of the imputation $g$-prior to the $g$-prior with imputed $X$, and to model selection for graphical models that provide an alternative justification for the $g$-prior for random $X$s.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03266v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1214/25-BA1531</arxiv:DOI>
      <arxiv:journal_reference>Bayesian Analysis, 2025, Volume 20, pages 1716-1721</arxiv:journal_reference>
      <dc:creator>Merlise A Clyde</dc:creator>
    </item>
    <item>
      <title>Parsimonious Factor Models for Asymmetric Dependence in Multivariate Extremes</title>
      <link>https://arxiv.org/abs/2512.03543</link>
      <description>arXiv:2512.03543v2 Announce Type: new 
Abstract: Modelling multivariate extreme events is essential when extrapolating beyond the range of observed data. Parametric models that are suitable for real-world extremes must be flexible -- particularly in their ability to capture asymmetric dependence structures -- while also remaining parsimonious for interpretability and computationally scalable in high dimensions. Although many models have been proposed, it is rare for any single construction to satisfy all of these requirements. For instance, the popular H\"usler-Reiss model is limited to symmetric dependence structures. In this manuscript, we introduce a class of additive factor models and derive their extreme-value limits. This leads to a broad and tractable family of models characterised by a manageable number of parameters. These models naturally accommodate asymmetric tail dependence and allow for non-stationary behaviour. We present the limiting models from both the componentwise-maxima and Peaks-over-Thresholds perspectives, via the multivariate extreme value and multivariate generalized Pareto distributions, respectively. Simulation studies illustrate identifiability properties based on existing inference methodologies. Finally, applications to summer temperature maxima in Melbourne, Australia, and to weekly negative returns from four major UK banks demonstrate improved fit compared with the H\"usler-Reiss model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03543v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pavel Krupskii, Boris B\'eranger</dc:creator>
    </item>
    <item>
      <title>Weighted Conformal Prediction for Survival Analysis under Covariate Shift</title>
      <link>https://arxiv.org/abs/2512.03738</link>
      <description>arXiv:2512.03738v1 Announce Type: new 
Abstract: Reliable uncertainty quantification is essential in survival prediction, particularly in clinical settings where erroneous decisions carry high risk. Conformal prediction has attracted substantial attention as it offers a model-agnostic framework with finite-sample coverage guarantees. Extending it to right-censored outcomes poses nontrivial challenges. Several adaptations of conformal approaches for survival outcomes have been developed, but they either rely on restrictive censoring settings or substantial computation. A recent conformal approach for right-censored data constructs censoring-adjusted p-values and enables prediction intervals in general survival settings. However, the empirical coverage depends sensitively on heuristic tuning choices and its validity is limited to scenarios without covariate shift. In this paper, we establish theoretical justification for its prediction-set construction, providing a principled basis for defining prediction-set bounds, and extend the approach to covariate-shift settings. Simulation studies and a real data application demonstrate that the proposed method achieves robust coverage and coherent interval structure across varying censoring levels and covariate-shift settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03738v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jaeyoung Shin (Department of Statistics and Data Science, Yonsei University, Seoul, South Korea), Chi Hyun Lee (Department of Statistics and Data Science, Yonsei University, Seoul, South Korea, Department of Applied Statistics, Yonsei University, Seoul, South Korea), Sangwook Kang (Department of Statistics and Data Science, Yonsei University, Seoul, South Korea, Department of Applied Statistics, Yonsei University, Seoul, South Korea)</dc:creator>
    </item>
    <item>
      <title>Using functional information for binary classifications</title>
      <link>https://arxiv.org/abs/2512.03761</link>
      <description>arXiv:2512.03761v1 Announce Type: new 
Abstract: The adequate use of information measured in a continuous manner along a period of time represents a methodological challenge. In the last decades, most of traditional statistical procedures have been extended for accommodating these functional data. The binary classification problem, which aims to correctly identify units as positive or negative based on marker values, is not aside of this scenario. The crucial point for making binary classifications based on a marker is to establish an order in the marker values, which is not immediate when these values are presented as functions. Here, we argue that if the marker is related to the characteristic under study, a trajectory from a positive participant should be more similar to trajectories from the positive population than to those drawn from the negative. With this criterion, a classification procedure based on the distance between the involved functions is proposed. Besides, we propose a fully non-parametric estimator for this so-called probability-based criterion, PBC. We explore its asymptotic properties, and its finite-sample behavior from an extensive Monte Carlo study. The observed results suggest that the proposed methodology works adequately, and frequently better than its competitors, for a wide variety of situations when the sample size in both the training and the testing cohorts is adequate. The practical use of the proposal is illustrated from real-world dataset. As online supplementary material, the manuscript includes a document with further simulations and additional comments. An R function which wraps up the implemented routines is also provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03761v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pablo Martinez-Camblor</dc:creator>
    </item>
    <item>
      <title>A comparison between initialization strategies for the infinite hidden Markov model</title>
      <link>https://arxiv.org/abs/2512.03777</link>
      <description>arXiv:2512.03777v1 Announce Type: new 
Abstract: Infinite hidden Markov models provide a flexible framework for modelling time series with structural changes and complex dynamics, without requiring the number of latent states to be specified in advance. This flexibility is achieved through the hierarchical Dirichlet process prior, while efficient Bayesian inference is enabled by the beam sampler, which combines dynamic programming with slice sampling to truncate the infinite state space adaptively. Despite extensive methodological developments, the role of initialization in this framework has received limited attention. This study addresses this gap by systematically evaluating initialization strategies commonly used for finite hidden Markov models and assessing their suitability in the infinite setting. Results from both simulated and real datasets show that distance-based clustering initializations consistently outperform model-based and uniform alternatives, the latter being the most widely adopted in the existing literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03777v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Federico P. Cortese, Luca Rossini</dc:creator>
    </item>
    <item>
      <title>SUP: An Inferable Private Multiple Testing Framework with Super Uniformity</title>
      <link>https://arxiv.org/abs/2512.03859</link>
      <description>arXiv:2512.03859v2 Announce Type: new 
Abstract: Multiple testing is widely applied across scientific fields, particularly in genomic and health data analysis, where protecting sensitive personal information is imperative. However, developing private multiple testing algorithms for super uniform $p$-values remains an open question, as privacy mechanisms introduce intricate dependence among the peeled $p$-values and disrupt their super uniformity, complicating post-selection inference. To address this, we introduce a general Super Uniform Private (SUP) multiple testing framework with three key components. First, we develop a novel \( p \)-value transformation that is compatible with diverse privacy regimes while retaining the super uniformity. Next, a reversed peeling algorithm is designed to reduce privacy budgets while facilitating inference. Then, we provide diverse rejection thresholds that are privacy-parameter-free and tailored for different Type-I errors, including the family-wise error rate (FWER) and the false discovery rate (FDR). Building upon these, we advance adaptive techniques to determine the peeling number and boost thresholds. Theoretically, we propose a technique overcoming the post-selection obstacle to Type-I error control, quantify the privacy-induced power loss of SUP relative to its non-private counterpart, and demonstrate that SUP surpasses existing private methods in terms of power. The results of extensive simulations and a real data application validate our theories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03859v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kehan Wang, Wenxuan Song, Wangli Xu, Linglong Kong</dc:creator>
    </item>
    <item>
      <title>Parsimonious Clustering of Covariance Matrices</title>
      <link>https://arxiv.org/abs/2512.03912</link>
      <description>arXiv:2512.03912v1 Announce Type: new 
Abstract: Functional connectivity (FC) derived from functional magnetic resonance imaging (fMRI) data offers vital insights for understanding brain function and neurological and psychiatric disorders. Unsupervised clustering methods are desired to group individuals based on shared features, facilitating clinical diagnosis. In this study, a parsimonious clustering model is proposed, which integrates the Mixture-of-Experts (MoE) and covariance regression framework, to cluster individuals based on FC captured by data covariance matrices in resting-state fMRI studies. The model assumes common linear projections across covariance matrices and a generalized linear model with covariates, allowing for flexible yet interpretable projection-specific clustering solutions. To evaluate the performance of the proposed framework, extensive simulation studies are conducted to assess clustering accuracy and robustness. The approach is applied to resting-state fMRI data from the Alzheimer's Disease Neuroimaging Initiative (ADNI). Subgroups are identified based on brain coherence and simultaneously uncover the association with demographic factors and cognitive functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03912v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yixi Xu, Yi Zhao</dc:creator>
    </item>
    <item>
      <title>When are novel methods for analyzing complex chemical mixtures in epidemiology beneficial?</title>
      <link>https://arxiv.org/abs/2512.03946</link>
      <description>arXiv:2512.03946v1 Announce Type: new 
Abstract: Estimating the health impacts of exposure to a mixture of chemicals poses many statistical challenges: multiple correlated exposure variables, moderate to high dimensionality, and possible nonlinear and interactive health effects of mixture components. Reviews of chemical mixture methods aim to help researchers select a statistical method suited to their goals and data, but examinations of empirical performance have emphasized novel methods purpose-built for analyzing complex chemical mixtures, or other more advanced methods, over more general methods which are widely used in many application domains. We conducted a broad experimental comparison, across simulated scenarios, of both more general methods (such as generalized linear models) and novel methods (such as Bayesian Kernel Machine Regression) designed to study chemical mixtures. We assessed methods based on their ability to control Type I error rate, maximize power, provide interpretable results, and make accurate predictions. We find that when there is moderate correlation between mixture components and the exposure-response function does not have complicated interactions, or when mixture components have opposite effects, general methods are preferred over novel ones. With highly interactive exposure-response functions or highly correlated exposures, novel methods provide important benefits. We provide a comprehensive summary of when different methods are most suitable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03946v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nate Wiecha, Emily Griffith, Brian J. Reich, Jane A. Hoppin</dc:creator>
    </item>
    <item>
      <title>Statistical hypothesis testing for differences between layers in dynamic multiplex networks</title>
      <link>https://arxiv.org/abs/2512.03983</link>
      <description>arXiv:2512.03983v1 Announce Type: new 
Abstract: With the emergence of dynamic multiplex networks, corresponding to graphs where multiple types of edges evolve over time, a key inferential task is to determine whether the layers associated with different edge types differ in their connectivity. In this work, we introduce a hypothesis testing framework, under a latent space network model, for assessing whether the layers share a common latent representation. The method we propose extends previous literature related to the problem of pairwise testing for random graphs and enables global testing of differences between layers in multiplex graphs. While we introduce the method as a test for differences between layers, it can easily be adapted to test for differences between time points. We construct a test statistic based on a spectral embedding of an unfolded representation of the graph adjacency matrices and demonstrate its ability to detect differences across layers in the asymptotic regime where the number of nodes in each graph tends to infinity. The finite-sample properties of the test are empirically demonstrated by assessing its performance on both simulated data and a biological dataset describing the neural activity of larval Drosophila.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03983v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maximilian Baum, Francesco Sanna Passino, Axel Gandy</dc:creator>
    </item>
    <item>
      <title>Inference for location and height of peaks of a standardized field after selection</title>
      <link>https://arxiv.org/abs/2512.04059</link>
      <description>arXiv:2512.04059v1 Announce Type: new 
Abstract: Peak inference concerns the use of local maxima ("peaks") of a noisy random field to detect and localize regions where underlying signal is present. We propose a peak inference method that first subjects observed peaks to a significance test of the null hypothesis that no signal is present, and then uses the peaks that are declared significant to construct post-selectively valid confidence regions for the location and height of nearby true peaks. We analyze the performance of this method in a smooth signal plus constant variance noise model under a high-curvature asymptotic assumption, and prove that it asymptotically controls both the number of false discoveries, and the number of confidence regions that do not contain a true peak, relative to the number of points at which inference is conducted. An important intermediate theoretical result uses the Kac-Rice formula to derive a novel approximation to the intensity function of a point process that counts local maxima, which is second-order accurate under the alternative, nearby high-curvature true peaks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.04059v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alden Green, Jonathan Taylor</dc:creator>
    </item>
    <item>
      <title>A Large Scale Heterogeneous Treatment Effect Estimation Framework and Its Applications of Users' Journey at Snap</title>
      <link>https://arxiv.org/abs/2512.03060</link>
      <description>arXiv:2512.03060v1 Announce Type: cross 
Abstract: Heterogeneous Treatment Effect (HTE) and Conditional Average Treatment Effect (CATE) models relax the assumption that treatment effects are the same for every user. We present a large scale industrial framework for estimating HTE using experimental data from hundreds of millions of Snapchat users. By combining results across many experiments, the framework uncovers latent user characteristics that were previously unmeasurable and produces stable treatment effect estimates at scale.
  We describe the core components that enabled this system, including experiment selection, base learner design, and incremental training. We also highlight two applications: user influenceability to ads and user sensitivity to ads. An online A/B test using influenceability scores for targeting showed an improvement on key business metrics that is more than six times larger than what is typically considered significant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03060v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jing Pan, Li Shi, Paul Lo</dc:creator>
    </item>
    <item>
      <title>SSLfmm: An R Package for Semi-Supervised Learning with a Mixed-Missingness Mechanism in Finite Mixture Models</title>
      <link>https://arxiv.org/abs/2512.03322</link>
      <description>arXiv:2512.03322v1 Announce Type: cross 
Abstract: Semi-supervised learning (SSL) constructs classifiers from datasets in which only a subset of observations is labelled, a situation that naturally arises because obtaining labels often requires expert judgement or costly manual effort. This motivates methods that integrate labelled and unlabelled data within a learning framework. Most SSL approaches assume that label absence is harmless, typically treated as missing completely at random or ignored, but in practice, the missingness process can be informative, as the chances of an observation being unlabelled may depend on the ambiguity of its feature vector. In such cases, the missingness indicators themselves provide additional information that, if properly modelled, may improve estimation efficiency. The \textbf{SSLfmm} package for R is designed to capture this behaviour by estimating the Bayes' classifier under a finite mixture model in which each component corresponding to a class follows a multivariate normal distribution. It incorporates a mixed-missingness mechanism that combines a missing completely at random (MCR) component with a (non-ignorable) missing at random (MAR) component, the latter modelling the probability of label missingness as a logistic function of the entropy based on the features. Parameters are estimated via an Expectation--Conditional Maximisation algorithm. In the two-class Gaussian setting with arbitrary covariance matrices, the resulting classifier trained on partially labelled data may, in some cases, achieve a lower misclassification rate than the supervised version in the case where all the labels are known. The package includes a practical tool for modelling and illustrates its performance through simulated examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03322v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Geoffrey J. McLachlan, Jinran Wu</dc:creator>
    </item>
    <item>
      <title>Bayesian Event-Based Model for Disease Subtype and Stage Inference</title>
      <link>https://arxiv.org/abs/2512.03467</link>
      <description>arXiv:2512.03467v1 Announce Type: cross 
Abstract: Chronic diseases often progress differently across patients. Rather than randomly varying, there are typically a small number of subtypes for how a disease progresses across patients. To capture this structured heterogeneity, the Subtype and Stage Inference Event-Based Model (SuStaIn) estimates the number of subtypes, the order of disease progression for each subtype, and assigns each patient to a subtype from primarily cross-sectional data. It has been widely applied to uncover the subtypes of many diseases and inform our understanding of them. But how robust is its performance? In this paper, we develop a principled Bayesian subtype variant of the event-based model (BEBMS) and compare its performance to SuStaIn in a variety of synthetic data experiments with varied levels of model misspecification. BEBMS substantially outperforms SuStaIn across ordering, staging, and subtype assignment tasks. Further, we apply BEBMS and SuStaIn to a real-world Alzheimer's data set. We find BEBMS has results that are more consistent with the scientific consensus of Alzheimer's disease progression than SuStaIn.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03467v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hongtao Hao, Joseph L. Austerweil</dc:creator>
    </item>
    <item>
      <title>Colored Markov Random Fields for Probabilistic Topological Modeling</title>
      <link>https://arxiv.org/abs/2512.03727</link>
      <description>arXiv:2512.03727v1 Announce Type: cross 
Abstract: Probabilistic Graphical Models (PGMs) encode conditional dependencies among random variables using a graph -nodes for variables, links for dependencies- and factorize the joint distribution into lower-dimensional components. This makes PGMs well-suited for analyzing complex systems and supporting decision-making. Recent advances in topological signal processing highlight the importance of variables defined on topological spaces in several application domains. In such cases, the underlying topology shapes statistical relationships, limiting the expressiveness of canonical PGMs. To overcome this limitation, we introduce Colored Markov Random Fields (CMRFs), which model both conditional and marginal dependencies among Gaussian edge variables on topological spaces, with a theoretical foundation in Hodge theory. CMRFs extend classical Gaussian Markov Random Fields by including link coloring: connectivity encodes conditional independence, while color encodes marginal independence. We quantify the benefits of CMRFs through a distributed estimation case study over a physical network, comparing it with baselines with different levels of topological prior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03727v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lorenzo Marinucci, Leonardo Di Nino, Gabriele D'Acunto, Mario Edoardo Pandolfo, Paolo Di Lorenzo, Sergio Barbarossa</dc:creator>
    </item>
    <item>
      <title>A decay-adjusted spatio-temporal model to account for the impact of mass drug administration on neglected tropical disease prevalence</title>
      <link>https://arxiv.org/abs/2512.03760</link>
      <description>arXiv:2512.03760v1 Announce Type: cross 
Abstract: Prevalence surveys are routinely used to monitor the effectiveness of mass drug administration (MDA) programmes for controlling neglected tropical diseases (NTDs). We propose a decay-adjusted spatio-temporal (DAST) model that explicitly accounts for the time-varying impact of MDA on NTD prevalence, providing a flexible and interpretable framework for estimating intervention effects from sparse survey data. Using case studies on soil-transmitted helminths and lymphatic filariasis, we show that DAST offers a practical alternative to standard geostatistical models when the objective includes quantifying MDA impact and supporting short-term programmatic forecasting. We also discuss extensions and identifiability challenges, advocating for data-driven parsimony over complexity in settings where the available data are too sparse to support the estimation of highly parameterised models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03760v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emanuele Giorgi, Claudio Fronterre, Peter J. Diggle</dc:creator>
    </item>
    <item>
      <title>A maximin optimal approach for sampling designs in two-phase studies</title>
      <link>https://arxiv.org/abs/2312.10596</link>
      <description>arXiv:2312.10596v3 Announce Type: replace 
Abstract: Data collection costs can vary widely across variables in data science tasks. Two-phase designs can be employed to save data collection costs. This paper considers the two-phase studies where inexpensive variables are collected for all subjects in the first phase, and expensive variables are measured for a subsample of subjects in the second phase based on a predetermined sampling rule. The estimation efficiency under two-phase designs relies heavily on the sampling rule. Existing literature primarily focuses on designing sampling rules for estimating a scalar parameter in some parametric models or specific estimating problems. However, real-world scenarios are usually model-unknown and involve two-phase designs for model-free estimation of a scalar or multi-dimensional parameter. This paper proposes a maximin criterion to design an optimal sampling rule based on semiparametric efficiency bounds. The proposed method is model-free and applicable to general estimating problems. The resulting sampling rule can minimize the semiparametric efficiency bound when the parameter is scalar and improve the bound for every component when the parameter is multi-dimensional. Simulation studies demonstrate that the proposed designs reduce the variance of the resulting estimator in various settings. The implementation of the proposed design is illustrated in a real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.10596v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruoyu Wang, Qihua Wang, Wang Miao</dc:creator>
    </item>
    <item>
      <title>Multiple Randomization Designs: Estimation and Inference with Interference</title>
      <link>https://arxiv.org/abs/2401.01264</link>
      <description>arXiv:2401.01264v2 Announce Type: replace 
Abstract: Classical designs of randomized experiments, going back to Fisher and Neyman in the 1930s still dominate practice even in online experimentation. However, such designs are of limited value for answering standard questions in settings, common in marketplaces, where multiple populations of agents interact strategically, leading to complex patterns of spillover effects. In this paper, we discuss new experimental designs and corresponding estimands to account for and capture these complex spillovers. We derive the finite-sample properties of tractable estimators for main effects, direct effects, and spillovers, and present associated central limit theorems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.01264v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Masoero, Suhas Vijaykumar, Thomas Richardson, James McQueen, Ido Rosen, Brian Burdick, Pat Bajari, Guido Imbens</dc:creator>
    </item>
    <item>
      <title>Marginally interpretable spatial logistic regression with bridge processes</title>
      <link>https://arxiv.org/abs/2412.04744</link>
      <description>arXiv:2412.04744v3 Announce Type: replace 
Abstract: In including random effects to account for dependent observations, the odds ratio interpretation of logistic regression coefficients is changed from population-averaged to subject-specific. This is unappealing in many applications, motivating a rich literature on methods that maintain the marginal logistic regression structure without random effects, such as generalized estimating equations. However, for spatial data, random effect approaches are appealing in providing a full probabilistic characterization of the data that can be used for prediction. We propose a new class of spatial logistic regression models that maintain both population-averaged and subject-specific interpretations through a novel class of bridge processes for spatial random effects. These processes are shown to have appealing computational and theoretical properties, including a scale mixture of normal representation. The new methodology is illustrated with simulations and an analysis of childhood malaria prevalence data in the Gambia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04744v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changwoo J. Lee, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>High-dimensional sliced inverse regression with endogeneity</title>
      <link>https://arxiv.org/abs/2412.15530</link>
      <description>arXiv:2412.15530v2 Announce Type: replace 
Abstract: Sliced inverse regression (SIR) is a popular sufficient dimension reduction method that identifies a few linear transformations of the covariates without losing regression information with the response. In high-dimensional settings, SIR can be combined with sparsity penalties to achieve sufficient dimension reduction and variable selection simultaneously. Nevertheless, both classical and sparse estimators assume the covariates are exogenous. However, endogeneity can arise in a variety of situations, such as when variables are omitted or are measured with error. In this article, we show such endogeneity invalidates SIR estimators, leading to inconsistent estimation of the true central subspace. To address this challenge, we propose a two-stage Lasso SIR estimator, which first constructs a sparse high-dimensional instrumental variables model to obtain fitted values of the covariates spanned by the instruments, and then applies SIR augmented with a Lasso penalty on these fitted values. We establish theoretical bounds for the estimation and selection consistency of the true central subspace for the proposed estimators, allowing the number of covariates and instruments to grow exponentially with the sample size. Simulation studies and applications to two real-world datasets in nutrition and genetics illustrate the superior empirical performance of the two-stage Lasso SIR estimator compared with existing methods that disregard endogeneity and/or nonlinearity in the outcome model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15530v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Linh H. Nghiem, Francis. K. C. Hui, Samuel Muller, A. H. Welsh</dc:creator>
    </item>
    <item>
      <title>Testing the Missing Completely at Random Assumption for Functional Data</title>
      <link>https://arxiv.org/abs/2505.08721</link>
      <description>arXiv:2505.08721v2 Announce Type: replace 
Abstract: We consider functional data which have only been observed on a subset of their domain. This paper aims to develop statistical tests to determine whether the function and the domain over which it is observed are independent. The assumption that data is missing completely at random (MCAR) is essential for many functional data methods handling incomplete observations. However, no general testing procedures have been established to validate this assumption. We address this critical gap by introducing a testing framework which is generally based on a partition of the observation patterns. Besides deterministic partitions, we also consider a data-driven approach based on clustering. We establish asymptotic results for our tests and illustrate the methodology in several real data applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08721v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maximilian Ofner, Siegfried H\"ormann, David Kraus, Dominik Liebl</dc:creator>
    </item>
    <item>
      <title>Likelihood-free Posterior Density Learning for Uncertainty Quantification in Inference Problems</title>
      <link>https://arxiv.org/abs/2508.00167</link>
      <description>arXiv:2508.00167v2 Announce Type: replace 
Abstract: Generative models and those with computationally intractable likelihoods are widely used to describe complex systems in the natural sciences, social sciences, and engineering. Fitting these models to data requires likelihood-free inference methods that explore the parameter space without explicit likelihood evaluations, relying instead on sequential simulation, which comes at the cost of computational efficiency and extensive tuning. We develop an alternative framework called kernel-adaptive synthetic posterior estimation (KASPE) that uses deep learning to directly reconstruct the mapping between the observed data and a finite-dimensional parametric representation of the posterior distribution, trained on a large number of simulated datasets. We provide theoretical justification for KASPE and a formal connection to the likelihood-based approach of expectation propagation. Simulation experiments demonstrate KASPE's flexibility and performance relative to existing likelihood-free methods including approximate Bayesian computation in challenging inferential settings involving posteriors with heavy tails, multiple local modes, and over the parameters of a nonlinear dynamical system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00167v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rui Zhang, Oksana A. Chkrebtii, Dongbin Xiu</dc:creator>
    </item>
    <item>
      <title>Tree-based methods for length-biased survival data</title>
      <link>https://arxiv.org/abs/2508.16312</link>
      <description>arXiv:2508.16312v2 Announce Type: replace 
Abstract: Left-truncated survival data commonly arise in prevalent cohort studies, where only individuals who have experienced disease onset and survived until enrollment in the study. When the onset process follows a stationary Poisson process, the resulting data are length-biased. This sampling mechanism induces a selection bias towards longer survival individuals, and statistical methods for traditional survival data are not directly applicable. While tree-based methods developed for left-truncated data can be applied, they may be inefficient for length-biased data, as they do not account for the distribution of truncation times. To address this, we propose new survival trees and forests for length-biased right-censored data within the conditional inference framework. Our approach uses a score function derived from the full likelihood to construct permutation test statistics for variable splitting. For survival prediction, we consider two estimators of the unbiased survival function, differing in statistical efficiency and computational complexity. These elements enhance efficiency in tree construction and improve accuracy of survival prediction in ensemble settings. Simulation studies demonstrate efficiency gains in both tree recovery and survival prediction, often exceeding the gains from ensembling alone. We further illustrate the utility of the proposed methods using lung cancer data from the Cancer Public Library Database, a nationwide cancer registry in South Korea.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16312v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinwoo Lee, Donghwan Lee, Hyunwoo Lee, Jiyu Sun</dc:creator>
    </item>
    <item>
      <title>Unbiased Kinetic Langevin Monte Carlo with Inexact Gradients</title>
      <link>https://arxiv.org/abs/2311.05025</link>
      <description>arXiv:2311.05025v4 Announce Type: replace-cross 
Abstract: We present an unbiased method for Bayesian posterior means based on kinetic Langevin dynamics that combines advanced splitting methods with enhanced gradient approximations. Our approach avoids Metropolis correction by coupling Markov chains at different discretization levels in a multilevel Monte Carlo approach. Theoretical analysis demonstrates that our proposed estimator is unbiased, attains finite variance, and satisfies a central limit theorem. It can achieve accuracy $\epsilon&gt;0$ for estimating expectations of Lipschitz functions in $d$ dimensions with $\mathcal{O}(d^{1/4}\epsilon^{-2})$ expected gradient evaluations, without assuming warm start. We exhibit similar bounds using both approximate and stochastic gradients, and our method's computational cost is shown to scale independently of the size of the dataset. The proposed method is tested using a multinomial regression problem on the MNIST dataset and a Poisson regression model for soccer scores. Experiments indicate that the number of gradient evaluations per effective sample is independent of dimension, even when using inexact gradients. For product distributions, we give dimension-independent variance bounds. Our results demonstrate that in large-scale applications, the unbiased algorithm we present can be 2-3 orders of magnitude more efficient than the ``gold-standard" randomized Hamiltonian Monte Carlo.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.05025v4</guid>
      <category>stat.CO</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Neil K. Chada, Benedict Leimkuhler, Daniel Paulin, Peter A. Whalley</dc:creator>
    </item>
    <item>
      <title>Transductive Conformal Inference for Full Ranking</title>
      <link>https://arxiv.org/abs/2501.11384</link>
      <description>arXiv:2501.11384v3 Announce Type: replace-cross 
Abstract: We introduce a method based on Conformal Prediction (CP) to quantify the uncertainty of full ranking algorithms. We focus on a specific scenario where $n+m$ items are to be ranked by some ``black box'' algorithm. It is assumed that the relative (ground truth) ranking of $n$ of them is known. The objective is then to quantify the error made by the algorithm on the ranks of the $m$ new items among the total $(n+m)$. In such a setting, the true ranks of the $n$ original items in the total $(n+m)$ depend on the (unknown) true ranks of the $m$ new ones. Consequently, we have no direct access to a calibration set to apply a classical CP method. To address this challenge, we propose to construct distribution-free bounds of the unknown conformity scores using recent results on the distribution of conformal p-values. Using these scores upper bounds, we provide valid prediction sets for the rank of any item. We also control the false coverage proportion, a crucial quantity when dealing with multiple prediction sets. Finally, we empirically show on both synthetic and real data the efficiency of our CP method for state-of-the-art algorithms such as RankNet or LambdaMart.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11384v3</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>NeurIPS 2025, The Thirty-Ninth Annual Conference on Neural Information Processing Systems, Dec 2025, San Diego (CA), United States</arxiv:journal_reference>
      <dc:creator>Jean-Baptiste Fermanian (UM, IMAG, IROKO), Pierre Humbert (SU, LPSM), Gilles Blanchard (LMO, DATASHAPE)</dc:creator>
    </item>
    <item>
      <title>Class conditional conformal prediction for multiple inputs by p-value aggregation</title>
      <link>https://arxiv.org/abs/2507.07150</link>
      <description>arXiv:2507.07150v2 Announce Type: replace-cross 
Abstract: Conformal prediction methods are statistical tools designed to quantify uncertainty and generate predictive sets with guaranteed coverage probabilities. This work introduces an innovative refinement to these methods for classification tasks, specifically tailored for scenarios where multiple observations (multi-inputs) of a single instance are available at prediction time. Our approach is particularly motivated by applications in citizen science, where multiple images of the same plant or animal are captured by individuals. Our method integrates the information from each observation into conformal prediction, enabling a reduction in the size of the predicted label set while preserving the required class-conditional coverage guarantee. The approach is based on the aggregation of conformal p-values computed from each observation of a multi-input. By exploiting the exact distribution of these p-values, we propose a general aggregation framework using an abstract scoring function, encompassing many classical statistical tools. Knowledge of this distribution also enables refined versions of standard strategies, such as majority voting. We evaluate our method on simulated and real data, with a particular focus on Pl@ntNet, a prominent citizen science platform that facilitates the collection and identification of plant species through user-submitted images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07150v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>NeurIPS 2025, The Thirty-Ninth Annual Conference on Neural Information Processing Systems, Dec 2025, San Diego (CA), United States</arxiv:journal_reference>
      <dc:creator>Jean-Baptiste Fermanian (IMAG, IROKO), Mohamed Hebiri (LAMA), Joseph Salmon (IMAG, IROKO)</dc:creator>
    </item>
    <item>
      <title>PCS Workflow for Veridical Data Science in the Age of AI</title>
      <link>https://arxiv.org/abs/2508.00835</link>
      <description>arXiv:2508.00835v2 Announce Type: replace-cross 
Abstract: Data science is a pillar of artificial intelligence (AI), which is transforming nearly every domain of human activity, from the social and physical sciences to engineering and medicine. While data-driven findings in AI offer unprecedented power to extract insights and guide decision-making, many are difficult or impossible to replicate. A key reason for this challenge is the uncertainty introduced by the many choices made throughout the data science life cycle (DSLC). Traditional statistical frameworks often fail to account for this uncertainty. The Predictability-Computability-Stability (PCS) framework for veridical (truthful) data science offers a principled approach to addressing this challenge throughout the DSLC. This paper presents an updated and streamlined PCS workflow, tailored for practitioners and enhanced with guided use of generative AI. We include a running example to display the PCS framework in action, and conduct a related case study which showcases the uncertainty in downstream predictions caused by judgment calls in the data cleaning stage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00835v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1098/rsta.2024.0605</arxiv:DOI>
      <dc:creator>Zachary T. Rewolinski, Bin Yu</dc:creator>
    </item>
    <item>
      <title>Differential Geometry of the Fixed-Rank Core Covariance Manifold</title>
      <link>https://arxiv.org/abs/2512.01070</link>
      <description>arXiv:2512.01070v2 Announce Type: replace-cross 
Abstract: We study the differential geometry of the fixed-rank core covariance manifold. According to Hoff, McCormack, and Zhang [J. R. Stat. Soc., B: Stat., 85 (2023), pp. 1659--1679], every covariance matrix $\Sigma$ of $p_1\times p_2$ matrix-variate data uniquely decomposes into a separable component $K$ and a core component $C$. Such a decomposition also exists for rank-$r$ $\Sigma$ if $p_1/p_2+p_2/p_1&lt;r$, with $C$ sharing the same rank. They posed an open question on whether a partial-isotropy structure can be imposed on $C$ for high-dimensional covariance estimation. We address this question by showing that a partial-isotropy rank-$r$ core is a non-trivial convex combination of a rank-$r$ core and $I_p$ for $p:=p_1p_2$, motivating the study of rank-$r$ cores. For fixed $r&gt;p_1/p_2+p_2/p_1$, we prove that the set of rank-$r$ cores, $\mathcal{C}_{p_1,p_2,r}^+$, is a compact, smooth, embedded submanifold of the set of rank-$r$ positive semi-definite matrices, except for a measure-zero subset associated with canonical decomposability. When $r=p$, the set of full-rank cores $\mathcal{C}_{p_1,p_2}^{++}$ is itself a smooth manifold. Moreover, the positive definite cone $\mathcal{S}_p^{++}$ is diffeomorphic to the product of the Kronecker and core covariance manifolds, providing new geometric insight into $\mathcal{S}_p^{++}$ via separability. Differential geometric quantities, such as the differential of the diffeomorphism, as well as the Riemannian gradient and Hessian operator on $\mathcal{C}_{p_1,p_2}^{++}$ and the manifolds used in constructing $\mathcal{C}_{p_1,p_2,r}^+$, are also derived. Lastly, we propose a partial-isotropy core shrinkage estimator for matrix-variate data, supported by numerical illustrations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01070v2</guid>
      <category>math.DG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bongjung Sung</dc:creator>
    </item>
    <item>
      <title>Adaptive Decentralized Federated Learning for Robust Optimization</title>
      <link>https://arxiv.org/abs/2512.02852</link>
      <description>arXiv:2512.02852v2 Announce Type: replace-cross 
Abstract: In decentralized federated learning (DFL), the presence of abnormal clients, often caused by noisy or poisoned data, can significantly disrupt the learning process and degrade the overall robustness of the model. Previous methods on this issue often require a sufficiently large number of normal neighboring clients or prior knowledge of reliable clients, which reduces the practical applicability of DFL. To address these limitations, we develop here a novel adaptive DFL (aDFL) approach for robust estimation. The key idea is to adaptively adjust the learning rates of clients. By assigning smaller rates to suspicious clients and larger rates to normal clients, aDFL mitigates the negative impact of abnormal clients on the global model in a fully adaptive way. Our theory does not put any stringent conditions on neighboring nodes and requires no prior knowledge. A rigorous convergence analysis is provided to guarantee the oracle property of aDFL. Extensive numerical experiments demonstrate the superior performance of the aDFL method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02852v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuyuan Wu, Feifei Wang, Yuan Gao, Rui Wang, Hansheng Wang</dc:creator>
    </item>
  </channel>
</rss>
