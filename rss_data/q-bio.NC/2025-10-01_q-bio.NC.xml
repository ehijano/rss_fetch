<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 02 Oct 2025 04:00:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Computational Advances in Taste Perception: From Ion Channels to Neural Coding</title>
      <link>https://arxiv.org/abs/2510.00010</link>
      <description>arXiv:2510.00010v1 Announce Type: new 
Abstract: Recent advances in computational neuroscience demand models that balance biophysical realism with scalability. We present a hybrid neuron model combining the biophysical fidelity of Hodgkin-Huxley (HH) dynamics for taste receptor cells with the computational efficiency of Izhikevich spiking neurons for large-network simulations. Our framework incorporates biomorphic taste cell models, featuring modality-specific receptor dynamics (T1R/T2R, ENaC, PKD) and Goldman-Hodgkin-Katz (GHK)-driven ion currents to accurately simulate gustatory transduction. Synaptic interactions are modeled via glutamate release kinetics with alpha-function profiles, AMPA receptor trafficking regulated by phosphorylation, and spike-timing-dependent plasticity (STDP) to enforce temporal coding. At the network level, we optimize multiscale learning, leveraging both temporal spike synchrony (van Rossum metrics) and combinatorial population coding (rank-order patterns). This approach bridges single-cell biophysics with ensemble-level computation, enabling efficient simulation of gustatory pathways while retaining biological fidelity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00010v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vladimir A. Lazovsky, Sergey V. Stasenko, Victor B. Kazantsev</dc:creator>
    </item>
    <item>
      <title>Robust State-space Reconstruction of Brain Dynamics via Bootstrap Monte Carlo SSA</title>
      <link>https://arxiv.org/abs/2510.00011</link>
      <description>arXiv:2510.00011v1 Announce Type: new 
Abstract: Reconstructing latent state-space geometry from time series provides a powerful route to studying nonlinear dynamics across complex systems. Delay-coordinate embedding provides the theoretical basis but assumes long, noise-free recordings, which many domains violate. In neuroimaging, for example, fMRI is short and noisy; low sampling and strong red noise obscure oscillations and destabilize embeddings. We propose bootstrap Monte Carlo SSA with a red-noise null and bootstrap stability to retain only oscillatory modes that reproducibly exceed noise. This produces reconstructions that are red-noise-robust and mode-robust, enhancing determinism and stabilizing subsequent embeddings. Our results show that BMC-SSA improves the reliability of functional measures and uncovers differences in state-space dynamics in fMRI, offering a general framework for robust embeddings of noisy, finite signals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00011v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sir-Lord Wiafe, Carter Hinsley, Vince D. Calhoun</dc:creator>
    </item>
    <item>
      <title>Evolutionary Kuramoto dynamics unravels origins of chimera states in neural populations</title>
      <link>https://arxiv.org/abs/2510.00423</link>
      <description>arXiv:2510.00423v1 Announce Type: new 
Abstract: Neural synchronization is central to cognition However, incomplete synchronization often produces chimera states where coherent and incoherent dynamics coexist. While previous studies have explored such patterns using networks of coupled oscillators, it remains unclear why neurons commit to communication or how chimera states persist. Here, we investigate the coevolution of neuronal phases and communication strategies on directed, weighted networks, where interaction payoffs depend on phase alignment and may be asymmetric due to unilateral communication. We find that both connection weights and directionality influence the stability of communicative strategies -- and, consequently, full synchronization -- as well as the strategic nature of neuronal interactions. Applying our framework to the C. elegans connectome, we show that emergent payoff structures, such as the snowdrift game, underpin the formation of chimera states. Our computational results demonstrate a promising neurogame-theoretic perspective, leveraging evolutionary graph theory to shed light on mechanisms of neuronal coordination beyond classical synchronization models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00423v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Zdyrski, Scott Pauls, Feng Fu</dc:creator>
    </item>
    <item>
      <title>Emergence of robust looming selectivity via coordinated inhibitory neural computations</title>
      <link>https://arxiv.org/abs/2510.00498</link>
      <description>arXiv:2510.00498v1 Announce Type: new 
Abstract: In the locust's lobula giant movement detector neural pathways, four categories of inhibition, i.e., global inhibition, self-inhibition, lateral inhibition, and feed-forward inhibition, have been functionally explored in the context of looming perception. However, their combined influence on shaping selectivity to looming motion remains unclear. Driven by recent physiological advancements, this paper offers new insights into the roles of these inhibitory mechanisms at multiple levels and scales in simulations, refining the specific selectivity for responding only to objects approaching the eyes while remaining unresponsive to other forms of movement. Within a feed-forward, multi-layer neural network framework, global inhibition, lateral inhibition, self-inhibition, and feed-forward inhibition are integrated. Global inhibition acts as an immediate feedback mechanism, normalising light intensities delivered by ommatidia, particularly addressing low-contrast looming. Self-inhibition, modelled numerically for the first time, suppresses translational motion. Lateral inhibition is formed by delayed local excitation spreading across a larger area. Notably, self-inhibition and lateral inhibition are sequential in time and are combined through feed-forward inhibition, which indicates the angular size subtended by moving objects. Together, these inhibitory processes attenuate motion-induced excitation at multiple levels and scales. This research suggests that self-inhibition may act earlier than lateral inhibition to rapidly reduce excitation in situ, thereby suppressing translational motion, and global inhibition can modulate excitation on a finer scale, enhancing selectivity in higher contrast range.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00498v1</guid>
      <category>q-bio.NC</category>
      <category>cs.NE</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Qinbing Fu, Ziyan Qin</dc:creator>
    </item>
    <item>
      <title>Emergence of Deviance Detection in Cortical Cultures through Maturation, Criticality, and Early Experience</title>
      <link>https://arxiv.org/abs/2510.00764</link>
      <description>arXiv:2510.00764v1 Announce Type: new 
Abstract: Mismatch negativity (MMN) in humans reflects deviance detection (DD), a core neural mechanism of predictive processing. However, the fundamental principles by which DD emerges and matures during early cortical development-potentially providing a neuronal scaffold for MMN-remain unclear. Here, we tracked the development of DD in dissociated cortical cultures grown on high-density CMOS microelectrode arrays from 10 to 35 days in vitro (DIV). Cultures were stimulated with oddball and many-standards control paradigms while spontaneous and evoked activity were recorded longitudinally. At early stages, stimulus-evoked responses were confined to fast components reflecting direct activation. From DIV15-20 onward, robust late responses appeared, and deviant stimuli progressively evoked stronger responses than frequent and control stimuli, marking the onset of DD. By DIV30, responses became stronger, faster, and more temporally precise. Neuronal avalanche analysis revealed a gradual transition from subcritical to near-critical dynamics, with cultures exhibiting power-law statistics showing the strongest deviant responses. Nonetheless, DD was also present in non-critical networks, indicating that criticality is not required for its emergence but instead stabilizes and amplifies predictive processing as networks mature. Early oddball experience reinforces the deviant pathway, resulting in faster conduction along those circuits. However, as frequent and deviant pathways become less distinct, the deviance detection index is reduced. Together, these findings demonstrate that DD arises intrinsically through local circuit maturation, while self-organization toward criticality and early experience further refine its strength and timing, providing mechanistic insight into predictive coding in simplified cortical networks and informing the design of adaptive, prediction-sensitive artificial systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00764v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhuo Zhang, Amit Yaron, Dai Akita, Tomoyo Isoguchi Shiramatsu, Zenas C. Chao, Hirokazu Takahashi</dc:creator>
    </item>
    <item>
      <title>Some Further Developments on a Neurobiologically-based Model for Color Sensations in Humans</title>
      <link>https://arxiv.org/abs/2510.01000</link>
      <description>arXiv:2510.01000v1 Announce Type: new 
Abstract: At HVEI-2012, I presented a neurobiologically-based model for trichromatic color sensations in humans, mapping the neural substrate for color sensations to V1-L4: the thalamic recipient layer of the primary visual cortex. In this paper, I propose that V1-L4 itself consists of three distinct sub-layers that directly correspond to the three primary color sensations: blue, red, and green. Furthermore, I apply this model to three aspects of color vision: the three-dimensional (3D) color solid, dichromatism, and ocular agnosticism. Regarding these aspects further: (1) 3D color solid: V1-L4 is known to exhibit a gradient of cell densities from its outermost layer (i.e., its pia side) to its innermost layer (i.e., its white matter side). Taken together with the proposition that the population size of a cell assembly directly corresponds with the magnitude of a color sensation, it can be inferred that the neurobiologically-based color solid is a tilted cuboid. (2) Chromatic color blindness: Using deuteranopia as an example, at the retinal level, M-cones are lost and replaced by L-cones. However, at the cortical level, deuteranopia manifests as a fusion of the two bottom layers of V1-L4. (3) Ocular agnosticism: Although color sensation is monocular, we normally are not aware of which eye we are seeing with. This visual phenomenon can be explained by the nature of ocular integration within V1-L4. A neurobiologically-based model for human color sensations could significantly contribute to future engineering efforts aimed at enhancing human color experiences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01000v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charles Q. Wu</dc:creator>
    </item>
    <item>
      <title>WaveMind: Towards a Conversational EEG Foundation Model Aligned to Textual and Visual Modalities</title>
      <link>https://arxiv.org/abs/2510.00032</link>
      <description>arXiv:2510.00032v1 Announce Type: cross 
Abstract: Electroencephalography (EEG) interpretation using multimodal large language models (MLLMs) offers a novel approach for analyzing brain signals. However, the complex nature of brain activity introduces critical challenges: EEG signals simultaneously encode both cognitive processes and intrinsic neural states, creating a mismatch in EEG paired-data modality that hinders effective cross-modal representation learning. Through a pivot investigation, we uncover complementary relationships between these modalities. Leveraging this insight, we propose mapping EEG signals and their corresponding modalities into a unified semantic space to achieve generalized interpretation. To fully enable conversational capabilities, we further introduce WaveMind-Instruct-338k, the first cross-task EEG dataset for instruction tuning. The resulting model demonstrates robust classification accuracy while supporting flexible, open-ended conversations across four downstream tasks, thereby offering valuable insights for both neuroscience research and the development of general-purpose EEG models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00032v1</guid>
      <category>eess.SP</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziyi Zeng, Zhenyang Cai, Yixi Cai, Xidong Wang, Junying Chen, Rongsheng Wang, Yipeng Liu, Siqi Cai, Benyou Wang, Zhiguo Zhang, Haizhou Li</dc:creator>
    </item>
    <item>
      <title>Integration of Calcium Imaging Traces via Deep Generative Modeling</title>
      <link>https://arxiv.org/abs/2501.14615</link>
      <description>arXiv:2501.14615v3 Announce Type: replace 
Abstract: Calcium imaging allows for the parallel measurement of large neuronal populations in a spatially resolved and minimally invasive manner, and has become a gold-standard for neuronal functionality. While deep generative models have been successfully applied to study the activity of neuronal ensembles, their potential for learning single-neuron representations from calcium imaging fluorescence traces remains largely unexplored, and batch effects remain an important hurdle. To address this, we explore supervised variational autoencoder architectures that learn compact representations of individual neurons from fluorescent traces without relying on spike inference algorithms. We find that this approach outperforms state-of-the-art models, preserving biological variability while mitigating batch effects. Across simulated and experimental datasets, this framework enables robust visualization, clustering, and interpretation of single-neuron dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14615v3</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Berta Ros, Mireia Olives-Verger, Caterina Fuses, Josep M Canals, Jordi Soriano, Jordi Abante</dc:creator>
    </item>
    <item>
      <title>Achieving More Human Brain-Like Vision via Human EEG Representational Alignment</title>
      <link>https://arxiv.org/abs/2401.17231</link>
      <description>arXiv:2401.17231v3 Announce Type: replace-cross 
Abstract: Despite advancements in artificial intelligence, object recognition models still lag behind in emulating visual information processing in human brains. Recent studies have highlighted the potential of using neural data to mimic brain processing; however, these often rely on invasive neural recordings from non-human subjects, leaving a critical gap in understanding human visual perception. Addressing this gap, we present, 'Re(presentational)Al(ignment)net', a vision model aligned with human brain activity based on non-invasive EEG, demonstrating a significantly higher similarity to human brain representations. Our innovative image-to-brain multi-layer encoding framework advances human neural alignment by optimizing multiple model layers and enabling the model to efficiently learn and mimic the human brain's visual representational patterns across object categories and different modalities. Our findings suggest that ReAlnets better align artificial neural networks with human brain representations, making it more similar to human brain processing than traditional computer vision models, which takes an important step toward bridging the gap between artificial and human vision and achieving more brain-like artificial intelligence systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.17231v3</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zitong Lu, Yile Wang, Julie D. Golomb</dc:creator>
    </item>
  </channel>
</rss>
