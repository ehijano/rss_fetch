<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 18 Dec 2025 02:34:34 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Time and Relations into Focus: Ontological Foundations of Object-Centric Event Data</title>
      <link>https://arxiv.org/abs/2512.14425</link>
      <description>arXiv:2512.14425v1 Announce Type: new 
Abstract: Object-centric process mining is a new branch of process mining where events are associated with multiple objects, and where object-to-object interactions are essential to understand the process dynamics. Traditional event data models, also called case-centric, are unable to cope with the complexity introduced by these more refined relationships. Several models have been made to move from case-centric to Object-Centric Event Data (OCED), trying to retain simplicity as much as possible. Still, these suffer from inherent ambiguities, and lack a comprehensive support of essential dimensions related to time and (dynamic) relations. In this work, we propose to fill this gap by leveraging a well-founded ontology of events and bringing ontological foundations to OCED, with a three-step approach. First, we start from key open issues reported in the literature regarding current OCED metamodels, and witness their ambiguity and expressiveness limitations on illustrative and representative examples proposed therein. Second, we consider the OCED Core Model, currently proposed as the basis for defining a new standard for object-centric event data, and we enhance it by grounding it on a lightweight version of UFO-B called gUFO, a well-known foundational ontology tailored to the representation of objects, events, time, and their (dynamic) relations. This results in a new metamodel, which we call gOCED. The third contribution then shows how gOCED at once covers the features of existing metamodels preserving their simplicity, and extends them with the essential features needed to overcome the ambiguity and expressiveness issues reported in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14425v1</guid>
      <category>cs.DB</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hosna Hooshyar, Mattia Fumagalli, Marco Montali, Giancarlo Guizzardi</dc:creator>
    </item>
    <item>
      <title>Beyond Text-to-SQL: Autonomous Research-Driven Database Exploration with DAR</title>
      <link>https://arxiv.org/abs/2512.14622</link>
      <description>arXiv:2512.14622v1 Announce Type: new 
Abstract: Large language models can already query databases, yet most existing systems remain reactive: they rely on explicit user prompts and do not actively explore data. We introduce DAR (Data Agnostic Researcher), a multi-agent system that performs end-to-end database research without human-initiated queries. DAR orchestrates specialized AI agents across three layers: initialization (intent inference and metadata extraction), execution (SQL and AI-based query synthesis with iterative validation), and synthesis (report generation with built-in quality control). All reasoning is executed directly inside BigQuery using native generative AI functions, eliminating data movement and preserving data governance. On a realistic asset-incident dataset, DAR completes the full analytical task in 16 minutes, compared to 8.5 hours for a professional analyst (approximately 32x times faster), while producing useful pattern-based insights and evidence-grounded recommendations. Although human experts continue to offer deeper contextual interpretation, DAR excels at rapid exploratory analysis. Overall, this work shifts database interaction from query-driven assistance toward autonomous, research-driven exploration within cloud data warehouses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14622v1</guid>
      <category>cs.DB</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ostap Vykhopen, Viktoria Skorik, Maxim Tereschenko, Veronika Solopova</dc:creator>
    </item>
    <item>
      <title>TiCard: Deployable EXPLAIN-only Residual Learning for Cardinality Estimation</title>
      <link>https://arxiv.org/abs/2512.14358</link>
      <description>arXiv:2512.14358v1 Announce Type: cross 
Abstract: Cardinality estimation is a key bottleneck for cost-based query optimization, yet deployable improvements remain difficult: classical estimators miss correlations, while learned estimators often require workload-specific training pipelines and invasive integration into the optimizer. This paper presents TiCard, a low intrusion, correction-based framework that augments (rather than replaces) a database's native estimator. TiCard learns multiplicative residual corrections using EXPLAIN-only features, and uses EXPLAIN ANALYZE only for offline labels. We study two practical instantiations: (i) a Gradient Boosting Regressor for sub-millisecond inference, and (ii) TabPFN, an in-context tabular foundation model that adapts by refreshing a small reference set without gradient retraining. On TiDB with TPCH and the Join Order Benchmark, in a low-trace setting (263 executions total; 157 used for learning), TiCard improves operator-level tail accuracy substantially: P90 Q-error drops from 312.85 (native) to 13.69 (TiCard-GBR), and P99 drops from 37,974.37 to 3,416.50 (TiCard-TabPFN), while a join-only policy preserves near-perfect median behavior. We position TiCard as an AI4DB building block focused on deployability: explicit scope, conservative integration policies, and an integration roadmap from offline correction to in-optimizer use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14358v1</guid>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qizhi Wang</dc:creator>
    </item>
    <item>
      <title>Chase Anonymisation: Privacy-Preserving Knowledge Graphs with Logical Reasoning</title>
      <link>https://arxiv.org/abs/2410.12418</link>
      <description>arXiv:2410.12418v2 Announce Type: replace 
Abstract: We propose a novel framework to enable Knowledge Graphs (KGs) sharing while ensuring that information that should remain private is not directly released nor indirectly exposed via derived knowledge, maintaining at the same time the embedded knowledge of the KGs to support business downstream tasks. Our approach produces a privacy-preserving KG as an augmentation of the input one via controlled addition of nodes and edges as well as re-labeling of nodes and perturbation of weights. We introduce a novel privacy measure for KGs, which considers derived knowledge, a new utility metric that captures the business semantics we want to preserve, and propose two novel anonymisation algorithms. Our extensive experimental evaluation, with both synthetic graphs and real-world datasets, confirms the effectiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12418v2</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the 42nd IEEE International Conference on Data Engineering (ICDE) 2026</arxiv:journal_reference>
      <dc:creator>Luigi Bellomarini, Costanza Catalano, Andrea Coletta, Michela Iezzi, Pierangela Samarati</dc:creator>
    </item>
    <item>
      <title>MatTools: Benchmarking Large Language Models for Materials Science Tools</title>
      <link>https://arxiv.org/abs/2505.10852</link>
      <description>arXiv:2505.10852v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are increasingly applied to materials science questions, including literature comprehension, property prediction, materials discovery and alloy design. At the same time, a wide range of physics-based computational approaches have been developed in which materials properties can be calculated. Here, we propose a benchmark application to evaluate the proficiency of LLMs to answer materials science questions through the generation and safe execution of codes based on such physics-based computational materials science packages. MatTools is built on two complementary components: a materials simulation tool question-answer (QA) benchmark and a real-world tool-usage benchmark. We designed an automated methodology to efficiently collect real-world materials science tool-use examples. The QA benchmark, derived from the pymatgen (Python Materials Genomics) codebase and documentation, comprises 69,225 QA pairs that assess the ability of an LLM to understand materials science tools. The real-world benchmark contains 49 tasks (138 subtasks) requiring the generation of functional Python code for materials property calculations. Our evaluation of diverse LLMs yields three key insights: (1)Generalists outshine specialists;(2)AI knows AI; and (3)Simpler is better. MatTools provides a standardized framework for assessing and improving LLM capabilities for materials science tool applications, facilitating the development of more effective AI systems for materials science and general scientific research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10852v2</guid>
      <category>cond-mat.mtrl-sci</category>
      <category>cs.CL</category>
      <category>cs.DB</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Siyu Liu, Bo Hu, Beilin Ye, Jiamin Xu, David J. Srolovitz, Tongqi Wen</dc:creator>
    </item>
    <item>
      <title>Towards Lazy B-Trees</title>
      <link>https://arxiv.org/abs/2507.00277</link>
      <description>arXiv:2507.00277v2 Announce Type: replace-cross 
Abstract: Lazy search trees (Sandlund &amp; Wild FOCS 2020, Sandlund &amp; Zhang SODA 2022) are sorted dictionaries whose update and query performance smoothly interpolates between that of efficient priority queues and binary search trees - automatically, depending on actual use; no adjustments are necessary to the data structure to realize the cost savings. In this paper, we design lazy B-trees, a variant of lazy search trees suitable for external memory that generalizes the speedup of B-trees over binary search trees wrt. input/output operations to the same smooth interpolation regime.
  A key technical difficulty to overcome is the lack of a (fully satisfactory) external variant of biased search trees, on which lazy search trees crucially rely. We give a construction for a subset of performance guarantees sufficient to realize external-memory lazy search trees, which we deem of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00277v2</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4230/LIPIcs.MFCS.2025.87</arxiv:DOI>
      <dc:creator>Casper Moldrup Rysgaard, Sebastian Wild</dc:creator>
    </item>
  </channel>
</rss>
