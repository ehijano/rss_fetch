<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 20 Feb 2026 05:00:53 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Bloc Voting on Single Peaked Preferences</title>
      <link>https://arxiv.org/abs/2602.16734</link>
      <description>arXiv:2602.16734v1 Announce Type: new 
Abstract: We analyze the winning coalitions that arise under Bloc voting when voters preferences are single-peaked. For small numbers of candidates and numbers of winners, we determine conditions under which candidates in winning coalitions are adjacent. We also analyze the results of pairwise contests between winning and losing candidates and assess when the winning coalitions satisfy several proposed extensions of the Condorcet criterion to multiwinner voting methods. Finally, we use Monte Carlo simulations to investigate how frequently these coalitions arise under different assumptions about voter behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16734v1</guid>
      <category>cs.GT</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ariel Calver (Seoyoung), Serena Pallan (Seoyoung),  Alice (Seoyoung),  Park, Jennifer Wilson</dc:creator>
    </item>
    <item>
      <title>Signaling in Data Markets via Free Samples</title>
      <link>https://arxiv.org/abs/2602.16919</link>
      <description>arXiv:2602.16919v1 Announce Type: new 
Abstract: We study a setting in which a data buyer seeks to estimate an unknown parameter by purchasing samples from one of K data sellers. Each seller has privately known data quality (e.g., high vs. low variance) and a private per-sample cost. We consider a multi-stage game in which the first stage is a free-trial stage in which the sellers have the option of signaling data quality by offering a few samples of data for free. Buyers update their beliefs based on the sample variance of the free data and then run a procurement auction to buy data in a second stage. For the auction stage, we characterize an approximately optimal Bayesian incentive compatible mechanism: the buyer selects a single seller by minimizing a belief-adjusted virtual cost and chooses the purchased sample size as a function of posterior quality and virtual cost. For the free-trial stage, we characterize the equilibrium, taking the above mechanism as the continuation game. Free trials may fail to emerge: for some parameters, all sellers reveal zero samples. However, under sufficiently strong competition (large K), there is an equilibrium in which sellers reveal the maximum allowable number of samples; in fact, it is the unique equilibrium.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16919v1</guid>
      <category>cs.GT</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nivasini Ananthakrishnan, Alireza Fallah, Michael I. Jordan</dc:creator>
    </item>
    <item>
      <title>Discovering Multiagent Learning Algorithms with Large Language Models</title>
      <link>https://arxiv.org/abs/2602.16928</link>
      <description>arXiv:2602.16928v1 Announce Type: new 
Abstract: Much of the advancement of Multi-Agent Reinforcement Learning (MARL) in imperfect-information games has historically depended on manual iterative refinement of baselines. While foundational families like Counterfactual Regret Minimization (CFR) and Policy Space Response Oracles (PSRO) rest on solid theoretical ground, the design of their most effective variants often relies on human intuition to navigate a vast algorithmic design space. In this work, we propose the use of AlphaEvolve, an evolutionary coding agent powered by large language models, to automatically discover new multiagent learning algorithms. We demonstrate the generality of this framework by evolving novel variants for two distinct paradigms of game-theoretic learning. First, in the domain of iterative regret minimization, we evolve the logic governing regret accumulation and policy derivation, discovering a new algorithm, Volatility-Adaptive Discounted (VAD-)CFR. VAD-CFR employs novel, non-intuitive mechanisms-including volatility-sensitive discounting, consistency-enforced optimism, and a hard warm-start policy accumulation schedule-to outperform state-of-the-art baselines like Discounted Predictive CFR+. Second, in the regime of population based training algorithms, we evolve training-time and evaluation-time meta strategy solvers for PSRO, discovering a new variant, Smoothed Hybrid Optimistic Regret (SHOR-)PSRO. SHOR-PSRO introduces a hybrid meta-solver that linearly blends Optimistic Regret Matching with a smoothed, temperature-controlled distribution over best pure strategies. By dynamically annealing this blending factor and diversity bonuses during training, the algorithm automates the transition from population diversity to rigorous equilibrium finding, yielding superior empirical convergence compared to standard static meta-solvers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16928v1</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zun Li, John Schultz, Daniel Hennes, Marc Lanctot</dc:creator>
    </item>
    <item>
      <title>Learning to Recommend in Unknown Games</title>
      <link>https://arxiv.org/abs/2602.16998</link>
      <description>arXiv:2602.16998v1 Announce Type: new 
Abstract: We study preference learning through recommendations in multi-agent game settings, where a moderator repeatedly interacts with agents whose utility functions are unknown. In each round, the moderator issues action recommendations and observes whether agents follow or deviate from them. We consider two canonical behavioral feedback models-best response and quantal response-and study how the information revealed by each model affects the learnability of agents' utilities. We show that under quantal-response feedback the game is learnable, up to a positive affine equivalence class, with logarithmic sample complexity in the desired precision, whereas best-response feedback can only identify a larger set of agents' utilities. We give a complete geometric characterization of this set. Moreover, we introduce a regret notion based on agents' incentives to deviate from recommendations and design an online algorithm with low regret under both feedback models, with bounds scaling linearly in the game dimension and logarithmically in time. Our results lay a theoretical foundation for AI recommendation systems in strategic multi-agent environments, where recommendation compliances are shaped by strategic interaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16998v1</guid>
      <category>cs.GT</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arwa Alanqary, Zakaria Baba, Manxi Wu, Alexandre M. Bayen</dc:creator>
    </item>
    <item>
      <title>Prophet Inequality with Conservative Prediction</title>
      <link>https://arxiv.org/abs/2602.17358</link>
      <description>arXiv:2602.17358v1 Announce Type: new 
Abstract: Prophet inequalities compare online stopping strategies against an omniscient "prophet" using distributional knowledge. In this work, we augment this model with a conservative prediction of the maximum realized value. We quantify the quality of this prediction using a parameter $\alpha \in [0,1]$, ranging from inaccurate to perfect. Our goal is to improve performance when predictions are accurate (consistency) while maintaining theoretical guarantees when they are not (robustness). We propose a threshold-based strategy oblivious to $\alpha$ (i.e., with $\alpha$ unknown to the algorithm) that matches the classic competitive ratio of $1/2$ at $\alpha=0$ and improves smoothly to $3/4$ at $\alpha=1$. We further prove that simultaneously achieving better than $3/4$ at $\alpha=1$ while maintaining $1/2$ at $\alpha=0$ is impossible. Finally, when $\alpha$ is known in advance, we present a strategy achieving a tight competitive ratio of $\frac{1}{2-\alpha}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17358v1</guid>
      <category>cs.GT</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johannes Br\"ustle, Ilan Reuven Cohen, Stefano Leonardi</dc:creator>
    </item>
    <item>
      <title>Algorithmic Collusion at Test Time: A Meta-game Design and Evaluation</title>
      <link>https://arxiv.org/abs/2602.17203</link>
      <description>arXiv:2602.17203v1 Announce Type: cross 
Abstract: The threat of algorithmic collusion, and whether it merits regulatory intervention, remains debated, as existing evaluations of its emergence often rely on long learning horizons, assumptions about counterparty rationality in adopting collusive strategies, and symmetry in hyperparameters and economic settings among players. To study collusion risk, we introduce a meta-game design for analyzing algorithmic behavior under test-time constraints. We model agents as possessing pretrained policies with distinct strategic characteristics (e.g., competitive, naively cooperative, robustly collusive), and formulate the problem as selecting a meta-strategy that combines a pretrained, initial policy with an in-game adaptation rule. We seek to examine whether collusion can emerge under rational choices and how agents co-adapt toward cooperation or competition. To this end, we sample normal-form empirical games over meta-strategy profiles, % across random initial game states, compute relevant game statistics (e.g., payoffs against individuals and regret against an equilibrium mixture of opponents), and construct empirical best-response graphs to uncover strategic relationships. We evaluate both reinforcement-learning and LLM-based strategies in repeated pricing games under symmetric and asymmetric cost settings, and present findings on the feasibility of algorithmic collusion and the effectiveness of pricing strategies in practical ``test-time'' environments.
  The source code and the full paper with appendix are available at: https://github.com/chailab-rutgers/CollusionMetagame.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17203v1</guid>
      <category>cs.MA</category>
      <category>cs.GT</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuhong Luo, Daniel Schoepflin, Xintong Wang</dc:creator>
    </item>
    <item>
      <title>Extending quantum theory with AI-assisted deterministic game theory</title>
      <link>https://arxiv.org/abs/2602.17213</link>
      <description>arXiv:2602.17213v1 Announce Type: cross 
Abstract: We present an AI-assisted framework for predicting individual runs of complex quantum experiments, including contextuality and causality (adaptive measurements), within our long-term programme of discovering a local hidden-variable theory that extends quantum theory. In order to circumvent impossibility theorems, we replace the assumption of free choice (measurement independence and parameter independence) with a weaker, compatibilistic version called contingent free choice.
  Our framework is based on interpreting complex quantum experiments as a Chess-like game between observers and the universe, which is seen as an economic agent minimizing action. The game structures corresponding to generic experiments such as fixed-causal-order process matrices or causal contextuality scenarios, together with a deterministic non-Nashian resolution algorithm that abandons unilateral deviation assumptions (free choice) and assumes Perfect Prediction instead, were described in previous work.
  In this new research, we learn the reward functions of the game, which contain a hidden variable, using neural networks. The cost function is the Kullback-Leibler divergence between the frequency histograms obtained through many deterministic runs of the game and the predictions of the extended Born rule.
  Using our framework on the specific case of the EPR 2-2-2 experiment acts as a proof-of-concept and a toy local-realist hidden-variable model that non-Nashian quantum theory is a promising avenue towards a local hidden-variable theory. Our framework constitutes a solid foundation, which can be further expanded in order to fully discover a complete quantum theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17213v1</guid>
      <category>quant-ph</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian Pauschitz, Ben Moseley, Ghislain Fourny</dc:creator>
    </item>
    <item>
      <title>Linear Convergence in Games with Delayed Feedback via Extra Prediction</title>
      <link>https://arxiv.org/abs/2602.17486</link>
      <description>arXiv:2602.17486v1 Announce Type: cross 
Abstract: Feedback delays are inevitable in real-world multi-agent learning. They are known to severely degrade performance, and the convergence rate under delayed feedback is still unclear, even for bilinear games. This paper derives the rate of linear convergence of Weighted Optimistic Gradient Descent-Ascent (WOGDA), which predicts future rewards with extra optimism, in unconstrained bilinear games. To analyze the algorithm, we interpret it as an approximation of the Extra Proximal Point (EPP), which is updated based on farther future rewards than the classical Proximal Point (PP). Our theorems show that standard optimism (predicting the next-step reward) achieves linear convergence to the equilibrium at a rate $\exp(-\Theta(t/m^{5}))$ after $t$ iterations for delay $m$. Moreover, employing extra optimism (predicting farther future reward) tolerates a larger step size and significantly accelerates the rate to $\exp(-\Theta(t/(m^{2}\log m)))$. Our experiments also show accelerated convergence driven by the extra optimism and are qualitatively consistent with our theorems. In summary, this paper validates that extra optimism is a promising countermeasure against performance degradation caused by feedback delays.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17486v1</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <category>math.OC</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuma Fujimoto, Kenshi Abe, Kaito Ariu</dc:creator>
    </item>
    <item>
      <title>Time-Varyingness in Auction Breaks Revenue Equivalence</title>
      <link>https://arxiv.org/abs/2410.12306</link>
      <description>arXiv:2410.12306v2 Announce Type: replace 
Abstract: Auction is applied for trade with various mechanisms. A simple but practical question is which mechanism, typically first-price or second-price auctions, is preferred from the perspective of bidders or sellers. A celebrated answer is revenue equivalence, where each bidder's equilibrium payoff is proven to be independent of auction mechanisms (and a seller's revenue, too). In reality, however, auction environments like the value distribution of items would vary over time, and such equilibrium bidding cannot always be achieved. Indeed, bidders must continue to track their equilibrium bidding by learning in first-price auctions, but they can keep their equilibrium bidding in second-price auctions. This study discusses whether and how revenue equivalence is violated in the long run by comparing the time series of non-equilibrium bidding in first-price auctions with those of equilibrium bidding in second-price auctions. We characterize the value distribution by two parameters: its basis value, which means the lowest price to bid, and its value interval, which means the width of possible values. Surprisingly, our theorems and experiments find that revenue equivalence is broken by the correlation between the basis value and the value interval, uncovering a novel phenomenon that could occur in the real world.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12306v2</guid>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <category>econ.TH</category>
      <category>math.DS</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuma Fujimoto, Kaito Ariu, Kenshi Abe</dc:creator>
    </item>
    <item>
      <title>Computing Perfect Bayesian Equilibria, with Application to Empirical Game-Theoretic Analysis</title>
      <link>https://arxiv.org/abs/2602.15233</link>
      <description>arXiv:2602.15233v2 Announce Type: replace 
Abstract: Perfect Bayesian Equilibrium (PBE) is a refinement of the Nash equilibrium for imperfect-information extensive-form games (EFGs) that enforces consistency between the two components of a solution: agents' strategy profile describing their decisions at information sets and the belief system quantifying their uncertainty over histories within an information set. We present a scalable approach for computing a PBE of an arbitrary two-player EFG. We adopt the definition of PBE enunciated by Bonanno in 2011 using a consistency concept based on the theory of belief revision due to Alchourr\'{o}n, G\"{a}rdenfors, and Makinson. Our algorithm for finding a PBE is an adaptation of Counterfactual Regret Minimization (CFR) that minimizes the expected regret at each information set given a belief system, while maintaining the necessary consistency criteria. We prove that our algorithm is correct for two-player zero-sum games and has a reasonable slowdown in time-complexity relative to classical CFR given the additional computation needed for refinement. We also experimentally demonstrate the competent performance of PBE-CFR in terms of equilibrium quality and running time on medium-to-large non-zero-sum EFGs. Finally, we investigate the effectiveness of using PBE for strategy exploration in empirical game-theoretic analysis. Specifically, we compute PBE as a meta-strategy solver (MSS) in a tree-exploiting variant of Policy Space Response Oracles (TE-PSRO). Our experiments show that PBE as an MSS leads to higher-quality empirical EFG models with complex imperfect information structures compared to MSSs based on an unrefined Nash equilibrium.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.15233v2</guid>
      <category>cs.GT</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christine Konicki, Mithun Chakraborty, Michael P. Wellman</dc:creator>
    </item>
    <item>
      <title>A Parametric Contextual Online Learning Theory of Brokerage</title>
      <link>https://arxiv.org/abs/2407.01566</link>
      <description>arXiv:2407.01566v3 Announce Type: replace-cross 
Abstract: We study the role of contextual information in the online learning problem of brokerage between traders. In this sequential problem, at each time step, two traders arrive with secret valuations about an asset they wish to trade. The learner (a broker) suggests a trading (or brokerage) price based on contextual data about the asset and the market conditions. Then, the traders reveal their willingness to buy or sell based on whether their valuations are higher or lower than the brokerage price. A trade occurs if one of the two traders decides to buy and the other to sell, i.e., if the broker's proposed price falls between the smallest and the largest of their two valuations. We design algorithms for this problem and prove optimal theoretical regret guarantees under various standard assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01566v3</guid>
      <category>q-fin.CP</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fran\c{c}ois Bachoc, Tommaso Cesari, Roberto Colomboni</dc:creator>
    </item>
    <item>
      <title>Nearly-Optimal Bandit Learning in Stackelberg Games with Side Information</title>
      <link>https://arxiv.org/abs/2502.00204</link>
      <description>arXiv:2502.00204v2 Announce Type: replace-cross 
Abstract: We study the problem of online learning in Stackelberg games with side information between a leader and a sequence of followers. In every round the leader observes contextual information and commits to a mixed strategy, after which the follower best-responds. We provide learning algorithms for the leader which achieve $O(T^{1/2})$ regret under bandit feedback, an improvement from the previously best-known rates of $O(T^{2/3})$. Our algorithms rely on a reduction to linear contextual bandits in the utility space: In each round, a linear contextual bandit algorithm recommends a utility vector, which our algorithm inverts to determine the leader's mixed strategy. We extend our algorithms to the setting in which the leader's utility function is unknown, and also apply it to the problems of bidding in second-price auctions with side information and online Bayesian persuasion with public and private states. Finally, we observe that our algorithms empirically outperform previous results on numerical simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00204v2</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maria-Florina Balcan, Martino Bernasconi, Matteo Castiglioni, Andrea Celli, Keegan Harris, Zhiwei Steven Wu</dc:creator>
    </item>
    <item>
      <title>AI-Assisted Decision Making with Human Learning</title>
      <link>https://arxiv.org/abs/2502.13062</link>
      <description>arXiv:2502.13062v2 Announce Type: replace-cross 
Abstract: AI systems increasingly support human decision-making. In many cases, despite the algorithm's superior performance, the final decision remains in human hands. For example, an AI may assist doctors in determining which diagnostic tests to run, but the doctor ultimately makes the diagnosis. This paper studies such AI-assisted decision-making settings, where the human learns through repeated interactions with the algorithm. In our framework, the algorithm -- designed to maximize decision accuracy according to its own model -- determines which features the human can consider. The human then makes a prediction based on their own less accurate model. We observe that the discrepancy between the algorithm's model and the human's model creates a fundamental tradeoff: Should the algorithm prioritize recommending more informative features, encouraging the human to learn their importance, even if it results in less accurate predictions in the short term until learning occurs? Or is it preferable to forgo educating the human and instead select features that align more closely with their existing understanding, minimizing the immediate cost of learning? Our analysis reveals how this trade-off is shaped by both the algorithm's patience (the time-discount rate of its objective over multiple periods) and the human's willingness and ability to learn. We show that optimal feature selection has a surprisingly clean combinatorial characterization, reducible to a stationary sequence of feature subsets that is tractable to compute. As the algorithm becomes more "patient" or the human's learning improves, the algorithm increasingly selects more informative features, enhancing both prediction accuracy and the human's understanding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13062v2</guid>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.HC</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3736252.3742492</arxiv:DOI>
      <dc:creator>Gali Noti, Kate Donahue, Jon Kleinberg, Sigal Oren</dc:creator>
    </item>
    <item>
      <title>Two-Player Zero-Sum Games with Bandit Feedback</title>
      <link>https://arxiv.org/abs/2506.14518</link>
      <description>arXiv:2506.14518v4 Announce Type: replace-cross 
Abstract: We study a two-player zero-sum game in which the row player aims to maximize their payoff against a competing column player, under an unknown payoff matrix estimated through bandit feedback. We propose three algorithms based on the Explore-Then-Commit (ETC) and action pair elimination frameworks. The first adapts it to zero-sum games, the second incorporates adaptive elimination that leverages the $\varepsilon$-Nash Equilibrium property to efficiently select the optimal action pair, and the third extends the elimination algorithm by employing non-uniform exploration. Our objective is to demonstrate the applicability of ETC and action pair elimination algorithms in a zero-sum game setting by focusing on learning pure strategy Nash Equilibria. A key contribution of our work is a derivation of instance-dependent upper bounds on the expected regret of our proposed algorithms, which has received limited attention in the literature on zero-sum games. Particularly, after $T$ rounds, we achieve an instance-dependent regret upper bounds of $O(\Delta + \sqrt{T})$ for ETC in zero-sum game setting and $O\left(\frac{\log (T \Delta^2)}{\Delta}\right)$ for the adaptive elimination algorithm and its variant with non-uniform exploration, where $\Delta$ denotes the suboptimality gap. Therefore, our results indicate that the ETC and action pair elimination algorithms perform effectively in zero-sum game settings, achieving regret bounds comparable to existing methods while providing insight through instance-dependent analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14518v4</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elif Y{\i}lmaz, Christos Dimitrakakis</dc:creator>
    </item>
    <item>
      <title>Puzzle it Out: Local-to-Global World Model for Offline Multi-Agent Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2601.07463</link>
      <description>arXiv:2601.07463v2 Announce Type: replace-cross 
Abstract: Offline multi-agent reinforcement learning (MARL) aims to solve cooperative decision-making problems in multi-agent systems using pre-collected datasets. Existing offline MARL methods primarily constrain training within the dataset distribution, resulting in overly conservative policies that struggle to generalize beyond the support of the data. While model-based approaches offer a promising solution by expanding the original dataset with synthetic data generated from a learned world model, the high dimensionality, non-stationarity, and complexity of multi-agent systems make it challenging to accurately estimate the transitions and reward functions in offline MARL. Given the difficulty of directly modeling joint dynamics, we propose a local-to-global (LOGO) world model, a novel framework that leverages local predictions-which are easier to estimate-to infer global state dynamics, thus improving prediction accuracy while implicitly capturing agent-wise dependencies. Using the trained world model, we generate synthetic data to augment the original dataset, expanding the effective state-action space. To ensure reliable policy learning, we further introduce an uncertainty-aware sampling mechanism that adaptively weights synthetic data by prediction uncertainty, reducing approximation error propagation to policies. In contrast to conventional ensemble-based methods, our approach requires only an additional encoder for uncertainty estimation, significantly reducing computational overhead while maintaining accuracy. Extensive experiments across 8 scenarios against 8 baselines demonstrate that our method surpasses state-of-the-art baselines on standard offline MARL benchmarks, establishing a new model-based baseline for generalizable offline multi-agent learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07463v2</guid>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sijia li, Xinran Li, Shibo Chen, Jun Zhang</dc:creator>
    </item>
    <item>
      <title>A Theory of Network Games Part 1: Utility Representations</title>
      <link>https://arxiv.org/abs/2602.16071</link>
      <description>arXiv:2602.16071v2 Announce Type: replace-cross 
Abstract: We demonstrate that a ubiquitous feature of network games, bilateral strategic interactions, is equivalent to having player utilities that are additively separable across opponents. We distinguish two formal notions of bilateral strategic interactions. Opponent independence means that player i's preferences over opponent j's action do not depend on what other opponents do. Strategic independence means that how opponent j's choice influences i's preference between any two actions does not depend on what other opponents do. If i's preferences jointly satisfy both conditions, then we can represent her preferences over strategy profiles using an additively separable utility. If i's preferences satisfy only strategic independence, then we can still represent her preferences over just her own actions using an additively separable utility. Common utilities based on a linear aggregate of opponent actions satisfy strategic independence and are therefore strategically equivalent to additively separable utilities--in fact, we can assume a utility that is linear in opponent actions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16071v2</guid>
      <category>econ.TH</category>
      <category>cs.GT</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joseph Root, Evan Sadler</dc:creator>
    </item>
  </channel>
</rss>
