<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 15 Apr 2025 03:10:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>On Quorum Sizes in DAG-Based BFT Protocols</title>
      <link>https://arxiv.org/abs/2504.08048</link>
      <description>arXiv:2504.08048v1 Announce Type: new 
Abstract: Several prominent DAG-based blockchain protocols, such as DAG-Rider, Tusk, and Bullshark, completely separate between equivocation elimination and committing; equivocation is handled through the use of a reliable Byzantine broadcast black-box protocol, while committing is handled by an independent DAG-based protocol. With such an architecture, a natural question that we study in this paper is whether the DAG protocol would work when the number of nodes (or validators) is only $2f+1$ (when equivocation is eliminated), and whether there are benefits in working with larger number of nodes, i.e., a total of $kf+1$ nodes for $k &gt; 3$.
  We find that while DAG-Rider's correctness is maintained with $2f+1$ nodes, the asynchronous versions of both Tusk and Bullshark inherently depends on having $3f+1$ nodes, regardless of equivocation. We also explore the impact of having larger number of nodes on the expected termination time of these three protocols.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08048v1</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Razya Ladelsky, Roy Friedman</dc:creator>
    </item>
    <item>
      <title>A Hybrid Cloud Management Plane for Data Processing Pipelines</title>
      <link>https://arxiv.org/abs/2504.08225</link>
      <description>arXiv:2504.08225v1 Announce Type: new 
Abstract: As organizations increasingly rely on data-driven insights, the ability to run data intensive applications seamlessly across multiple cloud environments becomes critical for tapping into cloud innovations while complying with various security and regulatory requirements. However, big data application development and deployment remain challenging to accomplish in such environments. With the increasing containerization and modernization of big data applications, we argue that a unified control/management plane now makes sense for running these applications in hybrid cloud environments. To this end, we study the problem of building a generic hybrid-cloud management plane to radically simplify managing big data applications. A generic architecture for hybrid-cloud management, called Titchener, is proposed in this paper. Titchener comprises of independent and loosely coupled local control planes interacting with a highly available public cloud hosted global management plane. We describe a possible instantiation of Titchener based on Kubernetes and address issues related to global service discovery, network connectivity and access control enforcement. We also validate our proposed designs with a real management plane implementation based on a popular big data workflow orchestration in hybrid-cloud environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08225v1</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vignesh Babu, Feng Lu, Haotian Wu, Cameron Moberg</dc:creator>
    </item>
    <item>
      <title>Jupiter: Fast and Resource-Efficient Collaborative Inference of Generative LLMs on Edge Devices</title>
      <link>https://arxiv.org/abs/2504.08242</link>
      <description>arXiv:2504.08242v1 Announce Type: new 
Abstract: Generative large language models (LLMs) have garnered significant attention due to their exceptional capabilities in various AI tasks. Traditionally deployed in cloud datacenters, LLMs are now increasingly moving towards more accessible edge platforms to protect sensitive user data and ensure privacy preservation. The limited computational resources of individual edge devices, however, can result in excessively prolonged inference latency and overwhelmed memory usage. While existing research has explored collaborative edge computing to break the resource wall of individual devices, these solutions yet suffer from massive communication overhead and under-utilization of edge resources. Furthermore, they focus exclusively on optimizing the prefill phase, neglecting the crucial autoregressive decoding phase for generative LLMs. To address that, we propose Jupiter, a fast, scalable, and resource-efficient collaborative edge AI system for generative LLM inference. Jupiter introduces a flexible pipelined architecture as a principle and differentiates its system design according to the differentiated characteristics of the prefill and decoding phases. For prefill phase, Jupiter submits a novel intra-sequence pipeline parallelism and develops a meticulous parallelism planning strategy to maximize resource efficiency; For decoding, Jupiter devises an effective outline-based pipeline parallel decoding mechanism combined with speculative decoding, which further magnifies inference acceleration. Extensive evaluation based on realistic implementation demonstrates that Jupiter remarkably outperforms state-of-the-art approaches under various edge environment setups, achieving up to 26.1x end-to-end latency reduction while rendering on-par generation quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08242v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shengyuan Ye, Bei Ouyang, Liekang Zeng, Tianyi Qian, Xiaowen Chu, Jian Tang, Xu Chen</dc:creator>
    </item>
    <item>
      <title>Self-Stabilizing Weakly Byzantine Perpetual Gathering of Mobile Agents</title>
      <link>https://arxiv.org/abs/2504.08271</link>
      <description>arXiv:2504.08271v1 Announce Type: new 
Abstract: We study the \emph{Byzantine} gathering problem involving $k$ mobile agents with unique identifiers (IDs), $f$ of which are Byzantine. These agents start the execution of a common algorithm from (possibly different) nodes in an $n$-node network, potentially starting at different times. Once started, the agents operate in synchronous rounds. We focus on \emph{weakly} Byzantine environments, where Byzantine agents can behave arbitrarily but cannot falsify their IDs. The goal is for all \emph{non-Byzantine} agents to eventually terminate at a single node simultaneously.
  In this paper, we first prove two impossibility results: (1) for any number of non-Byzantine agents, no algorithm can solve this problem without global knowledge of the network size or the number of agents, and (2) no self-stabilizing algorithm exists if $k\leq 2f$ even with $n$, $k$, $f$, and the length $\Lambda_g$ of the largest ID among IDs of non-Byzantine agents, where the self-stabilizing algorithm enables agents to gather starting from arbitrary (inconsistent) initial states. Next, based on these results, we introduce a \emph{perpetual gathering} problem and propose a self-stabilizing algorithm for this problem. This problem requires that all non-Byzantine agents always be co-located from a certain time onwards. If the agents know $\Lambda_g$ and upper bounds $N$, $K$, $F$ on $n$, $k$, $f$, the proposed algorithm works in $O(K\cdot F\cdot \Lambda_g\cdot X(N))$ rounds, where $X(n)$ is the time required to visit all nodes in a $n$-nodes network. Our results indicate that while no algorithm can solve the original self-stabilizing gathering problem for any $k$ and $f$ even with \emph{exact} global knowledge of the network size and the number of agents, the self-stabilizing perpetual gathering problem can always be solved with just upper bounds on this knowledge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08271v1</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jion Hirose, Ryota Eguchi, Yuichi Sudo</dc:creator>
    </item>
    <item>
      <title>Trabant: A Serverless Architecture for Multi-Tenant Orbital Edge Computing</title>
      <link>https://arxiv.org/abs/2504.08337</link>
      <description>arXiv:2504.08337v1 Announce Type: new 
Abstract: Orbital edge computing reduces the data transmission needs of Earth observation satellites by processing sensor data on-board, allowing near-real-time insights while minimizing downlink costs. However, current orbital edge computing architectures are inflexible, requiring custom mission planning and high upfront development costs. In this paper, we propose a novel approach: shared Earth observation satellites that are operated by a central provider but used by multiple tenants. Each tenant can execute their own logic on-board the satellite to filter, prioritize, and analyze sensor data.
  We introduce Trabant, a serverless architecture for shared satellite platforms, leveraging the Function-as-a-Service (FaaS) paradigm and time-shifted computing. This architecture abstracts operational complexities, enabling dynamic scheduling under satellite resource constraints, reducing deployment overhead, and aligning event-driven satellite observations with intermittent computation. We present the design of Trabant, demonstrate its capabilities with a proof-of-concept prototype, and evaluate it using real satellite computing telemetry data. Our findings suggest that Trabant can significantly reduce mission planning overheads, offering a scalable and efficient platform for diverse Earth observation missions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08337v1</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tobias Pfandzelter, Nikita Bauer, Alexander Leis, Corentin Perdrizet, Felix Trautwein, Trever Schirmer, Osama Abboud, David Bermbach</dc:creator>
    </item>
    <item>
      <title>Path Connected Dynamic Graphs with a Study of Dispersion and Exploration</title>
      <link>https://arxiv.org/abs/2504.08474</link>
      <description>arXiv:2504.08474v1 Announce Type: new 
Abstract: In dynamic graphs, edges may be added or deleted in each synchronous round. Various connectivity models exist based on constraints on these changes. One well-known model is the $T$-Interval Connectivity model, where the graph remains connected in every round, and the parameter $T$ reflects the duration of structural stability. Another model is Connectivity Time, where the union of edges across any $T$ consecutive rounds forms a connected graph. This is a weaker model, as the graph may be disconnected in individual rounds.
  In this work, we introduce a new connectivity model called $T$-Path Connectivity. Unlike $T$-Interval Connectivity, the graph may not be connected in each round, but for every pair of nodes $u,v$, there must exist a path connecting them in at least one round within any $T$ consecutive rounds. This model is strictly weaker than $T$-Interval Connectivity but stronger than the Connectivity Time model.
  We study the dispersion problem in the $T$-Path Connectivity model. While dispersion has been explored in the 1-Interval Connectivity model, we show that the existing algorithm with termination does not work in our model. We then identify the minimal necessary assumptions required to solve dispersion in the $T$-Path Connectivity model and provide an algorithm that solves it optimally under those conditions. Moreover, we prove that dispersion is unsolvable in the Connectivity Time model, even under several strong assumptions.
  We further initiate the study of the exploration problem under all three connectivity models. We present multiple impossibility results and, in most cases, establish tight bounds on the number of agents and time required. Our results demonstrate that, in both dispersion and exploration, the Connectivity Time model is strictly the weakest among the three.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08474v1</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ashish Saxena, Kaushik Mondal</dc:creator>
    </item>
    <item>
      <title>Orchestrating Agents and Data for Enterprise: A Blueprint Architecture for Compound AI</title>
      <link>https://arxiv.org/abs/2504.08148</link>
      <description>arXiv:2504.08148v1 Announce Type: cross 
Abstract: Large language models (LLMs) have gained significant interest in industry due to their impressive capabilities across a wide range of tasks. However, the widespread adoption of LLMs presents several challenges, such as integration into existing applications and infrastructure, utilization of company proprietary data, models, and APIs, and meeting cost, quality, responsiveness, and other requirements. To address these challenges, there is a notable shift from monolithic models to compound AI systems, with the premise of more powerful, versatile, and reliable applications. However, progress thus far has been piecemeal, with proposals for agentic workflows, programming models, and extended LLM capabilities, without a clear vision of an overall architecture. In this paper, we propose a 'blueprint architecture' for compound AI systems for orchestrating agents and data for enterprise applications. In our proposed architecture the key orchestration concept is 'streams' to coordinate the flow of data and instructions among agents. Existing proprietary models and APIs in the enterprise are mapped to 'agents', defined in an 'agent registry' that serves agent metadata and learned representations for search and planning. Agents can utilize proprietary data through a 'data registry' that similarly registers enterprise data of various modalities. Tying it all together, data and task 'planners' break down, map, and optimize tasks and queries for given quality of service (QoS) requirements such as cost, accuracy, and latency. We illustrate an implementation of the architecture for a use-case in the HR domain and discuss opportunities and challenges for 'agentic AI' in the enterprise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08148v1</guid>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>First Workshop on Data-AI Systems (DAIS), ICDE 2025</arxiv:journal_reference>
      <dc:creator>Eser Kandogan, Nikita Bhutani, Dan Zhang, Rafael Li Chen, Sairam Gurajada, Estevam Hruschka</dc:creator>
    </item>
    <item>
      <title>Efficient Architecture for RISC-V Vector Memory Access</title>
      <link>https://arxiv.org/abs/2504.08334</link>
      <description>arXiv:2504.08334v1 Announce Type: cross 
Abstract: Vector processors frequently suffer from inefficient memory accesses, particularly for strided and segment patterns. While coalescing strided accesses is a natural solution, effectively gathering or scattering elements at fixed strides remains challenging. Naive approaches rely on high-overhead crossbars that remap any byte between memory and registers, leading to physical design issues. Segment operations require row-column transpositions, typically handled using either element-level in-place transposition (degrading performance) or large buffer-based bulk transposition (incurring high area overhead). In this paper, we present EARTH, a novel vector memory access architecture designed to overcome these challenges through shifting-based optimizations. For strided accesses, EARTH integrates specialized shift networks for gathering and scattering elements. After coalescing multiple accesses within the same cache line, data is routed between memory and registers through the shifting network with minimal overhead. For segment operations, EARTH employs a shifted register bank enabling direct column-wise access, eliminating dedicated segment buffers while providing high-performance, in-place bulk transposition. Implemented on FPGA with Chisel HDL based on an open-source RISC-V vector unit, EARTH enhances performance for strided memory accesses, achieving 4x-8x speedups in benchmarks dominated by strided operations. Compared to conventional designs, EARTH reduces hardware area by 9% and power consumption by 41%, significantly advancing both performance and efficiency of vector processors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08334v1</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hongyi Guan, Yichuan Gao, Chenlu Miao, Haoyang Wu, Hang Zhu, Mingfeng Lin, Huayue Liang</dc:creator>
    </item>
    <item>
      <title>String Problems in the Congested Clique Model</title>
      <link>https://arxiv.org/abs/2504.08376</link>
      <description>arXiv:2504.08376v1 Announce Type: cross 
Abstract: In this paper we present algorithms for several string problems in the Congested Clique model. In the Congested Clique model, $n$ nodes (computers) are used to solve some problem. The input to the problem is distributed among the nodes, and the communication between the nodes is conducted in rounds. In each round, every node is allowed to send an $O(\log n)$-bit message to every other node in the network.
  We consider three fundamental string problems in the Congested Clique model. First, we present an $O(1)$ rounds algorithm for string sorting that supports strings of arbitrary length. Second, we present an $O(1)$ rounds combinatorial pattern matching algorithm. Finally, we present an $O(\log\log n)$ rounds algorithm for the computation of the suffix array and the corresponding Longest Common Prefix array of a given string.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08376v1</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shay Golan, Matan Kraus</dc:creator>
    </item>
    <item>
      <title>An Evaluation and Comparison of GPU Hardware and Solver Libraries for Accelerating the OPM Flow Reservoir Simulator</title>
      <link>https://arxiv.org/abs/2309.11488</link>
      <description>arXiv:2309.11488v2 Announce Type: replace 
Abstract: Realistic reservoir simulation is known to be prohibitively expensive in terms of computation time when increasing the accuracy of the simulation or by enlarging the model grid size. One method to address this issue is to parallelize the computation by dividing the model in several partitions and using multiple CPUs to compute the result using techniques such as MPI and multi-threading. Alternatively, GPUs are also a good candidate to accelerate the computation due to their massively parallel architecture that allows many floating point operations per second to be performed. The numerical iterative solver takes thus the most computational time and is challenging to solve efficiently due to the dependencies that exist in the model between cells. In this work, we evaluate the OPM Flow simulator and compare several state-of-the-art GPU solver libraries as well as custom developed solutions for a BiCGStab solver using an ILU0 preconditioner and benchmark their performance against the default DUNE library implementation running on multiple CPU processors using MPI. The evaluated GPU software libraries include a manual linear solver in OpenCL and the integration of several third party sparse linear algebra libraries, such as cuSparse, rocSparse, and amgcl. To perform our bench-marking, we use small, medium, and large use cases, starting with the public test case NORNE that includes approximately 50k active cells and ending with a large model that includes approximately 1 million active cells. We find that a GPU can accelerate a single dual-threaded MPI process up to 5.6 times, and that it can compare with around 8 dual-threaded MPI processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.11488v2</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tong Dong Qiu, Andreas Thune, Vinicius Oliveira Martins, Markus Blatt, Alf Birger Rustad, Razvan Nane</dc:creator>
    </item>
    <item>
      <title>AIArena: A Blockchain-Based Decentralized AI Training Platform</title>
      <link>https://arxiv.org/abs/2412.14566</link>
      <description>arXiv:2412.14566v3 Announce Type: replace-cross 
Abstract: The rapid advancement of AI has underscored critical challenges in its development and implementation, largely due to centralized control by a few major corporations. This concentration of power intensifies biases within AI models, resulting from inadequate governance and oversight mechanisms. Additionally, it limits public involvement and heightens concerns about the integrity of model generation. Such monopolistic control over data and AI outputs threatens both innovation and fair data usage, as users inadvertently contribute data that primarily benefits these corporations. In this work, we propose AIArena, a blockchain-based decentralized AI training platform designed to democratize AI development and alignment through on-chain incentive mechanisms. AIArena fosters an open and collaborative environment where participants can contribute models and computing resources. Its on-chain consensus mechanism ensures fair rewards for participants based on their contributions. We instantiate and implement AIArena on the public Base blockchain Sepolia testnet, and the evaluation results demonstrate the feasibility of AIArena in real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14566v3</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhipeng Wang, Rui Sun, Elizabeth Lui, Tuo Zhou, Yizhe Wen, Jiahao Sun</dc:creator>
    </item>
  </channel>
</rss>
