<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 16 Jan 2026 05:00:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Estimation of Parameters of the Truncated Normal Distribution with Unknown Bounds</title>
      <link>https://arxiv.org/abs/2601.09857</link>
      <description>arXiv:2601.09857v1 Announce Type: new 
Abstract: Estimators of parameters of truncated distributions, namely the truncated normal distribution, have been widely studied for a known truncation region. There is also literature for estimating the unknown bounds for known parent distributions. In this work, we develop a novel algorithm under the expectation-solution (ES) framework, which is an iterative method of solving nonlinear estimating equations, to estimate both the bounds and the location and scale parameters of the parent normal distribution utilizing the theory of best linear unbiased estimates from location-scale families of distribution and unbiased minimum variance estimation of truncation regions. The conditions for the algorithm to converge to the solution of the estimating equations for a fixed sample size are discussed, and the asymptotic properties of the estimators are characterized using results on M- and Z-estimation from empirical process theory. The proposed method is then compared to methods utilizing the known truncation bounds via Monte Carlo simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09857v1</guid>
      <category>stat.CO</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dylan Borchert, Semhar Michael, Christopher Saunders</dc:creator>
    </item>
    <item>
      <title>Mesh Denoising</title>
      <link>https://arxiv.org/abs/2601.10487</link>
      <description>arXiv:2601.10487v1 Announce Type: new 
Abstract: In this paper, we study four mesh denoising methods: linear filtering, a heat diffusion method, Sobolev regularization, and, to a lesser extent, a barycentric approach based on the Sinkhorn algorithm. We illustrate that, for a simple image denoising task, a naive choice of a Gibbs kernel can lead to unsatisfactory results. We demonstrate that while Sobolev regularization is the fastest method in our implementation, it produces slightly less faithful denoised meshes than the best results obtained with iterative filtering or heat diffusion. We empirically show that, for the large mesh considered, the heat diffusion method is slower and not more effective than filtering, whereas on a small mesh an appropriate choice of diffusion parameters can improve the quality. Finally, we observe that all three mesh-based methods perform markedly better on the large mesh than on the small one.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.10487v1</guid>
      <category>stat.CO</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Constantin Vaillant Tenzer</dc:creator>
    </item>
    <item>
      <title>Accelerated Regularized Wasserstein Proximal Sampling Algorithms</title>
      <link>https://arxiv.org/abs/2601.09848</link>
      <description>arXiv:2601.09848v1 Announce Type: cross 
Abstract: We consider sampling from a Gibbs distribution by evolving a finite number of particles using a particular score estimator rather than Brownian motion. To accelerate the particles, we consider a second-order score-based ODE, similar to Nesterov acceleration. In contrast to traditional kernel density score estimation, we use the recently proposed regularized Wasserstein proximal method, yielding the Accelerated Regularized Wasserstein Proximal method (ARWP). We provide a detailed analysis of continuous- and discrete-time non-asymptotic and asymptotic mixing rates for Gaussian initial and target distributions, using techniques from Euclidean acceleration and accelerated information gradients. Compared with the kinetic Langevin sampling algorithm, the proposed algorithm exhibits a higher contraction rate in the asymptotic time regime. Numerical experiments are conducted across various low-dimensional experiments, including multi-modal Gaussian mixtures and ill-conditioned Rosenbrock distributions. ARWP exhibits structured and convergent particles, accelerated discrete-time mixing, and faster tail exploration than the non-accelerated regularized Wasserstein proximal method and kinetic Langevin methods. Additionally, ARWP particles exhibit better generalization properties for some non-log-concave Bayesian neural network tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09848v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.CO</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hong Ye Tan, Stanley Osher, Wuchen Li</dc:creator>
    </item>
    <item>
      <title>Model selection by cross-validation in an expectile linear regression</title>
      <link>https://arxiv.org/abs/2601.09874</link>
      <description>arXiv:2601.09874v1 Announce Type: cross 
Abstract: For linear models that may have asymmetric errors, we study variable selection by cross-validation. The data are split into training and validation sets, with the number of observations in the validation set much larger than in the training set. For the model coefficients, the expectile or adaptive LASSO expectile estimators are calculated on the training set. These estimators will be used to calculate the cross-validation mean score (CVS) on the validation set. We show that the model that minimizes CVS is consistent in two cases: when the number of explanatory variables is fixed or when it depends on the number of observations. Monte Carlo simulations confirm the theoretical results and demonstrate the superiority of our estimation method compared to two others in the literature. The usefulness of the CV expectile model selection technique is illustrated by applying it to real data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09874v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bilel Bousselmi, Gabriela Ciuperca</dc:creator>
    </item>
    <item>
      <title>LangLasso: Interactive Cluster Descriptions through LLM Explanation</title>
      <link>https://arxiv.org/abs/2601.10458</link>
      <description>arXiv:2601.10458v1 Announce Type: cross 
Abstract: Dimensionality reduction is a powerful technique for revealing structure and potential clusters in data. However, as the axes are complex, non-linear combinations of features, they often lack semantic interpretability. Existing visual analytics (VA) methods support cluster interpretation through feature comparison and interactive exploration, but they require technical expertise and intense human effort. We present \textit{LangLasso}, a novel method that complements VA approaches through interactive, natural language descriptions of clusters using large language models (LLMs). It produces human-readable descriptions that make cluster interpretation accessible to non-experts and allow integration of external contextual knowledge beyond the dataset. We systematically evaluate the reliability of these explanations and demonstrate that \langlasso provides an effective first step for engaging broader audiences in cluster interpretation. The tool is available at https://langlasso.vercel.app</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.10458v1</guid>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Raphael Buchm\"uller, Dennis Collaris, Linhao Meng, Angelos Chatzimparmpas</dc:creator>
    </item>
    <item>
      <title>Composite likelihood inference for the Poisson log-normal model</title>
      <link>https://arxiv.org/abs/2402.14390</link>
      <description>arXiv:2402.14390v3 Announce Type: replace 
Abstract: The Poisson log-normal model is a latent variable model that provides a generic framework for the analysis of multivariate count data. Inferring its parameters can be a daunting task since the conditional distribution of the latent variables given the observed ones is intractable. For this model, variational approaches are the golden standard solution as they prove to be computationally efficient but lack theoretical guarantees on the estimates. Sampling-based solutions are quite the opposite. We first define a Monte Carlo EM algorithm that can achieve maximum likelihood estimators, but that is computationally efficient only for low-dimensional latent spaces. We then propose a novel inference procedure combining the EM framework with composite likelihood and importance sampling estimates. The algorithm preserves the desirable asymptotic properties of maximum likelihood estimators while circumventing the high-dimensional integration bottleneck, thus maintaining computational feasibility for moderately large datasets. This approach enables grounded parameter estimation, confidence intervals, and hypothesis testing. Application to the Barents Sea fish dataset demonstrates the algorithm capacity to identify significant environmental effects and residual interspecies correlations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.14390v3</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julien Stoehr (CEREMADE), Stephane S. Robin (LPSM)</dc:creator>
    </item>
    <item>
      <title>Modified Delayed Acceptance MCMC for Quasi-Bayesian Inference with Linear Moment Conditions</title>
      <link>https://arxiv.org/abs/2511.17117</link>
      <description>arXiv:2511.17117v4 Announce Type: replace 
Abstract: We develop a computationally efficient framework for quasi-Bayesian inference based on linear moment conditions. The approach employs a delayed acceptance Markov chain Monte Carlo (DA-MCMC) algorithm that uses a surrogate target kernel and a proposal distribution derived from an approximate conditional posterior, thereby exploiting the structure of the quasi-likelihood. Two implementations are introduced. DA-MCMC-Exact fully incorporates prior information into the proposal distribution and maximizes per-iteration efficiency, whereas DA-MCMC-Approx omits the prior in the proposal to reduce matrix inversions, improving numerical stability and computational speed in higher dimensions. Simulation studies on heteroskedastic linear regressions show substantial gains over standard MCMC and conventional DA-MCMC baselines, measured by multivariate effective sample size per iteration and per second. The Approx variant yields the best overall throughput, while the Exact variant attains the highest per-iteration efficiency. Applications to two empirical instrumental variable regressions corroborate these findings: the Approx implementation scales to larger designs where other methods become impractical, while still delivering precise inference. Although developed for moment-based quasi-posteriors, the proposed approach also extends to risk-based quasi-Bayesian formulations when first-order conditions are linear and can be transformed analogously. Overall, the proposed algorithms provide a practical and robust tool for quasi-Bayesian analysis in statistical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17117v4</guid>
      <category>stat.CO</category>
      <category>econ.EM</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masahiro Tanaka</dc:creator>
    </item>
    <item>
      <title>Population-Adjusted Indirect Treatment Comparison with the outstandR Package in R</title>
      <link>https://arxiv.org/abs/2601.07532</link>
      <description>arXiv:2601.07532v2 Announce Type: replace 
Abstract: Indirect treatment comparisons (ITCs) are essential in Health Technology Assessment (HTA) when head-to-head clinical trials are absent. A common challenge arises when attempting to compare a treatment with available individual patient data (IPD) against a competitor with only reported aggregate-level data (ALD), particularly when trial populations differ in effect modifiers. While methods such as Matching-Adjusted Indirect Comparison (MAIC) and Simulated Treatment Comparison (STC) exist to adjust for these cross-trial differences, software implementations have often been fragmented or limited in scope. This article introduces outstandR, an R package designed to provide a comprehensive and unified framework for population-adjusted indirect comparison (PAIC). Beyond standard weighting and regression approaches, outstandR implements advanced G-computation methods within both maximum likelihood and Bayesian frameworks, and Multiple Imputation Marginalization (MIM) to address non-collapsibility and missing data. By streamlining the workflow of covariate simulation, model standardization, and contrast estimation, outstandR enables robust and compatible evidence synthesis in complex decision-making scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07532v2</guid>
      <category>stat.CO</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan Green</dc:creator>
    </item>
  </channel>
</rss>
