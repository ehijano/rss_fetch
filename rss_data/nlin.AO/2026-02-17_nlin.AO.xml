<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>nlin.AO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/nlin.AO</link>
    <description>nlin.AO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/nlin.AO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 17 Feb 2026 05:01:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Supercritical Mass and Condensation in Fokker--Planck Equations for Consensus Formation</title>
      <link>https://arxiv.org/abs/2602.13276</link>
      <description>arXiv:2602.13276v1 Announce Type: cross 
Abstract: Inspired by recently developed Fokker--Planck models for Bose--Einstein statistics, we study a consensus formation model with condensation effects driven by a polynomial diffusion coefficient vanishing at the domain boundaries. For the underlying kinetic model, given by a nonlinear Fokker--Planck equation with superlinear drift, it was shown that if the initial mass exceeds a critical threshold, the solution may exhibit finite-time concentration in certain parameter regimes. Here, we show that this supercritical mass phenomenon persists for a broader class of diffusion functions and provide estimates of the critical mass required to induce finite-time loss of regularity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13276v1</guid>
      <category>math.AP</category>
      <category>cs.MA</category>
      <category>nlin.AO</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Monica Caloi, Mattia Zanella</dc:creator>
    </item>
    <item>
      <title>Memorisation and forgetting in a learning Hopfield neural network: bifurcation mechanisms, attractors and basins</title>
      <link>https://arxiv.org/abs/2508.10765</link>
      <description>arXiv:2508.10765v2 Announce Type: replace-cross 
Abstract: Despite explosive expansion of artificial intelligence based on artificial neural networks (ANNs), these are employed as "black boxes'', as it is unclear how, during learning, they form memories or develop unwanted features, including spurious memories and catastrophic forgetting. Much research is available on isolated aspects of learning ANNs, but due to their high dimensionality and non-linearity, their comprehensive analysis remains a challenge. In ANNs, knowledge is thought to reside in connection weights or in attractor basins, but these two paradigms are not linked explicitly. Here we comprehensively analyse mechanisms of memory formation in an 81-neuron Hopfield network undergoing Hebbian learning by revealing bifurcations leading to formation and destruction of attractors and their basin boundaries. We show that, by affecting evolution of connection weights, the applied stimuli induce a pitchfork and then a cascade of saddle-node bifurcations creating new attractors with their basins that can code true or spurious memories, and an abrupt disappearance of old memories (catastrophic forgetting). With successful learning, new categories are represented by the basins of newly born point attractors, and their boundaries by the stable manifolds of new saddles. With this, memorisation and forgetting represent two manifestations of the same mechanism. Our strategy to analyse high-dimensional learning ANNs is universal and applicable to recurrent ANNs of any form. The demonstrated mechanisms of memory formation and of catastrophic forgetting shed light on the operation of a wider class of recurrent ANNs and could aid the development of approaches to mitigate their flaws.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10765v2</guid>
      <category>math.DS</category>
      <category>cs.LG</category>
      <category>nlin.AO</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adam E. Essex (Loughborough University, England), Natalia B. Janson (Loughborough University, England), Rachel A. Norris (Loughborough University, England), Alexander G. Balanov (Loughborough University, England)</dc:creator>
    </item>
  </channel>
</rss>
