<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 20 Nov 2025 05:01:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Uncovering Treatment Effect Heterogeneity in Pragmatic Gerontology Trials</title>
      <link>https://arxiv.org/abs/2511.14893</link>
      <description>arXiv:2511.14893v1 Announce Type: new 
Abstract: Detecting heterogeneity in treatment response enriches the interpretation of gerontologic trials. In aging research, estimating the effect of the intervention on clinically meaningful outcomes faces analytical challenges when it is truncated by death. For example, in the Whole Systems Demonstrator trial, a large cluster-randomized study evaluating telecare among older adults, the overall effect of the intervention on quality of life was found to be null. However, this marginal intervention estimate obscures potential heterogeneity of individuals responding to the intervention, particularly among those who survive to the end of follow-up. To explore this heterogeneity, we adopt a causal framework grounded in principal stratification, targeting the Survivor Average Causal Effect (SACE)-the treatment effect among "always-survivors," or those who would survive regardless of treatment assignment. We extend this framework using Bayesian Additive Regression Trees (BART), a nonparametric machine learning method, to flexibly model both latent principal strata and stratum-specific potential outcomes. This enables the estimation of the Conditional SACE (CSACE), allowing us to uncover variation in treatment effects across subgroups defined by baseline characteristics. Our analysis reveals that despite the null average effect, some subgroups experience distinct quality of life benefits (or lack thereof) from telecare, highlighting opportunities for more personalized intervention strategies. This study demonstrates how embedding machine learning methods, such as BART, within a principled causal inference framework can offer deeper insights into trial data with complex features including truncation by death and clustering-key considerations in analyzing pragmatic gerontology trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14893v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changjun Li, Heather Allore, Michael O. Harhay, Fan Li, Guangyu Tong</dc:creator>
    </item>
    <item>
      <title>Fifty Shades of Greenwashing: The Political Economy of Climate Change Advertising on Social Media</title>
      <link>https://arxiv.org/abs/2511.14930</link>
      <description>arXiv:2511.14930v1 Announce Type: new 
Abstract: In this paper, we provide a novel measure for greenwashing -- i.e., climate-related misinformation -- that shows how polluting companies can use social media advertising related to climate change to redirect criticism. To do so, we identify greenwashing content in 11 million social-political ads in Meta's Ad Targeting Datset with a measurement technique that combines large language models, human coders, and advances in Bayesian item response theory. We show that what is called greenwashing has diverse actors and components, but we also identify a very pernicious form, which we call political greenwashing, that appears to be promoted by fossil fuel companies and related interest groups. Based on ad targeting data, we show that much of this advertising happens via organizations with undisclosed links to the fossil fuel industry. Furthermore, we show that greenwashing ad content is being micro-targeted at left-leaning communities with fossil fuel assets, though we also find comparatively little evidence of ad targeting aimed at influencing public opinion at the national level.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14930v1</guid>
      <category>stat.AP</category>
      <category>cs.AI</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robert Kubinec, Aseem Mahajan</dc:creator>
    </item>
    <item>
      <title>Principled Frequentist Estimation of Racial Disparity in Credit Approval under Unobserved Race</title>
      <link>https://arxiv.org/abs/2511.14951</link>
      <description>arXiv:2511.14951v1 Announce Type: new 
Abstract: Estimating racial disparities in loan-approval probabilities when race is unobserved is routinely required for fair lending compliance. In such cases, race probabilities-typically from Bayesian Improved Surname Geocoding (BISG)-stand in for true race. Prior work shows that common heuristic approaches, including the Threshold and Weighting estimators, are inconsistent under valid identification assumptions, compromising internal validity. A recent Bayesian approach demonstrates consistency under assumptions reasonable in many fair lending contexts. This approach hinges on the insight that identification requires the race predictors to be exogenous with respect to loan approval, essentially an instrumental-variables design. We present a frequentist counterpart to this solution via Ordinary Least Squares (OLS) and Maximum Likelihood Estimation (MLE) under a similar exogeneity assumption. To satisfy these assumptions in practice, we introduce (i) a surname-only proxy analogous to BISG and (ii) an income-stratified prior for race probabilities. Monte Carlo simulations and an application to 2023 Los Angeles HMDA data confirm superior performance: this method reduces RMSE in the LA Black/White adverse-impact ratio by 79.7% (from 10.639pp to 2.158pp) compared to a Weighting estimator with the standard prior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14951v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sam Fisher, Dmitry Lesnik, Tobias Sch\"afer</dc:creator>
    </item>
    <item>
      <title>Resource-Based Time and Cost Prediction in Project Networks: From Statistical Modeling to Graph Neural Networks</title>
      <link>https://arxiv.org/abs/2511.15003</link>
      <description>arXiv:2511.15003v1 Announce Type: new 
Abstract: Accurate prediction of project duration and cost remains one of the most challenging aspects of project management, particularly in resource-constrained and interdependent task networks. Traditional analytical techniques such as the Critical Path Method (CPM) and Program Evaluation and Review Technique (PERT) rely on simplified and often static assumptions regarding task interdependencies and resource performance. This study proposes a novel resource-based predictive framework that integrates network representations of project activities with graph neural networks (GNNs) to capture structural and contextual relationships among tasks, resources, and time-cost dynamics. The model represents the project as a heterogeneous activity-resource graph in which nodes denote activities and resources, and edges encode temporal and resource dependencies.
  We evaluate multiple learning paradigms, including GraphSAGE and Temporal Graph Networks, on both synthetic and benchmark project datasets. Experimental results show that the proposed GNN framework achieves an average 23 to 31 percent reduction in mean absolute error compared to traditional regression and tree-based methods, while improving the coefficient of determination R2 from approximately 0.78 to 0.91 for large and complex project networks. Furthermore, the learned embeddings provide interpretable insights into resource bottlenecks and critical dependencies, enabling more explainable and adaptive scheduling decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15003v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Reza Mirjalili, Behrad Braghi, Shahram Shadrokh Sikari</dc:creator>
    </item>
    <item>
      <title>Subnational Geocoding of Global Disasters Using Large Language Models</title>
      <link>https://arxiv.org/abs/2511.14788</link>
      <description>arXiv:2511.14788v1 Announce Type: cross 
Abstract: Subnational location data of disaster events are critical for risk assessment and disaster risk reduction. Disaster databases such as EM-DAT often report locations in unstructured textual form, with inconsistent granularity or spelling, that make it difficult to integrate with spatial datasets. We present a fully automated LLM-assisted workflow that processes and cleans textual location information using GPT-4o, and assigns geometries by cross-checking three independent geoinformation repositories: GADM, OpenStreetMap and Wikidata. Based on the agreement and availability of these sources, we assign a reliability score to each location while generating subnational geometries. Applied to the EM-DAT dataset from 2000 to 2024, the workflow geocodes 14,215 events across 17,948 unique locations. Unlike previous methods, our approach requires no manual intervention, covers all disaster types, enables cross-verification across multiple sources, and allows flexible remapping to preferred frameworks. Beyond the dataset, we demonstrate the potential of LLMs to extract and structure geographic information from unstructured text, offering a scalable and reliable method for related analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14788v1</guid>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michele Ronco, Damien Delforge, Wiebke S. J\"ager, Christina Corbane</dc:creator>
    </item>
    <item>
      <title>Extrinsic Total-Variance and Coplanarity via Oriented and Classical Projective Shape Analysis</title>
      <link>https://arxiv.org/abs/2511.14815</link>
      <description>arXiv:2511.14815v1 Announce Type: cross 
Abstract: Projective shape analysis provides a geometric framework for studying digital images acquired by pinhole digital cameras. In the classical projective shape (PS) method, landmark configurations are represented in $(\RP^2)^{k-4}$, where $k$ is the number of landmarks observed. This representation is invariant under the action of the full projective group on this space and is sign-blind, so opposite directions in $\R^{3}$ determine the same projective point and front--back orientation of a surface is not recorded. Oriented projective shape ($\OPS$) restores this information by working on a product of $k-4$ spheres $\SP^2$ instead of projective space and restricting attention to the orientation-preserving subgroup of projective transformations. In this paper we introduce an extrinsic total-variance index for OPS, resulting in the extrinsic Fr\'echet framework for the m dimensional case from the inclusion $\jdir:(\SP^m)^q\hookrightarrow(\R^{m+1})^q,q=k-m-2$. In the planar pentad case ($m=2$, $q=1$) the sample total extrinsic variance has a closed form in terms of the mean of a random sample of size $n$ of oriented projective coordinates in $S^2$. As an illustration, using an oriented projective frame, we analyze the Sope Creek stone data set, a benchmark and nearly planar example with $41$ images and $5$ landmarks. Using a delta-method applied to a large sample and a generalized Slutsky theorem argument, for an OPS leave-two-out diagnostic, one identifies coplanarity at the $5\%$ level, confirming the concentrated data coplanarity PS result in Patrangenaru(2001)\cite{Patrangenaru2001}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14815v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Musab Alamoudi, Robert L. Paige, Vic Patrangenaru</dc:creator>
    </item>
    <item>
      <title>Individualized Prediction Bands in Causal Inference with Continuous Treatments</title>
      <link>https://arxiv.org/abs/2511.15075</link>
      <description>arXiv:2511.15075v1 Announce Type: cross 
Abstract: Individualized treatments are crucial for optimal decision making and treatment allocation, specifically in personalized medicine based on the estimation of an individual's dose-response curve across a continuum of treatment levels, e.g., drug dosage. Current works focus on conditional mean and median estimates, which are useful but do not provide the full picture. We propose viewing causal inference with a continuous treatment as a covariate shift. This allows us to leverage existing weighted conformal prediction methods with both quantile and point estimates to compute individualized uncertainty quantification for dose-response curves. Our method, individualized prediction bands (IPB), is demonstrated via simulations and a real data analysis, which demonstrates the additional medical expenditure caused by continued smoking for selected individuals. The results demonstrate that IPB provides an effective solution to a gap in individual dose-response uncertainty quantification literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15075v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Max Sampson, Kung-Sik Chan</dc:creator>
    </item>
    <item>
      <title>The SolarEV City Paradox: A Critical Review of the Fragmented Integration of Rooftop Photovoltaics and Electric Vehicles for Urban Decarbonization</title>
      <link>https://arxiv.org/abs/2511.15091</link>
      <description>arXiv:2511.15091v1 Announce Type: cross 
Abstract: Urban decarbonization is central to meeting global climate goals, yet progress toward integrated low-carbon energy systems remains slow. The SolarEV City Concept, linking rooftop photovoltaics with electric vehicles as mobile storage offers a technically robust pathway for deep CO2 reduction, potentially meeting 60-95 percent of municipal electricity demand when deployed synergistically. Despite rapid global growth of PVs and EVs, integration through bidirectional Vehicle-to-Home and Vehicle-to-Grid systems has lagged, revealing a persistent SolarEV paradox. This review examines that paradox through a socio-technical framework across four dimensions, technology, economics, policy, and society. Cross-national comparison shows that while technical feasibility is well established, large-scale implementation is limited by fragmented charging-protocol standards, immature and often non-profitable V2G business models, regulatory misalignments between energy and transport sectors, and social-equity barriers that restrict participation mainly to high-income homeowners. Emerging national archetypes from Japans resilience-driven model to Europes regulation-first trajectory highlight strong path dependence in current integration strategies. The analysis concludes that advancing SolarEV Cities requires a shift from parallel PV-EV promotion toward coordinated policy frameworks, interoperable digital infrastructure, and inclusive market designs that distribute economic and resilience benefits more equitably. Achieving this integrated energy transition will require strategic collaboration among researchers, governments, industries, and communities to build adaptive, resilient, and socially just urban energy systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15091v1</guid>
      <category>physics.soc-ph</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>T. Kobashi, R. C. Mouli, J. Liu, S. Chang, C. D. Harper, R. Zhou, G. R. Dewi, U. W. R. Siagian, J. Kang, P. P. Patankar, Z. H. Rather, K. Say, T. Zhang, K. Tanaka, P. Ciais, D. M. Kammen</dc:creator>
    </item>
    <item>
      <title>Debiasing hazard-based, time-varying vaccine effects using vaccine-irrelevant infections: An observational extension of a pivotal Phase 3 COVID-19 vaccine efficacy trial</title>
      <link>https://arxiv.org/abs/2511.15099</link>
      <description>arXiv:2511.15099v1 Announce Type: cross 
Abstract: Understanding how vaccine effectiveness (VE) changes over time can provide evidence-based guidance for public health decision making. While commonly reported by practitioners, time-varying VE estimates obtained using Cox regression are vul- nerable to hidden biases. To address these limitations, we describe how to leverage vaccine-irrelevant infections to identify hazard-based, time-varying VE in the pres- ence of unmeasured confounding and selection bias. We articulate assumptions under which our approach identifies a causal effect of an intervention deferring vaccination and interaction with the community in which infections circulate. We develop sieve and efficient influence curve-based estimators and discuss imposing monotone shape constraints and estimating VE against multiple variants. As a case study, we examine the observational booster phase of the Coronavirus Vaccine Efficacy (COVE) trial of the Moderna mRNA-1273 COVID-19 vaccine which used symptom-triggered multi- plex PCR testing to identify acute respiratory illnesses (ARIs) caused by SARS-CoV-2 and 20 off-target pathogens previously identified as compelling negative controls for COVID-19. Accounting for vaccine-irrelevant ARIs supported that the mRNA-1273 booster was more effective and durable against Omicron COVID-19 than suggested by Cox regression. Our work offers an approach to mitigate bias in hazard-based, time- varying treatment effects in randomized and non-randomized studies using negative controls.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15099v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ethan Ashby, Dean Follmann, Holly Janes, Peter B. Gilbert, Ting Ye, Lindsey R. Baden, Hana M. El Sahly, Bo Zhang</dc:creator>
    </item>
    <item>
      <title>Gini Score under Ties and Case Weights</title>
      <link>https://arxiv.org/abs/2511.15446</link>
      <description>arXiv:2511.15446v1 Announce Type: cross 
Abstract: The Gini score is a popular tool in statistical modeling and machine learning for model validation and model selection. It is a purely rank based score that allows one to assess risk rankings. The Gini score for statistical modeling has mainly been used in a binary context, in which it has many equivalent reformulations such as the receiver operating characteristic (ROC) or the area under the curve (AUC). In the actuarial literature, this rank based score for binary responses has been extended to general real-valued random variables using Lorenz curves and concentration curves. While these initial concepts assume that the risk ranking is generated by a continuous distribution function, we discuss in this paper how the Gini score can be used in the case of ties in the risk ranking. Moreover, we adapt the Gini score to the common actuarial situation of having case weights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15446v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexej Brauer, Mario V. W\"uthrich</dc:creator>
    </item>
    <item>
      <title>Toward precision soil health: A regional framework for site-specific management across Missouri</title>
      <link>https://arxiv.org/abs/2510.26815</link>
      <description>arXiv:2510.26815v2 Announce Type: replace 
Abstract: Effective soil health management is crucial for sustaining agriculture, adopting ecosystem resilience, and preserving water quality. However, Missouri's diverse landscapes limit the effectiveness of broad generalized management recommendations. The lack of resolution in existing soil grouping systems necessitates data driven, site specific insights to guide tailored interventions. To address these critical challenges, a regional soil clustering framework designed to support precision soil health management strategies across the state. The methodology leveraged high resolution SSURGO dataset, explicitly processing soil properties aggregated across the 0 to 30 cm root zone. Multivariate analysis incorporating a variational autoencoder and KMeans clustering was used to group soils with similar properties. The derived clusters were validated using statistical metrics, including silhouette scores and checks against existing taxonomic units, to confirm their spatial coherence. This approach enabled us to delineate soil groups that capture textures, hydraulic properties, chemical fertility, and biological indicators unique to Missouri's diverse agroecological regions. The clustering map identified ten distinct soil health management zones. This alignment of 10 clusters was selected as optimal because it was sufficiently large to capture inherited soil patterns while remaining manageable for practical statewide application. Rooting depth limitation and saturated hydraulic conductivity emerged as principal variables driving soil differentiation. Each management zone is defined by a unique combination of clay, organic matter, pH, and available water capacity. This framework bridges sophisticated data analysis with actionable, site targeted recommendations, enabling conservation planners, and agronomists to optimize management practices and enhance resource efficiency statewide.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26815v2</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dipal Shah (School of Natural Resources, University of Missouri, Columbia, MO, USA), Jordon Wade (Crop Protection Research &amp; Development, Syngenta Group, Basel, Switzerland), Timothy Haithcoat (Institute for Data Science and Informatics, University of Missouri, Columbia, MO, USA), Robert Myers (School of Plant Science &amp; Technology, University of Missouri, Columbia, MO, USA), Kelly Wilson (School of Natural Resources, University of Missouri, Columbia, MO, USA)</dc:creator>
    </item>
    <item>
      <title>Decoupling Urban Food Accessibility Resilience during Disasters through Time-Series Analysis of Human Mobility and Power Outages</title>
      <link>https://arxiv.org/abs/2511.14706</link>
      <description>arXiv:2511.14706v2 Announce Type: replace 
Abstract: Disaster-induced power outages create cascading disruptions across urban lifelines, yet the timed coupling between grid failure and essential service access remains poorly quantified. Focusing on Hurricane Beryl in Houston (2024), this study integrates approximately 173000 15-minute outage records with over 1.25 million visits to 3187 food facilities to quantify how infrastructure performance and human access co-evolve. We construct daily indices for outage characteristics (intensity, duration) and food access metrics (redundancy, frequency, proximity), estimate cross-system lags through lagged correlations over zero to seven days, and identify recovery patterns using DTW k-means clustering. Overlaying these clusters yields compound power-access typologies and enables facility-level criticality screening. The analysis reveals a consistent two-day lag: food access reaches its nadir on July 8 at landfall while outage severity peaks around July 10, with negative correlations strongest at a two-day lag and losing significance by day four. We identify four compound typologies from high/low outage crossed with high/low access disruption levels. Road network sparsity, more than income, determines the depth and persistence of access loss. Through this analysis, we enumerate 294 critical food facilities in the study area requiring targeted continuity measures including backup power, microgrids, and feeder prioritization. The novelty lies in measuring interdependency at daily operational resolution while bridging scales from communities to individual facilities, converting dynamic coupling patterns into actionable interventions for phase-sensitive restoration and equity-aware preparedness. The framework is transferable to other lifelines and hazards, offering a generalizable template for diagnosing and mitigating cascading effects on community access during disaster recovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14706v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Junwei Ma, Bo Li, Xiangpeng Li, Ali Mostafavi</dc:creator>
    </item>
    <item>
      <title>Ridge Regression on Riemannian Manifolds for Time-Series Prediction</title>
      <link>https://arxiv.org/abs/2411.18339</link>
      <description>arXiv:2411.18339v3 Announce Type: replace-cross 
Abstract: We propose a natural intrinsic extension of ridge regression from Euclidean spaces to general Riemannian manifolds for time-series prediction. Our approach combines Riemannian least-squares fitting via B\'ezier curves, empirical covariance on manifolds, and Mahalanobis distance regularization. A key technical contribution is an explicit formula for the gradient of the objective function using adjoint differentials, enabling efficient numerical optimization via Riemannian gradient descent. We validate our framework through synthetic spherical experiments (achieving significant error reduction over unregularized regression) and hurricane forecasting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18339v3</guid>
      <category>math.DG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Esfandiar Nava-Yazdani</dc:creator>
    </item>
    <item>
      <title>Lightweight posterior construction for gravitational-wave catalogs with the Kolmogorov-Arnold network</title>
      <link>https://arxiv.org/abs/2508.18698</link>
      <description>arXiv:2508.18698v2 Announce Type: replace-cross 
Abstract: Neural density estimation has seen widespread applications in the gravitational-wave (GW) data analysis, which enables real-time parameter estimation for compact binary coalescences and enhances rapid inference for subsequent analysis such as population inference. In this work, we explore the application of using the Kolmogorov-Arnold network (KAN) to construct efficient and interpretable neural density estimators for lightweight posterior construction of GW catalogs. By replacing conventional activation functions with learnable splines, KAN achieves superior interpretability, higher accuracy, and greater parameter efficiency on related scientific tasks. Leveraging this feature, we propose a KAN-based neural density estimator, which ingests megabyte-scale GW posterior samples and compresses them into model weights of tens of kilobytes. Subsequently, analytic expressions requiring only several kilobytes can be further distilled from these neural network weights with minimal accuracy trade-off. In practice, GW posterior samples with fidelity can be regenerated rapidly using the model weights or analytic expressions for subsequent analysis. Our lightweight posterior construction strategy is expected to facilitate user-level data storage and transmission, paving a path for efficient analysis of numerous GW events in the next-generation GW detectors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18698v2</guid>
      <category>gr-qc</category>
      <category>astro-ph.HE</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenshuai Liu, Yiming Dong, Ziming Wang, Lijing Shao</dc:creator>
    </item>
    <item>
      <title>Making Evidence Actionable in Adaptive Learning Closing the Diagnostic Pedagogical Loop</title>
      <link>https://arxiv.org/abs/2511.13542</link>
      <description>arXiv:2511.13542v2 Announce Type: replace-cross 
Abstract: Adaptive learning often diagnoses precisely yet intervenes weakly, producing help that is mistimed or misaligned. This study presents evidence supporting an instructor-governed feedback loop that converts concept-level assessment evidence into vetted microinterventions. The adaptive learning algorithm includes three safeguards: adequacy as a hard guarantee of gap closure, attention as a budgeted limit for time and redundancy, and diversity as protection against overfitting to a single resource. We formulate intervention assignment as a binary integer program with constraints for coverage, time, difficulty windows derived from ability estimates, prerequisites encoded by a concept matrix, and anti-redundancy with diversity. Greedy selection serves low-richness and tight-latency settings, gradient-based relaxation serves rich repositories, and a hybrid switches along a richness-latency frontier. In simulation and in an introductory physics deployment with 1204 students, both solvers achieved full skill coverage for nearly all learners within bounded watch time. The gradient-based method reduced redundant coverage by about 12 percentage points relative to greedy and produced more consistent difficulty alignment, while greedy delivered comparable adequacy at lower computational cost in resource-scarce environments. Slack variables localized missing content and guided targeted curation, sustaining sufficiency across student subgroups. The result is a tractable and auditable controller that closes the diagnostic pedagogical loop and enables equitable, load-aware personalization at the classroom scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13542v2</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amirreza Mehrabi, Jason Wade Morphew, Breejha Quezada, N. Sanjay Rebello</dc:creator>
    </item>
  </channel>
</rss>
