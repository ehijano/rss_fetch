<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 04 Aug 2025 04:01:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Gradient-Boosted Pseudo-Weighting: Methods for Population Inference from Nonprobability samples</title>
      <link>https://arxiv.org/abs/2508.00089</link>
      <description>arXiv:2508.00089v1 Announce Type: new 
Abstract: Nonprobability samples have rapidly emerged to address time-sensitive priority topics in a variety of fields. While these data are timely, they are prone to selection bias. To mitigate selection bias, a large number of survey research literature has explored the use of propensity score (PS) adjustment methods to enhance population representativeness of nonprobability samples, using probability-based survey samples as external references. A recent advancement, the 2-step PS-based pseudo-weighting adjustment method (2PS, Li 2024), has been shown to improve upon recent developments with respect to mean squared error. However, the effectiveness of these methods in reducing bias critically depends on the ability of the underlying propensity model to accurately reflect the true selection process, which is challenging with parametric regression. In this study, we propose a set of pseudo-weight construction methods, which utilize gradient boosting methods (GBM) to estimate PSs in 2PS to construct pseudo-weights, offering greater flexibility compared to logistic regression-based methods. We compare the proposed GBM-based pseudo-weights with existing methods, including 2PS. The population mean estimators are evaluated via Monte Carlo simulation studies. We also evaluated prevalence of various health outcomes, including 15-year mortality, using 1988 ~ 1994 NHANES III as a nonprobability sample and the 1994 NHIS as the reference survey.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00089v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kangrui Liu, Lingxiao Wang, Yan Li</dc:creator>
    </item>
    <item>
      <title>AdapDISCOM: An Adaptive Sparse Regression Method for High-Dimensional Multimodal Data With Block-Wise Missingness and Measurement Errors</title>
      <link>https://arxiv.org/abs/2508.00120</link>
      <description>arXiv:2508.00120v1 Announce Type: new 
Abstract: Multimodal high-dimensional data are increasingly prevalent in biomedical research, yet they are often compromised by block-wise missingness and measurement errors, posing significant challenges for statistical inference and prediction. We propose AdapDISCOM, a novel adaptive direct sparse regression method that simultaneously addresses these two pervasive issues. Building on the DISCOM framework, AdapDISCOM introduces modality-specific weighting schemes to account for heterogeneity in data structures and error magnitudes across modalities. We establish the theoretical properties of AdapDISCOM, including model selection consistency and convergence rates under sub-Gaussian and heavy-tailed settings, and develop robust and computationally efficient variants (AdapDISCOM-Huber and Fast-AdapDISCOM). Extensive simulations demonstrate that AdapDISCOM consistently outperforms existing methods such as DISCOM, SCOM, and CoCoLasso, particularly under heterogeneous contamination and heavy-tailed distributions. Finally, we apply AdapDISCOM to Alzheimers Disease Neuroimaging Initiative (ADNI) data, demonstrating improved prediction of cognitive scores and reliable selection of established biomarkers, even with substantial missingness and measurement errors. AdapDISCOM provides a flexible, robust, and scalable framework for high-dimensional multimodal data analysis under realistic data imperfections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00120v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Abdoul O. Diakit\'e, Claudia Moreau, Gleb Bezgin, Nikhil Bhagwat, Pedro Rosa-Neto, Jean-Baptiste Poline, Simon Girard, Amadou Barry, for the Alzheimers Disease Neuroimaging Initiative</dc:creator>
    </item>
    <item>
      <title>Likelihood-free Posterior Density Learning for Uncertainty Quantification in Inference Problems</title>
      <link>https://arxiv.org/abs/2508.00167</link>
      <description>arXiv:2508.00167v1 Announce Type: new 
Abstract: Generative models and those with computationally intractable likelihoods are widely used to describe complex systems in the natural sciences, social sciences, and engineering. Fitting these models to data requires likelihood-free inference methods that explore the parameter space without explicit likelihood evaluations, relying instead on sequential simulation, which comes at the cost of computational efficiency and extensive tuning. We develop an alternative framework called kernel-adaptive synthetic posterior estimation (KASPE) that uses deep learning to directly reconstruct the mapping between the observed data and a finite-dimensional parametric representation of the posterior distribution, trained on a large number of simulated datasets. We provide theoretical justification for KASPE and a formal connection to the likelihood-based approach of expectation propagation. Simulation experiments demonstrate KASPE's flexibility and performance relative to existing likelihood-free methods including approximate Bayesian computation in challenging inferential settings involving posteriors with heavy tails, multiple local modes, and over the parameters of a nonlinear dynamical system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00167v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rui Zhang, Oksana A. Chkrebtii, Dongbin Xiu</dc:creator>
    </item>
    <item>
      <title>New Pilot-Study Design in Functional Data Analysis</title>
      <link>https://arxiv.org/abs/2508.00176</link>
      <description>arXiv:2508.00176v1 Announce Type: new 
Abstract: Efficient data collection is essential in applied studies where frequent measurements are costly, time-consuming, or burdensome. This challenge is especially pronounced in functional data settings, where each subject is observed at only a few time points due to practical constraints. Most existing design approaches focus on selecting optimal time points for individual subjects, typically relying on model parameters estimated from a pilot study. However, the design of the pilot study itself has received limited attention. We propose a framework for constructing pilot-study designs that support both accurate trajectory recovery and effective planning of future designs. A search algorithm is developed to generate such high-quality pilot-study designs. Simulation studies and a real data application demonstrate that our approach outperforms commonly used alternatives, highlighting its value in resource-limited settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00176v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ping-Han Huang, Ming-Hung Kao</dc:creator>
    </item>
    <item>
      <title>The hierarchical barycenter: conditional probability simulation with structured and unobserved covariates</title>
      <link>https://arxiv.org/abs/2508.00206</link>
      <description>arXiv:2508.00206v1 Announce Type: new 
Abstract: This paper presents a new method for conditional probability density simulation.The method is design to work with unstructured data set when data are not characterized by the same covariates yet share common information. Specific examples considered in the text are relative to two main classes: homogeneous data characterized by samples with missing value for the covariates and data set divided in two or more groups characterized by covariates that are only partially overlapping. The methodology is based on the mathematical theory of optimal transport extending the barycenter problem to the newly defined hierarchical barycenter problem. A newly, data driven, numerical procedure for the solution of the hierarchical barycenter problem is proposed and its advantages, over the use of classical barycenter, are illustrated on synthetic and real world data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00206v1</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Esteban G. Tabak, Giulio Trigila, Wenjun Zhao</dc:creator>
    </item>
    <item>
      <title>Predictiveness Curve Assessment under Competing Risks for Risk Prediction Models</title>
      <link>https://arxiv.org/abs/2508.00216</link>
      <description>arXiv:2508.00216v1 Announce Type: new 
Abstract: The predictiveness curve is a valuable tool for predictive evaluation, risk stratification, and threshold selection in a target population, given a single biomarker or a prediction model. In the presence of competing risks, regression models are often used to generate predictive risk scores or probabilistic predictions targeting the cumulative incidence function--distinct from the cumulative distribution function used in conventional predictiveness curve analyses. We propose estimation and inference procedures for the predictiveness curve with a competing risks regression model, to display the relationship between the cumulative incidence probability and the quantiles of model-based predictions. The estimation procedure combines cross-validation with a flexible regression model for tau-year event risk given the model-based risk score, with corresponding inference procedures via perturbation resampling. The proposed methods perform satisfactorily in simulation studies and are implemented through an R package. We apply the proposed methods to a cirrhosis study to depict the predictiveness curve with model-based predictions for liver-related mortality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00216v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Tao, Jing Ning, Wen Li, Wenyaw Chan, Xi Luo, Ruosha Li</dc:creator>
    </item>
    <item>
      <title>Factor Augmented Quantile Regression Model</title>
      <link>https://arxiv.org/abs/2508.00275</link>
      <description>arXiv:2508.00275v1 Announce Type: new 
Abstract: Along with the widespread adoption of high-dimensional data, traditional statistical methods face significant challenges in handling problems with high correlation of variables, heavy-tailed distribution, and coexistence of sparse and dense effects. In this paper, we propose a factor-augmented quantile regression (FAQR) framework to address these challenges simultaneously within a unified framework. The proposed FAQR combines the robustness of quantile regression and the ability of factor analysis to effectively capture dependencies among high-dimensional covariates, and also provides a framework to capture dense effects (through common factors) and sparse effects (through idiosyncratic components) of the covariates. To overcome the lack of smoothness of the quantile loss function, convolution smoothing is introduced, which not only improves computational efficiency but also eases theoretical derivation. Theoretical analysis establishes the accuracy of factor selection and consistency in parameter estimation under mild regularity conditions. Furthermore, we develop a Bootstrap-based diagnostic procedure to assess the adequacy of the factor model. Simulation experiments verify the rationality of FAQR in different noise scenarios such as normal and $t_2$ distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00275v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoyang Wei, Yanlin Tang, Xu Guo, Meiling Hao, Yanmei Shi</dc:creator>
    </item>
    <item>
      <title>Tensor Elliptical Graphic Model</title>
      <link>https://arxiv.org/abs/2508.00333</link>
      <description>arXiv:2508.00333v1 Announce Type: new 
Abstract: We address the problem of robust estimation of sparse high dimensional tensor elliptical graphical model. Most of the research focus on tensor graphical model under normality. To extend the tensor graphical model to more heavy-tailed scenarios, motivated by the fact that up to a constant, the spatial-sign covariance matrix can approximate the true covariance matrix when the dimension turns to infinity under tensor elliptical distribution, we proposed a spatial-sign-based estimator to robustly estimate tensor elliptical graphical model, the rate of which matches the existing rate under normality for a wider family of distribution, i.e. elliptical distribution. We also conducted extensive simulations and real data applications to illustrate the practical utility of the proposed methods, especially under heavy-tailed distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00333v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jixuan Liu, Zhengke Lu, Le Zhou, Long Feng, Zhaojun Wang</dc:creator>
    </item>
    <item>
      <title>Clustering Three-Way Data with Outliers</title>
      <link>https://arxiv.org/abs/2310.05288</link>
      <description>arXiv:2310.05288v3 Announce Type: cross 
Abstract: Matrix-variate distributions are a recent addition to the model-based clustering field, thereby making it possible to analyze data in matrix form with complex structure such as images and time series. Due to its recent appearance, there is limited literature on matrix-variate data, with even less on dealing with outliers in these models. An approach for clustering matrix-variate normal data with outliers is discussed. The approach, which uses the distribution of subset log-likelihoods, extends the OCLUST algorithm to matrix-variate normal data and uses an iterative approach to detect and trim outliers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.05288v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Katharine M. Clark, Paul D. McNicholas</dc:creator>
    </item>
    <item>
      <title>An EM Gradient Algorithm for Mixture Models with Components Derived from the Manly Transformation</title>
      <link>https://arxiv.org/abs/2410.00848</link>
      <description>arXiv:2410.00848v1 Announce Type: cross 
Abstract: Zhu and Melnykov (2018) develop a model to fit mixture models when the components are derived from the Manly transformation. Their EM algorithm utilizes Nelder-Mead optimization in the M-step to update the skew parameter, $\boldsymbol{\lambda}_g$. An alternative EM gradient algorithm is proposed, using one step of Newton's method, when initial estimates for the model parameters are good.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00848v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Katharine M. Clark, Paul D. McNicholas</dc:creator>
    </item>
    <item>
      <title>Efficient rare event estimation for multimodal and high-dimensional system reliability via subset adaptive importance sampling</title>
      <link>https://arxiv.org/abs/2508.00210</link>
      <description>arXiv:2508.00210v1 Announce Type: cross 
Abstract: Estimating rare events in complex systems is a key challenge in reliability analysis. The challenge grows in multimodal problems, where traditional methods often rely on a small set of design points and risk overlooking critical failure modes. Further, higher dimensions make the probability mass harder to capture and demand substantially larger sample sizes to estimate failures. In this work, we propose a new sampling strategy, subset adaptive importance sampling (SAIS), that combines the strengths of subset simulation and adaptive multiple importance sampling. SAIS iteratively refines a set of proposal distributions using weighted samples from previous stages, efficiently exploring complex and high-dimensional failure regions. Leveraging recent advances in adaptive importance sampling, SAIS yields low-variance estimates using fewer samples than state-of-the-art methods and achieves pronounced improvements in both accuracy and computational cost. Through a series of benchmark problems involving high-dimensional, nonlinear performance functions, and multimodal scenarios, we demonstrate that SAIS consistently outperforms competing methods in capturing diverse failure modes and estimating failure probabilities with high precision.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00210v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sara Helal, Victor Elvira</dc:creator>
    </item>
    <item>
      <title>Structural Causal Models for Extremes: an Approach Based on Exponent Measures</title>
      <link>https://arxiv.org/abs/2508.00223</link>
      <description>arXiv:2508.00223v1 Announce Type: cross 
Abstract: We introduce a new formulation of structural causal models for extremes, called the extremal structural causal model (eSCM). Unlike conventional structural causal models, where randomness is governed by a probability distribution, eSCMs use an exponent measure--an infinite-mass law that naturally arises in the analysis of multivariate extremes. Central to this framework are activation variables, which abstract the single-big-jump principle, along with additional randomization that enriches the class of eSCM laws. This formulation encompasses all possible laws of directed graphical models under the recently introduced notion of extremal conditional independence. We also identify an inherent asymmetry in eSCMs under natural assumptions, enabling the identifiability of causal directions, a central challenge in causal inference. Finally, we propose a method that utilizes this causal asymmetry and demonstrate its effectiveness in both simulated and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00223v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fei Fang, Shuyang Bai, Tiandong Wang</dc:creator>
    </item>
    <item>
      <title>Assessing (im)balance in signed brain networks</title>
      <link>https://arxiv.org/abs/2508.00542</link>
      <description>arXiv:2508.00542v1 Announce Type: cross 
Abstract: Many complex systems - be they financial, natural or social - are composed by units - such as stocks, neurons or agents - whose joint activity can be represented as a multivariate time series. An issue of both practical and theoretical importance concerns the possibility of inferring the presence of a static relationships between any two units solely from their dynamic state. The present contribution aims at providing an answer within the frame of traditional hypothesis testing. Briefly speaking, our suggestion is that of linking any two units if behaving in a sufficiently similar way. To achieve such a goal, we project a multivariate time series onto a signed graph, by i) comparing the empirical properties of the former with those expected under a suitable benchmark and ii) linking any two units with a positive (negative) edge in case the corresponding series share a significantly large number of concordant (discordant) values. To define our benchmarks, we adopt an information-theoretic approach that is rooted into the constrained maximisation of Shannon entropy, a procedure inducing an ensemble of multivariate time series that preserves some of the empirical properties on average while randomising everything else. We showcase the possible applications of our method by addressing one of the most timely issues in the domain of neurosciences, i.e. that of determining if brain networks are frustrated or not - and, in case, to what extent. As our results suggest, this is indeed the case, the structure of the negative subgraph being more prone to inter-subject variability than the complementary, positive subgraph. At the mesoscopic level, instead, the minimisation of the Bayesian Information Criterion instantiated with the Signed Stochastic Block Model reveals that brain areas gather into modules aligning with the statistical variant of the Relaxed Balance Theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00542v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>physics.data-an</category>
      <category>physics.med-ph</category>
      <category>stat.ME</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marzio Di Vece, Emanuele Agrimi, Samuele Tatullo, Tommaso Gili, Miguel Ib\'a\~nez-Berganza, Tiziano Squartini</dc:creator>
    </item>
    <item>
      <title>Multi-Band Variable-Lag Granger Causality: A Unified Framework for Causal Time Series Inference across Frequencies</title>
      <link>https://arxiv.org/abs/2508.00658</link>
      <description>arXiv:2508.00658v1 Announce Type: cross 
Abstract: Understanding causal relationships in time series is fundamental to many domains, including neuroscience, economics, and behavioral science. Granger causality is one of the well-known techniques for inferring causality in time series. Typically, Granger causality frameworks have a strong fix-lag assumption between cause and effect, which is often unrealistic in complex systems. While recent work on variable-lag Granger causality (VLGC) addresses this limitation by allowing a cause to influence an effect with different time lags at each time point, it fails to account for the fact that causal interactions may vary not only in time delay but also across frequency bands. For example, in brain signals, alpha-band activity may influence another region with a shorter delay than slower delta-band oscillations. In this work, we formalize Multi-Band Variable-Lag Granger Causality (MB-VLGC) and propose a novel framework that generalizes traditional VLGC by explicitly modeling frequency-dependent causal delays. We provide a formal definition of MB-VLGC, demonstrate its theoretical soundness, and propose an efficient inference pipeline. Extensive experiments across multiple domains demonstrate that our framework significantly outperforms existing methods on both synthetic and real-world datasets, confirming its broad applicability to any type of time series data. Code and datasets are publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00658v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chakattrai Sookkongwaree, Tattep Lakmuang, Chainarong Amornbunchornvej</dc:creator>
    </item>
    <item>
      <title>On admissibility in post-hoc hypothesis testing</title>
      <link>https://arxiv.org/abs/2508.00770</link>
      <description>arXiv:2508.00770v1 Announce Type: cross 
Abstract: The validity of classical hypothesis testing requires the significance level $\alpha$ be fixed before any statistical analysis takes place. This is a stringent requirement. For instance, it prohibits updating $\alpha$ during (or after) an experiment due to changing concern about the cost of false positives, or to reflect unexpectedly strong evidence against the null. Perhaps most disturbingly, witnessing a p-value $p\ll\alpha$ vs $p\leq \alpha$ has no (statistical) relevance for any downstream decision-making. Following recent work of Gr\"unwald (2024), we develop a theory of post-hoc hypothesis testing, enabling $\alpha$ to be chosen after seeing and analyzing the data. To study "good" post-hoc tests we introduce $\Gamma$-admissibility, where $\Gamma$ is a set of adversaries which map the data to a significance level. A test is $\Gamma$-admissible if, roughly speaking, there is no other test which performs at least as well and sometimes better across all adversaries in $\Gamma$. For point nulls and alternatives, we prove general properties of any $\Gamma$-admissible test for any $\Gamma$ and show that they must be based on e-values. We also classify the set of admissible tests for various specific $\Gamma$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00770v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ben Chugg, Tyron Lardy, Aaditya Ramdas, Peter Gr\"unwald</dc:creator>
    </item>
    <item>
      <title>Local Poisson Deconvolution for Discrete Signals</title>
      <link>https://arxiv.org/abs/2508.00824</link>
      <description>arXiv:2508.00824v1 Announce Type: cross 
Abstract: We analyze the statistical problem of recovering an atomic signal, modeled as a discrete uniform distribution $\mu$, from a binned Poisson convolution model. This question is motivated, among others, by super-resolution laser microscopy applications, where precise estimation of $\mu$ provides insights into spatial formations of cellular protein assemblies. Our main results quantify the local minimax risk of estimating $\mu$ for a broad class of smooth convolution kernels. This local perspective enables us to sharply quantify optimal estimation rates as a function of the clustering structure of the underlying signal. Moreover, our results are expressed under a multiscale loss function, which reveals that different parts of the underlying signal can be recovered at different rates depending on their local geometry. Overall, these results paint an optimistic perspective on the Poisson deconvolution problem, showing that accurate recovery is achievable under a much broader class of signals than suggested by existing global minimax analyses. Beyond Poisson deconvolution, our results also allow us to establish the local minimax rate of parameter estimation in Gaussian mixture models with uniform weights.
  We apply our methods to experimental super-resolution microscopy data to identify the location and configuration of individual DNA origamis. In addition, we complement our findings with numerical experiments on runtime and statistical recovery that showcase the practical performance of our estimators and their trade-offs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00824v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shayan Hundrieser, Tudor Manole, Danila Litskevich, Axel Munk</dc:creator>
    </item>
    <item>
      <title>Zero &amp; $N$-inflated overdispersed binomial models for sum-constrained Poisson count processes</title>
      <link>https://arxiv.org/abs/1407.0064</link>
      <description>arXiv:1407.0064v5 Announce Type: replace 
Abstract: A frequent challenge encountered with compositional ecological data is how to interpret and model data with a high proportion of zeros and $N$'s. Such data frequently occur in ecological applications where counts of species are collected until a pre-specified total imposed (typically) by sampling cost is reached. In the bivariate count (two-species) setting we focus on in this article, zero-inflation of one species will result in $N$-inflation of the other. This can lead to species absence being attributed to an unsuitable habitat as opposed to missingness by chance. Similarly, an excess of $N$'s will lead to misleading inferences about habitat preference and abundance estimates. Our contribution is to identify that two independent zero-inflated Poisson processes subject to a sum constraint provide a novel biologically-motivated generating mechanism for the occurrence of binomial count data exhibiting zero and $N$-inflation. We identify an extension to the model to capture additional overdispersion within the data resulting in a novel zero and $N$-inflated beta-binomial model. We consider two motivating datasets, one involving a pesticide treatment for an invasive species, and a second involving the abundance of two plant species. We demonstrate that incorporation of covariates in each case enable learning about sources of zero and $N$-inflation as well as abundance. We show that the models result in improved understanding of underlying biological processes as well as improved predictive performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:1407.0064v5</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>James Sweeney, John Haslett, Dipankar Bandyopadhyay, Michael Fop, Andrew C. Parnell</dc:creator>
    </item>
    <item>
      <title>Approximating optimal SMC proposal distributions in individual-based epidemic models</title>
      <link>https://arxiv.org/abs/2206.05161</link>
      <description>arXiv:2206.05161v4 Announce Type: replace 
Abstract: Many epidemic models are naturally defined as individual-based models: where we track the state of each individual within a susceptible population. Inference for individual-based models is challenging due to the high-dimensional state-space of such models, which increases exponentially with population size. We consider sequential Monte Carlo algorithms for inference for individual-based epidemic models where we make direct observations of the state of a sample of individuals. Standard implementations, such as the bootstrap filter or the auxiliary particle filter are inefficient due to mismatch between the proposal distribution of the state and future observations. We develop new efficient proposal distributions that take account of future observations, leveraging the properties that (i) we can analytically calculate the optimal proposal distribution for a single individual given future observations and the future infection rate of that individual; and (ii) the dynamics of individuals are independent if we condition on their infection rates. Thus we construct estimates of the future infection rate for each individual, and then use an independent proposal for the state of each individual given this estimate. Empirical results show order of magnitude improvement in efficiency of the sequential Monte Carlo sampler for both SIS and SEIR models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.05161v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Rimella, Christopher Jewell, Paul Fearnhead</dc:creator>
    </item>
    <item>
      <title>A p-value for Process Tracing and other N=1 Studies</title>
      <link>https://arxiv.org/abs/2310.13826</link>
      <description>arXiv:2310.13826v3 Announce Type: replace 
Abstract: We introduce a method for calculating \(p\)-values to test causal hypotheses in qualitative research \emph{a la} process tracing. As in an experiment, our \(p\)-value tells us how often one would make the same or more compelling observations favoring one theory while entertaining a rival theory. We adapt Fisher's (1935) randomization-based urn model to the reality of qualitative researchers, who cannot randomize history, but can make observations about historical processes. Our test includes a method of sensitivity analysis which allows researchers to account for the possibility of observation bias, as well as a framework for representing the varying strenght of individual pieces of evidence, altoguether informing the robustness of qualitative causal inefernce. We provide simulations and replications of previously published work to illustrate how to execute our test using any type of qualitative data about events that took place within one case. This approach adds to the pluralistic turn in the use of probability theory in theory-testing process tracing by offering a simple model with provable conservatism, while relying on few assumptions the consequences of which can be directly assessed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.13826v3</guid>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Matias Lopez, Jake Bowers</dc:creator>
    </item>
    <item>
      <title>Dimension-reduced Reconstruction Map Learning for Parameter Estimation in Likelihood-Free Inference Problems</title>
      <link>https://arxiv.org/abs/2407.13971</link>
      <description>arXiv:2407.13971v2 Announce Type: replace 
Abstract: Many application areas rely on models that can be readily simulated but lack a closed-form likelihood, or an accurate approximation under arbitrary parameter values. Existing parameter estimation approaches in this setting are generally approximate. Recent work on using neural network models to reconstruct the mapping from the data space to the parameters from a set of synthetic parameter-data pairs suffers from the curse of dimensionality, resulting in inaccurate estimation as the data size grows. We propose a dimension-reduced approach to likelihood-free estimation which combines the ideas of reconstruction map estimation with dimension-reduction approaches based on subject-specific knowledge. We examine the properties of reconstruction map estimation with and without dimension reduction and explore the trade-off between approximation error due to information loss from reducing the data dimension and approximation error. Numerical examples show that the proposed approach compares favorably with reconstruction map estimation, approximate Bayesian computation, and synthetic likelihood estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13971v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rui Zhang, Oksana A. Chkrebtii, Dongbin Xiu</dc:creator>
    </item>
    <item>
      <title>Localized Sparse Principal Component Analysis of Multivariate Time Series in Frequency Domain</title>
      <link>https://arxiv.org/abs/2408.08177</link>
      <description>arXiv:2408.08177v2 Announce Type: replace 
Abstract: Principal component analysis has been a main tool in multivariate analysis for estimating a low dimensional linear subspace that explains most of the variability in the data. However, in high-dimensional regimes, naive estimates of the principal loadings are not consistent and difficult to interpret. In the context of time series, principal component analysis of spectral density matrices can provide valuable, parsimonious information about the behavior of the underlying process, particularly if the principal components are interpretable in that they are sparse in coordinates and localized in frequency bands. In this paper, we introduce a formulation and consistent estimation procedure for interpretable principal component analysis for high-dimensional time series in the frequency domain. An efficient frequency-sequential algorithm is developed to compute sparse-localized estimates of the low-dimensional principal subspaces of the signal process. The method is motivated by and used to understand neurological mechanisms from high-density resting-state EEG in a study of first episode psychosis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08177v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jamshid Namdari, Amita Manatunga, Fabio Ferrarelli, Robert Krafty</dc:creator>
    </item>
    <item>
      <title>Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions</title>
      <link>https://arxiv.org/abs/2408.13414</link>
      <description>arXiv:2408.13414v3 Announce Type: replace 
Abstract: Fitting models to data is an important part of the practice of science. Advances in machine learning have made it possible to fit more -- and more complex -- models, but have also exacerbated a problem: when multiple models fit the data equally well, which one(s) should we pick? The answer depends entirely on the modelling goal. In the scientific context, the essential goal is _replicability_: if a model works well to describe one experiment, it should continue to do so when that experiment is replicated tomorrow, or in another laboratory. The selection criterion must therefore be robust to the variations inherent to the replication process. In this work we develop a nonparametric method for estimating uncertainty on a model's empirical risk when replications are non-stationary, thus ensuring that a model is only rejected when another is _reproducibly_ better. We illustrate the method with two examples: one a more classical setting, where the models are structurally distinct, and a machine learning-inspired setting, where they differ only in the value of their parameters. We show how, in this context of replicability or "epistemic uncertainty", it compares favourably to existing model selection criteria, and has more satisfactory behaviour with large experimental datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13414v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandre Ren\'e, Andr\'e Longtin</dc:creator>
    </item>
    <item>
      <title>Bayesian CART models for aggregate claim modeling</title>
      <link>https://arxiv.org/abs/2409.01908</link>
      <description>arXiv:2409.01908v2 Announce Type: replace 
Abstract: This paper proposes three types of Bayesian CART (or BCART) models for aggregate claim amount, namely, frequency-severity models, sequential models and joint models. We propose a general framework for the BCART models applicable to data with multivariate responses, which is particularly useful for the joint BCART models with a bivariate response: the number of claims and aggregate claim amount. To facilitate frequency-severity modeling, we investigate BCART models for the right-skewed and heavy-tailed claim severity data by using various distributions. We discover that the Weibull distribution is superior to gamma and lognormal distributions, due to its ability to capture different tail characteristics in tree models. Additionally, we find that sequential BCART models and joint BCART models, which incorporate dependence between the number of claims and average severity, are beneficial and thus preferable to the frequency-severity BCART models in which independence is assumed. The effectiveness of these models' performance is illustrated by carefully designed simulations and real insurance data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01908v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yaojun Zhang, Lanpeng Ji, Georgios Aivaliotis, Charles C. Taylor</dc:creator>
    </item>
    <item>
      <title>Forecasting Causal Effects of Future Interventions: Confounding and Transportability Issues</title>
      <link>https://arxiv.org/abs/2409.13060</link>
      <description>arXiv:2409.13060v2 Announce Type: replace 
Abstract: Recent developments in causal inference allow us to transport a causal effect of a time-fixed treatment from a randomized trial to a target population across space but within the same time frame. In contrast to transportability across space, transporting causal effects across time or forecasting causal effects of future interventions is more challenging due to time-varying confounders and time-varying effect modifiers. In this article, we seek to formally clarify the causal estimands for forecasting causal effects over time and the structural assumptions required to identify these estimands. Specifically, we develop a set of novel nonparametric identification formulas--g-computation formulas--for these causal estimands, and lay out the conditions required to accurately forecast causal effects from a past observed sample to a future population in a future time window. Our overarching objective is to leverage the modern causal inference theory to provide a theoretical framework for investigating whether the effects seen in a past sample would carry over to a new future population. Throughout the article, a working example addressing the effect of public policies or social events on COVID-related deaths is considered to contextualize the developments of analytical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13060v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laura Forastiere, Fan Li, Michela Baccini</dc:creator>
    </item>
    <item>
      <title>Graph-based Square-Root Estimation for Sparse Linear Regression</title>
      <link>https://arxiv.org/abs/2411.12479</link>
      <description>arXiv:2411.12479v2 Announce Type: replace 
Abstract: Sparse linear regression is one of the classic problems in the field of statistics, which has deep connections and high intersections with optimization, computation, and machine learning. To address the effective handling of high-dimensional data, the diversity of real noise, and the challenges in estimating standard deviation of the noise, we propose a novel and general graph-based square-root estimation (GSRE) model for sparse linear regression. Specifically, we use square-root-loss function to encourage the estimators to be independent of the unknown standard deviation of the error terms and design a sparse regularization term by using the graphical structure among predictors in a node-by-node form. Based on the predictor graphs with special structure, we highlight the generality by analyzing that the model in this paper is equivalent to several classic regression models. Theoretically, we also analyze the finite sample bounds, asymptotic normality and model selection consistency of GSRE method without relying on the standard deviation of error terms. In terms of computation, we employ the fast and efficient alternating direction method of multipliers. Finally, based on a large number of simulated and real data with various types of noise, we demonstrate the performance advantages of the proposed method in estimation, prediction and model selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12479v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peili Li, Zhuomei Li, Yunhai Xiao, Chao Ying, Zhou Yu</dc:creator>
    </item>
    <item>
      <title>COADVISE: Covariate Adjustment with Variable Selection in Randomized Controlled Trials</title>
      <link>https://arxiv.org/abs/2501.08945</link>
      <description>arXiv:2501.08945v4 Announce Type: replace 
Abstract: Adjusting for covariates in randomized controlled trials can enhance the credibility and efficiency of treatment effect estimation. However, handling numerous covariates and their complex (non-linear) transformations poses a challenge. Motivated by the case study of the Best Apnea Interventions for Research (BestAIR) trial data from the National Sleep Research Resource (NSRR), where the number of covariates (p=114) is comparable to the sample size (N=196), we propose a principled Covariate Adjustment with Variable Selection (COADVISE) framework. COADVISE enables variable selection for covariates most relevant to the outcome while accommodating both linear and nonlinear adjustments. This framework ensures consistent estimates with improved efficiency over unadjusted estimators and provides robust variance estimation, even under outcome model misspecification. We demonstrate efficiency gains through theoretical analysis, extensive simulations, and a re-analysis of the BestAIR trial data to compare alternative variable selection strategies, offering cautionary recommendations. A user-friendly R package, Coadvise, is available to facilitate practical implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08945v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yi Liu, Ke Zhu, Larry Han, Shu Yang</dc:creator>
    </item>
    <item>
      <title>Data-Driven Adjustment for Multiple Treatments</title>
      <link>https://arxiv.org/abs/2503.08971</link>
      <description>arXiv:2503.08971v2 Announce Type: replace 
Abstract: Covariate adjustment is one method of causal effect identification in non-experimental settings. Prior research provides routes for finding appropriate adjustments sets, but much of this research assumes knowledge of the underlying causal graph. In this paper, we present two routes for finding adjustment sets that do not require knowledge of a graph -- and instead rely on dependencies and independencies in the data directly. We consider a setting where the adjustment set is unaffected by treatment or outcome. The first route shows how to extend prior research in this area using a concept known as c-equivalence. Our second route provides sufficient criteria for finding adjustment sets in the setting of multiple treatments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08971v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sara LaPlante, Sofia Triantafillou, Emilija Perkovi\'c</dc:creator>
    </item>
    <item>
      <title>Conditional independence testing with a single realization of a multivariate nonstationary nonlinear time series</title>
      <link>https://arxiv.org/abs/2504.21647</link>
      <description>arXiv:2504.21647v2 Announce Type: replace 
Abstract: Identifying relationships among stochastic processes is a core objective in many fields, such as economics. While the standard toolkit for multivariate time series analysis has many advantages, it can be difficult to capture nonlinear dynamics using linear vector autoregressive models. This difficulty has motivated the development of methods for causal discovery and variable selection for nonlinear time series, which routinely employ tests for conditional independence. In this paper, we introduce the first framework for conditional independence testing that works with a single realization of a nonstationary nonlinear process. We also show how our framework can be used to test for independence. The key technical ingredients of our framework are time-varying nonlinear regression, estimation of local long-run covariance matrices of products of error processes, and a distribution-uniform strong Gaussian approximation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21647v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Wieck-Sosa, Michel F. C. Haddad, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Preconditioned Discrete-HAMS: A Second-order Irreversible Discrete Sampler</title>
      <link>https://arxiv.org/abs/2507.21982</link>
      <description>arXiv:2507.21982v2 Announce Type: replace 
Abstract: Gradient-based Markov Chain Monte Carlo methods have recently received much attention for sampling discrete distributions, with notable examples such as Norm Constrained Gradient (NCG), Auxiliary Variable Gradient (AVG), and Discrete Hamiltonian Assisted Metropolis Sampling (DHAMS). In this work, we propose the Preconditioned Discrete-HAMS (PDHAMS) algorithm, which extends DHAMS by incorporating a second-order, quadratic approximation of the potential function, and uses Gaussian integral trick to avoid directly sampling a pairwise Markov random field. The PDHAMS sampler not only satisfies generalized detailed balance, hence enabling irreversible sampling, but also is a rejection-free property for a target distribution with a quadratic potential function. In various numerical experiments, PDHAMS algorithms consistently yield superior performance compared with other methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21982v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuze Zhou, Zhiqiang Tan</dc:creator>
    </item>
    <item>
      <title>Assessing Racial Disparities in Healthcare Expenditures via Mediator Distribution Shifts</title>
      <link>https://arxiv.org/abs/2504.21688</link>
      <description>arXiv:2504.21688v2 Announce Type: replace-cross 
Abstract: Racial disparities in healthcare expenditures are well-documented, yet the underlying drivers remain complex and require further investigation. This study develops a framework for decomposing such disparities through shifts in the distributions of mediating variables, rather than treating race itself as a manipulable exposure. We define disparities as differences in covariate-adjusted outcome distributions across racial groups, and decompose the total disparity into two components: one attributable to differences in mediator distributions, and another residual component that would remain even after equalizing these distributions. Using data from the Medical Expenditures Panel Survey, we examine the extent to which expenditure disparities would persist or be reduced if mediators such as socioeconomic status, insurance access, health behaviors, or health status were equalized across racial groups. To ensure valid inference, we derive asymptotically linear estimators based on influence-function techniques and flexible machine learning tools, including super learners and a two-part model designed for the zero-inflated, right-skewed nature of expenditure data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21688v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaxian Ou, Xinwei He, David Benkeser, Razieh Nabi</dc:creator>
    </item>
    <item>
      <title>Conformal changepoint localization</title>
      <link>https://arxiv.org/abs/2505.00292</link>
      <description>arXiv:2505.00292v3 Announce Type: replace-cross 
Abstract: Changepoint localization is the problem of estimating the index at which a change occurred in the data generating distribution of an ordered list of data, or declaring that no change occurred. We present the broadly applicable CONCH (CONformal CHangepoint localization) algorithm, which uses a matrix of conformal p-values to produce a confidence interval for a (single) changepoint under the mild assumption that the pre-change and post-change distributions are each exchangeable. We exemplify the CONCH algorithm on a variety of synthetic and real-world datasets, including using black-box pre-trained classifiers to detect changes in sequences of images, text, and accelerometer data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00292v3</guid>
      <category>math.ST</category>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sanjit Dandapanthula, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Batched Nonparametric Bandits via k-Nearest Neighbor UCB</title>
      <link>https://arxiv.org/abs/2505.10498</link>
      <description>arXiv:2505.10498v2 Announce Type: replace-cross 
Abstract: We study sequential decision-making in batched nonparametric contextual bandits, where actions are selected over a finite horizon divided into a small number of batches. Motivated by constraints in domains such as medicine and marketing -- where online feedback is limited -- we propose a nonparametric algorithm that combines adaptive k-nearest neighbor (k-NN) regression with the upper confidence bound (UCB) principle. Our method, BaNk-UCB, is fully nonparametric, adapts to the context dimension, and is simple to implement. Unlike prior work relying on parametric or binning-based estimators, BaNk-UCB uses local geometry to estimate rewards and adaptively balances exploration and exploitation. We provide near-optimal regret guarantees under standard Lipschitz smoothness and margin assumptions, using a theoretically motivated batch schedule that balances regret across batches and achieves minimax-optimal rates. Empirical evaluations on synthetic and real-world datasets demonstrate that BaNk-UCB consistently outperforms binning-based baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10498v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sakshi Arya</dc:creator>
    </item>
  </channel>
</rss>
