<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SI</link>
    <description>cs.SI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 05 Nov 2024 05:00:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Analyzing Social Networks of Actors in Movies and TV Shows</title>
      <link>https://arxiv.org/abs/2411.00975</link>
      <description>arXiv:2411.00975v1 Announce Type: new 
Abstract: The paper offers a comprehensive analysis of social networks among movie actors and directors in the film industry. Utilizing data from IMDb and Netflix, we leverage Python and NetworkX to uncover valuable insights into the movie industry's intricate web of collaborations. Key findings include identifying the top actors and directors in the OTT sector, tracking the rise of movies on OTT platforms, and analyzing centrality measures for actors. We also explore the hidden patterns within the movie data, unveiling the shortest paths between actors and predicting future collaborations. Cluster analysis categorizes movies based on various criteria, revealing the most insular and liberal clusters and identifying crossover actors bridging different segments of the industry. The study highlights that actors predominantly collaborate within language groups, transcending national boundaries. We investigate the degree of isolation of Bollywood from global cinema and identify actors working across world clusters. The project provides valuable insights into the evolving dynamics of the film industry and the impact of OTT platforms, benefiting industry professionals, scholars, and enthusiasts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00975v1</guid>
      <category>cs.SI</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Sarthak Giri, Sneha Chaudhary, Bikalpa Gautam</dc:creator>
    </item>
    <item>
      <title>A Large-scale Time-aware Agents Simulation for Influencer Selection in Digital Advertising Campaigns</title>
      <link>https://arxiv.org/abs/2411.01143</link>
      <description>arXiv:2411.01143v1 Announce Type: new 
Abstract: In the digital world, influencers are pivotal as opinion leaders, shaping the views and choices of their influencees. Modern advertising often follows this trend, where marketers choose appropriate influencers for product endorsements, based on thorough market analysis. Previous studies on influencer selection have typically relied on numerical representations of individual opinions and interactions, a method that simplifies the intricacies of social dynamics. In this work, we first introduce a Time-aware Influencer Simulator (TIS), helping promoters identify and select the right influencers to market their products, based on LLM simulation. To validate our approach, we conduct experiments on the public advertising campaign dataset SAGraph which encompasses social relationships, posts, and user interactions. The results show that our method outperforms traditional numerical feature-based approaches and methods using limited LLM agents. Our research shows that simulating user timelines and content lifecycles over time simplifies scaling, allowing for large-scale agent simulations in social networks. Additionally, LLM-based agents for social recommendations and advertising offer substantial benefits for decision-making in promotional campaigns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01143v1</guid>
      <category>cs.SI</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xiaoqing Zhang, Xiuying Chen, Yuhan Liu, Jianzhou Wang, Zhenxing Hu, Rui Yan</dc:creator>
    </item>
    <item>
      <title>I've Heard This Before: Initial Results on Tiktok's Impact On the Re-Popularization of Songs</title>
      <link>https://arxiv.org/abs/2411.01239</link>
      <description>arXiv:2411.01239v1 Announce Type: new 
Abstract: With over a billion active users, TikTok's video-sharing service is currently one of the largest social media websites. This rise in TikTok's popularity has made the website a central platform for music discovery. In this paper, we analyze how TikTok helps to revitalize older songs. To do so, we use both the popularity of songs shared on TikTok and how the platform allows songs to propagate to other places on the Web. We analyze data from TokBoard, a website measuring such popularity over time, and Google Trends, which captures songs' overall Web search interest. Our analysis initially focuses on whether TokBoard can cause (Granger Causality) popularity on Google Trends. Next, we examine whether TikTok and Google Trends share the same virality patterns (via a Bass Model). To our knowledge, we are one of the first works to study song re-popularization via TikTok.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01239v1</guid>
      <category>cs.SI</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Breno Matos, Francisco Galuppo, Rennan Cordeiro, Flavio Figueiredo</dc:creator>
    </item>
    <item>
      <title>Assessing the Impact of Sampling, Remixes, and Covers on Original Song Popularity</title>
      <link>https://arxiv.org/abs/2411.01242</link>
      <description>arXiv:2411.01242v1 Announce Type: new 
Abstract: Music digitalization has introduced new forms of composition known as "musical borrowings", where composers use elements of existing songs -- such as melodies, lyrics, or beats -- to create new songs. Using Who Sampled data and Google Trends, we examine how the popularity of a borrowing song affects the original. Employing Regression Discontinuity Design (RDD) for short-term effects and Granger Causality for long-term impacts, we find evidence of causal popularity boosts in some cases. Borrowee songs can revive interest in older tracks, underscoring economic dynamics that may support fairer compensation in the music industry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01242v1</guid>
      <category>cs.SI</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guilherme Soares S. dos Santos, Flavio Figueiredo</dc:creator>
    </item>
    <item>
      <title>Unfiltered Conversations: A Dataset of 2024 U.S. Presidential Election Discourse on Truth Social</title>
      <link>https://arxiv.org/abs/2411.01330</link>
      <description>arXiv:2411.01330v1 Announce Type: new 
Abstract: Truth Social, launched as a social media platform with a focus on free speech, has become a prominent space for political discourse, attracting a user base with diverse, yet often conservative, viewpoints. As an emerging platform with minimal content moderation, Truth Social has facilitated discussions around contentious social and political issues but has also seen the spread of conspiratorial and hyper-partisan narratives. In this paper, we introduce and release a comprehensive dataset capturing activity on Truth Social related to the upcoming 2024 U.S. Presidential Election, including posts, replies, user interactions, content and media. This dataset comprises 1.5 million posts published between February, 2024 and October 2024, and encompasses key user engagement features and posts metadata. Data collection began in June 2024, though it includes posts published earlier, with the oldest post dating back to February 2022. This offers researchers a unique resource to study communication patterns, the formation of online communities, and the dissemination of information within Truth Social in the run-up to the election. By providing an in-depth view of Truth Social's user dynamics and content distribution, this dataset aims to support further research on political discourse within an alt-tech social media platform. The dataset is publicly available at https://github.com/kashish-s/TruthSocial_2024ElectionInitiative</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01330v1</guid>
      <category>cs.SI</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kashish Shah, Patrick Gerard, Luca Luceri, Emilio Ferrara</dc:creator>
    </item>
    <item>
      <title>Centrality in Collaboration: A Novel Algorithm for Social Partitioning Gradients in Community Detection for Multiple Oncology Clinical Trial Enrollments</title>
      <link>https://arxiv.org/abs/2411.01394</link>
      <description>arXiv:2411.01394v1 Announce Type: new 
Abstract: Patients at a comprehensive cancer center who do not achieve cure or remission following standard treatments often become candidates for clinical trials. Patients who participate in a clinical trial may be suitable for other studies. A key factor influencing patient enrollment in subsequent clinical trials is the structured collaboration between oncologists and most responsible physicians. Possible identification of these collaboration networks can be achieved through the analysis of patient movements between clinical trial intervention types with social network analysis and community detection algorithms. In the detection of oncologist working groups, the present study evaluates three community detection algorithms: Girvan-Newman, Louvain and an algorithm developed by the author. Girvan-Newman identifies each intervention as their own community, while Louvain groups interventions in a manner that is difficult to interpret. In contrast, the author's algorithm groups interventions in a way that is both intuitive and informative, with a gradient effect that is particularly useful for epidemiological research. This lays the groundwork for future subgroup analysis of clustered interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01394v1</guid>
      <category>cs.SI</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin Smith, Tyler Pittman, Wei Xu</dc:creator>
    </item>
    <item>
      <title>Effective Community Detection Over Streaming Bipartite Networks (Technical Report)</title>
      <link>https://arxiv.org/abs/2411.01424</link>
      <description>arXiv:2411.01424v1 Announce Type: new 
Abstract: The streaming bipartite graph is extensively used to model the dynamic relationship between two types of entities in many real-world applications, such as movie recommendations, location-based services, and online shopping. Since it contains abundant information, discovering the dense subgraph with high structural cohesiveness (i.e., community detection) in the bipartite streaming graph is becoming a valuable problem. Inspired by this, in this paper, we study the structure of community on the butterfly motif in the bipartite graph. We propose a novel problem, named Community Detection over Streaming Bipartite Network (CD-SBN), which aims to retrieve qualified communities with user-specific query keywords and high structural cohesiveness at snapshot and continuous scenarios. In particular, we formulate the user relationship score in the weighted bipartite network via the butterfly pattern and define a novel $(k,r,\sigma)$-bitruss as the community structure. To efficiently tackle the CD-SBN problem, we design effective pruning strategies to rule out false alarms of $(k,r,\sigma)$-bitruss and propose a hierarchical synopsis to facilitate the CD-SBN processing. Due to the dynamic of streaming bipartite networks, we devise an efficient procedure for incremental graph maintenance. We develop an efficient algorithm to answer the snapshot and continuous CD-SBN query by traversing the synopsis and applying the pruning strategies. With extensive experiments, we demonstrate the efficiency and effectiveness of our proposed CD-SBN processing approach over real/synthetic streaming bipartite networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01424v1</guid>
      <category>cs.SI</category>
      <category>cs.DB</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nan Zhang, Yutong Ye, Yuyang Wang Xiang Lian, Mingsong Chen</dc:creator>
    </item>
    <item>
      <title>Auditing Political Exposure Bias: Algorithmic Amplification on Twitter/X Approaching the 2024 U.S. Presidential Election</title>
      <link>https://arxiv.org/abs/2411.01852</link>
      <description>arXiv:2411.01852v1 Announce Type: new 
Abstract: Approximately 50% of tweets in X's user timelines are personalized recommendations from accounts they do not follow. This raises a critical question: what political content are users exposed to beyond their established networks, and how might this influence democratic discourse online? Due to the black-box nature and constant evolution of social media algorithms, much remains unknown about this aspect of users' content exposure, particularly as it pertains to potential biases in algorithmic curation. Prior research has shown that certain political groups and media sources are amplified within users' in-network tweets. However, the extent to which this amplification affects out-of-network recommendations remains unclear. As the 2024 U.S. Election approaches, addressing this question is essential for understanding the influence of algorithms on online political content consumption and its potential impact on users' perspectives. In this paper, we conduct a three-week audit of X's algorithmic content recommendations using a set of 120 sock-puppet monitoring accounts that capture tweets in their personalized ``For You'' timelines. Our objective is to quantify out-of-network content exposure for right- and left-leaning user profiles and to assess any potential biases in political exposure. Our findings indicate that X's algorithm skews exposure toward a few high-popularity accounts across all users, with right-leaning users experiencing the highest level of exposure inequality. Both left- and right-leaning users encounter amplified exposure to accounts aligned with their own political views and reduced exposure to opposing viewpoints. Additionally, we observe a right-leaning bias in exposure for new accounts within their default timelines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01852v1</guid>
      <category>cs.SI</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinyi Ye, Luca Luceri, Emilio Ferrara</dc:creator>
    </item>
    <item>
      <title>HACD: Harnessing Attribute Semantics and Mesoscopic Structure for Community Detection</title>
      <link>https://arxiv.org/abs/2411.01947</link>
      <description>arXiv:2411.01947v1 Announce Type: new 
Abstract: Community detection plays a pivotal role in uncovering closely connected subgraphs, aiding various real-world applications such as recommendation systems and anomaly detection. With the surge of rich information available for entities in real-world networks, the community detection problem in attributed networks has attracted widespread attention. While previous research has effectively leveraged network topology and attribute information for attributed community detection, these methods overlook two critical issues: (i) the semantic similarity between node attributes within the community, and (ii) the inherent mesoscopic structure, which differs from the pairwise connections of the micro-structure. To address these limitations, we propose HACD, a novel attributed community detection model based on heterogeneous graph attention networks. HACD treats node attributes as another type of node, constructs attributed networks into heterogeneous graph structures and employs attribute-level attention mechanisms to capture semantic similarity. Furthermore, HACD introduces a community membership function to explore mesoscopic community structures, enhancing the robustness of detected communities. Extensive experiments demonstrate the effectiveness and efficiency of HACD, outperforming state-of-the-art methods in attributed community detection tasks. Our code is publicly available at https://github.com/Anniran1/HACD1-wsdm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01947v1</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <category>cs.GR</category>
      <category>cs.LG</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anran Zhang, Xingfen Wang, Yuhan Zhao</dc:creator>
    </item>
    <item>
      <title>Memory-Efficient Community Detection on Large Graphs Using Weighted Sketches</title>
      <link>https://arxiv.org/abs/2411.02268</link>
      <description>arXiv:2411.02268v1 Announce Type: new 
Abstract: Community detection in graphs identifies groups of nodes with denser connections within the groups than between them, and while existing studies often focus on optimizing detection performance, memory constraints become critical when processing large graphs on shared-memory systems. We recently proposed efficient implementations of the Louvain, Leiden, and Label Propagation Algorithms (LPA) for community detection. However, these incur significant memory overhead from the use of collision-free per-thread hashtables. To address this, we introduce memory-efficient alternatives using weighted Misra-Gries (MG) sketches, which replace the per-thread hashtables, and reduce memory demands in Louvain, Leiden, and LPA implementations - while incurring only a minor quality drop (up to 1%) and moderate runtime penalties. We believe that these approaches, though slightly slower, are well-suited for parallel processing and could outperform current memory-intensive techniques on systems with many threads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02268v1</guid>
      <category>cs.SI</category>
      <category>cs.DC</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Subhajit Sahu</dc:creator>
    </item>
    <item>
      <title>Personality Analysis from Online Short Video Platforms with Multi-domain Adaptation</title>
      <link>https://arxiv.org/abs/2411.00813</link>
      <description>arXiv:2411.00813v1 Announce Type: cross 
Abstract: Personality analysis from online short videos has gained prominence due to its applications in personalized recommendation systems, sentiment analysis, and human-computer interaction. Traditional assessment methods, such as questionnaires based on the Big Five Personality Framework, are limited by self-report biases and are impractical for large-scale or real-time analysis. Leveraging the rich, multi-modal data present in short videos offers a promising alternative for more accurate personality inference. However, integrating these diverse and asynchronous modalities poses significant challenges, particularly in aligning time-varying data and ensuring models generalize well to new domains with limited labeled data. In this paper, we propose a novel multi-modal personality analysis framework that addresses these challenges by synchronizing and integrating features from multiple modalities and enhancing model generalization through domain adaptation. We introduce a timestamp-based modality alignment mechanism that synchronizes data based on spoken word timestamps, ensuring accurate correspondence across modalities and facilitating effective feature integration. To capture temporal dependencies and inter-modal interactions, we employ Bidirectional Long Short-Term Memory networks and self-attention mechanisms, allowing the model to focus on the most informative features for personality prediction. Furthermore, we develop a gradient-based domain adaptation method that transfers knowledge from multiple source domains to improve performance in target domains with scarce labeled data. Extensive experiments on real-world datasets demonstrate that our framework significantly outperforms existing methods in personality prediction tasks, highlighting its effectiveness in capturing complex behavioral cues and robustness in adapting to new domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00813v1</guid>
      <category>cs.MM</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <category>eess.AS</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sixu An, Xiangguo Sun, Yicong Li, Yu Yang, Guandong Xu</dc:creator>
    </item>
    <item>
      <title>Mobility-LLM: Learning Visiting Intentions and Travel Preferences from Human Mobility Data with Large Language Models</title>
      <link>https://arxiv.org/abs/2411.00823</link>
      <description>arXiv:2411.00823v1 Announce Type: cross 
Abstract: Location-based services (LBS) have accumulated extensive human mobility data on diverse behaviors through check-in sequences. These sequences offer valuable insights into users' intentions and preferences. Yet, existing models analyzing check-in sequences fail to consider the semantics contained in these sequences, which closely reflect human visiting intentions and travel preferences, leading to an incomplete comprehension. Drawing inspiration from the exceptional semantic understanding and contextual information processing capabilities of large language models (LLMs) across various domains, we present Mobility-LLM, a novel framework that leverages LLMs to analyze check-in sequences for multiple tasks. Since LLMs cannot directly interpret check-ins, we reprogram these sequences to help LLMs comprehensively understand the semantics of human visiting intentions and travel preferences. Specifically, we introduce a visiting intention memory network (VIMN) to capture the visiting intentions at each record, along with a shared pool of human travel preference prompts (HTPP) to guide the LLM in understanding users' travel preferences. These components enhance the model's ability to extract and leverage semantic information from human mobility data effectively. Extensive experiments on four benchmark datasets and three downstream tasks demonstrate that our approach significantly outperforms existing models, underscoring the effectiveness of Mobility-LLM in advancing our understanding of human mobility data within LBS contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00823v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.SI</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Letian Gong, Yan Lin, Xinyue Zhang, Yiwen Lu, Xuedi Han, Yichen Liu, Shengnan Guo, Youfang Lin, Huaiyu Wan</dc:creator>
    </item>
    <item>
      <title>Transparent Tagging for Strategic Social Nudges on User-Generated Misinformation</title>
      <link>https://arxiv.org/abs/2411.00825</link>
      <description>arXiv:2411.00825v1 Announce Type: cross 
Abstract: Social network platforms (SNP), such as X and TikTok, rely heavily on user-generated content to attract users and advertisers, yet they have limited control over content provision, which leads to the proliferation of misinformation across platforms. As countermeasures, SNPs have implemented various policies, such as tweet labeling, to notify users about potentially misleading information, influencing users' responses, either favorably or unfavorably, to the tagged contents. The population-level response creates a social nudge to the content provider that encourages it to supply more authentic content without exerting direct control over the provider. Yet, when designing such tagging policies to leverage social nudges, SNP must be cautious about the potential misdetection of misinformation (wrongly detecting factual content as misinformation and vice versa), which impairs its credibility to generic users and, hence, its ability to create social nudges. This work establishes a Bayesian persuaded branching process to study SNP's tagging policy design under misdetection. Misinformation circulation is modeled by a multi-type branching process, where users are persuaded through tagging to give positive and negative comments that influence the spread of misinformation. When translated into posterior belief space, the SNP's problem is reduced to an equality-constrained convex optimization, the optimal condition of which is given by the Lagrangian characterization. The key finding is that SNP's optimal policy is simply transparent tagging, i.e., revealing the content's authenticity to the user, albeit midsection, which nudges the provider not to generate misinformation. We corroborate our findings using numerical simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00825v1</guid>
      <category>cs.GT</category>
      <category>cs.SI</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ya-Ting Yang, Tao Li, Quanyan Zhu</dc:creator>
    </item>
    <item>
      <title>Bi-Level Graph Structure Learning for Next POI Recommendation</title>
      <link>https://arxiv.org/abs/2411.01169</link>
      <description>arXiv:2411.01169v1 Announce Type: cross 
Abstract: Next point-of-interest (POI) recommendation aims to predict a user's next destination based on sequential check-in history and a set of POI candidates. Graph neural networks (GNNs) have demonstrated a remarkable capability in this endeavor by exploiting the extensive global collaborative signals present among POIs. However, most of the existing graph-based approaches construct graph structures based on pre-defined heuristics, failing to consider inherent hierarchical structures of POI features such as geographical locations and visiting peaks, or suffering from noisy and incomplete structures in graphs. To address the aforementioned issues, this paper presents a novel Bi-level Graph Structure Learning (BiGSL) for next POI recommendation. BiGSL first learns a hierarchical graph structure to capture the fine-to-coarse connectivity between POIs and prototypes, and then uses a pairwise learning module to dynamically infer relationships between POI pairs and prototype pairs. Based on the learned bi-level graphs, our model then employs a multi-relational graph network that considers both POI- and prototype-level neighbors, resulting in improved POI representations. Our bi-level structure learning scheme is more robust to data noise and incompleteness, and improves the exploration ability for recommendation by alleviating sparsity issues. Experimental results on three real-world datasets demonstrate the superiority of our model over existing state-of-the-art methods, with a significant improvement in recommendation accuracy and exploration performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01169v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.SI</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TKDE.2024.3397683</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Knowledge and Data Engineering, vol. 36, no. 11, pp. 5695-5708, Nov. 2024</arxiv:journal_reference>
      <dc:creator>Liang Wang, Shu Wu, Qiang Liu, Yanqiao Zhu, Xiang Tao, Mengdi Zhang, Liang Wang</dc:creator>
    </item>
    <item>
      <title>Cloned Identity Detection in Social-Sensor Clouds based on Incomplete Profiles</title>
      <link>https://arxiv.org/abs/2411.01329</link>
      <description>arXiv:2411.01329v1 Announce Type: cross 
Abstract: We propose a novel approach to effectively detect cloned identities of social-sensor cloud service providers (i.e. social media users) in the face of incomplete non-privacy-sensitive profile data. Named ICD-IPD, the proposed approach first extracts account pairs with similar usernames or screen names from a given set of user accounts collected from a social media. It then learns a multi-view representation associated with a given account and extracts two categories of features for every single account. These two categories of features include profile and Weighted Generalised Canonical Correlation Analysis (WGCCA)-based features that may potentially contain missing values. To counter the impact of such missing values, a missing value imputer will next impute the missing values of the aforementioned profile and WGCCA-based features. After that, the proposed approach further extracts two categories of augmented features for each account pair identified previously, namely, 1) similarity and 2) differences-based features. Finally, these features are concatenated and fed into a Light Gradient Boosting Machine classifier to detect identity cloning. We evaluated and compared the proposed approach against the existing state-of-the-art identity cloning approaches and other machine or deep learning models atop a real-world dataset. The experimental results show that the proposed approach outperforms the state-of-the-art approaches and models in terms of Precision, Recall and F1-score.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01329v1</guid>
      <category>cs.CY</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TSC.2024.3479912</arxiv:DOI>
      <dc:creator>Ahmed Alharbi, Hai Dong, Xun Yi, Prabath Abeysekara</dc:creator>
    </item>
    <item>
      <title>PageRank Bandits for Link Prediction</title>
      <link>https://arxiv.org/abs/2411.01410</link>
      <description>arXiv:2411.01410v1 Announce Type: cross 
Abstract: Link prediction is a critical problem in graph learning with broad applications such as recommender systems and knowledge graph completion. Numerous research efforts have been directed at solving this problem, including approaches based on similarity metrics and Graph Neural Networks (GNN). However, most existing solutions are still rooted in conventional supervised learning, which makes it challenging to adapt over time to changing customer interests and to address the inherent dilemma of exploitation versus exploration in link prediction. To tackle these challenges, this paper reformulates link prediction as a sequential decision-making process, where each link prediction interaction occurs sequentially. We propose a novel fusion algorithm, PRB (PageRank Bandits), which is the first to combine contextual bandits with PageRank for collaborative exploitation and exploration. We also introduce a new reward formulation and provide a theoretical performance guarantee for PRB. Finally, we extensively evaluate PRB in both online and offline settings, comparing it with bandit-based and graph-based methods. The empirical success of PRB demonstrates the value of the proposed fusion approach. Our code is released at https://github.com/jiaruzouu/PRB.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01410v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.SI</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yikun Ban, Jiaru Zou, Zihao Li, Yunzhe Qi, Dongqi Fu, Jian Kang, Hanghang Tong, Jingrui He</dc:creator>
    </item>
    <item>
      <title>Against Multifaceted Graph Heterogeneity via Asymmetric Federated Prompt Learning</title>
      <link>https://arxiv.org/abs/2411.02003</link>
      <description>arXiv:2411.02003v1 Announce Type: cross 
Abstract: Federated Graph Learning (FGL) aims to collaboratively and privately optimize graph models on divergent data for different tasks. A critical challenge in FGL is to enable effective yet efficient federated optimization against multifaceted graph heterogeneity to enhance mutual performance. However, existing FGL works primarily address graph data heterogeneity and perform incapable of graph task heterogeneity. To address the challenge, we propose a Federated Graph Prompt Learning (FedGPL) framework to efficiently enable prompt-based asymmetric graph knowledge transfer between multifaceted heterogeneous federated participants. Generally, we establish a split federated framework to preserve universal and domain-specific graph knowledge, respectively. Moreover, we develop two algorithms to eliminate task and data heterogeneity for advanced federated knowledge preservation. First, a Hierarchical Directed Transfer Aggregator (HiDTA) delivers cross-task beneficial knowledge that is hierarchically distilled according to the directional transferability. Second, a Virtual Prompt Graph (VPG) adaptively generates graph structures to enhance data utility by distinguishing dominant subgraphs and neutralizing redundant ones. We conduct theoretical analyses and extensive experiments to demonstrate the significant accuracy and efficiency effectiveness of FedGPL against multifaceted graph heterogeneity compared to state-of-the-art baselines on large-scale federated graph datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02003v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.SI</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhuoning Guo, Ruiqian Han, Hao Liu</dc:creator>
    </item>
    <item>
      <title>Towards a valid bibliometric measure of epistemic breadth of researchers</title>
      <link>https://arxiv.org/abs/2411.02005</link>
      <description>arXiv:2411.02005v1 Announce Type: cross 
Abstract: The concept of epistemic breadth of the work of a researcher refers to the scope of their knowledge claims, as reflected in published research reports. Studies of epistemic breadth have been hampered by the lack of a validated measure of the concept. Here we introduce a knowledge space approach to the measurement of epistemic breadth and propose to use the semantic similarity network of an author's publication record to operationalize a measure. In this approach, each paper has its own location in a common abstract vector space based on its content. Proximity in knowledge space corresponds to thematic similarity of publications. Candidate measures of epistemic breadth derived from aggregate similarity values of researchers' bodies of work are tested against external validation data of researchers known to have made a major change in research topic and against self-citation data. We find that some candidate measures co-vary well with known epistemic breadth of researchers in the empirical data and can serve as valid indicators of the concept.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02005v1</guid>
      <category>cs.DL</category>
      <category>cs.SI</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Paul Donner, Clemens Bl\"umel</dc:creator>
    </item>
    <item>
      <title>Conversations with Data: How Data Journalism Affects Online Comments in the New York Times</title>
      <link>https://arxiv.org/abs/2411.02045</link>
      <description>arXiv:2411.02045v1 Announce Type: cross 
Abstract: Users in the data age have access to more data than ever before, but little is known how they interact with it. Using transparency and multimedia, data journalism (DJ) lets users explore and interpret data on their own. This study examines how DJ affects online comments as a case study of user interactions with data. The corpus comprises 6,400 stories and their comment sections from the DJ and other sections of the New York Times, from 2014-2022. Results indicate that DJ is positively associated with higher level of interactivity between the users. This relationship is mediated by statistical information, information sources, and static visualizations. However, there is a low level of interactivity with the content; consequently, only part of the users use it. The results demonstrate how data accessibility through DJ engages the users in conversation. According to deliberation theory, this creates a conducive environment for democratic processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02045v1</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Avner Kantor, Sheizaf Rafaeli</dc:creator>
    </item>
    <item>
      <title>A Survey on the Role of Crowds in Combating Online Misinformation: Annotators, Evaluators, and Creators</title>
      <link>https://arxiv.org/abs/2310.02095</link>
      <description>arXiv:2310.02095v2 Announce Type: replace 
Abstract: Online misinformation poses a global risk with significant real-world consequences. To combat misinformation, current research relies on professionals like journalists and fact-checkers for annotating and debunking misinformation, and develops automated machine learning methods for detecting misinformation. Complementary to these approaches, recent research has increasingly concentrated on utilizing the power of ordinary social media users, a.k.a. "crowd", who act as eyes-on-the-ground proactively questioning and countering misinformation. Notably, recent studies show that 96% of counter-misinformation responses originate from them. Acknowledging their prominent role, we present the first systematic and comprehensive survey of research papers that actively leverage the crowds to combat misinformation.
  We first identify 88 papers related to crowd-based efforts, following a meticulous annotation process adhering to the PRISMA framework. We then present key statistics related to misinformation, counter-misinformation, and crowd input in different formats and topics. Upon holistic analysis of the papers, we introduce a novel taxonomy of the roles played by the crowds: (i)annotators who actively identify misinformation; (ii)evaluators who assess counter-misinformation effectiveness; (iii)creators who create counter-misinformation. This taxonomy explores the crowd's capabilities in misinformation detection, identifies prerequisites for effective counter-misinformation, and analyzes crowd-generated counter-misinformation. Then, we delve into (i)distinguishing individual, collaborative, and machine-assisted labeling for annotators; (ii)analyzing the effectiveness of counter-misinformation through surveys, interviews, and in-lab experiments for evaluators; and (iii)characterizing creation patterns and creator profiles for creators. Finally, we outline potential future research in this field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.02095v2</guid>
      <category>cs.SI</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bing He, Yibo Hu, Yeon-Chang Lee, Soyoung Oh, Gaurav Verma, Srijan Kumar</dc:creator>
    </item>
    <item>
      <title>Graph sub-sampling for divide-and-conquer algorithms in large networks</title>
      <link>https://arxiv.org/abs/2409.06994</link>
      <description>arXiv:2409.06994v2 Announce Type: replace 
Abstract: As networks continue to increase in size, current methods must be capable of handling large numbers of nodes and edges in order to be practically relevant. Instead of working directly with the entire (large) network, analyzing sub-networks has become a popular approach. Due to a network's inherent inter-connectedness, sub-sampling is not a trivial task. While this problem has gained attention in recent years, it has not received sufficient attention from the statistics community. In this work, we provide a thorough comparison of seven graph sub-sampling algorithms by applying them to divide-and-conquer algorithms for community structure and core-periphery (CP) structure. After discussing the various algorithms and sub-sampling routines, we derive theoretical results for the mis-classification rate of the divide-and-conquer algorithm for CP structure under various sub-sampling schemes. We then perform extensive experiments on both simulated and real-world data to compare the various methods. For the community detection task, we found that sampling nodes uniformly at random yields the best performance. For CP structure on the other hand, there was no single winner, but algorithms which sampled core nodes at a higher rate consistently outperformed other sampling routines, e.g., random edge sampling and random walk sampling. The varying performance of the sampling algorithms on different tasks demonstrates the importance of carefully selecting a sub-sampling routine for the specific application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06994v2</guid>
      <category>cs.SI</category>
      <category>stat.CO</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric Yanchenko</dc:creator>
    </item>
    <item>
      <title>Network community detection via neural embeddings</title>
      <link>https://arxiv.org/abs/2306.13400</link>
      <description>arXiv:2306.13400v2 Announce Type: replace-cross 
Abstract: Recent advances in machine learning research have produced powerful neural graph embedding methods, which learn useful, low-dimensional vector representations of network data. These neural methods for graph embedding excel in graph machine learning tasks and are now widely adopted. However, how and why these methods work -- particularly how network structure gets encoded in the embedding -- remain largely unexplained. Here, we show that node2vec -- shallow, linear neural network -- encodes communities into separable clusters better than random partitioning down to the information-theoretic detectability limit for the stochastic block models. We show that this is due to the equivalence between the embedding learned by node2vec and the spectral embedding via the eigenvectors of the symmetric normalized Laplacian matrix. Numerical simulations demonstrate that node2vec is capable of learning communities on sparse graphs generated by the stochastic blockmodel, as well as on sparse degree-heterogeneous networks. Our results highlight the features of graph neural networks that enable them to separate communities in embedding space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.13400v2</guid>
      <category>physics.soc-ph</category>
      <category>cs.SI</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sadamori Kojaku, Filippo Radicchi, Yong-Yeol Ahn, Santo Fortunato</dc:creator>
    </item>
    <item>
      <title>Decentralized Social Networks and the Future of Free Speech Online</title>
      <link>https://arxiv.org/abs/2406.06934</link>
      <description>arXiv:2406.06934v2 Announce Type: replace-cross 
Abstract: Decentralized social networks like Mastodon and BlueSky are trending topics that have drawn much attention and discussion in recent years. By devolving powers from the central node to the end users, decentralized social networks aim to cure existing pathologies on the centralized platforms and have been viewed by many as the future of the Internet. This article critically and systematically assesses the decentralization project's prospect for communications online. It uses normative theories of free speech to examine whether and how the decentralization design could facilitate users' freedom of expression online. The analysis shows that both promises and pitfalls exist, highlighting the importance of value-based design in this area. Two most salient issues for the design of the decentralized networks are: how to balance the decentralization ideal with constant needs of centralization on the network, and how to empower users to make them truly capable of exercising their control. The article then uses some design examples, such as the shared blocklist and the opt-in search function, to illustrate the value considerations underlying the design choices. Some tentative proposals for law and policy interventions are offered to better facilitate the design of the new network. Rather than providing clear answers, the article seeks to map the value implications of the design choices, highlight the stakes, and point directions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06934v2</guid>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <category>cs.NI</category>
      <category>cs.SI</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.clsr.2024.106059</arxiv:DOI>
      <arxiv:journal_reference>Computer Law &amp; Security Review 55, 106059 (2024)</arxiv:journal_reference>
      <dc:creator>Tao Huang</dc:creator>
    </item>
    <item>
      <title>RED-CT: A Systems Design Methodology for Using LLM-labeled Data to Train and Deploy Edge Classifiers for Computational Social Science</title>
      <link>https://arxiv.org/abs/2408.08217</link>
      <description>arXiv:2408.08217v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have enhanced our ability to rapidly analyze and classify unstructured natural language data. However, concerns regarding cost, network limitations, and security constraints have posed challenges for their integration into work processes. In this study, we adopt a systems design approach to employing LLMs as imperfect data annotators for downstream supervised learning tasks, introducing novel system intervention measures aimed at improving classification performance. Our methodology outperforms LLM-generated labels in seven of eight tests, demonstrating an effective strategy for incorporating LLMs into the design and deployment of specialized, supervised learning models present in many industry use cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08217v2</guid>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Farr, Nico Manzonelli, Iain Cruickshank, Jevin West</dc:creator>
    </item>
    <item>
      <title>DuoGNN: Topology-aware Graph Neural Network with Homophily and Heterophily Interaction-Decoupling</title>
      <link>https://arxiv.org/abs/2409.19616</link>
      <description>arXiv:2409.19616v2 Announce Type: replace-cross 
Abstract: Graph Neural Networks (GNNs) have proven effective in various medical imaging applications, such as automated disease diagnosis. However, due to the local neighborhood aggregation paradigm in message passing which characterizes these models, they inherently suffer from two fundamental limitations: first, indistinguishable node embeddings due to heterophilic node aggregation (known as over-smoothing), and second, impaired message passing due to aggregation through graph bottlenecks (known as over-squashing). These challenges hinder the model expressiveness and prevent us from using deeper models to capture long-range node dependencies within the graph. Popular solutions in the literature are either too expensive to process large graphs due to high time complexity or do not generalize across all graph topologies. To address these limitations, we propose DuoGNN, a scalable and generalizable architecture which leverages topology to decouple homophilic and heterophilic edges and capture both short-range and long-range interactions. Our three core contributions introduce (i) a topological edge-filtering algorithm which extracts homophilic interactions and enables the model to generalize well for any graph topology, (ii) a heterophilic graph condensation technique which extracts heterophilic interactions and ensures scalability, and (iii) a dual homophilic and heterophilic aggregation pipeline which prevents over-smoothing and over-squashing during the message passing. We benchmark our model on medical and non-medical node classification datasets and compare it with its variants, showing consistent improvements across all tasks. Our DuoGNN code is available at https://github.com/basiralab/DuoGNN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19616v2</guid>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>K. Mancini, I. Rekik</dc:creator>
    </item>
    <item>
      <title>LLM Chain Ensembles for Scalable and Accurate Data Annotation</title>
      <link>https://arxiv.org/abs/2410.13006</link>
      <description>arXiv:2410.13006v2 Announce Type: replace-cross 
Abstract: The ability of large language models (LLMs) to perform zero-shot classification makes them viable solutions for data annotation in rapidly evolving domains where quality labeled data is often scarce and costly to obtain. However, the large-scale deployment of LLMs can be prohibitively expensive. This paper introduces an LLM chain ensemble methodology that aligns multiple LLMs in a sequence, routing data subsets to subsequent models based on classification uncertainty. This approach leverages the strengths of individual LLMs within a broader system, allowing each model to handle data points where it exhibits the highest confidence, while forwarding more complex cases to potentially more robust models. Our results show that the chain ensemble method often exceeds the performance of the best individual model in the chain and achieves substantial cost savings, making LLM chain ensembles a practical and efficient solution for large-scale data annotation challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13006v2</guid>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Farr, Nico Manzonelli, Iain Cruickshank, Kate Starbird, Jevin West</dc:creator>
    </item>
    <item>
      <title>Overcoming Non-Submodularity: Constant Approximation for Network Immunization</title>
      <link>https://arxiv.org/abs/2410.19205</link>
      <description>arXiv:2410.19205v2 Announce Type: replace-cross 
Abstract: Given a network with an ongoing epidemic, the network immunization problem seeks to identify a fixed number of nodes to immunize in order to maximize the number of infections prevented. One of the fundamental computational challenges in network immunization is that the objective function is generally neither submodular nor supermodular. As a result, no efficient algorithm is known to consistently find a solution with a constant approximation guarantee. Traditionally, this problem is addressed using proxy objectives, which offer better approximation properties. However, converting to these indirect optimizations often introduces losses in effectiveness. In this paper, we overcome these fundamental barriers by utilizing the underlying stochastic structures of the diffusion process. Similar to the traditional influence objective, the immunization objective is an expectation that can be expressed as the sum of objectives over deterministic instances. However, unlike the former, some of these terms are not submodular. The key step is proving that this sum has a bounded deviation from submodularity, thereby enabling the greedy algorithm to achieve constant factor approximation. We show that this approximation still stands considering a variety of immunization settings and spread models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19205v2</guid>
      <category>cs.DS</category>
      <category>cs.SI</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ajitesh Srivastava, Shang-Hua Teng</dc:creator>
    </item>
  </channel>
</rss>
