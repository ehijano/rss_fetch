<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 20 Sep 2024 04:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Verification with Common Knowledge of Rationality for Graph Games</title>
      <link>https://arxiv.org/abs/2409.12461</link>
      <description>arXiv:2409.12461v1 Announce Type: new 
Abstract: Realizability asks whether there exists a program satisfying its specification. In this problem, we assume that each agent has her own objective and behaves rationally to satisfy her objective. Traditionally, the rationality of agents is modeled by a Nash equilibrium (NE), where each agent has no incentive to change her strategy because she cannot satisfy her objective by changing her strategy alone. However, an NE is not always an appropriate notion for the rationality of agents because the condition of an NE is too strong; each agent is assumed to know strategies of the other agents completely. In this paper, we use an epistemic model to define common knowledge of rationality of all agents (CKR). We define the verification problem as a variant of the realizability problem, based on CKR, instead of NE. We then analyze the complexity of the verification problems for the class of positional strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12461v1</guid>
      <category>cs.GT</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rindo Nakanishi, Yoshiaki Takata, Hiroyuki Seki</dc:creator>
    </item>
    <item>
      <title>Estimating the number of reachable positions in Minishogi</title>
      <link>https://arxiv.org/abs/2409.00129</link>
      <description>arXiv:2409.00129v2 Announce Type: cross 
Abstract: To investigate the feasibility of strongly solving Minishogi (Gogo Shogi), it is necessary to know the number of its reachable positions from the initial position. However, there currently remains a significant gap between the lower and upper bounds of the value, since checking the legality of a Minishogi position is difficult. In this paper, the authors estimate the number of reachable positions by generating candidate positions using uniform random sampling and measuring the proportion of those reachable by a series of legal moves from the initial position. The experimental results reveal that the number of reachable Minishogi positions is approximately $2.38\times 10^{18}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00129v2</guid>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sotaro Ishii, Tetsuro Tanaka</dc:creator>
    </item>
    <item>
      <title>Autoformalization of Game Descriptions using Large Language Models</title>
      <link>https://arxiv.org/abs/2409.12300</link>
      <description>arXiv:2409.12300v1 Announce Type: cross 
Abstract: Game theory is a powerful framework for reasoning about strategic interactions, with applications in domains ranging from day-to-day life to international politics. However, applying formal reasoning tools in such contexts is challenging, as these scenarios are often expressed in natural language. To address this, we introduce a framework for the autoformalization of game-theoretic scenarios, which translates natural language descriptions into formal logic representations suitable for formal solvers. Our approach utilizes one-shot prompting and a solver that provides feedback on syntactic correctness to allow LLMs to refine the code. We evaluate the framework using GPT-4o and a dataset of natural language problem descriptions, achieving 98% syntactic correctness and 88% semantic correctness. These results show the potential of LLMs to bridge the gap between real-life strategic interactions and formal reasoning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12300v1</guid>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Agnieszka Mensfelt, Kostas Stathis, Vince Trencsenyi</dc:creator>
    </item>
    <item>
      <title>Stochastic Prediction Equilibrium for Dynamic Traffic Assignment</title>
      <link>https://arxiv.org/abs/2409.12650</link>
      <description>arXiv:2409.12650v1 Announce Type: cross 
Abstract: Stochastic effects significantly influence the dynamics of traffic flows. Many dynamic traffic assignment (DTA) models attempt to capture these effects by prescribing a specific ratio that determines how flow splits across different routes based on the routes' costs. In this paper, we propose a new framework for DTA that incorporates the interplay between the routing decisions of each single traffic participant, the stochastic nature of predicting the future state of the network, and the physical flow dynamics. Our framework consists of an edge loading operator modeling the physical flow propagation and a routing operator modeling the routing behavior of traffic participants. The routing operator is assumed to be set-valued and capable to model complex (deterministic) equilibrium conditions as well as stochastic equilibrium conditions assuming that measurements for predicting traffic are noisy. As our main results, we derive several quite general equilibrium existence and uniqueness results which not only subsume known results from the literature but also lead to new results. Specifically, for the new stochastic prediction equilibrium, we show existence and uniqueness under natural assumptions on the probability distribution over the predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12650v1</guid>
      <category>math.OC</category>
      <category>cs.GT</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lukas Graf, Tobias Harks, Michael Markl</dc:creator>
    </item>
    <item>
      <title>Evolution of Preferences in Multiple Populations</title>
      <link>https://arxiv.org/abs/1808.02451</link>
      <description>arXiv:1808.02451v4 Announce Type: replace 
Abstract: We study the evolution of preferences in multi-population settings that allow matches across distinct populations. Each individual has subjective preferences over potential outcomes, and chooses a best response based on his preferences and the information about the opponents' preferences. Individuals' realized fitnesses are given by material payoff functions. Following Dekel et al. (2007), we assume that individuals observe their opponents' preferences with probability $p$. We first derive necessary and sufficient conditions for stability for $p=1$ and $p=0$, and then check the robustness of our results against small perturbations on observability for the case of pure-strategy outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:1808.02451v4</guid>
      <category>cs.GT</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s00182-023-00869-w</arxiv:DOI>
      <arxiv:journal_reference>International Journal of Game Theory 53 (2024) 211-259</arxiv:journal_reference>
      <dc:creator>Yu-Sung Tu, Wei-Torng Juang</dc:creator>
    </item>
    <item>
      <title>Machine-Learned Prediction Equilibrium for Dynamic Traffic Assignment</title>
      <link>https://arxiv.org/abs/2109.06713</link>
      <description>arXiv:2109.06713v2 Announce Type: replace 
Abstract: We study a dynamic traffic assignment model, where agents base their instantaneous routing decisions on real-time delay predictions. We formulate a mathematically concise model and define dynamic prediction equilibrium (DPE) in which no agent can at any point during their journey improve their predicted travel time by switching to a different route. We demonstrate the versatility of our framework by showing that it subsumes the well-known full information and instantaneous information models, in addition to admitting further realistic predictors as special cases. We then proceed to derive properties of the predictors that ensure a dynamic prediction equilibrium exists. Additionally, we define $\varepsilon$-approximate DPE wherein no agent can improve their predicted travel time by more than $\varepsilon$ and provide further conditions of the predictors under which such an approximate equilibrium can be computed. Finally, we complement our theoretical analysis by an experimental study, in which we systematically compare the induced average travel times of different predictors, including two machine-learning based models trained on data gained from previously computed approximate equilibrium flows, both on synthetic and real world road networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2109.06713v2</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lukas Graf, Tobias Harks, Kostas Kollias, Michael Markl</dc:creator>
    </item>
    <item>
      <title>Incentive Allocation in Vertical Federated Learning Based on Bankruptcy Problem</title>
      <link>https://arxiv.org/abs/2307.03515</link>
      <description>arXiv:2307.03515v2 Announce Type: replace-cross 
Abstract: Vertical federated learning (VFL) is a promising approach for collaboratively training machine learning models using private data partitioned vertically across different parties. Ideally in a VFL setting, the active party (party possessing features of samples with labels) benefits by improving its machine learning model through collaboration with some passive parties (parties possessing additional features of the same samples without labels) in a privacy preserving manner. However, motivating passive parties to participate in VFL can be challenging. In this paper, we focus on the problem of allocating incentives to the passive parties by the active party based on their contributions to the VFL process. We address this by formulating the incentive allocation problem as a bankruptcy game, a concept from cooperative game theory. Using the Talmudic division rule, which leads to the Nucleolus as its solution, we ensure a fair distribution of incentives. We evaluate our proposed method on synthetic and real-world datasets and show that it ensures fairness and stability in incentive allocation among passive parties who contribute their data to the federated model. Additionally, we compare our method to the existing solution of calculating Shapley values and show that our approach provides a more efficient solution with fewer computations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.03515v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.GT</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Afsana Khan, Marijn ten Thij, Frank Thuijsman, Anna Wilbik</dc:creator>
    </item>
    <item>
      <title>Nicer Than Humans: How do Large Language Models Behave in the Prisoner's Dilemma?</title>
      <link>https://arxiv.org/abs/2406.13605</link>
      <description>arXiv:2406.13605v2 Announce Type: replace-cross 
Abstract: The behavior of Large Language Models (LLMs) as artificial social agents is largely unexplored, and we still lack extensive evidence of how these agents react to simple social stimuli. Testing the behavior of AI agents in classic Game Theory experiments provides a promising theoretical framework for evaluating the norms and values of these agents in archetypal social situations. In this work, we investigate the cooperative behavior of three LLMs (Llama2, Llama3, and GPT3.5) when playing the Iterated Prisoner's Dilemma against random adversaries displaying various levels of hostility. We introduce a systematic methodology to evaluate an LLM's comprehension of the game rules and its capability to parse historical gameplay logs for decision-making. We conducted simulations of games lasting for 100 rounds and analyzed the LLMs' decisions in terms of dimensions defined in the behavioral economics literature. We find that all models tend not to initiate defection but act cautiously, favoring cooperation over defection only when the opponent's defection rate is low. Overall, LLMs behave at least as cooperatively as the typical human player, although our results indicate some substantial differences among models. In particular, Llama2 and GPT3.5 are more cooperative than humans, and especially forgiving and non-retaliatory for opponent defection rates below 30%. More similar to humans, Llama3 exhibits consistently uncooperative and exploitative behavior unless the opponent always cooperates. Our systematic approach to the study of LLMs in game theoretical scenarios is a step towards using these simulations to inform practices of LLM auditing and alignment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13605v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>physics.soc-ph</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicol\'o Fontana, Francesco Pierri, Luca Maria Aiello</dc:creator>
    </item>
    <item>
      <title>Variance reduction in Texas hold'em and in video poker</title>
      <link>https://arxiv.org/abs/2409.03607</link>
      <description>arXiv:2409.03607v2 Announce Type: replace-cross 
Abstract: In Texas hold'em, after an all-in bet is made and called before the flop, the turn, or the river, the two players sometimes agree to run it $n$ times, meaning that the remaining five, two, or one cards are dealt out not just once but $n$ times successively without replacement, with $1/n$ of the pot attached to each run. In $n$-play video poker, five cards are dealt exactly as in the conventional single-play game. After the player chooses which cards to hold, new cards are drawn to replace the discards, not just once but $n$ times independently, with $1/n$ of the bet attached to each draw. In both scenarios the players are attempting to reduce the variance of the return without changing the mean. We quantify the extent to which the variance is reduced.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03607v2</guid>
      <category>math.PR</category>
      <category>cs.GT</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stewart N. Ethier</dc:creator>
    </item>
    <item>
      <title>Instigating Cooperation among LLM Agents Using Adaptive Information Modulation</title>
      <link>https://arxiv.org/abs/2409.10372</link>
      <description>arXiv:2409.10372v2 Announce Type: replace-cross 
Abstract: This paper introduces a novel framework combining LLM agents as proxies for human strategic behavior with reinforcement learning (RL) to engage these agents in evolving strategic interactions within team environments. Our approach extends traditional agent-based simulations by using strategic LLM agents (SLA) and introducing dynamic and adaptive governance through a pro-social promoting RL agent (PPA) that modulates information access across agents in a network, optimizing social welfare and promoting pro-social behavior. Through validation in iterative games, including the prisoner dilemma, we demonstrate that SLA agents exhibit nuanced strategic adaptations. The PPA agent effectively learns to adjust information transparency, resulting in enhanced cooperation rates. This framework offers significant insights into AI-mediated social dynamics, contributing to the deployment of AI in real-world team settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10372v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.GT</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiliang Chen, Sepehr Ilami, Nunzio Lore, Babak Heydari</dc:creator>
    </item>
    <item>
      <title>Opponent Shaping for Antibody Development</title>
      <link>https://arxiv.org/abs/2409.10588</link>
      <description>arXiv:2409.10588v2 Announce Type: replace-cross 
Abstract: Anti-viral therapies are typically designed or evolved towards the current strains of a virus. In learning terms, this corresponds to a myopic best response, i.e., not considering the possible adaptive moves of the opponent. However, therapy-induced selective pressures act on viral antigens to drive the emergence of mutated strains, against which initial therapies have reduced efficacy. To motivate our work, we consider antibody designs that target not only the current viral strains but also the wide range of possible future variants that the virus might evolve into under the evolutionary pressure exerted by said antibodies. Building on a computational model of binding between antibodies and viral antigens (the Absolut! framework), we design and implement a genetic simulation of the viral evolutionary escape. Crucially, this allows our antibody optimisation algorithm to consider and influence the entire escape curve of the virus, i.e. to guide (or ''shape'') the viral evolution. This is inspired by opponent shaping which, in general-sum learning, accounts for the adaptation of the co-player rather than playing a myopic best response. Hence we call the optimised antibodies shapers. Within our simulations, we demonstrate that our shapers target both current and simulated future viral variants, outperforming the antibodies chosen in a myopic way. Furthermore, we show that shapers exert specific evolutionary pressure on the virus compared to myopic antibodies. Altogether, shapers modify the evolutionary trajectories of viral strains and minimise the viral escape compared to their myopic counterparts. While this is a simple model, we hope that our proposed paradigm will enable the discovery of better long-lived vaccines and antibody therapies in the future, enabled by rapid advancements in the capabilities of simulation tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10588v2</guid>
      <category>q-bio.PE</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sebastian Towers, Aleksandra Kalisz, Philippe A. Robert, Alicia Higueruelo, Francesca Vianello, Ming-Han Chloe Tsai, Harrison Steel, Jakob N. Foerster</dc:creator>
    </item>
  </channel>
</rss>
