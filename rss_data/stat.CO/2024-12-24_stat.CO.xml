<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Dec 2024 07:13:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Amortising Variational Bayesian Inference over prior hyperparameters with a Normalising Flow</title>
      <link>https://arxiv.org/abs/2412.16419</link>
      <description>arXiv:2412.16419v1 Announce Type: new 
Abstract: In Bayesian inference prior hyperparameters are chosen subjectively or estimated using empirical Bayes methods. Generalised Bayesian Inference also has hyperparameters (the learning rate, and parameters of the loss). As part of the Generalised-Bayes workflow it is necessary to check sensitivity to the choice of hyperparameters, but running MCMC or fitting a variational approximation at each hyperparameter setting is impractical when there are more than a few hyperparameters. Simulation Based Inference has been used to amortise over data and hyperparameters and can be useful for Bayesian problems. However, there is no Simulation Based Inference for Generalised Bayes posteriors, as there is no generative model for the data. Working with a variational family parameterised by a normalising flow, we show how to fit a variational Generalised Bayes posterior, amortised over all hyperparameters. This may be sampled very efficiently at different hyperparameter values without refitting, and supports efficient robustness checks and hyperparameter selection. We show that there exist amortised normalising-flow architectures which are universal approximators. We test our approach on a relatively large-scale application of Generalised Bayesian Inference. The code is available online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16419v1</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laura Battaglia, Geoff Nicholls</dc:creator>
    </item>
    <item>
      <title>A Proximal Newton Adaptive Importance Sampler</title>
      <link>https://arxiv.org/abs/2412.16558</link>
      <description>arXiv:2412.16558v1 Announce Type: new 
Abstract: Adaptive importance sampling (AIS) algorithms are a rising methodology in signal processing, statistics, and machine learning. An effective adaptation of the proposals is key for the success of AIS. Recent works have shown that gradient information about the involved target density can greatly boost performance, but its applicability is restricted to differentiable targets. In this paper, we propose a proximal Newton adaptive importance sampler, an algorithm for estimating expectations with respect to non-smooth target distributions. We utilize a scaled Newton proximal gradient to adapt proposal distributions, obtaining efficient and optimized moves even when the target is not a differentiable density. We demonstrate the utility of the algorithm on two scenarios, either involving convex constraints or non-smooth sparse priors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16558v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>V\'ictor Elvira, \'Emilie Chouzenoux, O. Deniz Akyildiz</dc:creator>
    </item>
    <item>
      <title>Direct Inversion for the Squared Bessel Process and Applications</title>
      <link>https://arxiv.org/abs/2412.16655</link>
      <description>arXiv:2412.16655v1 Announce Type: new 
Abstract: In this paper we derive a new direct inversion method to simulate squared Bessel processes. Since the transition probability of these processes can be represented by a non-central chi-square distribution, we construct an efficient and accurate algorithm to simulate non-central chi-square variables. In this method, the dimension of the squared Bessel process, equivalently the degrees of freedom of the chi-square distribution, is treated as a variable. We therefore use a two-dimensional Chebyshev expansion to approximate the inverse function of the central chi-square distribution with one variable being the degrees of freedom. The method is accurate and efficient for any value of degrees of freedom including the computationally challenging case of small values. One advantage of the method is that noncentral chi-square samples can be generated for a whole range of values of degrees of freedom using the same Chebyshev coefficients. The squared Bessel process is a building block for the well-known Cox-Ingersoll-Ross (CIR) processes, which can be generated from squared Bessel processes through time change and linear transformation. Our direct inversion method thus allows the efficient and accurate simulation of these processes, which are used as models in a wide variety of applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16655v1</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>q-fin.CP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon J. A. Malham, Anke Wiese, Yifan Xu</dc:creator>
    </item>
    <item>
      <title>MOODE: An R Package for Multi-Objective Optimal Design of Experiments</title>
      <link>https://arxiv.org/abs/2412.17158</link>
      <description>arXiv:2412.17158v1 Announce Type: new 
Abstract: We describe the R package MOODE and demonstrate its use to find multi-objective optimal experimental designs. Multi-Objective Optimal Design of Experiments (MOODE) targets the experimental objectives directly, ensuring that the full set of research questions is answered as economically as possible. In particular, individual criteria aimed at optimizing inference are combined with lack-of-fit and MSE-based components in compound optimality criteria to target multiple and competing objectives reflecting the priorities and aims of the experimentation. The package implements either a point exchange or coordinate exchange algorithm as appropriate to find nearly optimal designs. We demonstrate the functionality of MOODE through the application of the methodology to two case studies of varying complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17158v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vasiliki Koutra, Olga Egorova, Steven G. Gilmour, Luzia A. Trinca</dc:creator>
    </item>
    <item>
      <title>HDTSA: An R package for high-dimensional time series analysis</title>
      <link>https://arxiv.org/abs/2412.17341</link>
      <description>arXiv:2412.17341v1 Announce Type: new 
Abstract: High-dimensional time series analysis has become increasingly important in fields such as finance, economics, and biology. The two primary tasks for high-dimensional time series analysis are modeling and statistical inference, which aim to capture the underlying dynamic structure and investigate valuable information in the data. This paper presents the HDTSA package for R, which provides a general framework for analyzing high-dimensional time series data. This package includes four dimension reduction methods for modeling: factor models, principal component analysis, CP-decomposition, and cointegration analysis. It also implements two recently proposed white noise test and martingale difference test in high-dimensional scenario for statistical inference. The methods provided in this package can help users to analyze high-dimensional time series data and make reliable predictions. To improve computational efficiency, the HDTSA package integrates C++ through the Rcpp package. We illustrate the functions of the HDTSA package using simulated examples and real-world applications from finance and economics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17341v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinyuan Chang, Jing He, Chen Lin, Qiwei Yao</dc:creator>
    </item>
    <item>
      <title>Distributed Estimation and Gap-Free Analysis of Canonical Correlations</title>
      <link>https://arxiv.org/abs/2412.17792</link>
      <description>arXiv:2412.17792v1 Announce Type: new 
Abstract: Massive data analysis calls for distributed algorithms and theories. We design a multi-round distributed algorithm for canonical correlation analysis. We construct principal directions through the convex formulation of canonical correlation analysis and use the shift-and-invert preconditioning iteration to expedite the convergence rate. This distributed algorithm is communication-efficient. The resultant estimate achieves the same convergence rate as if all observations were pooled together, but does not impose stringent restrictions on the number of machines. We take a gap-free analysis to bypass the widely used yet unrealistic assumption of an explicit gap between the successive canonical correlations in the canonical correlation analysis. Extensive simulations and applications to three benchmark image data are conducted to demonstrate the empirical performance of our proposed algorithms and theories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17792v1</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Canyi Chen, Liping Zhu</dc:creator>
    </item>
    <item>
      <title>Collision-based Dynamics for Multi-Marginal Optimal Transport</title>
      <link>https://arxiv.org/abs/2412.16385</link>
      <description>arXiv:2412.16385v1 Announce Type: cross 
Abstract: Inspired by the Boltzmann kinetics, we propose a collision-based dynamics with a Monte Carlo solution algorithm that approximates the solution of the multi-marginal optimal transport problem via randomized pairwise swapping of sample indices. The computational complexity and memory usage of the proposed method scale linearly with the number of samples, making it highly attractive for high-dimensional settings. In several examples, we demonstrate the efficiency of the proposed method compared to the state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16385v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohsen Sadr, Hossein Gorji</dc:creator>
    </item>
    <item>
      <title>Transport Quasi-Monte Carlo</title>
      <link>https://arxiv.org/abs/2412.16416</link>
      <description>arXiv:2412.16416v1 Announce Type: cross 
Abstract: Quasi-Monte Carlo (QMC) is a powerful method for evaluating high-dimensional integrals. However, its use is typically limited to distributions where direct sampling is straightforward, such as the uniform distribution on the unit hypercube or the Gaussian distribution. For general target distributions with potentially unnormalized densities, leveraging the low-discrepancy property of QMC to improve accuracy remains challenging. We propose training a transport map to push forward the uniform distribution on the unit hypercube to approximate the target distribution. Inspired by normalizing flows, the transport map is constructed as a composition of simple, invertible transformations. To ensure that RQMC achieves its superior error rate, the transport map must satisfy specific regularity conditions. We introduce a flexible parametrization for the transport map that not only meets these conditions but is also expressive enough to model complex distributions. Our theoretical analysis establishes that the proposed transport QMC estimator achieves faster convergence rates than standard Monte Carlo, under mild and easily verifiable growth conditions on the integrand. Numerical experiments confirm the theoretical results, demonstrating the effectiveness of the proposed method in Bayesian inference tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16416v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sifan Liu</dc:creator>
    </item>
    <item>
      <title>Empirical evaluation of normalizing flows in Markov Chain Monte Carlo</title>
      <link>https://arxiv.org/abs/2412.17136</link>
      <description>arXiv:2412.17136v1 Announce Type: cross 
Abstract: Recent advances in MCMC use normalizing flows to precondition target distributions and enable jumps to distant regions. However, there is currently no systematic comparison of different normalizing flow architectures for MCMC. As such, many works choose simple flow architectures that are readily available and do not consider other models. Guidelines for choosing an appropriate architecture would reduce analysis time for practitioners and motivate researchers to take the recommended models as foundations to be improved. We provide the first such guideline by extensively evaluating many normalizing flow architectures on various flow-based MCMC methods and target distributions. When the target density gradient is available, we show that flow-based MCMC outperforms classic MCMC for suitable NF architecture choices with minor hyperparameter tuning. When the gradient is unavailable, flow-based MCMC wins with off-the-shelf architectures. We find contractive residual flows to be the best general-purpose models with relatively low sensitivity to hyperparameter choice. We also provide various insights into normalizing flow behavior within MCMC when varying their hyperparameters, properties of target distributions, and the overall computational budget.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17136v1</guid>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Nabergoj, Erik \v{S}trumbelj</dc:creator>
    </item>
    <item>
      <title>Spatial function-on-function regression</title>
      <link>https://arxiv.org/abs/2412.17327</link>
      <description>arXiv:2412.17327v1 Announce Type: cross 
Abstract: We introduce a spatial function-on-function regression model to capture spatial dependencies in functional data by integrating spatial autoregressive techniques with functional principal component analysis. The proposed model addresses a critical gap in functional regression by enabling the analysis of functional responses influenced by spatially correlated functional predictors, a common scenario in fields such as environmental sciences, epidemiology, and socio-economic studies. The model employs a spatial functional principal component decomposition on the response and a classical functional principal component decomposition on the predictor, transforming the functional data into a finite-dimensional multivariate spatial autoregressive framework. This transformation allows efficient estimation and robust handling of spatial dependencies through least squares methods. In a series of extensive simulations, the proposed model consistently demonstrated superior performance in estimating both spatial autocorrelation and regression coefficient functions compared to some favorably existing traditional approaches, particularly under moderate to strong spatial effects. Application of the proposed model to Brazilian COVID-19 data further underscored its practical utility, revealing critical spatial patterns in confirmed cases and death rates that align with known geographic and social interactions. An R package provides a comprehensive implementation of the proposed estimation method, offering a user-friendly and efficient tool for researchers and practitioners to apply the methodology in real-world scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17327v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ufuk Beyaztas, Han Lin Shang, Gizel Bakicierler Sezer, Abhijit Mandal, Roger S. Zoh, Carmen D. Tekwe</dc:creator>
    </item>
    <item>
      <title>Digesting Gibbs Sampling Using R</title>
      <link>https://arxiv.org/abs/2410.14073</link>
      <description>arXiv:2410.14073v2 Announce Type: replace 
Abstract: This work aims to provide an environment for all users who are beginner in the context of the statistical simulation approaches. These techniques are known as the Monte Carlo methods as a whole nowadays. Indeed, the Monte Carlo, as a statistical simulation technique, itself involves the Markov chain Monte Carlo that attracts the attention of researchers from a wide variety of study fields. One may see the Markov chain Monte Carlo as statistical simulation approaches that work based on the iterative algorithms and so the others that are not based on iterative algorithm are the Monte Carlo approaches. We would recommend the reader(s) to learn the elementary undergraduate courses in calculus, probability, and statistics before studying or applying this report for practical purposes. The required topics may include, but not limited to, concept of mathematical function, limit, derivative, partial derivative, simple integrals, probability axioms, discrete and continuous random variables, probability distributions, concept of central tendency and variance, multivariate probability distributions, functions of random variables, and the central limit theorem (CLT).</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14073v2</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mahdi Teimouri</dc:creator>
    </item>
    <item>
      <title>Variational Sequential Optimal Experimental Design using Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2306.10430</link>
      <description>arXiv:2306.10430v2 Announce Type: replace-cross 
Abstract: We present variational sequential optimal experimental design (vsOED), a novel method for optimally designing a finite sequence of experiments within a Bayesian framework with information-theoretic criteria. vsOED employs a one-point reward formulation with variational posterior approximations, providing a provable lower bound to the expected information gain. Numerical methods are developed following an actor-critic reinforcement learning approach, including derivation and estimation of variational and policy gradients to optimize the design policy, and posterior approximation using Gaussian mixture models and normalizing flows. vsOED accommodates nuisance parameters, implicit likelihoods, and multiple candidate models, while supporting flexible design criteria that can target designs for model discrimination, parameter inference, goal-oriented prediction, and their weighted combinations. We demonstrate vsOED across various engineering and science applications, illustrating its superior sample efficiency compared to existing sequential experimental design algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.10430v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wanggang Shen, Jiayuan Dong, Xun Huan</dc:creator>
    </item>
    <item>
      <title>A Semi-supervised CART Model for Covariate Shift</title>
      <link>https://arxiv.org/abs/2410.20978</link>
      <description>arXiv:2410.20978v2 Announce Type: replace-cross 
Abstract: Machine learning models used in medical applications often face challenges due to the covariate shift, which occurs when there are discrepancies between the distributions of training and target data. This can lead to decreased predictive accuracy, especially with unknown outcomes in the target data. This paper introduces a semi-supervised classification and regression tree (CART) that uses importance weighting to address these distribution discrepancies. Our method improves the predictive performance of the CART model by assigning greater weights to training samples that more accurately represent the target distribution, especially in cases of covariate shift without target outcomes. In addition to CART, we extend this weighted approach to generalized linear model trees and tree ensembles, creating a versatile framework for managing the covariate shift in complex datasets. Through simulation studies and applications to real-world medical data, we demonstrate significant improvements in predictive accuracy. These findings suggest that our weighted approach can enhance reliability in medical applications and other fields where the covariate shift poses challenges to model performance across various data distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20978v2</guid>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingyang Cai, Thomas Klausch, Mark A. van de Wiel</dc:creator>
    </item>
    <item>
      <title>A partial likelihood approach to tree-based density modeling and its application in Bayesian inference</title>
      <link>https://arxiv.org/abs/2412.11692</link>
      <description>arXiv:2412.11692v2 Announce Type: replace-cross 
Abstract: Tree-based models for probability distributions are usually specified using a predetermined, data-independent collection of candidate recursive partitions of the sample space. To characterize an unknown target density in detail over the entire sample space, candidate partitions must have the capacity to expand deeply into all areas of the sample space with potential non-zero sampling probability. Such an expansive system of partitions often incurs prohibitive computational costs and makes inference prone to overfitting, especially in regions with little probability mass. Existing models typically make a compromise and rely on relatively shallow trees. This hampers one of the most desirable features of trees, their ability to characterize local features, and results in reduced statistical efficiency. Traditional wisdom suggests that this compromise is inevitable to ensure coherent likelihood-based reasoning, as a data-dependent partition system that allows deeper expansion only in regions with more observations would induce double dipping of the data and thus lead to inconsistent inference. We propose a simple strategy to restore coherency while allowing the candidate partitions to be data-dependent, using Cox's partial likelihood. This strategy parametrizes the tree-based sampling model according to the allocation of probability mass based on the observed data, and yet under appropriate specification, the resulting inference remains valid. Our partial likelihood approach is broadly applicable to existing likelihood-based methods and in particular to Bayesian inference on tree-based models. We give examples in density estimation in which the partial likelihood is endowed with existing priors on tree-based models and compare with the standard, full-likelihood approach. The results show substantial gains in estimation accuracy and computational efficiency from using the partial likelihood.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11692v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Li Ma, Benedetta Bruni</dc:creator>
    </item>
  </channel>
</rss>
