<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 28 Mar 2025 04:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A New Approach to Compositional Data Analysis using \(L^{\infty}\)-normalization with Applications to Vaginal Microbiome</title>
      <link>https://arxiv.org/abs/2503.21543</link>
      <description>arXiv:2503.21543v1 Announce Type: new 
Abstract: We introduce a novel approach to compositional data analysis based on $L^{\infty}$-normalization, addressing challenges posed by zero-rich high-throughput data. Traditional methods like Aitchison's transformations require excluding zeros, conflicting with the reality that omics datasets contain structural zeros that cannot be removed without violating inherent biological structures. Such datasets exist exclusively on the boundary of compositional space, making interior-focused approaches fundamentally misaligned.
  We present a family of $L^p$-normalizations, focusing on $L^{\infty}$-normalization due to its advantageous properties. This approach identifies compositional space with the $L^{\infty}$-simplex, represented as a union of top-dimensional faces called $L^{\infty}$-cells. Each cell consists of samples where one component's absolute abundance equals or exceeds all others, with a coordinate system identifying it with a d-dimensional unit cube.
  When applied to vaginal microbiome data, $L^{\infty}$-decomposition aligns with established Community State Types while offering advantages: each $L^{\infty}$-CST is named after its dominating component, has clear biological meaning, remains stable under sample changes, resolves cluster-based issues, and provides a coordinate system for exploring internal structure.
  We extend homogeneous coordinates through cube embedding, mapping data into a d-dimensional unit cube. These embeddings can be integrated via Cartesian product, providing unified representations from multiple perspectives. While demonstrated through microbiome studies, these methods apply to any compositional data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21543v1</guid>
      <category>stat.CO</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Pawel Gajer, Jacques Ravel</dc:creator>
    </item>
    <item>
      <title>A friendly introduction to triangular transport</title>
      <link>https://arxiv.org/abs/2503.21673</link>
      <description>arXiv:2503.21673v1 Announce Type: new 
Abstract: Decision making under uncertainty is a cross-cutting challenge in science and engineering. Most approaches to this challenge employ probabilistic representations of uncertainty. In complicated systems accessible only via data or black-box models, however, these representations are rarely known. We discuss how to characterize and manipulate such representations using triangular transport maps, which approximate any complex probability distribution as a transformation of a simple, well-understood distribution. The particular structure of triangular transport guarantees many desirable mathematical and computational properties that translate well into solving practical problems. Triangular maps are actively used for density estimation, (conditional) generative modelling, Bayesian inference, data assimilation, optimal experimental design, and related tasks. While there is ample literature on the development and theory of triangular transport methods, this manuscript provides a detailed introduction for scientists interested in employing measure transport without assuming a formal mathematical background. We build intuition for the key foundations of triangular transport, discuss many aspects of its practical implementation, and outline the frontiers of this field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21673v1</guid>
      <category>stat.CO</category>
      <category>physics.ao-ph</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Maximilian Ramgraber, Daniel Sharp, Mathieu Le Provost, Youssef Marzouk</dc:creator>
    </item>
    <item>
      <title>Scalable Expectation Estimation with Subtractive Mixture Models</title>
      <link>https://arxiv.org/abs/2503.21346</link>
      <description>arXiv:2503.21346v1 Announce Type: cross 
Abstract: Many Monte Carlo (MC) and importance sampling (IS) methods use mixture models (MMs) for their simplicity and ability to capture multimodal distributions. Recently, subtractive mixture models (SMMs), i.e. MMs with negative coefficients, have shown greater expressiveness and success in generative modeling. However, their negative parameters complicate sampling, requiring costly auto-regressive techniques or accept-reject algorithms that do not scale in high dimensions. In this work, we use the difference representation of SMMs to construct an unbiased IS estimator ($\Delta\text{Ex}$) that removes the need to sample from the SMM, enabling high-dimensional expectation estimation with SMMs. In our experiments, we show that $\Delta\text{Ex}$ can achieve comparable estimation quality to auto-regressive sampling while being considerably faster in MC estimation. Moreover, we conduct initial experiments with $\Delta\text{Ex}$ using hand-crafted proposals, gaining first insights into how to construct safe proposals for $\Delta\text{Ex}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21346v1</guid>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Lena Zellinger, Nicola Branchini, V\'ictor Elvira, Antonio Vergari</dc:creator>
    </item>
    <item>
      <title>Simulation-based assessment of a Bayesian survival model with flexible baseline hazard and time-dependent effects</title>
      <link>https://arxiv.org/abs/2503.21388</link>
      <description>arXiv:2503.21388v1 Announce Type: cross 
Abstract: There is increasing interest in flexible parametric models for the analysis of time-to-event data, yet Bayesian approaches that offer incorporation of prior knowledge remain underused. A flexible Bayesian parametric model has recently been proposed that uses M-splines to model the hazard function. We conducted a simulation study to assess the statistical performance of this model, which is implemented in the survextrap R package. Our simulation uses data generating mechanisms of realistic survival data based on two oncology clinical trials. Statistical performance is compared across a range of flexible models, varying the M-spline specification, smoothing procedure, priors, and other computational settings. We demonstrate good performance across realistic scenarios, including good fit of complex baseline hazard functions and time-dependent covariate effects. This work helps inform key considerations to guide model selection, as well as identifying appropriate default model settings in the software that should perform well in a broad range of applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21388v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iain R. Timmins, Fatemeh Torabi, Christopher H. Jackson, Paul C. Lambert, Michael J. Sweeting</dc:creator>
    </item>
    <item>
      <title>The penetrance R package for Estimation of Age Specific Risk in Family-based Studies</title>
      <link>https://arxiv.org/abs/2411.18816</link>
      <description>arXiv:2411.18816v3 Announce Type: replace 
Abstract: Reliable tools and software for penetrance (age-specific risk among those who carry a genetic variant) estimation are critical to improving clinical decision making and risk assessment for hereditary syndromes. We introduce penetrance, an open-source R package available on CRAN, to estimate age-specific penetrance using family-history pedigree data. The package employs a Bayesian estimation approach, allowing for the incorporation of prior knowledge through the specification of priors for the parameters of the carrier distribution. It also includes options to impute missing ages during the estimation process, addressing incomplete age information which is not uncommon in pedigree datasets. Our open-source software provides a flexible and user-friendly tool for researchers to estimate penetrance in complex family-based studies, facilitating improved genetic risk assessment in hereditary syndromes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18816v3</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolas Kubista, Danielle Braun, Giovanni Parmigiani</dc:creator>
    </item>
    <item>
      <title>Topological Graph Simplification Solutions to the Street Intersection Miscount Problem</title>
      <link>https://arxiv.org/abs/2407.00258</link>
      <description>arXiv:2407.00258v3 Announce Type: replace-cross 
Abstract: Street intersection counts and densities are ubiquitous measures in transport geography and planning. However, typical street network data and typical street network analysis tools can substantially overcount them. This article explains the three main reasons why this happens and presents solutions to each. It contributes algorithms to automatically simplify spatial graphs of urban street networks -- via edge simplification and node consolidation -- resulting in faster parsimonious models and more accurate network measures like intersection counts and densities, street segment lengths, and node degrees. These algorithms' information compression improves downstream graph analytics' memory and runtime efficiency, boosting analytical tractability without loss of model fidelity. Finally, this article validates these algorithms and empirically assesses intersection count biases worldwide to demonstrate the problem's widespread prevalence. Without consolidation, traditional methods would overestimate the median urban area intersection count by 14%. However, this bias varies drastically across regions, underscoring these algorithms' importance for consistent comparative empirical analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00258v3</guid>
      <category>physics.soc-ph</category>
      <category>cs.DM</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.CO</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Transactions in GIS, 2025</arxiv:journal_reference>
      <dc:creator>Geoff Boeing</dc:creator>
    </item>
    <item>
      <title>Gaussian process regression with log-linear scaling for common non-stationary kernels</title>
      <link>https://arxiv.org/abs/2407.03608</link>
      <description>arXiv:2407.03608v2 Announce Type: replace-cross 
Abstract: We introduce a fast algorithm for Gaussian process regression in low dimensions, applicable to a widely-used family of non-stationary kernels. The non-stationarity of these kernels is induced by arbitrary spatially-varying vertical and horizontal scales. In particular, any stationary kernel can be accommodated as a special case, and we focus especially on the generalization of the standard Mat\'ern kernel. Our subroutine for kernel matrix-vector multiplications scales almost optimally as $O(N\log N)$, where $N$ is the number of regression points. Like the recently developed equispaced Fourier Gaussian process (EFGP) methodology, which is applicable only to stationary kernels, our approach exploits non-uniform fast Fourier transforms (NUFFTs). We offer a complete analysis controlling the approximation error of our method, and we validate the method's practical performance with numerical experiments. In particular we demonstrate improved scalability compared to to state-of-the-art rank-structured approaches in spatial dimension $d&gt;1$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03608v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>stat.CO</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>P. Michael Kielstra, Michael Lindsey</dc:creator>
    </item>
    <item>
      <title>Learning state and proposal dynamics in state-space models using differentiable particle filters and neural networks</title>
      <link>https://arxiv.org/abs/2411.15638</link>
      <description>arXiv:2411.15638v2 Announce Type: replace-cross 
Abstract: State-space models are a popular statistical framework for analysing sequential data. Within this framework, particle filters are often used to perform inference on non-linear state-space models. We introduce a new method, StateMixNN, that uses a pair of neural networks to learn the proposal distribution and transition distribution of a particle filter. Both distributions are approximated using multivariate Gaussian mixtures. The component means and covariances of these mixtures are learnt as outputs of learned functions. Our method is trained targeting the log-likelihood, thereby requiring only the observation series, and combines the interpretability of state-space models with the flexibility and approximation power of artificial neural networks. The proposed method significantly improves recovery of the hidden state in comparison with the state-of-the-art, showing greater improvement in highly non-linear scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15638v2</guid>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Cox, Santiago Segarra, Victor Elvira</dc:creator>
    </item>
    <item>
      <title>DisSim-FinBERT: Text Simplification for Core Message Extraction in Complex Financial Texts</title>
      <link>https://arxiv.org/abs/2501.04959</link>
      <description>arXiv:2501.04959v2 Announce Type: replace-cross 
Abstract: This study proposes DisSim-FinBERT, a novel framework that integrates Discourse Simplification (DisSim) with Aspect-Based Sentiment Analysis (ABSA) to enhance sentiment prediction in complex financial texts. By simplifying intricate documents such as Federal Open Market Committee (FOMC) minutes, DisSim improves the precision of aspect identification, resulting in sentiment predictions that align more closely with economic events. The model preserves the original informational content and captures the inherent volatility of financial language, offering a more nuanced and accurate interpretation of long-form financial communications. This approach provides a practical tool for policymakers and analysts aiming to extract actionable insights from central bank narratives and other detailed economic documents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04959v2</guid>
      <category>econ.EM</category>
      <category>stat.CO</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Wonseong Kim, Christina Niklaus, Choong Lyol Lee, Siegfried Handschuh</dc:creator>
    </item>
  </channel>
</rss>
