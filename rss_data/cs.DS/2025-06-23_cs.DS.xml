<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Jun 2025 02:18:52 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Linearithmic Clean-up for Vector-Symbolic Key-Value Memory with Kroneker Rotation Products</title>
      <link>https://arxiv.org/abs/2506.15793</link>
      <description>arXiv:2506.15793v1 Announce Type: new 
Abstract: A computational bottleneck in current Vector-Symbolic Architectures (VSAs) is the ``clean-up'' step, which decodes the noisy vectors retrieved from the architecture. Clean-up typically compares noisy vectors against a ``codebook'' of prototype vectors, incurring computational complexity that is quadratic or similar. We present a new codebook representation that supports efficient clean-up, based on Kroneker products of rotation-like matrices. The resulting clean-up time complexity is linearithmic, i.e. $\mathcal{O}(N\,\text{log}\,N)$, where $N$ is the vector dimension and also the number of vectors in the codebook. Clean-up space complexity is $\mathcal{O}(N)$. Furthermore, the codebook is not stored explicitly in computer memory: It can be represented in $\mathcal{O}(\text{log}\,N)$ space, and individual vectors in the codebook can be materialized in $\mathcal{O}(N)$ time and space. At the same time, asymptotic memory capacity remains comparable to standard approaches. Computer experiments confirm these results, demonstrating several orders of magnitude more scalability than baseline VSA techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15793v1</guid>
      <category>cs.DS</category>
      <category>cs.AI</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruipeng Liu, Qinru Qiu, Simon Khan, Garrett E. Katz</dc:creator>
    </item>
    <item>
      <title>HybHuff: Lossless Compression for Hypergraphs via Entropy-Guided Huffman-Bitwise Coordination</title>
      <link>https://arxiv.org/abs/2506.15844</link>
      <description>arXiv:2506.15844v1 Announce Type: new 
Abstract: Hypergraphs provide a natural representation for many-to-many relationships in data-intensive applications, yet their scalability is often hindered by high memory consumption. While prior work has improved computational efficiency, reducing the space overhead of hypergraph representations remains a major challenge. This paper presents a hybrid compression framework for integer-based hypergraph adjacency formats, which adaptively combines Huffman encoding and bitwise encoding to exploit structural redundancy. We provide a theoretical analysis showing that an optimal encoding ratio exists between the two schemes, and introduce an empirical strategy to approximate this ratio for practical use. Experiments on real-world hypergraphs demonstrate that our method consistently outperforms standard compressors such as Zip and ZFP in compression rate by up to 2.3x with comparable decoding overhead. To assess practical utility, we integrate our framework with three common hypergraph workloads: breadth-first search, PageRank, and k-core label propagation, and show that compression incurs negligible performance loss. Extensive evaluations across four benchmark datasets confirm the efficiency and applicability of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15844v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tianyu Zhao, Dongfang Zhao, Luanzheng Guo, Nathan Tallent</dc:creator>
    </item>
    <item>
      <title>On the Efficient Discovery of Maximum $k$-Defective Biclique</title>
      <link>https://arxiv.org/abs/2506.16121</link>
      <description>arXiv:2506.16121v1 Announce Type: new 
Abstract: The problem of identifying the maximum edge biclique in bipartite graphs has attracted considerable attention in bipartite graph analysis, with numerous real-world applications such as fraud detection, community detection, and online recommendation systems. However, real-world graphs may contain noise or incomplete information, leading to overly restrictive conditions when employing the biclique model. To mitigate this, we focus on a new relaxed subgraph model, called the $k$-defective biclique, which allows for up to $k$ missing edges compared to the biclique model. We investigate the problem of finding the maximum edge $k$-defective biclique in a bipartite graph, and prove that the problem is NP-hard. To tackle this computation challenge, we propose a novel algorithm based on a new branch-and-bound framework, which achieves a worst-case time complexity of $O(m\alpha_k^n)$, where $\alpha_k &lt; 2$. We further enhance this framework by incorporating a novel pivoting technique, reducing the worst-case time complexity to $O(m\beta_k^n)$, where $\beta_k &lt; \alpha_k$. To improve the efficiency, we develop a series of optimization techniques, including graph reduction methods, novel upper bounds, and a heuristic approach. Extensive experiments on 10 large real-world datasets validate the efficiency and effectiveness of the proposed approaches. The results indicate that our algorithms consistently outperform state-of-the-art algorithms, offering up to $1000\times$ speedups across various parameter settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16121v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Donghang Cui, Ronghua Li, Qiangqiang Dai, Hongchao Qin, Guoren Wang</dc:creator>
    </item>
    <item>
      <title>Parallel batch queries on dynamic trees: algorithms and experiments</title>
      <link>https://arxiv.org/abs/2506.16477</link>
      <description>arXiv:2506.16477v1 Announce Type: new 
Abstract: Dynamic tree data structures maintain a forest while supporting insertion and deletion of edges and a broad set of queries in $O(\log n)$ time per operation. Such data structures are at the core of many modern algorithms. Recent work has extended dynamic trees so as to support batches of updates or queries so as to run in parallel, and these batch parallel dynamic trees are now used in several parallel algorithms. In this work we describe improvements to batch parallel dynamic trees, describe an implementation that incorporates these improvements, and experiments using it. The improvements includes generalizing prior work on RC (rake compress) trees to work with arbitrary degree while still supporting a rich set of queries, and describing how to support batch subtree queries, path queries, LCA queries, and nearest-marked-vertex queries in $O(k + k \log (1 + n/k))$ work and polylogarithmic span. Our implementation is the first general implementation of batch dynamic trees (supporting arbitrary degree and general queries). Our experiments include measuring the time to create the trees, varying batch sizes for updates and queries, and using the tree to implement incremental batch-parallel minimum spanning trees. To run the experiments we develop a forest generator that is parameterized to create distributions of trees of differing characteristics (e.g., degree, depth, and relative tree sizes). Our experiments show good speedup and that the algorithm performance is robust across forest characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16477v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Humza Ikram, Andrew Brady, Daniel Anderson, Guy Blelloch</dc:creator>
    </item>
    <item>
      <title>LMQ-Sketch: Lagom Multi-Query Sketch for High-Rate Online Analytics</title>
      <link>https://arxiv.org/abs/2506.16928</link>
      <description>arXiv:2506.16928v1 Announce Type: new 
Abstract: Data sketches balance resource efficiency with controllable approximations for extracting features in high-volume, high-rate data. Two important points of interest are highlighted separately in recent works; namely, to (1) answer multiple types of queries from one pass, and (2) query concurrently with updates. Several fundamental challenges arise when integrating these directions, which we tackle in this work. We investigate the trade-offs to be balanced and synthesize key ideas into LMQ-Sketch, a single, composite data sketch supporting multiple queries (frequency point queries, frequency moments F1, and F2) concurrently with updates. Our method 'Lagom' is a cornerstone of LMQ-Sketch for low-latency global querying (&lt;100 us), combining freshness, timeliness, and accuracy with a low memory footprint and high throughput (&gt;2B updates/s). We analyze and evaluate the accuracy of Lagom, which builds on a simple geometric argument and efficiently combines work distribution with synchronization for proper concurrency semantics -- monotonicity of operations and intermediate value linearizability. Comparing with state-of-the-art methods (which, as mentioned, only cover either mixed queries or concurrency), LMQ-Sketch shows highly competitive throughput, with additional accuracy guarantees and concurrency semantics, while also reducing the required memory budget by an order of magnitude. We expect the methodology to have broader impact on concurrent multi-query sketches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16928v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Hilgendorf, Marina Papatriantafilou</dc:creator>
    </item>
    <item>
      <title>When does FTP become FPT?</title>
      <link>https://arxiv.org/abs/2506.17008</link>
      <description>arXiv:2506.17008v1 Announce Type: new 
Abstract: In the problem Fault-Tolerant Path (FTP), we are given an edge-weighted directed graph G = (V, E), a subset U \subseteq E of vulnerable edges, two vertices s, t \in V, and integers k and \ell. The task is to decide whether there exists a subgraph H of G with total cost at most \ell such that, after the removal of any k vulnerable edges, H still contains an s-t-path. We study whether Fault-Tolerant Path is fixed-parameter tractable (FPT) and whether it admits a polynomial kernel under various parameterizations. Our choices of parameters include: the number of vulnerable edges in the input graph, the number of safe (i.e, invulnerable) edges in the input graph, the budget \ell, the minimum number of safe edges in any optimal solution, the minimum number of vulnerable edges in any optimal solution, the required redundancy k, and natural above- and below-guarantee parameterizations. We provide an almost complete description of the complexity landscape of FTP for these parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17008v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthias Bentert, Fedor V. Fomin, Petr A. Golovach, Laure Morelle</dc:creator>
    </item>
    <item>
      <title>Linear-Time Primitives for Algorithm Development in Graphical Causal Inference</title>
      <link>https://arxiv.org/abs/2506.15758</link>
      <description>arXiv:2506.15758v1 Announce Type: cross 
Abstract: We introduce CIfly, a framework for efficient algorithmic primitives in graphical causal inference that isolates reachability as a reusable core operation. It builds on the insight that many causal reasoning tasks can be reduced to reachability in purpose-built state-space graphs that can be constructed on the fly during traversal. We formalize a rule table schema for specifying such algorithms and prove they run in linear time. We establish CIfly as a more efficient alternative to the common primitives moralization and latent projection, which we show are computationally equivalent to Boolean matrix multiplication. Our open-source Rust implementation parses rule table text files and runs the specified CIfly algorithms providing high-performance execution accessible from Python and R. We demonstrate CIfly's utility by re-implementing a range of established causal inference tasks within the framework and by developing new algorithms for instrumental variables. These contributions position CIfly as a flexible and scalable backbone for graphical causal inference, guiding algorithm development and enabling easy and efficient deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15758v1</guid>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marcel Wien\"obst, Sebastian Weichwald, Leonard Henckel</dc:creator>
    </item>
    <item>
      <title>Advancing Stochastic 3-SAT Solvers by Dissipating Oversatisfied Constraints</title>
      <link>https://arxiv.org/abs/2506.15774</link>
      <description>arXiv:2506.15774v1 Announce Type: cross 
Abstract: We introduce and benchmark a stochastic local search heuristic for the NP-complete satisfiability problem 3-SAT that drastically outperforms existing solvers in the notoriously difficult realm of critically hard instances. Our construction is based on the crucial observation that well established previous approaches such as WalkSAT are prone to get stuck in local minima that are distinguished from true solutions by a larger number of oversatisfied combinatorial constraints. To address this issue, the proposed algorithm, coined DOCSAT, dissipates oversatisfied constraints (DOC), i.e. reduces their unfavorable abundance so as to render them critical. We analyze and benchmark our algorithm on a randomly generated sample of hard but satisfiable 3-SAT instances with varying problem sizes up to N=15000. Quite remarkably, we find that DOCSAT outperforms both WalkSAT and other well known algorithms including the complete solver Kissat, even when comparing its ability to solve the hardest quintile of the sample to the average performance of its competitors. The essence of DOCSAT may be seen as a way of harnessing statistical structure beyond the primary cost function of a combinatorial problem to avoid or escape local minima traps in stochastic local search, which opens avenues for generalization to other optimization problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15774v1</guid>
      <category>cs.AI</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J. Schwardt, J. C. Budich</dc:creator>
    </item>
    <item>
      <title>Local Routing on Ordered $\Theta$-graphs</title>
      <link>https://arxiv.org/abs/2506.16021</link>
      <description>arXiv:2506.16021v1 Announce Type: cross 
Abstract: The problem of locally routing on geometric networks using limited memory is extensively studied in computational geometry. We consider one particular graph, the ordered $\Theta$-graph, which is significantly harder to route on than the $\Theta$-graph, for which a number of routing algorithms are known. Currently, no local routing algorithm is known for the ordered $\Theta$-graph.
  We prove that, unfortunately, there does not exist a deterministic memoryless local routing algorithm that works on the ordered $\Theta$-graph. This motivates us to consider allowing a small amount of memory, and we present a deterministic $O(1)$-memory local routing algorithm that successfully routes from the source to the destination on the ordered $\Theta$-graph. We show that our local routing algorithm converges to the destination in $O(n)$ hops, where $n$ is the number of vertices. To the best of our knowledge, our algorithm is the first deterministic local routing algorithm that is guaranteed to reach the destination on the ordered $\Theta$-graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16021v1</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andr\'e van Renssen, Shuei Sakaguchi</dc:creator>
    </item>
    <item>
      <title>Parallel Point-to-Point Shortest Paths and Batch Queries</title>
      <link>https://arxiv.org/abs/2506.16488</link>
      <description>arXiv:2506.16488v1 Announce Type: cross 
Abstract: We propose Orionet, efficient parallel implementations of Point-to-Point Shortest Paths (PPSP) queries using bidirectional search (BiDS) and other heuristics, with an additional focus on batch PPSP queries. We present a framework for parallel PPSP built on existing single-source shortest paths (SSSP) frameworks by incorporating pruning conditions. As a result, we develop efficient parallel PPSP algorithms based on early termination, bidirectional search, A$^*$ search, and bidirectional A$^*$ all with simple and efficient implementations.
  We extend our idea to batch PPSP queries, which are widely used in real-world scenarios. We first design a simple and flexible abstraction to represent the batch so PPSP can leverage the shared information of the batch. Orionet formalizes the batch as a query graph represented by edges between queried sources and targets. In this way, we directly extended our PPSP framework to batched queries in a simple and efficient way.
  We evaluate Orionet on both single and batch PPSP queries using various graph types and distance percentiles of queried pairs, and compare it against two baselines, GraphIt and MBQ. Both of them support parallel single PPSP and A$^*$ using unidirectional search. On 14 graphs we tested, on average, our bidirectional search is 2.9$\times$ faster than GraphIt, and 6.8$\times$ faster than MBQ. Our bidirectional A$^*$ is 4.4$\times$ and 6.2$\times$ faster than the A$^*$ in GraphIt and MBQ, respectively. For batched PPSP queries, we also provide in-depth experimental evaluation, and show that Orionet provides strong performance compared to the plain solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16488v1</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3694906.3743311</arxiv:DOI>
      <dc:creator>Xiaojun Dong, Andy Li, Yan Gu, Yihan Sun</dc:creator>
    </item>
    <item>
      <title>A Distributional-Lifting Theorem for PAC Learning</title>
      <link>https://arxiv.org/abs/2506.16651</link>
      <description>arXiv:2506.16651v1 Announce Type: cross 
Abstract: The apparent difficulty of efficient distribution-free PAC learning has led to a large body of work on distribution-specific learning. Distributional assumptions facilitate the design of efficient algorithms but also limit their reach and relevance. Towards addressing this, we prove a distributional-lifting theorem: This upgrades a learner that succeeds with respect to a limited distribution family $\mathcal{D}$ to one that succeeds with respect to any distribution $D^\star$, with an efficiency overhead that scales with the complexity of expressing $D^\star$ as a mixture of distributions in $\mathcal{D}$.
  Recent work of Blanc, Lange, Malik, and Tan considered the special case of lifting uniform-distribution learners and designed a lifter that uses a conditional sample oracle for $D^\star$, a strong form of access not afforded by the standard PAC model. Their approach, which draws on ideas from semi-supervised learning, first learns $D^\star$ and then uses this information to lift.
  We show that their approach is information-theoretically intractable with access only to random examples, thereby giving formal justification for their use of the conditional sample oracle. We then take a different approach that sidesteps the need to learn $D^\star$, yielding a lifter that works in the standard PAC model and enjoys additional advantages: it works for all base distribution families, preserves the noise tolerance of learners, has better sample complexity, and is simpler.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16651v1</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guy Blanc, Jane Lange, Carmen Strassle, Li-Yang Tan</dc:creator>
    </item>
    <item>
      <title>Minimum-Weight Half-Plane Hitting Set</title>
      <link>https://arxiv.org/abs/2506.16979</link>
      <description>arXiv:2506.16979v1 Announce Type: cross 
Abstract: Given a set $P$ of $n$ weighted points and a set $H$ of $n$ half-planes in the plane, the hitting set problem is to compute a subset $P'$ of points from $P$ such that each half-plane contains at least one point from $P'$ and the total weight of the points in $P'$ is minimized. The previous best algorithm solves the problem in $O(n^{7/2}\log^2 n)$ time. In this paper, we present a new algorithm with runtime $O(n^{5/2}\log^2 n)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16979v1</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gang Liu, Haitao Wang</dc:creator>
    </item>
    <item>
      <title>Large Average Subtensor Problem: Ground-State, Algorithms, and Algorithmic Barriers</title>
      <link>https://arxiv.org/abs/2506.17118</link>
      <description>arXiv:2506.17118v1 Announce Type: cross 
Abstract: We introduce the large average subtensor problem: given an order-$p$ tensor over $\mathbb{R}^{N\times \cdots \times N}$ with i.i.d. standard normal entries and a $k\in\mathbb{N}$, algorithmically find a $k\times \cdots \times k$ subtensor with a large average entry. This generalizes the large average submatrix problem, a key model closely related to biclustering and high-dimensional data analysis, to tensors. For the submatrix case, Bhamidi, Dey, and Nobel~\cite{bhamidi2017energy} explicitly highlight the regime $k=\Theta(N)$ as an intriguing open question.
  Addressing the regime $k=\Theta(N)$ for tensors, we establish that the largest average entry concentrates around an explicit value $E_{\mathrm{max}}$, provided that the tensor order $p$ is sufficiently large. Furthermore, we prove that for any $\gamma&gt;0$ and large $p$, this model exhibits multi Overlap Gap Property ($m$-OGP) above the threshold $\gamma E_{\mathrm{max}}$. The $m$-OGP serves as a rigorous barrier for a broad class of algorithms exhibiting input stability. These results hold for both $k=\Theta(N)$ and $k=o(N)$. Moreover, for small $k$, specifically $k=o(\log^{1.5}N)$, we show that a certain polynomial-time algorithm identifies a subtensor with average entry $\frac{2\sqrt{p}}{p+1}E_{\mathrm{max}}$. In particular, the $m$-OGP is asymptotically sharp: onset of the $m$-OGP and the algorithmic threshold match as $p$ grows.
  Our results show that while the case $k=\Theta(N)$ remains open for submatrices, it can be rigorously analyzed for tensors in the large $p$ regime. This is achieved by interpreting the model as a Boolean spin glass and drawing on insights from recent advances in the Ising $p$-spin glass model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17118v1</guid>
      <category>math.ST</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abhishek Hegade K. R., Eren C. K{\i}z{\i}lda\u{g}</dc:creator>
    </item>
    <item>
      <title>It is high time we let go of the Mersenne Twister</title>
      <link>https://arxiv.org/abs/1910.06437</link>
      <description>arXiv:1910.06437v3 Announce Type: replace 
Abstract: When the Mersenne Twister made his first appearance in 1997 it was a powerful example of how linear maps on $\mathbf F_2$ could be used to generate pseudorandom numbers. In particular, the easiness with which generators with long periods could be defined gave the Mersenne Twister a large following, in spite of the fact that such long periods are not a measure of quality, and they require a large amount of memory. Even at the time of its publication, several defects of the Mersenne Twister were predictable, but they were somewhat obscured by other interesting properties. Today the Mersenne Twister is the default generator in C compilers, the Python language, the Maple mathematical computation system, and in many other environments. Nonetheless, knowledge accumulated in the last $20$ years suggests that the Mersenne Twister has, in fact, severe defects, and should never be used as a general-purpose pseudorandom number generator. Many of these results are folklore, or are scattered through very specialized literature. This paper surveys these results for the non-specialist, providing new, simple, understandable examples, and it is intended as a guide for the final user, or for language implementors, so that they can take an informed decision about whether to use the Mersenne Twister or not.</description>
      <guid isPermaLink="false">oai:arXiv.org:1910.06437v3</guid>
      <category>cs.DS</category>
      <category>cs.CR</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sebastiano Vigna</dc:creator>
    </item>
    <item>
      <title>Balancing the Spread of Two Opinions in Sparse Social Networks</title>
      <link>https://arxiv.org/abs/2105.10184</link>
      <description>arXiv:2105.10184v2 Announce Type: replace 
Abstract: Inspired by the famous Target Set Selection problem, we propose a new discrete model to simultaneously spread two opinions within a social network and perform an initial study of its complexity. Here, we are given a social network, a seed-set of agents for each opinion, two thresholds for each agent, a budget, and a number of rounds. The first threshold represents the willingness of an agent to adopt an opinion if the agent has no opinion at all, while the second threshold states the willingness to acquire a second opinion if the agent already has one. The goal is to add at most budget-many agents to the initial seed-sets such that the process started with these extended seed-sets stabilizes within the given number of rounds, with each agent having either both opinions or none. That is, our goal is to ensure that the spread of opinions is balanced.
  We show that the problem is NP-hard, and thus we study the problem from the perspective of parameterized complexity. In particular, we show that the problem is FPT when parameterized by the number of rounds, the maximum threshold, and the treewidth combined. This algorithm also applies to the combined parameter, the treedepth and the maximum threshold. Finally, we show that the problem is FPT when parameterized by the vertex cover number, the $3$-path vertex cover number, or the vertex integrity of the input network alone. To complement our tractability results, we show that the problem is W[1]-hard with respect to a) the sizes of the initial seed-sets and the feedback-vertex set number combined, even if all thresholds are bounded by a constant, and b) the budget, the 4-path vertex cover number, and the feedback-vertex set number combined, even if every activation process stabilizes in at most 4 rounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.10184v2</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Du\v{s}an Knop, \v{S}imon Schierreich, Ond\v{r}ej Such\'y</dc:creator>
    </item>
    <item>
      <title>A more efficient algorithm to compute the Rand Index for change-point problems</title>
      <link>https://arxiv.org/abs/2112.03738</link>
      <description>arXiv:2112.03738v2 Announce Type: replace 
Abstract: We provide a more efficient algorithm for computing the Rand Index when the data cluster comes from a change-point detection problem. Given $N$ data points and two clusterings of size $r$ and $s$, the algorithm runs on $O(r+s)$ time complexity and $O(1)$ memory complexity. The traditional algorithm, in contrast, runs on $O(rs+N)$ time complexity and $O(rs)$ memory complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.03738v2</guid>
      <category>cs.DS</category>
      <category>stat.CO</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas de Oliveira Prates</dc:creator>
    </item>
    <item>
      <title>Influential Slot and Tag Selection in Billboard Advertisement</title>
      <link>https://arxiv.org/abs/2401.10601</link>
      <description>arXiv:2401.10601v2 Announce Type: replace 
Abstract: The selection of influential billboard slots remains an important problem in billboard advertisements. Existing studies on this problem have not considered the case of context-specific influence probability. To bridge this gap, in this paper, we introduce the Context Dependent Influential Billboard Slot Selection Problem. First, we show that the problem is NP-hard. We also show that the influence function holds the bi-monotonicity, bi-submodularity, and non-negativity properties. We propose an orthant-wise Stochastic Greedy approach to solve this problem. We show that this method leads to a constant-factor approximation guarantee. Subsequently, we propose an orthant-wise Incremental and Lazy Greedy approach. In a generic sense, this is a method for maximizing a bi-submodular function under the cardinality constraint, which may also be of independent interest. We analyze the performance guarantee of this algorithm as well as time and space complexity. The proposed solution approaches have been implemented with real-world billboard and trajectory datasets. We compare the performance of our method with several baseline methods, and the results are reported. Our proposed orthant-wise stochastic greedy approach leads to significant results when the parameters are set properly with reasonable computational overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.10601v2</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dildar Ali, Suman Banerjee, Yamuna Prasad</dc:creator>
    </item>
    <item>
      <title>IID Prophet Inequality with Random Horizon: Going Beyond Increasing Hazard Rates</title>
      <link>https://arxiv.org/abs/2407.11752</link>
      <description>arXiv:2407.11752v4 Announce Type: replace 
Abstract: Prophet inequalities are a central object of study in optimal stopping theory. In the iid model, a gambler sees values in an online fashion, sampled independently from a given distribution. Upon observing each value, the gambler either accepts it as a reward or irrevocably rejects it and proceeds to observe the next value. The goal of the gambler, who cannot see the future, is maximising the expected value of the reward while competing against the expectation of a prophet (the offline maximum). In other words, one seeks to maximise the gambler-to-prophet ratio of the expectations.
  This model has been studied with infinite, finite and unknown number of values. When the gambler faces a random number of values, the model is said to have random horizon. We consider the model in which the gambler is given a priori knowledge of the horizon's distribution. Alijani et al. (2020) designed a single-threshold algorithm achieving a ratio of $1/2$ when the random horizon has an increasing hazard rate and is independent of the values. We prove that with a single threshold, a ratio of $1/2$ is actually achievable for several larger classes of horizon distributions, with the largest being known as the $\mathcal{G}$ class in reliability theory. Moreover, we show that this does not extend to its dual, the $\overline{\mathcal{G}}$ class (which includes the decreasing hazard rate class), while it can be extended to low-variance horizons. Finally, we construct the first example of a family of horizons, for which multiple thresholds are necessary to achieve a nonzero ratio. We establish that the Secretary Problem optimal stopping rule provides one such algorithm, paving the way towards the study of the model beyond single-threshold algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11752v4</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giordano Giambartolomei, Frederik Mallmann-Trenn, Raimundo Saona</dc:creator>
    </item>
    <item>
      <title>On the Limits of Language Generation: Trade-Offs Between Hallucination and Mode Collapse</title>
      <link>https://arxiv.org/abs/2411.09642</link>
      <description>arXiv:2411.09642v2 Announce Type: replace-cross 
Abstract: Specifying all desirable properties of a language model is challenging, but certain requirements seem essential. Given samples from an unknown language, the trained model should produce valid strings not seen in training and be expressive enough to capture the language's full richness. Otherwise, outputting invalid strings constitutes "hallucination," and failing to capture the full range leads to "mode collapse." We ask if a language model can meet both requirements.
  We investigate this within a statistical language generation setting building on Gold and Angluin. Here, the model receives random samples from a distribution over an unknown language K, which belongs to a possibly infinite collection of languages. The goal is to generate unseen strings from K. We say the model generates from K with consistency and breadth if, as training size increases, its output converges to all unseen strings in K.
  Kleinberg and Mullainathan [KM24] asked if consistency and breadth in language generation are possible. We answer this negatively: for a large class of language models, including next-token prediction models, this is impossible for most collections of candidate languages. This contrasts with [KM24]'s result, showing consistent generation without breadth is possible for any countable collection of languages. Our finding highlights that generation with breadth fundamentally differs from generation without breadth.
  As a byproduct, we establish near-tight bounds on the number of samples needed for generation with or without breadth.
  Finally, our results offer hope: consistent generation with breadth is achievable for any countable collection of languages when negative examples (strings outside K) are available alongside positive ones. This suggests that post-training feedback, which encodes negative examples, can be crucial in reducing hallucinations while limiting mode collapse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09642v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alkis Kalavasis, Anay Mehrotra, Grigoris Velegkas</dc:creator>
    </item>
    <item>
      <title>Sample-Efficient Estimation of Nonlinear Quantum State Functions</title>
      <link>https://arxiv.org/abs/2412.01696</link>
      <description>arXiv:2412.01696v3 Announce Type: replace-cross 
Abstract: Efficient estimation of nonlinear functions of quantum states is crucial for various key tasks in quantum computing, such as entanglement spectroscopy, fidelity estimation, and feature analysis of quantum data. Conventional methods using state tomography and estimating numerous terms of the series expansion are computationally expensive, while alternative approaches based on a purified query oracle impose practical constraints. In this paper, we introduce the quantum state function (QSF) framework by extending the SWAP test via linear combination of unitaries and parameterized quantum circuits. Our framework enables the implementation of arbitrarily normalized degree-$n$ polynomial functions of quantum states with precision $\varepsilon$ using $\mathcal{O}(n/\varepsilon^2)$ copies. We further apply QSF for developing quantum algorithms for fundamental tasks, including entropy, fidelity, and eigenvalue estimations. Specifically, for estimating von Neumann entropy, quantum relative entropy, and quantum state fidelity, where $\kappa$ and $\gamma$ represent the minimal nonzero eigenvalue and normalized factor, respectively, we achieve a sample complexity of $\tilde{\mathcal{O}}(\gamma^2/(\varepsilon^2\kappa))$. Our work establishes a concise and unified paradigm for estimating and realizing nonlinear functions of quantum states, paving the way for the practical processing and analysis of quantum data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01696v3</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hongshun Yao, Yingjian Liu, Tengxiang Lin, Xin Wang</dc:creator>
    </item>
    <item>
      <title>A Formal Correctness Proof of Edmonds' Blossom Shrinking Algorithm</title>
      <link>https://arxiv.org/abs/2412.20878</link>
      <description>arXiv:2412.20878v2 Announce Type: replace-cross 
Abstract: We present the first formal correctness proof of Edmonds' blossom shrinking algorithm for maximum cardinality matching in general graphs. We focus on formalising the mathematical structures and properties that allow the algorithm to run in worst-case polynomial running time. We formalise Berge's lemma, blossoms and their properties, and a mathematical model of the algorithm, showing that it is totally correct. We provide the first detailed proofs of many of the facts underlying the algorithm's correctness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20878v2</guid>
      <category>cs.LO</category>
      <category>cs.DS</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Abdulaziz</dc:creator>
    </item>
  </channel>
</rss>
