<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 01 May 2025 01:48:44 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Flexible extreme thresholds through generalised Bayesian model averaging</title>
      <link>https://arxiv.org/abs/2504.20216</link>
      <description>arXiv:2504.20216v1 Announce Type: new 
Abstract: Insurance products frequently cover significant claims arising from a variety of sources. To model losses from these products accurately, actuarial models must account for high-severity claims. A widely used strategy is to apply a mixture model, fitting one distribution to losses below a given threshold and modeling excess losses using extreme value theory. However, selecting an appropriate threshold remains an open question with no universally agreed-upon solution. Bayesian Model Averaging (BMA) provides a promising alternative by enabling the simultaneous consideration of multiple thresholds. In this paper, we show that an error integration BMA algorithm can effectively detect heterogeneous optimal thresholds that adapt to predictive variables through the combination of mixture models. This method enhances model accuracy by capturing the full loss distribution and lessening sensitivity to threshold choice. We validate the proposed approach using simulation studies and an application to an automobile claims dataset from a Canadian insurer. As a special case, we also study the homogeneous setting, where a single optimal threshold is selected, and compare it to automatic selection algorithms based on goodness-of-fit tests applied to an actuarial dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20216v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>S\'ebastien Jessup, M\'elina Mailhot, Mathieu Pigeon</dc:creator>
    </item>
    <item>
      <title>Identification and estimation of vaccine effectiveness in the test-negative design under equi-confounding</title>
      <link>https://arxiv.org/abs/2504.20360</link>
      <description>arXiv:2504.20360v1 Announce Type: new 
Abstract: The test-negative design (TND) is frequently used to evaluate vaccine effectiveness in real-world settings. In a TND study, individuals with similar symptoms who seek care are tested for the disease of interest, and vaccine effectiveness is estimated by comparing the vaccination history of test-positive cases and test-negative controls. Traditional approaches justify the TND by assuming either (a) receiving a test is a perfect proxy for unmeasured health-seeking behavior or (b) vaccination is unconfounded given measured covariates -- both of which may be unrealistic in practice. In this paper, we return to the original motivation for the TND and propose an alternative justification based on the assumption of \textit{odds ratio equi-confounding}, where unmeasured confounders influence test-positive and test-negative individuals equivalently on the odds ratio scale. We discuss the implications of this assumption for TND design and provide alternative estimators for the marginal risk ratio among the vaccinated under equi-confounding, including estimators based on outcome modeling and inverse probability weighting as well as a semiparametric estimator that is doubly-robust. When the equi-confounding assumption does not hold, we suggest a sensitivity analysis that parameterizes the magnitude of the deviation on the odds ratio scale. We conduct a simulation study to evaluate the empirical performance of our proposed estimators under a wide range of scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20360v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Christopher B. Boyer, Kendrick Qijun Li, Xu Shi, Eric J. Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Modeling Dependence in Omics Association Analysis via Structured Co-Expression Networks to Improve Power and Replicability</title>
      <link>https://arxiv.org/abs/2504.20431</link>
      <description>arXiv:2504.20431v1 Announce Type: new 
Abstract: Association analysis (e.g., differential expression analysis) and co-expression analysis are two major classes of statistical methods for omics data. While association analysis identifies individual features linked to health conditions, co-expression analysis examines dependencies among features to uncover functional modules and regulatory interactions. However, these approaches are often conducted separately, potentially leading to statistical inference with reduced sensitivity and replicability. To address this, we propose CoReg, a new statistical framework that integrates co-expression network analysis and factor models into the covariance modeling of multivariate regression. By accounting for the dependencies among omics features, CoReg enhances the power and sensitivity of association analysis while maintaining a well-controlled false discovery rate, thereby improving replicability across omics studies. We developed computationally efficient algorithms to implement CoReg and applied it to extensive simulation studies and real-world omics data analyses. Results demonstrate that CoReg improves statistical inference accuracy and replicability compared to conventional methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20431v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hwiyoung Lee, Yezhi Pan, Shuo Chen</dc:creator>
    </item>
    <item>
      <title>The Promises of Multiple Experiments: Identifying Joint Distribution of Potential Outcomes</title>
      <link>https://arxiv.org/abs/2504.20470</link>
      <description>arXiv:2504.20470v1 Announce Type: new 
Abstract: Typical causal effects are defined based on the marginal distribution of potential outcomes. However, many real-world applications require causal estimands involving the joint distribution of potential outcomes to enable more nuanced treatment evaluation and selection. In this article, we propose a novel framework for identifying and estimating the joint distribution of potential outcomes using multiple experimental datasets. We introduce the assumption of transportability of state transition probabilities for potential outcomes across datasets and establish the identification of the joint distribution under this assumption, along with a regular full-column rank condition. The key identification assumptions are testable in an overidentified setting and are analogous to those in the context of instrumental variables, with the dataset indicator serving as "instrument". Moreover, we propose an easy-to-use least-squares-based estimator for the joint distribution of potential outcomes in each dataset, proving its consistency and asymptotic normality. We further extend the proposed framework to identify and estimate principal causal effects. We empirically demonstrate the proposed framework by conducting extensive simulations and applying it to evaluate the surrogate endpoint in a real-world application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20470v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peng Wu, Xiaojie Mao</dc:creator>
    </item>
    <item>
      <title>Advanced biomarker analysis for early Alzheimer's detection: a 3-class classification approach</title>
      <link>https://arxiv.org/abs/2504.20577</link>
      <description>arXiv:2504.20577v1 Announce Type: new 
Abstract: The receiver operating characteristic (ROC) curve is an important tool for the discrimination of two populations. However, in many settings, the diagnostic decision is not limited to a binary choice. ROC surfaces are considered as a natural generalization of ROC curves in three-class diagnostic problems and the Volume Under the ROC Surface (VUS) was proposed as an index for the assessment of the diagnostic accuracy of the marker under consideration. In this paper, we propose an overlap measure (OVL) in the case of three-class diagnostic problems. Specifically, parametric and non-parametric approaches for the estimation of OVL are introduced. We evaluate this measure through simulations and compare it with the well-known measure given by VUS. Furthermore, our proposal is applied to the clinical diagnosis of early stage Alzheimer's disease.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20577v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sierra Marquina Victor Miguel, Pardo Maria del Carmen, Franco-Pereira Alba Maria</dc:creator>
    </item>
    <item>
      <title>Inference of high-dimensional weak instrumental variable regression models without ridge-regularization</title>
      <link>https://arxiv.org/abs/2504.20686</link>
      <description>arXiv:2504.20686v1 Announce Type: new 
Abstract: Inference of instrumental variable regression models with many weak instruments attracts many attentions recently. To extend the classical Anderson-Rubin test to high-dimensional setting, many procedures adopt ridge-regularization. However, we show that it is not necessary to consider ridge-regularization. Actually we propose a new quadratic-type test statistic which does not involve tuning parameters. Our quadratic-type test exhibits high power against dense alternatives. While for sparse alternatives, we derive the asymptotic distribution of an existing maximum-type test, enabling the use of less conservative critical values. To achieve strong performance across a wide range of scenarios, we further introduce a combined test procedure that integrates the strengths of both approaches. This combined procedure is powerful without requiring prior knowledge of the underlying sparsity of the first-stage model. Compared to existing methods, our proposed tests are easy to implement, free of tuning parameters, and robust to arbitrarily weak instruments as well as heteroskedastic errors. Simulation studies and empirical applications demonstrate the advantages of our methods over existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20686v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiarong Ding, Xu Guo, Yanmei Shi, Yuxin Wang</dc:creator>
    </item>
    <item>
      <title>Causal Identification in Time Series Models</title>
      <link>https://arxiv.org/abs/2504.20172</link>
      <description>arXiv:2504.20172v1 Announce Type: cross 
Abstract: In this paper, we analyze the applicability of the Causal Identification algorithm to causal time series graphs with latent confounders. Since these graphs extend over infinitely many time steps, deciding whether causal effects across arbitrary time intervals are identifiable appears to require computation on graph segments of unbounded size. Even for deciding the identifiability of intervention effects on variables that are close in time, no bound is known on how many time steps in the past need to be considered. We give a first bound of this kind that only depends on the number of variables per time step and the maximum time lag of any direct or latent causal effect. More generally, we show that applying the Causal Identification algorithm to a constant-size segment of the time series graph is sufficient to decide identifiability of causal effects, even across unbounded time intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20172v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Erik Jahn, Karthik Karnik, Leonard J. Schulman</dc:creator>
    </item>
    <item>
      <title>Sparse mixed linear modeling with anchor-based guidance for high-entropy alloy discovery</title>
      <link>https://arxiv.org/abs/2504.20354</link>
      <description>arXiv:2504.20354v1 Announce Type: cross 
Abstract: High-entropy alloys have attracted attention for their exceptional mechanical properties and thermal stability. However, the combinatorial explosion in the number of possible elemental compositions renders traditional trial-and-error experimental approaches highly inefficient for materials discovery. To solve this problem, machine learning techniques have been increasingly employed for property prediction and high-throughput screening. Nevertheless, highly accurate nonlinear models often suffer from a lack of interpretability, which is a major limitation. In this study, we focus on local data structures that emerge from the greedy search behavior inherent to experimental data acquisition. By introducing a linear and low-dimensional mixture regression model, we strike a balance between predictive performance and model interpretability. In addition, we develop an algorithm that simultaneously performs prediction and feature selection by considering multiple candidate descriptors. Through a case study on high-entropy alloys, this study introduces a method that combines anchor-guided clustering and sparse linear modeling to address biased data structures arising from greedy exploration in materials science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20354v1</guid>
      <category>cond-mat.mtrl-sci</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryo Murakami, Seiji Miura, Akihiro Endo, Satoshi Minamoto</dc:creator>
    </item>
    <item>
      <title>The Estimation of Continual Causal Effect for Dataset Shifting Streams</title>
      <link>https://arxiv.org/abs/2504.20471</link>
      <description>arXiv:2504.20471v1 Announce Type: cross 
Abstract: Causal effect estimation has been widely used in marketing optimization. The framework of an uplift model followed by a constrained optimization algorithm is popular in practice. To enhance performance in the online environment, the framework needs to be improved to address the complexities caused by temporal dataset shift. This paper focuses on capturing the dataset shift from user behavior and domain distribution changing over time. We propose an Incremental Causal Effect with Proxy Knowledge Distillation (ICE-PKD) framework to tackle this challenge. The ICE-PKD framework includes two components: (i) a multi-treatment uplift network that eliminates confounding bias using counterfactual regression; (ii) an incremental training strategy that adapts to the temporal dataset shift by updating with the latest data and protects generalization via replay-based knowledge distillation. We also revisit the uplift modeling metrics and introduce a novel metric for more precise online evaluation in multiple treatment scenarios. Extensive experiments on both simulated and online datasets show that the proposed framework achieves better performance. The ICE-PKD framework has been deployed in the marketing system of Huaxiaozhu, a ride-hailing platform in China.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20471v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Baining Chen, Yiming Zhang, Yuqiao Han, Ruyue Zhang, Ruihuan Du, Zhishuo Zhou, Zhengdan Zhu, Xun Liu, Jiecheng Guo</dc:creator>
    </item>
    <item>
      <title>Representation Learning Preserving Ignorability and Covariate Matching for Treatment Effects</title>
      <link>https://arxiv.org/abs/2504.20579</link>
      <description>arXiv:2504.20579v1 Announce Type: cross 
Abstract: Estimating treatment effects from observational data is challenging due to two main reasons: (a) hidden confounding, and (b) covariate mismatch (control and treatment groups not having identical distributions). Long lines of works exist that address only either of these issues. To address the former, conventional techniques that require detailed knowledge in the form of causal graphs have been proposed. For the latter, covariate matching and importance weighting methods have been used. Recently, there has been progress in combining testable independencies with partial side information for tackling hidden confounding. A common framework to address both hidden confounding and selection bias is missing. We propose neural architectures that aim to learn a representation of pre-treatment covariates that is a valid adjustment and also satisfies covariate matching constraints. We combine two different neural architectures: one based on gradient matching across domains created by subsampling a suitable anchor variable that assumes causal side information, followed by the other, a covariate matching transformation. We prove that approximately invariant representations yield approximate valid adjustment sets which would enable an interval around the true causal effect. In contrast to usual sensitivity analysis, where an unknown nuisance parameter is varied, we have a testable approximation yielding a bound on the effect estimate. We also outperform various baselines with respect to ATE and PEHE errors on causal benchmarks that include IHDP, Jobs, Cattaneo, and an image-based Crowd Management dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20579v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Praharsh Nanavati, Ranjitha Prasad, Karthikeyan Shanmugam</dc:creator>
    </item>
    <item>
      <title>The Leaderboard Illusion</title>
      <link>https://arxiv.org/abs/2504.20879</link>
      <description>arXiv:2504.20879v1 Announce Type: cross 
Abstract: Measuring progress is fundamental to the advancement of any scientific field. As benchmarks play an increasingly central role, they also grow more susceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard for ranking the most capable AI systems. Yet, in this work we identify systematic issues that have resulted in a distorted playing field. We find that undisclosed private testing practices benefit a handful of providers who are able to test multiple variants before public release and retract scores if desired. We establish that the ability of these providers to choose the best score leads to biased Arena scores due to selective disclosure of performance results. At an extreme, we identify 27 private LLM variants tested by Meta in the lead-up to the Llama-4 release. We also establish that proprietary closed models are sampled at higher rates (number of battles) and have fewer models removed from the arena than open-weight and open-source alternatives. Both these policies lead to large data access asymmetries over time. Providers like Google and OpenAI have received an estimated 19.2% and 20.4% of all data on the arena, respectively. In contrast, a combined 83 open-weight models have only received an estimated 29.7% of the total data. We show that access to Chatbot Arena data yields substantial benefits; even limited additional data can result in relative performance gains of up to 112% on the arena distribution, based on our conservative estimates. Together, these dynamics result in overfitting to Arena-specific dynamics rather than general model quality. The Arena builds on the substantial efforts of both the organizers and an open community that maintains this valuable evaluation platform. We offer actionable recommendations to reform the Chatbot Arena's evaluation framework and promote fairer, more transparent benchmarking for the field</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20879v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Shivalika Singh, Yiyang Nan, Alex Wang, Daniel D'Souza, Sayash Kapoor, Ahmet \"Ust\"un, Sanmi Koyejo, Yuntian Deng, Shayne Longpre, Noah Smith, Beyza Ermis, Marzieh Fadaee, Sara Hooker</dc:creator>
    </item>
    <item>
      <title>Time-Varying Dispersion Integer-Valued GARCH Models</title>
      <link>https://arxiv.org/abs/2208.02024</link>
      <description>arXiv:2208.02024v3 Announce Type: replace 
Abstract: We propose a general class of INteger-valued Generalized AutoRegressive Conditionally Heteroscedastic (INGARCH) processes by allowing time-varying mean and dispersion parameters, which we call time-varying dispersion INGARCH (tv-DINGARCH) models. More specifically, we consider mixed Poisson INGARCH models and allow for dynamic modeling of the dispersion parameter (as well as the mean), similar to the spirit of the ordinary GARCH models. We derive conditions to obtain first and second-order stationarity, and ergodicity as well. Estimation of the parameters is addressed and their associated asymptotic properties are established as well. A restricted bootstrap procedure is proposed for testing constant dispersion against time-varying dispersion. Monte Carlo simulation studies are presented for checking point estimation, standard errors, and the performance of the restricted bootstrap approach. We apply the tv-DINGARCH process to model the weekly number of reported measles infections in North Rhine-Westphalia, Germany, from January 2001 to May 2013, and compare its performance to the ordinary INGARCH approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.02024v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wagner Barreto-Souza, Luiza S. C. Piancastelli, Konstantinos Fokianos, Hernando Ombao</dc:creator>
    </item>
    <item>
      <title>Data fusion using weakly aligned sources</title>
      <link>https://arxiv.org/abs/2308.14836</link>
      <description>arXiv:2308.14836v4 Announce Type: replace 
Abstract: We introduce a new data fusion method that utilizes multiple data sources to estimate a smooth, finite-dimensional parameter. Most existing methods only make use of fully aligned data sources that share common conditional distributions of one or more variables of interest. However, in many settings, the scarcity of fully aligned sources can make existing methods require unduly large sample sizes to be useful. Our approach enables the incorporation of weakly aligned data sources that are not perfectly aligned, provided their degree of misalignment is known up to finite-dimensional parameters. {We quantify the additional efficiency gains achieved through the integration of these weakly aligned sources. We characterize the semiparametric efficiency bound and provide a general means to construct estimators achieving these efficiency gains.} We illustrate our results by fusing data from two harmonized HIV monoclonal antibody prevention efficacy trials to study how a neutralizing antibody biomarker associates with HIV genotype.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.14836v4</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sijia Li, Peter B. Gilbert, Rui Duan, Alex Luedtke</dc:creator>
    </item>
    <item>
      <title>Recursive random binning to detect and display pairwise dependence</title>
      <link>https://arxiv.org/abs/2311.08561</link>
      <description>arXiv:2311.08561v2 Announce Type: replace 
Abstract: Random binnings generated via recursive binary splits are introduced as a way to detect, measure the strength of, and to display the pattern of association between any two variates, whether one or both are continuous or categorical. This provides a single approach to ordering large numbers of variate pairs by their measure of dependence and then to examine any pattern of dependence via a common display, the departure display (colouring bins by a standardized Pearson residual). Continuous variates are first ranked and their rank pairs binned. The Pearson's goodness of fit statistic is applicable but the classic $\chi^2$ approximation to its null distribution is not. Theoretical and empirical investigations motivate several approximations, including a simple $\chi^2$ approximation with real-valued, yet intuitive, degrees of freedom. Alternatively, applying an inverse probability transform from the ranks before binning returns a simple Pearson statistic with the classic degrees of freedom. Recursive random binning with different approximations is compared to recent grid-based methods on a variety of non-null dependence patterns; the method with any of these approximations is found to be well-calibrated and relatively powerful against common test alternatives. Method and displays are illustrated by applying the screening methodology to a publicly available data set having several continuous and categorical measurements of each of 6,497 Portuguese wines. The software is publicly available as the R package AssocBin.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.08561v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chris Salahub, Wayne Oldford</dc:creator>
    </item>
    <item>
      <title>Policy Learning with Distributional Welfare</title>
      <link>https://arxiv.org/abs/2311.15878</link>
      <description>arXiv:2311.15878v4 Announce Type: replace 
Abstract: In this paper, we explore optimal treatment allocation policies that target distributional welfare. Most literature on treatment choice has considered utilitarian welfare based on the conditional average treatment effect (ATE). While average welfare is intuitive, it may yield undesirable allocations especially when individuals are heterogeneous (e.g., with outliers) - the very reason individualized treatments were introduced in the first place. This observation motivates us to propose an optimal policy that allocates the treatment based on the conditional quantile of individual treatment effects (QoTE). Depending on the choice of the quantile probability, this criterion can accommodate a policymaker who is either prudent or negligent. The challenge of identifying the QoTE lies in its requirement for knowledge of the joint distribution of the counterfactual outcomes, which is not generally point-identified. We introduce minimax policies that are robust to this model uncertainty. A range of identifying assumptions can be used to yield more informative policies. For both stochastic and deterministic policies, we establish the asymptotic bound on the regret of implementing the proposed policies. The framework can be generalized to any setting where welfare is defined as a functional of the joint distribution of the potential outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.15878v4</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifan Cui, Sukjin Han</dc:creator>
    </item>
    <item>
      <title>Estimating treatment effects from observational data under truncation by death using survival-incorporated quantiles</title>
      <link>https://arxiv.org/abs/2407.00846</link>
      <description>arXiv:2407.00846v2 Announce Type: replace 
Abstract: The issue of "truncation by death" commonly arises in clinical research: subjects may die before their follow-up assessment, resulting in undefined clinical outcomes. To address this issue, we focus on survival-incorporated quantiles -- quantiles of a composite outcome combining death and clinical outcomes -- to summarize the effect of treatment. Using inverse probability of treatment weighting (IPTW), we propose an estimator for survival-incorporated quantiles from observational data, applicable to settings of both point treatment and time-varying treatments. We establish consistency and asymptotic normality of the estimator under both the true and estimated propensity scores. While the variance properties of IPTW estimators for the mean have been studied, to our knowledge, this article is the first to show that the IPTW quantile estimator using the estimated propensity score yields lower asymptotic variance than the IPTW quantile estimator using the true propensity score. Extensive simulations show that survival-incorporated quantiles provide a simple and useful summary measure and confirm that using the estimated propensity score reduces the root mean square error. We apply our method to estimate the effect of statins on the change in cognitive function, incorporating death, using data from the Long Life Family Study (LLFS) -- a multicenter observational study of 4953 older adults with familial longevity. Our results indicate no significant difference in cognitive decline between statin users and non-users with a similar age- and sex-distribution at baseline. This study not only contributes to understand the cognitive effects of statins but also provides insights into analyzing clinical outcomes in the presence of death.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00846v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Qingyan Xiang, Paola Sebastiani, Thomas Perls, Stacy L. Andersen, Svetlana Ukraintseva, Mikael Thinggaard, Judith J. Lok</dc:creator>
    </item>
    <item>
      <title>A spatial-correlated multitask linear mixed-effects model for imaging genetics</title>
      <link>https://arxiv.org/abs/2407.04530</link>
      <description>arXiv:2407.04530v3 Announce Type: replace 
Abstract: Imaging genetics aims to uncover the hidden relationship between imaging quantitative traits (QTs) and genetic markers (e.g. single nucleotide polymorphism (SNP)), and brings valuable insights into the pathogenesis of complex diseases, such as cancers and cognitive disorders (e.g. the Alzheimer's Disease). However, most linear models in imaging genetics didn't explicitly model the inner relationship among QTs, which might miss some potential efficiency gains from information borrowing across brain regions. In this work, we developed a novel Bayesian regression framework for identifying significant associations between QTs and genetic markers while explicitly modeling spatial dependency between QTs, with the main contributions as follows. Firstly, we developed a spatial-correlated multitask linear mixed-effects model (LMM) to account for dependencies between QTs. We incorporated a population-level mixed effects term into the model, taking full advantage of the dependent structure of brain imaging-derived QTs. Secondly, we implemented the model in the Bayesian framework and derived a Markov chain Monte Carlo (MCMC) algorithm to achieve the model inference. Further, we incorporated the MCMC samples with the Cauchy combination test (CCT) to examine the association between SNPs and QTs, which avoided computationally intractable multi-test issues. The simulation studies indicated improved power of our proposed model compared to classic models where inner dependencies of QTs were not modeled. We also applied the new spatial model to an imaging dataset obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04530v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zhibin Pu, Shufei Ge</dc:creator>
    </item>
    <item>
      <title>When Knockoffs fail: diagnosing and fixing non-exchangeability of Knockoffs</title>
      <link>https://arxiv.org/abs/2407.06892</link>
      <description>arXiv:2407.06892v2 Announce Type: replace 
Abstract: Knockoffs are a popular statistical framework that addresses the challenging problem of conditional variable selection in high-dimensional settings with statistical control. Such statistical control is essential for the reliability of inference. However, knockoff guarantees rely on an exchangeability assumption that is difficult to test in practice, and there is little discussion in the literature on how to deal with unfulfilled hypotheses. This assumption is related to the ability to generate data similar to the observed data. To maintain reliable inference, we introduce a diagnostic tool based on Classifier Two-Sample Tests. Using simulations and real data, we show that violations of this assumption occur in common settings for classical knockoff generators, especially when the data have a strong dependence structure. As a consequence, knockoff-based inference suffers from a massive inflation of false positives. We show that the diagnostic tool correctly detects such behavior. We show that an alternative knockoff construction, based on constructing a predictor of each variable based on all others, solves the issue. We also propose a computationally-efficient variant of this algorithm and show empirically that this approach restores error control on simulated data and semi-simulated experiments based on neuroimaging data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06892v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alexandre Blain, Angel Reyero Lobo, Julia Linhart, Bertrand Thirion, Pierre Neuvial</dc:creator>
    </item>
    <item>
      <title>Dynamic linear regression models for forecasting time series with semi long memory errors</title>
      <link>https://arxiv.org/abs/2408.09096</link>
      <description>arXiv:2408.09096v2 Announce Type: replace 
Abstract: Dynamic linear regression models forecast the values of a time series based on a linear combination of a set of exogenous time series while incorporating a time series process for the error term. This error process is often assumed to follow a stationary autoregressive integrated moving average (ARIMA) model, or its seasonal variants, which is unable to capture a long-range dependence structure (long memory) of the error process. We propose a novel dynamic linear regression model that incorporates the long-range dependence feature of the errors and show that the proposed error process may: (i) have a significant impact on the posterior uncertainty of the estimated regression parameters and (ii) improve the model's forecasting ability. We develop a Markov chain Monte Carlo method to fit general dynamic linear regression models based on a frequency domain approach that enables fast, asymptotically exact Bayesian inference for large datasets. We demonstrate that our approximate algorithm is faster than the traditional time domain approaches, such as the Kalman filter and the multivariate Gaussian likelihood, while producing a highly accurate approximation to the posterior. The method is illustrated in simulated examples and two energy forecasting applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09096v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Goodwin, Matias Quiroz, Robert Kohn</dc:creator>
    </item>
    <item>
      <title>Community detection in multi-layer networks by regularized debiased spectral clustering</title>
      <link>https://arxiv.org/abs/2409.07956</link>
      <description>arXiv:2409.07956v2 Announce Type: replace 
Abstract: Community detection is a crucial problem in the analysis of multi-layer networks. While regularized spectral clustering methods using the classical regularized Laplacian matrix have shown great potential in handling sparse single-layer networks, to our knowledge, their potential in multi-layer network community detection remains unexplored. To address this gap, in this work, we introduce a new method, called regularized debiased sum of squared adjacency matrices (RDSoS), to detect communities in multi-layer networks. RDSoS is developed based on a novel regularized Laplacian matrix that regularizes the debiased sum of squared adjacency matrices. In contrast, the classical regularized Laplacian matrix typically regularizes the adjacency matrix of a single-layer network. Therefore, at a high level, our regularized Laplacian matrix extends the classical one to multi layer networks. We establish the consistency property of RDSoS under the multi-layer stochastic block model (MLSBM) and further extend RDSoS and its theoretical results to the degree-corrected version of the MLSBM model. Additionally, we introduce a sum of squared adjacency matrices modularity (SoS-modularity) to measure the quality of community partitions in multi-layer networks and estimate the number of communities by maximizing this metric. Our methods offer promising applications for predicting gene functions, improving recommender systems, detecting medical insurance fraud, and facilitating link prediction. Experimental results demonstrate that our methods exhibit insensitivity to the selection of the regularizer, generally outperform state-of-the-art techniques, uncover the assortative property of real networks, and that our SoS-modularity provides a more accurate assessment of community quality compared to the average of the Newman-Girvan modularity across layers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07956v2</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huan Qing</dc:creator>
    </item>
    <item>
      <title>The Building Blocks of Classical Nonparametric Two-Sample Testing Procedures: Statistically Equivalent Blocks</title>
      <link>https://arxiv.org/abs/2501.10844</link>
      <description>arXiv:2501.10844v2 Announce Type: replace 
Abstract: Statistically equivalent blocks are not frequently considered in the context of nonparametric two-sample hypothesis testing. Despite the limited exposure, this paper shows that a number of classical nonparametric hypothesis tests can be derived on the basis of statistically equivalent blocks and their frequencies. Far from being a moot historical point, this allows for a more unified approach in considering the many two-sample nonparametric tests based on ranks, signs, placements, order statistics, and runs. Perhaps more importantly, this approach also allows for the easy extension of many univariate nonparametric tests into arbitrarily high dimensions that retain all null properties regardless of dimensionality and are invariant to the scaling of the observations. These generalizations do not require depth functions or the explicit use of spatial signs or ranks and may be of use in various areas such as life-testing and quality control. In the manuscript, an overview of statistically equivalent blocks and tests based on these blocks are provided. This is followed by reformulations of some popular univariate tests and generalizations to higher dimensions. A brief simulation study and comments comparing the proposed methods to existing testing procedures are offered along with some conclusions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10844v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chase Holcombe</dc:creator>
    </item>
    <item>
      <title>Bhirkuti's Relative Efficiency (BRE): Examining its Performance in Psychometric Simulations</title>
      <link>https://arxiv.org/abs/2503.04775</link>
      <description>arXiv:2503.04775v3 Announce Type: replace 
Abstract: Traditional Relative Efficiency (RE), based solely on variance, has limitations in evaluating estimator performance, particularly in planned missing data designs. We introduce Bhirkuti's Relative Efficiency (BRE), a novel metric that integrates precision and accuracy to provide a more robust assessment of efficiency. To compute BRE, we use interquartile range (IQR) overlap to measure precision and apply a bias adjustment factor based on the absolute median relative bias (AMRB). Monte Carlo simulations using a Latent Growth Model (LGM) with planned missing data illustrate that BRE maintains theoretically consistency and interpretability, avoiding paradoxes such as RE exceeding 100%. Visualizations via boxplots and ridgeline plots confirm that BRE provides a stable and meaningful estimator efficiency evaluation, making it a valuable advancement in psychometric and statistical modeling. By addressing fundamental weaknesses in traditional RE, BRE provides a superior, theoretically justified alternative for relative efficiency in psychometric modeling, structural equation modeling, and missing data research. This advancement enhances data-driven decision-making and offers a methodologically rigorous tool for researchers analyzing incomplete datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04775v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aneel Bhusal, Todd D. Little</dc:creator>
    </item>
    <item>
      <title>Adaptive Design for Contour Estimation from Computer Experiments with Quantitative and Qualitative Inputs</title>
      <link>https://arxiv.org/abs/2504.05498</link>
      <description>arXiv:2504.05498v2 Announce Type: replace 
Abstract: Computer experiments with quantitative and qualitative inputs are widely used to study many scientific and engineering processes. Much of the existing work has focused on design and modeling or process optimization for such experiments. This paper proposes an adaptive design approach for estimating a contour from computer experiments with quantitative and qualitative inputs. A new criterion is introduced to search for the follow-up inputs. The key features of the proposed criterion are (a) the criterion yields adaptive search regions; and (b) it is region-based cooperative in that for each stage of the sequential procedure, the candidate points in the design space is divided into two disjoint groups using confidence bounds, and within each group, an acquisition function is used to select a candidate point. Among the two selected points, a point that is closer to the contour level with the higher uncertainty or that has higher uncertainty when the distance between its prediction and the contour level is within a threshold is chosen. The proposed approach provides empirically more accurate contour estimation than existing approaches as illustrated in numerical examples and a real application. Theoretical justification of the proposed adaptive search region is given.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05498v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. Shahrokhian, X. Deng, C. D. Lin, P. Ranjan, L. Xu</dc:creator>
    </item>
    <item>
      <title>Deep learning with missing data</title>
      <link>https://arxiv.org/abs/2504.15388</link>
      <description>arXiv:2504.15388v2 Announce Type: replace 
Abstract: In the context of multivariate nonparametric regression with missing covariates, we propose Pattern Embedded Neural Networks (PENNs), which can be applied in conjunction with any existing imputation technique. In addition to a neural network trained on the imputed data, PENNs pass the vectors of observation indicators through a second neural network to provide a compact representation. The outputs are then combined in a third neural network to produce final predictions. Our main theoretical result exploits an assumption that the observation patterns can be partitioned into cells on which the Bayes regression function behaves similarly, and belongs to a compositional H\"older class. It provides a finite-sample excess risk bound that holds for an arbitrary missingness mechanism, and in combination with a complementary minimax lower bound, demonstrates that our PENN estimator attains in typical cases the minimax rate of convergence as if the cells of the partition were known in advance, up to a poly-logarithmic factor in the sample size. Numerical experiments on simulated, semi-synthetic and real data confirm that the PENN estimator consistently improves, often dramatically, on standard neural networks without pattern embedding. Code to reproduce our experiments, as well as a tutorial on how to apply our method, is publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15388v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianyi Ma, Tengyao Wang, Richard J. Samworth</dc:creator>
    </item>
    <item>
      <title>Collaborative Inference for Sparse High-Dimensional Models with Non-Shared Data</title>
      <link>https://arxiv.org/abs/2504.19924</link>
      <description>arXiv:2504.19924v2 Announce Type: replace 
Abstract: In modern data analysis, statistical efficiency improvement is expected via effective collaboration among multiple data holders with non-shared data. In this article, we propose a collaborative score-type test (CST) for testing linear hypotheses, which accommodates potentially high-dimensional nuisance parameters and a diverging number of constraints and target parameters. Through a careful decomposition of the Kiefer-Bahadur representation for the traditional score statistic, we identify and approximate the key components using aggregated local gradient information from each data source. In addition, we employ a two-stage partial penalization strategy to shrink the approximation error and mitigate the bias from the high-dimensional nuisance parameters. Unlike existing methods, the CST procedure involves constrained optimization under non-shared and high-dimensional data settings, which requires novel theoretical developments. We derive the limiting distributions for the CST statistic under the null hypothesis and the local alternatives. Besides, the CST exhibits an oracle property and achieves the global statistical efficiency. Moreover, it relaxes the stringent restrictions on the number of data sources required in the current literature. Extensive numerical studies and a real example demonstrate the effectiveness and validity of our proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19924v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifan Gu, Hanfang Yang, Songshan Yang, Hui Zou</dc:creator>
    </item>
    <item>
      <title>Centered plug-in estimation of Wasserstein distances</title>
      <link>https://arxiv.org/abs/2203.11627</link>
      <description>arXiv:2203.11627v2 Announce Type: replace-cross 
Abstract: The plug-in estimator of the squared Euclidean 2-Wasserstein distance is conservative, however due to its large positive bias it is often uninformative. We eliminate most of this bias using a simple centering procedure based on linear combinations. We construct a pair of centered plug-in estimators that decrease with the true Wasserstein distance, and are therefore guaranteed to be informative, for any finite sample size. Crucially, we demonstrate that these estimators can often be viewed as complementary upper and lower bounds on the squared Wasserstein distance. Finally, we apply the estimators to Bayesian computation, developing methods for estimating (i) the bias of approximate inference methods and (ii) the convergence of MCMC algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.11627v2</guid>
      <category>stat.ML</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tam\'as P. Papp, Chris Sherlock</dc:creator>
    </item>
    <item>
      <title>Similarity matrix average for aggregating multiplex networks</title>
      <link>https://arxiv.org/abs/2208.06431</link>
      <description>arXiv:2208.06431v2 Announce Type: replace-cross 
Abstract: We introduce a methodology based on averaging similarity matrices with the aim of integrating the layers of a multiplex network into a single monoplex network. Multiplex networks are adopted for modelling a wide variety of real-world frameworks, such as multi-type relations in social, economic and biological structures. More specifically, multiplex networks are used when relations of different nature (layers) arise between a set of elements from a given population (nodes). A possible approach for investigating multiplex networks consists in aggregating the different layers in a single network (monoplex) which is a valid representation -- in some sense -- of all the layers. In order to obtain such an aggregated network, we propose a theoretical approach -- along with its practical implementation -- which stems on the concept of similarity matrix average. This methodology is finally applied to a multiplex similarity network of statistical journals, where the three considered layers express the similarity of the journals based on co-citations, common authors and common editors, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.06431v2</guid>
      <category>physics.soc-ph</category>
      <category>cs.CG</category>
      <category>physics.data-an</category>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1088/2632-072X/acda09</arxiv:DOI>
      <arxiv:journal_reference>Journal of Physics: Complexity, 4(2), 025017 (2023)</arxiv:journal_reference>
      <dc:creator>Federica Baccini, Lucio Barabesi, Eugenio Petrovich</dc:creator>
    </item>
    <item>
      <title>A multi-language toolkit for the semi-automated checking of research outputs</title>
      <link>https://arxiv.org/abs/2212.02935</link>
      <description>arXiv:2212.02935v4 Announce Type: replace-cross 
Abstract: This article presents a free and open source toolkit that supports the semi-automated checking of research outputs (SACRO) for privacy disclosure within secure data environments. SACRO is a framework that applies best-practice principles-based statistical disclosure control (SDC) techniques on-the-fly as researchers conduct their analyses. SACRO is designed to assist human checkers rather than seeking to replace them as with current automated rules-based approaches. The toolkit is composed of a lightweight Python package that sits over well-known analysis tools that produce outputs such as tables, plots, and statistical models. This package adds functionality to (i) automatically identify potentially disclosive outputs against a range of commonly used disclosure tests; (ii) apply optional disclosure mitigation strategies as requested; (iii) report reasons for applying SDC; and (iv) produce simple summary documents trusted research environment staff can use to streamline their workflow and maintain auditable records. This creates an explicit change in the dynamics so that SDC is something done with researchers rather than to them, and enables more efficient communication with checkers. A graphical user interface supports human checkers by displaying the requested output and results of the checks in an immediately accessible format, highlighting identified issues, potential mitigation options, and tracking decisions made. The major analytical programming languages used by researchers (Python, R, and Stata) are supported by providing front-end packages that interface with the core Python back-end. Source code, packages, and documentation are available under MIT license at https://github.com/AI-SDC/ACRO</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.02935v4</guid>
      <category>cs.CR</category>
      <category>cs.IR</category>
      <category>cs.SE</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Richard J. Preen, Maha Albashir, Simon Davy, Jim Smith</dc:creator>
    </item>
    <item>
      <title>On the Robustness of Kernel Goodness-of-Fit Tests</title>
      <link>https://arxiv.org/abs/2408.05854</link>
      <description>arXiv:2408.05854v3 Announce Type: replace-cross 
Abstract: Goodness-of-fit testing is often criticized for its lack of practical relevance: since ``all models are wrong'', the null hypothesis that the data conform to our model is ultimately always rejected as sample size grows. Despite this, probabilistic models are still used extensively, raising the more pertinent question of whether the model is \emph{good enough} for the task at hand. This question can be formalized as a robust goodness-of-fit testing problem by asking whether the data were generated from a distribution that is a mild perturbation of the model. In this paper, we show that existing kernel goodness-of-fit tests are not robust under common notions of robustness including both qualitative and quantitative robustness. We further show that robustification techniques using tilted kernels, while effective in the parameter estimation literature, are not sufficient to ensure both types of robustness in the testing setting. To address this, we propose the first robust kernel goodness-of-fit test, which resolves this open problem by using kernel Stein discrepancy (KSD) balls. This framework encompasses many well-known perturbation models, such as Huber's contamination and density-band models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05854v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xing Liu, Fran\c{c}ois-Xavier Briol</dc:creator>
    </item>
    <item>
      <title>Debiasing Functions of Private Statistics in Postprocessing</title>
      <link>https://arxiv.org/abs/2502.13314</link>
      <description>arXiv:2502.13314v4 Announce Type: replace-cross 
Abstract: Given a differentially private unbiased estimate $\tilde{q}=q(D) +\nu$ of a statistic $q(D)$, we wish to obtain unbiased estimates of functions of $q(D)$, such as $1/q(D)$, solely through post-processing of $\tilde{q}$, with no further access to the confidential dataset $D$. To this end, we adapt the deconvolution method used for unbiased estimation in the statistical literature, deriving unbiased estimators for a broad family of twice-differentiable functions when the privacy-preserving noise $\nu$ is drawn from the Laplace distribution (Dwork et al., 2006). We further extend this technique to a more general class of functions, deriving approximately optimal estimators that are unbiased for values in a user-specified interval (possibly extending to $\pm \infty$). We use these results to derive an unbiased estimator for private means when the size $n$ of the dataset is not publicly known. In a numerical application, we find that a mechanism that uses our estimator to return an unbiased sample size and mean outperforms a mechanism that instead uses the previously known unbiased privacy mechanism for such means (Kamath et al., 2023). We also apply our estimators to develop unbiased transformation mechanisms for per-record differential privacy, a privacy concept in which the privacy guarantee is a public function of a record's value (Seeman et al., 2024). Our mechanisms provide stronger privacy guarantees than those in prior work (Finley et al., 2024) by using Laplace, rather than Gaussian, noise. Finally, using a different approach, we go beyond Laplace noise by deriving unbiased estimators for polynomials under the weak condition that the noise distribution has sufficiently many moments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13314v4</guid>
      <category>cs.CR</category>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Flavio Calmon, Elbert Du, Cynthia Dwork, Brian Finley, Grigory Franguridi</dc:creator>
    </item>
    <item>
      <title>Transfer Learning Under High-Dimensional Network Convolutional Regression Model</title>
      <link>https://arxiv.org/abs/2504.19979</link>
      <description>arXiv:2504.19979v2 Announce Type: replace-cross 
Abstract: Transfer learning enhances model performance by utilizing knowledge from related domains, particularly when labeled data is scarce. While existing research addresses transfer learning under various distribution shifts in independent settings, handling dependencies in networked data remains challenging. To address this challenge, we propose a high-dimensional transfer learning framework based on network convolutional regression (NCR), inspired by the success of graph convolutional networks (GCNs). The NCR model incorporates random network structure by allowing each node's response to depend on its features and the aggregated features of its neighbors, capturing local dependencies effectively. Our methodology includes a two-step transfer learning algorithm that addresses domain shift between source and target networks, along with a source detection mechanism to identify informative domains. Theoretically, we analyze the lasso estimator in the context of a random graph based on the Erdos-Renyi model assumption, demonstrating that transfer learning improves convergence rates when informative sources are present. Empirical evaluations, including simulations and a real-world application using Sina Weibo data, demonstrate substantial improvements in prediction accuracy, particularly when labeled data in the target domain is limited.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19979v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liyuan Wang, Jiachen Chen, Kathryn L. Lunetta, Danyang Huang, Huimin Cheng, Debarghya Mukherjee</dc:creator>
    </item>
  </channel>
</rss>
