<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 19 Dec 2024 02:53:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Methodology for Fine-Grain GPU Power Visibility and Insights</title>
      <link>https://arxiv.org/abs/2412.12426</link>
      <description>arXiv:2412.12426v1 Announce Type: new 
Abstract: Ubiquity of AI coupled with its steep power demands make optimizing GPU power a priority as large GPU-based clusters are often employed to train and serve AI models. An important first step in optimizing GPU power consumption is high-fidelity and fine-grain power measurement of key AI computations on GPUs. To this end, we observe that as GPUs get more powerful, the resulting sub-millisecond to millisecond executions make fine-grain power analysis challenging. In this work, we first carefully identify the challenges in obtaining fine-grain GPU power profiles. To address these challenges, we devise FinGraV methodology where we employ execution time binning, careful CPU-GPU time synchronization, and power profile differentiation to collect fine-grain GPU power profiles across prominent AI computations and across spectrum of scenarios. Using FinGraV power profiles, we make several observations pertaining to GPU power variation over executions and over time, GPU sub-component power consumptions across different scenarios, and power behavior over interleaved executions of multiple computations. Equipped with these observations, we conclude with several recommendations to optimize the power for these ubiquitous accelerators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12426v1</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Varsha Singhania, Shaizeen Aga, Mohamed Assem Ibrahim</dc:creator>
    </item>
    <item>
      <title>Design and Performance Analysis of an Ultra-Low Power Integrate-and-Fire Neuron Circuit Using Nanoscale Side-contacted Field Effect Diode Technology</title>
      <link>https://arxiv.org/abs/2412.12443</link>
      <description>arXiv:2412.12443v1 Announce Type: new 
Abstract: Enhancing power efficiency and performance in neuromorphic computing systems is critical for next-generation artificial intelligence applications. We propose the Nanoscale Side-contacted Field Effect Diode (S-FED), a novel solution that significantly lowers power usage and improves circuit speed, facilitating efficient neuron circuit design. Our innovative integrate-and-fire (IF) neuron model demonstrates exceptional performance metrics: 44 nW power consumption (85% lower than current designs), 0.964 fJ energy per spike (36% improvement over state-of-the-art), and 20 MHz spiking frequency. The architecture exhibits robust stability across process-voltage-temperature (PVT) variations, maintaining consistent performance with less than 7% spike amplitude variation for channel lengths from 7.5nm to 15nm, supply voltages from 0.8V to 1.2V, and temperatures from -40{\deg}C to 120{\deg}C. The model features tunable thresholds from 0.8V to 1.4V and reliable operation across input spike pulse widths from 0.5 ns to 2 ns. This significant advancement in neuromorphic hardware paves the way for more efficient brain-inspired computing systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12443v1</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seyedmohamadjavad Motaman, Sarah Sharif, Yaser Banad</dc:creator>
    </item>
    <item>
      <title>if-ZKP: Intel FPGA-Based Acceleration of Zero Knowledge Proofs</title>
      <link>https://arxiv.org/abs/2412.12481</link>
      <description>arXiv:2412.12481v1 Announce Type: new 
Abstract: Zero-Knowledge Proofs (ZKPs) have emerged as an important cryptographic technique allowing one party (prover) to prove the correctness of a statement to some other party (verifier) and nothing else. ZKPs give rise to user's privacy in many applications such as blockchains, digital voting, and machine learning. Traditionally, ZKPs suffered from poor scalability but recently, a sub-class of ZKPs known as Zero-knowledge Succinct Non-interactive ARgument of Knowledges (zk-SNARKs) have addressed this challenge. They are getting significant attention and are being implemented by many public libraries. In this paper, we present a novel scalable architecture that is suitable for accelerating the zk-SNARK prover compute on FPGAs. We focus on the multi-scalar multiplication (MSM) that accounts for the majority of computation time spent in zk-SNARK systems. The MSM calculations extensive rely on modular arithmetic so highly optimized Intel IP Libraries for modular arithmetic are used. The proposed architecture exploits the parallelism inherent to MSM and is implemented using the Intel OneAPI framework for FPGAs. Our implementation runs 110x-150x faster compared to reference software library, uses a generic curve form in Jacobian coordinates and is the first to report FPGA hardware acceleration results for BLS12-381 and BN128 family of elliptic curves.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12481v1</guid>
      <category>cs.AR</category>
      <category>cs.CR</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shahzad Ahmad Butt, Benjamin Reynolds, Veeraraghavan Ramamurthy, Xiao Xiao, Pohrong Chu, Setareh Sharifian, Sergey Gribok, Bogdan Pasca</dc:creator>
    </item>
    <item>
      <title>Investigating the Effect of Electrical and Thermal Transport Properties on Oxide-Based Memristors Performance and Reliability</title>
      <link>https://arxiv.org/abs/2412.12450</link>
      <description>arXiv:2412.12450v1 Announce Type: cross 
Abstract: Achieving reliable resistive switching in oxide-based memristive devices requires precise control over conductive filament (CF) formation and behavior, yet the fundamental relationship between oxide material properties and switching uniformity remains incompletely understood. Here, we develop a comprehensive physical model to investigate how electrical and thermal conductivities influence CF dynamics in TaOx-based memristors. Our simulations reveal that higher electrical conductivity promotes oxygen vacancy generation and reduces forming voltage, while higher thermal conductivity enhances heat dissipation, leading to increased forming voltage. The uniformity of resistive switching is strongly dependent on the interplay between these transport properties. We identify two distinct pathways for achieving optimal High Resistance State (HRS) uniformity with standard deviation-to-mean ratios as low as 0.045, each governed by different balances of electrical and thermal transport mechanisms. For the Low Resistance State (LRS), high uniformity (0.009) can be maintained when either electrical or thermal conductivity is low. The resistance ratio between HRS and LRS shows a strong dependence on these conductivities, with higher ratios observed at lower conductivity values. These findings provide essential guidelines for material selection in RRAM devices, particularly for applications demanding high reliability and uniform switching characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12450v1</guid>
      <category>cs.ET</category>
      <category>cs.AR</category>
      <category>physics.app-ph</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Armin Gooran-Shoorakchaly, Sarah Sharif, Yaser Banad</dc:creator>
    </item>
    <item>
      <title>TurboAttention: Efficient Attention Approximation For High Throughputs LLMs</title>
      <link>https://arxiv.org/abs/2412.08585</link>
      <description>arXiv:2412.08585v3 Announce Type: replace-cross 
Abstract: Large language model (LLM) inference demands significant amount of computation and memory, especially in the key attention mechanism. While techniques, such as quantization and acceleration algorithms, like FlashAttention, have improved efficiency of the overall inference, they address different aspects of the problem: quantization focuses on weight-activation operations, while FlashAttention improves execution but requires high-precision formats. Recent Key-value (KV) cache quantization reduces memory bandwidth but still needs floating-point dequantization for attention operation.
  We present TurboAttention, a comprehensive approach to enable quantized execution of attention that simultaneously addresses both memory and computational efficiency. Our solution introduces two key innovations: FlashQ, a headwise attention quantization technique that enables both compression of KV cache and quantized execution of activation-activation multiplication, and Sparsity-based Softmax Approximation (SAS), which eliminates the need for dequantization to FP32 during exponentiation operation in attention. Experimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup in attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x maximum throughput over the FP16 baseline while outperforming state-of-the-art quantization and compression techniques across various datasets and models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08585v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao Kang, Srikant Bharadwaj, James Hensman, Tushar Krishna, Victor Ruhle, Saravan Rajmohan</dc:creator>
    </item>
  </channel>
</rss>
