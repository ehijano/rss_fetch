<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 02 Jul 2025 04:03:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Harnessing the Power of Reinforcement Learning for Adaptive MCMC</title>
      <link>https://arxiv.org/abs/2507.00671</link>
      <description>arXiv:2507.00671v1 Announce Type: new 
Abstract: Sampling algorithms drive probabilistic machine learning, and recent years have seen an explosion in the diversity of tools for this task. However, the increasing sophistication of sampling algorithms is correlated with an increase in the tuning burden. There is now a greater need than ever to treat the tuning of samplers as a learning task in its own right. In a conceptual breakthrough, Wang et al (2025) formulated Metropolis-Hastings as a Markov decision process, opening up the possibility for adaptive tuning using Reinforcement Learning (RL). Their emphasis was on theoretical foundations; realising the practical benefit of Reinforcement Learning Metropolis-Hastings (RLMH) was left for subsequent work. The purpose of this paper is twofold: First, we observe the surprising result that natural choices of reward, such as the acceptance rate, or the expected squared jump distance, provide insufficient signal for training RLMH. Instead, we propose a novel reward based on the contrastive divergence, whose superior performance in the context of RLMH is demonstrated. Second, we explore the potential of RLMH and present adaptive gradient-based samplers that balance flexibility of the Markov transition kernel with learnability of the associated RL task. A comprehensive simulation study using the posteriordb benchmark supports the practical effectiveness of RLMH.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00671v1</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Congye Wang, Matthew A. Fisher, Heishiro Kanagawa, Wilson Chen, Chris. J. Oates</dc:creator>
    </item>
    <item>
      <title>ForLion: An R Package for Finding Optimal Experimental Designs with Mixed Factors</title>
      <link>https://arxiv.org/abs/2507.00923</link>
      <description>arXiv:2507.00923v1 Announce Type: new 
Abstract: Optimal design is crucial for experimenters to maximize the information collected from experiments and estimate the model parameters most accurately. ForLion algorithms have been proposed to find D-optimal designs for experiments with mixed types of factors. In this paper, we introduce the ForLion package which implements the ForLion algorithm to construct locally D-optimal designs and the EW ForLion algorithm to generate robust EW D-optimal designs. The package supports experiments under linear models (LM), generalized linear models (GLM), and multinomial logistic models (MLM) with continuous, discrete, or mixed-type factors. It provides both optimal approximate designs and an efficient function converting approximate designs into exact designs with integer-valued allocations of experimental units. Tutorials are included to show the package's usage across different scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00923v1</guid>
      <category>stat.CO</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siting Lin, Yifei Huang, Jie Yang</dc:creator>
    </item>
    <item>
      <title>Toward a Data Processing Pipeline for Mobile-Phone Tracking Data</title>
      <link>https://arxiv.org/abs/2507.00952</link>
      <description>arXiv:2507.00952v1 Announce Type: new 
Abstract: As mobile phones become ubiquitous, high-frequency smartphone positioning data are increasingly being used by researchers studying the mobility patterns of individuals as they go about their daily routines and the consequences of these patterns for health, behavioral, and other outcomes. A complex data pipeline underlies empirical research leveraging mobile phone tracking data. A key component of this pipeline is transforming raw, time-stamped positions into analysis-ready data objects, typically space-time "trajectories." In this paper, we break down a key portion of the data analysis pipeline underlying the Adolescent Health and Development in Context (AHDC) Study, a large-scale, longitudinal study of youth residing in the Columbus, OH metropolitan area. Recognizing that the bespoke "binning algorithm" used by AHDC researchers resembles a time-series filtering algorithm, we propose a statistical framework - a formal probability model and computational approach to inference - inspired by the binning algorithm for transforming noisy, time-stamped geographic positioning observations into mobility trajectories that capture periods of travel and stability. Our framework, unlike the binning algorithm, allows for formal smoothing via a particle Gibbs algorithm, improving estimation of trajectories as compared to the original binning algorithm. We argue that our framework can be used as a default data processing tool for future mobile-phone tracking studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00952v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marcin Jurek, Catherine A. Calder, Corwin Zigler, Bethany Boettner, Christopher R. Browning</dc:creator>
    </item>
    <item>
      <title>clustra: A multi-platform k-means clustering algorithm for analysis of longitudinal trajectories in large electronic health records data</title>
      <link>https://arxiv.org/abs/2507.00962</link>
      <description>arXiv:2507.00962v1 Announce Type: new 
Abstract: Background and Objective: Variables collected over time, or longitudinally, such as biologic measurements in electronic health records data, are not simple to summarize with a single time-point, and thus can be more holistically conceptualized as trajectories over time. Cluster analysis with longitudinal data further allows for clinical representation of groups of subjects with similar trajectories and identification of unique characteristics, or phenotypes, that can be investigated as risk factors or disease outcomes. Some of the challenges in estimating these clustered trajectories lie in the handling of observations at inconsistent time intervals and the usability of algorithms across programming languages.
  Methods: We propose longitudinal trajectory clustering using a k-means algorithm with thin-plate regression splines, implemented across multiple platforms, the R package clustra and corresponding \SAS macros. The \SAS macros accommodate flexible clustering approaches, and also include visualization of the clusters, and silhouette plots for diagnostic evaluation of the appropriate cluster number. The R package, designed in parallel, has similar functionality, with additional multi-core processing and Rand-index-based diagnostics.
  Results: The package and macros achieve comparable results when applied to an example of simulated blood pressure measurements based on real data from Veterans Affairs Healthcare recipients who were initiated on anti-hypertensive medication.
  Conclusion: The R package clustra and the SAS macros integrate a K-means clustering algorithm for longitudinal trajectories that operates with large electronic health record data. The implementations provide comparable results in both platforms, satisfying the needs of investigators familiar with, or constrained by access to, one or the other platform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00962v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Nimish Adhikari, Hanna Gerlovin, George Ostrouchov, Rachel Ehrbar, Alyssa B. Dufour, Brian R. Ferolito, Serkalem Demissie, Lauren Costa, Yuk-Lam Ho, Laura Tarko, Edmon Begoli, Kelly Cho, David R. Gagnon</dc:creator>
    </item>
    <item>
      <title>Hebbian Physics Networks: A Self-Organizing Computational Architecture Based on Local Physical Laws</title>
      <link>https://arxiv.org/abs/2507.00641</link>
      <description>arXiv:2507.00641v1 Announce Type: cross 
Abstract: Traditional machine learning approaches in physics rely on global optimization, limiting interpretability and enforcing physical constraints externally. We introduce the Hebbian Physics Network (HPN), a self-organizing computational framework in which learning emerges from local Hebbian updates driven by violations of conservation laws. Grounded in non-equilibrium thermodynamics and inspired by Prigogine/'s theory of dissipative structures, HPNs eliminate the need for global loss functions by encoding physical laws directly into the system/'s local dynamics. Residuals - quantified imbalances in continuity, momentum, or energy - serve as thermodynamic signals that drive weight adaptation through generalized Hebbian plasticity. We demonstrate this approach on incompressible fluid flow and continuum diffusion, where physically consistent structures emerge from random initial conditions without supervision. HPNs reframe computation as a residual-driven thermodynamic process, offering an interpretable, scalable, and physically grounded alternative for modeling complex dynamical systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00641v1</guid>
      <category>nlin.AO</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gunjan Auti, Hirofumi Daiguji, Gouhei Tanaka</dc:creator>
    </item>
    <item>
      <title>Leveraging Nested MLMC for Sequential Neural Posterior Estimation with Intractable Likelihoods</title>
      <link>https://arxiv.org/abs/2401.16776</link>
      <description>arXiv:2401.16776v2 Announce Type: replace 
Abstract: There has been a growing interest in studying sequential neural posterior estimation (SNPE) techniques for their advantages in dealing with simulation-based models with intractable likelihoods. They are devoted to learning the posterior from adaptively proposed simulations using neural network-based conditional density estimators. As a SNPE technique, the automatic posterior transformation (APT) method proposed by Greenberg et al. (2019) performs notably and scales to high dimensional data. However, the APT method bears the computation of an expectation of the logarithm of an intractable normalizing constant, i.e., a nested expectation. Although atomic APT was proposed to solve this by discretizing the normalizing constant, it remains challenging to analyze the convergence of learning. In this paper, we propose a nested APT method to estimate the involved nested expectation instead. This facilitates establishing the convergence analysis. Since the nested estimators for the loss function and its gradient are biased, we make use of unbiased multi-level Monte Carlo (MLMC) estimators for debiasing. To further reduce the excessive variance of the unbiased estimators, this paper also develops some truncated MLMC estimators by taking account of the trade-off between the bias and the average cost. Numerical experiments for approximating complex posteriors with multimodal in moderate dimensions are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.16776v2</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiliang Yang, Yifei Xiong, Zhijian He</dc:creator>
    </item>
  </channel>
</rss>
