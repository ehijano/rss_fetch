<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NE</link>
    <description>cs.NE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 11 Sep 2025 01:19:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Use of a genetic algorithm in university scheduling for equitable and efficient determination of teaching assignments</title>
      <link>https://arxiv.org/abs/2509.06981</link>
      <description>arXiv:2509.06981v1 Announce Type: new 
Abstract: Here a genetic algorithm (GA) is presented that creates a teaching schedule for a university physics department by algorithmically assigning ${\sim}200$ classes to ${\sim}50$ professors for each of three academic terms per year. The algorithm is driven by chromosomes of the GA that encode proposed pairings between enumerated lists of professors and classes. The fitness of the pairings is measured by considering both contractual work constraints and individual teaching preferences. The algorithm uses standard crossover and mutation operations to seek ever more optimal schedules over many generations. Here we detail the implementation and performance of the algorithm, including some interpretability findings. Overall, we are very pleased with the algorithm, as it is typically able to converge within minutes, with over $90\%$ of needed classes assigned. A metric is used to assign each professor's schedule a score, which measures how well their preferences were satisfied. These scores can be used to ensure longitudinal equity in the assignment of classes among professors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06981v1</guid>
      <category>cs.NE</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tom Bensky, Karl Saunders</dc:creator>
    </item>
    <item>
      <title>Using Variable Interaction Graphs to Improve Particle Swarm Optimization</title>
      <link>https://arxiv.org/abs/2509.06985</link>
      <description>arXiv:2509.06985v1 Announce Type: new 
Abstract: This paper presents Variable Interaction Graph Particle Swarm Optimization (VIGPSO), an adaptation to Particle Swarm Optimization (PSO) that dynamically learns and exploits variable interactions during the optimization process. PSO is widely used for real-valued optimization problems but faces challenges in high-dimensional search spaces. While Variable Interaction Graphs (VIGs) have proven effective for optimization algorithms operating with known problem structure, their application to black-box optimization remains limited. VIGPSO learns how variables influence each other by analyzing how particles move through the search space, and uses these learned relationships to guide future particle movements. VIGPSO was evaluated against standard PSO on eight benchmark functions (three separable, two partially separable, and three non-separable) across 10, 30, 50 and 1000 dimensions. VIGPSO achieved statistically significant improvements ($p&lt;0.05$) over the standard PSO algorithm in 28 out of 32 test configurations, with particularly strong performance extending to the 1000-dimensional case. The algorithm showed increasing effectiveness with dimensionality, though at the cost of higher variance in some test cases. These results suggest that dynamic VIG learning can bridge the gap between black-box and gray-box optimization effectively in PSO, particularly for high-dimensional problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06985v1</guid>
      <category>cs.NE</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3712255.3726589</arxiv:DOI>
      <arxiv:journal_reference>Journal-ref: Proc. Genetic and Evolutionary Computation Conference Companion (GECCO '25 Companion), ACM, 2025, pp. 479-482</arxiv:journal_reference>
      <dc:creator>Caz L. Czworkowski, John W. Sheppard</dc:creator>
    </item>
    <item>
      <title>A multi-strategy improved gazelle optimization algorithm for solving numerical optimization and engineering applications</title>
      <link>https://arxiv.org/abs/2509.07211</link>
      <description>arXiv:2509.07211v1 Announce Type: new 
Abstract: Aiming at the shortcomings of the gazelle optimization algorithm, such as the imbalance between exploration and exploitation and the insufficient information exchange within the population, this paper proposes a multi-strategy improved gazelle optimization algorithm (MSIGOA). To address these issues, MSIGOA proposes an iteration-based updating framework that switches between exploitation and exploration according to the optimization process, which effectively enhances the balance between local exploitation and global exploration in the optimization process and improves the convergence speed. Two adaptive parameter tuning strategies improve the applicability of the algorithm and promote a smoother optimization process. The dominant population-based restart strategy enhances the algorithms ability to escape from local optima and avoid its premature convergence. These enhancements significantly improve the exploration and exploitation capabilities of MSIGOA, bringing superior convergence and efficiency in dealing with complex problems. In this paper, the parameter sensitivity, strategy effectiveness, convergence and stability of the proposed method are evaluated on two benchmark test sets including CEC2017 and CEC2022. Test results and statistical tests show that MSIGOA outperforms basic GOA and other advanced algorithms. On the CEC2017 and CEC2022 test sets, the proportion of functions where MSIGOA is not worse than GOA is 92.2% and 83.3%, respectively, and the proportion of functions where MSIGOA is not worse than other algorithms is 88.57% and 87.5%, respectively. Finally, the extensibility of MSIGAO is further verified by several engineering design optimization problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07211v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s10586-025-05445-3</arxiv:DOI>
      <arxiv:journal_reference>Cluster Computing, Springer, Volume 28, Article 643, 2025</arxiv:journal_reference>
      <dc:creator>Qi Diao, Chengyue Xie, Yuchen Yin, Hoileong Lee, Haolong Yang</dc:creator>
    </item>
    <item>
      <title>Breaking the Conventional Forward-Backward Tie in Neural Networks: Activation Functions</title>
      <link>https://arxiv.org/abs/2509.07236</link>
      <description>arXiv:2509.07236v1 Announce Type: new 
Abstract: Gradient-based neural network training traditionally enforces symmetry between forward and backward propagation, requiring activation functions to be differentiable (or sub-differentiable) and strictly monotonic in certain regions to prevent flat gradient areas. This symmetry, linking forward activations closely to backward gradients, significantly restricts the selection of activation functions, particularly excluding those with substantial flat or non-differentiable regions. In this paper, we challenge this assumption through mathematical analysis, demonstrating that precise gradient magnitudes derived from activation functions are largely redundant, provided the gradient direction is preserved. Empirical experiments conducted on foundational architectures - such as Multi-Layer Perceptrons (MLPs), Convolutional Neural Networks (CNNs), and Binary Neural Networks (BNNs) - confirm that relaxing forward-backward symmetry and substituting traditional gradients with simpler or stochastic alternatives does not impair learning and may even enhance training stability and efficiency. We explicitly demonstrate that neural networks with flat or non-differentiable activation functions, such as the Heaviside step function, can be effectively trained, thereby expanding design flexibility and computational efficiency. Further empirical validation with more complex architectures remains a valuable direction for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07236v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.neucom.2025.131178</arxiv:DOI>
      <arxiv:journal_reference>Neurocomputing Art. No. 131178, 2025</arxiv:journal_reference>
      <dc:creator>Luigi Troiano, Francesco Gissi, Vincenzo Benedetto, Genny Tortora</dc:creator>
    </item>
    <item>
      <title>Word2Spike: Poisson Rate Coding for Associative Memories and Neuromorphic Algorithms</title>
      <link>https://arxiv.org/abs/2509.07361</link>
      <description>arXiv:2509.07361v1 Announce Type: new 
Abstract: Spiking neural networks offer a promising path toward energy-efficient, brain-like associative memory. This paper introduces Word2Spike, a novel rate coding mechanism that combines continuous word embeddings and neuromorphic architectures. We develop a one-to-one mapping that converts multi-dimensional word vectors into spike-based attractor states using Poisson processes. Using BitNet b1.58 quantization, we maintain 97% semantic similarity of continuous embeddings on SimLex-999 while achieving 100% reconstruction accuracy on 10,000 words from OpenAI's text-embedding-3-large. We preserve analogy performance (100% of original embedding performance) even under intentionally introduced noise, indicating a resilient mechanism for semantic encoding in neuromorphic systems. Next steps include integrating the mapping with spiking transformers and liquid state machines (resembling Hopfield Networks) for further evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07361v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Archit Kalra, Midhun Sadanand</dc:creator>
    </item>
  </channel>
</rss>
