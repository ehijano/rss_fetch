<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 03 Apr 2025 04:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>MEEK: Re-thinking Heterogeneous Parallel Error Detection Architecture for Real-World OoO Superscalar Processors</title>
      <link>https://arxiv.org/abs/2504.01347</link>
      <description>arXiv:2504.01347v1 Announce Type: new 
Abstract: Heterogeneous parallel error detection is an approach to achieving fault-tolerant processors, leveraging multiple power-efficient cores to re-execute software originally run on a high-performance core. Yet, its complex components, gathering data cross-chip from many parts of the core, raise questions of how to build it into commodity cores without heavy design invasion and extensive re-engineering.
  We build the first full-RTL design, MEEK, into an open-source SoC, from microarchitecture and ISA to the OS and programming model. We identify and solve bottlenecks and bugs overlooked in previous work, and demonstrate that MEEK offers microsecond-level detection capacity with affordable overheads. By trading off architectural functionalities across codesigned hardware-software layers, MEEK features only light changes to a mature out-of-order superscalar core, simple coordinating software layers, and a few lines of operating-system code. The Repo. of MEEK's source code: https://github.com/SEU-ACAL/reproduce-MEEK-DAC-25.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01347v1</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhe Jiang, Minli Liao, Sam Ainsworth, Dean You, Timothy Jones</dc:creator>
    </item>
    <item>
      <title>HH-PIM: Dynamic Optimization of Power and Performance with Heterogeneous-Hybrid PIM for Edge AI Devices</title>
      <link>https://arxiv.org/abs/2504.01468</link>
      <description>arXiv:2504.01468v1 Announce Type: new 
Abstract: Processing-in-Memory (PIM) architectures offer promising solutions for efficiently handling AI applications in energy-constrained edge environments. While traditional PIM designs enhance performance and energy efficiency by reducing data movement between memory and processing units, they are limited in edge devices due to continuous power demands and the storage requirements of large neural network weights in SRAM and DRAM. Hybrid PIM architectures, incorporating non-volatile memories like MRAM and ReRAM, mitigate these limitations but struggle with a mismatch between fixed computing resources and dynamically changing inference workloads. To address these challenges, this study introduces a Heterogeneous-Hybrid PIM (HH-PIM) architecture, comprising high-performance MRAM-SRAM PIM modules and low-power MRAM-SRAM PIM modules. We further propose a data placement optimization algorithm that dynamically allocates data based on computational demand, maximizing energy efficiency. FPGA prototyping and power simulations with processors featuring HH-PIM and other PIM types demonstrate that the proposed HH-PIM achieves up to $60.43$ percent average energy savings over conventional PIMs while meeting application latency requirements. These results confirm the suitability of HH-PIM for adaptive, energy-efficient AI processing in edge devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01468v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sangmin Jeon, Kangju Lee, Kyeongwon Lee, Woojoo Lee</dc:creator>
    </item>
    <item>
      <title>MERE: Hardware-Software Co-Design for Masking Cache Miss Latency in Embedded Processors</title>
      <link>https://arxiv.org/abs/2504.01582</link>
      <description>arXiv:2504.01582v1 Announce Type: new 
Abstract: Runahead execution is a technique to mask memory latency caused by irregular memory accesses. By pre-executing the application code during occurrences of long-latency operations and prefetching anticipated cache-missed data into the cache hierarchy, runahead effectively masks memory latency for subsequent cache misses and achieves high prefetching accuracy; however, this technique has been limited to superscalar out-of-order and superscalar in-order cores. For implementation in scalar in-order cores, the challenges of area-/energy-constraint and severe cache contention remain.
  Here, we build the first full-stack system featuring runahead, MERE, from SoC and a dedicated ISA to the OS and programming model. Through this deployment, we show that enabling runahead in scalar in-order cores is possible, with minimal area and power overheads, while still achieving high performance. By re-constructing the sequential runahead employing a hardware/software co-design approach, the system can be implemented on a mature processor and SoC. Building on this, an adaptive runahead mechanism is proposed to mitigate the severe cache contention in scalar in-order cores. Combining this, we provide a comprehensive solution for embedded processors managing irregular workloads. Our evaluation demonstrates that the proposed MERE attains 93.5% of a 2-wide out-of-order core's performance while constraining area and power overheads below 5%, with the adaptive runahead mechanism delivering an additional 20.1% performance gain through mitigating the severe cache contention issues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01582v1</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dean You, Jieyu Jiang, Xiaoxuan Wang, Yushu Du, Zhihang Tan, Wenbo Xu, Hui Wang, Jiapeng Guan, Zhenyuan Wang, Ran Wei, Shuai Zhao, Zhe Jiang</dc:creator>
    </item>
    <item>
      <title>A flexible framework for early power and timing comparison of time-multiplexed CGRA kernel executions</title>
      <link>https://arxiv.org/abs/2504.01672</link>
      <description>arXiv:2504.01672v1 Announce Type: new 
Abstract: At the intersection between traditional CPU architectures and more specialized options such as FPGAs or ASICs lies the family of reconfigurable hardware architectures, termed Coarse-Grained Reconfigurable Arrays (CGRAs). CGRAs are composed of a 2-dimensional array of processing elements (PE), tightly integrated with each other, each capable of performing arithmetic and logic operations. The vast design space of CGRA implementations poses a challenge, which calls for fast exploration tools to prune it in advance of time-consuming syntheses. The proposed tool aims to simplify this process by simulating kernel execution and providing a characterization framework. The estimator returns energy and latency values otherwise only available through a time-consuming post-synthesis simulation, allowing for instantaneous comparative analysis between different kernels and hardware configurations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01672v1</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706594.3726977</arxiv:DOI>
      <dc:creator>Maxime Henri Aspros, Juan Sapriza, Giovanni Ansaloni, David Atienza</dc:creator>
    </item>
    <item>
      <title>PIMDAL: Mitigating the Memory Bottleneck in Data Analytics using a Real Processing-in-Memory System</title>
      <link>https://arxiv.org/abs/2504.01948</link>
      <description>arXiv:2504.01948v1 Announce Type: new 
Abstract: Database Management Systems (DBMSs) are crucial for efficient data management and analytics, and are used in several different application domains. Due to the increasing volume of data a DBMS deals with, current processor-centric architectures (e.g., CPUs, GPUs) suffer from data movement bottlenecks when executing key DBMS operations (e.g., selection, aggregation, ordering, and join). This happens mostly due to the limited memory bandwidth between compute and memory resources. Data-centric architectures like Processing-in-Memory (PIM) are a promising alternative for applications bottlenecked by data, placing compute resources close to where data resides. Previous works have evaluated using PIM for data analytics. However, they either do not use real-world architectures or they consider only a subset of the operators used in analytical queries. This work aims to fully evaluate a data-centric approach to data analytics, by using the real-world UPMEM PIM system. To this end we first present the PIM Data Analytics Library (PIMDAL), which implements four major DB operators: selection, aggregation, ordering and join. Second, we use hardware performance metrics to understand which properties of a PIM system are important for a high-performance implementation. Third, we compare PIMDAL to reference implementations on high-end CPU and GPU systems. Fourth, we use PIMDAL to implement five TPC-H queries to gain insights into analytical queries. We analyze and show how to overcome the three main limitations of the UPMEM system when implementing DB operators: (I) low arithmetic performance, (II) explicit memory management and (III) limited communication between compute units. Our evaluation shows PIMDAL achieves 3.9x the performance of a high-end CPU, on average across the five TPC-H queries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01948v1</guid>
      <category>cs.AR</category>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Manos Frouzakis, Juan G\'omez-Luna, Geraldo F. Oliveira, Mohammad Sadrosadati, Onur Mutlu</dc:creator>
    </item>
    <item>
      <title>GigaAPI for GPU Parallelization</title>
      <link>https://arxiv.org/abs/2504.01266</link>
      <description>arXiv:2504.01266v1 Announce Type: cross 
Abstract: GigaAPI is a user-space API that simplifies multi-GPU programming, bridging the gap between the capabilities of parallel GPU systems and the ability of developers to harness their full potential. The API offers a comprehensive set of functionalities, including fundamental GPU operations, image processing, and complex GPU tasks, abstracting away the intricacies of low-level CUDA and C++ programming. GigaAPI's modular design aims to inspire future NVIDIA researchers to create a generalized, dynamic, extensible, and cross-GPU architecture-compatible API. Through experiments and simulations, we demonstrate the general efficiency gains achieved by leveraging GigaAPI's simplified multi-GPU programming model and showcase our learning experience through setup and other aspects, as we were interested in learning complex CUDA programming and parallelism. We hope that this contributes to the democratization of parallel GPU computing, enabling researchers and practitioners to unlock new possibilities across diverse domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01266v1</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <category>cs.PF</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>M. Suvarna, O. Tehrani</dc:creator>
    </item>
    <item>
      <title>FireGuard: A Generalized Microarchitecture for Fine-Grained Security Analysis on OoO Superscalar Cores</title>
      <link>https://arxiv.org/abs/2504.01380</link>
      <description>arXiv:2504.01380v1 Announce Type: cross 
Abstract: High-performance security guarantees rely on hardware support. Generic programmable support for fine-grained instruction analysis has gained broad interest in the literature as a fundamental building block for the security of future processors. Yet, implementation in real out-of-order (OoO) superscalar processors presents tough challenges that cannot be explored in highly abstract simulators. We detail the challenges of implementing complex programmable pathways without critical paths or contention. We then introduce FireGuard, the first implementation of fine-grained instruction analysis on a real OoO superscalar processor. We establish an end-to-end system, including microarchitecture, SoC, ISA and programming model. Experiments show that our solution simultaneously ensures both security and performance of the system, with parallel scalability. We examine the feasibility of building FireGuard into modern SoCs: Apple's M1-Pro, Huawei's Kirin-960, and Intel's i7-12700F, where less than 1% silicon area is introduced. The Repo. of FireGuard's source code: https://github.com/SEU-ACAL/reproduce-FireGuard-DAC-25.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01380v1</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhe Jiang, Sam Ainsworth, Timothy Jones</dc:creator>
    </item>
    <item>
      <title>Versatile silicon integrated photonic processor: a reconfigurable solution for netx-generation AI clusters</title>
      <link>https://arxiv.org/abs/2504.01463</link>
      <description>arXiv:2504.01463v1 Announce Type: cross 
Abstract: The Artificial Intelligence models pose serious challenges in intensive computing and high-bandwidth communication for conventional electronic circuit-based computing clusters. Silicon photonic technologies, owing to their high speed, low latency, large bandwidth, and complementary metal-oxide-semiconductor compatibility, have been widely implemented for data transfer and actively explored as photonic neural networks in AI clusters. However, current silicon photonic integrated chips lack adaptability for multifuncional use and hardware-software systematic coordination. Here, we develop a reconfigurable silicon photonic processor with $40$ programmable unit cells integrating over $160$ component, which, to the best of our knowledge, is the first to realize diverse functions with a chip for AI clusters, from computing acceleration and signal processing to network swtiching and secure encryption. Through a self-developed automated testing, compilation, and tuning framework to the processor without in-network monitoring photodetectors, we implement $4\times4$ dual-direction unitary and $3\times3$ uni-direction non-unitary matrix multiplications, neural networks for image recognition, micro-ring modulator wavelength locking, $4\times4$ photonic channel switching , and silicon photonic physical unclonable functions. This optoelectronic processing system, incorporating the photonic processor and its software stack, paves the way for both advanced photonic system-on-chip design and the construction of photo-electronic AI clusters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01463v1</guid>
      <category>physics.optics</category>
      <category>cs.AR</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ying Zhu, Yifan Liu, Xinyu Yang, Kailai Liu, Xin Hua, Ming Luo, Jia Liu, Siyao Chang, Shengxiang Zhang, Miao Wu, Zhicheng Wang, Hongguang Zhang, Daigao Chen, Xi Xiao, Shaohua Yu</dc:creator>
    </item>
    <item>
      <title>An All-digital 8.6-nJ/Frame 65-nm Tsetlin Machine Image Classification Accelerator</title>
      <link>https://arxiv.org/abs/2501.19347</link>
      <description>arXiv:2501.19347v2 Announce Type: replace-cross 
Abstract: We present an all-digital programmable machine learning accelerator chip for image classification, underpinning on the Tsetlin machine (TM) principles. The TM is an emerging machine learning algorithm founded on propositional logic, utilizing sub-pattern recognition expressions called clauses. The accelerator implements the coalesced TM version with convolution, and classifies booleanized images of 28$\times$28 pixels with 10 categories. A configuration with 128 clauses is used in a highly parallel architecture. Fast clause evaluation is achieved by keeping all clause weights and Tsetlin automata (TA) action signals in registers. The chip is implemented in a 65 nm low-leakage CMOS technology, and occupies an active area of 2.7 mm$^2$. At a clock frequency of 27.8 MHz, the accelerator achieves 60.3k classifications per second, and consumes 8.6 nJ per classification. This demonstrates the energy-efficiency of the TM, which was the main motivation for developing this chip. The latency for classifying a single image is 25.4 $\mu$s which includes system timing overhead. The accelerator achieves 97.42%, 84.54% and 82.55% test accuracies for the datasets MNIST, Fashion-MNIST and Kuzushiji-MNIST, respectively, matching the TM software models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19347v2</guid>
      <category>cs.LG</category>
      <category>cs.AR</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Svein Anders Tunheim, Yujin Zheng, Lei Jiao, Rishad Shafik, Alex Yakovlev, Ole-Christoffer Granmo</dc:creator>
    </item>
  </channel>
</rss>
