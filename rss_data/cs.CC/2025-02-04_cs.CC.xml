<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 05 Feb 2025 02:49:22 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Compilation and Fast Model Counting beyond CNF</title>
      <link>https://arxiv.org/abs/2502.00434</link>
      <description>arXiv:2502.00434v1 Announce Type: new 
Abstract: Circuits in deterministic decomposable negation normal form (d-DNNF) are representations of Boolean functions that enable linear-time model counting. This paper strengthens our theoretical knowledge of what classes of functions can be efficiently transformed, or compiled, into d-DNNF. Our main contribution is the fixed-parameter tractable (FPT) compilation of conjunctions of specific constraints parameterized by incidence treewidth. This subsumes the known result for CNF. The constraints in question are all functions representable by constant-width ordered binary decision diagrams (OBDDs) for all variable orderings. For instance, this includes parity constraints and cardinality constraints with constant threshold. The running time of the FPT compilation is singly exponential in the incidence treewidth but hides large constants in the exponent. To balance that, we give a more efficient FPT algorithm for model counting that applies to a sub-family of the constraints and does not require compilation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00434v1</guid>
      <category>cs.CC</category>
      <category>cs.AI</category>
      <category>cs.LO</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.24963/ijcai.2024/367</arxiv:DOI>
      <dc:creator>Alexis de Colnet, Stefan Szeider, Tianwei Zhang</dc:creator>
    </item>
    <item>
      <title>Some structural complexity results for $\exists\mathbb R$</title>
      <link>https://arxiv.org/abs/2502.00680</link>
      <description>arXiv:2502.00680v1 Announce Type: new 
Abstract: The complexity class $\exists\mathbb R$, standing for the complexity of deciding the existential first order theory of the reals as real closed field in the Turing model, has raised considerable interest in recent years. It is well known that NP $ \subseteq \exists\mathbb R\subseteq$ PSPACE. In their compendium, Schaefer, Cardinal, and Miltzow give a comprehensive presentation of results together with a rich collection of open problems. Here, we answer some of them dealing with structural issues of $\exists\mathbb R$ as a complexity class. We show analogues of the classical results of Baker, Gill, and Solovay finding oracles which do and do not separate NP form $\exists\mathbb R$, of Ladner's theorem showing the existence of problems in $\exists\mathbb R \setminus$ NP not being complete for $\exists\mathbb R$ (in case the two classes are different), as well as a characterization of $\exists\mathbb R$ by means of descriptive complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00680v1</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Klaus Meer, Adrian Wurm</dc:creator>
    </item>
    <item>
      <title>Distribution-Specific Agnostic Conditional Classification With Halfspaces</title>
      <link>https://arxiv.org/abs/2502.00172</link>
      <description>arXiv:2502.00172v1 Announce Type: cross 
Abstract: We study ``selective'' or ``conditional'' classification problems under an agnostic setting. Classification tasks commonly focus on modeling the relationship between features and categories that captures the vast majority of data. In contrast to common machine learning frameworks, conditional classification intends to model such relationships only on a subset of the data defined by some selection rule. Most work on conditional classification either solves the problem in a realizable setting or does not guarantee the error is bounded compared to an optimal solution. In this work, we consider selective/conditional classification by sparse linear classifiers for subsets defined by halfspaces, and give both positive as well as negative results for Gaussian feature distributions. On the positive side, we present the first PAC-learning algorithm for homogeneous halfspace selectors with error guarantee $\bigO*{\sqrt{\mathrm{opt}}}$, where $\mathrm{opt}$ is the smallest conditional classification error over the given class of classifiers and homogeneous halfspaces. On the negative side, we find that, under cryptographic assumptions, approximating the conditional classification loss within a small additive error is computationally hard even under Gaussian distribution. We prove that approximating conditional classification is at least as hard as approximating agnostic classification in both additive and multiplicative form.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00172v1</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jizhou Huang, Brendan Juba</dc:creator>
    </item>
    <item>
      <title>Satisfactory Budget Division</title>
      <link>https://arxiv.org/abs/2502.00484</link>
      <description>arXiv:2502.00484v1 Announce Type: cross 
Abstract: A divisible budget must be allocated to several projects, and agents are asked for their opinion on how much they would give to each project. We consider that an agent is satisfied by a division of the budget if, for at least a certain predefined number $\tau$ of projects, the part of the budget actually allocated to each project is at least as large as the amount the agent requested. The objective is to find a budget division that ``best satisfies'' the agents. In this context, different problems can be stated and we address the following ones. We study $(i)$ the largest proportion of agents that can be satisfied for any instance, $(ii)$ classes of instances admitting a budget division that satisfies all agents, $(iii)$ the complexity of deciding if, for a given instance, every agent can be satisfied, and finally $(iv)$ the question of finding, for a given instance, the smallest total budget to satisfy all agents. We provide answers to these complementary questions for several natural values of the parameter $\tau$, capturing scenarios where we seek to satisfy for each agent all; almost all; half; or at least one of her requests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00484v1</guid>
      <category>cs.GT</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laurent Gourv\`es, Michael Lampis, Nikolaos Melissinos, Aris Pagourtzis</dc:creator>
    </item>
    <item>
      <title>Fair Vertex Problems Parameterized by Cluster Vertex Deletion</title>
      <link>https://arxiv.org/abs/2502.01400</link>
      <description>arXiv:2502.01400v1 Announce Type: cross 
Abstract: We study fair vertex problem metatheorems on graphs within the scope of structural parameterization in parameterized complexity. Unlike typical graph problems that seek the smallest set of vertices satisfying certain properties, the goal here is to find such a set that does not contain too many vertices in any neighborhood of any vertex. Formally, the task is to find a set $X\subseteq V(G)$ of fair cost $k$, i.e., such that for all $v\in V(G)$ $|X\cap N(v)|\le k$. Recently, Knop, Masa\v{r}\'ik, and Toufar [MFCS 2019] showed that all fair MSO$_1$ definable problems can be solved in FPT time parameterized by the twin cover of a graph. They asked whether such a statement would be achievable for a more general parameterization of cluster vertex deletion, i.e., the smallest number of vertices required to be removed from the graph such that what remains is a collection of cliques.
  In this paper, we prove that in full generality this is not possible by demonstrating a W[1]-hardness. On the other hand, we give a sufficient property under which a fair MSO$_1$ definable problem admits an FPT algorithm parameterized by the cluster vertex deletion number. Our algorithmic formulation is very general as it captures the fair variant of many natural vertex problems such as the Fair Feedback Vertex Set, the Fair Vertex Cover, the Fair Dominating Set, the Fair Odd Cycle Transversal, as well as a connected variant of thereof. Moreover, we solve the Fair $[\sigma,\rho]$-Domination problem for $\sigma$ finite, or $\sigma=\mathbb{N}$ and $\rho$ cofinite. Specifically, given finite or cofinite $\rho\subseteq \mathbb{N}$ and finite $\sigma$, or $\rho\subseteq \mathbb{N}$ cofinite and $\sigma=\mathbb{N}$, the task is to find set of vertices $X\subseteq V(G)$ of fair cost at most $k$ such that for all $v\in X$, $|N(v)\cap X|\in\sigma$ and for all $v\in V(G)\setminus X$, $|N(v)\cap X|\in\rho$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01400v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.LO</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tom\'a\v{s} Masa\v{r}\'ik, J\k{e}drzej Olkowski, Anna Zych-Pawlewicz</dc:creator>
    </item>
    <item>
      <title>On the Existence of Algebraic Natural Proofs</title>
      <link>https://arxiv.org/abs/2004.14147</link>
      <description>arXiv:2004.14147v4 Announce Type: replace 
Abstract: The framework of algebraically natural proofs was independently introduced in the works of Forbes, Shpilka and Volk (2018), and Grochow, Kumar, Saks and Saraf (2017), to study the efficacy of commonly used techniques for proving lower bounds in algebraic complexity. We use the known connections between algebraic hardness and pseudorandomness to shed some more light on the question relating to this framework, as follows.
  1. The subclass of $\mathsf{VP}$ that contains polynomial families with bounded coefficients, has efficient equations. Over finite fields, this result holds without any restriction on coefficients. Further, both these results extend to \emph{any} class that admits a low-variate, low-degree universal map: a generator for all polynomials in the class. Most well-studied classes have this property, e.g. \textsf{VNP}, \textsf{VBP}, \textsf{VF}.
  2. Over fields of characteristic zero, $\mathsf{VNP}$ does not have any efficient equations, if the Permanent is exponentially hard for algebraic circuits. Moreover, exponential hardness of the Permanent in the approximative sense, even rules out efficient equations of large degree. This gives the only known barrier to ``natural'' lower bound techniques (that follows from believable hardness assumptions), and also shows that the restriction on coefficients in the first category of results about $\mathsf{VNP}$ is necessary.
  The first set of results follows essentially by algebraizing the well-known method of generating hardness from non-trivial hitting sets (e.g. Heintz and Schnorr 1980). The conditional hardness of equations for $\mathsf{VNP}$ uses the fact that pseudorandomness against a class can be extracted from a polynomial that is (sufficiently) hard for that class (Kabanets and Impagliazzo, 2004).</description>
      <guid isPermaLink="false">oai:arXiv.org:2004.14147v4</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Prerona Chatterjee, Mrinal Kumar, C Ramya, Ramprasad Saptharishi, Anamay Tengse</dc:creator>
    </item>
    <item>
      <title>A proof of P != NP (New symmetric encryption algorithm against any linear attacks and differential attacks)</title>
      <link>https://arxiv.org/abs/2203.05022</link>
      <description>arXiv:2203.05022v2 Announce Type: replace 
Abstract: P vs NP problem is the most important unresolved problem in the field of computational complexity. Its impact has penetrated into all aspects of algorithm design, especially in the field of cryptography. The security of cryptographic algorithms based on short keys depends on whether P is equal to NP. In fact, Shannon[1] strictly proved that the one-time-pad system meets unconditional security, but because the one-time-pad system requires the length of key to be at least the length of plaintext, how to transfer the key is a troublesome problem that restricts the use of the one-time-pad system in practice. Cryptography algorithms used in practice are all based on short key, and the security of the short key mechanism is ultimately based on "one-way" assumption, that is, it is assumed that a one-way function exists. In fact, the existence of one-way function can directly lead to the important conclusion P != NP. In this paper, we originally constructed a short-key block cipher algorithm. The core feature of this algorithm is that for any block, when a plaintext-ciphertext pair is known, any key in the key space can satisfy the plaintext-ciphertext pair, that is, for each block, the plaintext-ciphertext pair and the key are independence, and the independence between blocks is also easy to construct. This feature is completely different from all existing short-key cipher algorithms. Based on the above feature, we construct a problem and theoretically prove that the problem satisfies the properties of one-way functions, thereby solving the problem of the existence of one-way functions, that is, directly proving that P != NP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.05022v2</guid>
      <category>cs.CC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Gao Ming</dc:creator>
    </item>
    <item>
      <title>One-Way Functions and Polynomial Time Dimension</title>
      <link>https://arxiv.org/abs/2411.02392</link>
      <description>arXiv:2411.02392v2 Announce Type: replace 
Abstract: Polynomial-time dimension (denoted $\mathrm{cdim}_{P}$) quantifies the density of information of infinite sequences using polynomial time betting algorithms called $s$-gales. An alternate quantification of the notion of polynomial time density of information is using polynomial-time Kolmogorov complexity rate (denoted $\mathcal{K}_\text{poly}$). The corresponding unbounded notions, namely, the constructive dimension and unbounded Kolmogorov complexity rates are equal for every sequence. Analogous notions are equal even at finite-state level. In view of this, it is reasonable to conjecture that $\mathrm{cdim}_{P}$ and $\mathcal{K}_\text{poly}$ are identical notions.
  In this paper we demonstrate that surprisingly, $\mathrm{cdim}_{P}$ and $\mathcal{K}_\text{poly}$ are distinct measures of information density if and only if one-way functions exist. We consider polynomial time samplable distributions over $\Sigma^\infty$ that uses short seeds to sample a finite string $w \in \Sigma^n$. We establish the following results. We first show that if one-way functions exist then there exist a polynomial time samplable distribution with respect to which $\mathrm{cdim}_{P}$ and $\mathcal{K}_\text{poly}$ are separated by a uniform gap with probability $1$. Conversely, we show that if there exists such a polynomial time samplable distribution, then infinitely-often one-way functions exist. Hence, we provide a new information theoretic characterisation of the existence of one-way functions.
  Using this new characterization, we solve an open problem posed by Hitchcock and Vinodchandran (CCC 2004) and Stull \cite{stullsurvey}. We demonstrate that if one-way functions exist, then there are individual sequences $X$ whose poly-time dimension strictly exceeds $\mathcal{K}_\text{poly}(X)$, that is $\mathrm{cdim}_{P}(X) &gt; \mathcal{K}_\text{poly}(X)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02392v2</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Satyadev Nandakumar, Subin Pulari, Akhil S, Suronjona Sarma</dc:creator>
    </item>
    <item>
      <title>Inverse Intersections for Boolean Satisfiability Problems</title>
      <link>https://arxiv.org/abs/2501.03281</link>
      <description>arXiv:2501.03281v2 Announce Type: replace 
Abstract: Boolean Satisfiability (SAT) problems are expressed as mathematical formulas. This paper presents an alternative matrix representation for any type of these SAT problems. It shows how to use this matrix representation to get the full set of valid assignments. It proves that this is the full set of answers for the given problem, and it shows that this is exponential in size, relative to the matrix. It then presents an algorithm that utilizes the inverses of the clauses in this matrix for faster searching through this set of answers. It shows that this algorithm is both correct and polynomial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03281v2</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul W. Homer</dc:creator>
    </item>
    <item>
      <title>Statistical and Computational Guarantees of Kernel Max-Sliced Wasserstein Distances</title>
      <link>https://arxiv.org/abs/2405.15441</link>
      <description>arXiv:2405.15441v3 Announce Type: replace-cross 
Abstract: Optimal transport has been very successful for various machine learning tasks; however, it is known to suffer from the curse of dimensionality. Hence, dimensionality reduction is desirable when applied to high-dimensional data with low-dimensional structures. The kernel max-sliced (KMS) Wasserstein distance is developed for this purpose by finding an optimal nonlinear mapping that reduces data into $1$ dimension before computing the Wasserstein distance. However, its theoretical properties have not yet been fully developed. In this paper, we provide sharp finite-sample guarantees under milder technical assumptions compared with state-of-the-art for the KMS $p$-Wasserstein distance between two empirical distributions with $n$ samples for general $p\in[1,\infty)$. Algorithm-wise, we show that computing the KMS $2$-Wasserstein distance is NP-hard, and then we further propose a semidefinite relaxation (SDR) formulation (which can be solved efficiently in polynomial time) and provide a relaxation gap for the obtained solution. We provide numerical examples to demonstrate the good performance of our scheme for high-dimensional two-sample testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15441v3</guid>
      <category>stat.ML</category>
      <category>cs.CC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jie Wang, March Boedihardjo, Yao Xie</dc:creator>
    </item>
    <item>
      <title>Computational Complexity-Constrained Spectral Efficiency Analysis for 6G Waveforms</title>
      <link>https://arxiv.org/abs/2407.05805</link>
      <description>arXiv:2407.05805v3 Announce Type: replace-cross 
Abstract: In this work, we present a tutorial on how to account for the computational time complexity overhead of signal processing in the spectral efficiency (SE) analysis of wireless waveforms. Our methodology is particularly relevant in scenarios where achieving higher SE entails a penalty in complexity, a common trade-off present in 6G candidate waveforms. We consider that SE derives from the data rate, which is impacted by time-dependent overheads. Thus, neglecting the computational complexity overhead in the SE analysis grants an unfair advantage to more computationally complex waveforms, as they require larger computational resources to meet a signal processing runtime below the symbol period. We demonstrate our points with two case studies. In the first, we refer to IEEE 802.11a-compliant baseband processors from the literature to show that their runtime significantly impacts the SE perceived by upper layers. In the second case study, we show that waveforms considered less efficient in terms of SE can outperform their more computationally expensive counterparts if provided with equivalent high-performance computational resources. Based on these cases, we believe our tutorial can address the comparative SE analysis of waveforms that operate under different computational resource constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05805v3</guid>
      <category>eess.SP</category>
      <category>cs.CC</category>
      <category>cs.PF</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saulo Queiroz, Jo\~ao P. Vilela, Benjamin Koon Kei Ng, Chan-Tong Lam, Edmundo Monteiro</dc:creator>
    </item>
    <item>
      <title>On the Tractability Landscape of the Conditional Minisum Approval Voting Rule</title>
      <link>https://arxiv.org/abs/2412.09005</link>
      <description>arXiv:2412.09005v2 Announce Type: replace-cross 
Abstract: This work examines the Conditional Approval Framework for elections involving multiple interdependent issues, specifically focusing on the Conditional Minisum Approval Voting Rule. We first conduct a detailed analysis of the computational complexity of this rule, demonstrating that no approach can significantly outperform the brute-force algorithm under common computational complexity assumptions and various natural input restrictions. In response, we propose two practical restrictions (the first in the literature) that make the problem computationally tractable and show that these restrictions are essentially tight. Overall, this work provides a clear picture of the tractability landscape of the problem, contributing to a comprehensive understanding of the complications introduced by conditional ballots and indicating that conditional approval voting can be applied in practice, albeit under specific conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09005v2</guid>
      <category>cs.GT</category>
      <category>cs.CC</category>
      <category>cs.MA</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.ipl.2025.106561</arxiv:DOI>
      <dc:creator>Georgios Amanatidis, Michael Lampis, Evangelos Markakis, Georgios Papasotiropoulos</dc:creator>
    </item>
    <item>
      <title>On Computational Limits and Provably Efficient Criteria of Visual Autoregressive Models: A Fine-Grained Complexity Analysis</title>
      <link>https://arxiv.org/abs/2501.04377</link>
      <description>arXiv:2501.04377v2 Announce Type: replace-cross 
Abstract: Recently, Visual Autoregressive ($\mathsf{VAR}$) Models introduced a groundbreaking advancement in the field of image generation, offering a scalable approach through a coarse-to-fine ``next-scale prediction'' paradigm. Suppose that $n$ represents the height and width of the last VQ code map generated by $\mathsf{VAR}$ models, the state-of-the-art algorithm in [Tian, Jiang, Yuan, Peng and Wang, NeurIPS 2024] takes $O(n^{4+o(1)})$ time, which is computationally inefficient. In this work, we analyze the computational limits and efficiency criteria of $\mathsf{VAR}$ Models through a fine-grained complexity lens. Our key contribution is identifying the conditions under which $\mathsf{VAR}$ computations can achieve sub-quadratic time complexity. We have proved that assuming the Strong Exponential Time Hypothesis ($\mathsf{SETH}$) from fine-grained complexity theory, a sub-quartic time algorithm for $\mathsf{VAR}$ models is impossible. To substantiate our theoretical findings, we present efficient constructions leveraging low-rank approximations that align with the derived criteria. This work initiates the study of the computational efficiency of the $\mathsf{VAR}$ model from a theoretical perspective. Our technique will shed light on advancing scalable and efficient image generation in $\mathsf{VAR}$ frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04377v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CC</category>
      <category>cs.CV</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yekun Ke, Xiaoyu Li, Yingyu Liang, Zhizhou Sha, Zhenmei Shi, Zhao Song</dc:creator>
    </item>
    <item>
      <title>Neural Algorithmic Reasoning for Hypergraphs with Looped Transformers</title>
      <link>https://arxiv.org/abs/2501.10688</link>
      <description>arXiv:2501.10688v2 Announce Type: replace-cross 
Abstract: Looped Transformers have shown exceptional neural algorithmic reasoning capability in simulating traditional graph algorithms, but their application to more complex structures like hypergraphs remains underexplored. Hypergraphs generalize graphs by modeling higher-order relationships among multiple entities, enabling richer representations but introducing significant computational challenges. In this work, we extend the Loop Transformer architecture's neural algorithmic reasoning capability to simulate hypergraph algorithms, addressing the gap between neural networks and combinatorial optimization over hypergraphs. Specifically, we propose a novel degradation mechanism for reducing hypergraphs to graph representations, enabling the simulation of graph-based algorithms, such as Dijkstra's shortest path. Furthermore, we introduce a hyperedge-aware encoding scheme to simulate hypergraph-specific algorithms, exemplified by Helly's algorithm. We establish theoretical guarantees for these simulations, demonstrating the feasibility of processing high-dimensional and combinatorial data using Loop Transformers. This work highlights the potential of Transformers as general-purpose algorithmic solvers for structured data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10688v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CC</category>
      <category>cs.CL</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoyu Li, Yingyu Liang, Jiangxuan Long, Zhenmei Shi, Zhao Song, Zhen Zhuang</dc:creator>
    </item>
    <item>
      <title>Separating complexity classes of LCL problems on grids</title>
      <link>https://arxiv.org/abs/2501.17445</link>
      <description>arXiv:2501.17445v2 Announce Type: replace-cross 
Abstract: We study the complexity of locally checkable labeling (LCL) problems on $\mathbb{Z}^n$ from the point of view of descriptive set theory, computability theory, and factors of i.i.d. Our results separate various complexity classes that were not previously known to be distinct and serve as counterexamples to a number of natural conjectures in the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17445v2</guid>
      <category>math.LO</category>
      <category>cs.CC</category>
      <category>math.CO</category>
      <category>math.PR</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Katalin Berlow, Anton Bernshteyn, Clark Lyons, Felix Weilacher</dc:creator>
    </item>
  </channel>
</rss>
