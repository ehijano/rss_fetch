<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 05 Sep 2025 04:00:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Combining Performance and Productivity: Accelerating the Network Sensing Graph Challenge with GPUs and Commodity Data Science Software</title>
      <link>https://arxiv.org/abs/2509.03653</link>
      <description>arXiv:2509.03653v1 Announce Type: new 
Abstract: The HPEC Graph Challenge is a collection of benchmarks representing complex workloads that test the hardware and software components of HPC systems, which traditional benchmarks, such as LINPACK, do not. The first benchmark, Subgraph Isomorphism, focused on several compute-bound and memory-bound kernels. The most recent of the challenges, the Anonymized Network Sensing Graph Challenge, represents a shift in direction, as it represents a longer end-to-end workload that requires many more software components, including, but not limited to, data I/O, data structures for representing graph data, and a wide range of functions for data preparation and network analysis. A notable feature of this new graph challenge is the use of GraphBLAS to represent the computational aspects of the problem statement. In this paper, we show an alternative interpretation of the GraphBLAS formulations using the language of data science. With this formulation, we show that the new graph challenge can be implemented using off-the-shelf ETL tools available in open-source, enterprise software such as NVIDIA's RAPIDS ecosystem. Using off-the-shelf software, RAPIDS cuDF and cupy, we enable significant software acceleration without requiring any specific HPC code and show speedups, over the same code running with Pandas on the CPU, of 147x-509x on an NVIDIA A100 GPU, 243x-1269X for an NVIDIA H100 GPU, and 332X-2185X for an NVIDIA H200 GPU.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03653v1</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siddharth Samsi, Dan Campbell, Emanuel Scoullos, Oded Green</dc:creator>
    </item>
    <item>
      <title>Distributed Download from an External Data Source in Asynchronous Faulty Settings</title>
      <link>https://arxiv.org/abs/2509.03755</link>
      <description>arXiv:2509.03755v1 Announce Type: new 
Abstract: The distributedData Retrieval (DR) model consists of $k$ peers connected by a complete peer-to-peer communication network, and a trusted external data source that stores an array $\textbf{X}$ of $n$ bits ($n \gg k$). Up to $\beta k$ of the peers might fail in any execution (for $\beta \in [0, 1)$). Peers can obtain the information either by inexpensive messages passed among themselves or through expensive queries to the source array $\textbf{X}$. In the DR model, we focus on designing protocols that minimize the number of queries performed by any nonfaulty peer (a measure referred to as query complexity) while maximizing the resilience parameter $\beta$.
  The Download problem requires each nonfaulty peer to correctly learn the entire array $\textbf{X}$. Earlier work on this problem focused on synchronous communication networks and established several deterministic and randomized upper and lower bounds. Our work is the first to extend the study of distributed data retrieval to asynchronous communication networks. We address the Download problem under both the Byzantine and crash failure models. We present query-optimal deterministic solutions in an asynchronous model that can tolerate any fixed fraction $\beta&lt;1$ of crash faults. In the Byzantine failure model, it is known that deterministic protocols incur a query complexity of $\Omega(n)$ per peer, even under synchrony. We extend this lower bound to randomized protocols in the asynchronous model for $\beta \geq 1/2$, and further show that for $\beta &lt; 1/2$, a randomized protocol exists with near-optimal query complexity. To the best of our knowledge, this is the first work to address the Download problem in asynchronous communication networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03755v1</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>John Augustine, Soumyottam Chatterjee, Valerie King, Manish Kumar, Shachar Meir, David Peleg</dc:creator>
    </item>
    <item>
      <title>Gathering of asynchronous robots on circle with limited visibility using finite communication</title>
      <link>https://arxiv.org/abs/2509.04004</link>
      <description>arXiv:2509.04004v1 Announce Type: new 
Abstract: This work addresses the gathering problem for a set of autonomous, anonymous, and homogeneous robots with limited visibility operating in a continuous circle. The robots are initially placed at distinct positions, forming a rotationally asymmetric configuration. The robots agree on the clockwise direction. In the $\theta$-visibility model, a robot can only see those robots on the circle that are at an angular distance $&lt;\theta$ from it. Di Luna \textit{et. al.} [DISC'20] have shown that, in $\pi/2$ visibility, gathering is impossible. In addition, they provided an algorithm for robots with $\pi$ visibility, operating under a semi-synchronous scheduler. In the $\pi$ visibility model, only one point, the point at the angular distance $\pi$ is removed from the visibility. Ghosh \textit{et. al.} [SSS'23] provided a gathering algorithm for $\pi$ visibility model with robot having finite memory ($\mathcal{FSTA}$), operating under a special asynchronous scheduler.
  If the robots can see all points on the circle, then the gathering can be done by electing a leader in the weakest robot model under a fully asynchronous scheduler. However, previous works have shown that even the removal of one point from the visibility makes gathering difficult. In both works, the robots had rigid movement. In this work, we propose an algorithm that solves the gathering problem under the $\pi$-visibility model for robots that have finite communication ability ($\mathcal{FCOM}$). In this work the robot movement is non-rigid and the robots work under a fully asynchronous scheduler.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04004v1</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Avisek Sharma, Satakshi Ghosh, Buddhadeb Sau</dc:creator>
    </item>
    <item>
      <title>Counterfactual simulations for large scale systems with burnout variables</title>
      <link>https://arxiv.org/abs/2509.04038</link>
      <description>arXiv:2509.04038v1 Announce Type: new 
Abstract: We consider large-scale systems influenced by burnout variables - state variables that start active, shape dynamics, and irreversibly deactivate once certain conditions are met. Simulating what-if scenarios in such systems is computationally demanding, as alternative trajectories often require sequential processing, which does not scale very well. This challenge arises in settings like online advertising, because of campaigns budgets, complicating counterfactual analysis despite rich data availability. We introduce a new type of algorithms based on what we refer to as uncertainty relaxation, that enables efficient parallel computation, significantly improving scalability for counterfactual estimation in systems with burnout variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04038v1</guid>
      <category>cs.DC</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Heymann</dc:creator>
    </item>
    <item>
      <title>LowDiff: Efficient Frequent Checkpointing via Low-Cost Differential for High-Performance Distributed Training Systems</title>
      <link>https://arxiv.org/abs/2509.04084</link>
      <description>arXiv:2509.04084v1 Announce Type: new 
Abstract: Distributed training of large deep-learning models often leads to failures, so checkpointing is commonly employed for recovery. State-of-the-art studies focus on frequent checkpointing for fast recovery from failures. However, it generates numerous checkpoints, incurring substantial costs and thus degrading training performance. Recently, differential checkpointing has been proposed to reduce costs, but it is limited to recommendation systems, so its application to general distributed training systems remains unexplored.
  This paper proposes LowDiff, an efficient frequent checkpointing framework that \textit{reuses} compressed gradients, serving as differential checkpoints to reduce cost. Furthermore, LowDiff incorporates a batched gradient write optimization to persist these differentials to storage efficiently. It also dynamically tunes both the checkpoint frequency and the batching size to maximize performance. We further enhance LowDiff with a layer-wise gradient reusing and snapshotting approach and a CPU-based asynchronous persistence strategy, enabling frequent checkpointing without gradient compression. Experiments on various workloads show that LowDiff can achieve checkpointing frequency up to per iteration with less than 3.1\% runtime overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04084v1</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenxuan Yao, Yuchong Hu, Feifan Liu, Zhengyu Liu, Dan Feng</dc:creator>
    </item>
    <item>
      <title>Trustworthy Second-hand Marketplace for Built Environment</title>
      <link>https://arxiv.org/abs/2509.04085</link>
      <description>arXiv:2509.04085v1 Announce Type: new 
Abstract: The construction industry faces significant challenges regarding material waste and sustainable practices, necessitating innovative solutions that integrate automation, traceability, and decentralised decision-making to enable efficient material reuse. This paper presents a blockchain-enabled digital marketplace for sustainable construction material reuse, ensuring transparency and traceability using InterPlanetary File System (IPFS). The proposed framework enhances trust and accountability in material exchange, addressing key challenges in industrial automation and circular supply chains. A framework has been developed to demonstrate the operational processes of the marketplace, illustrating its practical application and effectiveness. Our contributions show how the marketplace can facilitate the efficient and trustworthy exchange of reusable materials, representing a substantial step towards more sustainable construction practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04085v1</guid>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stanly Wilson, Kwabena Adu-Duodu, Yinhao Li, Ringo Sham, Yingli Wang, Ellis Solaiman, Charith Perera, Rajiv Ranjan, Omer Rana</dc:creator>
    </item>
    <item>
      <title>On the impact of unlimited computational power in OBLOT: consequences for synchronous robots on graphs</title>
      <link>https://arxiv.org/abs/2509.04383</link>
      <description>arXiv:2509.04383v1 Announce Type: new 
Abstract: The OBLOT model has been extensively studied in theoretical swarm robotics. It assumes weak capabilities for the involved mobile robots, such as they are anonymous, disoriented, no memory of past events (oblivious), and silent. Their only means of (implicit) communication is transferred to their positioning, i.e., stigmergic information. These limited capabilities make the design of distributed algorithms a challenging task. Over the last two decades, numerous research papers have addressed the question of which tasks can be accomplished within this model. Nevertheless, as it usually happens in distributed computing, also in OBLOT the computational power available to the robots is neglected as the main cost measures for the designed algorithms refer to the number of movements or the number of rounds required. In this paper, we prove that for synchronous robots moving on finite graphs, the unlimited computational power (other than finite time) has a significant impact. In fact, by exploiting it, we provide a definitive resolution algorithm that applies to a wide class of problems while guaranteeing the minimum number of moves and rounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04383v1</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Serafino Cicerone, Alessia Di Fonso, Gabriele Di Stefano, Alfredo Navarra</dc:creator>
    </item>
    <item>
      <title>Semi-decentralized Federated Time Series Prediction with Client Availability Budgets</title>
      <link>https://arxiv.org/abs/2509.03660</link>
      <description>arXiv:2509.03660v1 Announce Type: cross 
Abstract: Federated learning (FL) effectively promotes collaborative training among distributed clients with privacy considerations in the Internet of Things (IoT) scenarios. Despite of data heterogeneity, FL clients may also be constrained by limited energy and availability budgets. Therefore, effective selection of clients participating in training is of vital importance for the convergence of the global model and the balance of client contributions. In this paper, we discuss the performance impact of client availability with time-series data on federated learning. We set up three different scenarios that affect the availability of time-series data and propose FedDeCAB, a novel, semi-decentralized client selection method applying probabilistic rankings of available clients. When a client is disconnected from the server, FedDeCAB allows obtaining partial model parameters from the nearest neighbor clients for joint optimization, improving the performance of offline models and reducing communication overhead. Experiments based on real-world large-scale taxi and vessel trajectory datasets show that FedDeCAB is effective under highly heterogeneous data distribution, limited communication budget, and dynamic client offline or rejoining.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03660v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunkai Bao, Reza Safarzadeh, Xin Wang, Steve Drew</dc:creator>
    </item>
    <item>
      <title>Prob-GParareal: A Probabilistic Numerical Parallel-in-Time Solver for Differential Equations</title>
      <link>https://arxiv.org/abs/2509.03945</link>
      <description>arXiv:2509.03945v1 Announce Type: cross 
Abstract: We introduce Prob-GParareal, a probabilistic extension of the GParareal algorithm designed to provide uncertainty quantification for the Parallel-in-Time (PinT) solution of (ordinary and partial) differential equations (ODEs, PDEs). The method employs Gaussian processes (GPs) to model the Parareal correction function, as GParareal does, further enabling the propagation of numerical uncertainty across time and yielding probabilistic forecasts of system's evolution. Furthermore, Prob-GParareal accommodates probabilistic initial conditions and maintains compatibility with classical numerical solvers, ensuring its straightforward integration into existing Parareal frameworks. Here, we first conduct a theoretical analysis of the computational complexity and derive error bounds of Prob-GParareal. Then, we numerically demonstrate the accuracy and robustness of the proposed algorithm on five benchmark ODE systems, including chaotic, stiff, and bifurcation problems. To showcase the flexibility and potential scalability of the proposed algorithm, we also consider Prob-nnGParareal, a variant obtained by replacing the GPs in Parareal with the nearest-neighbors GPs, illustrating its increased performance on an additional PDE example. This work bridges a critical gap in the development of probabilistic counterparts to established PinT methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03945v1</guid>
      <category>stat.CO</category>
      <category>cs.DC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guglielmo Gattiglio, Lyudmila Grigoryeva, Massimiliano Tamborrino</dc:creator>
    </item>
    <item>
      <title>Cloud-Assisted Remote Control for Aerial Robots: From Theory to Proof-of-Concept Implementation</title>
      <link>https://arxiv.org/abs/2509.04095</link>
      <description>arXiv:2509.04095v1 Announce Type: cross 
Abstract: Cloud robotics has emerged as a promising technology for robotics applications due to its advantages of offloading computationally intensive tasks, facilitating data sharing, and enhancing robot coordination. However, integrating cloud computing with robotics remains a complex challenge due to network latency, security concerns, and the need for efficient resource management. In this work, we present a scalable and intuitive framework for testing cloud and edge robotic systems. The framework consists of two main components enabled by containerized technology: (a) a containerized cloud cluster and (b) the containerized robot simulation environment. The system incorporates two endpoints of a User Datagram Protocol (UDP) tunnel, enabling bidirectional communication between the cloud cluster container and the robot simulation environment, while simulating realistic network conditions. To achieve this, we consider the use case of cloud-assisted remote control for aerial robots, while utilizing Linux-based traffic control to introduce artificial delay and jitter, replicating variable network conditions encountered in practical cloud-robot deployments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04095v1</guid>
      <category>cs.RO</category>
      <category>cs.DC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/CCGridW65158.2025.00032</arxiv:DOI>
      <arxiv:journal_reference>2025 IEEE 25th International Symposium on Cluster, Cloud and Internet Computing Workshops (CCGridW)</arxiv:journal_reference>
      <dc:creator>Achilleas Santi Seisa, Viswa Narayanan Sankaranarayanan, Gerasimos Damigos, Sumeet Gajanan Satpute, George Nikolakopoulos</dc:creator>
    </item>
    <item>
      <title>Massively-Parallel Implementation of Inextensible Elastic Rods Using Inter-block GPU Synchronization</title>
      <link>https://arxiv.org/abs/2509.04277</link>
      <description>arXiv:2509.04277v1 Announce Type: cross 
Abstract: An elastic rod is a long and thin body able to sustain large global deformations, even if local strains are small. The Cosserat rod is a non-linear elastic rod with an oriented centreline, which enables modelling of bending, stretching and twisting deformations. It can be used for physically-based computer simulation of threads, wires, ropes, as well as flexible surgical instruments such as catheters, guidewires or sutures. We present a massively-parallel implementation of the original CoRdE model as well as our inextensible variation. By superseding the CUDA Scalable Programming Model and using inter-block synchronization, we managed to simulate multiple physics time-steps per single kernel launch utilizing all the GPU's streaming multiprocessors. Under some constraints, this results in nearly constant computation time, regardless of the number of Cosserat elements simulated. When executing 10 time-steps per single kernel launch, our implementation of the original, extensible CoRdE was x40.0 faster. In a number of tests, the GPU implementation of our inextensible CoRdE modification achieved an average speed-up of x15.11 over the corresponding CPU version. Simulating a catheter/guidewire pair (2x512 Cosserat elements) in a cardiovascular application resulted in a 13.5 fold performance boost, enabling for accurate real-time simulation at haptic interactive rates (0.5-1kHz).</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04277v1</guid>
      <category>cs.GR</category>
      <category>cs.DC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Przemyslaw Korzeniowski, Niels Hald, Fernando Bello</dc:creator>
    </item>
    <item>
      <title>ConServe: Fine-Grained GPU Harvesting for LLM Online and Offline Co-Serving</title>
      <link>https://arxiv.org/abs/2410.01228</link>
      <description>arXiv:2410.01228v2 Announce Type: replace 
Abstract: Large language model (LLM) serving demands low latency and high throughput, but high load variability makes it challenging to achieve high GPU utilization. In this paper, we identify a synergetic but overlooked opportunity to co-serve latency-critical online requests alongside latency-tolerant offline tasks such as model benchmarking. While promising, existing serving systems fail to co-serve them efficiently, as their coarse-grained resource management at the request or iteration level cannot harvest millisecond-level GPU idle cycles without introducing interference that violates online latency objectives. ConServe is a new LLM co-serving system that achieves high throughput and strong online latency guarantees by managing resources at finer granularities. ConServe introduces three techniques: (1) a latency-aware token-level scheduler that precisely sizes offline batches and tokens to fit within online latency objectives; (2) sub-iteration, layer-wise preemption that allows offline tasks to yield to online load spikes; and (3) incremental KV cache management that enables preempting and resuming offline requests at near-zero cost. Evaluations with Llama-3.1 and Qwen-2.5 models on real-world workloads show that ConServe delivers an average of 2.2$\times$ higher throughput and reduces online serving tail latency by 2.9$\times$ on average compared to state-of-the-art systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01228v2</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifan Qiao, Shu Anzai, Shan Yu, Haoran Ma, Shuo Yang, Yang Wang, Miryung Kim, Yongji Wu, Yang Zhou, Jiarong Xing, Joseph E. Gonzalez, Ion Stoica, Harry Xu</dc:creator>
    </item>
    <item>
      <title>Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and Multiple Level Analysis</title>
      <link>https://arxiv.org/abs/2501.12084</link>
      <description>arXiv:2501.12084v2 Announce Type: replace 
Abstract: This study presents a comprehensive multi-level analysis of the NVIDIA Hopper GPU architecture, focusing on its performance characteristics and novel features. We benchmark Hopper's memory subsystem, highlighting improvements in the L2 partitioned cache and global memory access compared to Ampere and Ada Lovelace. The evaluation of Hopper's fourth-generation tensor cores reveals the benefits of FP8 precision and asynchronous wgmma instructions for matrix operations. Additionally, we investigate the performance of DPX instructions for dynamic programming, distributed shared memory (DSM) for inter-SM communication, and the Tensor Memory Accelerator (TMA) for asynchronous data movement. Through multi-level evaluation, we discover that the Hopper architecture demonstrates significant acceleration potential in real-world applications. For instance, the asynchronous programming model supported by TMA achieves a 1.5x speedup in matrix multiplication, FP8 delivers nearly double the performance of FP16, and DPX instructions accelerate a computational biology algorithm by at least 4.75x. Our findings provide actionable insights for optimizing compute-intensive workloads, from AI training to bioinformatics, on Hopper GPUs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12084v2</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <category>cs.PF</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weile Luo, Ruibo Fan, Zeyu Li, Dayou Du, Hongyuan Liu, Qiang Wang, Xiaowen Chu</dc:creator>
    </item>
    <item>
      <title>Evaluating the Efficacy of LLM-Based Reasoning for Multiobjective HPC Job Scheduling</title>
      <link>https://arxiv.org/abs/2506.02025</link>
      <description>arXiv:2506.02025v2 Announce Type: replace 
Abstract: High-Performance Computing (HPC) job scheduling involves balancing conflicting objectives such as minimizing makespan, reducing wait times, optimizing resource use, and ensuring fairness. Traditional methods, including heuristic-based, e.g., First-Come-First-Served (FJFS) and Shortest Job First (SJF), or intensive optimization techniques, often lack adaptability to dynamic workloads and, more importantly, cannot simultaneously optimize multiple objectives in HPC systems. To address this, we propose a novel Large Language Model (LLM)-based scheduler using a ReAct-style framework (Reason + Act), enabling iterative, interpretable decision-making. The system incorporates a scratchpad memory to track scheduling history and refine decisions via natural language feedback, while a constraint enforcement module ensures feasibility and safety. We evaluate our approach using OpenAI's O4-Mini and Anthropic's Claude 3.7 across seven real-world HPC workload scenarios, including heterogeneous mixes, bursty patterns, and adversarial cases etc. Comparisons against FCFS, SJF, and Google OR-Tools (on 10 to 100 jobs) reveal that LLM-based scheduling effectively balances multiple objectives while offering transparent reasoning through natural language traces. The method excels in constraint satisfaction and adapts to diverse workloads without domain-specific training. However, a trade-off between reasoning quality and computational overhead challenges real-time deployment. This work presents the first comprehensive study of reasoning-capable LLMs for HPC scheduling, demonstrating their potential to handle multiobjective optimization while highlighting limitations in computational efficiency. The findings provide insights into leveraging advanced language models for complex scheduling problems in dynamic HPC environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02025v2</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Prachi Jadhav, Hongwei Jin, Ewa Deelman, Prasanna Balaprakash</dc:creator>
    </item>
    <item>
      <title>Skipper: Maximal Matching with a Single Pass over Edges</title>
      <link>https://arxiv.org/abs/2507.04420</link>
      <description>arXiv:2507.04420v3 Announce Type: replace 
Abstract: Maximal Matching (MM) is a fundamental graph problem with diverse applications. However, state-of-the-art parallel MM algorithms are limited by their need to process graph edges repeatedly over multiple iterations. Furthermore, optimized algorithms often require additional memory for graph contraction or edge filtering. In this paper, we introduce Skipper, an incremental asynchronous MM algorithm that (i) processes each edge deterministically and only once, (ii) skips a large fraction of edges during processing, and (iii) minimizes memory space utilization. Notably, Skipper requires (a) a single pass over the edges, and (b) only a single byte of memory space per vertex. Our evaluation of Skipper, using both real-world and synthetic graphs with up to 161 billion edges, and across three different computer architectures, shows that Skipper processes only 1.2% of the edges and delivers a 47.1 times average speedup (geometric mean). Moreover, Skipper's output quality is highly competitive, with an average size of 88.6% relative to the output of the Lim-Chung algorithm as a state-of-the-art MM algorithm with the largest output size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04420v3</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohsen Koohi Esfahani</dc:creator>
    </item>
    <item>
      <title>Understanding the Landscape of Ampere GPU Memory Errors</title>
      <link>https://arxiv.org/abs/2508.03513</link>
      <description>arXiv:2508.03513v2 Announce Type: replace 
Abstract: Graphics Processing Units (GPUs) have become a de facto solution for accelerating high-performance computing (HPC) applications. Understanding their memory error behavior is an essential step toward achieving efficient and reliable HPC systems. In this work, we present a large-scale cross-supercomputer study to characterize GPU memory reliability, covering three supercomputers - Delta, Polaris, and Perlmutter - all equipped with NVIDIA A100 GPUs. We examine error logs spanning 67.77 million GPU device-hours across 10,693 GPUs. We compare error rates and mean-time-between-errors (MTBE) and highlight both shared and distinct error characteristics among these three systems. Based on these observations and analyses, we discuss the implications and lessons learned, focusing on the reliable operation of supercomputers, the choice of checkpointing interval, and the comparison of reliability characteristics with those of previous-generation GPUs. Our characterization study provides valuable insights into fault-tolerant HPC system design and operation, enabling more efficient execution of HPC applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03513v2</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhu Zhu, Yu Sun, Dhatri Parakal, Bo Fang, Steven Farrell, Gregory H. Bauer, Brett Bode, Ian T. Foster, Michael E. Papka, William Gropp, Zhao Zhang, Lishan Yang</dc:creator>
    </item>
    <item>
      <title>Elastic Restaking Networks</title>
      <link>https://arxiv.org/abs/2503.00170</link>
      <description>arXiv:2503.00170v4 Announce Type: replace-cross 
Abstract: Many blockchain-based decentralized services require their validators (operators) to deposit stake (collateral), which is forfeited (slashed) if they misbehave. Restaking networks let validators secure multiple services by reusing stake. These networks have quickly gained traction, leveraging over \$20 billion in stake. However, restaking introduces a new attack vector where validators can coordinate to misbehave across multiple services simultaneously, extracting digital assets while forfeiting their stake only once.
  Previous work focused either on preventing coordinated misbehavior or on protecting services if all other services are Byzantine and might unjustly cause slashing due to bugs or malice. The first model overlooks how a single Byzantine service can collapse the network, while the second ignores shared-stake benefits.
  To bridge the gap, we analyze the system as a strategic game of coordinated misbehavior, when a given fraction of the services are Byzantine. We introduce elastic restaking networks, where validators can allocate portions of their stake that may cumulatively exceed their total stake, and when allocations are lost, the remaining stake stretches to cover remaining allocations. We show that elastic networks exhibit superior robustness compared to previous approaches, and demonstrate a synergistic effect where an elastic restaking network enhances its blockchain's security, contrary to community concerns of an opposite effect in existing networks. We then design incentives for tuning validators' allocations.
  Our elastic restaking system and incentive design have immediate practical implications for deployed restaking networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00170v4</guid>
      <category>cs.GT</category>
      <category>cs.DC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roi Bar-Zur, Ittay Eyal</dc:creator>
    </item>
    <item>
      <title>Memory-Centric Computing: Solving Computing's Memory Problem</title>
      <link>https://arxiv.org/abs/2505.00458</link>
      <description>arXiv:2505.00458v2 Announce Type: replace-cross 
Abstract: Computing has a huge memory problem. The memory system, consisting of multiple technologies at different levels, is responsible for most of the energy consumption, performance bottlenecks, robustness problems, monetary cost, and hardware real estate of a modern computing system. All this becomes worse as modern and emerging applications become more data-intensive (as we readily witness in e.g., machine learning, genome analysis, graph processing, and data analytics), making the memory system an even larger bottleneck. In this paper, we discuss two major challenges that greatly affect computing system performance and efficiency: 1) memory technology &amp; capacity scaling (at the lower device and circuit levels) and 2) system and application performance &amp; energy scaling (at the higher levels of the computing stack). We demonstrate that both types of scaling have become extremely difficult, wasteful, and costly due to the dominant processor-centric design &amp; execution paradigm of computers, which treats memory as a dumb and inactive component that cannot perform any computation. We show that moving to a memory-centric design &amp; execution paradigm can solve the major challenges, while enabling multiple other potential benefits. In particular, we demonstrate that: 1) memory technology scaling problems (e.g., RowHammer, RowPress, Variable Read Disturbance, data retention, and other issues awaiting to be discovered) can be much more easily and efficiently handled by enabling memory to autonomously manage itself; 2) system and application performance &amp; energy efficiency can, at the same time, be improved by orders of magnitude by enabling computation capability in memory chips and structures (i.e., processing in memory). We discuss adoption challenges against enabling memory-centric computing, and describe how we can get there step-by-step via an evolutionary path.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00458v2</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Onur Mutlu, Ataberk Olgun, Ismail Emir Yuksel</dc:creator>
    </item>
    <item>
      <title>Federated Isolation Forest for Efficient Anomaly Detection on Edge IoT Systems</title>
      <link>https://arxiv.org/abs/2506.05138</link>
      <description>arXiv:2506.05138v2 Announce Type: replace-cross 
Abstract: Recently, federated learning frameworks such as Python TestBed for Federated Learning Algorithms and MicroPython TestBed for Federated Learning Algorithms have emerged to tackle user privacy concerns and efficiency in embedded systems. Even more recently, an efficient federated anomaly detection algorithm, FLiForest, based on Isolation Forests has been developed, offering a low-resource, unsupervised method well-suited for edge deployment and continuous learning. In this paper, we present an application of Isolation Forest-based temperature anomaly detection, developed using the previously mentioned federated learning frameworks, aimed at small edge devices and IoT systems running MicroPython. The system has been experimentally evaluated, achieving over 96% accuracy in distinguishing normal from abnormal readings and above 78% precision in detecting anomalies across all tested configurations, while maintaining a memory usage below 160 KB during model training. These results highlight its suitability for resource-constrained environments and edge systems, while upholding federated learning principles of data privacy and collaborative learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05138v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Fri, 05 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/ZINC65316.2025.11103552</arxiv:DOI>
      <arxiv:journal_reference>Published by IEEE Xplore</arxiv:journal_reference>
      <dc:creator>Pavle Vasiljevic, Milica Matic, Miroslav Popovic</dc:creator>
    </item>
  </channel>
</rss>
