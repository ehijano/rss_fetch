<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 10 Dec 2024 05:00:30 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The Overlooked Risks of Non-Validated Exclusions</title>
      <link>https://arxiv.org/abs/2412.05398</link>
      <description>arXiv:2412.05398v1 Announce Type: new 
Abstract: Despite calls for reform to enhance forensic science, insufficient attention has been paid to the potential errors arising from exclusions. Often based on intuitive judgment rather than empirical evidence, exclusions can lead to significant errors. Additionally, exclusions can imply inclusions, especially when ancillary information narrows the pool of suspects in an investigation. Without empirical evidence demonstrating that class characteristics alone can consistently be used for comparison with high repeatability, replicability, and accuracy, these characteristics should not be employed for exclusions. Errors in exclusions require the same scrutiny as errors in inclusions. Validity studies and reports of the accuracy of a forensic method must include both false positive rates and false negative rates, since only focusing on false positive rates can lead, and has led, to errors and subsequent miscarriages of justice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05398v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maria Cuellar</dc:creator>
    </item>
    <item>
      <title>Simultaneous Reconstruction of Spatial Frequency Fields and Sample Locations via Bayesian Semi-Modular Inference</title>
      <link>https://arxiv.org/abs/2412.05763</link>
      <description>arXiv:2412.05763v1 Announce Type: new 
Abstract: Traditional methods for spatial inference estimate smooth interpolating fields based on features measured at well-located points. When the spatial locations of some observations are missing, joint inference of the fields and locations is possible as the fields inform the locations and vice versa. If the number of missing locations is large, conventional Bayesian Inference fails if the generative model for the data is even slightly mis-specified, due to feedback between estimated fields and the imputed locations. Semi-Modular Inference (SMI) offers a solution by controlling the feedback between different modular components of the joint model using a hyper-parameter called the influence parameter. Our work is motivated by linguistic studies on a large corpus of late-medieval English textual dialects. We simultaneously learn dialect fields using dialect features observed in ``anchor texts'' with known location and estimate the location of origin for ``floating'' textual dialects of unknown origin. The optimal influence parameter minimises a loss measuring the accuracy of held-out anchor data. We compute a (flow-based) variational approximation to the SMI posterior for our model. This allows efficient computation of the optimal influence. MCMC-based approaches, feasible on small subsets of the data, are used to check the variational approximation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05763v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chris U. Carmona, Ross A. Haines, Max Anderson Loake, Michael Benskin, Geoff K. Nicholls</dc:creator>
    </item>
    <item>
      <title>Unveiling True Talent: The Soccer Factor Model for Skill Evaluation</title>
      <link>https://arxiv.org/abs/2412.05911</link>
      <description>arXiv:2412.05911v1 Announce Type: new 
Abstract: Evaluating a soccer player's performance can be challenging due to the high costs and small margins involved in recruitment decisions. Raw observational statistics further complicate an accurate individual skill assessment as they do not abstract from the potentially confounding factor of team strength. We introduce the Soccer Factor Model (SFM), which corrects this bias by isolating a player's true skill from the team's influence. We compile a novel data set, web-scraped from publicly available data sources. Our empirical application draws on information of 144 players, playing a total of over 33,000 matches, in seasons 2000/01 through 2023/24. Not only does the SFM allow for a structural interpretation of a player's skill, but also stands out against more reduced-form benchmarks in terms of forecast accuracy. Moreover, we propose Skill- and Performance Above Replacement as metrics for fair cross-player comparisons. These, for example, allow us to settle the discussion about the GOAT of soccer in the first quarter of the twenty-first century.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05911v1</guid>
      <category>stat.AP</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandre Andorra, Maximilian G\"obel</dc:creator>
    </item>
    <item>
      <title>Methods to derive uncertainty intervals for lifetime risks for lung cancer related to occupational radon exposure</title>
      <link>https://arxiv.org/abs/2412.06054</link>
      <description>arXiv:2412.06054v1 Announce Type: new 
Abstract: Introduction
  Lifetime risks quantify health risks from radiation exposure and play an important role in radiation detriment and radon dose conversion. This study considers the lifetime risk of dying from lung cancer related to occupational radon exposure, focusing on lifetime excess absolute risk (LEAR), in addition to other lifetime risk measures. This article derives and discusses uncertainty intervals for these estimates.
  Methods
  Uncertainties in two components of lifetime risk calculations are modeled: risk model parameter estimates for excess relative risk of lung cancer and baseline mortality rates. Approximate normality assumption (ANA) methods and Bayesian techniques quantify risk model parameter uncertainty. The methods are applied to risk models from the German "Wismut" uranium miners cohort study (full cohort with follow-up 2018 and the 1960+ sub-cohort of miners hired in 1960 or later). Mortality rate uncertainty is assessed based on WHO data. Monte Carlo simulations yield uncertainty intervals, which are compared across different lifetime risk measures.
  Results
  Risk model parameter uncertainty is the largest contributor to lifetime risk uncertainty, with baseline mortality rate uncertainty also significant. For the 1960+ sub-cohort risk model, LEAR was 6.70% (95% uncertainty interval: [3.26, 12.28]) for an exposure of 2 Working Level Months from age 18-64, compared to 3.43% ([2.06, 4.84]) for the full cohort. Differences across lifetime risk measures are minor.
  Conclusion
  Here, risk model parameter uncertainty substantially drives lifetime risk uncertainty, supporting the use of ANA methods for practicality. Choice of lifetime risk measures has negligible impact. Derived uncertainty intervals align with the range of lifetime risk estimates from uranium miners studies in the literature and should inform radiation protection policies based on lifetime risks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06054v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Manuel Sommer, Nora Fenske, Christian Heumann, Peter Scholz-Kreisel, Felix Heinzl</dc:creator>
    </item>
    <item>
      <title>Feasibility of Estimating Macroscopic Fundamental Diagrams in Iran Traffic Network</title>
      <link>https://arxiv.org/abs/2412.06128</link>
      <description>arXiv:2412.06128v1 Announce Type: new 
Abstract: The increase in population and economic growth, coupled with accelerated urbanization and suburbanization, has exacerbated traffic congestion and environmental challenges in urban areas. To address these issues, a comprehensive traffic management program has been introduced, aimed at enhancing the regulation and control of traffic flow, thereby ensuring faster and safer travel. By leveraging data collected from various regions, tailored traffic management strategies can be implemented to meet the specific needs of different city sectors. This approach involves continuous monitoring of traffic conditions and the application of targeted interventions to mitigate congestion and improve overall traffic efficiency. A case study in Tehran exemplifies the application of this program. A designated section of the city's traffic network is being utilized to test the program's efficacy. The objective is to assess its practical effectiveness under real-world conditions and refine the program based on empirical findings. This initiative aims to provide a robust solution to Tehran's traffic challenges, contributing to improved traffic management and enhanced safety.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06128v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mahgam Tabatabaei, Reza Golshan Khavas, Ali Tavakoli Kashani</dc:creator>
    </item>
    <item>
      <title>Can Generalized Extreme Value Model Fit the Real Stocks</title>
      <link>https://arxiv.org/abs/2412.06226</link>
      <description>arXiv:2412.06226v1 Announce Type: new 
Abstract: The Generalized Extreme Value (GEV) distribution plays a critical role in risk assessment across various domains, such as hydrology, climate science, and finance. In this study, we investigate its application in analyzing intraday trading risks within the Chinese stock market, focusing on abrupt price movements influenced by unique trading regulations. To address limitations of traditional GEV parameter estimators, we leverage recently developed robust and asymptotically normal estimators, enabling accurate modeling of extreme intraday price fluctuations. We introduce two risk indicators: the mean risk level (mEVI) and a Stability Indicator (STI) to evaluate the stability of the shape parameter over time. Using data from 261 Chinese and 32 U.S. stocks (2015-2017), we find that Chinese stocks exhibit higher mEVI, corresponding to greater tail risk, while maintaining high model stability. Additionally, we show that Value at Risk (VaR) estimates derived from our GEV models outperform traditional GP and normal-based VaR methods in terms of variance and portfolio optimization. These findings underscore the versatility and efficiency of GEV modeling for intraday risk management and portfolio strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06226v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sen Lin, Ao Kong, Robert Azencott</dc:creator>
    </item>
    <item>
      <title>Generalized Design of Basket Trials with P-value Combination Test</title>
      <link>https://arxiv.org/abs/2412.06622</link>
      <description>arXiv:2412.06622v1 Announce Type: new 
Abstract: The oncology exploratory basket trial design with pruning and pooling (P&amp;P) approach has gained increasing popularity in recent years for its simplicity and efficiency. This method was proposed based on binary endpoint, limiting its wider application. This short communication proposed a generalized framework of using P-value combination test to implement pruning and pooling process in basket trials. Only P-values of any type of statistical testing from each cohort are needed for decision making, which provides great flexibility for basket trial designs with P&amp;P approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06622v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Heng Zhou, Linda Sun, Fang Liu, Cong Chen</dc:creator>
    </item>
    <item>
      <title>Accurate early detection of Parkinson's disease from SPECT imaging through Convolutional Neural Networks</title>
      <link>https://arxiv.org/abs/2412.05348</link>
      <description>arXiv:2412.05348v1 Announce Type: cross 
Abstract: Early and accurate detection of Parkinson's disease (PD) is a crucial diagnostic challenge carrying immense clinical significance, for effective treatment regimens and patient management. For instance, a group of subjects termed SWEDD who are clinically diagnosed as PD, but show normal Single Photon Emission Computed Tomography (SPECT) scans, change their diagnosis as non-PD after few years of follow up, and in the meantime, they are treated with PD medications which do more harm than good. In this work, machine learning models are developed using features from SPECT images to detect early PD and SWEDD subjects from normal. These models were observed to perform with high accuracy. It is inferred from the study that these diagnostic models carry potential to help PD clinicians in the diagnostic process</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05348v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>R. Prashanth</dc:creator>
    </item>
    <item>
      <title>Optimizing Returns from Experimentation Programs</title>
      <link>https://arxiv.org/abs/2412.05508</link>
      <description>arXiv:2412.05508v1 Announce Type: cross 
Abstract: Experimentation in online digital platforms is used to inform decision making. Specifically, the goal of many experiments is to optimize a metric of interest. Null hypothesis statistical testing can be ill-suited to this task, as it is indifferent to the magnitude of effect sizes and opportunity costs. Given access to a pool of related past experiments, we discuss how experimentation practice should change when the goal is optimization. We survey the literature on empirical Bayes analyses of A/B test portfolios, and single out the A/B Testing Problem (Azevedo et al., 2020) as a starting point, which treats experimentation as a constrained optimization problem. We show that the framework can be solved with dynamic programming and implemented by appropriately tuning $p$-value thresholds. Furthermore, we develop several extensions of the A/B Testing Problem and discuss the implications of these results on experimentation programs in industry. For example, under no-cost assumptions, firms should be testing many more ideas, reducing test allocation sizes, and relaxing $p$-value thresholds away from $p = 0.05$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05508v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Timothy Sudijono, Simon Ejdemyr, Apoorva Lal, Martin Tingley</dc:creator>
    </item>
    <item>
      <title>Efficiency of nonparametric superiority tests based on restricted mean survival time versus the log-rank test under proportional hazards</title>
      <link>https://arxiv.org/abs/2412.06442</link>
      <description>arXiv:2412.06442v1 Announce Type: cross 
Abstract: Background: For RCTs with time-to-event endpoints, proportional hazard (PH) models are typically used to estimate treatment effects and logrank tests are commonly used for hypothesis testing. There is growing support for replacing this approach with a model-free estimand and assumption-lean analysis method. One alternative is to base the analysis on the difference in restricted mean survival time (RMST) at a specific time, a single-number summary measure that can be defined without any restrictive assumptions on the outcome model. In a simple setting without covariates, an assumption-lean analysis can be achieved using nonparametric methods such as Kaplan Meier estimation. The main advantage of moving to a model-free summary measure and assumption-lean analysis is that the validity and interpretation of conclusions do not depend on the PH assumption. The potential disadvantage is that the nonparametric analysis may lose efficiency under PH. There is disagreement in recent literature on this issue. Methods: Asymptotic results and simulations are used to compare the efficiency of a log-rank test against a nonparametric analysis of the difference in RMST in a superiority trial under PH. Previous studies have separately examined the effect of event rates and the censoring distribution on relative efficiency. This investigation clarifies conflicting results from earlier research by exploring the joint effect of event rate and censoring distribution together. Several illustrative examples are provided. Results: In scenarios with high event rates and/or substantial censoring across a large proportion of the study window, and when both methods make use of the same amount of data, relative efficiency is close to unity. However, in cases with low event rates but when censoring is concentrated at the end of the study window, the PH analysis has a considerable efficiency advantage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06442v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dominic Magirr, Craig Wang, Xinlei Deng, Tim Morris, Mark Baillie</dc:creator>
    </item>
    <item>
      <title>Highest Posterior Density Intervals As Analogues to Profile Likelihood Ratio Confidence Intervals for Modes of Unimodal Distributions</title>
      <link>https://arxiv.org/abs/2412.06528</link>
      <description>arXiv:2412.06528v1 Announce Type: cross 
Abstract: In Bayesian statistics, the highest posterior density (HPD) interval is often used to describe properties of a posterior distribution. As a method for estimating confidence intervals (CIs), the HPD has two main desirable properties. Firstly, it is the shortest interval to have a specified coverage probability. Secondly, every point inside the HPD interval has a density greater than every point outside the interval. However, it is sometimes criticized for being transformation invariant.
  We make the case that the HPD interval is a natural analog to the frequentist profile likelihood ratio confidence interval (LRCI). First we provide background on the HPD interval as well as the Likelihood Ratio Test statistic and its inversion to generate asymptotically-correct CIs. Our main result is to show that the HPD interval has similar desirable properties as the profile LRCI, such as transformation invariance with respect to the mode for monotonic functions. We then discuss an application of the main result, an example case which compares the profile LRCI for the binomial probability parameter p with the Bayesian HPD interval for the beta distribution density function, both of which are used to estimate population proportions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06528v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. X. Venu</dc:creator>
    </item>
    <item>
      <title>VOPy: A Framework for Black-box Vector Optimization</title>
      <link>https://arxiv.org/abs/2412.06604</link>
      <description>arXiv:2412.06604v1 Announce Type: cross 
Abstract: We introduce VOPy, an open-source Python library designed to address black-box vector optimization, where multiple objectives must be optimized simultaneously with respect to a partial order induced by a convex cone. VOPy extends beyond traditional multi-objective optimization (MOO) tools by enabling flexible, cone-based ordering of solutions; with an application scope that includes environments with observation noise, discrete or continuous design spaces, limited budgets, and batch observations. VOPy provides a modular architecture, facilitating the integration of existing methods and the development of novel algorithms. We detail VOPy's architecture, usage, and potential to advance research and application in the field of vector optimization. The source code for VOPy is available at https://github.com/Bilkent-CYBORG/VOPy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06604v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ya\c{s}ar Cahit Y{\i}ld{\i}r{\i}m, Efe Mert Karag\"ozl\"u, \.Ilter Onat Korkmaz, \c{C}a\u{g}{\i}n Ararat, Cem Tekin</dc:creator>
    </item>
    <item>
      <title>Theoretical and Practical Limits of Kolmogorov-Zurbenko Periodograms with Dynamic Smoothing in Estimating Signal Frequencies</title>
      <link>https://arxiv.org/abs/2007.03031</link>
      <description>arXiv:2007.03031v3 Announce Type: replace 
Abstract: This investigation establishes the theoretical and practical limits of Kolmogorov-Zurbenko periodograms with dynamic smoothing in their estimation of signal frequencies in terms of their sensitivity, accuracy, resolution, and robustness. While the DiRienzo-Zurbenko algorithm performs dynamic smoothing based on local variation in a periodogram, the Neagu-Zurbenko algorithm performs dynamic smoothing based on local departure from linearity in a periodogram. This article begins with a summary of the statistical foundations for both the DiRienzo-Zurbenko algorithm and the Neagu-Zurbenko algorithm, followed by instructions for accessing and utilizing these approaches within the R statistical program platform. Brief definitions, importance, statistical bases, theoretical and practical limits, and demonstrations are provided for their sensitivity, accuracy, resolution, and robustness in estimating signal frequencies. Next using a simulated time series in which two signals close in frequency are embedded in a significant level of random noise, the predictive power of these approaches are compared to the autoregressive integral moving average (ARIMA) approach, with support again garnered for their being robust when data is missing. Throughout, the article contrasts the limits of Kolmogorov-Zurbenko periodograms with dynamic smoothing to those of log-periodograms with static smoothing, while also comparing the performance of the DiRienzo-Zurbenko algorithm to that of the Neagu-Zurbenko algorithm. It concludes by delineating next steps to establish the precision with which Kolmogorov-Zurbenko periodograms with dynamic smoothing estimate signal strength.</description>
      <guid isPermaLink="false">oai:arXiv.org:2007.03031v3</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Barry Loneck, Igor Zurbenko, Edward Valachovic</dc:creator>
    </item>
    <item>
      <title>Synthesizing data products, mathematical models, and observational measurements for lake temperature forecasting</title>
      <link>https://arxiv.org/abs/2407.03312</link>
      <description>arXiv:2407.03312v2 Announce Type: replace 
Abstract: We present a novel forecasting framework for lake water temperature, which is crucial for managing lake ecosystems and drinking water resources. The General Lake Model (GLM) has been previously used for this purpose, but, similar to many process-based simulation models, it: requires a large number of inputs, many of which are stochastic; presents challenges for uncertainty quantification (UQ); and can exhibit model bias. To address these issues, we propose a Gaussian process (GP) surrogate-based forecasting approach that efficiently handles large, high-dimensional data and accounts for input-dependent variability and systematic GLM bias. We validate the proposed approach and compare it with other forecasting methods, including a climatological model and raw GLM simulations. Our results demonstrate that our bias-corrected GP surrogate (GPBC) can outperform competing approaches in terms of forecast accuracy and UQ up to two weeks into the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03312v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Maike F. Holthuijzen, Robert B. Gramacy, Cayelan C. Carey, Dave M. Higdon, R. Quinn Thomas</dc:creator>
    </item>
    <item>
      <title>Covariate-Adjusted Functional Data Analysis for Structural Health Monitoring</title>
      <link>https://arxiv.org/abs/2408.02106</link>
      <description>arXiv:2408.02106v2 Announce Type: replace 
Abstract: Structural Health Monitoring (SHM) is increasingly applied in civil engineering. One of its primary purposes is detecting and assessing changes in structure conditions to increase safety and reduce potential maintenance downtime. Recent advancements, especially in sensor technology, facilitate data measurements, collection, and process automation, leading to large data streams. We propose a function-on-function regression framework for (nonlinear) modeling the sensor data and adjusting for covariate-induced variation. Our approach is particularly suited for long-term monitoring when several months or years of training data are available. It combines highly flexible yet interpretable semi-parametric modeling with functional principal component analysis and uses the corresponding out-of-sample Phase-II scores for monitoring. The method proposed can also be described as a combination of an ``input-output'' and an ``output-only'' method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02106v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philipp Wittenberg, Lizzie Neumann, Alexander Mendler, Jan Gertheiss</dc:creator>
    </item>
    <item>
      <title>GeneralizIT: A Python Solution for Generalizability Theory Computations</title>
      <link>https://arxiv.org/abs/2411.17880</link>
      <description>arXiv:2411.17880v2 Announce Type: replace 
Abstract: GeneralizIT is a Python package designed to streamline the application of Generalizability Theory (G-Theory) in research and practice. G-Theory extends classical test theory by estimating multiple sources of error variance, providing a more flexible and detailed approach to reliability assessment. Despite its advantages, G-Theory's complexity can present a significant barrier to researchers. GeneralizIT addresses this challenge by offering an intuitive, user-friendly mechanism to calculate variance components, generalizability coefficients E*rho^2 and dependability Phi and to perform decision (D) studies. D-Studies allow users to make decisions about potential study designs and target improvements in the reliability of certain facets. The package supports both fully crossed and nested designs, enabling users to perform in-depth reliability analysis with minimal coding effort. With built-in visualization tools and detailed reporting functions, GeneralizIT empowers researchers across disciplines, such as education, psychology, healthcare, and the social sciences, to harness the power of G-Theory for robust evidence-based insights. Whether applied to small or large datasets, GeneralizIT offers an accessible and computationally efficient solution to improve measurement reliability in complex data environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17880v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tyler J. Smith, Theresa Kline, Adrienne Kline</dc:creator>
    </item>
    <item>
      <title>Spatiotemporal Density Correction of Multivariate Global Climate Model Projections using Deep Learning</title>
      <link>https://arxiv.org/abs/2411.18799</link>
      <description>arXiv:2411.18799v2 Announce Type: replace 
Abstract: Global Climate Models (GCMs) are numerical models that simulate complex physical processes within the Earth's climate system and are essential for understanding and predicting climate change. However, GCMs suffer from systemic biases due to simplifications made to the underlying physical processes. GCM output therefore needs to be bias corrected before it can be used for future climate projections. Most common bias correction methods, however, cannot preserve spatial, temporal, or inter-variable dependencies. We propose a new semi-parametric conditional density estimation (SPCDE) for density correction of the joint distribution of daily precipitation and maximum temperature data obtained from gridded GCM spatial fields. The Vecchia approximation is employed to preserve dependencies in the observed field during the density correction process, which is carried out using semi-parametric quantile regression. The ability to calibrate joint distributions of GCM projections has potential advantages not only in estimating extremes, but also in better estimating compound hazards, like heat waves and drought, under potential climate change. Illustration on historical data from 1951-2014 over two 5x5 spatial grids in the US indicate that SPCDE can preserve key marginal and joint distribution properties of precipitation and maximum temperature, and predictions obtained using SPCDE are better calibrated compared to predictions using asynchronous quantile mapping and canonical correlation analysis, two commonly used bias correction approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18799v2</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Reetam Majumder, Shiqi Fang, A. Sankarasubramanian, Emily C. Hector, Brian J. Reich</dc:creator>
    </item>
    <item>
      <title>A unified quantile framework for nonlinear heterogeneous transcriptome-wide associations</title>
      <link>https://arxiv.org/abs/2207.12081</link>
      <description>arXiv:2207.12081v3 Announce Type: replace-cross 
Abstract: Transcriptome-wide association studies (TWAS) are powerful tools for identifying gene-level associations by integrating genome-wide association studies and gene expression data. However, most TWAS methods focus on linear associations between genes and traits, ignoring the complex nonlinear relationships that may be present in biological systems. To address this limitation, we propose a novel framework, QTWAS, which integrates a quantile-based gene expression model into the TWAS model, allowing for the discovery of nonlinear and heterogeneous gene-trait associations. Via comprehensive simulations and applications to both continuous and binary traits, we demonstrate that the proposed model is more powerful than conventional TWAS in identifying gene-trait associations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.12081v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianying Wang, Iuliana Ionita-Laza, Ying Wei</dc:creator>
    </item>
    <item>
      <title>Transportation Marketplace Rate Forecast Using Signature Transform</title>
      <link>https://arxiv.org/abs/2401.04857</link>
      <description>arXiv:2401.04857v3 Announce Type: replace-cross 
Abstract: Freight transportation marketplace rates are typically challenging to forecast accurately. In this work, we have developed a novel statistical technique based on signature transforms and have built a predictive and adaptive model to forecast these marketplace rates. Our technique is based on two key elements of the signature transform: one being its universal nonlinearity property, which linearizes the feature space and hence translates the forecasting problem into linear regression, and the other being the signature kernel, which allows for comparing computationally efficiently similarities between time series data. Combined, it allows for efficient feature generation and precise identification of seasonality and regime switching in the forecasting process.
  An algorithm based on our technique has been deployed by Amazon trucking operations, with far superior forecast accuracy and better interpretability versus commercially available industry models, even during the COVID-19 pandemic and the Ukraine conflict. Furthermore, our technique is able to capture the influence of business cycles and the heterogeneity of the marketplace, improving prediction accuracy by more than fivefold, with an estimated annualized saving of \$50MM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.04857v3</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haotian Gu, Xin Guo, Timothy L. Jacobs, Philip Kaminsky, Xinyu Li</dc:creator>
    </item>
    <item>
      <title>Crowdsourced Adaptive Surveys</title>
      <link>https://arxiv.org/abs/2401.12986</link>
      <description>arXiv:2401.12986v2 Announce Type: replace-cross 
Abstract: Public opinion surveys are vital for informing democratic decision-making, but responding to rapidly evolving information environments and measuring beliefs within niche communities can be challenging for traditional survey methods. This paper introduces a crowdsourced adaptive survey methodology (CSAS) that unites advances in natural language processing and adaptive algorithms to generate question banks that evolve with user input. The CSAS method converts open-ended text provided by participants into survey items and applies a multi-armed bandit algorithm to determine which questions should be prioritized in the survey. The method's adaptive nature allows for the exploration of new survey questions, while imposing minimal costs in survey length. Applications in the domains of Latino information environments, national issue importance, and local politics showcase CSAS's ability to identify topics that might otherwise escape the notice of survey researchers. I conclude by highlighting CSAS's potential to bridge conceptual gaps between researchers and participants in survey research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.12986v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yamil Velez</dc:creator>
    </item>
    <item>
      <title>Data Unfolding with Mean Integrated Square Error Optimization</title>
      <link>https://arxiv.org/abs/2402.12990</link>
      <description>arXiv:2402.12990v3 Announce Type: replace-cross 
Abstract: Experimental data in Particle and Nuclear physics, Particle Astrophysics and Radiation Protection Dosimetry are obtained from experimental facilities comprising a complex array of sensors, electronics and software. Computer simulation is used to study the measurement process. Probability Density Functions (PDFs) of measured physical parameters deviate from true PDFs due to resolution, bias, and efficiency effects. Good estimates of the true PDF are necessary for testing theoretical models, comparing results from different experiments, and combining results from various research endeavors. In the article, the histogram method is employed to estimate both the measured and true PDFs. The binning of histograms is determined using the K-means clustering algorithm. The true PDF is estimated through the maximization of the likelihood function with entropy regularization, utilizing a non-linear optimization algorithm specially designed for this purpose. The accuracy of the results is assessed using the Mean Integrated Square Error. To determine the optimal value for the regularization parameter, a bootstrap method is applied. Additionally, a mathematical model of the measurement system is formulated using system identification methods. This approach enhances the robustness and precision of the estimation process, providing a more reliable analysis of the system's characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.12990v3</guid>
      <category>physics.data-an</category>
      <category>hep-ex</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nikolay D. Gagunashvili</dc:creator>
    </item>
  </channel>
</rss>
