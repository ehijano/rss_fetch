<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 06 Nov 2024 05:01:30 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Estimating journey time for two-point vehicle re-identification survey with limited observable scope using 2-dimensional truncated distributions</title>
      <link>https://arxiv.org/abs/2411.02539</link>
      <description>arXiv:2411.02539v1 Announce Type: new 
Abstract: In transportation, Weigh-in motion (WIM) stations, Electronic Toll Collection (ETC) systems, Closed-circuit Television (CCTV) are widely deployed to collect data at different locations. Vehicle re-identification, by matching the same vehicle at different locations, is helpful in understanding the long-distance journey patterns. In this paper, the potential hazards of ignoring the survivorship bias effects are firstly identified and analyzed using a truncated distribution over a 2-dimensional time-time domain. Given journey time modeled as Exponential or Weibull distribution, Maximum Likelihood Estimation (MLE), Fisher Information (F.I.) and Bootstrap methods are formulated to estimate the parameter of interest and their confidence intervals. Besides formulating journey time distributions, an automated framework querying the observable time-time scope are proposed. For complex distributions (e.g, three parameter Weibull), distributions are modeled in PyTorch to automatically find first and second derivatives and estimated results. Three experiments are designed to demonstrate the effectiveness of the proposed method. In conclusion, the paper describes a very unique aspects in understanding and analyzing traffic status. Although the survivorship bias effects are not recognized and long-ignored, by accurately describing travel time over time-time domain, the proposed approach have potentials in travel time reliability analysis, understanding logistics systems, modeling/predicting product lifespans, etc.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02539v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Diyi Liu, Yangsong Gu, Lee D. Han</dc:creator>
    </item>
    <item>
      <title>Effects of survey design features on response rates: a meta-analytical approach using the example of crime surveys</title>
      <link>https://arxiv.org/abs/2411.02552</link>
      <description>arXiv:2411.02552v1 Announce Type: new 
Abstract: When conducting a survey, many choices regarding survey design features have to be made. These choices affect the response rate of a survey. This paper analyzes the individual effects of these survey design features on the response rate. For this purpose, data from a systematic review of crime surveys conducted in Germany between 2001--2021 were used. First, a meta-analysis of proportions is used to estimate the summary response rate. Second, a meta-regression was fitted, modeling the relationship between the observed response rates and survey-design features, such as the study year, target population, coverage area, data collection mode, and institute. The developed model informs about the influence of certain survey design features and can predict the expected response rate when (re-) designing a survey. This study highlights that a thoughtful survey design and professional survey administration can result in high response rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02552v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonas Klingwort, Vera Toepoel</dc:creator>
    </item>
    <item>
      <title>Deep learning-based modularized loading protocol for parameter estimation of Bouc-Wen class models</title>
      <link>https://arxiv.org/abs/2411.02776</link>
      <description>arXiv:2411.02776v1 Announce Type: cross 
Abstract: This study proposes a modularized deep learning-based loading protocol for optimal parameter estimation of Bouc-Wen (BW) class models. The protocol consists of two key components: optimal loading history construction and CNN-based rapid parameter estimation. Each component is decomposed into independent sub-modules tailored to distinct hysteretic behaviors-basic hysteresis, structural degradation, and pinching effect-making the protocol adaptable to diverse hysteresis models. Three independent CNN architectures are developed to capture the path-dependent nature of these hysteretic behaviors. By training these CNN architectures on diverse loading histories, minimal loading sequences, termed \textit{loading history modules}, are identified and then combined to construct an optimal loading history. The three CNN models, trained on the respective loading history modules, serve as rapid parameter estimators. Numerical evaluation of the protocol, including nonlinear time history analysis of a 3-story steel moment frame and fragility curve construction for a 3-story reinforced concrete frame, demonstrates that the proposed protocol significantly reduces total analysis time while maintaining or improving estimation accuracy. The proposed protocol can be extended to other hysteresis models, suggesting a systematic approach for identifying general hysteresis models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02776v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sebin Oh, Junho Song, Taeyong Kim</dc:creator>
    </item>
    <item>
      <title>Testing Generalizability in Causal Inference</title>
      <link>https://arxiv.org/abs/2411.03021</link>
      <description>arXiv:2411.03021v1 Announce Type: cross 
Abstract: Ensuring robust model performance across diverse real-world scenarios requires addressing both transportability across domains with covariate shifts and extrapolation beyond observed data ranges. However, there is no formal procedure for statistically evaluating generalizability in machine learning algorithms, particularly in causal inference. Existing methods often rely on arbitrary metrics like AUC or MSE and focus predominantly on toy datasets, providing limited insights into real-world applicability. To address this gap, we propose a systematic and quantitative framework for evaluating model generalizability under covariate distribution shifts, specifically within causal inference settings. Our approach leverages the frugal parameterization, allowing for flexible simulations from fully and semi-synthetic benchmarks, offering comprehensive evaluations for both mean and distributional regression methods. By basing simulations on real data, our method ensures more realistic evaluations, which is often missing in current work relying on simplified datasets. Furthermore, using simulations and statistical testing, our framework is robust and avoids over-reliance on conventional metrics. Grounded in real-world data, it provides realistic insights into model performance, bridging the gap between synthetic evaluations and practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03021v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel de Vassimon Manela, Linying Yang, Robin J. Evans</dc:creator>
    </item>
    <item>
      <title>Robust Market Interventions</title>
      <link>https://arxiv.org/abs/2411.03026</link>
      <description>arXiv:2411.03026v1 Announce Type: cross 
Abstract: A large differentiated oligopoly yields inefficient market equilibria. An authority with imprecise information about the primitives of the market aims to design tax/subsidy interventions that increase efficiency robustly, i.e., with high probability. We identify a condition on demand that guarantees the existence of such interventions, and we show how to construct them using noisy estimates of demand complementarities and substitutabilities across products. The analysis works by deriving a novel description of the incidence of market interventions in terms of spectral statistics of a Slutsky matrix. Our notion of recoverable structure ensures that parts of the spectrum that are useful for the design of interventions are statistically recoverable from noisy demand estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03026v1</guid>
      <category>econ.TH</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Galeotti, Benjamin Golub, Sanjeev Goyal, Eduard Talam\`as, Omer Tamuz</dc:creator>
    </item>
    <item>
      <title>The Effect of Funding on Student Achievement: Evidence from District of Columbia, Virginia, and Maryland</title>
      <link>https://arxiv.org/abs/2411.03147</link>
      <description>arXiv:2411.03147v1 Announce Type: cross 
Abstract: The question of how to best serve the student populations of our country is a complex topic. Since public funding is limited, we must explore the best ways to direct the money to improve student outcomes. Previous research has suggested that socio-economic status is the best predictor of student achievement, while other studies suggest that the amount of money spent on the student is a more significant factor. In this paper, we explore this question and its impacts on Maryland, Virginia, and the District of Columbia schools. We conclude that the graduation rate has a direct relationship with unemployment, suggesting that funding towards improving out-of-school opportunities and quality of life will significantly improve students chances of success. We do not find a significant relationship between per-pupil spending and student achievement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03147v1</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adam Raabe, Jessica Reynolds, Akshitha Kukudala, Huthaifa Ashqar</dc:creator>
    </item>
    <item>
      <title>Causal Responsibility Attribution for Human-AI Collaboration</title>
      <link>https://arxiv.org/abs/2411.03275</link>
      <description>arXiv:2411.03275v1 Announce Type: cross 
Abstract: As Artificial Intelligence (AI) systems increasingly influence decision-making across various fields, the need to attribute responsibility for undesirable outcomes has become essential, though complicated by the complex interplay between humans and AI. Existing attribution methods based on actual causality and Shapley values tend to disproportionately blame agents who contribute more to an outcome and rely on real-world measures of blameworthiness that may misalign with responsible AI standards. This paper presents a causal framework using Structural Causal Models (SCMs) to systematically attribute responsibility in human-AI systems, measuring overall blameworthiness while employing counterfactual reasoning to account for agents' expected epistemic levels. Two case studies illustrate the framework's adaptability in diverse human-AI collaboration scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03275v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Yahang Qi, Bernhard Sch\"olkopf, Zhijing Jin</dc:creator>
    </item>
    <item>
      <title>Elementary econometric and strategic analysis of curling matches</title>
      <link>https://arxiv.org/abs/2406.18601</link>
      <description>arXiv:2406.18601v2 Announce Type: replace 
Abstract: We develop a Markov model of curling matches, parametrised by the probability of winning an end and the probability distribution of scoring ends. In practical applications, these end-winning probabilities can be estimated econometrically, and are shown to depend on which team holds the hammer, as well as the offensive and defensive strengths of the respective teams. Using a maximum entropy argument, based on the idea of characteristic scoring patterns in curling, we predict that the points distribution of scoring ends should follow a constrained geometric distribution. We provide analytical results detailing when it is optimal to blank the end in preference to scoring one point and losing possession of the hammer. Statistical and simulation analysis of international curling matches is also performed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18601v2</guid>
      <category>stat.AP</category>
      <category>math.PR</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Managerial Finance, 2024</arxiv:journal_reference>
      <dc:creator>John Fry, Mark Austin, Silvio Fanzon</dc:creator>
    </item>
    <item>
      <title>Topological Feature Search Method for Multichannel EEG: Application in ADHD classification</title>
      <link>https://arxiv.org/abs/2404.06676</link>
      <description>arXiv:2404.06676v2 Announce Type: replace-cross 
Abstract: In recent years, the preliminary diagnosis of ADHD using EEG has attracted the attention from researchers. EEG, known for its expediency and efficiency, plays a pivotal role in the diagnosis and treatment of ADHD. However, the non-stationarity of EEG signals and inter-subject variability pose challenges to the diagnostic and classification processes. Topological Data Analysis offers a novel perspective for ADHD classification, diverging from traditional time-frequency domain features. However, conventional TDA models are restricted to single-channel time series and are susceptible to noise, leading to the loss of topological features in persistence diagrams.This paper presents an enhanced TDA approach applicable to multi-channel EEG in ADHD. Initially, optimal input parameters for multi-channel EEG are determined. Subsequently, each channel's EEG undergoes phase space reconstruction (PSR) followed by the utilization of k-Power Distance to Measure for approximating ideal point clouds. Then, multi-dimensional time series are re-embedded, and TDA is applied to obtain topological feature information. Gaussian function-based Multivariate Kernel Density Estimation is employed in the merger persistence diagram to filter out desired topological feature mappings. Finally, the persistence image method is employed to extract topological features, and the influence of various weighting functions on the results is discussed.The effectiveness of our method is evaluated using the IEEE ADHD dataset. Results demonstrate that the accuracy, sensitivity, and specificity reach 78.27%, 80.62%, and 75.63%, respectively. Compared to traditional TDA methods, our method was effectively improved and outperforms typical nonlinear descriptors. These findings indicate that our method exhibits higher precision and robustness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06676v2</guid>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianming Cai, Guoying Zhao, Junbin Zang, Chen Zong, Zhidong Zhang, Chenyang Xue</dc:creator>
    </item>
    <item>
      <title>Double Robust Variance Estimation with Parametric Working Models</title>
      <link>https://arxiv.org/abs/2404.16166</link>
      <description>arXiv:2404.16166v2 Announce Type: replace-cross 
Abstract: Doubly robust estimators have gained popularity in the field of causal inference due to their ability to provide consistent point estimates when either an outcome or exposure model is correctly specified. However, for nonrandomized exposures the influence function based variance estimator frequently used with doubly robust estimators of the average causal effect is only consistent when both working models (i.e., outcome and exposure models) are correctly specified. Here, the empirical sandwich variance estimator and the nonparametric bootstrap are demonstrated to be doubly robust variance estimators. That is, they are expected to provide valid estimates of the variance leading to nominal confidence interval coverage when only one working model is correctly specified. Simulation studies illustrate the properties of the influence function based, empirical sandwich, and nonparametric bootstrap variance estimators in the setting where parametric working models are assumed. Estimators are applied to data from the Improving Pregnancy Outcomes with Progesterone (IPOP) study to estimate the effect of maternal anemia on birth weight among women with HIV.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16166v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bonnie E. Shook-Sa, Paul N. Zivich, Chanhwa Lee, Keyi Xue, Rachael K. Ross, Jessie K. Edwards, Jeffrey S. A. Stringer, Stephen R. Cole</dc:creator>
    </item>
  </channel>
</rss>
