<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 04 Sep 2024 04:02:40 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Variable selection in the joint frailty model of recurrent and terminal events using Broken Adaptive Ridge regression</title>
      <link>https://arxiv.org/abs/2409.00291</link>
      <description>arXiv:2409.00291v1 Announce Type: new 
Abstract: We introduce a novel method to simultaneously perform variable selection and estimation in the joint frailty model of recurrent and terminal events using the Broken Adaptive Ridge Regression penalty. The BAR penalty can be summarized as an iteratively reweighted squared $L_2$-penalized regression, which approximates the $L_0$-regularization method. Our method allows for the number of covariates to diverge with the sample size. Under certain regularity conditions, we prove that the BAR estimator implemented under the model framework is consistent and asymptotically normally distributed, which are known as the oracle properties in the variable selection literature. In our simulation studies, we compare our proposed method to the Minimum Information Criterion (MIC) method. We apply our method on the Medical Information Mart for Intensive Care (MIMIC-III) database, with the aim of investigating which variables affect the risks of repeated ICU admissions and death during ICU stay.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00291v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian Chan, Fatemeh Mahmoudi, Chel Hee Lee, Quan Long, Xuewen Lu</dc:creator>
    </item>
    <item>
      <title>Bayesian nonparametric mixtures of categorical directed graphs for heterogeneous causal inference</title>
      <link>https://arxiv.org/abs/2409.00453</link>
      <description>arXiv:2409.00453v1 Announce Type: new 
Abstract: Quantifying causal effects of exposures on outcomes, such as a treatment and a disease respectively, is a crucial issue in medical science for the administration of effective therapies. Importantly, any related causal analysis should account for all those variables, e.g. clinical features, that can act as risk factors involved in the occurrence of a disease. In addition, the selection of targeted strategies for therapy administration requires to quantify such treatment effects at personalized level rather than at population level. We address these issues by proposing a methodology based on categorical Directed Acyclic Graphs (DAGs) which provide an effective tool to infer causal relationships and causal effects between variables. In addition, we account for population heterogeneity by considering a Dirichlet Process mixture of categorical DAGs, which clusters individuals into homogeneous groups characterized by common causal structures, dependence parameters and causal effects. We develop computational strategies for Bayesian posterior inference, from which a battery of causal effects at subject-specific level is recovered. Our methodology is evaluated through simulations and applied to a dataset of breast cancer patients to investigate cardiotoxic side effects that can be induced by the administrated anticancer therapies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00453v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Federico Castelletti, Laura Ferrini</dc:creator>
    </item>
    <item>
      <title>Examining the robustness of a model selection procedure in the binary latent block model through a language placement test data set</title>
      <link>https://arxiv.org/abs/2409.00470</link>
      <description>arXiv:2409.00470v1 Announce Type: new 
Abstract: When entering French university, the students' foreign language level is assessed through a placement test. In this work, we model the placement test results using binary latent block models which allow to simultaneously form homogeneous groups of students and of items. However, a major difficulty in latent block models is to select correctly the number of groups of rows and the number of groups of columns. The first purpose of this paper is to tune the number of initializations needed to limit the initial values problem in the estimation algorithm in order to propose a model selection procedure in the placement test context. Computational studies based on simulated data sets and on two placement test data sets are investigated. The second purpose is to investigate the robustness of the proposed model selection procedure in terms of stability of the students groups when the number of students varies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00470v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vincent Brault, Fr\'ed\'erique Letu\'e, Marie-Jos\'e Martinez</dc:creator>
    </item>
    <item>
      <title>Exact Exploratory Bi-factor Analysis: A Constraint-based Optimisation Approach</title>
      <link>https://arxiv.org/abs/2409.00679</link>
      <description>arXiv:2409.00679v1 Announce Type: new 
Abstract: Bi-factor analysis is a form of confirmatory factor analysis widely used in psychological and educational measurement. The use of a bi-factor model requires the specification of an explicit bi-factor structure on the relationship between the observed variables and the group factors. In practice, the bi-factor structure is sometimes unknown, in which case an exploratory form of bi-factor analysis is needed to find the bi-factor structure. Unfortunately, there are few methods for exploratory bi-factor analysis, with the exception of a rotation-based method proposed in Jennrich and Bentler (2011, 2012). However, this method only finds approximate bi-factor structures, as it does not yield an exact bi-factor loading structure, even after applying hard thresholding. In this paper, we propose a constraint-based optimisation method that learns an exact bi-factor loading structure from data, overcoming the issue with the rotation-based method. The key to the proposed method is a mathematical characterisation of the bi-factor loading structure as a set of equality constraints, which allows us to formulate the exploratory bi-factor analysis problem as a constrained optimisation problem in a continuous domain and solve the optimisation problem with an augmented Lagrangian method. The power of the proposed method is shown via simulation studies and a real data example. Extending the proposed method to exploratory hierarchical factor analysis is also discussed. The codes are available on ``https://anonymous.4open.science/r/Bifactor-ALM-C1E6".</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00679v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiawei Qiao, Yunxiao Chen, Zhiliang Ying</dc:creator>
    </item>
    <item>
      <title>Structural adaptation via directional regularity: rate accelerated estimation in multivariate functional data</title>
      <link>https://arxiv.org/abs/2409.00817</link>
      <description>arXiv:2409.00817v1 Announce Type: new 
Abstract: We introduce directional regularity, a new definition of anisotropy for multivariate functional data. Instead of taking the conventional view which determines anisotropy as a notion of smoothness along a dimension, directional regularity additionally views anisotropy through the lens of directions. We show that faster rates of convergence can be obtained through a change-of-basis by adapting to the directional regularity of a multivariate process. An algorithm for the estimation and identification of the change-of-basis matrix is constructed, made possible due to the unique replication structure of functional data. Non-asymptotic bounds are provided for our algorithm, supplemented by numerical evidence from an extensive simulation study. We discuss two possible applications of the directional regularity approach, and advocate its consideration as a standard pre-processing step in multivariate functional data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00817v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Omar Kassi, Sunny G. W. Wang</dc:creator>
    </item>
    <item>
      <title>Linear spline index regression model: Interpretability, nonlinearity and dimension reduction</title>
      <link>https://arxiv.org/abs/2409.01017</link>
      <description>arXiv:2409.01017v1 Announce Type: new 
Abstract: Inspired by the complexity of certain real-world datasets, this article introduces a novel flexible linear spline index regression model. The model posits piecewise linear effects of an index on the response, with continuous changes occurring at knots. Significantly, it possesses the interpretability of linear models, captures nonlinear effects similar to nonparametric models, and achieves dimension reduction like single-index models. In addition, the locations and number of knots remain unknown, which further enhances the adaptability of the model in practical applications. We propose a new method that combines penalized approaches and convolution techniques to simultaneously estimate the unknown parameters and determine the number of knots. Noteworthy is that the proposed method allows the number of knots to diverge with the sample size. We demonstrate that the proposed estimators can identify the number of knots with a probability approaching one and estimate the coefficients as efficiently as if the number of knots is known in advance. We also introduce a procedure to test the presence of knots. Simulation studies and two real datasets are employed to assess the finite sample performance of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01017v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lianqiang Qu, Long Lv, Meiling Hao, Liuquan Sun</dc:creator>
    </item>
    <item>
      <title>Statistical Jump Model for Mixed-Type Data with Missing Data Imputation</title>
      <link>https://arxiv.org/abs/2409.01208</link>
      <description>arXiv:2409.01208v1 Announce Type: new 
Abstract: In this paper, we address the challenge of clustering mixed-type data with temporal evolution by introducing the statistical jump model for mixed-type data. This novel framework incorporates regime persistence, enhancing interpretability and reducing the frequency of state switches, and efficiently handles missing data. The model is easily interpretable through its state-conditional means and modes, making it accessible to practitioners and policymakers. We validate our approach through extensive simulation studies and an empirical application to air quality data, demonstrating its superiority in inferring persistent air quality regimes compared to the traditional air quality index. Our contributions include a robust method for mixed-type temporal clustering, effective missing data management, and practical insights for environmental monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01208v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Federico P. Cortese, Antonio Pievatolo</dc:creator>
    </item>
    <item>
      <title>Nonparametric Estimation of Path-specific Effects in Presence of Nonignorable Missing Covariates</title>
      <link>https://arxiv.org/abs/2409.01248</link>
      <description>arXiv:2409.01248v1 Announce Type: new 
Abstract: The path-specific effect (PSE) is of primary interest in mediation analysis when multiple intermediate variables between treatment and outcome are observed, as it can isolate the specific effect through each mediator, thus mitigating potential bias arising from other intermediate variables serving as mediator-outcome confounders. However, estimation and inference of PSE become challenging in the presence of nonignorable missing covariates, a situation particularly common in epidemiological research involving sensitive patient information. In this paper, we propose a fully nonparametric methodology to address this challenge. We establish identification for PSE by expressing it as a functional of observed data and demonstrate that the associated nuisance functions can be uniquely determined through sequential optimization problems by leveraging a shadow variable. Then we propose a sieve-based regression imputation approach for estimation. We establish the large-sample theory for the proposed estimator, and introduce a robust and efficient approach to make inference for PSE. The proposed method is applied to the NHANES dataset to investigate the mediation roles of dyslipidemia and obesity in the pathway from Type 2 diabetes mellitus to cardiovascular disease.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01248v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiawei Shan, Ting Wang, Wei Li, Chunrong Ai</dc:creator>
    </item>
    <item>
      <title>Pearson's Correlation under the scope: Assessment of the efficiency of Pearson's correlation to select predictor variables for linear models</title>
      <link>https://arxiv.org/abs/2409.01295</link>
      <description>arXiv:2409.01295v1 Announce Type: new 
Abstract: This article examines the limitations of Pearson's correlation in selecting predictor variables for linear models. Using mtcars and iris datasets from R, this paper demonstrates the limitation of this correlation measure when selecting a proper independent variable to model miles per gallon (mpg) from mtcars data and the petal length from the iris data. This paper exhibits the findings by reporting Pearson's correlation values for two potential predictor variables for each response variable, then builds a linear model to predict the response variable using each predictor variable. The error metrics for each model are then reported to evaluate how reliable Pearson's correlation is in selecting the best predictor variable. The results show that Pearson's correlation can be deceiving if used to select the predictor variable to build a linear model for a dependent variable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01295v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mustafa Attallah</dc:creator>
    </item>
    <item>
      <title>A causal viewpoint on prediction model performance under changes in case-mix: discrimination and calibration respond differently for prognosis and diagnosis predictions</title>
      <link>https://arxiv.org/abs/2409.01444</link>
      <description>arXiv:2409.01444v1 Announce Type: new 
Abstract: Prediction models inform important clinical decisions, aiding in diagnosis, prognosis, and treatment planning. The predictive performance of these models is typically assessed through discrimination and calibration. However, changes in the distribution of the data impact model performance. In health-care, a typical change is a shift in case-mix: for example, for cardiovascular risk managment, a general practitioner sees a different mix of patients than a specialist in a tertiary hospital.
  This work introduces a novel framework that differentiates the effects of case-mix shifts on discrimination and calibration based on the causal direction of the prediction task. When prediction is in the causal direction (often the case for prognosis preditions), calibration remains stable under case-mix shifts, while discrimination does not. Conversely, when predicting in the anti-causal direction (often with diagnosis predictions), discrimination remains stable, but calibration does not.
  A simulation study and empirical validation using cardiovascular disease prediction models demonstrate the implications of this framework. This framework provides critical insights for evaluating and deploying prediction models across different clinical settings, emphasizing the importance of understanding the causal structure of the prediction task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01444v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wouter A. C. van Amsterdam</dc:creator>
    </item>
    <item>
      <title>Modelling Volatilities of High-dimensional Count Time Series with Network Structure and Asymmetry</title>
      <link>https://arxiv.org/abs/2409.01521</link>
      <description>arXiv:2409.01521v1 Announce Type: new 
Abstract: Modelling high-dimensional volatilities is a challenging topic, especially for high-dimensional discrete-valued time series data. This paper proposes a threshold spatial GARCH-type model for high-dimensional count data with network structure. The proposed model can simplify the parameterization by taking use of the network structure in data, and can capture the asymmetry in dynamics of volatilities by adopting a threshold structure. Our model is called Poisson Threshold Network GARCH model, because the conditional distributions are assumed to be Poisson distribution. Asymptotic theory of our maximum-likelihood-estimator (MLE) for the proposed spatial model is derived when both sample size and network dimension go to infinity. We get asymptotic statistical inferences via investigating the week dependence among components of the model and using limit theorems for weekly dependent random fields. Simulations are conducted to test the theoretical results, and the model is fitted to real count data as illustration of the proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01521v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yue Pan, Jiazhu Pan</dc:creator>
    </item>
    <item>
      <title>Multivariate Inference of Network Moments by Subsampling</title>
      <link>https://arxiv.org/abs/2409.01599</link>
      <description>arXiv:2409.01599v1 Announce Type: new 
Abstract: In this paper, we study the characterization of a network population by analyzing a single observed network, focusing on the counts of multiple network motifs or their corresponding multivariate network moments. We introduce an algorithm based on node subsampling to approximate the nontrivial joint distribution of the network moments, and prove its asymptotic accuracy. By examining the joint distribution of these moments, our approach captures complex dependencies among network motifs, making a significant advancement over earlier methods that rely on individual motifs marginally. This enables more accurate and robust network inference. Through real-world applications, such as comparing coexpression networks of distinct gene sets and analyzing collaboration patterns within the statistical community, we demonstrate that the multivariate inference of network moments provides deeper insights than marginal approaches, thereby enhancing our understanding of network mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01599v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingyu Qi, Tianxi Li, Wen Zhou</dc:creator>
    </item>
    <item>
      <title>Multi-objective Bayesian optimization for Likelihood-Free inference in sequential sampling models of decision making</title>
      <link>https://arxiv.org/abs/2409.01735</link>
      <description>arXiv:2409.01735v1 Announce Type: new 
Abstract: Joint modeling of different data sources in decision-making processes is crucial for understanding decision dynamics in consumer behavior models. Sequential Sampling Models (SSMs), grounded in neuro-cognitive principles, provide a systematic approach to combining information from multi-source data, such as those based on response times and choice outcomes. However, parameter estimation of SSMs is challenging due to the complexity of joint likelihood functions. Likelihood-Free inference (LFI) approaches enable Bayesian inference in complex models with intractable likelihoods, like SSMs, and only require the ability to simulate synthetic data from the model. Extending a popular approach to simulation efficient LFI for single-source data, we propose Multi-objective Bayesian Optimization for Likelihood-Free Inference (MOBOLFI) to estimate the parameters of SSMs calibrated using multi-source data. MOBOLFI models a multi-dimensional discrepancy between observed and simulated data, using a discrepancy for each data source. Multi-objective Bayesian Optimization is then used to ensure simulation efficient approximation of the SSM likelihood. The use of a multivariate discrepancy allows for approximations to individual data source likelihoods in addition to the joint likelihood, enabling both the detection of conflicting information and a deeper understanding of the importance of different data sources in estimating individual SSM parameters. We illustrate the advantages of our approach in comparison with the use of a single discrepancy in a simple synthetic data example and an SSM example with real-world data assessing preferences of ride-hailing drivers in Singapore to rent electric vehicles. Although we focus on applications to SSMs, our approach applies to the Likelihood-Free calibration of other models using multi-source data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01735v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Chen, Xinwei Li, Eui-Jin Kim, Prateek Bansal, David Nott</dc:creator>
    </item>
    <item>
      <title>Estimating Joint interventional distributions from marginal interventional data</title>
      <link>https://arxiv.org/abs/2409.01794</link>
      <description>arXiv:2409.01794v1 Announce Type: new 
Abstract: In this paper we show how to exploit interventional data to acquire the joint conditional distribution of all the variables using the Maximum Entropy principle. To this end, we extend the Causal Maximum Entropy method to make use of interventional data in addition to observational data. Using Lagrange duality, we prove that the solution to the Causal Maximum Entropy problem with interventional constraints lies in the exponential family, as in the Maximum Entropy solution. Our method allows us to perform two tasks of interest when marginal interventional distributions are provided for any subset of the variables. First, we show how to perform causal feature selection from a mixture of observational and single-variable interventional data, and, second, how to infer joint interventional distributions. For the former task, we show on synthetically generated data, that our proposed method outperforms the state-of-the-art method on merging datasets, and yields comparable results to the KCI-test which requires access to joint observations of all variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01794v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sergio Hernan Garrido Mejia, Elke Kirschbaum, Armin Keki\'c, Atalanti Mastakouri</dc:creator>
    </item>
    <item>
      <title>Partial membership models for soft clustering of multivariate football player performance data</title>
      <link>https://arxiv.org/abs/2409.01874</link>
      <description>arXiv:2409.01874v1 Announce Type: new 
Abstract: The standard mixture modelling framework has been widely used to study heterogeneous populations, by modelling them as being composed of a finite number of homogeneous sub-populations. However, the standard mixture model assumes that each data point belongs to one and only one mixture component, or cluster, but when data points have fractional membership in multiple clusters this assumption is unrealistic. It is in fact conceptually very different to represent an observation as partly belonging to multiple groups instead of belonging to one group with uncertainty. For this purpose, various soft clustering approaches, or individual-level mixture models, have been developed. In this context, Heller et al (2008) formulated the Bayesian partial membership model (PM) as an alternative structure for individual-level mixtures, which also captures partial membership in the form of attribute specific mixtures, but does not assume a factorization over attributes. Our work proposes using the PM for soft clustering of count data arising in football performance analysis and compare the results with those achieved with the mixed membership model and finite mixture model. Learning and inference are carried out using Markov chain Monte Carlo methods. The method is applied on Serie A football player data from the 2022/2023 football season, to estimate the positions on the field where the players tend to play, in addition to their primary position, based on their playing style. The application of partial membership model to football data could have practical implications for coaches, talent scouts, team managers and analysts. These stakeholders can utilize the findings to make informed decisions related to team strategy, talent acquisition, and statistical research, ultimately enhancing performance and understanding in the field of football.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01874v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emiliano Seri, Roberto Rocci, Thomas Brendan Murphy</dc:creator>
    </item>
    <item>
      <title>Bayesian CART models for aggregate claim modeling</title>
      <link>https://arxiv.org/abs/2409.01908</link>
      <description>arXiv:2409.01908v1 Announce Type: new 
Abstract: This paper proposes three types of Bayesian CART (or BCART) models for aggregate claim amount, namely, frequency-severity models, sequential models and joint models. We propose a general framework for the BCART models applicable to data with multivariate responses, which is particularly useful for the joint BCART models with a bivariate response: the number of claims and aggregate claim amount. To facilitate frequency-severity modeling, we investigate BCART models for the right-skewed and heavy-tailed claim severity data by using various distributions. We discover that the Weibull distribution is superior to gamma and lognormal distributions, due to its ability to capture different tail characteristics in tree models. Additionally, we find that sequential BCART models and joint BCART models, which incorporate dependence between the number of claims and average severity, are beneficial and thus preferable to the frequency-severity BCART models in which independence is assumed. The effectiveness of these models' performance is illustrated by carefully designed simulations and real insurance data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01908v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yaojun Zhang, Lanpeng Ji, Georgios Aivaliotis, Charles C. Taylor</dc:creator>
    </item>
    <item>
      <title>Variable selection in convex nonparametric least squares via structured Lasso: An application to the Swedish electricity market</title>
      <link>https://arxiv.org/abs/2409.01911</link>
      <description>arXiv:2409.01911v1 Announce Type: new 
Abstract: We study the problem of variable selection in convex nonparametric least squares (CNLS). Whereas the least absolute shrinkage and selection operator (Lasso) is a popular technique for least squares, its variable selection performance is unknown in CNLS problems. In this work, we investigate the performance of the Lasso CNLS estimator and find out it is usually unable to select variables efficiently. Exploiting the unique structure of the subgradients in CNLS, we develop a structured Lasso by combining $\ell_1$-norm and $\ell_{\infty}$-norm. To improve its predictive performance, we propose a relaxed version of the structured Lasso where we can control the two effects--variable selection and model shrinkage--using an additional tuning parameter. A Monte Carlo study is implemented to verify the finite sample performances of the proposed approaches. In the application of Swedish electricity distribution networks, when the regression model is assumed to be semi-nonparametric, our methods are extended to the doubly penalized CNLS estimators. The results from the simulation and application confirm that the proposed structured Lasso performs favorably, generally leading to sparser and more accurate predictive models, relative to the other variable selection methods in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01911v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiqiang Liao</dc:creator>
    </item>
    <item>
      <title>$Q_B$ Optimal Two-Level Designs for the Baseline Parameterization</title>
      <link>https://arxiv.org/abs/2409.01926</link>
      <description>arXiv:2409.01926v1 Announce Type: new 
Abstract: We have established the association matrix that expresses the estimator of effects under baseline parameterization, which has been considered in some recent literature, in an equivalent form as a linear combination of estimators of effects under the traditional centered parameterization. This allows the generalization of the $Q_B$ criterion which evaluates designs under model uncertainty in the traditional centered parameterization to be applicable to the baseline parameterization. Some optimal designs under the baseline parameterization seen in the previous literature are evaluated and it has been shown that at a given prior probability of a main effect being in the best model, the design converges to $Q_B$ optimal as the probability of an interaction being in the best model converges to 0 from above. The $Q_B$ optimal designs for two setups of factors and run sizes at various priors are found by an extended coordinate exchange algorithm and the evaluation of their performances are discussed. Comparisons have been made to those optimal designs restricted to level balance and orthogonality conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01926v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xietao Zhou, Steven G. Gilmour</dc:creator>
    </item>
    <item>
      <title>Spatially-dependent Indian Buffet Processes</title>
      <link>https://arxiv.org/abs/2409.01943</link>
      <description>arXiv:2409.01943v1 Announce Type: new 
Abstract: We develop a new stochastic process called spatially-dependent Indian buffet processes (SIBP) for spatially correlated binary matrices and propose general spatial factor models for various multivariate response variables. We introduce spatial dependency through the stick-breaking representation of the original Indian buffet process (IBP) and latent Gaussian process for the logit-transformed breaking proportion to capture underlying spatial correlation. We show that the marginal limiting properties of the number of non-zero entries under SIBP are the same as those in the original IBP, while the joint probability is affected by the spatial correlation. Using binomial expansion and Polya-gamma data augmentation, we provide a novel Gibbs sampling algorithm for posterior computation. The usefulness of the SIBP is demonstrated through simulation studies and two applications for large-dimensional multinomial data of areal dialects and geographical distribution of multiple tree species.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01943v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shonosuke Sugasawa, Daichi Mochihashi</dc:creator>
    </item>
    <item>
      <title>Formalizing the causal interpretation in accelerated failure time models with unmeasured heterogeneity</title>
      <link>https://arxiv.org/abs/2409.01983</link>
      <description>arXiv:2409.01983v1 Announce Type: new 
Abstract: In the presence of unmeasured heterogeneity, the hazard ratio for exposure has a complex causal interpretation. To address this, accelerated failure time (AFT) models, which assess the effect on the survival time ratio scale, are often suggested as a better alternative. AFT models also allow for straightforward confounder adjustment. In this work, we formalize the causal interpretation of the acceleration factor in AFT models using structural causal models and data under independent censoring. We prove that the acceleration factor is a valid causal effect measure, even in the presence of frailty and treatment effect heterogeneity. Through simulations, we show that the acceleration factor better captures the causal effect than the hazard ratio when both AFT and proportional hazards models apply. Additionally, we extend the interpretation to systems with time-dependent acceleration factors, revealing the challenge of distinguishing between a time-varying homogeneous effect and unmeasured heterogeneity. While the causal interpretation of acceleration factors is promising, we caution practitioners about potential challenges in estimating these factors in the presence of effect heterogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01983v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mari Brathovde, Hein Putter, Morten Valberg, Richard A. J. Post</dc:creator>
    </item>
    <item>
      <title>Objective Weights for Scoring: The Automatic Democratic Method</title>
      <link>https://arxiv.org/abs/2409.02087</link>
      <description>arXiv:2409.02087v1 Announce Type: new 
Abstract: When comparing performance (of products, services, entities, etc.), multiple attributes are involved. This paper deals with a way of weighting these attributes when one is seeking an overall score. It presents an objective approach to generating the weights in a scoring formula which avoids personal judgement. The first step is to find the maximum possible score for each assessed entity. These upper bound scores are found using Data Envelopment Analysis. In the second step the weights in the scoring formula are found by regressing the unique DEA scores on the attribute data. Reasons for using least squares and avoiding other distance measures are given. The method is tested on data where the true scores and weights are known. The method enables the construction of an objective scoring formula which has been generated from the data arising from all assessed entities and is, in that sense, democratic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02087v1</guid>
      <category>stat.ME</category>
      <category>econ.TH</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.22367/mcdm.2022.17.04</arxiv:DOI>
      <arxiv:journal_reference>Multiple Criteria Decision Making (17), 69-84 (2022)</arxiv:journal_reference>
      <dc:creator>Chris Tofallis</dc:creator>
    </item>
    <item>
      <title>CEopt: A MATLAB Package for Non-convex Optimization with the Cross-Entropy Method</title>
      <link>https://arxiv.org/abs/2409.00013</link>
      <description>arXiv:2409.00013v1 Announce Type: cross 
Abstract: This paper introduces CEopt (https://ceopt.org), a MATLAB tool leveraging the Cross-Entropy method for non-convex optimization. Due to the relative simplicity of the algorithm, it provides a kind of transparent ``gray-box'' optimization solver, with intuitive control parameters. Unique in its approach, CEopt effectively handles both equality and inequality constraints using an augmented Lagrangian method, offering robustness and scalability for moderately sized complex problems. Through select case studies, the package's applicability and effectiveness in various optimization scenarios are showcased, marking CEopt as a practical addition to optimization research and application toolsets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00013v1</guid>
      <category>stat.CO</category>
      <category>cs.MS</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Americo Cunha Jr, Marcos Vinicius Issa, Julio Cesar Basilio, Jos\'e Geraldo Telles Ribeiro</dc:creator>
    </item>
    <item>
      <title>Learning linear acyclic causal model including Gaussian noise using ancestral relationships</title>
      <link>https://arxiv.org/abs/2409.00417</link>
      <description>arXiv:2409.00417v1 Announce Type: cross 
Abstract: This paper discusses algorithms for learning causal DAGs. The PC algorithm makes no assumptions other than the faithfulness to the causal model and can identify only up to the Markov equivalence class. LiNGAM assumes linearity and continuous non-Gaussian disturbances for the causal model, and the causal DAG defining LiNGAM is shown to be fully identifiable. The PC-LiNGAM, a hybrid of the PC algorithm and LiNGAM, can identify up to the distribution-equivalence pattern of a linear causal model, even in the presence of Gaussian disturbances. However, in the worst case, the PC-LiNGAM has factorial time complexity for the number of variables. In this paper, we propose an algorithm for learning the distribution-equivalence patterns of a linear causal model with a lower time complexity than PC-LiNGAM, using the causal ancestor finding algorithm in Maeda and Shimizu, which is generalized to account for Gaussian disturbances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00417v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ming Cai, Penggang Gao, Hisayuki Hara</dc:creator>
    </item>
    <item>
      <title>CRUD-Capable Mobile Apps with R and shinyMobile: a Case Study in Rapid Prototyping</title>
      <link>https://arxiv.org/abs/2409.00582</link>
      <description>arXiv:2409.00582v1 Announce Type: cross 
Abstract: "Harden" is a Progressive Web Application (PWA) for Ecological Momentary Assessment (EMA) developed mostly in R, which runs on all platforms with an internet connection, including iOS and Android. It leverages the shinyMobile package for creating a reactive mobile user interface (UI), PostgreSQL for the database backend, and Google Cloud Run for scalable hosting in the cloud, with serverless execution. Using this technology stack, it was possible to rapidly prototype a fully CRUD-capable (Create, Read, Update, Delete) mobile app, with persistent user data across sessions, interactive graphs, and real-time statistical calculation. This framework is compared with current alternative frameworks for creating data science apps; it is argued that the shinyMobile package provides one of the most efficient methods for rapid prototyping and creation of statistical mobile apps that require advanced graphing capabilities. This paper outlines the methodology used to create the Harden application, and discusses the advantages and limitations of the shinyMobile approach to app development. It is hoped that this information will encourage other programmers versed in R to consider developing mobile apps with this framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00582v1</guid>
      <category>cs.SE</category>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan Henry</dc:creator>
    </item>
    <item>
      <title>Stochastic Monotonicity and Random Utility Models: The Good and The Ugly</title>
      <link>https://arxiv.org/abs/2409.00704</link>
      <description>arXiv:2409.00704v1 Announce Type: cross 
Abstract: When it comes to structural estimation of risk preferences from data on choices, random utility models have long been one of the standard research tools in economics. A recent literature has challenged these models, pointing out some concerning monotonicity and, thus, identification problems. In this paper, we take a second look and point out that some of the criticism - while extremely valid - may have gone too far, demanding monotonicity of choice probabilities in decisions where it is not so clear whether it should be imposed. We introduce a new class of random utility models based on carefully constructed generalized risk premia which always satisfy our relaxed monotonicity criteria. Moreover, we show that some of the models used in applied research like the certainty-equivalent-based random utility model for CARA utility actually lie in this class of monotonic stochastic choice models. We conclude that not all random utility models are bad.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00704v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Henk Keffert, Nikolaus Schweizer</dc:creator>
    </item>
    <item>
      <title>EnsLoss: Stochastic Calibrated Loss Ensembles for Preventing Overfitting in Classification</title>
      <link>https://arxiv.org/abs/2409.00908</link>
      <description>arXiv:2409.00908v1 Announce Type: cross 
Abstract: Empirical risk minimization (ERM) with a computationally feasible surrogate loss is a widely accepted approach for classification. Notably, the convexity and calibration (CC) properties of a loss function ensure consistency of ERM in maximizing accuracy, thereby offering a wide range of options for surrogate losses. In this article, we propose a novel ensemble method, namely \textsc{EnsLoss}, which extends the ensemble learning concept to combine loss functions within the ERM framework. A key feature of our method is the consideration on preserving the ``legitimacy'' of the combined losses, i.e., ensuring the CC properties. Specifically, we first transform the CC conditions of losses into loss-derivatives, thereby bypassing the need for explicit loss functions and directly generating calibrated loss-derivatives. Therefore, inspired by Dropout, \textsc{EnsLoss} enables loss ensembles through one training process with doubly stochastic gradient descent (i.e., random batch samples and random calibrated loss-derivatives). We theoretically establish the statistical consistency of our approach and provide insights into its benefits. The numerical effectiveness of \textsc{EnsLoss} compared to fixed loss methods is demonstrated through experiments on a broad range of 14 OpenML tabular datasets and 46 image datasets with various deep learning architectures. Python repository and source code are available on \textsc{GitHub} at \url{https://github.com/statmlben/rankseg}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00908v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ben Dai</dc:creator>
    </item>
    <item>
      <title>Simultaneous Inference for Non-Stationary Random Fields, with Application to Gridded Data Analysis</title>
      <link>https://arxiv.org/abs/2409.01220</link>
      <description>arXiv:2409.01220v1 Announce Type: cross 
Abstract: Current statistics literature on statistical inference of random fields typically assumes that the fields are stationary or focuses on models of non-stationary Gaussian fields with parametric/semiparametric covariance families, which may not be sufficiently flexible to tackle complex modern-era random field data. This paper performs simultaneous nonparametric statistical inference for a general class of non-stationary and non-Gaussian random fields by modeling the fields as nonlinear systems with location-dependent transformations of an underlying `shift random field'. Asymptotic results, including concentration inequalities and Gaussian approximation theorems for high dimensional sparse linear forms of the random field, are derived. A computationally efficient locally weighted multiplier bootstrap algorithm is proposed and theoretically verified as a unified tool for the simultaneous inference of the aforementioned non-stationary non-Gaussian random field. Simulations and real-life data examples demonstrate good performances and broad applications of the proposed algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01220v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yunyi Zhang, Zhou Zhou</dc:creator>
    </item>
    <item>
      <title>Sample Complexity of the Sign-Perturbed Sums Method</title>
      <link>https://arxiv.org/abs/2409.01243</link>
      <description>arXiv:2409.01243v1 Announce Type: cross 
Abstract: We study the sample complexity of the Sign-Perturbed Sums (SPS) method, which constructs exact, non-asymptotic confidence regions for the true system parameters under mild statistical assumptions, such as independent and symmetric noise terms. The standard version of SPS deals with linear regression problems, however, it can be generalized to stochastic linear (dynamical) systems, even with closed-loop setups, and to nonlinear and nonparametric problems, as well. Although the strong consistency of the method was rigorously proven, the sample complexity of the algorithm was only analyzed so far for scalar linear regression problems. In this paper we study the sample complexity of SPS for general linear regression problems. We establish high probability upper bounds for the diameters of SPS confidence regions for finite sample sizes and show that the SPS regions shrink at the same, optimal rate as the classical asymptotic confidence ellipsoids. Finally, the difference between the theoretical bounds and the empirical sizes of SPS confidence regions is investigated experimentally.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01243v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <category>eess.SY</category>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Szabolcs Szentp\'eteri, Bal\'azs Csan\'ad Cs\'aji</dc:creator>
    </item>
    <item>
      <title>Double Machine Learning meets Panel Data -- Promises, Pitfalls, and Potential Solutions</title>
      <link>https://arxiv.org/abs/2409.01266</link>
      <description>arXiv:2409.01266v1 Announce Type: cross 
Abstract: Estimating causal effect using machine learning (ML) algorithms can help to relax functional form assumptions if used within appropriate frameworks. However, most of these frameworks assume settings with cross-sectional data, whereas researchers often have access to panel data, which in traditional methods helps to deal with unobserved heterogeneity between units. In this paper, we explore how we can adapt double/debiased machine learning (DML) (Chernozhukov et al., 2018) for panel data in the presence of unobserved heterogeneity. This adaptation is challenging because DML's cross-fitting procedure assumes independent data and the unobserved heterogeneity is not necessarily additively separable in settings with nonlinear observed confounding. We assess the performance of several intuitively appealing estimators in a variety of simulations. While we find violations of the cross-fitting assumptions to be largely inconsequential for the accuracy of the effect estimates, many of the considered methods fail to adequately account for the presence of unobserved heterogeneity. However, we find that using predictive models based on the correlated random effects approach (Mundlak, 1978) within DML leads to accurate coefficient estimates across settings, given a sample size that is large relative to the number of observed confounders. We also show that the influence of the unobserved heterogeneity on the observed confounders plays a significant role for the performance of most alternative methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01266v1</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan Fuhr, Dominik Papies</dc:creator>
    </item>
    <item>
      <title>Stein transport for Bayesian inference</title>
      <link>https://arxiv.org/abs/2409.01464</link>
      <description>arXiv:2409.01464v1 Announce Type: cross 
Abstract: We introduce $\textit{Stein transport}$, a novel methodology for Bayesian inference designed to efficiently push an ensemble of particles along a predefined curve of tempered probability distributions. The driving vector field is chosen from a reproducing kernel Hilbert space and can be derived either through a suitable kernel ridge regression formulation or as an infinitesimal optimal transport map in the Stein geometry. The update equations of Stein transport resemble those of Stein variational gradient descent (SVGD), but introduce a time-varying score function as well as specific weights attached to the particles. While SVGD relies on convergence in the long-time limit, Stein transport reaches its posterior approximation at finite time $t=1$. Studying the mean-field limit, we discuss the errors incurred by regularisation and finite-particle effects, and we connect Stein transport to birth-death dynamics and Fisher-Rao gradient flows. In a series of experiments, we show that in comparison to SVGD, Stein transport not only often reaches more accurate posterior approximations with a significantly reduced computational budget, but that it also effectively mitigates the variance collapse phenomenon commonly observed in SVGD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01464v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nikolas N\"usken</dc:creator>
    </item>
    <item>
      <title>Smoothed Robust Phase Retrieval</title>
      <link>https://arxiv.org/abs/2409.01570</link>
      <description>arXiv:2409.01570v1 Announce Type: cross 
Abstract: The phase retrieval problem in the presence of noise aims to recover the signal vector of interest from a set of quadratic measurements with infrequent but arbitrary corruptions, and it plays an important role in many scientific applications. However, the essential geometric structure of the nonconvex robust phase retrieval based on the $\ell_1$-loss is largely unknown to study spurious local solutions, even under the ideal noiseless setting, and its intrinsic nonsmooth nature also impacts the efficiency of optimization algorithms. This paper introduces the smoothed robust phase retrieval (SRPR) based on a family of convolution-type smoothed loss functions. Theoretically, we prove that the SRPR enjoys a benign geometric structure with high probability: (1) under the noiseless situation, the SRPR has no spurious local solutions, and the target signals are global solutions, and (2) under the infrequent but arbitrary corruptions, we characterize the stationary points of the SRPR and prove its benign landscape, which is the first landscape analysis of phase retrieval with corruption in the literature. Moreover, we prove the local linear convergence rate of gradient descent for solving the SRPR under the noiseless situation. Experiments on both simulated datasets and image recovery are provided to demonstrate the numerical performance of the SRPR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01570v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhong Zheng, Lingzhou Xue</dc:creator>
    </item>
    <item>
      <title>Taming Randomness in Agent-Based Models using Common Random Numbers</title>
      <link>https://arxiv.org/abs/2409.02086</link>
      <description>arXiv:2409.02086v1 Announce Type: cross 
Abstract: Random numbers are at the heart of every agent-based model (ABM) of health and disease. By representing each individual in a synthetic population, agent-based models enable detailed analysis of intervention impact and parameter sensitivity. Yet agent-based modeling has a fundamental signal-to-noise problem, in which small differences between simulations cannot be reliably differentiated from stochastic noise resulting from misaligned random number realizations. We introduce a novel methodology that eliminates noise due to misaligned random numbers, a first for agent-based modeling. Our approach enables meaningful individual-level analysis between ABM scenarios because all differences are driven by mechanistic effects rather than random number noise. A key result is that many fewer simulations are needed for some applications. We demonstrate the benefits of our approach on three disparate examples and discuss limitations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02086v1</guid>
      <category>q-bio.QM</category>
      <category>q-bio.PE</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel J. Klein, Romesh G. Abeysuriya, Robyn M. Stuart, Cliff C. Kerr</dc:creator>
    </item>
    <item>
      <title>Beta-CoRM: A Bayesian Approach for $n$-gram Profiles Analysis</title>
      <link>https://arxiv.org/abs/2011.11558</link>
      <description>arXiv:2011.11558v3 Announce Type: replace 
Abstract: $n$-gram profiles have been successfully and widely used to analyse long sequences of potentially differing lengths for clustering or classification. Mainly, machine learning algorithms have been used for this purpose but, despite their predictive performance, these methods cannot discover hidden structures or provide a full probabilistic representation of the data. A novel class of Bayesian generative models designed for $n$-gram profiles used as binary attributes have been designed to address this. The flexibility of the proposed modelling allows to consider a straightforward approach to feature selection in the generative model. Furthermore, a slice sampling algorithm is derived for a fast inferential procedure, which is applied to synthetic and real data scenarios and shows that feature selection can improve classification accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2011.11558v3</guid>
      <category>stat.ME</category>
      <category>cs.CR</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jos\'e A. Perusqu\'ia, Jim E. Griffin, Cristiano Villa</dc:creator>
    </item>
    <item>
      <title>Long-term Causal Inference Under Persistent Confounding via Data Combination</title>
      <link>https://arxiv.org/abs/2202.07234</link>
      <description>arXiv:2202.07234v5 Announce Type: replace 
Abstract: We study the identification and estimation of long-term treatment effects when both experimental and observational data are available. Since the long-term outcome is observed only after a long delay, it is not measured in the experimental data, but only recorded in the observational data. However, both types of data include observations of some short-term outcomes. In this paper, we uniquely tackle the challenge of persistent unmeasured confounders, i.e., some unmeasured confounders that can simultaneously affect the treatment, short-term outcomes and the long-term outcome, noting that they invalidate identification strategies in previous literature. To address this challenge, we exploit the sequential structure of multiple short-term outcomes, and develop three novel identification strategies for the average long-term treatment effect. We further propose three corresponding estimators and prove their asymptotic consistency and asymptotic normality. We finally apply our methods to estimate the effect of a job training program on long-term employment using semi-synthetic data. We numerically show that our proposals outperform existing methods that fail to handle persistent confounders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.07234v5</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guido Imbens, Nathan Kallus, Xiaojie Mao, Yuhao Wang</dc:creator>
    </item>
    <item>
      <title>Evidence Estimation in Gaussian Graphical Models Using a Telescoping Block Decomposition of the Precision Matrix</title>
      <link>https://arxiv.org/abs/2205.01016</link>
      <description>arXiv:2205.01016v4 Announce Type: replace 
Abstract: Marginal likelihood, also known as model evidence, is a fundamental quantity in Bayesian statistics. It is used for model selection using Bayes factors or for empirical Bayes tuning of prior hyper-parameters. Yet, the calculation of evidence has remained a longstanding open problem in Gaussian graphical models. Currently, the only feasible solutions that exist are for special cases such as the Wishart or G-Wishart, in moderate dimensions. We develop an approach based on a novel telescoping block decomposition of the precision matrix that allows the estimation of evidence by application of Chib's technique under a very broad class of priors under mild requirements. Specifically, the requirements are: (a) the priors on the diagonal terms on the precision matrix can be written as gamma or scale mixtures of gamma random variables and (b) those on the off-diagonal terms can be represented as normal or scale mixtures of normal. This includes structured priors such as the Wishart or G-Wishart, and more recently introduced element-wise priors, such as the Bayesian graphical lasso and the graphical horseshoe. Among these, the true marginal is known in an analytically closed form for Wishart, providing a useful validation of our approach. For the general setting of the other three, and several more priors satisfying conditions (a) and (b) above, the calculation of evidence has remained an open question that this article resolves under a unifying framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.01016v4</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anindya Bhadra, Ksheera Sagar, David Rowe, Sayantan Banerjee, Jyotishka Datta</dc:creator>
    </item>
    <item>
      <title>Inference for linear functionals of high-dimensional longitudinal proteomics data using generalized estimating equations</title>
      <link>https://arxiv.org/abs/2207.11686</link>
      <description>arXiv:2207.11686v3 Announce Type: replace 
Abstract: Regression analysis of correlated data, where multiple correlated responses are recorded on the same unit, is ubiquitous in many scientific areas. With the advent of new technologies, in particular high-throughput omics profiling assays, such correlated data increasingly consist of large number of variables compared with the available sample size. Motivated by recent longitudinal proteomics studies of COVID-19, we propose a novel inference procedure for linear functionals of high-dimensional regression coefficients in generalized estimating equations, which are widely used to analyze correlated data. Our estimator for this more general inferential target, obtained via constructing projected estimating equations, is shown to be asymptotically normally distributed under mild regularity conditions. We also introduce a data-driven cross-validation procedure to select the tuning parameter for estimating the projection direction, which is not addressed in the existing procedures. We illustrate the utility of the proposed procedure in providing confidence intervals for associations of individual proteins and severe COVID risk scores obtained based on high-dimensional proteomics data, and demonstrate its robust finite-sample performance, especially in estimation bias and confidence interval coverage, via extensive simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.11686v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lu Xia, Ali Shojaie</dc:creator>
    </item>
    <item>
      <title>Large-Scale Low-Rank Gaussian Process Prediction with Support Points</title>
      <link>https://arxiv.org/abs/2207.12804</link>
      <description>arXiv:2207.12804v2 Announce Type: replace 
Abstract: Low-rank approximation is a popular strategy to tackle the "big n problem" associated with large-scale Gaussian process regressions. Basis functions for developing low-rank structures are crucial and should be carefully specified. Predictive processes simplify the problem by inducing basis functions with a covariance function and a set of knots. The existing literature suggests certain practical implementations of knot selection and covariance estimation; however, theoretical foundations explaining the influence of these two factors on predictive processes are lacking. In this paper, the asymptotic prediction performance of the predictive process and Gaussian process predictions is derived and the impacts of the selected knots and estimated covariance are studied. We suggest the use of support points as knots, which best represent data locations. Extensive simulation studies demonstrate the superiority of support points and verify our theoretical results. Real data of precipitation and ozone are used as examples, and the efficiency of our method over other widely used low-rank approximation methods is verified.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.12804v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yan Song, Wenlin Dai, Marc G. Genton</dc:creator>
    </item>
    <item>
      <title>Estimating network-mediated causal effects via principal components network regression</title>
      <link>https://arxiv.org/abs/2212.12041</link>
      <description>arXiv:2212.12041v3 Announce Type: replace 
Abstract: We develop a method to decompose causal effects on a social network into an indirect effect mediated by the network, and a direct effect independent of the social network. To handle the complexity of network structures, we assume that latent social groups act as causal mediators. We develop principal components network regression models to differentiate the social effect from the non-social effect. Fitting the regression models is as simple as principal components analysis followed by ordinary least squares estimation. We prove asymptotic theory for regression coefficients from this procedure and show that it is widely applicable, allowing for a variety of distributions on the regression errors and network edges. We carefully characterize the counterfactual assumptions necessary to use the regression models for causal inference, and show that current approaches to causal network regression may result in over-control bias. The structure of our method is very general, so that it is applicable to many types of structured data beyond social networks, such as text, areal data, psychometrics, images and omics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.12041v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alex Hayes, Mark M. Fredrickson, Keith Levin</dc:creator>
    </item>
    <item>
      <title>Robust Realized Integrated Beta Estimator with Application to Dynamic Analysis of Integrated Beta</title>
      <link>https://arxiv.org/abs/2301.06742</link>
      <description>arXiv:2301.06742v2 Announce Type: replace 
Abstract: In this paper, we develop a robust non-parametric realized integrated beta estimator using high-frequency financial data contaminated by microstructure noises, which is robust to the stylized features, such as the time-varying beta and the dependence structure of microstructure noises. With this robust realized integrated beta estimator, we investigate dynamic structures of integrated betas and find an auto-regressive--moving-average (ARMA) structure. To model this dynamic structure, we utilize the ARMA model for daily integrated market betas. We call this the dynamic realized beta (DR Beta). We further introduce a high-frequency data generating process by filling the gap between the high-frequency-based non-parametric estimator and low-frequency dynamic structure. Then, we propose a quasi-likelihood procedure for estimating the model parameters with the robust realized integrated beta estimator as the proxy. We also establish asymptotic theorems for the proposed estimator and conduct a simulation study to check the performance of finite samples of the estimator. The empirical study with the S&amp;P 500 index and the top 50 large trading volume stocks from the S&amp;P 500 illustrates that the proposed DR Beta model with the robust realized beta estimator effectively accounts for dynamics in the market beta of individual stocks and better predicts future market betas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.06742v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minseog Oh, Donggyu Kim, Yazhen Wang</dc:creator>
    </item>
    <item>
      <title>Bayesian nonparametric modeling of latent partitions via Stirling-gamma priors</title>
      <link>https://arxiv.org/abs/2306.02360</link>
      <description>arXiv:2306.02360v2 Announce Type: replace 
Abstract: Dirichlet process mixtures are particularly sensitive to the value of the precision parameter controlling the behavior of the latent partition. Randomization of the precision through a prior distribution is a common solution, which leads to more robust inferential procedures. However, existing prior choices do not allow for transparent elicitation, due to the lack of analytical results. We introduce and investigate a novel prior for the Dirichlet process precision, the Stirling-gamma distribution. We study the distributional properties of the induced random partition, with an emphasis on the number of clusters. Our theoretical investigation clarifies the reasons of the improved robustness properties of the proposed prior. Moreover, we show that, under specific choices of its hyperparameters, the Stirling-gamma distribution is conjugate to the random partition of a Dirichlet process. We illustrate with an ecological application the usefulness of our approach for the detection of communities of ant workers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.02360v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessandro Zito, Tommaso Rigon, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>Flexible Bayesian Modeling for Longitudinal Binary and Ordinal Responses</title>
      <link>https://arxiv.org/abs/2307.00224</link>
      <description>arXiv:2307.00224v2 Announce Type: replace 
Abstract: Longitudinal studies with binary or ordinal responses are widely encountered in various disciplines, where the primary focus is on the temporal evolution of the probability of each response category. Traditional approaches build from the generalized mixed effects modeling framework. Even amplified with nonparametric priors placed on the fixed or random effects, such models are restrictive due to the implied assumptions on the marginal expectation and covariance structure of the responses. We tackle the problem from a functional data analysis perspective, treating the observations for each subject as realizations from subject-specific stochastic processes at the measured times. We develop the methodology focusing initially on binary responses, for which we assume the stochastic processes have Binomial marginal distributions. Leveraging the logits representation, we model the discrete space processes through sequences of continuous space processes. We utilize a hierarchical framework to model the mean and covariance kernel of the continuous space processes nonparametrically and simultaneously through a Gaussian process prior and an Inverse-Wishart process prior, respectively. The prior structure results in flexible inference for the evolution and correlation of binary responses, while allowing for borrowing of strength across all subjects. The modeling approach can be naturally extended to ordinal responses. Here, the continuation-ratio logits factorization of the multinomial distribution is key for efficient modeling and inference, including a practical way of dealing with unbalanced longitudinal data. The methodology is illustrated with synthetic data examples and an analysis of college students' mental health status data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.00224v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jizhou Kang, Athanasios Kottas</dc:creator>
    </item>
    <item>
      <title>Scalable Estimation of Multinomial Response Models with Random Consideration Sets</title>
      <link>https://arxiv.org/abs/2308.12470</link>
      <description>arXiv:2308.12470v3 Announce Type: replace 
Abstract: A common assumption in the fitting of unordered multinomial response models for $J$ mutually exclusive categories is that the responses arise from the same set of $J$ categories across subjects. However, when responses measure a choice made by the subject, it is more appropriate to condition the distribution of multinomial responses on a subject-specific consideration set, drawn from the power set of $\{1,2,\ldots,J\}$. This leads to a mixture of multinomial response models governed by a probability distribution over the $J^{\ast} = 2^J -1$ consideration sets. We introduce a novel method for estimating such generalized multinomial response models based on the fundamental result that any mass distribution over $J^{\ast}$ consideration sets can be represented as a mixture of products of $J$ component-specific inclusion-exclusion probabilities. Moreover, under time-invariant consideration sets, the conditional posterior distribution of consideration sets is sparse. These features enable a scalable MCMC algorithm for sampling the posterior distribution of parameters, random effects, and consideration sets. Under regularity conditions, the posterior distributions of the marginal response probabilities and the model parameters satisfy consistency. The methodology is demonstrated in a longitudinal data set on weekly cereal purchases that cover $J = 101$ brands, a dimension substantially beyond the reach of existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.12470v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siddhartha Chib, Kenichi Shimizu</dc:creator>
    </item>
    <item>
      <title>Multivariate moment least-squares estimators for reversible Markov chains</title>
      <link>https://arxiv.org/abs/2310.06330</link>
      <description>arXiv:2310.06330v2 Announce Type: replace 
Abstract: Markov chain Monte Carlo (MCMC) is a commonly used method for approximating expectations with respect to probability distributions. Uncertainty assessment for MCMC estimators is essential in practical applications. Moreover, for multivariate functions of a Markov chain, it is important to estimate not only the auto-correlation for each component but also to estimate cross-correlations, in order to better assess sample quality, improve estimates of effective sample size, and use more effective stopping rules. Berg and Song [2022] introduced the moment least squares (momentLS) estimator, a shape-constrained estimator for the autocovariance sequence from a reversible Markov chain, for univariate functions of the Markov chain. Based on this sequence estimator, they proposed an estimator of the asymptotic variance of the sample mean from MCMC samples. In this study, we propose novel autocovariance sequence and asymptotic variance estimators for Markov chain functions with multiple components, based on the univariate momentLS estimators from Berg and Song [2022]. We demonstrate strong consistency of the proposed auto(cross)-covariance sequence and asymptotic variance matrix estimators. We conduct empirical comparisons of our method with other state-of-the-art approaches on simulated and real-data examples, using popular samplers including the random-walk Metropolis sampler and the No-U-Turn sampler from STAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.06330v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hyebin Song, Stephen Berg</dc:creator>
    </item>
    <item>
      <title>A Simple Bias Reduction for Chatterjee's Correlation</title>
      <link>https://arxiv.org/abs/2312.15496</link>
      <description>arXiv:2312.15496v3 Announce Type: replace 
Abstract: Chatterjee's rank correlation coefficient $\xi_n$ is an empirical index for detecting functional dependencies between two variables $X$ and $Y$. It is an estimator for a theoretical quantity $\xi$ that is zero for independence and one if $Y$ is a measurable function of $X$. Based on an equivalent characterization of sorted numbers, we derive an upper bound for $\xi_n$ and suggest a simple normalization aimed at reducing its bias for small sample size $n$. In Monte Carlo simulations of various models, the normalization reduced the bias in all cases. The mean squared error was reduced, too, for values of $\xi$ greater than about 0.4. Moreover, we observed that non-parametric confidence intervals for $\xi$ based on bootstrapping $\xi_n$ in the usual n-out-of-n way have a coverage probability close to zero. This is remedied by an m-out-of-n bootstrap without replacement in combination with our normalization method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.15496v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christoph Dalitz, Juliane Arning, Steffen Goebbels</dc:creator>
    </item>
    <item>
      <title>The power of visualizing distributional differences: Formal graphical $n$-sample tests</title>
      <link>https://arxiv.org/abs/2403.01838</link>
      <description>arXiv:2403.01838v2 Announce Type: replace 
Abstract: Classical tests are available for the two-sample test of correspondence of distribution functions. From these, the Kolmogorov-Smirnov test provides also the graphical interpretation of the test results, in different forms. Here, we propose modifications of the Kolmogorov-Smirnov test with higher power. The proposed tests are based on the so-called global envelope test which allows for graphical interpretation, similarly as the Kolmogorov-Smirnov test. The tests are based on rank statistics and are suitable also for the comparison of $n$ samples, with $n \geq 2$. We compare the alternatives for the two-sample case through an extensive simulation study and discuss their interpretation. Finally, we apply the tests to real data. Specifically, we compare the height distributions between boys and girls at different ages, the sepal length distributions of different flower species, and distributions of standardized residuals from a time series model for different exchange courses using the proposed methodologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01838v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Konstantinos Konstantinou, Tom\'a\v{s} Mrkvi\v{c}ka, Mari Myllym\"aki</dc:creator>
    </item>
    <item>
      <title>Quantile balancing inverse probability weighting for non-probability samples</title>
      <link>https://arxiv.org/abs/2403.09726</link>
      <description>arXiv:2403.09726v2 Announce Type: replace 
Abstract: Usage of non-statistical data sources for statistical purposes have become increasingly popular in recent years, also in official statistics. However, statistical inference based on non-probability samples is made more difficult by nature of them being biased and not representative of the target population. In this paper we propose quantile balancing inverse probability weighting estimator (QBIPW) for non-probability samples. We use the idea of Harms and Duchesne (2006) which allows to include quantile information in the estimation process so known totals and distribution for auxiliary variables are being reproduced. We discuss the estimation of the QBIPW probabilities and its variance. Our simulation study has demonstrated that the proposed estimators are robust against model mis-specification and, as a result, help to reduce bias and mean squared error. Finally, we applied the proposed method to estimate the share of vacancies aimed at Ukrainian workers in Poland using an integrated set of administrative and survey data about job vacancies</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09726v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maciej Ber\k{e}sewicz, Marcin Szymkowiak, Piotr Chlebicki</dc:creator>
    </item>
    <item>
      <title>L-Estimation in Instrumental Variables Regression for Censored Data in Presence of Endogeneity and Dependent Errors</title>
      <link>https://arxiv.org/abs/2405.19145</link>
      <description>arXiv:2405.19145v2 Announce Type: replace 
Abstract: In this article, we propose L-estimators of the unknown parameters in the instrumental variables regression in the presence of censored data under endogeneity. We allow the random errors involved in the model to be dependent. The proposed estimation procedure is a two-stage procedure, and the large sample properties of the proposed estimators are established. The utility of the proposed methodology is demonstrated for various simulated data and a benchmark real data set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19145v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Swati Shukla, Subhra Sankar Dhar,  Shalabh</dc:creator>
    </item>
    <item>
      <title>Functional Clustering for Longitudinal Associations between Social Determinants of Health and Stroke Mortality in the US</title>
      <link>https://arxiv.org/abs/2406.10499</link>
      <description>arXiv:2406.10499v3 Announce Type: replace 
Abstract: Understanding the longitudinally changing associations between Social Determinants of Health (SDOH) and stroke mortality is essential for effective stroke management. Previous studies have uncovered significant regional disparities in the relationships between SDOH and stroke mortality. However, existing studies have not utilized longitudinal associations to develop data-driven methods for regional division in stroke control. To fill this gap, we propose a novel clustering method to analyze SDOH -- stroke mortality associations in US counties. To enhance the interpretability of the clustering outcomes, we introduce a novel regularized expectation-maximization algorithm equipped with various sparsity-and-smoothness-pursued penalties, aiming at simultaneous clustering and variable selection in longitudinal associations. As a result, we can identify crucial SDOH that contribute to longitudinal changes in stroke mortality. This facilitates the clustering of US counties into different regions based on the relationships between these SDOH and stroke mortality. The effectiveness of our proposed method is demonstrated through extensive numerical studies. By applying our method to longitudinal data on SDOH and stroke mortality at the county level, we identify 18 important SDOH for stroke mortality and divide the US counties into two clusters based on these selected SDOH. Our findings unveil complex regional heterogeneity in the longitudinal associations between SDOH and stroke mortality, providing valuable insights into region-specific SDOH adjustments for mitigating stroke mortality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10499v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fangzhi Luo, Jianbin Tan, Donglan Zhang, Hui Huang, Ye Shen</dc:creator>
    </item>
    <item>
      <title>Horseshoe-type Priors for Independent Component Estimation</title>
      <link>https://arxiv.org/abs/2406.17058</link>
      <description>arXiv:2406.17058v2 Announce Type: replace 
Abstract: Independent Component Estimation (ICE) has many applications in modern day machine learning as a feature engineering extraction method. Horseshoe-type priors are used to provide scalable algorithms that enables both point estimates via expectation-maximization (EM) and full posterior sampling via Markov Chain Monte Carlo (MCMC) algorithms. Our methodology also applies to flow-based methods for nonlinear feature extraction and deep learning. We also discuss how to implement conditional posteriors and envelope-based methods for optimization. Through this hierarchy representation, we unify a number of hitherto disparate estimation procedures. We illustrate our methodology and algorithms on a numerical example. Finally, we conclude with directions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17058v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jyotishka Datta, Nicholas G. Polson</dc:creator>
    </item>
    <item>
      <title>On a General Theoretical Framework of Reliability</title>
      <link>https://arxiv.org/abs/2407.00716</link>
      <description>arXiv:2407.00716v2 Announce Type: replace 
Abstract: Reliability is an essential measure of how closely observed scores represent latent scores (reflecting constructs), assuming some latent variable measurement model. We present a general theoretical framework of reliability, placing emphasis on measuring the association between latent and observed scores. This framework was inspired by McDonald's (2011) regression framework, which highlighted the coefficient of determination as a measure of reliability. We extend McDonald's (2011) framework beyond coefficients of determination and introduce four desiderata for reliability measures (estimability, normalization, symmetry, and invariance). We also present theoretical examples to illustrate distinct measures of reliability and report on a numerical study that demonstrates the behavior of different reliability measures. We conclude with a discussion on the use of reliability coefficients and outline future avenues of research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00716v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yang Liu, Jolynn Pek, Alberto Maydeu-Olivares</dc:creator>
    </item>
    <item>
      <title>Distributionally robust risk evaluation with an isotonic constraint</title>
      <link>https://arxiv.org/abs/2407.06867</link>
      <description>arXiv:2407.06867v2 Announce Type: replace 
Abstract: Statistical learning under distribution shift is challenging when neither prior knowledge nor fully accessible data from the target distribution is available. Distributionally robust learning (DRL) aims to control the worst-case statistical performance within an uncertainty set of candidate distributions, but how to properly specify the set remains challenging. To enable distributional robustness without being overly conservative, in this paper, we propose a shape-constrained approach to DRL, which incorporates prior information about the way in which the unknown target distribution differs from its estimate. More specifically, we assume the unknown density ratio between the target distribution and its estimate is isotonic with respect to some partial order. At the population level, we provide a solution to the shape-constrained optimization problem that does not involve the isotonic constraint. At the sample level, we provide consistency results for an empirical estimator of the target in a range of different settings. Empirical studies on both synthetic and real data examples demonstrate the improved accuracy of the proposed shape-constrained approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06867v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yu Gui, Rina Foygel Barber, Cong Ma</dc:creator>
    </item>
    <item>
      <title>Minimum Gamma Divergence for Regression and Classification Problems</title>
      <link>https://arxiv.org/abs/2408.01893</link>
      <description>arXiv:2408.01893v2 Announce Type: replace 
Abstract: The book is structured into four main chapters. Chapter 1 introduces the foundational concepts of divergence measures, including the well-known Kullback-Leibler divergence and its limitations. It then presents a detailed exploration of power divergences, such as the $\alpha$, $\beta$, and $\gamma$-divergences, highlighting their unique properties and advantages. Chapter 2 explores minimum divergence methods for regression models, demonstrating how these methods can improve robustness and efficiency in statistical estimation. Chapter 3 extends these methods to Poisson point processes, with a focus on ecological applications, providing a robust framework for modeling species distributions and other spatial phenomena. Finally, Chapter 4 explores the use of divergence measures in machine learning, including applications in Boltzmann machines, AdaBoost, and active learning. The chapter emphasizes the practical benefits of these measures in enhancing model robustness and performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01893v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shinto Eguchi</dc:creator>
    </item>
    <item>
      <title>A Restricted Latent Class Model with Polytomous Attributes and Respondent-Level Covariates</title>
      <link>https://arxiv.org/abs/2408.13143</link>
      <description>arXiv:2408.13143v2 Announce Type: replace 
Abstract: We present an exploratory restricted latent class model where response data is for a single time point, polytomous, and differing across items, and where latent classes reflect a multi-attribute state where each attribute is ordinal. Our model extends previous work to allow for correlation of the attributes through a multivariate probit and to allow for respondent-specific covariates. We demonstrate that the model recovers parameters well in a variety of realistic scenarios, and apply the model to the analysis of a particular dataset designed to diagnose depression. The application demonstrates the utility of the model in identifying the latent structure of depression beyond single-factor approaches which have been used in the past.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13143v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric Alan Wayman, Steven Andrew Culpepper, Jeff Douglas, Jesse Bowers</dc:creator>
    </item>
    <item>
      <title>On the role of surrogates in the efficient estimation of treatment effects with limited outcome data</title>
      <link>https://arxiv.org/abs/2003.12408</link>
      <description>arXiv:2003.12408v4 Announce Type: replace-cross 
Abstract: In many experimental and observational studies, the outcome of interest is often difficult or expensive to observe, reducing effective sample sizes for estimating average treatment effects (ATEs) even when identifiable. We study how incorporating data on units for which only surrogate outcomes not of primary interest are observed can increase the precision of ATE estimation. We refrain from imposing stringent surrogacy conditions, which permit surrogates as perfect replacements for the target outcome. Instead, we supplement the available, albeit limited, observations of the target outcome with abundant observations of surrogate outcomes, without any assumptions beyond unconfounded treatment assignment and missingness and corresponding overlap conditions. To quantify the potential gains, we derive the difference in efficiency bounds on ATE estimation with and without surrogates, both when an overwhelming or comparable number of units have missing outcomes. We develop robust ATE estimation and inference methods that realize these efficiency gains. We empirically demonstrate the gains by studying long-term-earning effects of job training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2003.12408v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nathan Kallus, Xiaojie Mao</dc:creator>
    </item>
    <item>
      <title>Estimation of Optimal Dynamic Treatment Assignment Rules under Policy Constraints</title>
      <link>https://arxiv.org/abs/2106.05031</link>
      <description>arXiv:2106.05031v5 Announce Type: replace-cross 
Abstract: Many policies involve dynamics in their treatment assignments, where individuals receive sequential interventions over multiple stages. We study estimation of an optimal dynamic treatment regime that guides the optimal treatment assignment for each individual at each stage based on their history. We propose an empirical welfare maximization approach in this dynamic framework, which estimates the optimal dynamic treatment regime using data from an experimental or quasi-experimental study while satisfying exogenous constraints on policies. The paper proposes two estimation methods: one solves the treatment assignment problem sequentially through backward induction, and the other solves the entire problem simultaneously across all stages. We establish finite-sample upper bounds on worst-case average welfare regrets for these methods and show their optimal $n^{-1/2}$ convergence rates. We also modify the simultaneous estimation method to accommodate intertemporal budget/capacity constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2106.05031v5</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shosei Sakaguchi</dc:creator>
    </item>
    <item>
      <title>Leave-group-out cross-validation for latent Gaussian models</title>
      <link>https://arxiv.org/abs/2210.04482</link>
      <description>arXiv:2210.04482v5 Announce Type: replace-cross 
Abstract: Evaluating the predictive performance of a statistical model is commonly done using cross-validation. Although the leave-one-out method is frequently employed, its application is justified primarily for independent and identically distributed observations. However, this method tends to mimic interpolation rather than prediction when dealing with dependent observations. This paper proposes a modified cross-validation for dependent observations. This is achieved by excluding an automatically determined set of observations from the training set to mimic a more reasonable prediction scenario. Also, within the framework of latent Gaussian models, we illustrate a method to adjust the joint posterior for this modified cross-validation to avoid model refitting. This new approach is accessible in the R-INLA package (www.r-inla.org).</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.04482v5</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhedong Liu, Haavard Rue</dc:creator>
    </item>
    <item>
      <title>A multi-language toolkit for supporting automated checking of research outputs</title>
      <link>https://arxiv.org/abs/2212.02935</link>
      <description>arXiv:2212.02935v2 Announce Type: replace-cross 
Abstract: This article presents the automatic checking of research outputs package acro, which assists researchers and data governance teams by automatically applying best-practice principles-based statistical disclosure control (SDC) techniques on-the-fly as researchers conduct their analyses. acro distinguishes between: research output that is safe to publish; output that requires further analysis; and output that cannot be published because it creates substantial risk of disclosing private data. This is achieved through the use of a lightweight Python wrapper that sits over well-known analysis tools that produce outputs such as tables, plots, and statistical models. This adds functionality to (i) identify potentially disclosive outputs against a range of commonly used disclosure tests; (ii) apply disclosure mitigation strategies where required; (iii) report reasons for applying SDC; and (iv) produce simple summary documents trusted research environment staff can use to streamline their workflow. The major analytical programming languages used by researchers are supported: Python, R, and Stata. The acro code and documentation are available under an MIT license at https://github.com/AI-SDC/ACRO</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.02935v2</guid>
      <category>cs.CR</category>
      <category>cs.IR</category>
      <category>cs.SE</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Richard J. Preen, Maha Albashir, Simon Davy, Jim Smith</dc:creator>
    </item>
    <item>
      <title>A stopping rule for randomly sampling bipartite networks with fixed degree sequences</title>
      <link>https://arxiv.org/abs/2305.04937</link>
      <description>arXiv:2305.04937v5 Announce Type: replace-cross 
Abstract: Statistical analysis of bipartite networks frequently requires randomly sampling from the set of all bipartite networks with the same degree sequence as an observed network. Trade algorithms offer an efficient way to generate samples of bipartite networks by incrementally `trading' the positions of some of their edges. However, it is difficult to know how many such trades are required to ensure that the sample is random. I propose a stopping rule that focuses on the distance between sampled networks and the observed network, and stops performing trades when this distribution stabilizes. Analyses demonstrate that, for over 650 different degree sequences, using this stopping rule ensures a random sample with a high probability, and that it is practical for use in empirical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.04937v5</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zachary P. Neal</dc:creator>
    </item>
    <item>
      <title>Parameter Inference for Hypo-Elliptic Diffusions under a Weak Design Condition</title>
      <link>https://arxiv.org/abs/2312.04444</link>
      <description>arXiv:2312.04444v2 Announce Type: replace-cross 
Abstract: We address the problem of parameter estimation for degenerate diffusion processes defined via the solution of Stochastic Differential Equations (SDEs) with diffusion matrix that is not full-rank. For this class of hypo-elliptic diffusions recent works have proposed contrast estimators that are asymptotically normal, provided that the step-size in-between observations $\Delta=\Delta_n$ and their total number $n$ satisfy $n \to \infty$, $n \Delta_n \to \infty$, $\Delta_n \to 0$, and additionally $\Delta_n = o (n^{-1/2})$. This latter restriction places a requirement for a so-called `rapidly increasing experimental design'. In this paper, we overcome this limitation and develop a general contrast estimator satisfying asymptotic normality under the weaker design condition $\Delta_n = o(n^{-1/p})$ for general $p \ge 2$. Such a result has been obtained for elliptic SDEs in the literature, but its derivation in a hypo-elliptic setting is highly non-trivial. We provide numerical results to illustrate the advantages of the developed theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.04444v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuga Iguchi, Alexandros Beskos</dc:creator>
    </item>
    <item>
      <title>Measure transport with kernel mean embeddings</title>
      <link>https://arxiv.org/abs/2401.12967</link>
      <description>arXiv:2401.12967v2 Announce Type: replace-cross 
Abstract: Kalman filters constitute a scalable and robust methodology for approximate Bayesian inference, matching first and second order moments of the target posterior. To improve the accuracy in nonlinear and non-Gaussian settings, we extend this principle to include more or different characteristics, based on kernel mean embeddings (KMEs) of probability measures into reproducing kernel Hilbert spaces. Focusing on the continuous-time setting, we develop a family of interacting particle systems (termed $\textit{KME-dynamics}$) that bridge between prior and posterior, and that include the Kalman-Bucy filter as a special case. KME-dynamics does not require the score of the target, but rather estimates the score implicitly and intrinsically, and we develop links to score-based generative modeling and importance reweighting. A variant of KME-dynamics has recently been derived from an optimal transport and Fisher-Rao gradient flow perspective by Maurais and Marzouk, and we expose further connections to (kernelised) diffusion maps, leading to a variational formulation of regression type. Finally, we conduct numerical experiments on toy examples and the Lorenz 63 and 96 models, comparing our results against the ensemble Kalman filter and the mapping particle filter (Pulido and van Leeuwen, 2019, J. Comput. Phys.). Our experiments show particular promise for a hybrid modification (called Kalman-adjusted KME-dynamics).</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.12967v2</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L. Wang, N. N\"usken</dc:creator>
    </item>
    <item>
      <title>Bivariate change point detection in movement direction and speed</title>
      <link>https://arxiv.org/abs/2402.02489</link>
      <description>arXiv:2402.02489v2 Announce Type: replace-cross 
Abstract: Biological movement patterns can sometimes be quasi linear with abrupt changes in direction and speed, as in plastids in root cells investigated here. For the analysis of such changes we propose a new stochastic model for movement along linear structures. Maximum likelihood estimators are provided, and due to serial dependencies of increments, the classical MOSUM statistic is replaced by a moving kernel estimator. Convergence of the resulting difference process and strong consistency of the variance estimator are shown. We estimate the change points and propose a graphical technique to distinguish between change points in movement direction and speed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02489v2</guid>
      <category>math.ST</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Solveig Plomer, Theresa Ernst, Philipp Gebhardt, Enrico Schleiff, Ralph Neininger, Gaby Schneider</dc:creator>
    </item>
    <item>
      <title>Adaptive Split Balancing for Optimal Random Forest</title>
      <link>https://arxiv.org/abs/2402.11228</link>
      <description>arXiv:2402.11228v2 Announce Type: replace-cross 
Abstract: In this paper, we propose a new random forest algorithm that constructs the trees using a novel adaptive split-balancing method. Rather than relying on the widely-used random feature selection, we propose a permutation-based balanced splitting criterion. The adaptive split balancing forest (ASBF), achieves minimax optimality under the Lipschitz class. Its localized version, which fits local regressions at the leaf level, attains the minimax rate under the broad H\"older class $\mathcal{H}^{q,\beta}$ of problems for any $q\in\mathbb{N}$ and $\beta\in(0,1]$. We identify that over-reliance on auxiliary randomness in tree construction may compromise the approximation power of trees, leading to suboptimal results. Conversely, the proposed less random, permutation-based approach demonstrates optimality over a wide range of models. Although random forests are known to perform well empirically, their theoretical convergence rates are slow. Simplified versions that construct trees without data dependence offer faster rates but lack adaptability during tree growth. Our proposed method achieves optimality in simple, smooth scenarios while adaptively learning the tree structure from the data. Additionally, we establish uniform upper bounds and demonstrate that ASBF improves dimensionality dependence in average treatment effect estimation problems. Simulation studies and real-world applications demonstrate our methods' superior performance over existing random forests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.11228v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuqian Zhang, Weijie Ji, Jelena Bradic</dc:creator>
    </item>
    <item>
      <title>Uplift Modeling Under Limited Supervision</title>
      <link>https://arxiv.org/abs/2403.19289</link>
      <description>arXiv:2403.19289v4 Announce Type: replace-cross 
Abstract: Estimating causal effects in e-commerce tends to involve costly treatment assignments which can be impractical in large-scale settings. Leveraging machine learning to predict such treatment effects without actual intervention is a standard practice to diminish the risk. However, existing methods for treatment effect prediction tend to rely on training sets of substantial size, which are built from real experiments and are thus inherently risky to create. In this work we propose a graph neural network to diminish the required training set size, relying on graphs that are common in e-commerce data. Specifically, we view the problem as node regression with a restricted number of labeled instances, develop a two-model neural architecture akin to previous causal effect estimators, and test varying message-passing layers for encoding. Furthermore, as an extra step, we combine the model with an acquisition function to guide the creation of the training set in settings with extremely low experimental budget. The framework is flexible since each step can be used separately with other models or treatment policies. The experiments on real large-scale networks indicate a clear advantage of our methodology over the state of the art, which in many cases performs close to random, underlining the need for models that can generalize with limited supervision to reduce experimental risks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19289v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>George Panagopoulos, Daniele Malitesta, Fragkiskos D. Malliaros, Jun Pang</dc:creator>
    </item>
    <item>
      <title>Extending the blended generalized extreme value distribution</title>
      <link>https://arxiv.org/abs/2407.06875</link>
      <description>arXiv:2407.06875v2 Announce Type: replace-cross 
Abstract: The generalized extreme value (GEV) distribution is commonly employed to help estimate the likelihood of extreme events in many geophysical and other application areas. The recently proposed blended generalized extreme value (bGEV) distribution modifies the GEV with positive shape parameter to avoid a hard lower bound that complicates fitting and inference. Here, the bGEV is extended to the GEV with negative shape parameter, avoiding a hard upper bound that is unrealistic in many applications. This extended bGEV is shown to improve on the GEV for forecasting heat and sea level extremes based on past data. Software implementing this bGEV and applying it to the example temperature and sea level data is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06875v2</guid>
      <category>stat.AP</category>
      <category>physics.ao-ph</category>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Nir Y. Krakauer</dc:creator>
    </item>
  </channel>
</rss>
