<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 10 Jul 2024 01:37:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 09 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Poster: Flexible Scheduling of Network and Computing Resources for Distributed AI Tasks</title>
      <link>https://arxiv.org/abs/2407.04845</link>
      <description>arXiv:2407.04845v1 Announce Type: new 
Abstract: Many emerging Artificial Intelligence (AI) applications require on-demand provisioning of large-scale computing, which can only be enabled by leveraging distributed computing services interconnected through networking. To address such increasing demand for networking to serve AI tasks, we investigate new scheduling strategies to improve communication efficiency and test them on a programmable testbed. We also show relevant challenges and research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04845v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruikun Wang, Jiawei Zhang, Qiaolun Zhang, Bojun Zhang, Zhiqun Gu, Aryanaz Attarpour, Yuefeng Ji, Massimo Tornatore</dc:creator>
    </item>
    <item>
      <title>Cost and Power-Consumption Analysis for Power Profile Monitoring with Multiple Monitors per Link in Optical Networks</title>
      <link>https://arxiv.org/abs/2407.04977</link>
      <description>arXiv:2407.04977v1 Announce Type: new 
Abstract: Network monitoring is essential to collect compre-hensive data on signal quality in optical networks. As deploying large amounts of monitoring equipment results in elevated cost and power consumption, novel low-cost monitoring methods are continuously being investigated. A new technique called Power Profile Monitoring (PPM) has recently gained traction thanks to its ability to monitor an entire lightpath using a single post-processing unit at the lightpath receiver. PPM does not require to deploy an individual monitor for each span, as in the traditional monitoring technique using Optical Time-Domain Reflectometer (OTDR). PPM and OTDR have different monitoring applications, which will be elaborated in our discussion, hence they can be considered either alternative or complementary techniques according to the targeted monitoring capabilities to be implemented in the network. In this work, we aim to quantify the cost and power consumption of PPM (using OTDR as a baseline reference), as this analysis can provide guidelines for the implementation and deployment of PPM. First, we discuss how PPM and OTDR monitors are deployed, and we formally state a new Optimized Monitoring Placement (OMP) problem for PPM. Solving the OMP problem allows to identify the minimum number of PPM monitors that guarantees that all links in the networks are monitored by at least n PPM monitors (note that using n &gt; 1 allows for increased monitoring accuracy). We prove the NP-hardness of the OMP problem and formulate it using an Integer Linear Programming (ILP) model. Finally, we also devise a heuristic algorithm for the OMP problem to scale to larger topologies. Our numerical results, obtained on realistic topologies, suggest that the cost (power) of one PPM module should be lower than 2.6 times and 10.2 times that of one OTDR for nation-wide and continental-wide topology, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04977v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiaolun Zhang, Patricia Layec, Alix May, Annalisa Morea, Aryanaz Attarpour, Massimo Tornatore</dc:creator>
    </item>
    <item>
      <title>Listen-While-Talking: Toward dApp-based Real-Time Spectrum Sharing in O-RAN</title>
      <link>https://arxiv.org/abs/2407.05027</link>
      <description>arXiv:2407.05027v1 Announce Type: new 
Abstract: This demo paper presents a dApp-based real-time spectrum sharing scenario where a 5th generation (5G) base station implementing the NR stack adapts its transmission and reception strategies based on the incumbent priority users in the Citizen Broadband Radio Service (CBRS) band. The dApp is responsible for obtaining relevant measurements from the Next Generation Node Base (gNB), running the spectrum sensing inference, and configuring the gNB with a control action upon detecting the primary incumbent user transmissions. This approach is built on dApps, which extend the O-RAN framework to the real-time and user plane domains. Thus, it avoids the need of dedicated Spectrum Access Systems (SASs) in the CBRS band. The demonstration setup is based on the open-source 5G OpenAirInterface (OAI) framework, where we have implemented a dApp interfaced with a gNB and communicating with a Commercial Off-the-Shelf (COTS) User Equipment (UE) in an over-the-air wireless environment. When an incumbent user has active transmission, the dApp will detect and inform the primary user presence to the gNB. The dApps will also enforce a control policy that adapts the scheduling and transmission policy of the Radio Access Network (RAN). This demo provides valuable insights into the potential of using dApp-based spectrum sensing with O-RAN architecture in next generation cellular networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05027v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rajeev Gangula, Andrea Lacava, Michele Polese, Salvatore D'Oro, Leonardo Bonati, Florian Kaltenberger, Pedram Johari, Tommaso Melodia</dc:creator>
    </item>
    <item>
      <title>Fault-tolerant Network Design for Bounded Delay Data Transfer from PMUs to Control Center</title>
      <link>https://arxiv.org/abs/2407.05211</link>
      <description>arXiv:2407.05211v1 Announce Type: new 
Abstract: Communication network design for monitoring the state of an electric power grid has received significant attention in recent years. In order to measure stability of a power grid, it is imperative that measurement data collected by the Phasor Measurement Units (PMUs) located at the Sub-Stations (SS) must arrive at the Control Center (CC) within a specified delay threshold delta. In earlier papers we formalized the design problem as the Rooted Delay Constrained Minimum Spanning Tree (RDCMST) problem. However, RDCMST does not provide any fault-tolerance capability, as failure of just one communication link would prevent PMU data from reaching the CC. In this paper, we study the optimal cost network design problem with fault tolerance capability. In our model the PMU data from the SSs will have a path to reach the CC in spite of the failure of at most R links within the delay threshold delta. If R = 1, each SS will have two link disjoint paths of length at most delta to the CC. In other words, each SS will be on a cycle with the CC. We refer to this problem as the Rooted Delay Constrained Minimum Cost Cycle Cover (RDCMCCC) problem. We provide computational complexity analysis, an Integer Linear Programming formulation to find the optimal solution and a heuristic based on the sweeping technique. We evaluate the performance of our heuristic with real substation location data of Arizona.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05211v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>A. Sen, C. Sumnicht, S. Adeniye, D. Patel, S. Choudhuri</dc:creator>
    </item>
    <item>
      <title>A Queueing Theoretic Perspective on Low-Latency LLM Inference with Variable Token Length</title>
      <link>https://arxiv.org/abs/2407.05347</link>
      <description>arXiv:2407.05347v1 Announce Type: new 
Abstract: Large language models (LLMs) propel the prosperity of interactive AI applications showcased by ChatGPT that demand timely response of inference services. However, LLM inference is computation intensive and memory intensive, and improper parameter configuration at LLM platforms may exacerbate the inference time. In this paper, we analyze the impact of LLM output token distribution on the inference queueing delay, where the max-token clipping and the batched inference are considered. By formulating an M/G/1 model, we observe that enforcing a maximum output token limit on a very small fraction of inference requests can significantly reduce the queueing delay, and our model facilitates the selection of the optimal limit. For the batch inference, we model the service process as a bulk queue in which the batch processing time is affected by the batch size and the maximum token size inside this batch jointly. The queueing delays of the batching of all buffered requests (dynamic batching), the batching of constant number of requests (fixed batching), and the batching without intra-batch waiting (elastic batching) are derived. Experimental results show that our mathematical models coincide with the event-driven simulations well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05347v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuqing Yang, Yuedong Xu, Lei Jiao</dc:creator>
    </item>
    <item>
      <title>Adaptive Video Streaming over 6G Networks: Buffer Control and User Behavior Analysis</title>
      <link>https://arxiv.org/abs/2407.05436</link>
      <description>arXiv:2407.05436v1 Announce Type: new 
Abstract: This paper delves into the synergistic potential of adaptive video streaming over emerging 6G wireless networks, emphasizing innovative buffer control techniques and detailed analysis of user viewing behaviors. As 6G technology heralds a new era with significantly enhanced capabilities including higher bandwidths, lower latencies, and increased connection densities, it is poised to fundamentally transform video streaming services. This study explores the integration of these technological advancements to optimize video streaming processes, ensuring seamless service delivery and superior Quality of Experience (QoE) for users. We propose novel buffer management strategies that leverage the ultra-reliable and low-latency communication features of 6G networks to mitigate issues related to video streaming such as rebuffering and quality fluctuations. Additionally, we examine how insights into viewing behaviors can inform adaptive streaming algorithms, allowing for real-time adjustments that align with user preferences and viewing conditions. The implications of our findings are demonstrated through rigorous simulation studies, which validate the effectiveness of our proposed solutions across diverse scenarios. This research not only highlights the challenges faced in deploying adaptive streaming solutions over 6G but also outlines future directions for research and development in this fast-evolving field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05436v1</guid>
      <category>cs.NI</category>
      <category>cs.GR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kevin Nassisid, Teef David, Kassi Muhammad</dc:creator>
    </item>
    <item>
      <title>Cost-Efficient Computation Offloading in SAGIN: A Deep Reinforcement Learning and Perception-Aided Approach</title>
      <link>https://arxiv.org/abs/2407.05571</link>
      <description>arXiv:2407.05571v1 Announce Type: new 
Abstract: The Space-Air-Ground Integrated Network (SAGIN), crucial to the advancement of sixth-generation (6G) technology, plays a key role in ensuring universal connectivity, particularly by addressing the communication needs of remote areas lacking cellular network infrastructure. This paper delves into the role of unmanned aerial vehicles (UAVs) within SAGIN, where they act as a control layer owing to their adaptable deployment capabilities and their intermediary role. Equipped with millimeter-wave (mmWave) radar and vision sensors, these UAVs are capable of acquiring multi-source data, which helps to diminish uncertainty and enhance the accuracy of decision-making. Concurrently, UAVs collect tasks requiring computing resources from their coverage areas, originating from a variety of mobile devices moving at different speeds. These tasks are then allocated to ground base stations (BSs), low-earth-orbit (LEO) satellite, and local processing units to improve processing efficiency. Amidst this framework, our study concentrates on devising dynamic strategies for facilitating task hosting between mobile devices and UAVs, offloading computations, managing associations between UAVs and BSs, and allocating computing resources. The objective is to minimize the time-averaged network cost, considering the uncertainty of device locations, speeds, and even types. To tackle these complexities, we propose a deep reinforcement learning and perception-aided online approach (DRL-and-Perception-aided Approach) for this joint optimization in SAGIN, tailored for an environment filled with uncertainties. The effectiveness of our proposed approach is validated through extensive numerical simulations, which quantify its performance relative to various network parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05571v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yulan Gao, Ziqiang Ye, Han Yu</dc:creator>
    </item>
    <item>
      <title>Can We Benefit from Reconfigurable Intelligent Surfaces in Upper Mid-Band 6G Networks? A Critical Look for Promising Use Cases</title>
      <link>https://arxiv.org/abs/2407.05754</link>
      <description>arXiv:2407.05754v1 Announce Type: new 
Abstract: The upper mid-band frequencies (i.e., 7-24,GHz) are regarded as the golden bands for the sixth-generation (6G) wireless communication systems, combining good coverage, much new spectrum, and many antennas in compact form factors. The first 6G networks will most likely use this band. There is much prior work on channel modeling, coexistence, and possible implementation scenarios in these bands. On the other hand, the use of reconfigurable intelligent surfaces (RISs) has not yet been examined in these bands, even if this transformative technology has generally garnered considerable attention in recent years for its ability to enhance spectral efficiency, coverage, and reliability of wireless channels. There are significant frequency-specific challenges related to RIS deployment, use cases, number of required elements, channel estimation, and control. These are previously unaddressed for the upper mid-band. In this paper, we aim to bridge this gap by exploring various use cases, including RIS-assisted fixed wireless access (FWA), enhanced capacity in mobile communications, and increased reliability at the cell edge. We identify the conditions under which RIS can provide major benefits and optimal strategies for deploying RIS to enhance the performance of 6G upper mid-band communication systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05754v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Ferdi Kara, \"Ozlem Tu\u{g}fe Demir, Emil Bj\"ornson</dc:creator>
    </item>
    <item>
      <title>Performance Evaluation of MLO for XR Streaming: Can Wi-Fi 7 Meet the Expectations?</title>
      <link>https://arxiv.org/abs/2407.05802</link>
      <description>arXiv:2407.05802v1 Announce Type: new 
Abstract: Extended Reality (XR) has stringent throughput and delay requirements that are hard to meet with current wireless technologies. Missing these requirements can lead to worsened picture quality, perceived lag between user input and corresponding output, and even dizziness for the end user. In this paper, we study the capability of upcoming Wi-Fi 7, and its novel support for Multi-Link Operation (MLO), to cope with these tight requirements. Our study is based on simulation results extracted from an MLO-compliant simulator that realistically reproduces VR traffic. Results show that MLO can sustain VR applications. By jointly using multiple links with independent channel access procedures, MLO can reduce the overall delay, which is especially useful in the uplink, as it has more stringent requirements than the downlink, and is instrumental in delivering the expected performance. We show that using MLO can allow more users per network than an equivalent number of links using SLO. We also show that while maintaining the same overall bandwidth, a higher number of MLO links with narrow channels leads to lower delays than a lower number of links with wider channels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05802v1</guid>
      <category>cs.NI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marc Carrascosa-Zamacois, Lorenzo Galati-Giordano, Francesc Wilhelmi, Gianluca Fontanesi, Anders Jonsson, Giovanni Geraci, Boris Bellalta</dc:creator>
    </item>
    <item>
      <title>Intelligent Routing as a Service (iRaaS)</title>
      <link>https://arxiv.org/abs/2407.05901</link>
      <description>arXiv:2407.05901v1 Announce Type: new 
Abstract: The scope of the Sixth-Generation Self-Organized Networks (6G-SON) advances its predecessor's capability towards agility, flexibility, and adaptability. On-demand overlay networking technologies have shown a prominent maturity while coping with the rising complexity and scale of enterprise, service provider, and data centre networks. The Software-Defined Networking paradigm has recently offered Model Driven Programmability, minimizing network management complexity through automation and orchestration. However, leveraging Machine Learning-driven network optimization, a.k.a. Knowledge-Defined Networking (KDN), has still been a domain of interest for the Network Softwarization research community. In this article, we propose Intelligent Routing as a Service (iRaaS) architecture as an application layer cognitive routing framework for KDNs. iRaaS offers routing logic customization (i.e., customizing metric function and path-finding algorithm) and provides an option to include heuristic parameters from trained models as a part of the metric calculation. iRaaS sits on the application plane above the knowledge plane in a KDN stack, thus providing platform- and vendor-agnostic coupling with existing network infrastructures. This article covers the scope of iRaaS by using reliability as a heuristic for standard path-discovery algorithms, e.g., Shortest Path First (SPF) and Diffusion Update algorithm (DUAL), along with the architectural specification. We validate our approach through a Proof-of-Concept deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05901v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saptarshi Ghosh, Konstantinos Antonakoglou, Ioannis Mavromatis, Kostas Katsaros</dc:creator>
    </item>
    <item>
      <title>Harnessing Federated Generative Learning for Green and Sustainable Internet of Things</title>
      <link>https://arxiv.org/abs/2407.05915</link>
      <description>arXiv:2407.05915v1 Announce Type: new 
Abstract: The rapid proliferation of devices in the Internet of Things (IoT) has ushered in a transformative era of data-driven connectivity across various domains. However, this exponential growth has raised pressing concerns about environmental sustainability and data privacy. In response to these challenges, this paper introduces One-shot Federated Learning (OSFL), an innovative paradigm that harmonizes sustainability and machine learning within IoT ecosystems. OSFL revolutionizes the traditional Federated Learning (FL) workflow by condensing multiple iterative communication rounds into a single operation, thus significantly reducing energy consumption, communication overhead, and latency. This breakthrough is coupled with the strategic integration of generative learning techniques, ensuring robust data privacy while promoting efficient knowledge sharing among IoT devices. By curtailing resource utilization, OSFL aligns seamlessly with the vision of green and sustainable IoT, effectively extending device lifespans and mitigating their environmental footprint. Our research underscores the transformative potential of OSFL, poised to reshape the landscape of IoT applications across domains such as energy-efficient smart cities and groundbreaking healthcare solutions. This contribution marks a pivotal step towards a more responsible, sustainable, and technologically advanced future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05915v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Yuanhang Qi, M. Shamim Hossain</dc:creator>
    </item>
    <item>
      <title>Time-Sensitive Networking over 5G: Experimental Evaluation of a Hybrid 5G and TSN System with IEEE 802.1Qbv Traffic</title>
      <link>https://arxiv.org/abs/2407.05989</link>
      <description>arXiv:2407.05989v1 Announce Type: new 
Abstract: Underpinned by the IEEE 802.1 standards, Time-sensitive networking (TSN) empowers standard Ethernet to handle stringent real-time requirements of industrial networking. TSN and private 5G will co-exist in industrial systems; hence, converged operation of the two is crucial to achieving end-to-end deterministic performance. This work conducts a testbed-based evaluation of a hybrid 5G and TSN system with over-the-air transmission of scheduled real-time TSN traffic (based on IEEE 802.1Qbv standard). The main objective is to bring the dynamics of hybrid 5G and TSN deployments to spotlight. The testbed comprises off-the-shelf TSN and 5G devices and a near product-grade 5G system. The results show the impact of 802.1Qbv parameters and 5G system capabilities on end-to-end deterministic performance. The findings of this study have significance for design and optimization of 3GPP-defined bridge model (black box model) for 5G/TSN integration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05989v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adnan Aijaz, Sajida Gufran</dc:creator>
    </item>
    <item>
      <title>Delay-Aware Robust Edge Network Hardening Under Decision-Dependent Uncertainty</title>
      <link>https://arxiv.org/abs/2407.06142</link>
      <description>arXiv:2407.06142v1 Announce Type: new 
Abstract: Edge computing promises to offer low-latency and ubiquitous computation to numerous devices at the network edge. For delay-sensitive applications, link delays can have a direct impact on service quality. These delays can fluctuate drastically over time due to various factors such as network congestion, changing traffic conditions, cyberattacks, component failures, and natural disasters. Thus, it is crucial to efficiently harden the edge network to mitigate link delay variation as well as ensure a stable and improved user experience. To this end, we propose a novel robust model for optimal edge network hardening, considering the link delay uncertainty. Departing from the existing literature that treats uncertainties as exogenous, our model incorporates an endogenous uncertainty set to properly capture the impact of hardening and workload allocation decisions on link delays. However, the endogenous set introduces additional complexity to the problem due to the interdependence between decisions and uncertainties. We present two efficient methods to transform the problem into a solvable form. Extensive numerical results are shown to demonstrate the effectiveness of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06142v1</guid>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaming Cheng, Duong Thuy Anh Nguyen, Ni Trieu, Duong Tung Nguyen</dc:creator>
    </item>
    <item>
      <title>Accurate Passive Radar via an Uncertainty-Aware Fusion of Wi-Fi Sensing Data</title>
      <link>https://arxiv.org/abs/2407.04733</link>
      <description>arXiv:2407.04733v1 Announce Type: cross 
Abstract: Wi-Fi devices can effectively be used as passive radar systems that sense what happens in the surroundings and can even discern human activity. We propose, for the first time, a principled architecture which employs Variational Auto-Encoders for estimating a latent distribution responsible for generating the data, and Evidential Deep Learning for its ability to sense out-of-distribution activities. We verify that the fused data processed by different antennas of the same Wi-Fi receiver results in increased accuracy of human activity recognition compared with the most recent benchmarks, while still being informative when facing out-of-distribution samples and enabling semantic interpretation of latent variables in terms of physical phenomena. The results of this paper are a first contribution toward the ultimate goal of providing a flexible, semantic characterisation of black-swan events, i.e., events for which we have limited to no training data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04733v1</guid>
      <category>eess.SP</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.23919/FUSION52260.2023.10224098</arxiv:DOI>
      <dc:creator>Marco Cominelli, Francesco Gringoli, Lance M. Kaplan, Mani B. Srivastava, Federico Cerutti</dc:creator>
    </item>
    <item>
      <title>Neuro-Symbolic Fusion of Wi-Fi Sensing Data for Passive Radar with Inter-Modal Knowledge Transfer</title>
      <link>https://arxiv.org/abs/2407.04734</link>
      <description>arXiv:2407.04734v1 Announce Type: cross 
Abstract: Wi-Fi devices, akin to passive radars, can discern human activities within indoor settings due to the human body's interaction with electromagnetic signals. Current Wi-Fi sensing applications predominantly employ data-driven learning techniques to associate the fluctuations in the physical properties of the communication channel with the human activity causing them. However, these techniques often lack the desired flexibility and transparency. This paper introduces DeepProbHAR, a neuro-symbolic architecture for Wi-Fi sensing, providing initial evidence that Wi-Fi signals can differentiate between simple movements, such as leg or arm movements, which are integral to human activities like running or walking. The neuro-symbolic approach affords gathering such evidence without needing additional specialised data collection or labelling. The training of DeepProbHAR is facilitated by declarative domain knowledge obtained from a camera feed and by fusing signals from various antennas of the Wi-Fi receivers. DeepProbHAR achieves results comparable to the state-of-the-art in human activity recognition. Moreover, as a by-product of the learning process, DeepProbHAR generates specialised classifiers for simple movements that match the accuracy of models trained on finely labelled datasets, which would be particularly costly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04734v1</guid>
      <category>eess.SP</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marco Cominelli, Francesco Gringoli, Lance M. Kaplan, Mani B. Srivastava, Trevor Bihl, Erik P. Blasch, Nandini Iyer, Federico Cerutti</dc:creator>
    </item>
    <item>
      <title>Cellular Automata as a Network Topology</title>
      <link>https://arxiv.org/abs/2407.05048</link>
      <description>arXiv:2407.05048v1 Announce Type: cross 
Abstract: Cellular automata represent physical systems where both space and time are discrete, and the associated physical quantities assume a limited set of values. While previous research has applied cellular automata in modeling chemical, biological, and physical systems, its potential for modeling topological systems, specifically network topologies, remains underexplored. This paper investigates the use of cellular automata to model decentralized network topologies, which could enhance load balancing, fault tolerance, scalability, and the propagation and dissemination of information in distributed systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05048v1</guid>
      <category>nlin.CG</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Temitayo Adefemi</dc:creator>
    </item>
    <item>
      <title>Reverse Engineered MiniFS File System</title>
      <link>https://arxiv.org/abs/2407.05064</link>
      <description>arXiv:2407.05064v1 Announce Type: cross 
Abstract: In an era where digital connectivity is increasingly foundational to daily life, the security of Wi-Fi Access Points (APs) is a critical concern. This paper addresses the vulnerabilities inherent in Wi-Fi APs, with a particular focus on those using proprietary file systems like MiniFS found in TP-Link's AC1900 WiFi router. Through reverse engineering, we unravel the structure and operation of MiniFS, marking a significant advancement in our understanding of this previously opaque file system. Our investigation reveals not only the architecture of MiniFS but also identifies several private keys and underscores a concerning lack of cryptographic protection. These findings point to broader security vulnerabilities, emphasizing the risks of security-by-obscurity practices in an interconnected environment. Our contributions are twofold: firstly, based, on the file system structure, we develop a methodology for the extraction and analysis of MiniFS, facilitating the identification and mitigation of potential vulnerabilities. Secondly, our work lays the groundwork for further research into WiFi APs' security, particularly those running on similar proprietary systems. By highlighting the critical need for transparency and community engagement in firmware analysis, this study contributes to the development of more secure network devices, thus enhancing the overall security posture of digital infrastructures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05064v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3664476.3664511</arxiv:DOI>
      <dc:creator>Dmitrii Belimov, Evgenii Vinogradov</dc:creator>
    </item>
    <item>
      <title>A Generalized Transformer-based Radio Link Failure Prediction Framework in 5G RANs</title>
      <link>https://arxiv.org/abs/2407.05197</link>
      <description>arXiv:2407.05197v1 Announce Type: cross 
Abstract: Radio link failure (RLF) prediction system in Radio Access Networks (RANs) is critical for ensuring seamless communication and meeting the stringent requirements of high data rates, low latency, and improved reliability in 5G networks. However, weather conditions such as precipitation, humidity, temperature, and wind impact these communication links. Usually, historical radio link Key Performance Indicators (KPIs) and their surrounding weather station observations are utilized for building learning-based RLF prediction models. However, such models must be capable of learning the spatial weather context in a dynamic RAN and effectively encoding time series KPIs with the weather observation data. Existing works fail to incorporate both of these essential design aspects of the prediction models. This paper fills the gap by proposing GenTrap, a novel RLF prediction framework that introduces a graph neural network (GNN)-based learnable weather effect aggregation module and employs state-of-the-art time series transformer as the temporal feature extractor for radio link failure prediction. The proposed aggregation method of GenTrap can be integrated into any existing prediction model to achieve better performance and generalizability. We evaluate GenTrap on two real-world datasets (rural and urban) with 2.6 million KPI data points and show that GenTrap offers a significantly higher F1-score (0.93 for rural and 0.79 for urban) compared to its counterparts while possessing generalization capability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05197v1</guid>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kazi Hasan, Thomas Trappenberg, Israat Haque</dc:creator>
    </item>
    <item>
      <title>Competitive Analysis of Online Path Selection: Impacts of Path Length, Topology, and System-Level Costs</title>
      <link>https://arxiv.org/abs/2407.05239</link>
      <description>arXiv:2407.05239v1 Announce Type: cross 
Abstract: Consider a communication network to which a sequence of self-interested users come and send requests for data transmission between nodes. This work studies the question of how to guide the path selection choices made by those online-arriving users and maximize the social welfare. Competitive analysis is the main technical tool. Specifically, the impacts of path length bounds and topology on the competitive ratio of the designed algorithm are analyzed theoretically and explored experimentally. We observe intricate and interesting relationships between the empirical performance and the studied network parameters, which shed some light on how to design the network. We also investigate the influence of system-level costs on the optimal algorithm design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05239v1</guid>
      <category>cs.DS</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ying Cao, Siyuan Yu, Xiaoqi Tan, Danny H. K. Tsang</dc:creator>
    </item>
    <item>
      <title>6GSoft: Software for Edge-to-Cloud Continuum</title>
      <link>https://arxiv.org/abs/2407.05963</link>
      <description>arXiv:2407.05963v2 Announce Type: cross 
Abstract: In the era of 6G, developing and managing software requires cutting-edge software engineering (SE) theories and practices tailored for such complexity across a vast number of connected edge devices. Our project aims to lead the development of sustainable methods and energy-efficient orchestration models specifically for edge environments, enhancing architectural support driven by AI for contemporary edge-to-cloud continuum computing. This initiative seeks to position Finland at the forefront of the 6G landscape, focusing on sophisticated edge orchestration and robust software architectures to optimize the performance and scalability of edge networks. Collaborating with leading Finnish universities and companies, the project emphasizes deep industry-academia collaboration and international expertise to address critical challenges in edge orchestration and software architecture, aiming to drive significant advancements in software productivity and market impact.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05963v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <category>cs.SI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammad Azeem Akbar, Matteo Esposito, Sami Hyrynsalmi, Karthikeyan Dinesh Kumar, Valentina Lenarduzzi, Xiaozhou Li, Ali Mehraj, Tommi Mikkonen, Sergio Moreschini, Niko M\"akitalo, Markku Oivo, Anna-Sofia Paavonen, Risha Parveen, Kari Smolander, Ruoyu Su, Kari Syst\"a, Davide Taibi, Nan Yang, Zheying Zhang, Muhammad Zohaib</dc:creator>
    </item>
    <item>
      <title>LLMcap: Large Language Model for Unsupervised PCAP Failure Detection</title>
      <link>https://arxiv.org/abs/2407.06085</link>
      <description>arXiv:2407.06085v1 Announce Type: cross 
Abstract: The integration of advanced technologies into telecommunication networks complicates troubleshooting, posing challenges for manual error identification in Packet Capture (PCAP) data. This manual approach, requiring substantial resources, becomes impractical at larger scales. Machine learning (ML) methods offer alternatives, but the scarcity of labeled data limits accuracy. In this study, we propose a self-supervised, large language model-based (LLMcap) method for PCAP failure detection. LLMcap leverages language-learning abilities and employs masked language modeling to learn grammar, context, and structure. Tested rigorously on various PCAPs, it demonstrates high accuracy despite the absence of labeled data during training, presenting a promising solution for efficient network analysis. Index Terms: Network troubleshooting, Packet Capture Analysis, Self-Supervised Learning, Large Language Model, Network Quality of Service, Network Performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06085v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lukasz Tulczyjew, Kinan Jarrah, Charles Abondo, Dina Bennett, Nathanael Weill</dc:creator>
    </item>
    <item>
      <title>IEEE 802.11be Wi-Fi 7: Feature Summary and Performance Evaluation</title>
      <link>https://arxiv.org/abs/2309.15951</link>
      <description>arXiv:2309.15951v2 Announce Type: replace 
Abstract: While the pace of commercial scale application of Wi-Fi 6 accelerates, the IEEE 802.11 Working Group is about to complete the development of a new amendment standard IEEE 802.11be -- Extremely High Throughput (EHT), also known as Wi-Fi 7, which can be used to meet the demand for the throughput of 4K/8K videos up to tens of Gbps and low-latency video applications such as virtual reality (VR) and augmented reality (AR). Wi-Fi 7 not only scales Wi-Fi 6 with doubled bandwidth, but also supports real-time applications, which brings revolutionary changes to Wi-Fi. In this article, we start by introducing the main objectives and timeline of Wi-Fi 7 and then list the latest key techniques which promote the performance improvement of Wi-Fi 7. Finally, we validate the most critical objectives of Wi-Fi 7 -- the potential up to 30 Gbps throughput and lower latency. System-level simulation results suggest that by combining the new techniques, Wi-Fi 7 achieves 30 Gbps throughput and lower latency than Wi-Fi 6.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.15951v2</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoqian Liu, Yuhan Dong, Yiqing Li, Yousi Lin, Xun Yang, Ming Gan</dc:creator>
    </item>
    <item>
      <title>Wi-Fi 8: Embracing the Millimeter-Wave Era</title>
      <link>https://arxiv.org/abs/2309.16813</link>
      <description>arXiv:2309.16813v2 Announce Type: replace 
Abstract: With the increasing demands in communication, Wi-Fi technology is advancing towards its next generation. As high-need applications like Virtual Reality (VR) and Augmented Reality (AR) emerge, the role of millimeter-wave (mmWave) technology becomes critical. This paper explores Wi-Fi 8's potential features, especially its integration of mmWave technology. We address the challenges of implementing mmWave under current protocols and examine the compatibility of new features with mmWave. Our study includes system-level simulations, upclocking the 802.11ac PPDU to 60 GHz, and considers hardware limitations. The results demonstrate significant performance improvements with mmWave in Wi-Fi 8, indicating its feasibility for high-demand wireless scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.16813v2</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoqian Liu, Tingwei Chen, Yuhan Dong, Zhi Mao, Ming Gan, Xun Yang, Jianmin Lu</dc:creator>
    </item>
    <item>
      <title>Connectivity Aware and Energy Efficient Self-Organizing Distributed IoT Topology Control</title>
      <link>https://arxiv.org/abs/2310.00394</link>
      <description>arXiv:2310.00394v2 Announce Type: replace 
Abstract: Internet of Things has pervaded every area of modern life. From a research and industry standpoint, there has been an increasing demand and desire in recent years to develop Internet of Things networks with distributed structure. Wireless communication under emergency circumstances is one of the important applications that distributed Internet of Things can have. In order for a network to be functional in this scenario, it must be developed without the aid of a pre-established or centralized structure and operated in a self-organized manner to accommodate the communication requirements of the time. Although the design and development of such networks can be highly advantageous, they frequently confront difficulties, the most significant of which is attaining and maintaining effective connectivity to have reliable communications despite the requirement to optimize energy usage. In this study, we present a model for self-organizing topology control for ad hoc-based Internet of Things networks that can address the aforementioned challenges. The model that will be presented employs the notion of the Hamiltonian function in classical mechanics and has two key objectives: regulating the network's topology and dynamics to enhance connectivity to a desirable level while requiring the least amount of energy possible. The results of the simulation indicate that the proposed model satisfactorily fulfills the goals of the problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.00394v2</guid>
      <category>cs.NI</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Azra Seyyedi, Sina Dortaj, Mahdi Bohlouli, SeyedEhsan Nedaaee Oskoee</dc:creator>
    </item>
    <item>
      <title>MIST: An Efficient Approach for Software-Defined Multicast in Wireless Mesh Networks</title>
      <link>https://arxiv.org/abs/2312.04418</link>
      <description>arXiv:2312.04418v2 Announce Type: replace 
Abstract: Multicasting is a vital information dissemination technique in Software-Defined Networking (SDN). With SDN, a multicast service can incorporate network functions implemented at different nodes, which is referred to as software-defined multicast. Emerging ubiquitous wireless networks for 5G and Beyond (B5G) inherently support multicast. However, the broadcast nature of wireless channels, especially in dense deployments, leads to neighborhood interference as a primary system degradation factor, which introduces a new challenge for software-defined multicast in wireless mesh networks. To tackle this, this paper introduces an innovative approach, based on the idea of minimizing both the total length cost of the multicast tree and the interference at the same time. Accordingly, a novel bicriteria optimization problem is formulated--\emph{Minimum Interference Steiner Tree (MIST)}, which is the edge-weighted variant of the vertex-weighted secluded Steiner tree problem \cite{chechik2013secluded}. To solve the bicriteria problem, instead of resorting to heuristics, this paper employs an innovative approach that is an approximate algorithm for MIST but with guaranteed performance. Specifically, the approach exploits the monotone submodularity property of the interference metric and identifies Pareto optimal solutions for MIST, then converts the problem into the submodular minimization under Steiner tree constraints, and designs a two-stage relaxation algorithm. Simulation results demonstrate and validate the performance of the proposed algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.04418v2</guid>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rupei Xu, Yuming Jiang, Jason P. Jue</dc:creator>
    </item>
    <item>
      <title>SIRD: A Sender-Informed, Receiver-Driven Datacenter Transport Protocol</title>
      <link>https://arxiv.org/abs/2312.15403</link>
      <description>arXiv:2312.15403v3 Announce Type: replace 
Abstract: Datacenter congestion management protocols must navigate the throughput-latency buffering trade-off in the presence of growing constraints due to switching hardware trends, oversubscribed topologies, and varying network configurability and features. In this context, receiver-driven protocols, which schedule packet transmissions instead of reacting to congestion, have shown great promise and work exceptionally well when the bottleneck lies at the ToR-to-receiver link. However, independent receiver schedules may collide if a shared link is the bottleneck instead.
  We present SIRD, a receiver-driven congestion control protocol designed around the simple insight that single-owner links should be scheduled while shared links should be managed through traditional congestion control algorithms. The approach achieves the best of both worlds by allowing precise control of the most common bottleneck and robust bandwidth sharing for shared bottlenecks. SIRD is implemented by end hosts and does not depend on Ethernet priorities or extensive network configuration.
  We compare SIRD to state-of-the-art receiver-driven protocols (Homa, dcPIM, and ExpressPass) and production-grade reactive protocols (Swift and DCTCP) and show that SIRD is the only one that can consistently maximize link utilization, minimize queuing, and obtain near-optimal latency across a wide set of workloads and traffic patterns. SIRD causes 12x less peak buffering than Homa and achieves competitive latency and utilization without requiring Ethernet priorities. Unlike dcPIM, SIRD operates without latency-inducing message exchange rounds and outperforms it in utilization, buffering, and tail latency by 9%, 43%, and 46% respectively. Finally, SIRD achieves 10x lower tail latency and 26% higher utilization than ExpressPass.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.15403v3</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Konstantinos Prasopoulos, Edouard Bugnion, Marios Kogias</dc:creator>
    </item>
  </channel>
</rss>
