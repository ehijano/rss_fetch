<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.CP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.CP</link>
    <description>q-fin.CP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.CP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 22 Apr 2024 04:02:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 22 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Continuous-time Risk-sensitive Reinforcement Learning via Quadratic Variation Penalty</title>
      <link>https://arxiv.org/abs/2404.12598</link>
      <description>arXiv:2404.12598v1 Announce Type: cross 
Abstract: This paper studies continuous-time risk-sensitive reinforcement learning (RL) under the entropy-regularized, exploratory diffusion process formulation with the exponential-form objective. The risk-sensitive objective arises either as the agent's risk attitude or as a distributionally robust approach against the model uncertainty. Owing to the martingale perspective in Jia and Zhou (2023) the risk-sensitive RL problem is shown to be equivalent to ensuring the martingale property of a process involving both the value function and the q-function, augmented by an additional penalty term: the quadratic variation of the value process, capturing the variability of the value-to-go along the trajectory. This characterization allows for the straightforward adaptation of existing RL algorithms developed for non-risk-sensitive scenarios to incorporate risk sensitivity by adding the realized variance of the value process. Additionally, I highlight that the conventional policy gradient representation is inadequate for risk-sensitive problems due to the nonlinear nature of quadratic variation; however, q-learning offers a solution and extends to infinite horizon settings. Finally, I prove the convergence of the proposed algorithm for Merton's investment problem and quantify the impact of temperature parameter on the behavior of the learning procedure. I also conduct simulation experiments to demonstrate how risk-sensitive RL improves the finite-sample performance in the linear-quadratic control problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12598v1</guid>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>q-fin.CP</category>
      <category>q-fin.PM</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yanwei Jia</dc:creator>
    </item>
    <item>
      <title>Higher order approximation of option prices in Barndorff-Nielsen and Shephard models</title>
      <link>https://arxiv.org/abs/2401.14390</link>
      <description>arXiv:2401.14390v2 Announce Type: replace 
Abstract: We present an approximation method based on the mixing formula (Hull &amp; White 1987, Romano &amp; Touzi 1997) for pricing European options in Barndorff-Nielsen and Shephard models. This approximation is based on a Taylor expansion of the option price. It is implemented using a recursive algorithm that allows us to obtain closed form approximations of the option price of any order (subject to technical conditions on the background driving L\'evy process). This method can be used for any type of Barndorff-Nielsen and Shephard stochastic volatility model. Explicit results are presented in the case where the stationary distribution of the background driving L\'evy process is inverse Gaussian or gamma. In both of these cases, the approximation compares favorably to option prices produced by the characteristic function. In particular, we also perform an error analysis of the approximation, which is partially based on the results of Das &amp; Langren\'e (2022). We obtain asymptotic results for the error of the $N^{\text{th}}$ order approximation and error bounds when the variance process satisfies an inverse Gaussian Ornstein-Uhlenbeck process or a gamma Ornstein-Uhlenbeck process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.14390v2</guid>
      <category>q-fin.CP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>\'Alvaro Guinea Juli\'a, Alet Roux</dc:creator>
    </item>
    <item>
      <title>A Comparison of Traditional and Deep Learning Methods for Parameter Estimation of the Ornstein-Uhlenbeck Process</title>
      <link>https://arxiv.org/abs/2404.11526</link>
      <description>arXiv:2404.11526v2 Announce Type: replace 
Abstract: We consider the Ornstein-Uhlenbeck (OU) process, a stochastic process widely used in finance, physics, and biology. Parameter estimation of the OU process is a challenging problem. Thus, we review traditional tracking methods and compare them with novel applications of deep learning to estimate the parameters of the OU process. We use a multi-layer perceptron to estimate the parameters of the OU process and compare its performance with traditional parameter estimation methods, such as the Kalman filter and maximum likelihood estimation. We find that the multi-layer perceptron can accurately estimate the parameters of the OU process given a large dataset of observed trajectories; however, traditional parameter estimation methods may be more suitable for smaller datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11526v2</guid>
      <category>q-fin.CP</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jacob Fein-Ashley</dc:creator>
    </item>
  </channel>
</rss>
