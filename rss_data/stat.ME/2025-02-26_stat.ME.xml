<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 26 Feb 2025 05:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Pseudo-R2D2 prior for high-dimensional ordinal regression</title>
      <link>https://arxiv.org/abs/2502.17491</link>
      <description>arXiv:2502.17491v1 Announce Type: new 
Abstract: Ordinal regression with a high-dimensional covariate space has many important application areas including gene expression studies. The lack of an intrinsic numeric value associated with ordinal responses, however, makes methods based on continuous data, like linear regression, inappropriate. In this work, we extend the R2D2 prior framework to the high-dimensional ordinal setting. Since the $R^2$ definition used in the R2D2 prior relies on means and variances, it cannot be used for ordinal regression as these two quantities are not suitable for such data. Instead, by simulating data and using McFadden's coefficient-of-determination ($R^2_M$), we show that a generalized inverse Gaussian prior distribution on the global variance parameter approximately induces a beta prior distribution on $R^2_M$. The proposed prior can be implemented in $\texttt{Stan}$ and an $\texttt{R}$ package is also developed. Our method demonstrates excellent inference properties on simulated data, as well as yielding accurate predictions when applied to a liver tissue gene expression dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17491v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric Yanchenko</dc:creator>
    </item>
    <item>
      <title>Theoretical analysis and improvements in cubic transmutations of probability distributions</title>
      <link>https://arxiv.org/abs/2502.17586</link>
      <description>arXiv:2502.17586v1 Announce Type: new 
Abstract: In statistics, processed data are becoming increasingly complex, and classical probability distributions are limited in their ability to model them. This is why, to better model data, extensive work has been conducted on extending classical probability distributions. Generally, this extension is achieved by transforming the cumulative distribution function of a baseline distribution through the addition of one or more parameters to enhance its flexibility. Cubic transmutation (CT) is one of the most popular methods for such extensions. However, CT does not have a unique definition because different approaches for CT have been proposed in the literature but are yet to be compared. The main goal of this paper is to compare these different approaches from both theoretical and empirical viewpoints. We study the relationships between the different approaches and we propose modified versions based on the extension of parameter ranges. The results are illustrated using Pareto distribution as baseline distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17586v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Issa Cherif Geraldo, Edoh Katchekpele, Tchilabalo Abozou Kpanzou</dc:creator>
    </item>
    <item>
      <title>On the use of Mutual Information for Testing Independence</title>
      <link>https://arxiv.org/abs/2502.17636</link>
      <description>arXiv:2502.17636v1 Announce Type: new 
Abstract: In this paper we use a well know method in statistics, the $\delta$-method, to provide an asymptotic distribution for the Mutual Information, and construct and independence test based on it. Interesting connections are found with the likelihood ratio test and the chi-square goodness of fit test. In general, the difference between the Mutual Information evaluated at the true probabilities and at the empirical distribution, can be approximated by the sum of a normal random variable and a linear combination of chi-squares random variables. This summands are not independent, however the normal terms vanishes when testing independence, making the test statistic being asymptotically a linear combination of chi-squares. The $\delta$-method gives a general framework for computing the asymptotic distribution of other information based measures. A common difficulty is calculating the first and second-order derivatives, which is already challenging in the case of Mutual Information. However, this difficulty can be circumvallated by using advance symbolic software such as Mathematica. Finally, we explore the underlying geometry of the Mutual Information and propose other statical measures which may give competing alternatives to classical tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17636v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marius Marinescu, Costel Balcau</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Covariate-Dependent Gaussian Graphical Models</title>
      <link>https://arxiv.org/abs/2502.17684</link>
      <description>arXiv:2502.17684v1 Announce Type: new 
Abstract: Motivated by dynamic biologic network analysis, we propose a covariate-dependent Gaussian graphical model (cdexGGM) for capturing network structure that varies with covariates through a novel parameterization. Utilizing a likelihood framework, our methodology jointly estimates all dynamic edge and vertex parameters. We further develop statistical inference procedures to test the dynamic nature of the underlying network. Concerning large-scale networks, we perform composite likelihood estimation with an $\ell_1$ penalty to discover sparse dynamic network structures. We establish the estimation error bound in $\ell_2$ norm and validate the sign consistency in the high-dimensional context. We apply our method to an influenza vaccine data set to model the dynamic gene network that evolves with time. We also investigate a Down syndrome data set to model the dynamic protein network which varies under a factorial experimental design. These applications demonstrate the applicability and effectiveness of the proposed model. The supplemental materials for this article are available online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17684v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiacheng Wang, Xin Gao</dc:creator>
    </item>
    <item>
      <title>Semiparametric estimation for multivariate Hawkes processes using dependent Dirichlet processes: An application to order flow data in financial markets</title>
      <link>https://arxiv.org/abs/2502.17723</link>
      <description>arXiv:2502.17723v1 Announce Type: new 
Abstract: The order flow in high-frequency financial markets has been of particular research interest in recent years, as it provides insights into trading and order execution strategies and leads to better understanding of the supply-demand interplay and price formation. In this work, we propose a semiparametric multivariate Hawkes process model that relies on (mixtures of) dependent Dirichlet processes to analyze order flow data. Such a formulation avoids the kind of strong parametric assumptions about the excitation functions of the Hawkes process that often accompany traditional models and which, as we show, are not justified in the case of order flow data. It also allows us to borrow information across dimensions, improving estimation of the individual excitation functions. To fit the model, we develop two algorithms, one using Markov chain Monte Carlo methods and one using a stochastic variational approximation. In the context of simulation studies, we show that our model outperforms benchmark methods in terms of lower estimation error for both algorithms. In the context of real order flow data, we show that our model can capture features of the excitation functions such as non-monotonicity that cannot be accommodated by standard parametric models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17723v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Alex Ziyu Jiang, Abel Rodriguez</dc:creator>
    </item>
    <item>
      <title>Uncertainty Quantification for LLM-Based Survey Simulations</title>
      <link>https://arxiv.org/abs/2502.17773</link>
      <description>arXiv:2502.17773v1 Announce Type: new 
Abstract: We investigate the reliable use of simulated survey responses from large language models (LLMs) through the lens of uncertainty quantification. Our approach converts synthetic data into confidence sets for population parameters of human responses, addressing the distribution shift between the simulated and real populations. A key innovation lies in determining the optimal number of simulated responses: too many produce overly narrow confidence sets with poor coverage, while too few yield excessively loose estimates. To resolve this, our method adaptively selects the simulation sample size, ensuring valid average-case coverage guarantees. It is broadly applicable to any LLM, irrespective of its fidelity, and any procedure for constructing confidence sets. Additionally, the selected sample size quantifies the degree of misalignment between the LLM and the target human population. We illustrate our method on real datasets and LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17773v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengpiao Huang, Yuhang Wu, Kaizheng Wang</dc:creator>
    </item>
    <item>
      <title>DPGLM: A Semiparametric Bayesian GLM with Inhomogeneous Normalized Random Measures</title>
      <link>https://arxiv.org/abs/2502.17827</link>
      <description>arXiv:2502.17827v1 Announce Type: new 
Abstract: We introduce a varying weight dependent Dirichlet process (DDP) model to implement a semi-parametric GLM. The model extends a recently developed semi-parametric generalized linear model (SPGLM) by adding a nonparametric Bayesian prior on the baseline distribution of the GLM. We show that the resulting model takes the form of an inhomogeneous completely random measure that arises from exponential tilting of a normalized completely random measure. Building on familiar posterior simulation methods for mixtures with respect to normalized random measures we introduce posterior simulation in the resulting semi-parametric GLM model. The proposed methodology is validated through a series of simulation studies and is illustrated using data from a speech intelligibility study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17827v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Entejar Alam, Paul J. Rathouz, Peter M\"uller</dc:creator>
    </item>
    <item>
      <title>Principled priors for Bayesian inference of circular models</title>
      <link>https://arxiv.org/abs/2502.18223</link>
      <description>arXiv:2502.18223v1 Announce Type: new 
Abstract: Advancements in computational power and methodologies have enabled research on massive datasets. However, tools for analyzing data with directional or periodic characteristics, such as wind directions and customers' arrival time in 24-hour clock, remain underdeveloped. While statisticians have proposed circular distributions for such analyses, significant challenges persist in constructing circular statistical models, particularly in the context of Bayesian methods. These challenges stem from limited theoretical development and a lack of historical studies on prior selection for circular distribution parameters.
  In this article, we propose a principled, practical and systematic framework for selecting priors that effectively prevents overfitting in circular scenarios, especially when there is insufficient information to guide prior selection. We introduce well-examined Penalized Complexity (PC) priors for the most widely used circular distributions. Comprehensive comparisons with existing priors in the literature are conducted through simulation studies and a practical case study. Finally, we discuss the contributions and implications of our work, providing a foundation for further advancements in constructing Bayesian circular statistical models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18223v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xiang Ye, Janet Van Niekerk, H\r{a}vard Rue</dc:creator>
    </item>
    <item>
      <title>On Learning Causal Structures from Non-Experimental Data without Any Faithfulness Assumption</title>
      <link>https://arxiv.org/abs/1802.07051</link>
      <description>arXiv:1802.07051v2 Announce Type: cross 
Abstract: Consider the problem of learning, from non-experimental data, the causal (Markov equivalence) structure of the true, unknown causal Bayesian network (CBN) on a given, fixed set of (categorical) variables. This learning problem is known to be so hard that there is no learning algorithm that converges to the truth for all possible CBNs (on the given set of variables). So the convergence property has to be sacrificed for some CBNs---but for which? In response, the standard practice has been to design and employ learning algorithms that secure the convergence property for at least all the CBNs that satisfy the famous faithfulness condition, which implies sacrificing the convergence property for some CBNs that violate the faithfulness condition (Spirtes et al. 2000). This standard design practice can be justified by assuming---that is, accepting on faith---that the true, unknown CBN satisfies the faithfulness condition. But the real question is this: Is it possible to explain, without assuming the faithfulness condition or any of its weaker variants, why it is mandatory rather than optional to follow the standard design practice? This paper aims to answer the above question in the affirmative. We first define an array of modes of convergence to the truth as desiderata that might or might not be achieved by a causal learning algorithm. Those modes of convergence concern (i) how pervasive the domain of convergence is on the space of all possible CBNs and (ii) how uniformly the convergence happens. Then we prove a result to the following effect: for any learning algorithm that tackles the causal learning problem in question, if it achieves the best achievable mode of convergence (considered in this paper), then it must follow the standard design practice of converging to the truth for at least all CBNs that satisfy the faithfulness condition---it is a requirement, not an option.</description>
      <guid isPermaLink="false">oai:arXiv.org:1802.07051v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hanti Lin, Jiji Zhang</dc:creator>
    </item>
    <item>
      <title>Frequentist Statistics as Internalist Reliabilism</title>
      <link>https://arxiv.org/abs/2411.08547</link>
      <description>arXiv:2411.08547v3 Announce Type: cross 
Abstract: There has long been an impression that reliabilism implies externalism and that frequentist statistics, due to its reliabilist nature, is inherently externalist. I argue, however, that frequentist statistics can plausibly be understood as a form of internalist reliabilism -- internalist in the conventional sense, yet reliabilist in certain unconventional and intriguing ways. Crucially, in developing the thesis that reliabilism does not imply externalism, my aim is not to stretch the meaning of `reliabilism' merely to sever the implication. Instead, it is to gain a deeper understanding of frequentist statistics, which stands as one of the most sustained attempts by scientists to develop an epistemology for their own use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08547v3</guid>
      <category>stat.OT</category>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hanti Lin</dc:creator>
    </item>
    <item>
      <title>Unified Inductive Logic: From Formal Learning to Statistical Inference to Supervised Learning</title>
      <link>https://arxiv.org/abs/2412.02969</link>
      <description>arXiv:2412.02969v1 Announce Type: cross 
Abstract: While the traditional conception of inductive logic is Carnapian, I develop a Peircean alternative and use it to unify formal learning theory, statistics, and a significant part of machine learning: supervised learning. Some crucial standards for evaluating non-deductive inferences have been assumed separately in those areas, but can actually be justified by a unifying principle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02969v1</guid>
      <category>stat.OT</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hanti Lin</dc:creator>
    </item>
    <item>
      <title>Estimating Time Delays between Signals under Mixed Noise Influence with Novel Cross- and Bispectral Methods</title>
      <link>https://arxiv.org/abs/2502.17474</link>
      <description>arXiv:2502.17474v1 Announce Type: cross 
Abstract: A common problem to signal processing are biases introduced by correlated noise. In Time-Delay Estimation (TDE), which quantifies a time lag between two signals, noise mixing introduces a bias towards zero delay in conventional TDE protocols based on the cross- or bispectrum. Here we propose two novel TDE approaches that address these shortcomings: (1) A cross-spectrum based TDE protocol that relies on estimating the periodicity of the phase spectrum rather than its slope, and (2) a bispectrum based TDE analysis, bispectral antisymmetrization, which removes contributions from not just Gaussian but all independent sources. In a simulation study, we compare conventional and novel TDE protocols and resolve differences in performance with respect to noise Gaussianity and auto-correlation structure. As a proof-of-concept, we also perform TDE analysis on a neural stimulation dataset (n=3). We find that antisymmetrization consistently outperforms conventional bispectral TDE methods at low signal-to-noise ratios (SNR) and removes spurious zero-delay estimates in all mixed-noise environments. TDE based on phase periodicity also improves signal sensitivity compared to conventional cross-spectral methods. These observations are stable with respect to the magnitude of the delay and the statistical properties of the noise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17474v1</guid>
      <category>eess.SP</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tin Jurhar, Franziska Pellegrini, Ana I. Nu\~nes del Toro, Tilman Stephani, Guido Nolte, Stefan Haufe</dc:creator>
    </item>
    <item>
      <title>Flexible Counterfactual Explanations with Generative Models</title>
      <link>https://arxiv.org/abs/2502.17613</link>
      <description>arXiv:2502.17613v1 Announce Type: cross 
Abstract: Counterfactual explanations provide actionable insights to achieve desired outcomes by suggesting minimal changes to input features. However, existing methods rely on fixed sets of mutable features, which makes counterfactual explanations inflexible for users with heterogeneous real-world constraints. Here, we introduce Flexible Counterfactual Explanations, a framework incorporating counterfactual templates, which allows users to dynamically specify mutable features at inference time. In our implementation, we use Generative Adversarial Networks (FCEGAN), which align explanations with user-defined constraints without requiring model retraining or additional optimization. Furthermore, FCEGAN is designed for black-box scenarios, leveraging historical prediction datasets to generate explanations without direct access to model internals. Experiments across economic and healthcare datasets demonstrate that FCEGAN significantly improves counterfactual explanations' validity compared to traditional benchmark methods. By integrating user-driven flexibility and black-box compatibility, counterfactual templates support personalized explanations tailored to user constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17613v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Stig Hellemans, Andres Algaba, Sam Verboven, Vincent Ginis</dc:creator>
    </item>
    <item>
      <title>A Unified Framework for Semiparametrically Efficient Semi-Supervised Learning</title>
      <link>https://arxiv.org/abs/2502.17741</link>
      <description>arXiv:2502.17741v1 Announce Type: cross 
Abstract: We consider statistical inference under a semi-supervised setting where we have access to both a labeled dataset consisting of pairs $\{X_i, Y_i \}_{i=1}^n$ and an unlabeled dataset $\{ X_i \}_{i=n+1}^{n+N}$. We ask the question: under what circumstances, and by how much, can incorporating the unlabeled dataset improve upon inference using the labeled data? To answer this question, we investigate semi-supervised learning through the lens of semiparametric efficiency theory. We characterize the efficiency lower bound under the semi-supervised setting for an arbitrary inferential problem, and show that incorporating unlabeled data can potentially improve efficiency if the parameter is not well-specified. We then propose two types of semi-supervised estimators: a safe estimator that imposes minimal assumptions, is simple to compute, and is guaranteed to be at least as efficient as the initial supervised estimator; and an efficient estimator, which -- under stronger assumptions -- achieves the semiparametric efficiency bound. Our findings unify existing semiparametric efficiency results for particular special cases, and extend these results to a much more general class of problems. Moreover, we show that our estimators can flexibly incorporate predicted outcomes arising from ``black-box" machine learning models, and thereby achieve the same goal as prediction-powered inference (PPI), but with superior theoretical guarantees. We also provide a complete understanding of the theoretical basis for the existing set of PPI methods. Finally, we apply the theoretical framework developed to derive and analyze efficient semi-supervised estimators in a number of settings, including M-estimation, U-statistics, and average treatment effect estimation, and demonstrate the performance of the proposed estimators via simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17741v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zichun Xu, Daniela Witten, Ali Shojaie</dc:creator>
    </item>
    <item>
      <title>Generating Correlation Matrices with Graph Structures Using Convex Optimization</title>
      <link>https://arxiv.org/abs/2502.17981</link>
      <description>arXiv:2502.17981v1 Announce Type: cross 
Abstract: This work deals with the generation of theoretical correlation matrices with specific sparsity patterns, associated to graph structures. We present a novel approach based on convex optimization, offering greater flexibility compared to existing techniques, notably by controlling the mean of the entry distribution in the generated correlation matrices. This allows for the generation of correlation matrices that better represent realistic data and can be used to benchmark statistical methods for graph inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17981v1</guid>
      <category>eess.SP</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ali Fakhar (STATIFY, LJK), K\'evin Polisano (SVH, LJK), Ir\`ene Gannaz (G-SCOP\_GROG, G-SCOP), Sophie Achard (STATIFY, LJK)</dc:creator>
    </item>
    <item>
      <title>Sequential Outlier Detection in Non-Stationary Time Series</title>
      <link>https://arxiv.org/abs/2502.18038</link>
      <description>arXiv:2502.18038v1 Announce Type: cross 
Abstract: A novel method for sequential outlier detection in non-stationary time series is proposed. The method tests the null hypothesis of ``no outlier'' at each time point, addressing the multiple testing problem by bounding the error probability of successive tests, using extreme value theory. The asymptotic properties of the test statistic are studied under the null hypothesis and alternative. The finite sample properties of the new detection scheme are investigated by means of a simulation study, and the method is compared with alternative procedures which have recently been proposed in the statistics and machine learning literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18038v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian Heinrichs, Patrick Bastian, Holger Dette</dc:creator>
    </item>
    <item>
      <title>Near-Optimal Approximations for Bayesian Inference in Function Space</title>
      <link>https://arxiv.org/abs/2502.18279</link>
      <description>arXiv:2502.18279v1 Announce Type: cross 
Abstract: We propose a scalable inference algorithm for Bayes posteriors defined on a reproducing kernel Hilbert space (RKHS). Given a likelihood function and a Gaussian random element representing the prior, the corresponding Bayes posterior measure $\Pi_{\text{B}}$ can be obtained as the stationary distribution of an RKHS-valued Langevin diffusion. We approximate the infinite-dimensional Langevin diffusion via a projection onto the first $M$ components of the Kosambi-Karhunen-Lo\`eve expansion. Exploiting the thus obtained approximate posterior for these $M$ components, we perform inference for $\Pi_{\text{B}}$ by relying on the law of total probability and a sufficiency assumption. The resulting method scales as $O(M^3+JM^2)$, where $J$ is the number of samples produced from the posterior measure $\Pi_{\text{B}}$. Interestingly, the algorithm recovers the posterior arising from the sparse variational Gaussian process (SVGP) (see Titsias, 2009) as a special case, owed to the fact that the sufficiency assumption underlies both methods. However, whereas the SVGP is parametrically constrained to be a Gaussian process, our method is based on a non-parametric variational family $\mathcal{P}(\mathbb{R}^M)$ consisting of all probability measures on $\mathbb{R}^M$. As a result, our method is provably close to the optimal $M$-dimensional variational approximation of the Bayes posterior $\Pi_{\text{B}}$ in $\mathcal{P}(\mathbb{R}^M)$ for convex and Lipschitz continuous negative log likelihoods, and coincides with SVGP for the special case of a Gaussian error likelihood.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18279v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Veit Wild, James Wu, Dino Sejdinovic, Jeremias Knoblauch</dc:creator>
    </item>
    <item>
      <title>Graphical lasso for extremes</title>
      <link>https://arxiv.org/abs/2307.15004</link>
      <description>arXiv:2307.15004v2 Announce Type: replace 
Abstract: In this paper, we estimate the sparse dependence structure in the tail region of a multivariate random vector, potentially of high dimension. The tail dependence is modeled via a graphical model for extremes embedded in the H\"usler-Reiss distribution. We propose the extreme graphical lasso procedure to estimate the sparsity in the tail dependence, similar to the Gaussian graphical lasso in high dimensional statistics. We prove its consistency in identifying the graph structure and estimating model parameters. The efficiency and accuracy of the proposed method are illustrated by simulations and real data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.15004v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Phyllis Wan, Chen Zhou</dc:creator>
    </item>
    <item>
      <title>Recovery and inference of causal effects with sequential adjustment for confounding and attrition</title>
      <link>https://arxiv.org/abs/2401.16990</link>
      <description>arXiv:2401.16990v4 Announce Type: replace 
Abstract: Confounding bias and selection bias bring two significant challenges to the validity of conclusions drawn from applied causal inference. The latter can stem from informative missingness, such as in cases of attrition. We introduce the Sequential Adjustment Criteria (SAC), which extend available graphical conditions for recovering causal effects from confounding and attrition using sequential regressions, allowing for the inclusion of post-exposure and forbidden variables in the adjustment sets. We propose an estimator for the recovered Average Treatment Effect (ATE) based on Targeted Minimum-Loss Estimation (TMLE), which exhibits multiple robustness under certain technical conditions. This approach ensures consistency even in scenarios where the Double Inverse Probability Weighting (DIPW) and the naive plug-in sequential regressions approaches fall short. Through a simulation study, we assess the performance of the proposed estimator against alternative methods across different graph setups and model specification scenarios. As a motivating application, we examine the effect of pharmacological treatment for Attention-Deficit/Hyperactivity Disorder (ADHD) upon the scores obtained by diagnosed Norwegian schoolchildren in national tests using observational data ($n=9\,352$). Our findings align with the accumulated clinical evidence, affirming a positive but small impact of medication on academic achievement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.16990v4</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Johan de Aguas, Johan Pensar, Tom\'as Varnet P\'erez, Guido Biele</dc:creator>
    </item>
    <item>
      <title>Decision synthesis in monetary policy</title>
      <link>https://arxiv.org/abs/2406.03321</link>
      <description>arXiv:2406.03321v2 Announce Type: replace 
Abstract: The macroeconomy is a sophisticated dynamic system involving significant uncertainties that complicate modelling. In response, decision-makers consider multiple models that provide different predictions and policy recommendations which are then synthesized into a policy decision. In this setting, we develop Bayesian predictive decision synthesis (BPDS) to formalize monetary policy decision processes. BPDS draws on recent developments in model combination and statistical decision theory that yield new opportunities in combining multiple models, emphasizing the integration of decision goals, expectations and outcomes into the model synthesis process. Our case study concerns central bank policy decisions about target interest rates with a focus on implications for multi-step macroeconomic forecasting. This application also motivates new methodological developments in conditional forecasting and BPDS, presented and developed here.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03321v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tony Chernis, Gary Koop, Emily Tallman, Mike West</dc:creator>
    </item>
    <item>
      <title>Tail-robust factor modelling of vector and tensor time series in high dimensions</title>
      <link>https://arxiv.org/abs/2407.09390</link>
      <description>arXiv:2407.09390v2 Announce Type: replace 
Abstract: We study the problem of factor modelling vector- and tensor-valued time series in the presence of heavy tails in the data, which produce anomalous observations with non-negligible probability. For this, we propose to combine a two-step procedure for tensor data decomposition with data truncation, which is easy to implement and does not require an iterative search for a numerical solution. Departing away from the light-tail assumptions often adopted in the time series factor modelling literature, we derive the consistency and asymptotic normality of the proposed estimators while assuming the existence of the $(2 + 2\epsilon)$-th moment only for some $\epsilon \in (0, 1)$. Our rates explicitly depend on $\eps$ characterising the effect of heavy tails, and on the chosen level of truncation. We also propose a consistent criterion for determining the number of factors. Simulation studies and applications to two macroeconomic datasets demonstrate the good performance of the proposed estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09390v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matteo Barigozzi, Haeran Cho, Hyeyoung Maeng</dc:creator>
    </item>
    <item>
      <title>Mean Squared Prediction Error Estimators of EBLUP of a Small Area Mean Under a Semi-Parametric Fay-Herriot Model</title>
      <link>https://arxiv.org/abs/2409.16409</link>
      <description>arXiv:2409.16409v2 Announce Type: replace 
Abstract: In this paper we derive a second-order unbiased (or nearly unbiased) mean squared prediction error (MSPE) estimator of the empirical best linear unbiased predictor (EBLUP) of a small area mean for a semi-parametric extension to the well-known Fay-Herriot model. Specifically, we derive our MSPE estimator essentially assuming certain moment conditions on both the sampling errors and random effects distributions. The normality-based Prasad-Rao MSPE estimator has a surprising robustness property in that it remains second-order unbiased under the non-normality of random effects when a simple Prasad-Rao method-of-moments estimator is used for the variance component and the sampling error distribution is normal. We show that the normality-based MSPE estimator is no longer second-order unbiased when the sampling error distribution has non-zero kurtosis or when the Fay-Herriot moment method is used to estimate the variance component, even when the sampling error distribution is normal. It is interesting to note that when the simple method-of moments estimator is used for the variance component, our proposed MSPE estimator does not require the estimation of kurtosis of the random effects. Results of a simulation study on the accuracy of the proposed MSPE estimator, under non-normality of both sampling and random effects distributions, are also presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16409v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shijie Chen, P. Lahiri, J. N. K. Rao</dc:creator>
    </item>
    <item>
      <title>Bayesian Calibration for Prediction in a Multi-Output Transposition Context</title>
      <link>https://arxiv.org/abs/2410.00116</link>
      <description>arXiv:2410.00116v3 Announce Type: replace 
Abstract: Numerical simulations are widely used to predict the behavior of physical systems, with Bayesian approaches being particularly well suited for this purpose. However, experimental observations are necessary to calibrate certain simulator parameters for the prediction. In this work, we use a multi-output simulator to predict all its outputs, including those that have never been experimentally observed. This situation is referred to as the transposition context. To accurately quantify the discrepancy between model outputs and real data in this context, conventional methods cannot be applied, and the Bayesian calibration must be augmented by incorporating a joint model error across all outputs. To achieve this, the proposed method is to consider additional numerical input parameters within a hierarchical Bayesian model, which includes hyperparameters for the prior distribution of the calibration variables. This approach is applied on a computer code with three outputs that models the Taylor cylinder impact test with a small number of observations. The outputs are considered as the observed variables one at a time, to work with three different transposition situations. The proposed method is compared with other approaches that embed model errors to demonstrate the significance of the hierarchical formulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00116v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charlie Sire, Josselin Garnier, C\'edric Durantin, Baptiste Kerleguer, Gilles Defaux, Guillaume Perrin</dc:creator>
    </item>
    <item>
      <title>The square array design</title>
      <link>https://arxiv.org/abs/2412.09166</link>
      <description>arXiv:2412.09166v2 Announce Type: replace 
Abstract: This paper is about the construction of augmented row-column designs for unreplicated trials. The method uses the representation of a $k \times t$ equireplicate incomplete-block design with $t$ treatments in $t$ blocks of size $k$, termed an auxiliary block design, as a $t \times t$ square array design with $k$ controls, where $k&lt;t$. This can be regarded as an extension of the representation of a Youden square as a partial latin square for unreplicated trials. Properties of the designs, in particular in relation to connectedness and randomization, are explored. Particular attention is given to square array designs which minimize the average variances of the estimates of paired comparisons between test lines and controls and between test-line and test-line effects. The use of equireplicate cyclic designs as auxiliary block designs is highlighted. These provide a flexible and workable family of augmented row-column square array designs. Designs whose auxiliary block designs are not cyclic are also covered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09166v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>R. A. Bailey, L. M. Haines</dc:creator>
    </item>
    <item>
      <title>A partial likelihood approach to tree-based density modeling and its application in Bayesian inference</title>
      <link>https://arxiv.org/abs/2412.11692</link>
      <description>arXiv:2412.11692v3 Announce Type: replace 
Abstract: Tree-based priors for probability distributions are usually specified using a predetermined, data-independent collection of candidate recursive partitions of the sample space. To characterize an unknown target density in detail over the entire sample space, candidate partitions must have the capacity to expand deeply into all areas of the sample space with potential non-zero sampling probability. Such an expansive system of partitions often incurs prohibitive computational costs and makes inference prone to overfitting, especially in regions with little probability mass. Thus, existing models typically make a compromise and rely on relatively shallow trees. This hampers one of the most desirable features of trees, their ability to characterize local features, and results in reduced statistical efficiency. Traditional wisdom suggests that this compromise is inevitable to ensure coherent likelihood-based reasoning in Bayesian inference, as a data-dependent partition system that allows deeper expansion only in regions with more observations would induce double dipping of the data. We propose a simple strategy to restore coherency while allowing the candidate partitions to be data-dependent, using Cox's partial likelihood. Our partial likelihood approach is broadly applicable to existing likelihood-based methods and, in particular, to Bayesian inference on tree-based models. We give examples in density estimation in which the partial likelihood is endowed with existing priors on tree-based models and compare with the standard, full-likelihood approach. The results show substantial gains in estimation accuracy and computational efficiency from adopting the partial likelihood.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11692v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Li Ma, Benedetta Bruni</dc:creator>
    </item>
    <item>
      <title>Overcoming data challenges to measure whole-person health in electronic health records</title>
      <link>https://arxiv.org/abs/2502.05380</link>
      <description>arXiv:2502.05380v3 Announce Type: replace 
Abstract: The allostatic load index (ALI) is a composite measure of whole-person health. Data from electronic health records (EHR) present a huge opportunity to operationalize the ALI in the learning health system, except they are prone to missingness and errors. Validation of EHR data (e.g., through chart reviews) can provide better-quality data, but realistically, only a subset of patients' data can be validated, and most protocols do not recover missing data. Using a representative sample of 1000 patients from the EHR at an extensive learning health system (100 of whom could be validated), we propose methods to design, conduct, and analyze statistically efficient and robust studies of the ALI and healthcare utilization. With semiparametric maximum likelihood estimation, we robustly incorporate all available data into statistical models. Using targeted design strategies, we examine ways to select the most informative patients for validation. Incorporating clinical expertise, we devise a novel validation protocol to promote the quality and completeness of EHR data. Validating the EHR data uncovered relatively low error rates and recovered some missing data. Through simulation studies based on preliminary data, residual sampling was identified as the most informative strategy for completing our validation study. Statistical models of partially validated data indicated higher odds of engaging in the healthcare system were associated with worse whole-person health (i.e., higher ALI), adjusting for age. Targeted validation with an enriched protocol allowed us to ensure the quality and promote the completeness of the EHR. Findings from our validation study were incorporated into analyses as we operationalize the ALI as a whole-person health measure intended to predict healthcare utilization in the academic learning health system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05380v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarah C. Lotspeich, Sheetal Kedar, Rabeya Tahir, Aidan D. Keleghan, Amelia Miranda, Stephany N. Duda, Michael P. Bancks, Brian J. Wells, Ashish K. Khanna, Joseph Rigdon</dc:creator>
    </item>
    <item>
      <title>A Short Note of Comparison between Convex and Non-convex Penalized Likelihood</title>
      <link>https://arxiv.org/abs/2502.07655</link>
      <description>arXiv:2502.07655v2 Announce Type: replace 
Abstract: This paper compares convex and non-convex penalized likelihood methods in high-dimensional statistical modeling, focusing on their strengths and limitations. Convex penalties, like LASSO, offer computational efficiency and strong theoretical guarantees but often introduce bias in parameter estimation. Non-convex penalties, such as SCAD and MCP, reduce bias and achieve oracle properties but pose optimization challenges due to non-convexity. The paper highlights key differences in bias-variance trade-offs, computational complexity, and robustness, offering practical guidance for method selection. It concludes that the choice depends on the problem context, balancing accuracy</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07655v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kasy Du</dc:creator>
    </item>
    <item>
      <title>A Latent Causal Inference Framework for Ordinal Variables</title>
      <link>https://arxiv.org/abs/2502.10276</link>
      <description>arXiv:2502.10276v2 Announce Type: replace 
Abstract: Ordinal variables, such as on the Likert scale, are common in applied research. Yet, existing methods for causal inference tend to target nominal or continuous data. When applied to ordinal data, this fails to account for the inherent ordering or imposes well-defined relative magnitudes. Hence, there is a need for specialised methods to compute interventional effects between ordinal variables while accounting for their ordinality. One potential framework is to presume a latent Gaussian Directed Acyclic Graph (DAG) model: that the ordinal variables originate from marginally discretising a set of Gaussian variables whose latent covariance matrix is constrained to satisfy the conditional independencies inherent in a DAG. Conditioned on a given latent covariance matrix and discretisation thresholds, we derive a closed-form function for ordinal causal effects in terms of interventional distributions in the latent space. Our causal estimation combines naturally with algorithms to learn the latent DAG and its parameters, like the Ordinal Structural EM algorithm. Simulations demonstrate the applicability of the proposed approach in estimating ordinal causal effects both for known and unknown structures of the latent graph. As an illustration of a real-world use case, the method is applied to survey data of 408 patients from a study on the functional relationships between symptoms of obsessive-compulsive disorder and depression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10276v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Martina Scauda, Jack Kuipers, Giusi Moffa</dc:creator>
    </item>
    <item>
      <title>Network regression and supervised centrality estimation</title>
      <link>https://arxiv.org/abs/2111.12921</link>
      <description>arXiv:2111.12921v3 Announce Type: replace-cross 
Abstract: The centrality in a network is often used to measure nodes' importance and model network effects on a certain outcome. Empirical studies widely adopt a two-stage procedure, which first estimates the centrality from the observed noisy network and then infers the network effect from the estimated centrality, even though it lacks theoretical understanding. We propose a unified modeling framework to study the properties of centrality estimation and inference and the subsequent network regression analysis with noisy network observations. Furthermore, we propose a supervised centrality estimation methodology, which aims to simultaneously estimate both centrality and network effect. We showcase the advantages of our method compared with the two-stage method both theoretically and numerically via extensive simulations and a case study in predicting currency risk premiums from the global trade network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.12921v3</guid>
      <category>econ.EM</category>
      <category>cs.SI</category>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junhui Cai, Dan Yang, Ran Chen, Wu Zhu, Haipeng Shen, Linda Zhao</dc:creator>
    </item>
    <item>
      <title>Approximate Factor Models for Functional Time Series</title>
      <link>https://arxiv.org/abs/2201.02532</link>
      <description>arXiv:2201.02532v4 Announce Type: replace-cross 
Abstract: We propose a novel approximate factor model tailored for analyzing time-dependent curve data. Our model decomposes such data into two distinct components: a low-dimensional predictable factor component and an unpredictable error term. These components are identified through the autocovariance structure of the underlying functional time series. The model parameters are consistently estimated using the eigencomponents of a cumulative autocovariance operator and an information criterion is proposed to determine the appropriate number of factors. Applications to mortality and yield curve modeling illustrate key advantages of our approach over the widely used functional principal component analysis, as it offers parsimonious structural representations of the underlying dynamics along with gains in out-of-sample forecast performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.02532v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sven Otto, Nazarii Salish</dc:creator>
    </item>
    <item>
      <title>A projection based approach for interactive fixed effects panel data models</title>
      <link>https://arxiv.org/abs/2201.11482</link>
      <description>arXiv:2201.11482v3 Announce Type: replace-cross 
Abstract: This paper introduces a straightforward sieve-based approach for estimating and conducting inference on regression parameters in panel data models with interactive fixed effects. The method's key assumption is that factor loadings can be decomposed into an unknown smooth function of individual characteristics plus an idiosyncratic error term. Our estimator offers advantages over existing approaches by taking a simple partial least squares form, eliminating the need for iterative procedures or preliminary factor estimation. In deriving the asymptotic properties, we discover that the limiting distribution exhibits a discontinuity that depends on how well our basis functions explain the factor loadings, as measured by the variance of the error factor loadings. This finding reveals that conventional ``plug-in'' methods using the estimated asymptotic covariance can produce excessively conservative coverage probabilities. We demonstrate that uniformly valid non-conservative inference can be achieved through the cross-sectional bootstrap method. Monte Carlo simulations confirm the estimator's strong performance in terms of mean squared error and good coverage results for the bootstrap procedure. We demonstrate the practical relevance of our methodology by analyzing growth rate determinants across OECD countries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.11482v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Georg Keilbar, Juan M. Rodriguez-Poo, Alexandra Soberon, Weining Wang</dc:creator>
    </item>
    <item>
      <title>Towards a Unified Theory for Semiparametric Data Fusion with Individual-Level Data</title>
      <link>https://arxiv.org/abs/2409.09973</link>
      <description>arXiv:2409.09973v2 Announce Type: replace-cross 
Abstract: We address the goal of conducting inference about a smooth finite-dimensional parameter by utilizing individual-level data from various independent sources. Recent advancements have led to the development of a comprehensive theory capable of handling scenarios where different data sources align with, possibly distinct subsets of, conditional distributions of a single factorization of the joint target distribution. While this theory proves effective in many significant contexts, it falls short in certain common data fusion problems, such as two-sample instrumental variable analysis, settings that integrate data from epidemiological studies with diverse designs (e.g., prospective cohorts and retrospective case-control studies), and studies with variables prone to measurement error that are supplemented by validation studies. In this paper, we extend the aforementioned comprehensive theory to allow for the fusion of individual-level data from sources aligned with conditional distributions that do not correspond to a single factorization of the target distribution. Assuming conditional and marginal distribution alignments, we provide universal results that characterize the class of all influence functions of regular asymptotically linear estimators and the efficient influence function of any pathwise differentiable parameter, irrespective of the number of data sources, the specific parameter of interest, or the statistical model for the target distribution. This theory paves the way for machine-learning debiased, semiparametric efficient estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09973v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ellen Graham (University of Washington), Marco Carone (University of Washington), Andrea Rotnitzky (University of Washington)</dc:creator>
    </item>
  </channel>
</rss>
