<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 26 Aug 2024 04:00:43 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Interactive Design-of-Experiments: Optimizing a Cooling System</title>
      <link>https://arxiv.org/abs/2408.12607</link>
      <description>arXiv:2408.12607v1 Announce Type: new 
Abstract: The optimization of cooling systems is important in many cases, for example for cabin and battery cooling in electric cars. Such an optimization is governed by multiple, conflicting objectives and it is performed across a multi-dimensional parameter space. The extent of the parameter space, the complexity of the non-linear model of the system, as well as the time needed per simulation run and factors that are not modeled in the simulation necessitate an iterative, semi-automatic approach. We present an interactive visual optimization approach, where the user works with a p-h diagram to steer an iterative, guided optimization process. A deep learning (DL) model provides estimates for parameters, given a target characterization of the system, while numerical simulation is used to compute system characteristics for an ensemble of parameter sets. Since the DL model only serves as an approximation of the inverse of the cooling system and since target characteristics can be chosen according to different, competing objectives, an iterative optimization process is realized, developing multiple sets of intermediate solutions, which are visually related to each other. The standard p-h diagram, integrated interactively in this approach, is complemented by a dual, also interactive visual representation of additional expressive measures representing the system characteristics. We show how the known four-points semantic of the p-h diagram meaningfully transfers to the dual data representation. When evaluating this approach in the automotive domain, we found that our solution helped with the overall comprehension of the cooling system and that it lead to a faster convergence during optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12607v1</guid>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rainer Splechtna, Majid Behravan, Mario Jelovic, Denis Gracanin, Helwig Hauser, Kresimir Matkovic</dc:creator>
    </item>
    <item>
      <title>Using a negative spatial auto-correlation index to evaluate and improve intrinsic TagMap's multi-scale visualization capabilities</title>
      <link>https://arxiv.org/abs/2408.12610</link>
      <description>arXiv:2408.12610v1 Announce Type: new 
Abstract: The popularity of tag clouds has sparked significant interest in the geographic research community, leading to the development of map-based adaptations known as intrinsic tag maps. However, existing methodologies for tag maps primarily focus on tag layout at specific scales, which may result in large empty areas or close proximity between tags when navigating across multiple scales. This issue arises because initial tag layouts may not ensure an even distribution of tags with varying sizes across the region. To address this problem, we incorporate the negative spatial auto-correlation index into tag maps to assess the uniformity of tag size distribution. Subsequently, we integrate this index into a TIN-based intrinsic tag map layout approach to enhance its ability to support multi-scale visualization. This enhancement involves iteratively filtering out candidate tags and selecting optimal tags that meet the defined index criteria. Experimental findings from two representative areas (the USA and Italy) demonstrate the efficacy of our approach in enhancing multi-scale visualization capabilities, albeit with trade-offs in compactness and time efficiency. Specifically, when retaining the same number of tags in the layout, our approach achieves higher compactness but requires more time. Conversely, when reducing the number of tags in the layout, our approach exhibits reduced time requirements but lower compactness. Furthermore, we discuss the effectiveness of various applied strategies aligned with existing approaches to generate diverse intrinsic tag maps tailored to user preferences. Additional details and resources can be found on our project website: https://github.com/TrentonWei/Multi-scale-TagMap.git.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12610v1</guid>
      <category>cs.HC</category>
      <category>cs.IR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiwei Wei, Nai Yang</dc:creator>
    </item>
    <item>
      <title>Educational Customization by Homogenous Grouping of e-Learners based on their Learning Styles</title>
      <link>https://arxiv.org/abs/2408.12619</link>
      <description>arXiv:2408.12619v1 Announce Type: new 
Abstract: The E-learning environment offers greater flexibility compared to face-to-face interactions, allowing for adapting educational content to meet learners' individual needs and abilities through personalization and customization of e-content and the educational process. Despite the advantages of this approach, customizing the learning environment can reduce the costs of tutoring systems for similar learners by utilizing the same content and process for co-like learning groups. Various indicators for grouping learners exist, but many of them are conceptual, uncertain, and subject to change over time. In this article, we propose using the Felder-Silverman model, which is based on learning styles, to group similar learners. Additionally, we model the behaviors and actions of e-learners in a network environment using Fuzzy Set Theory (FST). After identifying the learning styles of the learners, co-like learning groups are formed, and each group receives adaptive content based on their preferences, needs, talents, and abilities. By comparing the results of the experimental and control groups, we determine the effectiveness of the proposed grouping method. In terms of "educational success," the weighted average score of the experimental group is 17.65 out of 20, while the control group achieves a score of 12.6 out of 20. Furthermore, the "educational satisfaction" of the experimental group is 67%, whereas the control group's satisfaction level is 37%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12619v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammadreza amiri, GholamAli montazer, Ebrahim Mousavi</dc:creator>
    </item>
    <item>
      <title>Bridging the gap between natural user expression with complex automation programming in smart homes</title>
      <link>https://arxiv.org/abs/2408.12687</link>
      <description>arXiv:2408.12687v1 Announce Type: new 
Abstract: A long-standing challenge in end-user programming (EUP) is to trade off between natural user expression and the complexity of programming tasks. As large language models (LLMs) are empowered to handle semantic inference and natural language understanding, it remains under-explored how such capabilities can facilitate end-users to configure complex automation more naturally and easily. We propose AwareAuto, an EUP system that standardizes user expression and finishes two-step inference with the LLMs to achieve automation generation. AwareAuto allows contextual, multi-modality, and flexible user expression to configure complex automation tasks (e.g., dynamic parameters, multiple conditional branches, and temporal constraints), which are non-manageable in traditional EUP solutions. By studying realistic, complex rules data, AwareAuto gains 91.7% accuracy in matching user intentions and feasibility. We introduced user interaction to ensure system controllability and usability. We discuss the opportunities and challenges of incorporating LLMs in end-user programming techniques and grounding complex smart home contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12687v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingtian Shi, Xiaoyi Liu, Chun Yu, Tianao Yang, Cheng Gao, Chen Liang, Yuanchun Shi</dc:creator>
    </item>
    <item>
      <title>SonarWatch: Field sensing technique for smartwatches based on ultrasound and motion</title>
      <link>https://arxiv.org/abs/2408.12689</link>
      <description>arXiv:2408.12689v1 Announce Type: new 
Abstract: A smartwatch worn continuously on the wrist has the potential to perceive rich interactive gestures and natural behaviors of the user. Unfortunately, the current interaction functionality of smartwatches is primarily limited by the small touch screen. This paper proposes SonarWatch, a novel sensing technique that uses the acoustic field generated by the transceiver on the opposite sides of the watch to detect the presence of nearby objects and their shapes. This enables a range of gesture interactions and natural behavior perception. We designed an algorithm combining IMU and acoustic fields to identify these actions and optimize power consumption. We tested the performance of SonarWatch in different noise environments, achieving an overall accuracy of 93.7%. Its power consumption is close to that of physiological sensors. SonarWatch can achieve the above capabilities by utilizing the existing built-in sensors, making it a technology with solid practical value.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12689v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingtian Shi, Chun Yu, Xuyang Lu, Xing-Dong Yang, Yuntao Wang, Yuanchun Shi</dc:creator>
    </item>
    <item>
      <title>CasualGaze: Towards Modeling and Recognizing Casual Gaze Behavior for Efficient Gaze-based Object Selection</title>
      <link>https://arxiv.org/abs/2408.12710</link>
      <description>arXiv:2408.12710v1 Announce Type: new 
Abstract: We present CasualGaze, a novel eye-gaze-based target selection technique to support natural and casual eye-gaze input. Unlike existing solutions that require users to keep the eye-gaze center on the target actively, CasualGaze allows users to glance at the target object to complete the selection simply. To understand casual gaze behavior, we studied the spatial distribution of casual gaze for different layouts and user behavior in a simulated real-world environment. Results revealed the impacts of object parameters, the speed and randomness features of casual gaze, and special gaze behavior patterns in "blurred areas". Based on the results, we devised CasualGaze algorithms, employing a bivariate Gaussian distribution model along with temporal compensation and voting algorithms for robust target prediction. Usability evaluation study showed significant improvements in recognition and selection speed for CasualGaze compared with two baseline techniques. Subjective ratings and comments further supported the preference for CasualGaze regarding efficiency, accuracy, and stability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12710v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingtian Shi, Yukang Yan, Zisu Li, Chen Liang, Yuntao Wang, Chun Yu, Yuanchun Shi</dc:creator>
    </item>
    <item>
      <title>Visual Verity in AI-Generated Imagery: Computational Metrics and Human-Centric Analysis</title>
      <link>https://arxiv.org/abs/2408.12762</link>
      <description>arXiv:2408.12762v1 Announce Type: new 
Abstract: The rapid advancements in AI technologies have revolutionized the production of graphical content across various sectors, including entertainment, advertising, and e-commerce. These developments have spurred the need for robust evaluation methods to assess the quality and realism of AI-generated images. To address this, we conducted three studies. First, we introduced and validated a questionnaire called Visual Verity, which measures photorealism, image quality, and text-image alignment. Second, we applied this questionnaire to assess images from AI models (DALL-E2, DALL-E3, GLIDE, Stable Diffusion) and camera-generated images, revealing that camera-generated images excelled in photorealism and text-image alignment, while AI models led in image quality. We also analyzed statistical properties, finding that camera-generated images scored lower in hue, saturation, and brightness. Third, we evaluated computational metrics' alignment with human judgments, identifying MS-SSIM and CLIP as the most consistent with human assessments. Additionally, we proposed the Neural Feature Similarity Score (NFSS) for assessing image quality. Our findings highlight the need for refining computational metrics to better capture human visual perception, thereby enhancing AI-generated content evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12762v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Memoona Aziz, Umair Rahman, Syed Ali Safi, Amir Zaib Abbasi</dc:creator>
    </item>
    <item>
      <title>The Model Mastery Lifecycle: A Framework for Designing Human-AI Interaction</title>
      <link>https://arxiv.org/abs/2408.12781</link>
      <description>arXiv:2408.12781v1 Announce Type: new 
Abstract: The utilization of AI in an increasing number of fields is the latest iteration of a long process, where machines and systems have been replacing humans, or changing the roles that they play, in various tasks. Although humans are often resistant to technological innovation, especially in workplaces, there is a general trend towards increasing automation, and more recently, AI. AI is now capable of carrying out, or assisting with, many tasks that used to be regarded as exclusively requiring human expertise. In this paper we consider the case of tasks that could be performed either by human experts or by AI and locate them on a continuum running from exclusively human task performance at one end to AI autonomy on the other, with a variety of forms of human-AI interaction between those extremes. Implementation of AI is constrained by the context of the systems and workflows that it will be embedded within. There is an urgent need for methods to determine how AI should be used in different situations and to develop appropriate methods of human-AI interaction so that humans and AI can work together effectively to perform tasks. In response to the evolving landscape of AI progress and increasing mastery, we introduce an AI Mastery Lifecycle framework and discuss its implications for human-AI interaction. The framework provides guidance on human-AI task allocation and how human-AI interfaces need to adapt to improvements in AI task performance over time. Within the framework we identify a zone of uncertainty where the issues of human-AI task allocation and user interface design are likely to be most challenging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12781v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mark Chignell, Mu-Huan Miles Chung, Jaturong Kongmanee, Khilan Jerath, Abhay Raman</dc:creator>
    </item>
    <item>
      <title>Predicting Affective States from Screen Text Sentiment</title>
      <link>https://arxiv.org/abs/2408.12844</link>
      <description>arXiv:2408.12844v1 Announce Type: new 
Abstract: The proliferation of mobile sensing technologies has enabled the study of various physiological and behavioural phenomena through unobtrusive data collection from smartphone sensors. This approach offers real-time insights into individuals' physical and mental states, creating opportunities for personalised treatment and interventions. However, the potential of analysing the textual content viewed on smartphones to predict affective states remains underexplored. To better understand how the screen text that users are exposed to and interact with can influence their affects, we investigated a subset of data obtained from a digital phenotyping study of Australian university students conducted in 2023. We employed linear regression, zero-shot, and multi-shot prompting using a large language model (LLM) to analyse relationships between screen text and affective states. Our findings indicate that multi-shot prompting substantially outperforms both linear regression and zero-shot prompting, highlighting the importance of context in affect prediction. We discuss the value of incorporating textual and sentiment data for improving affect prediction, providing a basis for future advancements in understanding smartphone use and wellbeing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12844v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3675094.3678489</arxiv:DOI>
      <dc:creator>Songyan Teng, Tianyi Zhang, Simon D'Alfonso, Vassilis Kostakos</dc:creator>
    </item>
    <item>
      <title>Avatar Visual Similarity for Social HCI: Increasing Self-Awareness</title>
      <link>https://arxiv.org/abs/2408.13084</link>
      <description>arXiv:2408.13084v1 Announce Type: new 
Abstract: Self-awareness is a critical factor in social human-human interaction and, hence, in social HCI interaction. Increasing self-awareness through mirrors or video recordings is common in face-to-face trainings, since it influences antecedents of self-awareness like explicit identification and implicit affective identification (affinity). However, increasing self-awareness has been scarcely examined in virtual trainings with virtual avatars, which allow for adjusting the similarity, e.g. to avoid negative effects of self-consciousness. Automatic visual similarity in avatars is an open issue related to high costs. It is important to understand which features need to be manipulated and which degree of similarity is necessary for self-awareness to leverage the added value of using avatars for self-awareness. This article examines the relationship between avatar visual similarity and increasing self-awareness in virtual training environments. We define visual similarity based on perceptually important facial features for human-human identification and develop a theory-based methodology to systematically manipulate visual similarity of virtual avatars and support self-awareness. Three personalized versions of virtual avatars with varying degrees of visual similarity to participants were created (weak, medium and strong facial features manipulation). In a within-subject study (N=33), we tested effects of degree of similarity on perceived similarity, explicit identification and implicit affective identification (affinity). Results show significant differences between the weak similarity manipulation, and both the strong manipulation and the random avatar for all three antecedents of self-awareness. An increasing degree of avatar visual similarity influences antecedents of self-awareness in virtual environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13084v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bernhard Hilpert, Claudio Alves da Silva, Leon Christidis, Chirag Bhuvaneshwara, Patrick Gebhard, Fabrizio Nunnari, Dimitra Tsovaltzi</dc:creator>
    </item>
    <item>
      <title>Wheeler: A Three-Wheeled Input Device for Usable, Efficient, and Versatile Non-Visual Interaction</title>
      <link>https://arxiv.org/abs/2408.13166</link>
      <description>arXiv:2408.13166v1 Announce Type: new 
Abstract: Blind users rely on keyboards and assistive technologies like screen readers to interact with user interface (UI) elements. In modern applications with complex UI hierarchies, navigating to different UI elements poses a significant accessibility challenge. Users must listen to screen reader audio descriptions and press relevant keyboard keys one at a time. This paper introduces Wheeler, a novel three-wheeled, mouse-shaped stationary input device, to address this issue. Informed by participatory sessions, Wheeler enables blind users to navigate up to three hierarchical levels in an app independently using three wheels instead of navigating just one level at a time using a keyboard. The three wheels also offer versatility, allowing users to repurpose them for other tasks, such as 2D cursor manipulation. A study with 12 blind users indicates a significant reduction (40%) in navigation time compared to using a keyboard. Further, a diary study with our blind co-author highlights Wheeler's additional benefits, such as accessing UI elements with partial metadata and facilitating mixed-ability collaboration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13166v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3654777.3676396</arxiv:DOI>
      <dc:creator>Md Touhidul Islam, Noushad Sojib, Imran Kabir, Ashiqur Rahman Amit, Mohammad Ruhul Amin, Syed Masum Billah</dc:creator>
    </item>
    <item>
      <title>Demonstration of Wheeler: A Three-Wheeled Input Device for Usable, Efficient, and Versatile Non-Visual Interaction</title>
      <link>https://arxiv.org/abs/2408.13173</link>
      <description>arXiv:2408.13173v1 Announce Type: new 
Abstract: Navigating multi-level menus with complex hierarchies remains a big challenge for blind and low-vision users, who predominantly use screen readers to interact with computers. To that end, we demonstrate Wheeler, a three-wheeled input device with two side buttons that can speed up complex multi-level hierarchy navigation in common applications. When in operation, the three wheels of Wheeler are each mapped to a different level in the application hierarchy. Each level can be independently traversed using its designated wheel, allowing users to navigate through multiple levels efficiently. Wheeler's three wheels can also be repurposed for other tasks such as 2D cursor manipulation. In this demonstration, we describe the different operation modes and usage of Wheeler.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13173v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3672539.3686749</arxiv:DOI>
      <dc:creator>Md Touhidul Islam, Noushad Sojib, Imran Kabir, Ashiqur Rahman Amit, Mohammad Ruhul Amin, Syed Masum Billah</dc:creator>
    </item>
    <item>
      <title>Towards an Accessible, Noninvasive Micronutrient Status Assessment Method: A Comprehensive Review of Existing Techniques</title>
      <link>https://arxiv.org/abs/2408.11877</link>
      <description>arXiv:2408.11877v1 Announce Type: cross 
Abstract: Nutrients are critical to the functioning of the human body and their imbalance can result in detrimental health concerns. The majority of nutritional literature focuses on macronutrients, often ignoring the more critical nuances of micronutrient balance, which require more precise regulation. Currently, micronutrient status is routinely assessed via complex methods that are arduous for both the patient and the clinician. To address the global burden of micronutrient malnutrition, innovations in assessment must be accessible and noninvasive. In support of this task, this article synthesizes useful background information on micronutrients themselves, reviews the state of biofluid and physiological analyses for their assessment, and presents actionable opportunities to push the field forward. By taking a unique, clinical perspective that is absent from technological research on the topic, we find that the state of the art suffers from limited clinical relevance, a lack of overlap between biofluid and physiological approaches, and highly invasive and inaccessible solutions. Future work has the opportunity to maximize the impact of a novel assessment method by incorporating clinical relevance, the holistic nature of micronutrition, and prioritizing accessible and noninvasive systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11877v1</guid>
      <category>q-bio.QM</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew Balch, Maria A. Cardei, Sibylle Kranz, Afsaneh Doryab</dc:creator>
    </item>
    <item>
      <title>Improving Radiography Machine Learning Workflows via Metadata Management for Training Data Selection</title>
      <link>https://arxiv.org/abs/2408.12655</link>
      <description>arXiv:2408.12655v1 Announce Type: cross 
Abstract: Most machine learning models require many iterations of hyper-parameter tuning, feature engineering, and debugging to produce effective results. As machine learning models become more complicated, this pipeline becomes more difficult to manage effectively. In the physical sciences, there is an ever-increasing pool of metadata that is generated by the scientific research cycle. Tracking this metadata can reduce redundant work, improve reproducibility, and aid in the feature and training dataset engineering process. In this case study, we present a tool for machine learning metadata management in dynamic radiography. We evaluate the efficacy of this tool against the initial research workflow and discuss extensions to general machine learning pipelines in the physical sciences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12655v1</guid>
      <category>cs.LG</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mirabel Reid, Christine Sweeney, Oleg Korobkin</dc:creator>
    </item>
    <item>
      <title>Impact of Usability Mechanisms: A Family of Experiments on Efficiency, Effectiveness and User Satisfaction</title>
      <link>https://arxiv.org/abs/2408.12736</link>
      <description>arXiv:2408.12736v1 Announce Type: cross 
Abstract: Context: The usability software quality attribute aims to improve system user performance. In a previous study, we found evidence of the impact of a set of usability characteristics from the viewpoint of users in terms of efficiency, effectiveness and satisfaction. However, the impact level appears to depend on the usability feature and suggest priorities with respect to their implementation depending on how they promote user performance. Objectives: We use a family of three experiments to increase the precision and generalization of the results in the baseline experiment and provide findings on the impact on user performance of the Abort Operation, Progress Feedback and Preferences usability mechanisms. Method: We conduct two replications of the baseline experiment in academic settings. We analyse the data of 367 experimental subjects and apply aggregation (meta-analysis) procedures. Results: We find that the Abort Operation and Preferences usability mechanisms appear to improve system usability a great deal with respect to efficiency, effectiveness and user satisfaction. Conclusions: We find that the family of experiments further corroborates the results of the baseline experiment. Most of the results are statistically significant, and, because of the large number of experimental subjects, the evidence that we gathered in the replications is sufficient to outweigh other experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12736v1</guid>
      <category>cs.SE</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan M. Ferreira, Francy Rodr\'iguez, Adri\'an Santos, Silvia T. Acu\~na, Natalia Juristo</dc:creator>
    </item>
    <item>
      <title>Towards Human-Robot Teaming through Augmented Reality and Gaze-Based Attention Control</title>
      <link>https://arxiv.org/abs/2408.12823</link>
      <description>arXiv:2408.12823v1 Announce Type: cross 
Abstract: Robots are now increasingly integrated into various real world applications and domains. In these new domains, robots are mostly employed to improve, in some ways, the work done by humans. So, the need for effective Human-Robot Teaming (HRT) capabilities grows. These capabilities usually involve the dynamic collaboration between humans and robots at different levels of involvement, leveraging the strengths of both to efficiently navigate complex situations. Crucial to this collaboration is the ability of robotic systems to adjust their level of autonomy to match the needs of the task and the human team members.
  This paper introduces a system designed to control attention using HRT through the use of ground robots and augmented reality (AR) technology. Traditional methods of controlling attention, such as pointing, touch, and voice commands, sometimes fall short in precision and subtlety. Our system overcomes these limitations by employing AR headsets to display virtual visual markers. These markers act as dynamic cues to attract and shift human attention seamlessly, irrespective of the robot's physical location.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12823v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yousra Shleibik, Elijah Alabi, Christopher Reardon</dc:creator>
    </item>
    <item>
      <title>Underwater SONAR Image Classification and Analysis using LIME-based Explainable Artificial Intelligence</title>
      <link>https://arxiv.org/abs/2408.12837</link>
      <description>arXiv:2408.12837v1 Announce Type: cross 
Abstract: Deep learning techniques have revolutionized image classification by mimicking human cognition and automating complex decision-making processes. However, the deployment of AI systems in the wild, especially in high-security domains such as defence, is curbed by the lack of explainability of the model. To this end, eXplainable AI (XAI) is an emerging area of research that is intended to explore the unexplained hidden black box nature of deep neural networks. This paper explores the application of the eXplainable Artificial Intelligence (XAI) tool to interpret the underwater image classification results, one of the first works in the domain to the best of our knowledge. Our study delves into the realm of SONAR image classification using a custom dataset derived from diverse sources, including the Seabed Objects KLSG dataset, the camera SONAR dataset, the mine SONAR images dataset, and the SCTD dataset. An extensive analysis of transfer learning techniques for image classification using benchmark Convolutional Neural Network (CNN) architectures such as VGG16, ResNet50, InceptionV3, DenseNet121, etc. is carried out. On top of this classification model, a post-hoc XAI technique, viz. Local Interpretable Model-Agnostic Explanations (LIME) are incorporated to provide transparent justifications for the model's decisions by perturbing input data locally to see how predictions change. Furthermore, Submodular Picks LIME (SP-LIME) a version of LIME particular to images, that perturbs the image based on the submodular picks is also extensively studied. To this end, two submodular optimization algorithms i.e. Quickshift and Simple Linear Iterative Clustering (SLIC) are leveraged towards submodular picks. The extensive analysis of XAI techniques highlights interpretability of the results in a more human-compliant way, thus boosting our confidence and reliability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12837v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Purushothaman Natarajan, Athira Nambiar</dc:creator>
    </item>
    <item>
      <title>iSee: Advancing Multi-Shot Explainable AI Using Case-based Recommendations</title>
      <link>https://arxiv.org/abs/2408.12941</link>
      <description>arXiv:2408.12941v1 Announce Type: cross 
Abstract: Explainable AI (XAI) can greatly enhance user trust and satisfaction in AI-assisted decision-making processes. Recent findings suggest that a single explainer may not meet the diverse needs of multiple users in an AI system; indeed, even individual users may require multiple explanations. This highlights the necessity for a "multi-shot" approach, employing a combination of explainers to form what we introduce as an "explanation strategy". Tailored to a specific user or a user group, an "explanation experience" describes interactions with personalised strategies designed to enhance their AI decision-making processes. The iSee platform is designed for the intelligent sharing and reuse of explanation experiences, using Case-based Reasoning to advance best practices in XAI. The platform provides tools that enable AI system designers, i.e. design users, to design and iteratively revise the most suitable explanation strategy for their AI system to satisfy end-user needs. All knowledge generated within the iSee platform is formalised by the iSee ontology for interoperability. We use a summative mixed methods study protocol to evaluate the usability and utility of the iSee platform with six design users across varying levels of AI and XAI expertise. Our findings confirm that the iSee platform effectively generalises across applications and its potential to promote the adoption of XAI best practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12941v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.IR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anjana Wijekoon, Nirmalie Wiratunga, David Corsar, Kyle Martin, Ikechukwu Nkisi-Orji, Chamath Palihawadana, Marta Caro-Mart\'inez, Belen D\'iaz-Agudo, Derek Bridge, Anne Liret</dc:creator>
    </item>
    <item>
      <title>A Survey on Drowsiness Detection -- Modern Applications and Methods</title>
      <link>https://arxiv.org/abs/2408.12990</link>
      <description>arXiv:2408.12990v1 Announce Type: cross 
Abstract: Drowsiness detection holds paramount importance in ensuring safety in workplaces or behind the wheel, enhancing productivity, and healthcare across diverse domains. Therefore accurate and real-time drowsiness detection plays a critical role in preventing accidents, enhancing safety, and ultimately saving lives across various sectors and scenarios. This comprehensive review explores the significance of drowsiness detection in various areas of application, transcending the conventional focus solely on driver drowsiness detection. We delve into the current methodologies, challenges, and technological advancements in drowsiness detection schemes, considering diverse contexts such as public transportation, healthcare, workplace safety, and beyond. By examining the multifaceted implications of drowsiness, this work contributes to a holistic understanding of its impact and the crucial role of accurate and real-time detection techniques in enhancing safety and performance. We identified weaknesses in current algorithms and limitations in existing research such as accurate and real-time detection, stable data transmission, and building bias-free systems. Our survey frames existing works and leads to practical recommendations like mitigating the bias issue by using synthetic data, overcoming the hardware limitations with model compression, and leveraging fusion to boost model performance. This is a pioneering work to survey the topic of drowsiness detection in such an entirely and not only focusing on one single aspect. We consider the topic of drowsiness detection as a dynamic and evolving field, presenting numerous opportunities for further exploration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12990v1</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Biying Fu, Fadi Boutros, Chin-Teng Lin, Naser Damer</dc:creator>
    </item>
    <item>
      <title>VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints</title>
      <link>https://arxiv.org/abs/2408.13019</link>
      <description>arXiv:2408.13019v1 Announce Type: cross 
Abstract: Emotion recognition can enhance humanized machine responses to user commands, while voiceprint-based perception systems can be easily integrated into commonly used devices like smartphones and stereos. Despite having the largest number of speakers, there is a noticeable absence of high-quality corpus datasets for emotion recognition using Chinese voiceprints. Hence, this paper introduces the VCEMO dataset to address this deficiency. The proposed dataset is constructed from everyday conversations and comprises over 100 users and 7,747 textual samples. Furthermore, this paper proposes a multimodal-based model as a benchmark, which effectively fuses speech, text, and external knowledge using a co-attention structure. The system employs contrastive learning-based regulation for the uneven distribution of the dataset and the diversity of emotional expressions. The experiments demonstrate the significant improvement of the proposed model over SOTA on the VCEMO and IEMOCAP datasets. Code and dataset will be released for research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13019v1</guid>
      <category>cs.MM</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinghua Tang, Liyun Zhang, Yu Lu, Dian Ding, Lanqing Yang, YiChao Chen, Minjie Bian, Xiaoshan Li, Guangtao Xue</dc:creator>
    </item>
    <item>
      <title>AI Reliance and Decision Quality: Fundamentals, Interdependence, and the Effects of Interventions</title>
      <link>https://arxiv.org/abs/2304.08804</link>
      <description>arXiv:2304.08804v2 Announce Type: replace 
Abstract: In AI-assisted decision-making, a central promise of having a human-in-the-loop is that they should be able to complement the AI system by overriding its wrong recommendations. In practice, however, we often see that humans cannot assess the correctness of AI recommendations and, as a result, adhere to wrong or override correct advice. Different ways of relying on AI recommendations have immediate, yet distinct, implications for decision quality. Unfortunately, reliance and decision quality are often inappropriately conflated in the current literature on AI-assisted decision-making. In this work, we disentangle and formalize the relationship between reliance and decision quality, and we characterize the conditions under which human-AI complementarity is achievable. To illustrate how reliance and decision quality relate to one another, we propose a visual framework and demonstrate its usefulness for interpreting empirical findings, including the effects of interventions like explanations. Overall, our research highlights the importance of distinguishing between reliance behavior and decision quality in AI-assisted decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.08804v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jakob Schoeffer, Johannes Jakubik, Michael Voessing, Niklas Kuehl, Gerhard Satzger</dc:creator>
    </item>
    <item>
      <title>Digital citizen science for ethical surveillance of physical activity among youth: mobile ecological momentary assessments vs. retrospective recall</title>
      <link>https://arxiv.org/abs/2308.10626</link>
      <description>arXiv:2308.10626v2 Announce Type: replace 
Abstract: Physical inactivity is the fourth leading risk factor of mortality globally. Hence, understanding the physical activity (PA) patterns of youth is essential to manage and mitigate non-communicable diseases. As digital citizen science approaches utilizing citizen-owned smartphones to ethically obtain PA big data can transform PA surveillance, this study aims to understand the frequency of PA reported by youth using smartphone-deployed retrospective validated surveys compared to prospective time-triggered mobile ecological momentary assessments (mEMAs). Using a digital citizen science methodology, this study recruited youth citizen scientists (N = 808) in 2018 (August 31- December 31) in Saskatchewan, Canada. Youth citizen scientists (age 13 to 21) reported their PA using prospective mEMAs and retrospective surveys over an eight-day period. A significant difference was found in reporting the frequency of PA retrospectively vs. prospectively via mEMAs (p &lt; 0.000). Ethnicity, parental education, and strength training were associated with prospective PA frequency; however, no associations were significant with retrospective PA frequency. With access to ubiquitous digital devices growing worldwide, and youth having particularly high digital literacy, digital citizen science for the ethical surveillance of PA using mEMAs presents a promising approach for the management and prevention of non-communicable diseases among youth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.10626v2</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sheriff Tolulope Ibrahim, Jamin Patel, Tarun Reddy Katapally</dc:creator>
    </item>
    <item>
      <title>Writing with AI Lowers Psychological Ownership, but Longer Prompts Can Help</title>
      <link>https://arxiv.org/abs/2404.03108</link>
      <description>arXiv:2404.03108v2 Announce Type: replace 
Abstract: Feelings of something belonging to someone is called "psychological ownership." A common assumption is that writing with generative AI lowers psychological ownership, but the extent to which this occurs is unclear. We report on two experiments to examine the relationship between psychological ownership and prompt length. Participants wrote short stories either completely by themselves or wrote prompts of varying lengths, enforced through word limits. Results show that when participants wrote longer prompts, they had higher levels of psychological ownership. Their comments suggest they thought more about their prompts, often adding more details. However, benefits plateaued when prompt length was 75-100% of the target story length. To work in current interfaces, we propose creating a "soft constraint" in the interface to nudge behaviour. Results show that a submit button that must be held down a long time if the prompt is short is effective at increasing prompt length.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03108v2</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nikhita Joshi, Daniel Vogel</dc:creator>
    </item>
    <item>
      <title>AppAgent v2: Advanced Agent for Flexible Mobile Interactions</title>
      <link>https://arxiv.org/abs/2408.11824</link>
      <description>arXiv:2408.11824v2 Announce Type: replace 
Abstract: With the advancement of Multimodal Large Language Models (MLLM), LLM-driven visual agents are increasingly impacting software interfaces, particularly those with graphical user interfaces. This work introduces a novel LLM-based multimodal agent framework for mobile devices. This framework, capable of navigating mobile devices, emulates human-like interactions. Our agent constructs a flexible action space that enhances adaptability across various applications including parser, text and vision descriptions. The agent operates through two main phases: exploration and deployment. During the exploration phase, functionalities of user interface elements are documented either through agent-driven or manual explorations into a customized structured knowledge base. In the deployment phase, RAG technology enables efficient retrieval and update from this knowledge base, thereby empowering the agent to perform tasks effectively and accurately. This includes performing complex, multi-step operations across various applications, thereby demonstrating the framework's adaptability and precision in handling customized task workflows. Our experimental results across various benchmarks demonstrate the framework's superior performance, confirming its effectiveness in real-world scenarios. Our code will be open source soon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11824v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yanda Li, Chi Zhang, Wanqi Yang, Bin Fu, Pei Cheng, Xin Chen, Ling Chen, Yunchao Wei</dc:creator>
    </item>
    <item>
      <title>Did the Roll-Out of Community Notes Reduce Engagement With Misinformation on X/Twitter?</title>
      <link>https://arxiv.org/abs/2307.07960</link>
      <description>arXiv:2307.07960v2 Announce Type: replace-cross 
Abstract: Developing interventions that successfully reduce engagement with misinformation on social media is challenging. One intervention that has recently gained great attention is X/Twitter's Community Notes (previously known as "Birdwatch"). Community Notes is a crowdsourced fact-checking approach that allows users to write textual notes to inform others about potentially misleading posts on X/Twitter. Yet, empirical evidence regarding its effectiveness in reducing engagement with misinformation on social media is missing. In this paper, we perform a large-scale empirical study to analyze whether the introduction of the Community Notes feature and its roll-out to users in the U.S. and around the world have reduced engagement with misinformation on X/Twitter in terms of retweet volume and likes. We employ Difference-in-Differences (DiD) models and Regression Discontinuity Design (RDD) to analyze a comprehensive dataset consisting of all fact-checking notes and corresponding source tweets since the launch of Community Notes in early 2021. Although we observe a significant increase in the volume of fact-checks carried out via Community Notes, particularly for tweets from verified users with many followers, we find no evidence that the introduction of Community Notes significantly reduced engagement with misleading tweets on X/Twitter. Rather, our findings suggest that Community Notes might be too slow to effectively reduce engagement with misinformation in the early (and most viral) stage of diffusion. Our work emphasizes the importance of evaluating fact-checking interventions in the field and offers important implications to enhance crowdsourced fact-checking strategies on social media.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.07960v2</guid>
      <category>cs.SI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3686967</arxiv:DOI>
      <dc:creator>Yuwei Chuai, Haoye Tian, Nicolas Pr\"ollochs, Gabriele Lenzini</dc:creator>
    </item>
    <item>
      <title>XEQ Scale for Evaluating XAI Experience Quality Grounded in Psychometric Theory</title>
      <link>https://arxiv.org/abs/2407.10662</link>
      <description>arXiv:2407.10662v3 Announce Type: replace-cross 
Abstract: Explainable Artificial Intelligence (XAI) aims to improve the transparency of autonomous decision-making through explanations. Recent literature has emphasised users' need for holistic "multi-shot" explanations and the ability to personalise their engagement with XAI systems. We refer to this user-centred interaction as an XAI Experience. Despite advances in creating XAI experiences, evaluating them in a user-centred manner has remained challenging. To address this, we introduce the XAI Experience Quality (XEQ) Scale (pronounced "Seek" Scale), for evaluating the user-centred quality of XAI experiences. Furthermore, XEQ quantifies the quality of experiences across four evaluation dimensions: learning, utility, fulfilment and engagement. These contributions extend the state-of-the-art of XAI evaluation, moving beyond the one-dimensional metrics frequently developed to assess single-shot explanations. In this paper, we present the XEQ scale development and validation process, including content validation with XAI experts as well as discriminant and construct validation through a large-scale pilot study. Out pilot study results offer strong evidence that establishes the XEQ Scale as a comprehensive framework for evaluating user-centred XAI experiences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10662v3</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anjana Wijekoon, Nirmalie Wiratunga, David Corsar, Kyle Martin, Ikechukwu Nkisi-Orji, Belen D\'iaz-Agudo, Derek Bridge</dc:creator>
    </item>
    <item>
      <title>Scaling Virtual World with Delta-Engine</title>
      <link>https://arxiv.org/abs/2408.05842</link>
      <description>arXiv:2408.05842v2 Announce Type: replace-cross 
Abstract: In this paper, we focus on the \emph{virtual world}, a cyberspace where people can live in. An ideal virtual world shares great similarity with our real world. One of the crucial aspects is its evolving nature, reflected by individuals' capability to grow and thereby influence the objective world. Such dynamics is unpredictable and beyond the reach of existing systems. For this, we propose a special engine called \textbf{\emph{Delta-Engine}} to drive this virtual world. $\Delta$ associates the world's evolution to the engine's scalability. It consists of a base engine and a neural proxy. The base engine programs the prototype of the virtual world; given a trigger, the neural proxy generates new snippets on the base engine through \emph{incremental prediction}.
  This paper presents a full-stack introduction to the delta-engine. The key feature of the delta-engine is its scalability to unknown elements within the world, Technically, it derives from the prefect co-work of the neural proxy and the base engine, and the alignment with high-quality data. We introduce an engine-oriented fine-tuning method that embeds the base engine into the proxy. We then discuss the human-LLM collaborative design to produce novel and interesting data efficiently. Eventually, we propose three evaluation principles to comprehensively assess the performance of a delta engine: naive evaluation, incremental evaluation, and adversarial evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05842v2</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongqiu Wu, Zekai Xu, Tianyang Xu, Jiale Hong, Weiqi Wu, Yan Wang, Hai Zhao, Min Zhang, Zhezhi He</dc:creator>
    </item>
  </channel>
</rss>
