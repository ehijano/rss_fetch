<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.IT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.IT</link>
    <description>cs.IT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.IT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 04 Feb 2025 04:06:57 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Distributed Offloading in Multi-Access Edge Computing Systems: A Mean-Field Perspective</title>
      <link>https://arxiv.org/abs/2501.18718</link>
      <description>arXiv:2501.18718v1 Announce Type: new 
Abstract: Multi-access edge computing (MEC) technology is a promising solution to assist power-constrained IoT devices by providing additional computing resources for time-sensitive tasks. In this paper, we consider the problem of optimal task offloading in MEC systems with due consideration of the timeliness and scalability issues under two scenarios of equitable and priority access to the edge server (ES). In the first scenario, we consider a MEC system consisting of $N$ devices assisted by one ES, where the devices can split task execution between a local processor and the ES, with equitable access to the ES. In the second scenario, we consider a MEC system consisting of one primary user, $N$ secondary users and one ES. The primary user has priority access to the ES while the secondary users have equitable access to the ES amongst themselves. In both scenarios, due to the power consumption associated with utilizing the local resource and task offloading, the devices must optimize their actions. Additionally, since the ES is a shared resource, other users' offloading activity serves to increase latency incurred by each user. We thus model both scenarios using a non-cooperative game framework. However, the presence of a large number of users makes it nearly impossible to compute the equilibrium offloading policies for each user, which would require a significant information exchange overhead between users. Thus, to alleviate such scalability issues, we invoke the paradigm of mean-field games to compute approximate Nash equilibrium policies for each user using their local information, and further study the trade-offs between increasing information freshness and reducing power consumption for each user. Using numerical evaluations, we show that our approach can recover the offloading trends displayed under centralized solutions, and provide additional insights into the results obtained.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18718v1</guid>
      <category>cs.IT</category>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shubham Aggarwal, Muhammad Aneeq uz Zaman, Melih Bastopcu, Sennur Ulukus, Tamer Ba\c{s}ar</dc:creator>
    </item>
    <item>
      <title>Integrated Communication and Binary State Detection Under Unequal Error Constraints</title>
      <link>https://arxiv.org/abs/2501.18911</link>
      <description>arXiv:2501.18911v1 Announce Type: new 
Abstract: This work considers a problem of integrated sensing and communication (ISAC) in which the goal of sensing is to detect a binary state. Unlike most approaches that minimize the total detection error probability, in our work, we disaggregate the error probability into false alarm and missed detection probabilities and investigate their information-theoretic three-way tradeoff including communication data rate. We consider a broadcast channel that consists of a transmitter, a communication receiver, and a detector where the receiver's and the detector's channels are affected by an unknown binary state. We consider and present results on two different state-dependent models. In the first setting, the state is fixed throughout the entire transmission, for which we fully characterize the optimal three-way tradeoff between the coding rate for communication and the two possibly nonidentical error exponents for sensing in the asymptotic regime. The achievability and converse proofs rely on the analysis of the cumulant-generating function of the log-likelihood ratio. In the second setting, the state changes every symbol in an independently and identically distributed (i.i.d.) manner, for which we characterize the optimal tradeoff region based on the analysis of the receiver operating characteristic (ROC) curves.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18911v1</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daewon Seo, Sung Hoon Lim</dc:creator>
    </item>
    <item>
      <title>Extension of Optimal Locally Repairable codes</title>
      <link>https://arxiv.org/abs/2501.18989</link>
      <description>arXiv:2501.18989v1 Announce Type: new 
Abstract: Recent studies have delved into the construction of locally repairable codes (LRCs) with optimal minimum distance from function fields. In this paper, we present several novel constructions by extending the findings of optimally designed locally repairable codes documented in the literature. Let $C$ denote an optimal LRC of locality $r$, implying that every repairable block of $C$ is a $[r+1, r]$ MDS code, and $C$ maximizes its minimum distance. By extending a single coordinate of one of these blocks, we demonstrate that the resulting code remains an optimally designed locally repairable code. This suggests that the maximal length of an optimal LRC from rational function fields can be extended up to $q+2$ over a finite field $\mathbb{F}_q$. In addition, we give a new construction of optimal $(r, 3)$-LRC by extending one coordinate in each block within $C$. Furthermore, we propose a novel family of LRCs with Roth-Lempel type that are optimal under certain conditions. Finally, we explore optimal LRCs derived from elliptic function fields and extend a single coordinate of such codes. This approach leads us to confirm that the new codes are also optimal, thereby allowing their lengths to reach $q + 2\sqrt{q} - 2r - 2$ with locality $r$. We also consider the construction of optimal $(r, 3)$-LRC in elliptic function fields, with exploring one more condition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18989v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunlong Zhu, Chang-An Zhao</dc:creator>
    </item>
    <item>
      <title>Upper Bounds on the Minimum Distance of Structured LDPC Codes</title>
      <link>https://arxiv.org/abs/2501.19125</link>
      <description>arXiv:2501.19125v1 Announce Type: new 
Abstract: We investigate the minimum distance of structured binary Low-Density Parity-Check (LDPC) codes whose parity-check matrices are of the form $[\mathbf{C} \vert \mathbf{M}]$ where $\mathbf{C}$ is circulant and of column weight $2$, and $\mathbf{M}$ has fixed column weight $r \geq 3$ and row weight at least $1$. These codes are of interest because they are LDPC codes which come with a natural linear-time encoding algorithm. We show that the minimum distance of these codes is in $O(n^{\frac{r-2}{r-1} + \epsilon})$, where $n$ is the code length and $\epsilon &gt; 0$ is arbitrarily small. This improves the previously known upper bound in $O(n^{\frac{r-1}{r}})$ on the minimum distance of such codes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19125v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fran\c{c}ois Arnault, Philippe Gaborit, Wouter Rozendaal, Nicolas Saussay, Gilles Z\'emor</dc:creator>
    </item>
    <item>
      <title>Minimax discrete distribution estimation with self-consumption</title>
      <link>https://arxiv.org/abs/2501.19273</link>
      <description>arXiv:2501.19273v1 Announce Type: new 
Abstract: Learning distributions from i.i.d. samples is a well-understood problem. However, advances in generative machine learning prompt an interesting new, non-i.i.d. setting: after receiving a certain number of samples, an estimated distribution is fixed, and samples from this estimate are drawn and introduced into the sample corpus, undifferentiated from real samples. Subsequent generations of estimators now face contaminated environments, an effect referred to in the machine learning literature as self-consumption. In this paper, we study the effect of such contamination from previous estimates on the minimax loss of multi-stage discrete distribution estimation.
  In the data accumulation setting, where all batches of samples are available for estimation, we provide minimax bounds for the expected $\ell_2^2$ and $\ell_1$ losses at every stage. We show examples where our bounds match under mild conditions, and there is a strict gap with the corresponding oracle-assisted minimax loss where real and synthetic samples are differentiated. We also provide a lower bound on the minimax loss in the data replacement setting, where only the latest batch of samples is available, and use it to find a lower bound for the worst-case loss for bounded estimate trajectories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19273v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Millen Kanabar, Michael Gastpar</dc:creator>
    </item>
    <item>
      <title>Quantum-Inspired Fidelity-based Divergence</title>
      <link>https://arxiv.org/abs/2501.19307</link>
      <description>arXiv:2501.19307v1 Announce Type: new 
Abstract: Kullback--Leibler (KL) divergence is a fundamental measure of the dissimilarity between two probability distributions, but it can become unstable in high-dimensional settings due to its sensitivity to mismatches in distributional support. To address robustness limitations, we propose a novel Quantum-Inspired Fidelity-based Divergence (QIF), leveraging quantum information principles yet efficiently computable on classical hardware. Compared to KL divergence, QIF demonstrates improved numerical stability under partial or near-disjoint support conditions, thereby reducing the need for extensive regularization in specific scenarios. Moreover, QIF admits well-defined theoretical bounds and continuous similarity measures. Building on this, we introduce a novel regularization method, QR-Drop, which utilizes QIF to improve generalization in machine learning models. Empirical results show that QR-Drop effectively mitigates overfitting and outperforms state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19307v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifeng Peng, Dantong Li, Xinyi Li, Zhiding Liang, Yongshan Ding, Ying Wang</dc:creator>
    </item>
    <item>
      <title>A New Statistical Approach to the Performance Analysis of Vision-based Localization</title>
      <link>https://arxiv.org/abs/2501.18758</link>
      <description>arXiv:2501.18758v1 Announce Type: cross 
Abstract: Many modern wireless devices with accurate positioning needs also have access to vision sensors, such as a camera, radar, and Light Detection and Ranging (LiDAR). In scenarios where wireless-based positioning is either inaccurate or unavailable, using information from vision sensors becomes highly desirable for determining the precise location of the wireless device. Specifically, vision data can be used to estimate distances between the target (where the sensors are mounted) and nearby landmarks. However, a significant challenge in positioning using these measurements is the inability to uniquely identify which specific landmark is visible in the data. For instance, when the target is located close to a lamppost, it becomes challenging to precisely identify the specific lamppost (among several in the region) that is near the target. This work proposes a new framework for target localization using range measurements to multiple proximate landmarks. The geometric constraints introduced by these measurements are utilized to narrow down candidate landmark combinations corresponding to the range measurements and, consequently, the target's location on a map. By modeling landmarks as a marked Poisson point process (PPP), we show that three noise-free range measurements are sufficient to uniquely determine the correct combination of landmarks in a two-dimensional plane. For noisy measurements, we provide a mathematical characterization of the probability of correctly identifying the observed landmark combination based on a novel joint distribution of key random variables. Our results demonstrate that the landmark combination can be identified using ranges, even when individual landmarks are visually indistinguishable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18758v1</guid>
      <category>cs.CV</category>
      <category>cs.IT</category>
      <category>eess.IV</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haozhou Hu, Harpreet S. Dhillon, R. Michael Buehrer</dc:creator>
    </item>
    <item>
      <title>A topological theory for qLDPC: non-Clifford gates and magic state fountain on homological product codes with constant rate and beyond the $N^{1/3}$ distance barrier</title>
      <link>https://arxiv.org/abs/2501.19375</link>
      <description>arXiv:2501.19375v1 Announce Type: cross 
Abstract: We develop a unified theory for fault-tolerant quantum computation in quantum low-density parity-check (qLDPC) and topological codes. We show that there exist hidden simplicial complex structures encoding the topological data for all qLDPC and CSS codes obtained from product construction by generalizing the Freedman-Hastings code-to-manifold mapping. This is achieved by building manifolds corresponding to high-dimensional topological expanders from the Tanner graphs of the skeleton classical or quantum codes, which further form a product manifold and an associated thickened product code defined on its triangulation with only a constant qubit overhead. This suggests that qLDPC or more generally CSS codes obtained from product constructions are topological, and hence can admit cohomology operations such as cup products, physically corresponding to higher symmetries in the underlying topological quantum field theory. When applying this mapping to a 3D hypergraph product code obtained from the product of 3 copies of good classical expander codes, we obtain the first non-Clifford logical CCZ gates via constant depth circuits on a code with constant stabilizer weight $w=O(1)$, constant rate $K=\Theta(N)$, and polynomial distance $D=\Omega(N^{1/3})$. When applied to 3D homological product codes consisting of the product of a pair of good quantum and classical LDPC codes, we can further improve the distance to $D=\Omega(\sqrt{N})$ exceeding the $N^{1/3}$ distance barrier implied by the Bravyi-K\"onig bound for conventional topological codes. Our work suggests that it is feasible to apply native logical non-Clifford gates on qLDPC codes or directly inject high-fidelity magic states as resources (`magic state fountain') without the distillation process. For the homological product construction, the fountain can inject $\Theta(\sqrt{N})$ magic states in parallel in a single round.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19375v1</guid>
      <category>quant-ph</category>
      <category>cond-mat.str-el</category>
      <category>cs.IT</category>
      <category>hep-th</category>
      <category>math.GT</category>
      <category>math.IT</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guanyu Zhu</dc:creator>
    </item>
    <item>
      <title>Distributed Joint User Activity Detection, Channel Estimation, and Data Detection via Expectation Propagation in Cell-Free Massive MIMO</title>
      <link>https://arxiv.org/abs/2405.09914</link>
      <description>arXiv:2405.09914v2 Announce Type: replace 
Abstract: We consider the uplink of a grant-free cell-free massive multiple-input multiple-output (GF-CF-MaMIMO) system. We propose an algorithm for distributed joint activity detection, channel estimation, and data detection (JACD) based on expectation propagation (EP) called JACD-EP. We develop the algorithm by factorizing the a posteriori probability (APP) of activities, channels, and transmitted data, then, mapping functions and variables onto a factor graph, and finally, performing a message passing on the resulting factor graph. If users with the same pilot sequence are sufficiently distant from each other, the JACD-EP algorithm is able to mitigate the effects of pilot contamination which naturally occurs in grant-free systems due to the large number of potential users and limited signaling resources. Furthermore, it outperforms state-of-the-art algorithms for JACD in GF-CF-MaMIMO systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09914v2</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/SPAWC60668.2024.10694527</arxiv:DOI>
      <arxiv:journal_reference>2024 IEEE 25th International Workshop on Signal Processing Advances in Wireless Communications (SPAWC)</arxiv:journal_reference>
      <dc:creator>Christian Forsch, Alexander Karataev, Laura Cottatellucci</dc:creator>
    </item>
    <item>
      <title>Lossless data compression at pragmatic rates</title>
      <link>https://arxiv.org/abs/2501.10103</link>
      <description>arXiv:2501.10103v2 Announce Type: replace 
Abstract: The problem of variable-rate lossless data compression is considered, for codes with and without prefix constraints. Sharp bounds are derived for the best achievable compression rate of memoryless sources, when the excess-rate probability is required to be exponentially small in the blocklength. Accurate nonasymptotic expansions with explicit constants are obtained for the optimal rate, using tools from large deviations and Gaussian approximation. Examples are shown indicating that, in the small excess-rate-probability regime, the approximation to the fundamental limit of the compression rate suggested by these bounds is significantly more accurate than the approximations provided by either normal approximation or error exponents. The new bounds reinforce the crucial operational conclusion that, in applications where the blocklength is relatively short and where stringent guarantees are required on the rate, the best achievable rate is no longer close to the entropy. Rather, it is an appropriate, more pragmatic rate, determined via the inverse error exponent function and the blocklength.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10103v2</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Andreas Theocharous, Ioannis Kontoyiannis</dc:creator>
    </item>
    <item>
      <title>A Unified Representation of Density-Power-Based Divergences Reducible to M-Estimation</title>
      <link>https://arxiv.org/abs/2501.16287</link>
      <description>arXiv:2501.16287v3 Announce Type: replace 
Abstract: Density-power-based divergences are known to provide robust inference procedures against outliers, and their extensions have been widely studied. A characteristic of successful divergences is that the estimation problem can be reduced to M-estimation. In this paper, we define a norm-based Bregman density power divergence (NB-DPD) -- density-power-based divergence with functional flexibility within the framework of Bregman divergences that can be reduced to M-estimation. We show that, by specifying the function $\phi_\gamma$, NB-DPD reduces to well-known divergences, such as the density power divergence and the $\gamma$-divergence. Furthermore, by examining the combinations of functions $\phi_\gamma$ corresponding to existing divergences, we show that a new divergence connecting these existing divergences can be derived. Finally, we show that the redescending property, one of the key indicators of robustness, holds only for the $\gamma$-divergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16287v3</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kobayashi</dc:creator>
    </item>
    <item>
      <title>Uncoded Download in Lagrange-Coded Elastic Computing with Straggler Tolerance</title>
      <link>https://arxiv.org/abs/2501.16298</link>
      <description>arXiv:2501.16298v2 Announce Type: replace 
Abstract: Coded elastic computing, introduced by Yang et al. in 2018, is a technique designed to mitigate the impact of elasticity in cloud computing systems, where machines can be preempted or be added during computing rounds. This approach utilizes maximum distance separable (MDS) coding for both storage and download in matrix-matrix multiplications. The proposed scheme is unable to tolerate stragglers and has high encoding complexity and upload cost. In 2023, we addressed these limitations by employing uncoded storage and Lagrange-coded download. However, it results in a large storage size. To address the challenges of storage size and upload cost, in this paper, we focus on Lagrange-coded elastic computing based on uncoded download. We propose a new class of elastic computing schemes, using Lagrange-coded storage with uncoded download (LCSUD). Our proposed schemes address both elasticity and straggler challenges while achieving lower storage size, reduced encoding complexity, and upload cost compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16298v2</guid>
      <category>cs.IT</category>
      <category>cs.DC</category>
      <category>math.IT</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xi Zhong, Samuel Lu, Joerg Kliewer, Mingyue Ji</dc:creator>
    </item>
    <item>
      <title>Simple Worst-Case Optimal Adaptive Prefix-Free Coding</title>
      <link>https://arxiv.org/abs/2109.02997</link>
      <description>arXiv:2109.02997v3 Announce Type: replace-cross 
Abstract: We give a new and simple worst-case optimal algorithm for adaptive prefix-free coding that matches Gagie and Nekrich's bounds except for lower-order terms, and uses no data structures more complicated than a lookup table. Moreover, when Gagie and Nekrich's algorithm is modified for adaptive alphabetic prefix-free coding its decoding time slows down to $O (\log \log n)$ per character, but ours can be modified for this problem with no asymptotic slowdown. As far as we know, this gives the first algorithm for this problem that is simultaneously worst-case optimal in terms of encoding and decoding time and of encoding length.</description>
      <guid isPermaLink="false">oai:arXiv.org:2109.02997v3</guid>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Travis Gagie</dc:creator>
    </item>
    <item>
      <title>Theoretical guarantees on the best-of-n alignment policy</title>
      <link>https://arxiv.org/abs/2401.01879</link>
      <description>arXiv:2401.01879v2 Announce Type: replace-cross 
Abstract: A simple and effective method for the inference-time alignment of generative models is the best-of-$n$ policy, where $n$ samples are drawn from a reference policy, ranked based on a reward function, and the highest ranking one is selected. A commonly used analytical expression in the literature claims that the KL divergence between the best-of-$n$ policy and the reference policy is equal to $\log (n) - (n-1)/n.$ We disprove the validity of this claim, and show that it is an upper bound on the actual KL divergence. We also explore the tightness of this upper bound in different regimes, and propose a new estimator for the KL divergence and empirically show that it provides a tight approximation. We also show that the win rate of the best-of-$n$ policy against the reference policy is upper bounded by $n/(n+1)$ and derive bounds on the tightness of this characterization. We conclude with analyzing the tradeoffs between win rate and KL divergence of the best-of-$n$ alignment policy, which demonstrate that very good tradeoffs are achievable with $n &lt; 1000$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.01879v2</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ahmad Beirami, Alekh Agarwal, Jonathan Berant, Alexander D'Amour, Jacob Eisenstein, Chirag Nagpal, Ananda Theertha Suresh</dc:creator>
    </item>
    <item>
      <title>Contraction of Private Quantum Channels and Private Quantum Hypothesis Testing</title>
      <link>https://arxiv.org/abs/2406.18651</link>
      <description>arXiv:2406.18651v2 Announce Type: replace-cross 
Abstract: A quantum generalized divergence by definition satisfies the data-processing inequality; as such, the relative decrease in such a divergence under the action of a quantum channel is at most one. This relative decrease is formally known as the contraction coefficient of the channel and the divergence. Interestingly, there exist combinations of channels and divergences for which the contraction coefficient is strictly less than one. Furthermore, understanding the contraction coefficient is fundamental for the study of statistical tasks under privacy constraints. To this end, here we establish upper bounds on contraction coefficients for the hockey-stick divergence under privacy constraints, where privacy is quantified with respect to the quantum local differential privacy (QLDP) framework, and we fully characterize the contraction coefficient for the trace distance under privacy constraints. With the machinery developed, we also determine an upper bound on the contraction of both the Bures distance and quantum relative entropy relative to the normalized trace distance, under QLDP constraints. Next, we apply our findings to establish bounds on the sample complexity of quantum hypothesis testing under privacy constraints. Furthermore, we study various scenarios in which the sample complexity bounds are tight, while providing order-optimal quantum channels that achieve those bounds. Lastly, we show how private quantum channels provide fairness and Holevo information stability in quantum learning settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18651v2</guid>
      <category>quant-ph</category>
      <category>cs.CR</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TIT.2025.3527859</arxiv:DOI>
      <dc:creator>Theshani Nuradha, Mark M. Wilde</dc:creator>
    </item>
    <item>
      <title>Adaptive Learn-then-Test: Statistically Valid and Efficient Hyperparameter Selection</title>
      <link>https://arxiv.org/abs/2409.15844</link>
      <description>arXiv:2409.15844v2 Announce Type: replace-cross 
Abstract: We introduce adaptive learn-then-test (aLTT), an efficient hyperparameter selection procedure that provides finite-sample statistical guarantees on the population risk of AI models. Unlike the existing learn-then-test (LTT) technique, which relies on conventional p-value-based multiple hypothesis testing (MHT), aLTT implements sequential data-dependent MHT with early termination by leveraging e-processes. As a result, aLTT can reduce the number of testing rounds, making it particularly well-suited for scenarios in which testing is costly or presents safety risks. Apart from maintaining statistical validity, in applications such as online policy selection for offline reinforcement learning and prompt engineering, aLTT is shown to achieve the same performance as LTT while requiring only a fraction of the testing rounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15844v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matteo Zecchin, Sangwoo Park, Osvaldo Simeone</dc:creator>
    </item>
    <item>
      <title>Sociotechnical Approach to Enterprise Generative Artificial Intelligence (E-GenAI)</title>
      <link>https://arxiv.org/abs/2409.17408</link>
      <description>arXiv:2409.17408v2 Announce Type: replace-cross 
Abstract: In this theoretical article, a sociotechnical approach is proposed to characterize. First, the business ecosystem, focusing on the relationships among Providers, Enterprise, and Customers through SCM, ERP, and CRM platforms to align: (1) Business Intelligence (BI), Fuzzy Logic (FL), and TRIZ (Theory of Inventive Problem Solving), through the OID model, and (2) Knowledge Management (KM) and Imperfect Knowledge Management (IKM), through the OIDK model. Second, the article explores the E-GenAI business ecosystem, which integrates GenAI-based platforms for SCM, ERP, and CRM with GenAI-based platforms for BI, FL, TRIZ, KM, and IKM, to align Large Language Models (LLMs) through the E-GenAI (OID) model. Finally, to understand the dynamics of LLMs, we utilize finite automata to model the relationships between Followers and Followees. This facilitates the construction of LLMs that can identify specific characteristics of users on a social media platform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17408v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leoncio Jimenez, Francisco Venegas</dc:creator>
    </item>
    <item>
      <title>Learning multivariate Gaussians with imperfect advice</title>
      <link>https://arxiv.org/abs/2411.12700</link>
      <description>arXiv:2411.12700v3 Announce Type: replace-cross 
Abstract: We revisit the problem of distribution learning within the framework of learning-augmented algorithms. In this setting, we explore the scenario where a probability distribution is provided as potentially inaccurate advice on the true, unknown distribution. Our objective is to develop learning algorithms whose sample complexity decreases as the quality of the advice improves, thereby surpassing standard learning lower bounds when the advice is sufficiently accurate.
  Specifically, we demonstrate that this outcome is achievable for the problem of learning a multivariate Gaussian distribution $N(\boldsymbol{\mu}, \boldsymbol{\Sigma})$ in the PAC learning setting. Classically, in the advice-free setting, $\tilde{\Theta}(d^2/\varepsilon^2)$ samples are sufficient and worst case necessary to learn $d$-dimensional Gaussians up to TV distance $\varepsilon$ with constant probability. When we are additionally given a parameter $\tilde{\boldsymbol{\Sigma}}$ as advice, we show that $\tilde{O}(d^{2-\beta}/\varepsilon^2)$ samples suffices whenever $\| \tilde{\boldsymbol{\Sigma}}^{-1/2} \boldsymbol{\Sigma} \tilde{\boldsymbol{\Sigma}}^{-1/2} - \boldsymbol{I_d} \|_1 \leq \varepsilon d^{1-\beta}$ (where $\|\cdot\|_1$ denotes the entrywise $\ell_1$ norm) for any $\beta &gt; 0$, yielding a polynomial improvement over the advice-free setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12700v3</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arnab Bhattacharyya, Davin Choo, Philips George John, Themis Gouleakis</dc:creator>
    </item>
  </channel>
</rss>
