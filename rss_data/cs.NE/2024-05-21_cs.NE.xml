<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NE</link>
    <description>cs.NE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 21 May 2024 04:00:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 21 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Discovering Physics-Informed Neural Networks Model for Solving Partial Differential Equations through Evolutionary Computation</title>
      <link>https://arxiv.org/abs/2405.11208</link>
      <description>arXiv:2405.11208v1 Announce Type: new 
Abstract: In recent years, the researches about solving partial differential equations (PDEs) based on artificial neural network have attracted considerable attention. In these researches, the neural network models are usually designed depend on human experience or trial and error. Despite the emergence of several model searching methods, these methods primarily concentrate on optimizing the hyperparameters of fully connected neural network model based on the framework of physics-informed neural networks (PINNs), and the corresponding search spaces are relatively restricted, thereby limiting the exploration of superior models. This article proposes an evolutionary computation method aimed at discovering the PINNs model with higher approximation accuracy and faster convergence rate. In addition to searching the numbers of layers and neurons per hidden layer, this method concurrently explores the optimal shortcut connections between the layers and the novel parametric activation functions expressed by the binary trees. In evolution, the strategy about dynamic population size and training epochs (DPSTE) is adopted, which significantly increases the number of models to be explored and facilitates the discovery of models with fast convergence rate. In experiments, the performance of different models that are searched through Bayesian optimization, random search and evolution is compared in solving Klein-Gordon, Burgers, and Lam\'e equations. The experimental results affirm that the models discovered by the proposed evolutionary computation method generally exhibit superior approximation accuracy and convergence rate, and these models also show commendable generalization performance with respect to the source term, initial and boundary conditions, equation coefficient and computational domain. The corresponding code is available at https://github.com/MathBon/Discover-PINNs-Model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11208v1</guid>
      <category>cs.NE</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.swevo.2024.101589</arxiv:DOI>
      <arxiv:journal_reference>Swarm and Evolutionary Computation, 88, 2024, 101589</arxiv:journal_reference>
      <dc:creator>Bo Zhang, Chao Yang</dc:creator>
    </item>
    <item>
      <title>CMA-ES with Adaptive Reevaluation for Multiplicative Noise</title>
      <link>https://arxiv.org/abs/2405.11471</link>
      <description>arXiv:2405.11471v1 Announce Type: new 
Abstract: The covariance matrix adaptation evolution strategy (CMA-ES) is a powerful optimization method for continuous black-box optimization problems. Several noise-handling methods have been proposed to bring out the optimization performance of the CMA-ES on noisy objective functions. The adaptations of the population size and the learning rate are two major approaches that perform well under additive Gaussian noise. The reevaluation technique is another technique that evaluates each solution multiple times. In this paper, we discuss the difference between those methods from the perspective of stochastic relaxation that considers the maximization of the expected utility function. We derive that the set of maximizers of the noise-independent utility, which is used in the reevaluation technique, certainly contains the optimal solution, while the noise-dependent utility, which is used in the population size and leaning rate adaptations, does not satisfy it under multiplicative noise. Based on the discussion, we develop the reevaluation adaptation CMA-ES (RA-CMA-ES), which computes two update directions using half of the evaluations and adapts the number of reevaluations based on the estimated correlation of those two update directions. The numerical simulation shows that the RA-CMA-ES outperforms the comparative method under multiplicative noise, maintaining competitive performance under additive noise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11471v1</guid>
      <category>cs.NE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Kento Uchida, Kenta Nishihara, Shinichi Shirakawa</dc:creator>
    </item>
    <item>
      <title>Optimization of Worker Scheduling at Logistics Depots Using Genetic Algorithms and Simulated Annealing</title>
      <link>https://arxiv.org/abs/2405.11729</link>
      <description>arXiv:2405.11729v1 Announce Type: new 
Abstract: This paper addresses the optimization of scheduling for workers at a logistics depot using a combination of genetic algorithm and simulated annealing algorithm. The efficient scheduling of permanent and temporary workers is crucial for optimizing the efficiency of the logistics depot while minimizing labor usage. The study begins by establishing a 0-1 integer linear programming model, with decision variables determining the scheduling of permanent and temporary workers for each time slot on a given day. The objective function aims to minimize person-days, while constraints ensure fulfillment of hourly labor requirements, limit workers to one time slot per day, cap consecutive working days for permanent workers, and maintain non-negativity and integer constraints. The model is then solved using genetic algorithms and simulated annealing. Results indicate that, for this problem, genetic algorithms outperform simulated annealing in terms of solution quality. The optimal solution reveals a minimum of 29857 person-days.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11729v1</guid>
      <category>cs.NE</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinxin Xu, Haixin Wu, Yu Cheng, Liyang Wang, Xin Yang, Xintong Fu, Yuelong Su</dc:creator>
    </item>
    <item>
      <title>Quantifying Individual and Joint Module Impact in Modular Optimization Frameworks</title>
      <link>https://arxiv.org/abs/2405.11964</link>
      <description>arXiv:2405.11964v1 Announce Type: new 
Abstract: This study explores the influence of modules on the performance of modular optimization frameworks for continuous single-objective black-box optimization. There is an extensive variety of modules to choose from when designing algorithm variants, however, there is a rather limited understanding of how each module individually influences the algorithm performance and how the modules interact with each other when combined. We use the functional ANOVA (f-ANOVA) framework to quantify the influence of individual modules and module combinations for two algorithms, the modular Covariance Matrix Adaptation (modCMA) and the modular Differential Evolution (modDE). We analyze the performance data from 324 modCMA and 576 modDE variants on the BBOB benchmark collection, for two problem dimensions, and three computational budgets. Noteworthy findings include the identification of important modules that strongly influence the performance of modCMA, such as the~\textit{weights\ option} and~\textit{mirrored} modules for low dimensional problems, and the~\textit{base\ sampler} for high dimensional problems. The large individual influence of the~\textit{lpsr} module makes it very important for the performance of modDE, regardless of the problem dimensionality and the computational budget. When comparing modCMA and modDE, modDE undergoes a shift from individual modules being more influential, to module combinations being more influential, while modCMA follows the opposite pattern, with an increase in problem dimensionality and computational budget.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11964v1</guid>
      <category>cs.NE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ana Nikolikj, Ana Kostovska, Diederick Vermetten, Carola Doerr, Tome Eftimov</dc:creator>
    </item>
    <item>
      <title>Large Language Models for Tuning Evolution Strategies</title>
      <link>https://arxiv.org/abs/2405.10999</link>
      <description>arXiv:2405.10999v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) exhibit world knowledge and inference capabilities, making them powerful tools for various applications. This paper proposes a feedback loop mechanism that leverages these capabilities to tune Evolution Strategies (ES) parameters effectively. The mechanism involves a structured process of providing programming instructions, executing the corresponding code, and conducting thorough analysis. This process is specifically designed for the optimization of ES parameters. The method operates through an iterative cycle, ensuring continuous refinement of the ES parameters. First, LLMs process the instructions to generate or modify the code. The code is then executed, and the results are meticulously logged. Subsequent analysis of these results provides insights that drive further improvements. An experiment on tuning the learning rates of ES using the LLaMA3 model demonstrate the feasibility of this approach. This research illustrates how LLMs can be harnessed to improve ES algorithms' performance and suggests broader applications for similar feedback loop mechanisms in various domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10999v1</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>cs.NE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oliver Kramer</dc:creator>
    </item>
    <item>
      <title>On the Expressivity of Recurrent Neural Cascades with Identity</title>
      <link>https://arxiv.org/abs/2405.11657</link>
      <description>arXiv:2405.11657v1 Announce Type: cross 
Abstract: Recurrent Neural Cascades (RNC) are the class of recurrent neural networks with no cyclic dependencies among recurrent neurons. Their subclass RNC+ with positive recurrent weights has been shown to be closely connected to the star-free regular languages, which are the expressivity of many well-established temporal logics. The existing expressivity results show that the regular languages captured by RNC+ are the star-free ones, and they leave open the possibility that RNC+ may capture languages beyond regular. We exclude this possibility for languages that include an identity element, i.e., an input that can occur an arbitrary number of times without affecting the output. Namely, in the presence of an identity element, we show that the languages captured by RNC+ are exactly the star-free regular languages. Identity elements are ubiquitous in temporal patterns, and hence our results apply to a large number of applications. The implications of our results go beyond expressivity. At their core, we establish a close structural correspondence between RNC+ and semiautomata cascades, showing that every neuron can be equivalently captured by a three-state semiautomaton. A notable consequence of this result is that RNC+ are no more succinct than cascades of three-state semiautomata.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11657v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.FL</category>
      <category>cs.LO</category>
      <category>cs.NE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nadezda A. Knorozova, Alessandro Ronca</dc:creator>
    </item>
    <item>
      <title>Alternators For Sequence Modeling</title>
      <link>https://arxiv.org/abs/2405.11848</link>
      <description>arXiv:2405.11848v1 Announce Type: cross 
Abstract: This paper introduces alternators, a novel family of non-Markovian dynamical models for sequences. An alternator features two neural networks: the observation trajectory network (OTN) and the feature trajectory network (FTN). The OTN and the FTN work in conjunction, alternating between outputting samples in the observation space and some feature space, respectively, over a cycle. The parameters of the OTN and the FTN are not time-dependent and are learned via a minimum cross-entropy criterion over the trajectories. Alternators are versatile. They can be used as dynamical latent-variable generative models or as sequence-to-sequence predictors. When alternators are used as generative models, the FTN produces interpretable low-dimensional latent variables that capture the dynamics governing the observations. When alternators are used as sequence-to-sequence predictors, the FTN learns to predict the observed features. In both cases, the OTN learns to produce sequences that match the data. Alternators can uncover the latent dynamics underlying complex sequential data, accurately forecast and impute missing data, and sample new trajectories. We showcase the capabilities of alternators in three applications. We first used alternators to model the Lorenz equations, often used to describe chaotic behavior. We then applied alternators to Neuroscience, to map brain activity to physical activity. Finally, we applied alternators to Climate Science, focusing on sea-surface temperature forecasting. In all our experiments, we found alternators are stable to train, fast to sample from, yield high-quality generated samples and latent variables, and outperform strong baselines such as neural ODEs and diffusion models in the domains we studied.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11848v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>physics.ao-ph</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Reza Rezaei, Adji Bousso Dieng</dc:creator>
    </item>
    <item>
      <title>Trochoid Search Optimization</title>
      <link>https://arxiv.org/abs/2312.13597</link>
      <description>arXiv:2312.13597v2 Announce Type: replace 
Abstract: This paper introduces the Trochoid Search Optimization Algorithm (TSO), a novel metaheuristic leveraging the mathematical properties of trochoid curves. The TSO algorithm employs a unique combination of simultaneous translational and rotational motions inherent in trochoids, fostering a refined equilibrium between explorative and exploitative search capabilities. Notably, TSO consists of two pivotal phases global and local search that collectively contribute to its efficiency and efficacy. Experimental validation demonstrates the TSO algorithm's remarkable performance across various benchmark functions, showcasing its competitive edge in balancing exploration and exploitation within the search space. A distinguishing feature of TSO lies in its simplicity, marked by a minimal requirement for user-defined parameters, making it an accessible yet powerful optimization tool.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.13597v2</guid>
      <category>cs.NE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abdesslem Layeb</dc:creator>
    </item>
    <item>
      <title>Large Language Models as Hyper-Heuristics for Combinatorial Optimization</title>
      <link>https://arxiv.org/abs/2402.01145</link>
      <description>arXiv:2402.01145v2 Announce Type: replace 
Abstract: The omnipresence of NP-hard combinatorial optimization problems (COPs) compels domain experts to engage in trial-and-error heuristic design. The long-standing endeavor of design automation has gained new momentum with the rise of large language models (LLMs). This paper introduces Language Hyper-Heuristics (LHHs), an emerging variant of Hyper-Heuristics that leverages LLMs for heuristic generation, featuring minimal manual intervention and open-ended heuristic spaces. To empower LHHs, we present Reflective Evolution (ReEvo), a novel integration of evolutionary search for efficiently exploring the heuristic space, and LLM reflections to provide verbal gradients within the space. Across five heterogeneous algorithmic types, six different COPs, and both white-box and black-box views of COPs, ReEvo yields state-of-the-art and competitive meta-heuristics, evolutionary algorithms, heuristics, and neural solvers, while being more sample-efficient than prior LHHs. Our code is available: https://github.com/ai4co/LLM-as-HH.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.01145v2</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoran Ye, Jiarui Wang, Zhiguang Cao, Federico Berto, Chuanbo Hua, Haeyeon Kim, Jinkyoo Park, Guojie Song</dc:creator>
    </item>
    <item>
      <title>Artificial Cardiac Conduction System: Simulating Heart Function for Advanced Computational Problem Solving</title>
      <link>https://arxiv.org/abs/2404.02907</link>
      <description>arXiv:2404.02907v2 Announce Type: replace 
Abstract: This work proposes a novel bio-inspired metaheuristic called Artificial Cardiac Conduction System (ACCS) inspired by the human cardiac conduction system. The ACCS algorithm imitates the functional behaviour of the human heart that generates and sends signals to the heart muscle, initiating it to contract. Four nodes in the myocardium layer participate in generating and controlling heart rate, such as the sinoatrial, atrioventricular, bundle of His, and Purkinje fibres. The mechanism of controlling the heart rate through these four nodes is implemented. The algorithm is then benchmarked on 19 well-known mathematical test functions as it can determine the exploitation and exploration capability of the algorithm. The results are verified by a comparative study with Whale Optimization Algorithm (WOA), Particle Swarm Optimization (PSO), Gravitational Search Algorithm (GSA), Differential Evolution (DE), and Fast Evolutionary Programming (FEP). The algorithm undergoes a rigorous evaluation using the CEC-C06 2019 Benchmark Test Functions, illuminating its adeptness in both exploitation and exploration. Validation ensues through a meticulous comparative analysis involving the Dragonfly Algorithm (DA), WOA, PSO, Lagrange Elementary Optimization (Leo), and the Ant Nesting Algorithm (ANA). The results show that the ACCS algorithm can provide very competitive results compared to these well-known metaheuristics and other conventional methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02907v2</guid>
      <category>cs.NE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rebaz Mohammed Dler Omer, Nawzad K. Al-Salihi, Tarik A. Rashid, Aso M. Aladdin, Mokhtar Mohammadi, Jafar Majidpour</dc:creator>
    </item>
    <item>
      <title>Dynamics of specialization in neural modules under resource constraints</title>
      <link>https://arxiv.org/abs/2106.02626</link>
      <description>arXiv:2106.02626v5 Announce Type: replace-cross 
Abstract: It has long been believed that the brain is highly modular both in terms of structure and function, although recent evidence has led some to question the extent of both types of modularity. We used artificial neural networks to test the hypothesis that structural modularity is sufficient to guarantee functional specialization, and find that in general, this doesn't necessarily hold. We then systematically tested which features of the environment and network do lead to the emergence of specialization. We used a simple toy environment, task and network, allowing us precise control, and show that in this setup, several distinct measures of specialization give qualitatively similar results. We further find that in this setup (1) specialization can only emerge in environments where features of that environment are meaningfully separable, (2) specialization preferentially emerges when the network is strongly resource-constrained, and (3) these findings are qualitatively similar across the different variations of network architectures that we tested, but that the quantitative relationships depend on the precise architecture. Finally, we show that functional specialization varies dynamically across time, and demonstrate that these dynamics depend on both the timing and bandwidth of information flow in the network. We conclude that a static notion of specialization, based on structural modularity, is likely too simple a framework for understanding intelligence in situations of real-world complexity, from biology to brain-inspired neuromorphic systems. We propose that thoroughly stress testing candidate definitions of functional modularity in simplified scenarios before extending to more complex data, network models and electrophysiological recordings is likely to be a fruitful approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2106.02626v5</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriel B\'ena, Dan F. M. Goodman</dc:creator>
    </item>
    <item>
      <title>A Novel Convolutional Neural Network Architecture with a Continuous Symmetry</title>
      <link>https://arxiv.org/abs/2308.01621</link>
      <description>arXiv:2308.01621v4 Announce Type: replace-cross 
Abstract: This paper introduces a new Convolutional Neural Network (ConvNet) architecture inspired by a class of partial differential equations (PDEs) called quasi-linear hyperbolic systems. With comparable performance on the image classification task, it allows for the modification of the weights via a continuous group of symmetry. This is a significant shift from traditional models where the architecture and weights are essentially fixed. We wish to promote the (internal) symmetry as a new desirable property for a neural network, and to draw attention to the PDE perspective in analyzing and interpreting ConvNets in the broader Deep Learning community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.01621v4</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Yao Liu, Hang Shao, Bing Bai</dc:creator>
    </item>
    <item>
      <title>Engineered Ordinary Differential Equations as Classification Algorithm (EODECA): thorough characterization and testing</title>
      <link>https://arxiv.org/abs/2312.14681</link>
      <description>arXiv:2312.14681v2 Announce Type: replace-cross 
Abstract: EODECA (Engineered Ordinary Differential Equations as Classification Algorithm) is a novel approach at the intersection of machine learning and dynamical systems theory, presenting a unique framework for classification tasks [1]. This method stands out with its dynamical system structure, utilizing ordinary differential equations (ODEs) to efficiently handle complex classification challenges. The paper delves into EODECA's dynamical properties, emphasizing its resilience against random perturbations and robust performance across various classification scenarios. Notably, EODECA's design incorporates the ability to embed stable attractors in the phase space, enhancing reliability and allowing for reversible dynamics. In this paper, we carry out a comprehensive analysis by expanding on the work [1], and employing a Euler discretization scheme. In particular, we evaluate EODECA's performance across five distinct classification problems, examining its adaptability and efficiency. Significantly, we demonstrate EODECA's effectiveness on the MNIST and Fashion MNIST datasets, achieving impressive accuracies of $98.06\%$ and $88.21\%$, respectively. These results are comparable to those of a multi-layer perceptron (MLP), underscoring EODECA's potential in complex data processing tasks. We further explore the model's learning journey, assessing its evolution in both pre and post training environments and highlighting its ability to navigate towards stable attractors. The study also investigates the invertibility of EODECA, shedding light on its decision-making processes and internal workings. This paper presents a significant step towards a more transparent and robust machine learning paradigm, bridging the gap between machine learning algorithms and dynamical systems methodologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14681v2</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <category>nlin.PS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Raffaele Marino, Lorenzo Buffoni, Lorenzo Chicchi, Lorenzo Giambagli, Duccio Fanelli</dc:creator>
    </item>
    <item>
      <title>Cluster-Based Normalization Layer for Neural Networks</title>
      <link>https://arxiv.org/abs/2403.16798</link>
      <description>arXiv:2403.16798v2 Announce Type: replace-cross 
Abstract: Deep learning grapples with challenges in training neural networks, notably internal covariate shift and label shift. Conventional normalization techniques like Batch Normalization (BN) partially mitigate these issues but are hindered by constraints such as dependency on batch size and distribution assumptions. Similarly, mixture normalization (MN) encounters computational barriers in handling diverse Gaussian distributions. This paper introduces Cluster-based Normalization (CB-Norm), presenting two variants: Supervised Cluster-based Normalization (SCB-Norm) and Unsupervised Cluster-based Normalization (UCB-Norm), offering a pioneering single-step normalization strategy. CB-Norm employs a Gaussian mixture model to address gradient stability and learning acceleration challenges. SCB-Norm utilizes predefined data partitioning, termed clusters, for supervised normalization, while UCB-Norm adaptively clusters neuron activations during training, eliminating reliance on predefined partitions. This approach simultaneously tackles clustering and resolution tasks within neural networks, reducing computational complexity compared to existing methods. CB-Norm outperforms traditional techniques like BN and MN, enhancing neural network performance across diverse learning scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16798v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bilal Faye, Hanane Azzag, Mustapha Lebbah</dc:creator>
    </item>
  </channel>
</rss>
