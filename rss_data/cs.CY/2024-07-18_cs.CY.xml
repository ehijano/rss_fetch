<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 19 Jul 2024 04:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 19 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>AI Agents and Education: Simulated Practice at Scale</title>
      <link>https://arxiv.org/abs/2407.12796</link>
      <description>arXiv:2407.12796v1 Announce Type: new 
Abstract: This paper explores the potential of generative AI in creating adaptive educational simulations. By leveraging a system of multiple AI agents, simulations can provide personalized learning experiences, offering students the opportunity to practice skills in scenarios with AI-generated mentors, role-players, and instructor-facing evaluators. We describe a prototype, PitchQuest, a venture capital pitching simulator that showcases the capabilities of AI in delivering instruction, facilitating practice, and providing tailored feedback. The paper discusses the pedagogy behind the simulation, the technology powering it, and the ethical considerations in using AI for education. While acknowledging the limitations and need for rigorous testing, we propose that generative AI can significantly lower the barriers to creating effective, engaging simulations, opening up new possibilities for experiential learning at scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12796v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ethan Mollick, Lilach Mollick, Natalie Bach, LJ Ciccarelli, Ben Przystanski, Daniel Ravipinto</dc:creator>
    </item>
    <item>
      <title>Evaluation of LLMs Biases Towards Elite Universities: A Persona-Based Exploration</title>
      <link>https://arxiv.org/abs/2407.12801</link>
      <description>arXiv:2407.12801v1 Announce Type: new 
Abstract: Elite universities are a dream destination for not just students but also top employers who get a supply of amazing talents. When we hear about top universities, the first thing that comes to mind is their academic rigor, prestigious reputation, and highly successful alumni. However, society at large is not just represented by a few elite universities, but several others. We have seen several examples where many, even without formal education, built big businesses. There are various instances in which several people, however talented, couldn't make it to top elite universities because of several resource constraints. For recruitment of candidates, we do see candidates from a few elite universities well represented in top technology companies. However, we found during our study that LLMs go overboard in representing that. Why is it a problem, though? LLMs are now becoming mainstream and may play a role in evaluating candidates' relevance in the recruitment process across industries. Our study investigates whether LLMs are biased toward Elite universities like Stanford University, Harvard University, University of California, Berkley, and MIT. Our research compares the performance of three popular large language models by adopting a novel persona-based approach and compares the predicted educational backgrounds of professionals in the technology industry with actual data collected from LinkedIn. Specifically, we examined GPT-3.5, Gemini, and Claude 3 Sonnet predictions for job positions such as VP Product, Director of Product, Product Manager, VP Engineering, Director of Engineering, and Software Engineer at Microsoft, Meta, and Google. We noticed biases in LLMs' prediction of educational backgrounds. We are confident that our research will propel the study of LLM biases and our suggested strategies could mitigate biases in LLM-based use cases and applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12801v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shailja Gupta, Rajesh Ranjan</dc:creator>
    </item>
    <item>
      <title>A Survey of Scam Exposure, Victimization, Types, Vectors, and Reporting in 12 Countries</title>
      <link>https://arxiv.org/abs/2407.12896</link>
      <description>arXiv:2407.12896v1 Announce Type: new 
Abstract: Scams are a widespread issue with severe consequences for both victims and perpetrators, but existing data collection is fragmented, precluding global and comparative local understanding. The present study addresses this gap through a nationally representative survey (n = 8,369) on scam exposure, victimization, types, vectors, and reporting in 12 countries: Belgium, Egypt, France, Hungary, Indonesia, Mexico, Romania, Slovakia, South Africa, South Korea, Sweden, and the United Kingdom. We analyze 6 survey questions to build a detailed quantitative picture of the scams landscape in each country, and compare across countries to identify global patterns. We find, first, that residents of less affluent countries suffer financial loss from scams more often. Second, we find that the internet plays a key role in scams across the globe, and that GNI per-capita is strongly associated with specific scam types and contact vectors. Third, we find widespread under-reporting, with residents of less affluent countries being less likely to know how to report a scam. Our findings contribute valuable insights for researchers, practitioners, and policymakers in the online fraud and scam prevention space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12896v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mo Houtti, Abhishek Roy, Venkata Narsi Reddy Gangula, Ashley Marie Walker</dc:creator>
    </item>
    <item>
      <title>Securing the Future of GenAI: Policy and Technology</title>
      <link>https://arxiv.org/abs/2407.12999</link>
      <description>arXiv:2407.12999v1 Announce Type: new 
Abstract: The rise of Generative AI (GenAI) brings about transformative potential across sectors, but its dual-use nature also amplifies risks. Governments globally are grappling with the challenge of regulating GenAI, balancing innovation against safety. China, the United States (US), and the European Union (EU) are at the forefront with initiatives like the Management of Algorithmic Recommendations, the Executive Order, and the AI Act, respectively. However, the rapid evolution of GenAI capabilities often outpaces the development of comprehensive safety measures, creating a gap between regulatory needs and technical advancements.
  A workshop co-organized by Google, University of Wisconsin, Madison (UW-Madison), and Stanford University aimed to bridge this gap between GenAI policy and technology. The diverse stakeholders of the GenAI space -- from the public and governments to academia and industry -- make any safety measures under consideration more complex, as both technical feasibility and regulatory guidance must be realized. This paper summarizes the discussions during the workshop which addressed questions, such as: How regulation can be designed without hindering technological progress? How technology can evolve to meet regulatory standards? The interplay between legislation and technology is a very vast topic, and we don't claim that this paper is a comprehensive treatment on this topic. This paper is meant to capture findings based on the workshop, and hopefully, can guide discussion on this topic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12999v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mihai Christodorescu, Ryan Craven, Soheil Feizi, Neil Gong, Mia Hoffmann, Somesh Jha, Zhengyuan Jiang, Mehrdad Saberi Kamarposhti, John Mitchell, Jessica Newman, Emelia Probasco, Yanjun Qi, Khawaja Shams, Matthew Turek</dc:creator>
    </item>
    <item>
      <title>FernUni LLM Experimental Infrastructure (FLEXI) -- Enabling Experimentation and Innovation in Higher Education Through Access to Open Large Language Models</title>
      <link>https://arxiv.org/abs/2407.13013</link>
      <description>arXiv:2407.13013v1 Announce Type: new 
Abstract: Using the full potential of LLMs in higher education is hindered by challenges with access to LLMs. The two main access modes currently discussed are paying for a cloud-based LLM or providing a locally maintained open LLM. In this paper, we describe the current state of establishing an open LLM infrastructure at FernUniversit\"at in Hagen under the project name FLEXI (FernUni LLM Experimental Infrastructure). FLEXI enables experimentation within teaching and research with the goal of generating strongly needed evidence in favor (or against) the use of locally maintained open LLMs in higher education. The paper will provide some practical guidance for everyone trying to decide whether to run their own LLM server.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13013v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Torsten Zesch, Michael Hanses, Niels Seidel, Piush Aggarwal, Dirk Veiel, Claudia de Witt</dc:creator>
    </item>
    <item>
      <title>From Principles to Practices: Lessons Learned from Applying Partnership on AI's (PAI) Synthetic Media Framework to 11 Use Cases</title>
      <link>https://arxiv.org/abs/2407.13025</link>
      <description>arXiv:2407.13025v1 Announce Type: new 
Abstract: 2023 was the year the world woke up to generative AI, and 2024 is the year policymakers are responding more firmly. Importantly, this policy momentum is taking place alongside real world creation and distribution of synthetic media. Social media platforms, news organizations, dating apps, image generation companies, and more are already navigating a world of AI-generated visuals and sounds, already changing hearts and minds, as policymakers try to catch up. How, then, can AI governance capture the complexity of the synthetic media landscape? How can it attend to synthetic media's myriad uses, ranging from storytelling to privacy preservation, to deception, fraud, and defamation, taking into account the many stakeholders involved in its development, creation, and distribution? And what might it mean to govern synthetic media in a manner that upholds the truth while bolstering freedom of expression? What follows is the first known collection of diverse examples of the implementation of synthetic media governance that responds to these questions, specifically through Partnership on AI's (PAI) Responsible Practices for Synthetic Media - a voluntary, normative Framework for creating, distributing, and building technology for synthetic media responsibly, launched in February 2023. In this paper, we present a case bank of real world examples that help operationalize the Framework - highlighting areas synthetic media governance can be applied, augmented, expanded, and refined for use, in practice. Read together, the cases emphasize distinct elements of AI policymaking and seven emergent best practices supporting transparency, safety, expression, and digital dignity online: consent, disclosure, and differentiation between harmful and creative use cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13025v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Claire R. Leibowicz (Partnership on AI), Christian H. Cardona (Partnership on AI)</dc:creator>
    </item>
    <item>
      <title>Decolonial AI as Disenclosure</title>
      <link>https://arxiv.org/abs/2407.13050</link>
      <description>arXiv:2407.13050v1 Announce Type: new 
Abstract: The development and deployment of machine learning and AI engender 'AI colonialism', a term that conceptually overlaps with 'data colonialism', as a form of injustice. AI colonialism is in need of decolonization for three reasons. Politically, because it enforces digital capitalism's hegemony. Ecologically, as it negatively impacts the environment and intensifies the extraction of natural resources and consumption of energy. Epistemically, since the social systems within which AI is embedded reinforce Western universalism by imposing Western colonial values on the global South when these manifest in the digital realm is a form of digital capitalism. These reasons require a new conceptualization of AI decolonization. First this paper draws from the historical debates on the concepts of colonialism and decolonization. Secondly it retrieves Achille Mbembe's notion of decolonization as disenclosure to argue that the decolonization of AI will have to be the abolishment of political, ecological and epistemic borders erected and reinforced in the phases of its design, production, development of AI in the West and drawing from the knowledge from the global South. In conclusion, it is discussed how conceiving of decolonial AI as form of disenclosure opens up new ways to think about and intervene in colonial instantiations of AI development and deployment, in order to empower 'the wretched of AI', re-ecologise the unsustainable ecologies AI depends on and to counter the colonial power structures unreflective AI deployment risks to reinforce.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13050v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4236/jss.2024.122032</arxiv:DOI>
      <arxiv:journal_reference>Open Journal of Social Sciences, 12, 574-603 (2024)</arxiv:journal_reference>
      <dc:creator>W. J. T. Mollema</dc:creator>
    </item>
    <item>
      <title>Matchings, Predictions and Counterfactual Harm in Refugee Resettlement Processes</title>
      <link>https://arxiv.org/abs/2407.13052</link>
      <description>arXiv:2407.13052v1 Announce Type: new 
Abstract: Resettlement agencies have started to adopt data-driven algorithmic matching to match refugees to locations using employment rate as a measure of utility. Given a pool of refugees, data-driven algorithmic matching utilizes a classifier to predict the probability that each refugee would find employment at any given location. Then, it uses the predicted probabilities to estimate the expected utility of all possible placement decisions. Finally, it finds the placement decisions that maximize the predicted utility by solving a maximum weight bipartite matching problem. In this work, we argue that, using existing solutions, there may be pools of refugees for which data-driven algorithmic matching is (counterfactually) harmful -- it would have achieved lower utility than a given default policy used in the past, had it been used. Then, we develop a post-processing algorithm that, given placement decisions made by a default policy on a pool of refugees and their employment outcomes, solves an inverse~matching problem to minimally modify the predictions made by a given classifier. Under these modified predictions, the optimal matching policy that maximizes predicted utility on the pool is guaranteed to be not harmful. Further, we introduce a Transformer model that, given placement decisions made by a default policy on multiple pools of refugees and their employment outcomes, learns to modify the predictions made by a classifier so that the optimal matching policy that maximizes predicted utility under the modified predictions on an unseen pool of refugees is less likely to be harmful than under the original predictions. Experiments on simulated resettlement processes using synthetic refugee data created from a variety of publicly available data suggest that our methodology may be effective in making algorithmic placement decisions that are less likely to be harmful than existing solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13052v1</guid>
      <category>cs.CY</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seungeon Lee, Nina Corvelo Benz, Suhas Thejaswi, Manuel Gomez-Rodriguez</dc:creator>
    </item>
    <item>
      <title>E2Vec: Feature Embedding with Temporal Information for Analyzing Student Actions in E-Book Systems</title>
      <link>https://arxiv.org/abs/2407.13053</link>
      <description>arXiv:2407.13053v1 Announce Type: new 
Abstract: Digital textbook (e-book) systems record student interactions with textbooks as a sequence of events called EventStream data. In the past, researchers extracted meaningful features from EventStream, and utilized them as inputs for downstream tasks such as grade prediction and modeling of student behavior. Previous research evaluated models that mainly used statistical-based features derived from EventStream logs, such as the number of operation types or access frequencies. While these features are useful for providing certain insights, they lack temporal information that captures fine-grained differences in learning behaviors among different students. This study proposes E2Vec, a novel feature representation method based on word embeddings. The proposed method regards operation logs and their time intervals for each student as a string sequence of characters and generates a student vector of learning activity features that incorporates time information. We applied fastText to generate an embedding vector for each of 305 students in a dataset from two years of computer science courses. Then, we investigated the effectiveness of E2Vec in an at-risk detection task, demonstrating potential for generalizability and performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13053v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuma Miyazaki, Valdemar \v{S}v\'abensk\'y, Yuta Taniguchi, Fumiya Okubo, Tsubasa Minematsu, Atsushi Shimada</dc:creator>
    </item>
    <item>
      <title>Prioritizing High-Consequence Biological Capabilities in Evaluations of Artificial Intelligence Models</title>
      <link>https://arxiv.org/abs/2407.13059</link>
      <description>arXiv:2407.13059v1 Announce Type: new 
Abstract: As a result of rapidly accelerating AI capabilities, over the past year, national governments and multinational bodies have announced efforts to address safety, security and ethics issues related to AI models. One high priority among these efforts is the mitigation of misuse of AI models. Many biologists have for decades sought to reduce the risks of scientific research that could lead, through accident or misuse, to high-consequence disease outbreaks. Scientists have carefully considered what types of life sciences research have the potential for both benefit and risk (dual-use), especially as scientific advances have accelerated our ability to engineer organisms and create novel variants of pathogens. Here we describe how previous experience and study by scientists and policy professionals of dual-use capabilities in the life sciences can inform risk evaluations of AI models with biological capabilities. We argue that AI model evaluations should prioritize addressing high-consequence risks (those that could cause large-scale harm to the public, such as pandemics), and that these risks should be evaluated prior to model deployment so as to allow potential biosafety and/or biosecurity measures. Scientists' experience with identifying and mitigating dual-use biological risks can help inform new approaches to evaluating biological AI models. Identifying which AI capabilities post the greatest biosecurity and biosafety concerns is necessary in order to establish targeted AI safety evaluation methods, secure these tools against accident and misuse, and avoid impeding immense potential benefits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13059v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jaspreet Pannu, Doni Bloomfield, Alex Zhu, Robert MacKnight, Gabe Gomes, Anita Cicero, Thomas V. Inglesby</dc:creator>
    </item>
    <item>
      <title>Use of Boosting Algorithms in Household-Level Poverty Measurement: A Machine Learning Approach to Predict and Classify Household Wealth Quintiles in the Philippines</title>
      <link>https://arxiv.org/abs/2407.13061</link>
      <description>arXiv:2407.13061v1 Announce Type: new 
Abstract: This study assessed the effectiveness of machine learning models in predicting poverty levels in the Philippines using five boosting algorithms: Adaptive Boosting (AdaBoost), CatBoosting (CatBoost), Gradient Boosting Machine (GBM), Light Gradient Boosting Machine (LightGBM), and Extreme Gradient Boosting (XGBoost). CatBoost emerged as the superior model and achieved the highest scores across accuracy, precision, recall, and F1-score at 91 percent, while XGBoost and GBM followed closely with 89 percent and 88 percent respectively. Additionally, the research examined the computational efficiency of these models to analyze the balance between training time, testing speed, and model size factors crucial for real-world applications. Despite its longer training duration, CatBoost demonstrated high testing efficiency. These results indicate that machine learning can aid in poverty prediction and in the development of targeted policy interventions. Future studies should focus on incorporating a wider variety of data to enhance the predictive accuracy and policy utility of these models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13061v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Erika Lynet Salvador</dc:creator>
    </item>
    <item>
      <title>A Gentle Approach to Multi-Sensor Fusion Data Using Linear Kalman Filter</title>
      <link>https://arxiv.org/abs/2407.13062</link>
      <description>arXiv:2407.13062v1 Announce Type: new 
Abstract: This research paper delves into the Linear Kalman Filter (LKF), highlighting its importance in merging data from multiple sensors. The Kalman Filter is known for its recursive solution to the linear filtering problem in discrete data, making it ideal for estimating states in dynamic systems by reducing noise in measurements and processes. Our focus is on linear dynamic systems due to the LKF's assumptions about system dynamics, measurement noise, and initial conditions. We thoroughly explain the principles, assumptions, and mechanisms of the LKF, emphasizing its practical application in multi-sensor data fusion. This fusion is essential for integrating diverse sensory inputs, thereby improving the accuracy and reliability of state estimations. To illustrate the LKF's real-world applicability and versatility, the paper presents two physical examples where the LKF significantly enhances precision and stability in dynamic systems. These examples not only demonstrate the theoretical concepts but also provide practical insights into implementing the LKF in multi-sensor data fusion scenarios. Our discussion underscores the LKF's crucial role in fields such as robotics, navigation, and signal processing. By combining an in-depth exploration of the LKF's theoretical foundations with practical examples, this paper aims to provide a comprehensive and accessible understanding of multi-sensor data fusion. Our goal is to contribute to the growing body of knowledge in this important area of research, promoting further innovations and advancements in data fusion technologies and encouraging their wider adoption across various scientific and industrial fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13062v1</guid>
      <category>cs.CY</category>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Parsa Veysi, Mohsen Adeli, Nayerosadat Peirov Naziri, Ehsan Adeli</dc:creator>
    </item>
    <item>
      <title>The Cost of Arbitrariness for Individuals: Examining the Legal and Technical Challenges of Model Multiplicity</title>
      <link>https://arxiv.org/abs/2407.13070</link>
      <description>arXiv:2407.13070v1 Announce Type: new 
Abstract: Model multiplicity, the phenomenon where multiple models achieve similar performance despite different underlying learned functions, introduces arbitrariness in model selection. While this arbitrariness may seem inconsequential in expectation, its impact on individuals can be severe. This paper explores various individual concerns stemming from multiplicity, including the effects of arbitrariness beyond final predictions, disparate arbitrariness for individuals belonging to protected groups, and the challenges associated with the arbitrariness of a single algorithmic system creating a monopoly across various contexts. It provides both an empirical examination of these concerns and a comprehensive analysis from the legal standpoint, addressing how these issues are perceived in the anti-discrimination law in Canada. We conclude the discussion with technical challenges in the current landscape of model multiplicity to meet legal requirements and the legal gap between current law and the implications of arbitrariness in model selection, highlighting relevant future research directions for both disciplines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13070v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Prakhar Ganesh, Ihsan Ibrahim Daldaban, Ignacio Cofone, Golnoosh Farnadi</dc:creator>
    </item>
    <item>
      <title>Analysing the Public Discourse around OpenAI's Text-To-Video Model 'Sora' using Topic Modeling</title>
      <link>https://arxiv.org/abs/2407.13071</link>
      <description>arXiv:2407.13071v1 Announce Type: new 
Abstract: The recent introduction of OpenAI's text-to-video model Sora has sparked widespread public discourse across online communities. This study aims to uncover the dominant themes and narratives surrounding Sora by conducting topic modeling analysis on a corpus of 1,827 Reddit comments from five relevant subreddits (r/OpenAI, r/technology, r/singularity, r/vfx, and r/ChatGPT). The comments were collected over a two-month period following Sora's announcement in February 2024. After preprocessing the data, Latent Dirichlet Allocation (LDA) was employed to extract four key topics: 1) AI Impact and Trends in Sora Discussions, 2) Public Opinion and Concerns about Sora, 3) Artistic Expression and Video Creation with Sora, and 4) Sora's Applications in Media and Entertainment. Visualizations including word clouds, bar charts, and t-SNE clustering provided insights into the importance of topic keywords and the distribution of comments across topics. The results highlight prominent narratives around Sora's potential impact on industries and employment, public sentiment and ethical concerns, creative applications, and use cases in the media and entertainment sectors. While limited to Reddit data within a specific timeframe, this study offers a framework for understanding public perceptions of emerging generative AI technologies through online discourse analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13071v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Vatsal Vinay Parikh</dc:creator>
    </item>
    <item>
      <title>Accuracy of training data and model outputs in Generative AI: CREATe Response to the Information Commissioner Office Consultation</title>
      <link>https://arxiv.org/abs/2407.13072</link>
      <description>arXiv:2407.13072v1 Announce Type: new 
Abstract: The accuracy of Generative AI is increasingly critical as Large Language Models become more widely adopted. Due to potential flaws in training data and hallucination in outputs, inaccuracy can significantly impact individuals interests by distorting perceptions and leading to decisions based on flawed information. Therefore, ensuring these models accuracy is not only a technical necessity but also a regulatory imperative. ICO call for evidence on the accuracy of Generative AI marks a timely effort in ensuring responsible Generative AI development and use.
  CREATe, as the Centre for Regulation of the Creative Economy based at the University of Glasgow, has conducted relevant research involving intellectual property, competition, information and technology law. We welcome the ICO call for evidence on the accuracy of Generative AI, and we are happy to highlight aspects of data protection law and AI regulation that we believe should receive attention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13072v1</guid>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zihao Li, Weiwei Yi, Jiahong Chen</dc:creator>
    </item>
    <item>
      <title>Visions of a Discipline: Analyzing Introductory AI Courses on YouTube</title>
      <link>https://arxiv.org/abs/2407.13077</link>
      <description>arXiv:2407.13077v1 Announce Type: new 
Abstract: Education plays an indispensable role in fostering societal well-being and is widely regarded as one of the most influential factors in shaping the future of generations to come. As artificial intelligence (AI) becomes more deeply integrated into our daily lives and the workforce, educational institutions at all levels are directing their focus on resources that cater to AI education. Our work investigates the current landscape of introductory AI courses on YouTube, and the potential for introducing ethics in this context. We qualitatively analyze the 20 most watched introductory AI courses on YouTube, coding a total of 92.2 hours of educational content viewed by close to 50 million people. Introductory AI courses do not meaningfully engage with ethical or societal challenges of AI (RQ1). When \textit{defining and framing AI}, introductory AI courses foreground excitement around AI's transformative role in society, over-exaggerate AI's current and future abilities, and anthropomorphize AI (RQ2). In \textit{teaching AI}, we see a widespread reliance on corporate AI tools and frameworks as well as a prioritization on a hands-on approach to learning rather than on conceptual foundations (RQ3). In promoting key \textit{AI practices}, introductory AI courses abstract away entirely the socio-technical nature of AI classification and prediction, for example by favoring data quantity over data quality (RQ4). We extend our analysis with recommendations that aim to integrate ethical reflections into introductory AI courses. We recommend that introductory AI courses should (1) highlight ethical challenges of AI to present a more balanced perspective, (2) raise ethical issues explicitly relevant to the technical concepts discussed and (3) nurture a sense of accountability in future AI developers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13077v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Severin Engelmann, Madiha Zahrah Choksi, Angelina Wang, Casey Fiesler</dc:creator>
    </item>
    <item>
      <title>Participatory Approaches in AI Development and Governance: A Principled Approach</title>
      <link>https://arxiv.org/abs/2407.13100</link>
      <description>arXiv:2407.13100v1 Announce Type: new 
Abstract: The widespread adoption of Artificial Intelligence (AI) technologies in the public and private sectors has resulted in them significantly impacting the lives of people in new and unexpected ways. In this context, it becomes important to inquire how their design, development and deployment takes place. Upon this inquiry, it is seen that persons who will be impacted by the deployment of these systems have little to no say in how they are developed. Seeing this as a lacuna, this research study advances the premise that a participatory approach is beneficial (both practically and normatively) to building and using more responsible, safe, and human-centric AI systems. Normatively, it enhances the fairness of the process and empowers citizens in voicing concerns to systems that may heavily impact their lives. Practically, it provides developers with new avenues of information which will be beneficial to them in improving the quality of the AI algorithm. The paper advances this argument first, by describing the life cycle of an AI system; second, by identifying criteria which may be used to identify relevant stakeholders for a participatory exercise; and third, by mapping relevant stakeholders to different stages of AI lifecycle. This paper forms the first part of a two-part series on participatory governance in AI. The second paper will expand upon and concretise the principles developed in this paper and apply the same to actual use cases of AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13100v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ambreesh Parthasarathy, Aditya Phalnikar, Ameen Jauhar, Dhruv Somayajula, Gokul S Krishnan, Balaraman Ravindran</dc:creator>
    </item>
    <item>
      <title>Participatory Approaches in AI Development and Governance: Case Studies</title>
      <link>https://arxiv.org/abs/2407.13103</link>
      <description>arXiv:2407.13103v1 Announce Type: new 
Abstract: This paper forms the second of a two-part series on the value of a participatory approach to AI development and deployment. The first paper had crafted a principled, as well as pragmatic, justification for deploying participatory methods in these two exercises (that is, development and deployment of AI). The pragmatic justification is that it improves the quality of the overall algorithm by providing more granular and minute information. The more principled justification is that it offers a voice to those who are going to be affected by the deployment of the algorithm, and through engagement attempts to build trust and buy-in for an AI system. By a participatory approach, we mean including various stakeholders (defined a certain way) in the actual decision making process through the life cycle of an AI system. Despite the justifications offered above, actual implementation depends crucially on how stakeholders in the entire process are identified, what information is elicited from them, and how it is incorporated. This paper will test these preliminary conclusions in two sectors, the use of facial recognition technology in the upkeep of law and order and the use of large language models in the healthcare sector. These sectors have been chosen for two primary reasons. Since Facial Recognition Technologies are a branch of AI solutions that are well-researched and the impact of which is well documented, it provides an established space to illustrate the various aspects of adapting PAI to an existing domain, especially one that has been quite contentious in the recent past. LLMs in healthcare provide a canvas for a relatively less explored space, and helps us illustrate how one could possibly envision enshrining the principles of PAI for a relatively new technology, in a space where innovation must always align with patient welfare.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13103v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ambreesh Parthasarathy, Aditya Phalnikar, Gokul S Krishnan, Ameen Jauhar, Balaraman Ravindran</dc:creator>
    </item>
    <item>
      <title>Survey on Plagiarism Detection in Large Language Models: The Impact of ChatGPT and Gemini on Academic Integrity</title>
      <link>https://arxiv.org/abs/2407.13105</link>
      <description>arXiv:2407.13105v1 Announce Type: new 
Abstract: The rise of Large Language Models (LLMs) such as ChatGPT and Gemini has posed new challenges for the academic community. With the help of these models, students can easily complete their assignments and exams, while educators struggle to detect AI-generated content. This has led to a surge in academic misconduct, as students present work generated by LLMs as their own, without putting in the effort required for learning. As AI tools become more advanced and produce increasingly human-like text, detecting such content becomes more challenging. This development has significantly impacted the academic world, where many educators are finding it difficult to adapt their assessment methods to this challenge.
  This research first demonstrates how LLMs have increased academic dishonesty, and then reviews state-of-the-art solutions for academic plagiarism in detail. A survey of datasets, algorithms, tools, and evasion strategies for plagiarism detection has been conducted, focusing on how LLMs and AI-generated content (AIGC) detection have affected this area. The survey aims to identify the gaps in existing solutions. Lastly, potential long-term solutions are presented to address the issue of academic plagiarism using LLMs based on AI tools and educational approaches in an ever-changing world.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13105v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shushanta Pudasaini, Luis Miralles-Pechu\'an, David Lillis, Marisa Llorens Salvador</dc:creator>
    </item>
    <item>
      <title>A Framework for Spatio-Temporal Graph Analytics In Field Sports</title>
      <link>https://arxiv.org/abs/2407.13109</link>
      <description>arXiv:2407.13109v1 Announce Type: new 
Abstract: The global sports analytics industry has a market value of USD 3.78 billion in 2023. The increase of wearables such as GPS sensors has provided analysts with large fine-grained datasets detailing player performance. Traditional analysis of this data focuses on individual athletes with measures of internal and external loading such as distance covered in speed zones or rate of perceived exertion. However these metrics do not provide enough information to understand team dynamics within field sports. The spatio-temporal nature of match play necessitates an investment in date-engineering to adequately transform the data into a suitable format to extract features such as areas of activity. In this paper we present an approach to construct Time-Window Spatial Activity Graphs (TWGs) for field sports. Using GPS data obtained from Gaelic Football matches we demonstrate how our approach can be utilised to extract spatio-temporal features from GPS sensor data</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13109v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Valerio Antonini, Michael Scriney, Alessandra Mileo, Mark Roantree</dc:creator>
    </item>
    <item>
      <title>Improvement of Applicability in Student Performance Prediction Based on Transfer Learning</title>
      <link>https://arxiv.org/abs/2407.13112</link>
      <description>arXiv:2407.13112v1 Announce Type: new 
Abstract: Predicting student performance under varying data distributions is a challenging task. This study proposes a method to improve prediction accuracy by employing transfer learning techniques on the dataset with varying distributions. Using datasets from mathematics and Portuguese language courses, the model was trained and evaluated to enhance its generalization ability and prediction accuracy. The datasets used in this study were sourced from Kaggle, comprising a variety of attributes such as demographic details, social factors, and academic performance. The methodology involves using an Artificial Neural Network (ANN) combined with transfer learning, where some layer weights were progressively frozen, and the remaining layers were fine-tuned. Experimental results demonstrated that this approach excels in reducing Root Mean Square Error (RMSE) and Mean Absolute Error (MAE), while improving the coefficient of determination (R2). The model was initially trained on a subset with a larger sample size and subsequently fine-tuned on another subset. This method effectively facilitated knowledge transfer, enhancing model performance on tasks with limited data. The results demonstrate that freezing more layers improves performance for complex and noisy data, whereas freezing fewer layers is more effective for simpler and larger datasets. This study highlights the potential of transfer learning in predicting student performance and suggests future research to explore domain adaptation techniques for unlabeled datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13112v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yan Zhao</dc:creator>
    </item>
    <item>
      <title>SOMONITOR: Explainable Marketing Data Processing and Analysis with Large Language Models</title>
      <link>https://arxiv.org/abs/2407.13117</link>
      <description>arXiv:2407.13117v1 Announce Type: new 
Abstract: Online marketing faces formidable challenges in managing and interpreting immense volumes of data necessary for competitor analysis, content research, and strategic branding. It is impossible to review hundreds to thousands of transient online content items by hand, and partial analysis often leads to suboptimal outcomes and poorly performing campaigns. We introduce an explainable AI framework SoMonitor that aims to synergize human intuition with AI-based efficiency, helping marketers across all stages of the marketing funnel, from strategic planning to content creation and campaign execution. SoMonitor incorporates a CTR prediction and ranking model for advertising content and uses large language models (LLMs) to process high-performing competitor content, identifying core content pillars such as target audiences, customer needs, and product features. These pillars are then organized into broader categories, including communication themes and targeted customer personas. By integrating these insights with data from the brand's own advertising campaigns, SoMonitor constructs a narrative for addressing new customer personas and simultaneously generates detailed content briefs in the form of user stories that can be directly applied by marketing teams to streamline content production and campaign execution. The adoption of SoMonitor in daily operations allows digital marketers to quickly parse through extensive datasets, offering actionable insights that significantly enhance campaign effectiveness and overall job satisfaction</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13117v1</guid>
      <category>cs.CY</category>
      <category>cs.MM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qi Yang, Sergey Nikolenko, Marlo Ongpin, Ilia Gossoudarev, Yu-Yi Chu-Farseeva, Aleksandr Farseev</dc:creator>
    </item>
    <item>
      <title>Generative AI and the problem of existential risk</title>
      <link>https://arxiv.org/abs/2407.13365</link>
      <description>arXiv:2407.13365v1 Announce Type: new 
Abstract: Ever since the launch of ChatGPT, Generative AI has been a focal point for concerns about AI's perceived existential risk. Once a niche topic in AI research and philosophy, AI safety and existential risk has now entered mainstream debate among policy makers and leading foundation models developers, much to the chagrin of those who see it as a distraction from addressing more pressing nearer-term harms. This chapter aims to demystify the debate by highlighting the key worries that underpin existential risk fears in relation to generative AI, and spotlighting the key actions that governments and industry are taking thus far to helping address them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13365v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lynette Webb, Daniel Sch\"onberger</dc:creator>
    </item>
    <item>
      <title>Personal Data Transfers to Non-EEA Domains: A Tool for Citizens and An Analysis on Italian Public Administration Websites</title>
      <link>https://arxiv.org/abs/2407.13467</link>
      <description>arXiv:2407.13467v1 Announce Type: new 
Abstract: Six years after the entry into force of the GDPR, European companies and organizations still have difficulties complying with it: the amount of fines issued by the European data protection authorities is continuously increasing. Personal data transfers are no exception. In this work we analyse the personal data transfers from more than 20000 Italian Public Administration (PA) entities to third countries. We developed "Minos", a user-friendly application which allows to navigate the web while recording HTTP requests. Then, we used the back-end of Minos to automate the analysis. We found that about 14% of the PAs websites transferred data out of the European Economic Area (EEA). This number is an underestimation because only visits to the home pages were object of the analysis. The top 3 destinations of the data transfers are Amazon, Google and Fonticons, accounting for about the 70% of the bad requests. The most recurrent services which are the object of the requests are cloud computing services and content delivery networks (CDNs). Our results highlight that, in Italy, a relevant portion of public administrations websites transfers personal data to non EEA countries. In terms of technology policy, these results stress the need for further incentives to improve the PA digital infrastructures. Finally, while working on refinements of Minos, the version here described is openly available on Zenodo: it can be helpful to a variety of actors (citizens, researchers, activists, policy makers) to increase awareness and enlarge the investigation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13467v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Lorenzo Laudadio, Antonio Vetr\`o, Riccardo Coppola, Juan Carlos De Martin, Marco Torchiano</dc:creator>
    </item>
    <item>
      <title>Training Foundation Models as Data Compression: On Information, Model Weights and Copyright Law</title>
      <link>https://arxiv.org/abs/2407.13493</link>
      <description>arXiv:2407.13493v1 Announce Type: new 
Abstract: The training process of foundation models as for other classes of deep learning systems is based on minimizing the reconstruction error over a training set. For this reason, they are susceptible to the memorization and subsequent reproduction of training samples. In this paper, we introduce a training-as-compressing perspective, wherein the model's weights embody a compressed representation of the training data. From a copyright standpoint, this point of view implies that the weights could be considered a reproduction or a derivative work of a potentially protected set of works. We investigate the technical and legal challenges that emerge from this framing of the copyright of outputs generated by foundation models, including their implications for practitioners and researchers. We demonstrate that adopting an information-centric approach to the problem presents a promising pathway for tackling these emerging complex legal issues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13493v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giorgio Franceschelli, Claudia Cevenini, Mirco Musolesi</dc:creator>
    </item>
    <item>
      <title>Decentralised Governance for Autonomous Cyber-Physical Systems</title>
      <link>https://arxiv.org/abs/2407.13566</link>
      <description>arXiv:2407.13566v1 Announce Type: new 
Abstract: This paper examines the potential for Cyber-Physical Systems (CPS) to be governed in a decentralised manner, whereby blockchain-based infrastructure facilitates the communication between digital and physical domains through self-governing and self-organising principles. Decentralised governance paradigms that integrate computation in physical domains (such as 'Decentralised Autonomous Organisations' (DAOs)) represent a novel approach to autono-mous governance and operations. These have been described as akin to cybernetic systems. Through the lens of a case study of an autonomous cabin called "no1s1" which demonstrates self-ownership via blockchain-based control and feedback loops, this research explores the potential for blockchain infrastructure to be utilised in the management of physical systems. By highlighting the considerations and challenges of decentralised governance in managing autonomous physical spaces, the study reveals that autonomy in the governance of autonomous CPS is not merely a technological feat but also involves a complex mesh of functional and social dynamics. These findings underscore the importance of developing continuous feedback loops and adaptive governance frameworks within decentralised CPS to address both expected and emergent challenges. This investigation contributes to the fields of infra-structure studies and Cyber-Physical Systems engineering. It also contributes to the discourse on decentralised governance and autonomous management of physical spaces by offering both practical insights and providing a framework for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13566v1</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kelsie Nabben (European University Institute), Hongyang Wang (ETH Zurich), Michael Zargham (Block Science)</dc:creator>
    </item>
    <item>
      <title>Shaded Route Planning Using Active Segmentation and Identification of Satellite Images</title>
      <link>https://arxiv.org/abs/2407.13689</link>
      <description>arXiv:2407.13689v1 Announce Type: new 
Abstract: Heatwaves pose significant health risks, particularly due to prolonged exposure to high summer temperatures. Vulnerable groups, especially pedestrians and cyclists on sun-exposed sidewalks, motivate the development of a route planning method that incorporates somatosensory temperature effects through shade ratio consideration. This paper is the first to introduce a pipeline that utilizes segmentation foundation models to extract shaded areas from high-resolution satellite images. These areas are then integrated into a multi-layered road map, enabling users to customize routes based on a balance between distance and shade exposure, thereby enhancing comfort and health during outdoor activities. Specifically, we construct a graph-based representation of the road map, where links indicate connectivity and are updated with shade ratio data for dynamic route planning. This system is already implemented online, with a video demonstration, and will be specifically adapted to assist travelers during the 2024 Olympic Games in Paris.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13689v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Longchao Da, Rohan Chhibba, Rushabh Jaiswal, Ariane Middel, Hua Wei</dc:creator>
    </item>
    <item>
      <title>OxonFair: A Flexible Toolkit for Algorithmic Fairness</title>
      <link>https://arxiv.org/abs/2407.13710</link>
      <description>arXiv:2407.13710v1 Announce Type: new 
Abstract: We present OxonFair, a new open source toolkit for enforcing fairness in binary classification. Compared to existing toolkits: (i) We support NLP and Computer Vision classification as well as standard tabular problems. (ii) We support enforcing fairness on validation data, making us robust to a wide range of overfitting challenges. (iii) Our approach can optimize any measure based on True Positives, False Positive, False Negatives, and True Negatives. This makes it easily extendable and much more expressive than existing toolkits. It supports 9/9 and 10/10 of the decision-based group metrics of two popular review papers. (iv) We jointly optimize a performance objective. This not only minimizes degradation while enforcing fairness, but can improve the performance of otherwise inadequately tuned unfair baselines. OxonFair is compatible with standard ML toolkits including sklearn, Autogluon, and PyTorch and is available online at https://github.com/oxfordinternetinstitute/oxonfair</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13710v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eoin Delaney, Zihao Fu, Sandra Wachter, Brent Mittelstadt, Chris Russell</dc:creator>
    </item>
    <item>
      <title>"I understand why I got this grade": Automatic Short Answer Grading with Feedback</title>
      <link>https://arxiv.org/abs/2407.12818</link>
      <description>arXiv:2407.12818v1 Announce Type: cross 
Abstract: The demand for efficient and accurate assessment methods has intensified as education systems transition to digital platforms. Providing feedback is essential in educational settings and goes beyond simply conveying marks as it justifies the assigned marks. In this context, we present a significant advancement in automated grading by introducing Engineering Short Answer Feedback (EngSAF) -- a dataset of 5.8k student answers accompanied by reference answers and questions for the Automatic Short Answer Grading (ASAG) task. The EngSAF dataset is meticulously curated to cover a diverse range of subjects, questions, and answer patterns from multiple engineering domains. We leverage state-of-the-art large language models' (LLMs) generative capabilities with our Label-Aware Synthetic Feedback Generation (LASFG) strategy to include feedback in our dataset. This paper underscores the importance of enhanced feedback in practical educational settings, outlines dataset annotation and feedback generation processes, conducts a thorough EngSAF analysis, and provides different LLMs-based zero-shot and finetuned baselines for future comparison. Additionally, we demonstrate the efficiency and effectiveness of the ASAG system through its deployment in a real-world end-semester exam at the Indian Institute of Technology Bombay (IITB), showcasing its practical viability and potential for broader implementation in educational institutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12818v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dishank Aggarwal, Pushpak Bhattacharyya, Bhaskaran Raman</dc:creator>
    </item>
    <item>
      <title>Limits to Predicting Online Speech Using Large Language Models</title>
      <link>https://arxiv.org/abs/2407.12850</link>
      <description>arXiv:2407.12850v1 Announce Type: cross 
Abstract: We study the predictability of online speech on social media, and whether predictability improves with information outside a user's own posts. Recent work suggests that the predictive information contained in posts written by a user's peers can surpass that of the user's own posts. Motivated by the success of large language models, we empirically test this hypothesis. We define unpredictability as a measure of the model's uncertainty, i.e., its negative log-likelihood on future tokens given context. As the basis of our study, we collect a corpus of 6.25M posts from more than five thousand X (previously Twitter) users and their peers. Across three large language models ranging in size from 1 billion to 70 billion parameters, we find that predicting a user's posts from their peers' posts performs poorly. Moreover, the value of the user's own posts for prediction is consistently higher than that of their peers'. Across the board, we find that the predictability of social media posts remains low, comparable to predicting financial news without context. We extend our investigation with a detailed analysis about the causes of unpredictability and the robustness of our findings. Specifically, we observe that a significant amount of predictive uncertainty comes from hashtags and @-mentions. Moreover, our results replicate if instead of prompting the model with additional context, we finetune on additional context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12850v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mina Remeli, Moritz Hardt, Robert C. Williamson</dc:creator>
    </item>
    <item>
      <title>AI AI Bias: Large Language Models Favor Their Own Generated Content</title>
      <link>https://arxiv.org/abs/2407.12856</link>
      <description>arXiv:2407.12856v1 Announce Type: cross 
Abstract: Are large language models (LLMs) biased towards text generated by LLMs over text authored by humans, leading to possible anti-human bias? Utilizing a classical experimental design inspired by employment discrimination studies, we tested widely-used LLMs, including GPT-3.5 and GPT4, in binary-choice scenarios. These involved LLM-based agents selecting between products and academic papers described either by humans or LLMs under identical conditions. Our results show a consistent tendency for LLM-based AIs to prefer LLM-generated content. This suggests the possibility of AI systems implicitly discriminating against humans, giving AI agents an unfair advantage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12856v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Walter Laurito, Benjamin Davis, Peli Grietzer, Tom\'a\v{s} Gaven\v{c}iak, Ada B\"ohm, Jan Kulveit</dc:creator>
    </item>
    <item>
      <title>The Foundation Model Transparency Index v1.1: May 2024</title>
      <link>https://arxiv.org/abs/2407.12929</link>
      <description>arXiv:2407.12929v1 Announce Type: cross 
Abstract: Foundation models are increasingly consequential yet extremely opaque. To characterize the status quo, the Foundation Model Transparency Index was launched in October 2023 to measure the transparency of leading foundation model developers. The October 2023 Index (v1.0) assessed 10 major foundation model developers (e.g. OpenAI, Google) on 100 transparency indicators (e.g. does the developer disclose the wages it pays for data labor?). At the time, developers publicly disclosed very limited information with the average score being 37 out of 100. To understand how the status quo has changed, we conduct a follow-up study (v1.1) after 6 months: we score 14 developers against the same 100 indicators. While in v1.0 we searched for publicly available information, in v1.1 developers submit reports on the 100 transparency indicators, potentially including information that was not previously public. We find that developers now score 58 out of 100 on average, a 21 point improvement over v1.0. Much of this increase is driven by developers disclosing information during the v1.1 process: on average, developers disclosed information related to 16.6 indicators that was not previously public. We observe regions of sustained (i.e. across v1.0 and v1.1) and systemic (i.e. across most or all developers) opacity such as on copyright status, data access, data labor, and downstream impact. We publish transparency reports for each developer that consolidate information disclosures: these reports are based on the information disclosed to us via developers. Our findings demonstrate that transparency can be improved in this nascent ecosystem, the Foundation Model Transparency Index likely contributes to these improvements, and policymakers should consider interventions in areas where transparency has not improved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12929v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rishi Bommasani, Kevin Klyman, Sayash Kapoor, Shayne Longpre, Betty Xiong, Nestor Maslej, Percy Liang</dc:creator>
    </item>
    <item>
      <title>Socially Assistive Robot in Sexual Health: Group and Individual Student-Robot Interaction Activities Promoting Disclosure, Learning and Positive Attitudes</title>
      <link>https://arxiv.org/abs/2407.13030</link>
      <description>arXiv:2407.13030v1 Announce Type: cross 
Abstract: Comprehensive sex education (SE) is crucial in promoting sexual health and responsible behavior among students, particularly in elementary schools. Despite its significance, teaching SE can be challenging due to students' attitudes, shyness, and emotional barriers. Socially assistive robots (SARs) sometimes are perceived as more trustworthy than humans, based on research showing that they are not anticipated as judgmental. Inspired by those evidences, this study aims to assess the success of a SAR as a facilitator for SE lessons for elementary school students. This study conducted two experiments to assess the effectiveness of a SAR in facilitating SE education for elementary school students. We conducted two experiments, a) a group activity in the school classroom where the Nao robot gave a SE lecture, and we evaluated how much information the students acquired from the lecture, and b) an individual activity where the students interacted 1:1 with the robot, and we evaluated their attitudes towards the subject of SE, and if they felt comfortable to ask SE related questions to the robot. Data collected from pre- and post-questionnaires, as well as video annotations, revealed that the SAR significantly improved students' attitudes toward SE. Furthermore, students were more open to asking SE-related questions to the robot than their human teacher. The study emphasized specific SAR characteristics, such as embodiment and non-judgmental behavior, as key factors contributing to their effectiveness in supporting SE education, paving the way for innovative and effective approaches to sexual education in schools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13030v1</guid>
      <category>cs.RO</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Anna-Maria Velentza (School of Educational &amp; Social Policies, University of Macedonia, GR, Laboratory of Informatics and Robotics Applications in Education and Society), Efthymia Kefalouka (School of Educational &amp; Social Policies, University of Macedonia, GR), Nikolaos Fachantidis (School of Educational &amp; Social Policies, University of Macedonia, GR, Laboratory of Informatics and Robotics Applications in Education and Society)</dc:creator>
    </item>
    <item>
      <title>On the modification and revocation of open source licences</title>
      <link>https://arxiv.org/abs/2407.13064</link>
      <description>arXiv:2407.13064v1 Announce Type: cross 
Abstract: Historically, open source commitments have been deemed irrevocable once materials are released under open source licenses. In this paper, the authors argue for the creation of a subset of rights that allows open source contributors to force users to (i) update to the most recent version of a model, (ii) accept new use case restrictions, or even (iii) cease using the software entirely. While this would be a departure from the traditional open source approach, the legal, reputational and moral risks related to open-sourcing AI models could justify contributors having more control over downstream uses. Recent legislative changes have also opened the door to liability of open source contributors in certain cases. The authors believe that contributors would welcome the ability to ensure that downstream users are implementing updates that address issues like bias, guardrail workarounds or adversarial attacks on their contributions. Finally, this paper addresses how this license category would interplay with RAIL licenses, and how it should be operationalized and adopted by key stakeholders such as OSS platforms and scanning tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13064v1</guid>
      <category>cs.DL</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul Gagnon, Misha Benjamin, Justine Gauthier, Catherine Regis, Jenny Lee, Alexei Nordell-Markovits</dc:creator>
    </item>
    <item>
      <title>Large Language Model Agents for Improving Engagement with Behavior Change Interventions: Application to Digital Mindfulness</title>
      <link>https://arxiv.org/abs/2407.13067</link>
      <description>arXiv:2407.13067v1 Announce Type: cross 
Abstract: Although engagement in self-directed wellness exercises typically declines over time, integrating social support such as coaching can sustain it. However, traditional forms of support are often inaccessible due to the high costs and complex coordination. Large Language Models (LLMs) show promise in providing human-like dialogues that could emulate social support. Yet, in-depth, in situ investigations of LLMs to support behavior change remain underexplored. We conducted two randomized experiments to assess the impact of LLM agents on user engagement with mindfulness exercises. First, a single-session study, involved 502 crowdworkers; second, a three-week study, included 54 participants. We explored two types of LLM agents: one providing information and another facilitating self-reflection. Both agents enhanced users' intentions to practice mindfulness. However, only the information-providing LLM, featuring a friendly persona, significantly improved engagement with the exercises. Our findings suggest that specific LLM agents may bridge the social support gap in digital health interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13067v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harsh Kumar, Suhyeon Yoo, Angela Zavaleta Bernuy, Jiakai Shi, Huayin Luo, Joseph Williams, Anastasia Kuzminykh, Ashton Anderson, Rachel Kornfield</dc:creator>
    </item>
    <item>
      <title>Reimagining Communities through Transnational Bengali Decolonial Discourse with YouTube Content Creators</title>
      <link>https://arxiv.org/abs/2407.13131</link>
      <description>arXiv:2407.13131v1 Announce Type: cross 
Abstract: Colonialism--the policies and practices wherein a foreign body imposes its ways of life on local communities--has historically impacted how collectives perceive themselves in relation to others. One way colonialism has impacted how people see themselves is through nationalism, where nationalism is often understood through shared language, culture, religion, and geopolitical borders. The way colonialism has shaped people's experiences with nationalism has shaped historical conflicts between members of different nation-states for a long time. While recent social computing research has studied how colonially marginalized people can engage in discourse to decolonize or re-imagine and reclaim themselves and their communities on their own terms--what is less understood is how technology can better support decolonial discourses in an effort to re-imagine nationalism. To understand this phenomenon, this research draws on a semi-structured interview study with YouTubers who make videos about culturally Bengali people whose lives were upended as a product of colonization and are now dispersed across Bangladesh, India, and Pakistan. This research seeks to understand people's motivations and strategies for engaging in video-mediated decolonial discourse in transnational contexts. We discuss how our work demonstrates the potential of the sociomateriality of decolonial discourse online and extends an invitation to foreground complexities of nationalism in social computing research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13131v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dipto Das, Dhwani Gandhi, Bryan Semaan</dc:creator>
    </item>
    <item>
      <title>How to quantify an examination? Evidence from physics examinations via complex networks</title>
      <link>https://arxiv.org/abs/2407.13161</link>
      <description>arXiv:2407.13161v1 Announce Type: cross 
Abstract: Given the untapped potential for continuous improvement of examinations, quantitative investigations of examinations could guide efforts to considerably improve learning efficiency and evaluation and thus greatly help both learners and educators. However, there is a general lack of quantitative methods for investigating examinations. To address this gap, we propose a new metric via complex networks; i.e., the knowledge point network (KPN) of an examination is constructed by representing the knowledge points (concepts, laws, etc.) as nodes and adding links when these points appear in the same question. Then, the topological quantities of KPNs, such as degree, centrality, and community, can be employed to systematically explore the structural properties and evolution of examinations. In this work, 35 physics examinations from the NCEE examination spanning from 2006 to 2020 were investigated as an evidence. We found that the constructed KPNs are scale-free networks that show strong assortativity and small-world effects in most cases. The communities within the KPNs are obvious, and the key nodes are mainly related to mechanics and electromagnetism. Different question types are related to specific knowledge points, leading to noticeable structural variations in KPNs. Moreover, changes in the KPN topology between examinations administered in different years may offer insights guiding college entrance examination reforms. Based on topological quantities such as the average degree, network density, average clustering coefficient, and network transitivity, the Fd is proposed to evaluate examination difficulty. All the above results show that our approach can comprehensively quantify the knowledge structures and examination characteristics. These networks may elucidate comprehensive examination knowledge graphs for educators and guide improvements in teaching.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13161v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.CY</category>
      <category>physics.ed-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Min Xia, Zhu Su, Weibing Deng, Xiumei Feng, Benwei Zhang</dc:creator>
    </item>
    <item>
      <title>Evaluating Large Language Models for Anxiety and Depression Classification using Counseling and Psychotherapy Transcripts</title>
      <link>https://arxiv.org/abs/2407.13228</link>
      <description>arXiv:2407.13228v1 Announce Type: cross 
Abstract: We aim to evaluate the efficacy of traditional machine learning and large language models (LLMs) in classifying anxiety and depression from long conversational transcripts. We fine-tune both established transformer models (BERT, RoBERTa, Longformer) and more recent large models (Mistral-7B), trained a Support Vector Machine with feature engineering, and assessed GPT models through prompting. We observe that state-of-the-art models fail to enhance classification outcomes compared to traditional machine learning methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13228v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junwei Sun, Siqi Ma, Yiran Fan, Peter Washington</dc:creator>
    </item>
    <item>
      <title>Empirical Analysis of Sri Lankan Mobile Health Ecosystem: A Precursor to an Effective Stakeholder Engagement</title>
      <link>https://arxiv.org/abs/2407.13415</link>
      <description>arXiv:2407.13415v1 Announce Type: cross 
Abstract: Sri Lanka recently passed its first privacy legislation covering a wide range of sectors, including health. As a precursor for effective stakeholder engagement in the health domain to understand the most effective way to implement legislation in healthcare, we have analyzed 41 popular mobile apps and web portals. We found that 78% of the tested systems have third-party domains receiving sensitive health data with minimal visibility to the consumers. We discuss how this will create potential issues in preparing for the new privacy legislation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13415v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.SE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kenneth Thilakarathna, Sachintha Pitigala, Jayantha Fernando, Primal Wijesekera</dc:creator>
    </item>
    <item>
      <title>A Security Assessment tool for Quantum Threat Analysis</title>
      <link>https://arxiv.org/abs/2407.13523</link>
      <description>arXiv:2407.13523v1 Announce Type: cross 
Abstract: The rapid advancement of quantum computing poses a significant threat to many current security algorithms used for secure communication, digital authentication, and information encryption. A sufficiently powerful quantum computer could potentially exploit vulnerabilities in these algorithms, rendering data in transit insecure. This threat is expected to materialize within the next 20 years. Immediate transition to quantum-resilient cryptographic schemes is crucial, primarily to mitigate store-now-decrypt-later attacks and to ensure the security of products with decade-long operational lives. This transition requires a systematic approach to identifying and upgrading vulnerable cryptographic implementations. This work developed a quantum assessment tool for organizations, providing tailored recommendations for transitioning their security protocols into a post-quantum world. The work included a systematic evaluation of the proposed solution using qualitative feedback from network administrators and cybersecurity experts. This feedback was used to refine the accuracy and usability of the assessment process. The results demonstrate its effectiveness and usefulness in helping organizations prepare for quantum computing threats. The assessment tool is publicly available at (https://quantum-watch.soton.ac.uk).</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13523v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Basel Halak, Cristian Sebastian Csete, Edward Joyce, Jack Papaioannou, Alexandre Pires, Jin Soma, Betul Gokkaya, Michael Murphy</dc:creator>
    </item>
    <item>
      <title>Beyond Incompatibility: Trade-offs between Mutually Exclusive Fairness Criteria in Machine Learning and Law</title>
      <link>https://arxiv.org/abs/2212.00469</link>
      <description>arXiv:2212.00469v5 Announce Type: replace 
Abstract: Fair and trustworthy AI is becoming ever more important in both machine learning and legal domains. One important consequence is that decision makers must seek to guarantee a 'fair', i.e., non-discriminatory, algorithmic decision procedure. However, there are several competing notions of algorithmic fairness that have been shown to be mutually incompatible under realistic factual assumptions. This concerns, for example, the widely used fairness measures of 'calibration within groups' and 'balance for the positive/negative class'. In this paper, we present a novel algorithm (FAir Interpolation Method: FAIM) for continuously interpolating between these three fairness criteria. Thus, an initially unfair prediction can be remedied to, at least partially, meet a desired, weighted combination of the respective fairness conditions. We demonstrate the effectiveness of our algorithm when applied to synthetic data, the COMPAS data set, and a new, real-world data set from the e-commerce sector. Finally, we discuss to what extent FAIM can be harnessed to comply with conflicting legal obligations. The analysis suggests that it may operationalize duties in traditional legal fields, such as credit scoring and criminal justice proceedings, but also for the latest AI regulations put forth in the EU, like the Digital Markets Act and the recently enacted AI Act.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.00469v5</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Meike Zehlike, Alex Loosley, H{\aa}kan Jonsson, Emil Wiedemann, Philipp Hacker</dc:creator>
    </item>
    <item>
      <title>Collaborative Design for Job-Seekers with Autism: A Conceptual Framework for Future Research</title>
      <link>https://arxiv.org/abs/2405.06078</link>
      <description>arXiv:2405.06078v2 Announce Type: replace 
Abstract: The success of employment is highly related to a job seeker's capability of communicating and collaborating with others. While leveraging one's network during the job-seeking process is intuitive to the neurotypical, this can be challenging for people with autism. Recent empirical findings have started to show how facilitating collaboration between people with autism and their social surroundings through new design can improve their chances of employment. This work aims to provide actionable guidelines and conceptual frameworks that future researchers and practitioners can apply to improve collaborative design for job-seekers with autism. Built upon the literature on past technological interventions built for supporting job-seekers with autism, we define three major research challenges of (1) communication support, (2) employment stage-wise support, and (3) group work support. For each challenge, we review the current state-of-the-art practices and possible future solutions. We then suggest future designs that can provide breakthroughs from the interdisciplinary lens of human-AI collaboration, health services, group work, accessibility computing, and natural language processing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06078v2</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sungsoo Ray Hong, Marcos Zampieri, Brittany N. Hand, Vivian Motti, Dongjun Chung, Ozlem Uzuner</dc:creator>
    </item>
    <item>
      <title>Charting the Landscape of Nefarious Uses of Generative Artificial Intelligence for Online Election Interference</title>
      <link>https://arxiv.org/abs/2406.01862</link>
      <description>arXiv:2406.01862v2 Announce Type: replace 
Abstract: Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs) pose significant risks, particularly in the realm of online election interference. This paper explores the nefarious applications of GenAI, highlighting their potential to disrupt democratic processes through deepfakes, botnets, targeted misinformation campaigns, and synthetic identities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01862v2</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emilio Ferrara</dc:creator>
    </item>
    <item>
      <title>A Labelled Dataset for Sentiment Analysis of Videos on YouTube, TikTok, and Other Sources about the 2024 Outbreak of Measles</title>
      <link>https://arxiv.org/abs/2406.07693</link>
      <description>arXiv:2406.07693v3 Announce Type: replace 
Abstract: The work of this paper presents a dataset that contains the data of 4011 videos about the ongoing outbreak of measles published on 264 websites on the internet between January 1, 2024, and May 31, 2024. The dataset is available at https://dx.doi.org/10.21227/40s8-xf63. These websites primarily include YouTube and TikTok, which account for 48.6% and 15.2% of the videos, respectively. The remainder of the websites include Instagram and Facebook as well as the websites of various global and local news organizations. For each of these videos, the URL of the video, title of the post, description of the post, and the date of publication of the video are presented as separate attributes in the dataset. After developing this dataset, sentiment analysis (using VADER), subjectivity analysis (using TextBlob), and fine-grain sentiment analysis (using DistilRoBERTa-base) of the video titles and video descriptions were performed. This included classifying each video title and video description into (i) one of the sentiment classes i.e. positive, negative, or neutral, (ii) one of the subjectivity classes i.e. highly opinionated, neutral opinionated, or least opinionated, and (iii) one of the fine-grain sentiment classes i.e. fear, surprise, joy, sadness, anger, disgust, or neutral. These results are presented as separate attributes in the dataset for the training and testing of machine learning algorithms for performing sentiment analysis or subjectivity analysis in this field as well as for other applications. Finally, this paper also presents a list of open research questions that may be investigated using this dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07693v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nirmalya Thakur, Vanessa Su, Mingchen Shao, Kesha A. Patel, Hongseok Jeong, Victoria Knieling, Andrew Bian</dc:creator>
    </item>
    <item>
      <title>An Analysis of European Data and AI Regulations for Automotive Organizations</title>
      <link>https://arxiv.org/abs/2407.11271</link>
      <description>arXiv:2407.11271v2 Announce Type: replace 
Abstract: This report summarizes the European Union's series of data and AI regulations and analyzes them for managers in automotive vehicle manufacturing organizations. In particular, we highlight the relevant ideas of the regulations, including how they find their roots in earlier legislation, how they contradict and complement each other, as well as the business opportunities that these regulations offer. The structure of the report is as follows. First, we address the GDPR as the cornerstone against which the requirements of other regulations are weighed and legislated. Second, we explain the EU Data Act since it directly addresses Internet of Things (IoT) for businesses in the private sector and imposes strict requirements on large data generators such as vehicle manufacturers. For manufacturers, compliance with the EU Data Act is a prerequisite for the subsequent legislation, in particular the EU AI Act. Third, we explain the Data Governance Act, Digital Services Act, Digital Markets Act, and EU AI Act in chronological order. Overall, we characterize European Union data regulations as a wave set, rooted in historical precedent, with important implications for the automotive industry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11271v2</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charlotte A. Shahlaei, Nicholas Berente</dc:creator>
    </item>
    <item>
      <title>Deep Generative Attacks and Countermeasures for Data-Driven Offline Signature Verification</title>
      <link>https://arxiv.org/abs/2312.00987</link>
      <description>arXiv:2312.00987v2 Announce Type: replace-cross 
Abstract: This study investigates the vulnerabilities of data-driven offline signature verification (DASV) systems to generative attacks and proposes robust countermeasures. Specifically, we explore the efficacy of Variational Autoencoders (VAEs) and Conditional Generative Adversarial Networks (CGANs) in creating deceptive signatures that challenge DASV systems. Using the Structural Similarity Index (SSIM) to evaluate the quality of forged signatures, we assess their impact on DASV systems built with Xception, ResNet152V2, and DenseNet201 architectures. Initial results showed False Accept Rates (FARs) ranging from 0% to 5.47% across all models and datasets. However, exposure to synthetic signatures significantly increased FARs, with rates ranging from 19.12% to 61.64%. The proposed countermeasure, i.e., retraining the models with real + synthetic datasets, was very effective, reducing FARs between 0% and 0.99%. These findings emphasize the necessity of investigating vulnerabilities in security systems like DASV and reinforce the role of generative methods in enhancing the security of data-driven systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.00987v2</guid>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>An Ngo, Rajesh Kumar, Phuong Cao</dc:creator>
    </item>
    <item>
      <title>PersLLM: A Personified Training Approach for Large Language Models</title>
      <link>https://arxiv.org/abs/2407.12393</link>
      <description>arXiv:2407.12393v2 Announce Type: replace-cross 
Abstract: Large language models exhibit aspects of human-level intelligence that catalyze their application as human-like agents in domains such as social simulations, human-machine interactions, and collaborative multi-agent systems. However, the absence of distinct personalities, such as displaying ingratiating behaviors, inconsistent opinions, and uniform response patterns, diminish LLMs utility in practical applications. Addressing this, the development of personality traits in LLMs emerges as a crucial area of research to unlock their latent potential. Existing methods to personify LLMs generally involve strategies like employing stylized training data for instruction tuning or using prompt engineering to simulate different personalities. These methods only capture superficial linguistic styles instead of the core of personalities and are therefore not stable. In this study, we propose PersLLM, integrating psychology-grounded principles of personality: social practice, consistency, and dynamic development, into a comprehensive training methodology. We incorporate personality traits directly into the model parameters, enhancing the model's resistance to induction, promoting consistency, and supporting the dynamic evolution of personality. Single-agent evaluation validates our method's superiority, as it produces responses more aligned with reference personalities compared to other approaches. Case studies for multi-agent communication highlight its benefits in enhancing opinion consistency within individual agents and fostering collaborative creativity among multiple agents in dialogue contexts, potentially benefiting human simulation and multi-agent cooperation. Additionally, human-agent interaction evaluations indicate that our personified models significantly enhance interactive experiences, underscoring the practical implications of our research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12393v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zheni Zeng, Jiayi Chen, Huimin Chen, Yukun Yan, Yuxuan Chen, Zhiyuan Liu, Maosong Sun</dc:creator>
    </item>
  </channel>
</rss>
