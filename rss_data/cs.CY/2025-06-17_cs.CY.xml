<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 17 Jun 2025 04:00:50 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Artificial Intelligence and Civil Discourse: How LLMs Moderate Climate Change Conversations</title>
      <link>https://arxiv.org/abs/2506.12077</link>
      <description>arXiv:2506.12077v1 Announce Type: new 
Abstract: As large language models (LLMs) become increasingly integrated into online platforms and digital communication spaces, their potential to influence public discourse - particularly in contentious areas like climate change - requires systematic investigation. This study examines how LLMs naturally moderate climate change conversations through their distinct communicative behaviors. We conduct a comparative analysis of conversations between LLMs and human users on social media platforms, using five advanced models: three open-source LLMs (Gemma, Llama 3, and Llama 3.3) and two commercial systems (GPT-4o by OpenAI and Claude 3.5 by Anthropic). Through sentiment analysis, we assess the emotional characteristics of responses from both LLMs and humans. The results reveal two key mechanisms through which LLMs moderate discourse: first, LLMs consistently display emotional neutrality, showing far less polarized sentiment than human users. Second, LLMs maintain lower emotional intensity across contexts, creating a stabilizing effect in conversations. These findings suggest that LLMs possess inherent moderating capacities that could improve the quality of public discourse on controversial topics. This research enhances our understanding of how AI might support more civil and constructive climate change discussions and informs the design of AI-assisted communication tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12077v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Wenlu Fan, Wentao Xu</dc:creator>
    </item>
    <item>
      <title>Intelligent Automation for FDI Facilitation: Optimizing Tariff Exemption Processes with OCR And Large Language Models</title>
      <link>https://arxiv.org/abs/2506.12093</link>
      <description>arXiv:2506.12093v1 Announce Type: new 
Abstract: Tariff exemptions are fundamental to attracting Foreign Direct Investment (FDI) into the manufacturing sector, though the associated administrative processes present areas for optimization for both investing entities and the national tax authority. This paper proposes a conceptual framework to empower tax administration by leveraging a synergistic integration of Optical Character Recognition (OCR) and Large Language Model (LLM) technologies. The proposed system is designed to first utilize OCR for intelligent digitization, precisely extracting data from diverse application documents and key regulatory texts such as tariff orders. Subsequently, the LLM would enhance the capabilities of administrative officers by automating the critical and time-intensive task of verifying submitted HS Tariff Codes for machinery, equipment, and raw materials against official exemption lists. By enhancing the speed and precision of these initial assessments, this AI-driven approach systematically reduces potential for non-alignment and non-optimized exemption utilization, thereby streamlining the investment journey for FDI companies. For the national administration, the benefits include a significant boost in operational capacity, reduced administrative load, and a strengthened control environment, ultimately improving the ease of doing business and solidifying the nation's appeal as a premier destination for high-value manufacturing FDI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12093v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammad Sukri Bin Ramli</dc:creator>
    </item>
    <item>
      <title>Military AI Cyber Agents (MAICAs) Constitute a Global Threat to Critical Infrastructure</title>
      <link>https://arxiv.org/abs/2506.12094</link>
      <description>arXiv:2506.12094v1 Announce Type: new 
Abstract: This paper argues that autonomous AI cyber-weapons - Military-AI Cyber Agents (MAICAs) - create a credible pathway to catastrophic risk. It sets out the technical feasibility of MAICAs, explains why geopolitics and the nature of cyberspace make MAICAs a catastrophic risk, and proposes political, defensive-AI and analogue-resilience measures to blunt the threat.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12094v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Timothy Dubber, Seth Lazar</dc:creator>
    </item>
    <item>
      <title>"I Hadn't Thought About That": Creators of Human-like AI Weigh in on Ethics And Neurodivergence</title>
      <link>https://arxiv.org/abs/2506.12098</link>
      <description>arXiv:2506.12098v1 Announce Type: new 
Abstract: Human-like AI agents such as robots and chatbots are becoming increasingly popular, but they present a variety of ethical concerns. The first concern is in how we define humanness, and how our definition impacts communities historically dehumanized by scientific research. Autistic people in particular have been dehumanized by being compared to robots, making it even more important to ensure this marginalization is not reproduced by AI that may promote neuronormative social behaviors. Second, the ubiquitous use of these agents raises concerns surrounding model biases and accessibility. In our work, we investigate the experiences of the people who build and design these technologies to gain insights into their understanding and acceptance of neurodivergence, and the challenges in making their work more accessible to users with diverse needs. Even though neurodivergent individuals are often marginalized for their unique communication styles, nearly all participants overlooked the conclusions their end-users and other AI system makers may draw about communication norms from the implementation and interpretation of humanness applied in participants' work. This highlights a major gap in their broader ethical considerations, compounded by some participants' neuronormative assumptions about the behaviors and traits that distinguish "humans" from "bots" and the replication of these assumptions in their work. We examine the impact this may have on autism inclusion in society and provide recommendations for additional systemic changes towards more ethical research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12098v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3715275.3732218</arxiv:DOI>
      <arxiv:journal_reference>In The 2025 ACM Conference on Fairness, Accountability, and Transparency (FAccT '25), June 23--26, 2025, Athens, Greece</arxiv:journal_reference>
      <dc:creator>Naba Rizvi, Taggert Smith, Tanvi Vidyala, Mya Bolds, Harper Strickland, Andrew Begel, Rua Williams, Imani Munyaka</dc:creator>
    </item>
    <item>
      <title>SocialCredit+</title>
      <link>https://arxiv.org/abs/2506.12099</link>
      <description>arXiv:2506.12099v1 Announce Type: new 
Abstract: SocialCredit+ is AI powered credit scoring system that leverages publicly available social media data to augment traditional credit evaluation. It uses a conversational banking assistant to gather user consent and fetch public profiles. Multimodal feature extractors analyze posts, bios, images, and friend networks to generate a rich behavioral profile. A specialized Sharia-compliance layer flags any non-halal indicators and prohibited financial behavior based on Islamic ethics. The platform employs a retrieval-augmented generation module: an LLM accesses a domain specific knowledge base to generate clear, text-based explanations for each decision. We describe the end-to-end architecture and data flow, the models used, and system infrastructure. Synthetic scenarios illustrate how social signals translate into credit-score factors. This paper emphasizes conceptual novelty, compliance mechanisms, and practical impact, targeting AI researchers, fintech practitioners, ethical banking jurists, and investors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12099v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Thabassum Aslam, Anees Aslam</dc:creator>
    </item>
    <item>
      <title>Information Suppression in Large Language Models: Auditing, Quantifying, and Characterizing Censorship in DeepSeek</title>
      <link>https://arxiv.org/abs/2506.12349</link>
      <description>arXiv:2506.12349v1 Announce Type: new 
Abstract: This study examines information suppression mechanisms in DeepSeek, an open-source large language model (LLM) developed in China. We propose an auditing framework and use it to analyze the model's responses to 646 politically sensitive prompts by comparing its final output with intermediate chain-of-thought (CoT) reasoning. Our audit unveils evidence of semantic-level information suppression in DeepSeek: sensitive content often appears within the model's internal reasoning but is omitted or rephrased in the final output. Specifically, DeepSeek suppresses references to transparency, government accountability, and civic mobilization, while occasionally amplifying language aligned with state propaganda. This study underscores the need for systematic auditing of alignment, content moderation, information suppression, and censorship practices implemented into widely-adopted AI models, to ensure transparency, accountability, and equitable access to unbiased information obtained by means of these systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12349v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peiran Qiu, Siyi Zhou, Emilio Ferrara</dc:creator>
    </item>
    <item>
      <title>Accessibility Barriers in Multi-Terabyte Public Datasets: The Gap Between Promise and Practice</title>
      <link>https://arxiv.org/abs/2506.13256</link>
      <description>arXiv:2506.13256v1 Announce Type: new 
Abstract: The promise of "free and open" multi-terabyte datasets often collides with harsh realities. While these datasets may be technically accessible, practical barriers -- from processing complexity to hidden costs -- create a system that primarily serves well-funded institutions. This study examines accessibility challenges across web crawls, satellite imagery, scientific data, and collaborative projects, revealing a consistent two-tier system where theoretical openness masks practical exclusivity. Our analysis demonstrates that datasets marketed as "publicly accessible" typically require minimum investments of \$1,000+ for meaningful analysis, with complex processing pipelines demanding \$10,000-100,000+ in infrastructure costs. The infrastructure requirements -- distributed computing knowledge, domain expertise, and substantial budgets -- effectively gatekeep these datasets despite their "open" status, limiting practical accessibility to those with institutional support or substantial resources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13256v1</guid>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <category>cs.IR</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marc Bara</dc:creator>
    </item>
    <item>
      <title>The Transition Matrix -- A classification of navigational patterns between LMS course sections</title>
      <link>https://arxiv.org/abs/2506.13275</link>
      <description>arXiv:2506.13275v1 Announce Type: new 
Abstract: Learning management systems (LMS) like Moodle are increasingly used to support university teaching. As Moodle courses become more complex, incorporating diverse interactive elements, it is important to understand how students navigate through course sections and whether course designs are meeting student needs. While substantial research exists on student usage of individual LMS elements, there is a lack of research on broader navigational patterns between course sections and how these patterns differ across courses. This study analyzes navigational data from 747 courses in the Moodle LMS at a technical university of applied sciences, representing (after filtering) around 4,400 students and 1.8 million logged events. By mapping section names across a large sample of courses, the analysis enables cross-course comparisons of student navigational sequences between sections. Transition matrices and heat map visualizations are used to identify common navigational patterns. Findings include that many of the generated heatmap include one or more diagonal axis, indicating that students typically navigate from the current to the next or previous section. More fine-grained patterns show typical behavior for blended learning scenarios. Other patterns include dominant sections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13275v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tobias Hildebrandt, Lars Mehnen</dc:creator>
    </item>
    <item>
      <title>pySpainMobility: a Python Package to Access and Manage Spanish Open Mobility Data</title>
      <link>https://arxiv.org/abs/2506.13385</link>
      <description>arXiv:2506.13385v1 Announce Type: new 
Abstract: Mobility patterns play a critical role in a wide range of societal challenges, from epidemic modeling and emergency response to transportation planning and regional development. Yet, access to high-quality, timely, and openly available mobility data remains limited. In response, the Spanish Ministry of Transportation and Sustainable Mobility has released daily mobility datasets based on anonymized mobile phone data, covering districts, municipalities, and greater urban areas from February 2020 to June 2021 and again from January 2022 onward. This paper presents pySpainMobility, a Python package that simplifies access to these datasets and their associated study areas through a standardized, well-documented interface. By lowering the technical barrier to working with large-scale mobility data, the package enables reproducible analysis and supports applications across research, policy, and operational domains. The library is available at https://github.com/pySpainMobility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13385v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ciro Beneduce, Tania Gull\'on Mu\~noz-Repiso, Bruno Lepri, Massimiliano Luca</dc:creator>
    </item>
    <item>
      <title>Navigating through CS1: The Role of Self-Regulation and Supervision in Student Progress</title>
      <link>https://arxiv.org/abs/2506.13461</link>
      <description>arXiv:2506.13461v1 Announce Type: new 
Abstract: The need for students' self-regulation for fluent transitioning to university studies is known. Our aim was to integrate study-supportive activities with course supervision activities within CS1. We educated TAs to pay attention to students' study ability and self-regulation. An interview study ($N=14$) was undertaken to investigate this approach. A thematic analysis yielded rather mixed results in light of our aims. Self-regulation was underpinned by the influences external to our setting, including labor market-related needs, earlier crises in study habits, and personal characteristics such as passion, grit, creativity, and valuation of utility. Safety in one-to-one supervision was considered essential, while shyness, fear, and even altruism caused self-handicapping during the course. Students were aware of their learning styles and need for self-regulation, while did not always know how to self-regulate or preferred to externalize it. The results highlight that supporting self-regulation should be integrated with students' personal histories and experiences, and thereby calls attention to transformative learning pedagogies. The thematization can help to understand CS1 students' self-regulation processes and improve CS1 support practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13461v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ville Isom\"ott\"onen, Denis Zhidkikh</dc:creator>
    </item>
    <item>
      <title>Safe-Child-LLM: A Developmental Benchmark for Evaluating LLM Safety in Child-AI Interactions</title>
      <link>https://arxiv.org/abs/2506.13510</link>
      <description>arXiv:2506.13510v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) increasingly power applications used by children and adolescents, ensuring safe and age-appropriate interactions has become an urgent ethical imperative. Despite progress in AI safety, current evaluations predominantly focus on adults, neglecting the unique vulnerabilities of minors engaging with generative AI. We introduce Safe-Child-LLM, a comprehensive benchmark and dataset for systematically assessing LLM safety across two developmental stages: children (7-12) and adolescents (13-17). Our framework includes a novel multi-part dataset of 200 adversarial prompts, curated from red-teaming corpora (e.g., SG-Bench, HarmBench), with human-annotated labels for jailbreak success and a standardized 0-5 ethical refusal scale. Evaluating leading LLMs -- including ChatGPT, Claude, Gemini, LLaMA, DeepSeek, Grok, Vicuna, and Mistral -- we uncover critical safety deficiencies in child-facing scenarios. This work highlights the need for community-driven benchmarks to protect young users in LLM interactions. To promote transparency and collaborative advancement in ethical AI development, we are publicly releasing both our benchmark datasets and evaluation codebase at https://github.com/The-Responsible-AI-Initiative/Safe_Child_LLM_Benchmark.git</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13510v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junfeng Jiao, Saleh Afroogh, Kevin Chen, Abhejay Murali, David Atkinson, Amit Dhurandhar</dc:creator>
    </item>
    <item>
      <title>An LLM's Apology: Outsourcing Awkwardness in the Age of AI</title>
      <link>https://arxiv.org/abs/2506.13685</link>
      <description>arXiv:2506.13685v1 Announce Type: new 
Abstract: A key part of modern social dynamics is flaking at short notice. However, anxiety in coming up with believable and socially acceptable reasons to do so can instead lead to 'ghosting', awkwardness, or implausible excuses, risking emotional harm and resentment in the other party. The ability to delegate this task to a Large Language Model (LLM) could substantially reduce friction and enhance the flexibility of user's social life while greatly minimising the aforementioned creative burden and moral qualms. We introduce FLAKE-Bench, an evaluation of models' capacity to effectively, kindly, and humanely extract themselves from a diverse set of social, professional and romantic scenarios. We report the efficacy of 10 frontier or recently-frontier LLMs in bailing on prior commitments, because nothing says "I value our friendship" like having AI generate your cancellation texts. We open-source FLAKE-Bench at github.com/Cloakless/flake-bench to support future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13685v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Twm Stone, Anna Soligo</dc:creator>
    </item>
    <item>
      <title>Bias Delayed is Bias Denied? Assessing the Effect of Reporting Delays on Disparity Assessments</title>
      <link>https://arxiv.org/abs/2506.13735</link>
      <description>arXiv:2506.13735v1 Announce Type: new 
Abstract: Conducting disparity assessments at regular time intervals is critical for surfacing potential biases in decision-making and improving outcomes across demographic groups. Because disparity assessments fundamentally depend on the availability of demographic information, their efficacy is limited by the availability and consistency of available demographic identifiers. While prior work has considered the impact of missing data on fairness, little attention has been paid to the role of delayed demographic data. Delayed data, while eventually observed, might be missing at the critical point of monitoring and action -- and delays may be unequally distributed across groups in ways that distort disparity assessments. We characterize such impacts in healthcare, using electronic health records of over 5M patients across primary care practices in all 50 states. Our contributions are threefold. First, we document the high rate of race and ethnicity reporting delays in a healthcare setting and demonstrate widespread variation in rates at which demographics are reported across different groups. Second, through a set of retrospective analyses using real data, we find that such delays impact disparity assessments and hence conclusions made across a range of consequential healthcare outcomes, particularly at more granular levels of state-level and practice-level assessments. Third, we find limited ability of conventional methods that impute missing race in mitigating the effects of reporting delays on the accuracy of timely disparity assessments. Our insights and methods generalize to many domains of algorithmic fairness where delays in the availability of sensitive information may confound audits, thus deserving closer attention within a pipeline-aware machine learning framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13735v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jennah Gosciak (Cornell University), Aparna Balagopalan (Massachusetts Institute of Technology), Derek Ouyang (Stanford University), Allison Koenecke (Cornell University), Marzyeh Ghassemi (Massachusetts Institute of Technology), Daniel E. Ho (Stanford University)</dc:creator>
    </item>
    <item>
      <title>Organizational Adaptation to Generative AI in Cybersecurity: A Systematic Review</title>
      <link>https://arxiv.org/abs/2506.12060</link>
      <description>arXiv:2506.12060v1 Announce Type: cross 
Abstract: Cybersecurity organizations are adapting to GenAI integration through modified frameworks and hybrid operational processes, with success influenced by existing security maturity, regulatory requirements, and investments in human capital and infrastructure. This qualitative research employs systematic document analysis and comparative case study methodology to examine how cybersecurity organizations adapt their threat modeling frameworks and operational processes to address generative artificial intelligence integration. Through examination of 25 studies from 2022 to 2025, the research documents substantial transformation in organizational approaches to threat modeling, moving from traditional signature-based systems toward frameworks incorporating artificial intelligence capabilities. The research identifies three primary adaptation patterns: Large Language Model integration for security applications, GenAI frameworks for risk detection and response automation, and AI/ML integration for threat hunting. Organizations with mature security infrastructures, particularly in finance and critical infrastructure sectors, demonstrate higher readiness through structured governance approaches, dedicated AI teams, and robust incident response processes. Organizations achieve successful GenAI integration when they maintain appropriate human oversight of automated systems, address data quality concerns and explainability requirements, and establish governance frameworks tailored to their specific sectors. Organizations encounter ongoing difficulties with privacy protection, bias reduction, personnel training, and defending against adversarial attacks. This work advances understanding of how organizations adopt innovative technologies in high-stakes environments and offers actionable insights for cybersecurity professionals implementing GenAI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12060v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Christopher Nott</dc:creator>
    </item>
    <item>
      <title>Modeling Earth-Scale Human-Like Societies with One Billion Agents</title>
      <link>https://arxiv.org/abs/2506.12078</link>
      <description>arXiv:2506.12078v1 Announce Type: cross 
Abstract: Understanding how complex societal behaviors emerge from individual cognition and interactions requires both high-fidelity modeling of human behavior and large-scale simulations. Traditional agent-based models (ABMs) have been employed to study these dynamics for decades, but are constrained by simplified agent behaviors that fail to capture human complexity. Recent advances in large language models (LLMs) offer new opportunities by enabling agents to exhibit sophisticated social behaviors that go beyond rule-based logic, yet face significant scaling challenges. Here we present Light Society, an agent-based simulation framework that advances both fronts, efficiently modeling human-like societies at planetary scale powered by LLMs. Light Society formalizes social processes as structured transitions of agent and environment states, governed by a set of LLM-powered simulation operations, and executed through an event queue. This modular design supports both independent and joint component optimization, supporting efficient simulation of societies with over one billion agents. Large-scale simulations of trust games and opinion propagation--spanning up to one billion agents--demonstrate Light Society's high fidelity and efficiency in modeling social trust and information diffusion, while revealing scaling laws whereby larger simulations yield more stable and realistic emergent behaviors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12078v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoxiang Guan, Jiyan He, Liyang Fan, Zhenzhen Ren, Shaobin He, Xin Yu, Yuan Chen, Shuxin Zheng, Tie-Yan Liu, Zhen Liu</dc:creator>
    </item>
    <item>
      <title>Risks &amp; Benefits of LLMs &amp; GenAI for Platform Integrity, Healthcare Diagnostics, Cybersecurity, Privacy &amp; AI Safety: A Comprehensive Survey, Roadmap &amp; Implementation Blueprint</title>
      <link>https://arxiv.org/abs/2506.12088</link>
      <description>arXiv:2506.12088v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) and generative AI (GenAI) systems such as ChatGPT, Claude, Gemini, LLaMA, and Copilot, developed by OpenAI, Anthropic, Google, Meta, and Microsoft are reshaping digital platforms and app ecosystems while introducing key challenges in cybersecurity, privacy, and platform integrity. Our analysis shows alarming trends: LLM-assisted malware is projected to rise from 2% in 2021 to 50% by 2025; AI-generated Google reviews grew from 1.2% in 2021 to 12.21% in 2023, with an expected 30% by 2025; AI scam reports surged 456%; and misinformation sites increased over 1500%, with a 50-60% increase in deepfakes in 2024. Concurrently, as LLMs have facilitated code development, mobile app submissions grew from 1.8 million in 2020 to 3.0 million in 2024, with 3.6 million expected by 2025. To address AI threats, platforms from app stores like Google Play and Apple to developer hubs like GitHub Copilot, and social platforms like TikTok and Facebook, to marketplaces like Amazon are deploying AI and LLM-based defenses. This highlights the dual nature of these technologies as both the source of new threats and the essential tool for their mitigation. Integrating LLMs into clinical diagnostics also raises concerns about accuracy, bias, and safety, needing strong governance. Drawing on a comprehensive analysis of 455 references, this paper presents a survey of LLM and GenAI risks. We propose a strategic roadmap and operational blueprint integrating policy auditing (CCPA, GDPR), fraud detection, and compliance automation, and an advanced LLM-DA stack with modular components including multi LLM routing, agentic memory, and governance layers to enhance platform integrity. We also provide actionable insights, cross-functional best practices, and real-world case studies. These contributions offer paths to scalable trust, safety, and responsible innovation across digital platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12088v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kiarash Ahi</dc:creator>
    </item>
    <item>
      <title>Modelado y gemelos digitales en el contexto fotovoltaico</title>
      <link>https://arxiv.org/abs/2506.12102</link>
      <description>arXiv:2506.12102v1 Announce Type: cross 
Abstract: The photovoltaic industry faces the challenge of optimizing the performance and management of its systems in an increasingly digitalized environment. In this context, digital twins offer an innovative solution: virtual models that replicate in real time the behavior of solar installations. This technology makes it possible to anticipate failures, improve operational efficiency and facilitate data-driven decision-making. This report analyzes its application in the photovoltaic sector, highlighting its benefits and transformative potential.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12102v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.CY</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Franco Bertani Matung, Juan Cruz Esquembre Santamar\'ia, Ricardo R. Palma, Fabricio Orlando Sanchez Varretti</dc:creator>
    </item>
    <item>
      <title>The Amazon Nova Family of Models: Technical Report and Model Card</title>
      <link>https://arxiv.org/abs/2506.12103</link>
      <description>arXiv:2506.12103v1 Announce Type: cross 
Abstract: We present Amazon Nova, a new generation of state-of-the-art foundation models that deliver frontier intelligence and industry-leading price performance. Amazon Nova Pro is a highly-capable multimodal model with the best combination of accuracy, speed, and cost for a wide range of tasks. Amazon Nova Lite is a low-cost multimodal model that is lightning fast for processing images, video, documents and text. Amazon Nova Micro is a text-only model that delivers our lowest-latency responses at very low cost. Amazon Nova Canvas is an image generation model that creates professional grade images with rich customization controls. Amazon Nova Reel is a video generation model offering high-quality outputs, customization, and motion control. Our models were built responsibly and with a commitment to customer trust, security, and reliability. We report benchmarking results for core capabilities, agentic performance, long context, functional adaptation, runtime performance, and human evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12103v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amazon AGI (JC), Aaron Langford (JC), Aayush Shah (JC), Abhanshu Gupta (JC), Abhimanyu Bhatter (JC), Abhinav Goyal (JC), Abhinav Mathur (JC), Abhinav Mohanty (JC), Abhishek Kumar (JC), Abhishek Sethi (JC), Abi Komma (JC), Abner Pena (JC), Achin Jain (JC), Adam Kunysz (JC), Adam Opyrchal (JC), Adarsh Singh (JC), Aditya Rawal (JC), Adok Achar Budihal Prasad (JC), Adri\`a de Gispert (JC), Agnika Kumar (JC), Aishwarya Aryamane (JC), Ajay Nair (JC), Akilan M (JC), Akshaya Iyengar (JC), Akshaya Vishnu Kudlu Shanbhogue (JC), Alan He (JC), Alessandra Cervone (JC), Alex Loeb (JC), Alex Zhang (JC), Alexander Fu (JC), Alexander Lisnichenko (JC), Alexander Zhipa (JC), Alexandros Potamianos (JC), Ali Kebarighotbi (JC), Aliakbar Daronkolaei (JC), Alok Parmesh (JC), Amanjot Kaur Samra (JC), Ameen Khan (JC), Amer Rez (JC), Amir Saffari (JC), Amit Agarwalla (JC), Amit Jhindal (JC), Amith Mamidala (JC), Ammar Asmro (JC), Amulya Ballakur (JC), Anand Mishra (JC), Anand Sridharan (JC), Anastasiia Dubinina (JC), Andre Lenz (JC), Andreas Doerr (JC), Andrew Keating (JC), Andrew Leaver (JC), Andrew Smith (JC), Andrew Wirth (JC), Andy Davey (JC), Andy Rosenbaum (JC), Andy Sohn (JC), Angela Chan (JC), Aniket Chakrabarti (JC), Anil Ramakrishna (JC), Anirban Roy (JC), Anita Iyer (JC), Anjali Narayan-Chen (JC), Ankith Yennu (JC), Anna Dabrowska (JC), Anna Gawlowska (JC), Anna Rumshisky (JC), Anna Turek (JC), Anoop Deoras (JC), Anton Bezruchkin (JC), Anup Prasad (JC), Anupam Dewan (JC), Anwith Kiran (JC), Apoorv Gupta (JC), Aram Galstyan (JC), Aravind Manoharan (JC), Arijit Biswas (JC), Arindam Mandal (JC), Arpit Gupta (JC), Arsamkhan Pathan (JC), Arun Nagarajan (JC), Arushan Rajasekaram (JC), Arvind Sundararajan (JC), Ashwin Ganesan (JC), Ashwin Swaminathan (JC), Athanasios Mouchtaris (JC), Audrey Champeau (JC), Avik Ray (JC), Ayush Jaiswal (JC), Ayush Sharma (JC), Bailey Keefer (JC), Balamurugan Muthiah (JC), Beatriz Leon-Millan (JC), Ben Koopman (JC), Ben Li (JC), Benjamin Biggs (JC), Benjamin Ott (JC), Bhanu Vinzamuri (JC), Bharath Venkatesh (JC), Bhavana Ganesh (JC), Bhoomit Vasani (JC), Bill Byrne (JC), Bill Hsu (JC), Bincheng Wang (JC), Blake King (JC), Blazej Gorny (JC), Bo Feng (JC), Bo Zheng (JC), Bodhisattwa Paul (JC), Bofan Sun (JC), Bofeng Luo (JC), Bowen Chen (JC), Bowen Xie (JC), Boya Yu (JC), Brendan Jugan (JC), Brett Panosh (JC), Brian Collins (JC), Brian Thompson (JC), Can Karakus (JC), Can Liu (JC), Carl Lambrecht (JC), Carly Lin (JC), Carolyn Wang (JC), Carrie Yuan (JC), Casey Loyda (JC), Cezary Walczak (JC), Chalapathi Choppa (JC), Chandana Satya Prakash (JC), Chankrisna Richy Meas (JC), Charith Peris (JC), Charles Recaido (JC), Charlie Xu (JC), Charul Sharma (JC), Chase Kernan (JC), Chayut Thanapirom (JC), Chengwei Su (JC), Chenhao Xu (JC), Chenhao Yin (JC), Chentao Ye (JC), Chenyang Tao (JC), Chethan Parameshwara (JC), Ching-Yun Chang (JC), Chong Li (JC), Chris Hench (JC), Chris Tran (JC), Christophe Dupuy (JC), Christopher Davis (JC), Christopher DiPersio (JC), Christos Christodoulopoulos (JC), Christy Li (JC), Chun Chen (JC), Claudio Delli Bovi (JC), Clement Chung (JC), Cole Hawkins (JC), Connor Harris (JC), Corey Ropell (JC), Cynthia He (JC), DK Joo (JC), Dae Yon Hwang (JC), Dan Rosen (JC), Daniel Elkind (JC), Daniel Pressel (JC), Daniel Zhang (JC), Danielle Kimball (JC), Daniil Sorokin (JC), Dave Goodell (JC), Davide Modolo (JC), Dawei Zhu (JC), Deepikaa Suresh (JC), Deepti Ragha (JC), Denis Filimonov (JC), Denis Foo Kune (JC), Denis Romasanta Rodriguez (JC), Devamanyu Hazarika (JC), Dhananjay Ram (JC), Dhawal Parkar (JC), Dhawal Patel (JC), Dhwanil Desai (JC), Dinesh Singh Rajput (JC), Disha Sule (JC), Diwakar Singh (JC), Dmitriy Genzel (JC), Dolly Goldenberg (JC), Dongyi He (JC), Dumitru Hanciu (JC), Dushan Tharmal (JC), Dzmitry Siankovich (JC), Edi Cikovic (JC), Edwin Abraham (JC), Ekraam Sabir (JC), Elliott Olson (JC), Emmett Steven (JC), Emre Barut (JC), Eric Jackson (JC), Ethan Wu (JC), Evelyn Chen (JC), Ezhilan Mahalingam (JC), Fabian Triefenbach (JC), Fan Yang (JC), Fangyu Liu (JC), Fanzi Wu (JC), Faraz Tavakoli (JC), Farhad Khozeimeh (JC), Feiyang Niu (JC), Felix Hieber (JC), Feng Li (JC), Firat Elbey (JC), Florian Krebs (JC), Florian Saupe (JC), Florian Spr\"unken (JC), Frank Fan (JC), Furqan Khan (JC), Gabriela De Vincenzo (JC), Gagandeep Kang (JC), George Ding (JC), George He (JC), George Yeung (JC), Ghada Qaddoumi (JC), Giannis Karamanolakis (JC), Goeric Huybrechts (JC), Gokul Maddali (JC), Gonzalo Iglesias (JC), Gordon McShane (JC), Gozde Sahin (JC), Guangtai Huang (JC), Gukyeong Kwon (JC), Gunnar A. Sigurdsson (JC), Gurpreet Chadha (JC), Gururaj Kosuru (JC), Hagen Fuerstenau (JC), Hah Hah (JC), Haja Maideen (JC), Hajime Hosokawa (JC), Han Liu (JC), Han-Kai Hsu (JC), Hann Wang (JC), Hao Li (JC), Hao Yang (JC), Haofeng Zhu (JC), Haozheng Fan (JC), Harman Singh (JC), Harshavardhan Kaluvala (JC), Hashim Saeed (JC), He Xie (JC), Helian Feng (JC), Hendrix Luo (JC), Hengzhi Pei (JC), Henrik Nielsen (JC), Hesam Ilati (JC), Himanshu Patel (JC), Hongshan Li (JC), Hongzhou Lin (JC), Hussain Raza (JC), Ian Cullinan (JC), Imre Kiss (JC), Inbarasan Thangamani (JC), Indrayani Fadnavis (JC), Ionut Teodor Sorodoc (JC), Irem Ertuerk (JC), Iryna Yemialyanava (JC), Ishan Soni (JC), Ismail Jelal (JC), Ivan Tse (JC), Jack FitzGerald (JC), Jack Zhao (JC), Jackson Rothgeb (JC), Jacky Lee (JC), Jake Jung (JC), Jakub Debski (JC), Jakub Tomczak (JC), James Jeun (JC), James Sanders (JC), Jason Crowley (JC), Jay Lee (JC), Jayakrishna Anvesh Paidy (JC), Jayant Tiwari (JC), Jean Farmer (JC), Jeff Solinsky (JC), Jenna Lau (JC), Jeremy Savareese (JC), Jerzy Zagorski (JC), Ji Dai (JC),  Jiacheng (JC),  Gu (Skyler), Jiahui Li (Skyler),  Jian (Skyler),  Zheng (QZ), Jianhua Lu (QZ), Jianhua Wang (QZ), Jiawei Dai (QZ), Jiawei Mo (QZ), Jiaxi Xu (QZ), Jie Liang (QZ), Jie Yang (QZ), Jim Logan (QZ), Jimit Majmudar (QZ), Jing Liu (QZ), Jinghong Miao (QZ), Jingru Yi (QZ), Jingyang Jin (QZ), Jiun-Yu Kao (QZ), Jixuan Wang (QZ), Jiyang Wang (QZ), Joe Pemberton (QZ), Joel Carlson (QZ), Joey Blundell (QZ), John Chin-Jew (QZ), John He (QZ), Jonathan Ho (QZ), Jonathan Hueser (QZ), Jonathan Lunt (QZ), Jooyoung Lee (QZ), Joshua Tan (QZ), Joyjit Chatterjee (QZ), Judith Gaspers (QZ), Jue Wang (QZ), Jun Fang (QZ), Jun Tang (QZ), Jun Wan (QZ), Jun Wu (QZ), Junlei Wang (QZ), Junyi Shi (QZ), Justin Chiu (QZ), Justin Satriano (QZ), Justin Yee (QZ), Jwala Dhamala (QZ), Jyoti Bansal (QZ), Kai Zhen (QZ), Kai-Wei Chang (QZ), Kaixiang Lin (QZ), Kalyan Raman (QZ), Kanthashree Mysore Sathyendra (QZ), Karabo Moroe (QZ), Karan Bhandarkar (QZ), Karan Kothari (QZ), Karolina Owczarzak (QZ), Karthick Gopalswamy (QZ), Karthick Ravi (QZ), Karthik Ramakrishnan (QZ), Karthika Arumugam (QZ), Kartik Mehta (QZ), Katarzyna Konczalska (QZ), Kavya Ravikumar (QZ), Ke Tran (QZ), Kechen Qin (QZ), Kelin Li (QZ), Kelvin Li (QZ), Ketan Kulkarni (QZ), Kevin Angelo Rodrigues (QZ), Keyur Patel (QZ), Khadige Abboud (QZ), Kiana Hajebi (QZ), Klaus Reiter (QZ), Kris Schultz (QZ), Krishna Anisetty (QZ), Krishna Kotnana (QZ), Kristen Li (QZ), Kruthi Channamallikarjuna (QZ), Krzysztof Jakubczyk (QZ), Kuba Pierewoj (QZ), Kunal Pal (QZ), Kunwar Srivastav (QZ), Kyle Bannerman (QZ), Lahari Poddar (QZ), Lakshmi Prasad (QZ), Larry Tseng (QZ), Laxmikant Naik (QZ), Leena Chennuru Vankadara (QZ), Lenon Minorics (QZ), Leo Liu (QZ), Leonard Lausen (QZ), Leonardo F. R. Ribeiro (QZ), Li Zhang (QZ), Lili Gehorsam (QZ), Ling Qi (QZ), Lisa Bauer (QZ), Lori Knapp (QZ), Lu Zeng (QZ), Lucas Tong (QZ), Lulu Wong (QZ), Luoxin Chen (QZ), Maciej Rudnicki (QZ), Mahdi Namazifar (QZ), Mahesh Jaliminche (QZ), Maira Ladeira Tanke (QZ), Manasi Gupta (QZ), Mandeep Ahlawat (QZ), Mani Khanuja (QZ), Mani Sundaram (QZ), Marcin Leyk (QZ), Mariusz Momotko (QZ), Markus Boese (QZ), Markus Dreyer (QZ), Markus Mueller (QZ), Mason Fu (QZ), Mateusz G\'orski (QZ), Mateusz Mastalerczyk (QZ), Matias Mora (QZ), Matt Johnson (QZ), Matt Scott (QZ), Matthew Wen (QZ), Max Barysau (QZ), Maya Boumerdassi (QZ), Maya Krishnan (QZ), Mayank Gupta (QZ), Mayank Hirani (QZ), Mayank Kulkarni (QZ), Meganathan Narayanasamy (QZ), Melanie Bradford (QZ), Melanie Gens (QZ), Melissa Burke (QZ), Meng Jin (QZ), Miao Chen (QZ), Michael Denkowski (QZ), Michael Heymel (QZ), Michael Krestyaninov (QZ), Michal Obirek (QZ), Michalina Wichorowska (QZ), Micha{\l} Miotk (QZ), Milosz Watroba (QZ), Mingyi Hong (QZ), Mingzhi Yu (QZ), Miranda Liu (QZ), Mohamed Gouda (QZ), Mohammad El-Shabani (QZ), Mohammad Ghavamzadeh (QZ), Mohit Bansal (QZ), Morteza Ziyadi (QZ), Nan Xia (QZ), Nathan Susanj (QZ), Nav Bhasin (QZ), Neha Goswami (QZ), Nehal Belgamwar (QZ), Nicolas Anastassacos (QZ), Nicolas Bergeron (QZ), Nidhi Jain (QZ), Nihal Jain (QZ), Niharika Chopparapu (QZ), Nik Xu (QZ), Nikko Strom (QZ), Nikolaos Malandrakis (QZ), Nimisha Mishra (QZ), Ninad Parkhi (QZ), Ninareh Mehrabi (QZ), Nishita Sant (QZ), Nishtha Gupta (QZ), Nitesh Sekhar (QZ), Nithin Rajeev (QZ), Nithish Raja Chidambaram (QZ), Nitish Dhar (QZ), Noor Bhagwagar (QZ), Noy Konforty (QZ), Omar Babu (QZ), Omid Razavi (QZ), Orchid Majumder (QZ), Osama Dar (QZ), Oscar Hsu (QZ), Pablo Kvitca (QZ), Pallavi Pandey (QZ), Parker Seegmiller (QZ), Patrick Lange (QZ), Paul Ferraro (QZ), Payal Motwani (QZ), Pegah Kharazmi (QZ), Pei Wang (QZ), Pengfei Liu (QZ), Peter Bradtke (QZ), Peter G\"otz (QZ), Peter Zhou (QZ), Pichao Wang (QZ), Piotr Poskart (QZ), Pooja Sonawane (QZ), Pradeep Natarajan (QZ), Pradyun Ramadorai (QZ), Pralam Shah (QZ), Prasad Nirantar (QZ), Prasanthi Chavali (QZ), Prashan Wanigasekara (QZ), Prashant Saraf (QZ), Prashun Dey (QZ), Pratyush Pant (QZ), Prerak Pradhan (QZ), Preyaa Patel (QZ), Priyanka Dadlani (QZ), Prudhvee Narasimha Sadha (QZ), Qi Dong (QZ), Qian Hu (QZ),  Qiaozi (QZ),  Gao (Sean), Qing Liu (Sean), Quinn Lam (Sean), Quynh Do (Sean), R. Manmatha (Sean), Rachel Willis (Sean), Rafael Liu (Sean), Rafal Ellert (Sean), Rafal Kalinski (Sean), Rafi Al Attrach (Sean), Ragha Prasad (Sean), Ragini Prasad (Sean), Raguvir Kunani (Sean), Rahul Gupta (Sean), Rahul Sharma (Sean), Rahul Tewari (Sean), Rajaganesh Baskaran (Sean), Rajan Singh (Sean), Rajiv Gupta (Sean), Rajiv Reddy (Sean), Rajshekhar Das (Sean), Rakesh Chada (Sean), Rakesh Vaideeswaran Mahesh (Sean), Ram Chandrasekaran (Sean), Ramesh Nallapati (Sean), Ran Xue (Sean), Rashmi Gangadharaiah (Sean), Ravi Rachakonda (Sean), Renxian Zhang (Sean), Rexhina Blloshmi (Sean), Rishabh Agrawal (Sean), Robert Enyedi (Sean), Robert Lowe (Sean), Robik Shrestha (Sean), Robinson Piramuthu (Sean), Rohail Asad (Sean), Rohan Khanna (Sean), Rohan Mukherjee (Sean), Rohit Mittal (Sean), Rohit Prasad (Sean), Rohith Mysore Vijaya Kumar (Sean), Ron Diamant (Sean), Ruchita Gupta (Sean), Ruiwen Li (Sean), Ruoying Li (Sean), Rushabh Fegade (Sean), Ruxu Zhang (Sean), Ryan Arbow (Sean), Ryan Chen (Sean), Ryan Gabbard (Sean), Ryan Hoium (Sean), Ryan King (Sean), Sabarishkumar Iyer (Sean), Sachal Malick (Sean), Sahar Movaghati (Sean), Sai Balakavi (Sean), Sai Jakka (Sean), Sai Kashyap Paruvelli (Sean), Sai Muralidhar Jayanthi (Sean), Saicharan Shriram Mujumdar (Sean), Sainyam Kapoor (Sean), Sajjad Beygi (Sean), Saket Dingliwal (Sean), Saleh Soltan (Sean), Sam Ricklin (Sean), Sam Tucker (Sean), Sameer Sinha (Sean), Samridhi Choudhary (Sean), Samson Tan (Sean), Samuel Broscheit (Sean), Samuel Schulter (Sean), Sanchit Agarwal (Sean), Sandeep Atluri (Sean), Sander Valstar (Sean), Sanjana Shankar (Sean), Sanyukta Sanyukta (Sean), Sarthak Khanna (Sean), Sarvpriye Khetrapal (Sean), Satish Janakiraman (Sean), Saumil Shah (Sean), Saurabh Akolkar (Sean), Saurabh Giri (Sean), Saurabh Khandelwal (Sean), Saurabh Pawar (Sean), Saurabh Sahu (Sean), Sean Huang (Sean), Sejun Ra (Sean), Senthilkumar Gopal (Sean), Sergei Dobroshinsky (Sean), Shadi Saba (Sean), Shamik Roy (Sean), Shamit Lal (Sean), Shankar Ananthakrishnan (Sean), Sharon Li (Sean), Shashwat Srijan (Sean), Shekhar Bhide (Sean), Sheng Long Tang (Sean), Sheng Zha (Sean), Shereen Oraby (Sean), Sherif Mostafa (Sean), Shiqi Li (Sean), Shishir Bharathi (Sean), Shivam Prakash (Sean), Shiyuan Huang (Sean), Shreya Yembarwar (Sean), Shreyas Pansare (Sean), Shreyas Subramanian (Sean), Shrijeet Joshi (Sean), Shuai Liu (Sean), Shuai Tang (Sean), Shubham Chandak (Sean), Shubham Garg (Sean), Shubham Katiyar (Sean), Shubham Mehta (Sean), Shubham Srivastav (Sean), Shuo Yang (Sean), Siddalingesha D S (Sean), Siddharth Choudhary (Sean), Siddharth Singh Senger (Sean), Simon Babb (Sean), Sina Moeini (Sean), Siqi Deng (Sean), Siva Loganathan (Sean), Slawomir Domagala (Sean), Sneha Narkar (Sean), Sneha Wadhwa (Sean), Songyang Zhang (Sean), Songyao Jiang (Sean), Sony Trenous (Sean), Soumajyoti Sarkar (Sean), Soumya Saha (Sean), Sourabh Reddy (Sean), Sourav Dokania (Sean), Spurthideepika Sandiri (Sean), Spyros Matsoukas (Sean), Sravan Bodapati (Sean), Sri Harsha Reddy Wdaru (Sean), Sridevi Yagati Venkateshdatta (Sean), Srikanth Ronanki (Sean), Srinivasan R Veeravanallur (Sean), Sriram Venkatapathy (Sean), Sriramprabhu Sankaraguru (Sean), Sruthi Gorantla (Sean), Sruthi Karuturi (Sean), Stefan Schroedl (Sean), Subendhu Rongali (Sean), Subhasis Kundu (Sean), Suhaila Shakiah (Sean), Sukriti Tiwari (Sean), Sumit Bharti (Sean), Sumita Sami (Sean), Sumith Mathew (Sean), Sunny Yu (Sean), Sunwoo Kim (Sean), Suraj Bajirao Malode (Sean), Susana Cumplido Riel (Sean), Swapnil Palod (Sean), Swastik Roy (Sean), Syed Furqhan (Sean), Tagyoung Chung (Sean), Takuma Yoshitani (Sean), Taojiannan Yang (Sean), Tejaswi Chillakura (Sean), Tejwant Bajwa (Sean), Temi Lajumoke (Sean), Thanh Tran (Sean), Thomas Gueudre (Sean), Thomas Jung (Sean), Tianhui Li (Sean), Tim Seemman (Sean), Timothy Leffel (Sean), Tingting Xiang (Sean), Tirth Patel (Sean), Tobias Domhan (Sean), Tobias Falke (Sean), Toby Guo (Sean), Tom Li (Sean), Tomasz Horszczaruk (Sean), Tomasz Jedynak (Sean), Tushar Kulkarni (Sean), Tyst Marin (Sean), Tytus Metrycki (Sean), Tzu-Yen Wang (Sean), Umang Jain (Sean), Upendra Singh (Sean), Utkarsh Chirimar (Sean), Vaibhav Gupta (Sean), Vanshil Shah (Sean), Varad Deshpande (Sean), Varad Gunjal (Sean), Varsha Srikeshava (Sean), Varsha Vivek (Sean), Varun Bharadwaj (Sean), Varun Gangal (Sean), Varun Kumar (Sean), Venkatesh Elango (Sean), Vicente Ordonez (Sean), Victor Soto (Sean), Vignesh Radhakrishnan (Sean), Vihang Patel (Sean), Vikram Singh (Sean), Vinay Varma Kolanuvada (Sean), Vinayshekhar Bannihatti Kumar (Sean), Vincent Auvray (Sean), Vincent Cartillier (Sean), Vincent Ponzo (Sean), Violet Peng (Sean), Vishal Khandelwal (Sean), Vishal Naik (Sean), Vishvesh Sahasrabudhe (Sean), Vitaliy Korolev (Sean), Vivek Gokuladas (Sean), Vivek Madan (Sean), Vivek Subramanian (Sean), Volkan Cevher (Sean), Vrinda Gupta (Sean), Wael Hamza (Sean), Wei Zhang (Sean), Weitong Ruan (Sean), Weiwei Cheng (Sean), Wen Zhang (Sean), Wenbo Zhao (Sean), Wenyan Yao (Sean), Wenzhuo Ouyang (Sean), Wesley Dashner (Sean), William Campbell (Sean), William Lin (Sean), Willian Martin (Sean), Wyatt Pearson (Sean), Xiang Jiang (Sean), Xiangxing Lu (Sean), Xiangyang Shi (Sean), Xianwen Peng (Sean), Xiaofeng Gao (Sean), Xiaoge Jiang (Sean), Xiaohan Fei (Sean), Xiaohui Wang (Sean), Xiaozhou Joey Zhou (Sean), Xin Feng (Sean), Xinyan Zhao (Sean), Xinyao Wang (Sean), Xinyu Li (Sean), Xu Zhang (Sean), Xuan Wang (Sean), Xuandi Fu (Sean), Xueling Yuan (Sean), Xuning Wang (Sean), Yadunandana Rao (Sean), Yair Tavizon (Sean), Yan Rossiytsev (Sean), Yanbei Chen (Sean), Yang Liu (Sean), Yang Zou (Sean), Yangsook Park (Sean), Yannick Versley (Sean), Yanyan Zhang (Sean), Yash Patel (Sean), Yen-Cheng Lu (Sean), Yi Pan (Sean),  Yi-Hsiang (Sean),  Lai (Rex), Yichen Hu (Rex), Yida Wang (Rex), Yiheng Zhou (Rex), Yilin Xiang (Rex), Ying Shi (Rex), Ying Wang (Rex), Yishai Galatzer (Rex), Yongxin Wang (Rex), Yorick Shen (Rex), Yuchen Sun (Rex), Yudi Purwatama (Rex),  Yue (Rex),  Wu (Chris), Yue Gu (Chris), Yuechun Wang (Chris), Yujun Zeng (Chris), Yuncong Chen (Chris), Yunke Zhou (Chris), Yusheng Xie (Chris), Yvon Guy (Chris), Zbigniew Ambrozinski (Chris), Zhaowei Cai (Chris), Zhen Zhang (Chris), Zheng Wang (Chris), Zhenghui Jin (Chris), Zhewei Zhao (Chris), Zhiheng Li (Chris), Zhiheng Luo (Chris), Zhikang Zhang (Chris), Zhilin Fang (Chris), Zhiqi Bu (Chris), Zhiyuan Wang (Chris), Zhizhong Li (Chris), Zijian Wang (Chris),  Zimeng (Chris),  Qiu, Zishi Li</dc:creator>
    </item>
    <item>
      <title>Large Language Models for History, Philosophy, and Sociology of Science: Interpretive Uses, Methodological Challenges, and Critical Perspectives</title>
      <link>https://arxiv.org/abs/2506.12242</link>
      <description>arXiv:2506.12242v1 Announce Type: cross 
Abstract: This paper explores the use of large language models (LLMs) as research tools in the history, philosophy, and sociology of science (HPSS). LLMs are remarkably effective at processing unstructured text and inferring meaning from context, offering new affordances that challenge long-standing divides between computational and interpretive methods. This raises both opportunities and challenges for HPSS, which emphasizes interpretive methodologies and understands meaning as context-dependent, ambiguous, and historically situated. We argue that HPSS is uniquely positioned not only to benefit from LLMs' capabilities but also to interrogate their epistemic assumptions and infrastructural implications. To this end, we first offer a concise primer on LLM architectures and training paradigms tailored to non-technical readers. We frame LLMs not as neutral tools but as epistemic infrastructures that encode assumptions about meaning, context, and similarity, conditioned by their training data, architecture, and patterns of use. We then examine how computational techniques enhanced by LLMs, such as structuring data, detecting patterns, and modeling dynamic processes, can be applied to support interpretive research in HPSS. Our analysis compares full-context and generative models, outlines strategies for domain and task adaptation (e.g., continued pretraining, fine-tuning, and retrieval-augmented generation), and evaluates their respective strengths and limitations for interpretive inquiry in HPSS. We conclude with four lessons for integrating LLMs into HPSS: (1) model selection involves interpretive trade-offs; (2) LLM literacy is foundational; (3) HPSS must define its own benchmarks and corpora; and (4) LLMs should enhance, not replace, interpretive methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12242v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arno Simons, Michael Zichert, Adrian W\"uthrich</dc:creator>
    </item>
    <item>
      <title>EgoPrivacy: What Your First-Person Camera Says About You?</title>
      <link>https://arxiv.org/abs/2506.12258</link>
      <description>arXiv:2506.12258v1 Announce Type: cross 
Abstract: While the rapid proliferation of wearable cameras has raised significant concerns about egocentric video privacy, prior work has largely overlooked the unique privacy threats posed to the camera wearer. This work investigates the core question: How much privacy information about the camera wearer can be inferred from their first-person view videos? We introduce EgoPrivacy, the first large-scale benchmark for the comprehensive evaluation of privacy risks in egocentric vision. EgoPrivacy covers three types of privacy (demographic, individual, and situational), defining seven tasks that aim to recover private information ranging from fine-grained (e.g., wearer's identity) to coarse-grained (e.g., age group). To further emphasize the privacy threats inherent to egocentric vision, we propose Retrieval-Augmented Attack, a novel attack strategy that leverages ego-to-exo retrieval from an external pool of exocentric videos to boost the effectiveness of demographic privacy attacks. An extensive comparison of the different attacks possible under all threat models is presented, showing that private information of the wearer is highly susceptible to leakage. For instance, our findings indicate that foundation models can effectively compromise wearer privacy even in zero-shot settings by recovering attributes such as identity, scene, gender, and race with 70-80% accuracy. Our code and data are available at https://github.com/williamium3000/ego-privacy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12258v1</guid>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yijiang Li, Genpei Zhang, Jiacheng Cheng, Yi Li, Xiaojun Shan, Dashan Gao, Jiancheng Lyu, Yuan Li, Ning Bi, Nuno Vasconcelos</dc:creator>
    </item>
    <item>
      <title>Energy-Efficient Green AI Architectures for Circular Economies Through Multi-Layered Sustainable Resource Optimization Framework</title>
      <link>https://arxiv.org/abs/2506.12262</link>
      <description>arXiv:2506.12262v1 Announce Type: cross 
Abstract: In this research paper, we propose a new type of energy-efficient Green AI architecture to support circular economies and address the contemporary challenge of sustainable resource consumption in modern systems. We introduce a multi-layered framework and meta-architecture that integrates state-of-the-art machine learning algorithms, energy-conscious computational models, and optimization techniques to facilitate decision-making for resource reuse, waste reduction, and sustainable production.We tested the framework on real-world datasets from lithium-ion battery recycling and urban waste management systems, demonstrating its practical applicability. Notably, the key findings of this study indicate a 25 percent reduction in energy consumption during workflows compared to traditional methods and an 18 percent improvement in resource recovery efficiency. Quantitative optimization was based on mathematical models such as mixed-integer linear programming and lifecycle assessments. Moreover, AI algorithms improved classification accuracy on urban waste by 20 percent, while optimized logistics reduced transportation emissions by 30 percent. We present graphical analyses and visualizations of the developed framework, illustrating its impact on energy efficiency and sustainability as reflected in the simulation results. This paper combines the principles of Green AI with practical insights into how such architectural models contribute to circular economies, presenting a fully scalable and scientifically rooted solution aligned with applicable UN Sustainability Goals worldwide. These results open avenues for incorporating newly developed AI technologies into sustainable management strategies, potentially safeguarding local natural capital while advancing technological progress.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12262v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>cs.CY</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ripal Ranpara</dc:creator>
    </item>
    <item>
      <title>Bridging the Digital Divide: Small Language Models as a Pathway for Physics and Photonics Education in Underdeveloped Regions</title>
      <link>https://arxiv.org/abs/2506.12403</link>
      <description>arXiv:2506.12403v1 Announce Type: cross 
Abstract: Limited infrastructure, scarce educational resources, and unreliable internet access often hinder physics and photonics education in underdeveloped regions. These barriers create deep inequities in Science, Technology, Engineering, and Mathematics (STEM) education. This article explores how Small Language Models (SLMs)-compact, AI-powered tools that can run offline on low-power devices, offering a scalable solution. By acting as virtual tutors, enabling native-language instruction, and supporting interactive learning, SLMs can help address the shortage of trained educators and laboratory access. By narrowing the digital divide through targeted investment in AI technologies, SLMs present a scalable and inclusive solution to advance STEM education and foster scientific empowerment in marginalized communities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12403v1</guid>
      <category>physics.ed-ph</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Asghar Ghorbani, Hanieh Fattahi</dc:creator>
    </item>
    <item>
      <title>Feeling Machines: Ethics, Culture, and the Rise of Emotional AI</title>
      <link>https://arxiv.org/abs/2506.12437</link>
      <description>arXiv:2506.12437v1 Announce Type: cross 
Abstract: This paper explores the growing presence of emotionally responsive artificial intelligence through a critical and interdisciplinary lens. Bringing together the voices of early-career researchers from multiple fields, it explores how AI systems that simulate or interpret human emotions are reshaping our interactions in areas such as education, healthcare, mental health, caregiving, and digital life. The analysis is structured around four central themes: the ethical implications of emotional AI, the cultural dynamics of human-machine interaction, the risks and opportunities for vulnerable populations, and the emerging regulatory, design, and technical considerations. The authors highlight the potential of affective AI to support mental well-being, enhance learning, and reduce loneliness, as well as the risks of emotional manipulation, over-reliance, misrepresentation, and cultural bias. Key challenges include simulating empathy without genuine understanding, encoding dominant sociocultural norms into AI systems, and insufficient safeguards for individuals in sensitive or high-risk contexts. Special attention is given to children, elderly users, and individuals with mental health challenges, who may interact with AI in emotionally significant ways. However, there remains a lack of cognitive or legal protections which are necessary to navigate such engagements safely. The report concludes with ten recommendations, including the need for transparency, certification frameworks, region-specific fine-tuning, human oversight, and longitudinal research. A curated supplementary section provides practical tools, models, and datasets to support further work in this domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12437v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vivek Chavan, Arsen Cenaj, Shuyuan Shen, Ariane Bar, Srishti Binwani, Tommaso Del Becaro, Marius Funk, Lynn Greschner, Roberto Hung, Stina Klein, Romina Kleiner, Stefanie Krause, Sylwia Olbrych, Vishvapalsinhji Parmar, Jaleh Sarafraz, Daria Soroko, Daksitha Withanage Don, Chang Zhou, Hoang Thuy Duong Vu, Parastoo Semnani, Daniel Weinhardt, Elisabeth Andre, J\"org Kr\"uger, Xavier Fresquet</dc:creator>
    </item>
    <item>
      <title>Towards Fairness Assessment of Dutch Hate Speech Detection</title>
      <link>https://arxiv.org/abs/2506.12502</link>
      <description>arXiv:2506.12502v1 Announce Type: cross 
Abstract: Numerous studies have proposed computational methods to detect hate speech online, yet most focus on the English language and emphasize model development. In this study, we evaluate the counterfactual fairness of hate speech detection models in the Dutch language, specifically examining the performance and fairness of transformer-based models. We make the following key contributions. First, we curate a list of Dutch Social Group Terms that reflect social context. Second, we generate counterfactual data for Dutch hate speech using LLMs and established strategies like Manual Group Substitution (MGS) and Sentence Log-Likelihood (SLL). Through qualitative evaluation, we highlight the challenges of generating realistic counterfactuals, particularly with Dutch grammar and contextual coherence. Third, we fine-tune baseline transformer-based models with counterfactual data and evaluate their performance in detecting hate speech. Fourth, we assess the fairness of these models using Counterfactual Token Fairness (CTF) and group fairness metrics, including equality of odds and demographic parity. Our analysis shows that models perform better in terms of hate speech detection, average counterfactual fairness and group fairness. This work addresses a significant gap in the literature on counterfactual fairness for hate speech detection in Dutch and provides practical insights and recommendations for improving both model performance and fairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12502v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julie Bauer, Rishabh Kaushal, Thales Bertaglia, Adriana Iamnitchi</dc:creator>
    </item>
    <item>
      <title>Regulating Next-Generation Implantable Brain-Computer Interfaces: Recommendations for Ethical Development and Implementation</title>
      <link>https://arxiv.org/abs/2506.12540</link>
      <description>arXiv:2506.12540v1 Announce Type: cross 
Abstract: Brain-computer interfaces offer significant therapeutic opportunities for a variety of neurophysiological and neuropsychiatric disorders and may perhaps one day lead to augmenting the cognition and decision-making of the healthy brain. However, existing regulatory frameworks designed for implantable medical devices are inadequate to address the unique ethical, legal, and social risks associated with next-generation networked brain-computer interfaces. In this article, we make nine recommendations to support developers in the design of BCIs and nine recommendations to support policymakers in the application of BCIs, drawing insights from the regulatory history of IMDs and principles from AI ethics. We begin by outlining the historical development of IMDs and the regulatory milestones that have shaped their oversight. Next, we summarize similarities between IMDs and emerging implantable BCIs, identifying existing provisions for their regulation. We then use two case studies of emerging cutting-edge BCIs, the HALO and SCALO computer systems, to highlight distinctive features in the design and application of next-generation BCIs arising from contemporary chip architectures, which necessitate reevaluating regulatory approaches. We identify critical ethical considerations for these BCIs, including unique conceptions of autonomy, identity, and mental privacy. Based on these insights, we suggest potential avenues for the ethical regulation of BCIs, emphasizing the importance of interdisciplinary collaboration and proactive mitigation of potential harms. The goal is to support the responsible design and application of new BCIs, ensuring their safe and ethical integration into medical practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12540v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Renee Sirbu, Jessica Morley, Tyler Schroder, Mariarosaria Taddeo, Raghavendra Pradyumna Pothukuchi, Muhammed Ugur, Abhishek Bhattacharjee, Luciano Floridi</dc:creator>
    </item>
    <item>
      <title>Fairness Research For Machine Learning Should Integrate Societal Considerations</title>
      <link>https://arxiv.org/abs/2506.12556</link>
      <description>arXiv:2506.12556v1 Announce Type: cross 
Abstract: Enhancing fairness in machine learning (ML) systems is increasingly important nowadays. While current research focuses on assistant tools for ML pipelines to promote fairness within them, we argue that: 1) The significance of properly defined fairness measures remains underestimated; and 2) Fairness research in ML should integrate societal considerations. The reasons include that detecting discrimination is critical due to the widespread deployment of ML systems and that human-AI feedback loops amplify biases, even when only small social and political biases persist.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12556v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yijun Bian, Lei You</dc:creator>
    </item>
    <item>
      <title>Rethinking Hate Speech Detection on Social Media: Can LLMs Replace Traditional Models?</title>
      <link>https://arxiv.org/abs/2506.12744</link>
      <description>arXiv:2506.12744v1 Announce Type: cross 
Abstract: Hate speech detection across contemporary social media presents unique challenges due to linguistic diversity and the informal nature of online discourse. These challenges are further amplified in settings involving code-mixing, transliteration, and culturally nuanced expressions. While fine-tuned transformer models, such as BERT, have become standard for this task, we argue that recent large language models (LLMs) not only surpass them but also redefine the landscape of hate speech detection more broadly. To support this claim, we introduce IndoHateMix, a diverse, high-quality dataset capturing Hindi-English code-mixing and transliteration in the Indian context, providing a realistic benchmark to evaluate model robustness in complex multilingual scenarios where existing NLP methods often struggle. Our extensive experiments show that cutting-edge LLMs (such as LLaMA-3.1) consistently outperform task-specific BERT-based models, even when fine-tuned on significantly less data. With their superior generalization and adaptability, LLMs offer a transformative approach to mitigating online hate in diverse environments. This raises the question of whether future works should prioritize developing specialized models or focus on curating richer and more varied datasets to further enhance the effectiveness of LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12744v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Daman Deep Singh, Ramanuj Bhattacharjee, Abhijnan Chakraborty</dc:creator>
    </item>
    <item>
      <title>Prosocial Design in Trust and Safety</title>
      <link>https://arxiv.org/abs/2506.12792</link>
      <description>arXiv:2506.12792v1 Announce Type: cross 
Abstract: This chapter presents an overview of Prosocial Design, an approach to platform design and governance that recognizes design choices influence behavior and that those choices can or should be made toward supporting healthy interactions and other prosocial outcomes. The authors discuss several core principles of Prosocial Design and its relationship to Trust and Safety and other related fields. As a primary contribution, the chapter reviews relevant research to demonstrate how Prosocial Design can be an effective approach to reducing rule-breaking and other harmful behavior and how it can help to stem the spread of harmful misinformation. Prosocial Design is a nascent and evolving field and research is still limited. The authors hope this chapter will not only inspire more research and the adoption of a prosocial design approach, but that it will also provoke discussion about the principles of Prosocial Design and its potential to support Trust and Safety.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12792v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Gr\"uning, Julia Kamin</dc:creator>
    </item>
    <item>
      <title>Governments Should Mandate Tiered Anonymity on Social-Media Platforms to Counter Deepfakes and LLM-Driven Mass Misinformation</title>
      <link>https://arxiv.org/abs/2506.12814</link>
      <description>arXiv:2506.12814v1 Announce Type: cross 
Abstract: This position paper argues that governments should mandate a three-tier anonymity framework on social-media platforms as a reactionary measure prompted by the ease-of-production of deepfakes and large-language-model-driven misinformation. The tiers are determined by a given user's $\textit{reach score}$: Tier 1 permits full pseudonymity for smaller accounts, preserving everyday privacy; Tier 2 requires private legal-identity linkage for accounts with some influence, reinstating real-world accountability at moderate reach; Tier 3 would require per-post, independent, ML-assisted fact-checking, review for accounts that would traditionally be classed as sources-of-mass-information.
  An analysis of Reddit shows volunteer moderators converge on comparable gates as audience size increases -- karma thresholds, approval queues, and identity proofs -- demonstrating operational feasibility and social legitimacy. Acknowledging that existing engagement incentives deter voluntary adoption, we outline a regulatory pathway that adapts existing US jurisprudence and recent EU-UK safety statutes to embed reach-proportional identity checks into existing platform tooling, thereby curbing large-scale misinformation while preserving everyday privacy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12814v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Khachaturov, Roxanne Schnyder, Robert Mullins</dc:creator>
    </item>
    <item>
      <title>Rethinking Optimization: A Systems-Based Approach to Social Externalities</title>
      <link>https://arxiv.org/abs/2506.12825</link>
      <description>arXiv:2506.12825v1 Announce Type: cross 
Abstract: Optimization is widely used for decision making across various domains, valued for its ability to improve efficiency. However, poor implementation practices can lead to unintended consequences, particularly in socioeconomic contexts where externalities (costs or benefits to third parties outside the optimization process) are significant. To propose solutions, it is crucial to first characterize involved stakeholders, their goals, and the types of subpar practices causing unforeseen outcomes. This task is complex because affected stakeholders often fall outside the direct focus of optimization processes. Also, incorporating these externalities into optimization requires going beyond traditional economic frameworks, which often focus on describing externalities but fail to address their normative implications or interconnected nature, and feedback loops. This paper suggests a framework that combines systems thinking with the economic concept of externalities to tackle these challenges. This approach aims to characterize what went wrong, who was affected, and how (or where) to include them in the optimization process. Economic externalities, along with their established quantification methods, assist in identifying "who was affected and how" through stakeholder characterization. Meanwhile, systems thinking (an analytical approach to comprehending relationships in complex systems) provides a holistic, normative perspective. Systems thinking contributes to an understanding of interconnections among externalities, feedback loops, and determining "when" to incorporate them in the optimization. Together, these approaches create a comprehensive framework for addressing optimization's unintended consequences, balancing descriptive accuracy with normative objectives. Using this, we examine three common types of subpar practices: ignorance, error, and prioritization of short-term goals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12825v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pegah Nokhiz, Aravinda Kanchana Ruwanpathirana, Helen Nissenbaum</dc:creator>
    </item>
    <item>
      <title>Rethinking Test-Time Scaling for Medical AI: Model and Task-Aware Strategies for LLMs and VLMs</title>
      <link>https://arxiv.org/abs/2506.13102</link>
      <description>arXiv:2506.13102v1 Announce Type: cross 
Abstract: Test-time scaling has recently emerged as a promising approach for enhancing the reasoning capabilities of large language models or vision-language models during inference. Although a variety of test-time scaling strategies have been proposed, and interest in their application to the medical domain is growing, many critical aspects remain underexplored, including their effectiveness for vision-language models and the identification of optimal strategies for different settings. In this paper, we conduct a comprehensive investigation of test-time scaling in the medical domain. We evaluate its impact on both large language models and vision-language models, considering factors such as model size, inherent model characteristics, and task complexity. Finally, we assess the robustness of these strategies under user-driven factors, such as misleading information embedded in prompts. Our findings offer practical guidelines for the effective use of test-time scaling in medical applications and provide insights into how these strategies can be further refined to meet the reliability and interpretability demands of the medical domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13102v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Gyutaek Oh, Seoyeon Kim, Sangjoon Park, Byung-Hoon Kim</dc:creator>
    </item>
    <item>
      <title>A Game-Theoretic Negotiation Framework for Cross-Cultural Consensus in LLMs</title>
      <link>https://arxiv.org/abs/2506.13245</link>
      <description>arXiv:2506.13245v1 Announce Type: cross 
Abstract: The increasing prevalence of large language models (LLMs) is influencing global value systems. However, these models frequently exhibit a pronounced WEIRD (Western, Educated, Industrialized, Rich, Democratic) cultural bias due to lack of attention to minority values. This monocultural perspective may reinforce dominant values and marginalize diverse cultural viewpoints, posing challenges for the development of equitable and inclusive AI systems. In this work, we introduce a systematic framework designed to boost fair and robust cross-cultural consensus among LLMs. We model consensus as a Nash Equilibrium and employ a game-theoretic negotiation method based on Policy-Space Response Oracles (PSRO) to simulate an organized cross-cultural negotiation process. To evaluate this approach, we construct regional cultural agents using data transformed from the World Values Survey (WVS). Beyond the conventional model-level evaluation method, We further propose two quantitative metrics, Perplexity-based Acceptence and Values Self-Consistency, to assess consensus outcomes. Experimental results indicate that our approach generates consensus of higher quality while ensuring more balanced compromise compared to baselines. Overall, it mitigates WEIRD bias by guiding agents toward convergence through fair and gradual negotiation steps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13245v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.GT</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guoxi Zhang, Jiawei Chen, Tianzhuo Yang, Jiaming Ji, Yaodong Yang, Juntao Dai</dc:creator>
    </item>
    <item>
      <title>Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses</title>
      <link>https://arxiv.org/abs/2506.13384</link>
      <description>arXiv:2506.13384v1 Announce Type: cross 
Abstract: Large language models (LLMs) offer the potential to simulate human-like responses and behaviors, creating new opportunities for psychological science. In the context of self-regulated learning (SRL), if LLMs can reliably simulate survey responses at scale and speed, they could be used to test intervention scenarios, refine theoretical models, augment sparse datasets, and represent hard-to-reach populations. However, the validity of LLM-generated survey responses remains uncertain, with limited research focused on SRL and existing studies beyond SRL yielding mixed results. Therefore, in this study, we examined LLM-generated responses to the 44-item Motivated Strategies for Learning Questionnaire (MSLQ; Pintrich \&amp; De Groot, 1990), a widely used instrument assessing students' learning strategies and academic motivation. Particularly, we used the LLMs GPT-4o, Claude 3.7 Sonnet, Gemini 2 Flash, LLaMA 3.1-8B, and Mistral Large. We analyzed item distributions, the psychological network of the theoretical SRL dimensions, and psychometric validity based on the latent factor structure. Our results suggest that Gemini 2 Flash was the most promising LLM, showing considerable sampling variability and producing underlying dimensions and theoretical relationships that align with prior theory and empirical findings. At the same time, we observed discrepancies and limitations, underscoring both the potential and current constraints of using LLMs for simulating psychological survey data and applying it in educational contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13384v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leonie V. D. E. Vogelsmeier, Eduardo Oliveira, Kamila Misiejuk, Sonsoles L\'opez-Pernas, Mohammed Saqr</dc:creator>
    </item>
    <item>
      <title>Navigating LLM Ethics: Advancements, Challenges, and Future Directions</title>
      <link>https://arxiv.org/abs/2406.18841</link>
      <description>arXiv:2406.18841v5 Announce Type: replace 
Abstract: This study addresses ethical issues surrounding Large Language Models (LLMs) within the field of artificial intelligence. It explores the common ethical challenges posed by both LLMs and other AI systems, such as privacy and fairness, as well as ethical challenges uniquely arising from LLMs. It highlights challenges such as hallucination, verifiable accountability, and decoding censorship complexity, which are unique to LLMs and distinct from those encountered in traditional AI systems. The study underscores the need to tackle these complexities to ensure accountability, reduce biases, and enhance transparency in the influential role that LLMs play in shaping information dissemination. It proposes mitigation strategies and future directions for LLM ethics, advocating for interdisciplinary collaboration. It recommends ethical frameworks tailored to specific domains and dynamic auditing systems adapted to diverse contexts. This roadmap aims to guide responsible development and integration of LLMs, envisioning a future where ethical considerations govern AI advancements in society.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18841v5</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junfeng Jiao, Saleh Afroogh, Yiming Xu, Connor Phillips</dc:creator>
    </item>
    <item>
      <title>Impact of Shared E-scooter Introduction on Public Transport Demand: A Case Study in Santiago, Chile</title>
      <link>https://arxiv.org/abs/2409.17814</link>
      <description>arXiv:2409.17814v2 Announce Type: replace 
Abstract: This study examines how the introduction of shared electric scooters (e-scooters) affects public transport demand in Santiago, Chile, analyzing whether they complement or substitute for existing transit services. We used smart card data from the integrated public transport system of Santiago and GPS traces from e-scooter trips during the initial deployment period. We employed a difference-in-differences approach with negative binomial regression models across three urban regions identified through k-means clustering: Central, Intermediate, and Peripheral. Results reveal spatially heterogeneous effects on public transport boardings and alightings. In the Central Region, e-scooter introduction was associated with significant substitution effects, showing a 23.87% reduction in combined bus and metro boardings, suggesting e-scooters replace short public transport trips in high-density areas. The Intermediate Region showed strong complementary effects, with a 33.6% increase in public transport boardings and 4.08% increase in alightings, indicating e-scooters successfully serve as first/last-mile connectors that enhance transit accessibility. The Peripheral Region exhibited no significant effects. Metro services experienced stronger impacts than bus services, with metro boardings increasing 9.77\% in the Intermediate Region. Our findings advance understanding of micromobility-transit interactions by demonstrating that both substitution and complementarity can coexist within the same urban system, depending on local accessibility conditions. These results highlight the need for spatially differentiated mobility policies that recognize e-scooters' variable roles across urban environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17814v2</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniela Opitz, Eduardo Graells-Garrido, Jacqueline Arriagada, Matilde Rivas, Natalia Meza</dc:creator>
    </item>
    <item>
      <title>LLMs and Childhood Safety: Identifying Risks and Proposing a Protection Framework for Safe Child-LLM Interaction</title>
      <link>https://arxiv.org/abs/2502.11242</link>
      <description>arXiv:2502.11242v4 Announce Type: replace 
Abstract: This study examines the growing use of Large Language Models (LLMs) in child-centered applications, highlighting safety and ethical concerns such as bias, harmful content, and cultural insensitivity. Despite their potential to enhance learning, there is a lack of standardized frameworks to mitigate these risks. Through a systematic literature review, we identify key parental and empirical concerns, including toxicity and ethical breaches in AI outputs. Moreover, to address these issues, this paper proposes a protection framework for safe Child-LLM interaction, incorporating metrics for content safety, behavioral ethics, and cultural sensitivity. The framework provides practical tools for evaluating LLM safety, offering guidance for developers, policymakers, and educators to ensure responsible AI deployment for children.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11242v4</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junfeng Jiao, Saleh Afroogh, Kevin Chen, Abhejay Murali, David Atkinson, Amit Dhurandhar</dc:creator>
    </item>
    <item>
      <title>What do Large Language Models Say About Animals? Investigating Risks of Animal Harm in Generated Text</title>
      <link>https://arxiv.org/abs/2503.04804</link>
      <description>arXiv:2503.04804v3 Announce Type: replace 
Abstract: As machine learning systems become increasingly embedded in society, their impact on human and nonhuman life continues to escalate. Technical evaluations have addressed a variety of potential harms from large language models (LLMs) towards humans and the environment, but there is little empirical work regarding harms towards nonhuman animals. Following the growing recognition of animal protection in regulatory and ethical AI frameworks, we present AnimalHarmBench (AHB), a benchmark for risks of animal harm in LLM-generated text. Our benchmark dataset comprises 1,850 curated questions from Reddit post titles and 2,500 synthetic questions based on 50 animal categories (e.g., cats, reptiles) and 50 ethical scenarios with a 70-30 public-private split. Scenarios include open-ended questions about how to treat animals, practical scenarios with potential animal harm, and willingness-to-pay measures for the prevention of animal harm. Using the LLM-as-a-judge framework, responses are evaluated for their potential to increase or decrease harm, and evaluations are debiased for the tendency of judges to judge their own outputs more favorably. AHB reveals significant differences across frontier LLMs, animal categories, scenarios, and subreddits. We conclude with future directions for technical research and addressing the challenges of building evaluations on complex social and moral topics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04804v3</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arturs Kanepajs, Aditi Basu, Sankalpa Ghose, Constance Li, Akshat Mehta, Ronak Mehta, Samuel David Tucker-Davis, Eric Zhou, Bob Fischer, Jacy Reese Anthis</dc:creator>
    </item>
    <item>
      <title>Transparency in Healthcare AI: Testing European Regulatory Provisions against Users' Transparency Needs</title>
      <link>https://arxiv.org/abs/2505.17105</link>
      <description>arXiv:2505.17105v2 Announce Type: replace 
Abstract: Artificial Intelligence (AI) plays an essential role in healthcare and is pervasively incorporated into medical software and equipment. In the European Union, healthcare is a high-risk application domain for AI, and providers must prepare Instructions for Use (IFU) according to the European regulation 2024/1689 (AI Act). To this regulation, the principle of transparency is cardinal and requires the IFU to be clear and relevant to the users. This study tests whether these latter requirements are satisfied by the IFU structure. A survey was administered online via the Qualtrics platform to four types of direct stakeholders, i.e., managers (N = 238), healthcare professionals (N = 115), patients (N = 229), and Information Technology experts (N = 230). The participants rated the relevance of a set of transparency needs and indicated the IFU section addressing them. The results reveal differentiated priorities across stakeholders and a troubled mapping of transparency needs onto the IFU structure. Recommendations to build a locally meaningful IFU are derived.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17105v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Anna Spagnolli, Cecilia Tolomini, Elisa Beretta, Claudio Sarra</dc:creator>
    </item>
    <item>
      <title>KI4Demokratie: An AI-Based Platform for Monitoring and Fostering Democratic Discourse</title>
      <link>https://arxiv.org/abs/2506.09947</link>
      <description>arXiv:2506.09947v2 Announce Type: replace 
Abstract: Social media increasingly fuel extremism, especially right-wing extremism, and enable the rapid spread of antidemocratic narratives. Although AI and data science are often leveraged to manipulate political opinion, there is a critical need for tools that support effective monitoring without infringing on freedom of expression. We present KI4Demokratie, an AI-based platform that assists journalists, researchers, and policymakers in monitoring right-wing discourse that may undermine democratic values. KI4Demokratie applies machine learning models to a large-scale German online data gathered on a daily basis, providing a comprehensive view of trends in the German digital sphere. Early analysis reveals both the complexity of tracking organized extremist behavior and the promise of our integrated approach, especially during key events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09947v2</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rudy Alexandro Garrido Veliz, Till Nikolaus Schaland, Simon Bergmoser, Florian Horwege, Somya Bansal, Ritesh Nahar, Martin Semmann, J\"org Forthmann, Seid Muhie Yimam</dc:creator>
    </item>
    <item>
      <title>The Urban Model Platform: A Public Backbone for Modeling and Simulation in Urban Digital Twins</title>
      <link>https://arxiv.org/abs/2506.10964</link>
      <description>arXiv:2506.10964v2 Announce Type: replace 
Abstract: Urban digital twins are increasingly perceived as a way to pool the growing digital resources of cities for the purpose of a more sustainable and integrated urban planning. Models and simulations are central to this undertaking: They enable "what if?" scenarios, create insights and describe relationships between the vast data that is being collected. However, the process of integrating and subsequently using models in urban digital twins is an inherently complex undertaking. It raises questions about how to represent urban complexity, how to deal with uncertain assUrban Model Platformtions and modeling paradigms, and how to capture underlying power relations. Existent approaches in the domain largely focus on monolithic and centralized solutions in the tradition of neoliberal city-making, oftentimes prohibiting pluralistic and open interoperable models. Using a participatory design for participatory systems approach together with the City of Hamburg, Germany, we find that an open Urban Model Platform can function both as a public technological backbone for modeling and simulation in urban digital twins and as a socio-technical framework for a collaborative and pluralistic representation of urban processes. Such a platform builds on open standards, allows for a decentralized integration of models, enables communication between models and supports a multi-model approach to representing urban systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10964v2</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rico H Herzog, Till Degkwitz, Trivik Verma</dc:creator>
    </item>
    <item>
      <title>Roadmap on Incentive Compatibility for AI Alignment and Governance in Sociotechnical Systems</title>
      <link>https://arxiv.org/abs/2402.12907</link>
      <description>arXiv:2402.12907v3 Announce Type: replace-cross 
Abstract: The burgeoning integration of artificial intelligence (AI) into human society brings forth significant implications for societal governance and safety. While considerable strides have been made in addressing AI alignment challenges, existing methodologies primarily focus on technical facets, often neglecting the intricate sociotechnical nature of AI systems, which can lead to a misalignment between the development and deployment contexts. To this end, we posit a new problem worth exploring: Incentive Compatibility Sociotechnical Alignment Problem (ICSAP). We hope this can call for more researchers to explore how to leverage the principles of Incentive Compatibility (IC) from game theory to bridge the gap between technical and societal components to maintain AI consensus with human societies in different contexts. We further discuss three classical game problems for achieving IC: mechanism design, contract theory, and Bayesian persuasion, in addressing the perspectives, potentials, and challenges of solving ICSAP, and provide preliminary implementation conceptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.12907v3</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.GT</category>
      <category>cs.HC</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhaowei Zhang, Fengshuo Bai, Mingzhi Wang, Haoyang Ye, Chengdong Ma, Yaodong Yang</dc:creator>
    </item>
    <item>
      <title>Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse</title>
      <link>https://arxiv.org/abs/2410.21333</link>
      <description>arXiv:2410.21333v4 Announce Type: replace-cross 
Abstract: Chain-of-thought (CoT) prompting has become a widely used strategy for improving large language and multimodal model performance. However, it is still an open question under which settings CoT systematically reduces performance. In this paper, we seek to identify the characteristics of tasks where CoT reduces performance by drawing inspiration from cognitive psychology, focusing on six representative tasks from the psychological literature where deliberation hurts performance in humans. In three of these tasks, state-of-the-art models exhibit significant performance drop-offs with CoT (up to 36.3\% absolute accuracy for OpenAI o1-preview compared to GPT-4o), while in others, CoT effects are mixed, with positive, neutral, and negative changes. While models and humans do not exhibit perfectly parallel cognitive processes, considering cases where thinking has negative consequences for humans helps identify settings where it negatively impacts models. By connecting the literature on human verbal thinking and deliberation with evaluations of CoT, we offer a perspective for understanding the impact of inference-time reasoning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21333v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ryan Liu, Jiayi Geng, Addison J. Wu, Ilia Sucholutsky, Tania Lombrozo, Thomas L. Griffiths</dc:creator>
    </item>
    <item>
      <title>Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions</title>
      <link>https://arxiv.org/abs/2502.04322</link>
      <description>arXiv:2502.04322v2 Announce Type: replace-cross 
Abstract: Despite extensive safety alignment efforts, large language models (LLMs) remain vulnerable to jailbreak attacks that elicit harmful behavior. While existing studies predominantly focus on attack methods that require technical expertise, two critical questions remain underexplored: (1) Are jailbroken responses truly useful in enabling average users to carry out harmful actions? (2) Do safety vulnerabilities exist in more common, simple human-LLM interactions? In this paper, we demonstrate that LLM responses most effectively facilitate harmful actions when they are both actionable and informative--two attributes easily elicited in multi-step, multilingual interactions. Using this insight, we propose HarmScore, a jailbreak metric that measures how effectively an LLM response enables harmful actions, and Speak Easy, a simple multi-step, multilingual attack framework. Notably, by incorporating Speak Easy into direct request and jailbreak baselines, we see an average absolute increase of 0.319 in Attack Success Rate and 0.426 in HarmScore in both open-source and proprietary LLMs across four safety benchmarks. Our work reveals a critical yet often overlooked vulnerability: Malicious users can easily exploit common interaction patterns for harmful intentions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04322v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yik Siu Chan, Narutatsu Ri, Yuxin Xiao, Marzyeh Ghassemi</dc:creator>
    </item>
    <item>
      <title>Truth Knows No Language: Evaluating Truthfulness Beyond English</title>
      <link>https://arxiv.org/abs/2502.09387</link>
      <description>arXiv:2502.09387v3 Announce Type: replace-cross 
Abstract: We introduce a professionally translated extension of the TruthfulQA benchmark designed to evaluate truthfulness in Basque, Catalan, Galician, and Spanish. Truthfulness evaluations of large language models (LLMs) have primarily been conducted in English. However, the ability of LLMs to maintain truthfulness across languages remains under-explored. Our study evaluates 12 state-of-the-art open LLMs, comparing base and instruction-tuned models using human evaluation, multiple-choice metrics, and LLM-as-a-Judge scoring. Our findings reveal that, while LLMs perform best in English and worst in Basque (the lowest-resourced language), overall truthfulness discrepancies across languages are smaller than anticipated. Furthermore, we show that LLM-as-a-Judge correlates more closely with human judgments than multiple-choice metrics, and that informativeness plays a critical role in truthfulness assessment. Our results also indicate that machine translation provides a viable approach for extending truthfulness benchmarks to additional languages, offering a scalable alternative to professional translation. Finally, we observe that universal knowledge questions are better handled across languages than context- and time-dependent ones, highlighting the need for truthfulness evaluations that account for cultural and temporal variability. Dataset and code are publicly available under open licenses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09387v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Blanca Calvo Figueras, Eneko Sagarzazu, Julen Etxaniz, Jeremy Barnes, Pablo Gamallo, Iria De Dios Flores, Rodrigo Agerri</dc:creator>
    </item>
    <item>
      <title>Evaluating how LLM annotations represent diverse views on contentious topics</title>
      <link>https://arxiv.org/abs/2503.23243</link>
      <description>arXiv:2503.23243v2 Announce Type: replace-cross 
Abstract: Researchers have proposed the use of generative large language models (LLMs) to label data for research and applied settings. This literature emphasizes the improved performance of these models relative to other natural language models, noting that generative LLMs typically outperform other models and even humans across several metrics. Previous literature has examined bias across many applications and contexts, but less work has focused specifically on bias in generative LLMs' responses to subjective annotation tasks. This bias could result in labels applied by LLMs that disproportionately align with majority groups over a more diverse set of viewpoints. In this paper, we evaluate how LLMs represent diverse viewpoints on these contentious tasks. Across four annotation tasks on four datasets, we show that LLMs do not show systematic substantial disagreement with annotators on the basis of demographics. Rather, we find that multiple LLMs tend to be biased in the same directions on the same demographic categories within the same datasets. Moreover, the disagreement between human annotators on the labeling task -- a measure of item difficulty -- is far more predictive of LLM agreement with human annotators. We conclude with a discussion of the implications for researchers and practitioners using LLMs for automated data annotation tasks. Specifically, we emphasize that fairness evaluations must be contextual, model choice alone will not solve potential issues of bias, and item difficulty must be integrated into bias assessments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23243v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Megan A. Brown, Shubham Atreja, Libby Hemphill, Patrick Y. Wu</dc:creator>
    </item>
  </channel>
</rss>
