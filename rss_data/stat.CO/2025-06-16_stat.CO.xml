<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 17 Jun 2025 02:27:57 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Filtrated Grouping in Multiple Functional Regression</title>
      <link>https://arxiv.org/abs/2506.11369</link>
      <description>arXiv:2506.11369v1 Announce Type: cross 
Abstract: In this article, we develop a novel covariate grouping framework in the context of multiple functional regression, in which a scalar response is associated with multiple functional covariates. We apply this approach to examine the relationship between chronological age and gait angular kinematics in a cohort of healthy individuals. This application is motivated by the need to understand and communicate the risk of chronic joint disease associated with aging by studying how age influences gait patterns. A key challenge stems from the significant interdependence among various joints, which provides important insights into how movement coordination evolves with aging. This limitation drives the primary objective of this work: to develop an efficient methodology to unravel both the association between chronological age and joint kinematics, and the coordination across different joints. To achieve this goal, we develop a forest-structured covariate grouping framework in which different functional covariates are aggregated hierarchically based on the level of coefficient homogeneity. This approach allows for the analysis of both common and idiosyncratic effects of covariates in a nuanced, multi-resolution manner. The identification of the forest structure is entirely data-driven and requires no prior knowledge, providing valuable insights into the interdependence among covariates. Compared to existing methods, the proposed regression framework demonstrates superior predictive power and offers more insightful interpretability. In addition, the proposed framework is broadly applicable and can be readily extended to analyze other types of multivariate functional data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11369v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuhao Jiao, Hernando Ombao, Ian W. McKeague</dc:creator>
    </item>
    <item>
      <title>Modeling Complex Life Systems: Bayesian Inference for Weibull Failure Times Using Adaptive MCMC</title>
      <link>https://arxiv.org/abs/2506.11949</link>
      <description>arXiv:2506.11949v1 Announce Type: cross 
Abstract: This research develops a Bayesian framework for analyzing failure times using the Weibull distribution, addressing challenges in prior selection due to the lack of conjugate priors and multi-dimensional sufficient statistics. We propose an adaptive semi-parametric MCMC algorithm for lifetime data analysis, employing a hierarchical Bayesian model and the No-U-Turn Sampler (NUTS) in STAN. Twenty-four combinations of prior distributions are evaluated, with a noninformative LogNormal hyper-prior ensuring flexibility. A simulation study of seventy-two datasets with varying structures compares MCMC and classical methods, identifying optimal priors for Bayesian regularization. The approach effectively handles the Increasing Hazard Rate (IHR) and Decreasing Hazard Rate (DHR) scenarios. Finally, we demonstrate the algorithm's utility by predicting the remaining lifetime of prostate cancer patients, showcasing its practical application. This work advances Bayesian methodologies for modeling complex life systems and testing processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11949v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tobias Oketch, Mohammad Sepehrifar</dc:creator>
    </item>
    <item>
      <title>Quantile Slice Sampling</title>
      <link>https://arxiv.org/abs/2407.12608</link>
      <description>arXiv:2407.12608v2 Announce Type: replace 
Abstract: We propose and demonstrate a novel, effective approach to slice sampling. Using the probability integral transform, we first generalize Neal's shrinkage algorithm, standardizing the procedure to an automatic and universal starting point: the unit interval. This enables the introduction of approximate (pseudo-) targets through the factorization used in importance sampling, a technique that popularized elliptical slice sampling, while still sampling from the correct target distribution. Accurate pseudo-targets can boost sampler efficiency by requiring fewer rejections and by reducing skewness in the transformed target. This strategy is effective when a natural, possibly crude approximation to the target exists. Alternatively, obtaining a marginal pseudo-target from initial samples provides an intuitive and automatic tuning procedure. We consider two metrics for evaluating the quality of approximation; each can be used as a criterion to find an optimal pseudo-target or as an interpretable diagnostic. We examine performance of the proposed sampler relative to other popular, easily implemented MCMC samplers on standard targets in isolation, and as steps within a Gibbs sampler in a Bayesian modeling context. We extend the transformation method to multivariate slice samplers and demonstrate with a constrained state-space model for which a readily available forward-backward algorithm provides the target approximation. Supplemental materials and accompanying R package qslice are available online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12608v2</guid>
      <category>stat.CO</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthew J. Heiner, Samuel B. Johnson, Joshua R. Christensen, David B. Dahl</dc:creator>
    </item>
    <item>
      <title>Fast Approximate Solution of Stein Equations for Post-Processing of MCMC</title>
      <link>https://arxiv.org/abs/2501.06634</link>
      <description>arXiv:2501.06634v2 Announce Type: replace 
Abstract: Bayesian inference is conceptually elegant, but calculating posterior expectations can entail a heavy computational cost. Monte Carlo methods are reliable and supported by strong asymptotic guarantees, but do not leverage smoothness of the integrand. Solving Stein equations has emerged as a possible alternative, providing a framework for numerical approximation of posterior expectations in which smoothness can be exploited. However, existing numerical methods for Stein equations are associated with high computational cost due to the need to solve large linear systems. This paper considers the combination of iterative linear solvers and preconditioning strategies to obtain fast approximate solutions of Stein equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06634v2</guid>
      <category>stat.CO</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qingyang Liu, Heishiro Kanagawa, Matthew A. Fisher, Fran\c{c}ois-Xavier Briol, Chris. J. Oates</dc:creator>
    </item>
    <item>
      <title>Spectrally Deconfounded Random Forests</title>
      <link>https://arxiv.org/abs/2502.03969</link>
      <description>arXiv:2502.03969v2 Announce Type: replace 
Abstract: We introduce a modification of Random Forests to estimate functions when unobserved confounding variables are present. The technique is tailored for high-dimensional settings with many observed covariates. We use spectral deconfounding techniques to minimize a deconfounded version of the least squares objective, resulting in the Spectrally Deconfounded Random Forests (SDForests). We show how the omitted variable bias gets small given some assumptions. We compare the performance of SDForests to classical Random Forests in a simulation study and a semi-synthetic setting using single-cell gene expression data. Empirical results suggest that SDForests outperform classical Random Forests in estimating the direct regression function, even if the theoretical assumptions, requiring linear and dense confounding, are not perfectly met, and that SDForests have comparable performance in the non-confounded case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03969v2</guid>
      <category>stat.CO</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Markus Ulmer, Cyrill Scheidegger, Peter B\"uhlmann</dc:creator>
    </item>
    <item>
      <title>pencal: an R Package for the Dynamic Prediction of Survival with Many Longitudinal Predictors</title>
      <link>https://arxiv.org/abs/2309.15600</link>
      <description>arXiv:2309.15600v3 Announce Type: replace-cross 
Abstract: In survival analysis, longitudinal information on the health status of a patient can be used to dynamically update the predicted probability that a patient will experience an event of interest. Traditional approaches to dynamic prediction such as joint models become computationally unfeasible with more than a handful of longitudinal covariates, warranting the development of methods that can handle a larger number of longitudinal covariates. We introduce the R package pencal, which implements a Penalized Regression Calibration (PRC) approach that makes it possible to handle many longitudinal covariates as predictors of survival. pencal uses mixed-effects models to summarize the trajectories of the longitudinal covariates up to a prespecified landmark time, and a penalized Cox model to predict survival based on both baseline covariates and summary measures of the longitudinal covariates. This article illustrates the structure of the R package, provides a step by step example showing how to estimate PRC, compute dynamic predictions of survival and validate performance, and shows how parallelization can be used to significantly reduce computing time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.15600v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>The R Journal, 16 (2), 134-153 (2024)</arxiv:journal_reference>
      <dc:creator>Mirko Signorelli</dc:creator>
    </item>
  </channel>
</rss>
