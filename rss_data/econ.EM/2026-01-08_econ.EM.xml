<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 09 Jan 2026 02:42:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Minimax regret treatment rules with finite samples when a quantile is the object of interest</title>
      <link>https://arxiv.org/abs/2601.03428</link>
      <description>arXiv:2601.03428v1 Announce Type: new 
Abstract: Consider a setup in which a decision maker is informed about the population by a finite sample and based on that sample has to decide whether or not to apply a certain treatment. We work out finite sample minimax regret treatment rules under various sampling schemes when outcomes are restricted onto the unit interval. In contrast to Stoye (2009) where the focus is on maximization of expected utility the focus here is instead on a particular quantile of the outcome distribution. We find that in the case where the sample consists of a fixed number of untreated and a fixed number of treated units, any treatment rule is minimax regret optimal. The same is true in the case of random treatment assignment in the sample with any assignment probability and in the case of testing an innovation when the known quantile of the untreated population equals 1/2. However if the known quantile exceeds 1/2 then never treating is the unique optimal rule and if it is smaller than 1/2 always treating is optimal. We also consider the case where a covariate is included.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03428v1</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patrik Guggenberger, Nihal Mehta, Nikita Pavlov</dc:creator>
    </item>
    <item>
      <title>Content vs. Form: What Drives the Writing Score Gap Across Socioeconomic Backgrounds? A Generated Panel Approach</title>
      <link>https://arxiv.org/abs/2601.03469</link>
      <description>arXiv:2601.03469v1 Announce Type: new 
Abstract: Students from different socioeconomic backgrounds exhibit persistent gaps in test scores, gaps that can translate into unequal educational and labor-market outcomes later in life. In many assessments, performance reflects not only what students know, but also how effectively they can communicate that knowledge. This distinction is especially salient in writing assessments, where scores jointly reward the substance of students' ideas and the way those ideas are expressed. As a result, observed score gaps may conflate differences in underlying content with differences in expressive skill. A central question, therefore, is how much of the socioeconomic-status (SES) gap in scores is driven by differences in what students say versus how they say it. We study this question using a large corpus of persuasive essays written by U.S. middle- and high-school students. We introduce a new measurement strategy that separates content from style by leveraging large language models to generate multiple stylistic variants of each essay. These rewrites preserve the underlying arguments while systematically altering surface expression, creating a "generated panel" that introduces controlled within-essay variation in style. This approach allows us to decompose SES gaps in writing scores into contributions from content and style. We find an SES gap of 0.67 points on a 1-6 scale. Approximately 69% of the gap is attributable to differences in essay content quality, Style differences account for 26% of the gap, and differences in evaluation standards across SES groups account for the remaining 5%. These patterns seems stable across demographic subgroups and writing tasks. More broadly, our approach shows how large language models can be used to generate controlled variation in observational data, enabling researchers to isolate and quantify the contributions of otherwise entangled factors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03469v1</guid>
      <category>econ.EM</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nadav Kunievsky, Pedro Pertusi</dc:creator>
    </item>
    <item>
      <title>Uncovering Sparse Financial Networks with Information Criteria</title>
      <link>https://arxiv.org/abs/2601.03598</link>
      <description>arXiv:2601.03598v1 Announce Type: new 
Abstract: Empirical measures of financial connectedness based on Forecast Error Variance Decompositions (FEVDs) often yield dense network structures that obscure true transmission channels and complicate the identification of systemic risk. This paper proposes a novel information-criterion-based approach to uncover sparse, economically meaningful financial networks. By reformulating FEVD-based connectedness as a regression problem, we develop a model selection framework that consistently recovers the active set of spillover channels. We extend this method to generalized FEVDs to accommodate correlated shocks and introduce a data-driven procedure for tuning the penalty parameter using pseudo-out-of-sample forecast performance. Monte Carlo simulations demonstrate the approach's effectiveness with finite samples and its robustness to approximately sparse networks and heavy-tailed errors. Applications to global stock markets, S&amp;P 500 sectoral indices, and commodity futures highlight the prevalence of sparse networks in empirical settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03598v1</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fu Ouyang, Thomas T. Yang, Wenying Yao</dc:creator>
    </item>
    <item>
      <title>Multivariate kernel regression in vector and product metric spaces</title>
      <link>https://arxiv.org/abs/2601.03750</link>
      <description>arXiv:2601.03750v1 Announce Type: new 
Abstract: This paper derives limit properties of nonparametric kernel regression estimators without requiring existence of density for regressors in $\mathbb{R}^{q}.$ In functional regression limit properties are established for multivariate functional regression. The rate and asymptotic normality for the Nadaraya-Watson (NW) estimator is established for distributions of regressors in $\mathbb{R}^{q}$ that allow for mass points, factor structure, multicollinearity and nonlinear dependence, as well as fractal distribution; when bounded density exists we provide statistical guarantees for the standard rate and the asymptotic normality without requiring smoothness. We demonstrate faster convergence associated with dimension reducing types of singularity, such as a fractal distribution or a factor structure in the regressors. The paper extends asymptotic normality of kernel functional regression to multivariate regression over a product of any number of metric spaces. Finite sample evidence confirms rate improvement due to singularity in regression over $\mathbb{R}^{q}.$ For functional regression the simulations underline the importance of accounting for multiple functional regressors. We demonstrate the applicability and advantages of the NW estimator in our empirical study, which reexamines the job training program evaluation based on the LaLonde data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03750v1</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jeconom.2025.106168</arxiv:DOI>
      <dc:creator>Marcia Schafgans, Victoria Zinde-Walsh</dc:creator>
    </item>
    <item>
      <title>Reverse Stress Testing Geopolitical Risk in Corporate Credit Portfolios: A Formal and Operational Framework</title>
      <link>https://arxiv.org/abs/2601.03983</link>
      <description>arXiv:2601.03983v1 Announce Type: new 
Abstract: This paper proposes a formal framework for reverse stress testing geopolitical risk in corporate credit portfolios. A joint macro-financial scenario vector, augmented with an explicit geopolitical risk factor, is mapped into stressed probabilities of default and losses given default. These stresses are then propagated to portfolio tail losses through a latent factor structure and translated into a stressed CET1 ratio, jointly accounting for capital depletion and risk-weighted asset dynamics. Reverse stress testing is formulated as a constrained maximum likelihood problem over the scenario space. This yields a geopolitical point reverse stress test, or design point, defined as the most probable scenario that breaches a prescribed capital adequacy constraint under a reference distribution. The framework further characterises neighbourhoods and near optimal sets of reverse stress scenarios, allowing for sensitivity analysis and governance oriented interpretation. The approach is compatible with internal rating based models and supports implementation at the exposure or sector level.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03983v1</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christophe Hurlin, Quentin Lajaunie, Yoann Pull</dc:creator>
    </item>
    <item>
      <title>Mean Square Errors of factors extracted using principal components, linear projections, and Kalman filter</title>
      <link>https://arxiv.org/abs/2601.04087</link>
      <description>arXiv:2601.04087v1 Announce Type: new 
Abstract: Factor extraction from systems of variables with a large cross-sectional dimension, $N$, is often based on either Principal Components (PC)-based procedures, or Kalman filter (KF)-based procedures. Measuring the uncertainty of the extracted factors is important when, for example, they have a direct interpretation and/or they are used to summarized the information in a large number of potential predictors. In this paper, we compare the finite $N$ mean square errors (MSEs) of PC and KF factors extracted under different structures of the idiosyncratic cross-correlations. We show that the MSEs of PC-based factors, implicitly based on treating the true underlying factors as deterministic, are larger than the corresponding MSEs of KF factors, obtained by treating the true factors as either serially independent or autocorrelated random variables. We also study and compare the MSEs of PC and KF factors estimated when the idiosyncratic components are wrongly considered as if they were cross-sectionally homoscedastic and/or uncorrelated. The relevance of the results for the construction of confidence intervals for the factors are illustrated with simulated data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.04087v1</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matteo Barigozzi, Diego Fresoli, Esther Ruiz</dc:creator>
    </item>
    <item>
      <title>Ridge Estimation of High Dimensional Two-Way Fixed Effect Regression</title>
      <link>https://arxiv.org/abs/2601.04101</link>
      <description>arXiv:2601.04101v1 Announce Type: new 
Abstract: We study a ridge estimator for the high-dimensional two-way fixed effect regression model with a sparse bipartite network. We develop concentration inequalities showing that when the ridge parameters increase as the log of the network size, the bias, and the variance-covariance matrix of the vector of estimated fixed effects converge to deterministic equivalents that depend only on the expected network. We provide simulations and an application using administrative data on wages for worker-firm matches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.04101v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junnan He, Jean-Marc Robin</dc:creator>
    </item>
    <item>
      <title>Governance of Technological Transition: A Predator-Prey Analysis of AI Capital in China's Economy and Its Policy Implications</title>
      <link>https://arxiv.org/abs/2601.03547</link>
      <description>arXiv:2601.03547v1 Announce Type: cross 
Abstract: The rapid integration of Artificial Intelligence (AI) into China's economy presents a classic governance challenge: how to harness its growth potential while managing its disruptive effects on traditional capital and labor markets. This study addresses this policy dilemma by modeling the dynamic interactions between AI capital, physical capital, and labor within a Lotka-Volterra predator-prey framework. Using annual Chinese data (2016-2023), we quantify the interaction strengths, identify stable equilibria, and perform a global sensitivity analysis. Our results reveal a consistent pattern where AI capital acts as the 'prey', stimulating both physical capital accumulation and labor compensation (wage bill), while facing only weak constraining feedback. The equilibrium points are stable nodes, indicating a policy-mediated convergence path rather than volatile cycles. Critically, the sensitivity analysis shows that the labor market equilibrium is overwhelmingly driven by AI-related parameters, whereas the physical capital equilibrium is also influenced by its own saturation dynamics. These findings provide a systemic, quantitative basis for policymakers: (1) to calibrate AI promotion policies by recognizing the asymmetric leverage points in capital vs. labor markets; (2) to anticipate and mitigate structural rigidities that may arise from current regulatory settings; and (3) to prioritize interventions that foster complementary growth between AI and traditional economic structures while ensuring broad-base distribution of technological gains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03547v1</guid>
      <category>econ.GN</category>
      <category>cs.CY</category>
      <category>econ.EM</category>
      <category>q-fin.EC</category>
      <category>stat.ME</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kunpeng Wang, Jiahui Hu</dc:creator>
    </item>
    <item>
      <title>Nickell Bias in Panel Local Projection: Financial Crises Are Worse Than You Think</title>
      <link>https://arxiv.org/abs/2302.13455</link>
      <description>arXiv:2302.13455v5 Announce Type: replace 
Abstract: Panel local projection (LP) with fixed-effects (FE) is widely adopted for evaluating the economic consequences of financial crises across countries. This paper highlights a fundamental methodological issue: the presence of the Nickell bias in the panel FE estimator due to inherent dynamic structures of predictive specifications, even if the regressors have no lagged dependent variables. The Nickell bias invalidates the standard inferential procedure based on the $t$-statistic. We propose a split-panel jackknife (SPJ) estimator as a simple, easy-to-implement, and yet effective solution to eliminate the bias and restore valid statistical inference. We revisit four influential empirical studies on the impact of financial crises, and find that the FE method underestimates the economic losses of financial crises relative to the SPJ estimates. Replication files are available at https://metricshilab.github.io/panel-lp-replication/, with links to R and Stata packages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.13455v5</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziwei Mei, Liugang Sheng, Zhentao Shi</dc:creator>
    </item>
    <item>
      <title>Kernel Choice Matters for Local Polynomial Density Estimators at Boundaries</title>
      <link>https://arxiv.org/abs/2306.07619</link>
      <description>arXiv:2306.07619v4 Announce Type: replace 
Abstract: Local polynomial density (LPD) estimators are widely used for inference on boundary features of the density function. Contrary to conventional wisdom, we show that kernel choice substantially affects efficiency. Theory, simulations, and empirical evidence indicate that the popular triangular kernel delivers large mean squared error, wide confidence intervals, and limited power for detecting discontinuities. Moreover, small-sample variance can explode because the finite-sample variance is infinite under compactly supported kernels. As a simple yet powerful remedy, we recommend using the Gaussian or Laplace kernels. These alternatives yield marked efficiency gains and eliminate variance explosions, improving the reliability of LPD-based inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.07619v4</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shunsuke Imai, Yuta Okamoto</dc:creator>
    </item>
    <item>
      <title>Simple Inference on a Simplex-Valued Weight</title>
      <link>https://arxiv.org/abs/2501.15692</link>
      <description>arXiv:2501.15692v2 Announce Type: replace 
Abstract: In many applications, the parameter of interest involves a simplex-valued weight which is identified as a solution to an optimization problem. Examples include synthetic control methods with group-level weights and various methods of model averaging and forecast combinations. The simplex constraint on the weight poses a challenge in statistical inference due to the constraint potentially binding. In this paper, we propose a simple method of constructing a confidence set for the weight using an adaptive test based on the projection on a polyhedral cone and prove that the method is asymptotically uniformly valid. The procedure does not require tuning parameters or simulations to compute critical values. The confidence set accommodates both the cases of point-identification or set-identification of the weight. We illustrate the method with an empirical example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15692v2</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nathan Canen, Kyungchul Song</dc:creator>
    </item>
    <item>
      <title>Causal Inference for Aggregated Treatment</title>
      <link>https://arxiv.org/abs/2506.22885</link>
      <description>arXiv:2506.22885v2 Announce Type: replace 
Abstract: In this paper, we study causal inference when the treatment variable is an aggregation of multiple sub-treatment variables. Researchers often report marginal causal effects for the aggregated treatment, implicitly assuming that the target parameter corresponds to a well-defined average of sub-treatment effects. We show that, even in an ideal scenario for causal inference such as random assignment, the weights underlying this average have some key undesirable properties: they are not unique, they can be negative, and, holding all else constant, these issues become exponentially more likely to occur as the number of sub-treatments increases and the support of each sub-treatment grows. We propose approaches to avoid these problems, depending on whether or not the sub-treatment variables are observed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22885v2</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carolina Caetano, Gregorio Caetano, Brantly Callaway, Derek Dyal</dc:creator>
    </item>
    <item>
      <title>Statistical Inference in Large Multi-way Networks</title>
      <link>https://arxiv.org/abs/2512.02203</link>
      <description>arXiv:2512.02203v2 Announce Type: replace 
Abstract: We propose a new method to estimate structural parameters in multi-way networks while controlling for rich structures of fixed effects. The method is based on a series of classification tasks and is agnostic to both the number and structure of fixed effects. In contrast to full maximum likelihood approaches, our estimator does not suffer from the incidental parameter problem. For sparsely connected networks, it is also computationally faster than PPML. We provide empirical evidence that our estimator yields more reliable confidence intervals than PPML and its bias-correction strategies. These improvements hold even under model misspecification and are more pronounced in sparse settings. While PPML remains competitive in dense, low-dimensional data, our approach offers a robust alternative for multi-way models that scales efficiently with sparsity. The method is applied to study the causal effect of a policy reform on spatial accessibility to health care in France.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02203v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas Resende, Guillaume Lecu\'e, Lionel Wilner, Philippe Chon\'e</dc:creator>
    </item>
    <item>
      <title>LASSO Inference for High Dimensional Predictive Regressions</title>
      <link>https://arxiv.org/abs/2409.10030</link>
      <description>arXiv:2409.10030v3 Announce Type: replace-cross 
Abstract: LASSO inflicts shrinkage bias on estimated coefficients, which undermines asymptotic normality and invalidates standard inferential procedures based on the t-statistic. Given cross sectional data, the desparsified LASSO has emerged as a well-known remedy for correcting the shrinkage bias. In the context of high dimensional predictive regression, the desparsified LASSO faces an additional challenge: the Stambaugh bias arising from nonstationary regressors modeled as local unit roots. To restore standard inference, we propose a novel estimator called IVX-desparsified LASSO (XDlasso). XDlasso simultaneously eliminates both shrinkage bias and Stambaugh bias and does not require prior knowledge about the identities of nonstationary and stationary regressors. We establish the asymptotic properties of XDlasso for hypothesis testing, and our theoretical findings are supported by Monte Carlo simulations. Applying our method to real-world applications from the FRED-MD database, we investigate two important empirical questions: (i) the predictability of the U.S. stock returns based on the earnings-price ratio, and (ii) the predictability of the U.S. inflation using the unemployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10030v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhan Gao, Ji Hyung Lee, Ziwei Mei, Zhentao Shi</dc:creator>
    </item>
    <item>
      <title>Estimation of large approximate dynamic matrix factor models based on the EM algorithm and Kalman filtering</title>
      <link>https://arxiv.org/abs/2502.04112</link>
      <description>arXiv:2502.04112v3 Announce Type: replace-cross 
Abstract: This paper considers an approximate dynamic matrix factor model that accounts for the time series nature of the data by explicitly modelling the time evolution of the factors. We study estimation of the model parameters based on the Expectation Maximization (EM) algorithm, implemented jointly with the Kalman smoother which gives estimates of the factors. We establish the consistency of the estimated loadings and factor matrices as the sample size $T$ and the matrix dimensions $p_1$ and $p_2$ diverge to infinity. We then extend this approach to: (a) the case of arbitrary patterns of missing data and (b) the presence of common stochastic trends. The finite sample properties of the estimators are assessed through a large simulation study and two applications on: (i) a financial dataset of volatility proxies and (ii) a macroeconomic dataset covering the main euro area countries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04112v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matteo Barigozzi, Luca Trapin</dc:creator>
    </item>
    <item>
      <title>Ill-Conditioned Orthogonal Scores in Double Machine Learning</title>
      <link>https://arxiv.org/abs/2512.07083</link>
      <description>arXiv:2512.07083v2 Announce Type: replace-cross 
Abstract: Double Machine Learning is often justified by nuisance-rate conditions, yet finite-sample reliability also depends on the conditioning of the orthogonal-score Jacobian. This conditioning is typically assumed rather than tracked. When residualized treatment variance is small, the Jacobian is ill-conditioned and small systematic nuisance errors can be amplified, so nominal confidence intervals may look precise yet systematically under-cover. Our main result is an exact identity for the cross-fitted PLR-DML estimator, with no Taylor approximation. From this identity, we derive a stochastic-order bound that separates oracle noise from a conditioning-amplified nuisance remainder and yields a sufficiency condition for root-n-inference. We further connect the amplification factor to semiparametric efficiency geometry via the Riesz representer and use a triangular-array framework to characterize regimes as residual treatment variation weakens. These results motivate an out-of-fold diagnostic that summarizes the implied amplification scale. We do not propose universal thresholds. Instead, we recommend reporting the diagnostic alongside cross-learner sensitivity summaries as a fragility assessment, illustrated in simulation and an empirical example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07083v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriel Saco</dc:creator>
    </item>
    <item>
      <title>Exponentially weighted estimands and the exponential family: filtering, prediction and smoothing</title>
      <link>https://arxiv.org/abs/2512.16745</link>
      <description>arXiv:2512.16745v2 Announce Type: replace-cross 
Abstract: We propose using a discounted version of a convex combination of the log-likelihood with the corresponding expected log-likelihood such that when they are maximized they yield a filter, predictor and smoother for time series. This paper then focuses on working out the implications of this in the case of the canonical exponential family. The results are simple exact filters, predictors and smoothers with linear recursions. A theory for these models is developed and the models are illustrated on simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16745v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simon Donker van Heel, Neil Shephard</dc:creator>
    </item>
  </channel>
</rss>
