<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 25 Jul 2025 04:02:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Partitioned Wild Bootstrap for Panel Data Quantile Regression</title>
      <link>https://arxiv.org/abs/2507.18494</link>
      <description>arXiv:2507.18494v1 Announce Type: new 
Abstract: Practical inference procedures for quantile regression models of panel data have been a pervasive concern in empirical work, and can be especially challenging when the panel is observed over many time periods and temporal dependence needs to be taken into account. In this paper, we propose a new bootstrap method that applies random weighting to a partition of the data -- partition-invariant weights are used in the bootstrap data generating process -- to conduct statistical inference for conditional quantiles in panel data that have significant time-series dependence. We demonstrate that the procedure is asymptotically valid for approximating the distribution of the fixed effects quantile regression estimator. The bootstrap procedure offers a viable alternative to existing resampling methods. Simulation studies show numerical evidence that the novel approach has accurate small sample behavior, and an empirical application illustrates its use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18494v1</guid>
      <category>econ.EM</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antonio F. Galvao, Carlos Lamarche, Thomas Parker</dc:creator>
    </item>
    <item>
      <title>How weak are weak factors? Uniform inference for signal strength in signal plus noise models</title>
      <link>https://arxiv.org/abs/2507.18554</link>
      <description>arXiv:2507.18554v1 Announce Type: cross 
Abstract: The paper analyzes four classical signal-plus-noise models: the factor model, spiked sample covariance matrices, the sum of a Wigner matrix and a low-rank perturbation, and canonical correlation analysis with low-rank dependencies. The objective is to construct confidence intervals for the signal strength that are uniformly valid across all regimes - strong, weak, and critical signals. We demonstrate that traditional Gaussian approximations fail in the critical regime. Instead, we introduce a universal transitional distribution that enables valid inference across the entire spectrum of signal strengths. The approach is illustrated through applications in macroeconomics and finance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18554v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anna Bykhovskaya, Vadim Gorin, Sasha Sodin</dc:creator>
    </item>
    <item>
      <title>Local Polynomial Estimation of Time-Varying Parameters in Nonlinear Models</title>
      <link>https://arxiv.org/abs/1904.05209</link>
      <description>arXiv:1904.05209v3 Announce Type: replace 
Abstract: We develop a novel asymptotic theory for local polynomial extremum estimators of time-varying parameters in a broad class of nonlinear time series models. We show the proposed estimators are consistent and follow normal distributions in large samples under weak conditions. We also provide a precise characterisation of the leading bias term due to smoothing, which has not been done before. We demonstrate the usefulness of our general results by establishing primitive conditions for local (quasi-)maximum-likelihood estimators of time-varying models threshold autoregressions, ARCH models and Poisson autogressions with exogenous co--variates, to be normally distributed in large samples and characterise their leading biases. An empirical study of US corporate default counts demonstrates the applicability of the proposed local linear estimator for Poisson autoregression, shedding new light on the dynamic properties of US corporate defaults.</description>
      <guid isPermaLink="false">oai:arXiv.org:1904.05209v3</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dennis Kristensen, Young Jun Lee</dc:creator>
    </item>
    <item>
      <title>Normalizations and misspecification in skill formation models</title>
      <link>https://arxiv.org/abs/2104.00473</link>
      <description>arXiv:2104.00473v3 Announce Type: replace 
Abstract: An important class of structural models studies the determinants of skill formation and the optimal timing of interventions. In this paper, I provide new identification results for these models and investigate the effects of seemingly innocuous scale and location restrictions on parameters of interest. To do so, I first characterize the identified set of all parameters without these additional restrictions and show that important policy-relevant parameters are point identified under weaker assumptions than commonly used in the literature. The implications of imposing standard scale and location restrictions depend on how the model is specified, but they generally impact the interpretation of parameters and may affect counterfactuals. Importantly, with the popular CES production function, commonly used scale restrictions fix identified parameters and lead to misspecification. Consequently, simply changing the units of measurements of observed variables might yield ineffective investment strategies and misleading policy recommendations. I show how existing estimators can easily be adapted to solve these issues. As a byproduct, this paper also presents a general and formal definition of when restrictions are truly normalizations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2104.00473v3</guid>
      <category>econ.EM</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joachim Freyberger</dc:creator>
    </item>
    <item>
      <title>Estimation and Inference of Average Treatment Effect in Percentage Points under Heterogeneity</title>
      <link>https://arxiv.org/abs/2408.06624</link>
      <description>arXiv:2408.06624v2 Announce Type: replace 
Abstract: In semi-logarithmic regressions, treatment coefficients are often interpreted as approximations of the average treatment effect (ATE) in percentage points. This paper highlights the overlooked bias of this approximation under treatment effect heterogeneity, arising from Jensen's inequality. The issue is particularly relevant for difference-in-differences designs with log-transformed outcomes and staggered treatment adoption, where treatment effects often vary across groups and periods. I propose new estimation and inference methods for the ATE in percentage points, which are applicable when treatment effects vary across and within groups. I establish the methods' large-sample properties and demonstrate their finite-sample performance through simulations, revealing substantial discrepancies between conventional and proposed measures. Two empirical applications further underscore the practical importance of these methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06624v2</guid>
      <category>econ.EM</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ying Zeng</dc:creator>
    </item>
    <item>
      <title>Inference on High Dimensional Selective Labeling Models</title>
      <link>https://arxiv.org/abs/2410.18381</link>
      <description>arXiv:2410.18381v3 Announce Type: replace 
Abstract: A class of simultaneous equation models arise in the many domains where observed binary outcomes are themselves a consequence of the existing choices of of one of the agents in the model. These models are gaining increasing interest in the computer science and machine learning literatures where they refer the potentially endogenous sample selection as the {\em selective labels} problem. Empirical settings for such models arise in fields as diverse as criminal justice, health care, and insurance. For important recent work in this area, see for example Lakkaruju et al. (2017), Kleinberg et al. (2018), and Coston et al.(2021) where the authors focus on judicial bail decisions, and where one observes the outcome of whether a defendant filed to return for their court appearance only if the judge in the case decides to release the defendant on bail. Identifying and estimating such models can be computationally challenging for two reasons. One is the nonconcavity of the bivariate likelihood function, and the other is the large number of covariates in each equation. Despite these challenges, in this paper we propose a novel distribution free estimation procedure that is computationally friendly in many covariates settings. The new method combines the semiparametric batched gradient descent algorithm introduced in Khan et al.(2023) with a novel sorting algorithms incorporated to control for selection bias. Asymptotic properties of the new procedure are established under increasing dimension conditions in both equations, and its finite sample properties are explored through a simulation study and an application using judicial bail data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18381v3</guid>
      <category>econ.EM</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Shakeeb Khan, Elie Tamer, Qingsong Yao</dc:creator>
    </item>
    <item>
      <title>Good Controls Gone Bad: Difference-in-Differences with Covariates</title>
      <link>https://arxiv.org/abs/2412.14447</link>
      <description>arXiv:2412.14447v2 Announce Type: replace 
Abstract: This paper introduces the two-way common causal covariates (CCC) assumption, which is necessary to get an unbiased estimate of the ATT when using time-varying covariates in existing Difference-in-Differences methods. The two-way CCC assumption implies that the effect of the covariates remain the same between groups and across time periods. This assumption has been implied in previous literature, but has not been explicitly addressed. Through theoretical proofs and a Monte Carlo simulation study, we show that the standard TWFE and the CS-DID estimators are biased when the two-way CCC assumption is violated. We propose a new estimator called the Intersection Difference-in-differences (DID-INT) which can provide an unbiased estimate of the ATT under two-way CCC violations. DID-INT can also identify the ATT under heterogeneous treatment effects and with staggered treatment rollout. The estimator relies on parallel trends of the residuals of the outcome variable, after appropriately adjusting for covariates. This covariate residualization can recover parallel trends that are hidden with conventional estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14447v2</guid>
      <category>econ.EM</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sunny Karim, Matthew D. Webb</dc:creator>
    </item>
    <item>
      <title>A Simulated Reconstruction and Reidentification Attack on the 2010 U.S. Census: Full Technical Report</title>
      <link>https://arxiv.org/abs/2312.11283</link>
      <description>arXiv:2312.11283v2 Announce Type: replace-cross 
Abstract: Statistical agencies routinely use different strategies to protect the confidentiality of tabular data from those used to protect the individual records in publicly released microdata. Aggregation is assumed to make the resulting statistics inherently less disclosive than the microdata. The 2010 U.S. Census used different disclosure limitation rules for its tabular and microdata publications. We show that the assumption that these tabular data are inherently less disclosive than their underlying microdata is wrong. The 2010 Census published more than 150 billion statistics in 180 table sets, almost all at the most detailed geographic level -- individual census blocks. Using only 34 of the published table sets, we reconstructed microdata for five variables (census block, sex, age, race, and ethnicity). Using only published data, an attacker using our methods can verify that all records in 70% of all census blocks (97 million people) are perfectly reconstructed. We confirm through reidentification studies that an attacker can, within census blocks with perfect reconstruction accuracy, correctly infer the actual census response on race and ethnicity for 3.4 million vulnerable people (unique persons with race and ethnicity different from the modal person on the census block) with 95\% accuracy. Next, we show that the more robust disclosure limitation framework used for the 2020 U.S. Census defends against attacks that are based on reconstruction. Finally, we show that available alternatives to the 2020 Census Disclosure Avoidance System would either fail to protect confidentiality or overly degrade the statistics' utility for the primary statutory use case: redrawing the boundaries of all the nation's legislative and voting districts in compliance with the 1965 Voting Rights Act. This is the full technical report. For the summary paper see https://doi.org/10.1162/99608f92.4a1ebf70.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.11283v2</guid>
      <category>stat.AP</category>
      <category>cs.CR</category>
      <category>econ.EM</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1162/99608f92.4a1ebf70</arxiv:DOI>
      <dc:creator>John M. Abowd, Tamara Adams, Robert Ashmead, David Darais, Sourya Dey, Simson L. Garfinkel, Nathan Goldschlag, Daniel Kifer, Philip Leclerc, Ethan Lew, Scott Moore, Rolando A. Rodr\'iguez, Ramy N. Tadros, Lars Vilhuber</dc:creator>
    </item>
  </channel>
</rss>
