<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.LO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.LO</link>
    <description>cs.LO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.LO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 16 Oct 2025 01:45:37 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Ground Stratification for a Logic of Definitions with Induction</title>
      <link>https://arxiv.org/abs/2510.12297</link>
      <description>arXiv:2510.12297v1 Announce Type: new 
Abstract: The logic underlying the Abella proof assistant includes mechanisms for interpreting atomic predicates through fixed point definitions that can additionally be treated inductively or co-inductively. However, the original formulation of the logic includes a strict stratification condition on definitions that is too restrictive for some applications such as those that use a logical relations based approach to semantic equivalence. Tiu has shown how this restriction can be eased by utilizing a weaker notion referred to as ground stratification. Tiu's results were limited to a version of the logic that does not treat inductive definitions. We show here that they can be extended to cover such definitions. While our results are obtained by using techniques that have been previously deployed in related ways in this context, their use is sensitive to the particular way in which we generalize the logic. In particular, although ground stratification may be used with arbitrary fixed-point definitions, we show that weakening stratification to this form for inductive definitions leads to inconsistency. The particular generalization we describe accords well with the way logical relations are used in practice. Our results are also a intermediate step to building a more flexible form for definitions into the full logic underlying Abella, which additionally includes co-induction, generic quantification, and a mechanism referred to as nominal abstraction for analyzing occurrences of objects in terms that are governed by generic quantifiers.
</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12297v1</guid>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.431.1</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 431, 2025, pp. 1-16</arxiv:journal_reference>
      <dc:creator>Nathan Guermond (University of Minnesota), Gopalan Nadathur (University of Minnesota)</dc:creator>
    </item>
    <item>
      <title>Flavors of Quantifiers in Hyperlogics</title>
      <link>https://arxiv.org/abs/2510.12298</link>
      <description>arXiv:2510.12298v1 Announce Type: new 
Abstract: Hypertrace logic is a sorted first-order logic with separate sorts for time and execution traces. Its formulas specify hyperproperties, which are properties relating multiple traces. In this work, we extend hypertrace logic by introducing trace quantifiers that range over the set of all possible traces. In this extended logic, formulas can quantify over two kinds of trace variables: constrained trace variables, which range over a fixed set of traces defined by the model, and unconstrained trace variables, which can be assigned to any trace. In comparison, hyperlogics such as HyperLTL have only constrained trace quantifiers. We use hypertrace logic to study how different quantifier patterns affect the decidability of the satisfiability problem. We prove that hypertrace logic without constrained trace quantifiers is equivalent to monadic second-order logic of one successor (S1S), and therefore satisfiable, and that the trace-prefixed fragment (all trace quantifiers precede all time quantifiers) is equivalent to HyperQPTL. Moreover, we show that all hypertrace formulas where the only alternation between constrained trace quantifiers is from an existential to a universal quantifier are equisatisfiable to formulas without constraints on their trace variables and, therefore, decidable as well. Our framework allows us to study also time-prefixed hyperlogics, for which we provide new decidability and undecidability results</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12298v1</guid>
      <category>cs.LO</category>
      <category>cs.FL</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marek Chalupa, Thomas A. Henzinger, Ana Oliveira da Costa</dc:creator>
    </item>
    <item>
      <title>On the Formal Metatheory of the Pure Type Systems using One-sorted Variable Names and Multiple Substitutions</title>
      <link>https://arxiv.org/abs/2510.12300</link>
      <description>arXiv:2510.12300v1 Announce Type: new 
Abstract: We develop formal theories of conversion for Church-style lambda-terms with Pi-types in first-order syntax using one-sorted variables names and Stoughton's multiple substitutions. We then formalize the Pure Type Systems along some fundamental metatheoretic properties: weakening, syntactic validity, closure under alpha-conversion and substitution. Finally, we compare our formalization with others related. The whole development has been machine-checked using the Agda system.  Our work demonstrates that the mechanization of dependent type theory by using conventional syntax and without identifying alpha-convertible lambda-terms is feasible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12300v1</guid>
      <category>cs.LO</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.431.2</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 431, 2025, pp. 17-33</arxiv:journal_reference>
      <dc:creator>Sebasti\'an Urciuoli (Universidad ORT Uruguay)</dc:creator>
    </item>
    <item>
      <title>CoLF Logic Programming as Infinitary Proof Exploration</title>
      <link>https://arxiv.org/abs/2510.12302</link>
      <description>arXiv:2510.12302v1 Announce Type: new 
Abstract: Logical Frameworks such as Automath [de Bruijn, 1968] or LF [Harper et al., 1993] were originally conceived as metalanguages for the specification of foundationally uncommitted deductive systems, yielding generic proof checkers. Their high level of abstraction was soon exploited to also express algorithms over deductive systems such as theorem provers, type-checkers, evaluators, compilers, proof transformers, etc. in the paradigm of computation-as-proof-construction. This has been realized in languages such as $\lambda$-Prolog [Miller et al., 1991] or Elf [Pfenning, 1991] based on backward chaining, and LolliMon [Lopez et al., 2005] or Celf [Schack-Nielsen and Schuermann, 2008], which integrated forward chaining. None of these early frameworks supported the direct expression of infinitary objects or proofs, which are available in the recently developed CoLF$^\omega$ [Chen, 2023]. In this work-in-progress report, we sketch an approach to computation-as-proof-construction over the first-order fragment of CoLF$^\omega$ (called CoLF$^\omega_1$ ) that already includes infinitary objects and proofs. A key idea is the interpretation of logic variables as communication channels and computation as concurrent message-passing. This is realized in a concrete compiler from CoLF$^\omega_1$ to Sax, a proof-theoretically inspired parallel programming language based on the proof-reduction in the semi-axiomatic sequent calculus [DeYoung et al., 2020].</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12302v1</guid>
      <category>cs.LO</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.431.3</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 431, 2025, pp. 34-41</arxiv:journal_reference>
      <dc:creator>Zhibo Chen (Carnegie Mellon University), Frank Pfenning (Carnegie Mellon University)</dc:creator>
    </item>
    <item>
      <title>Type Theory with Single Substitutions</title>
      <link>https://arxiv.org/abs/2510.12303</link>
      <description>arXiv:2510.12303v1 Announce Type: new 
Abstract: Type theory can be described as a generalised algebraic theory. This automatically gives a notion of model and the existence of the syntax as the initial model, which is a quotient inductive-inductive type. Algebraic definitions of type theory include Ehrhard's definition of model, categories with families (CwFs), contextual categories, Awodey's natural models, C-systems, B-systems. With the exception of B-systems, these notions are based on a parallel substitution calculus where substitutions form a category. In this paper we define a single substitution calculus (SSC) for type theory and show that the SSC syntax and the CwF syntax are isomorphic for a theory with dependent function space and a hierarchy of universes. SSC only includes single substitutions and single weakenings, and eight equations relating these: four equations describe how to substitute variables, and there are four equations on types which are needed to typecheck the other equations. SSC provides a simple, minimalistic alternative to parallel substitution calculi or B-systems for defining type theory. SSC relates to CwF as extensional combinatory calculus relates to lambda calculus: there are more models of the former, but the syntaxes are equivalent. If we have some additional type formers, we show that an SSC model gives rise to a CwF.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12303v1</guid>
      <category>cs.LO</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.431.4</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 431, 2025, pp. 42-64</arxiv:journal_reference>
      <dc:creator>Ambrus Kaposi (E\"otv\"os Lor\'and University), Szumi Xie (E\"otv\"os Lor\'and University)</dc:creator>
    </item>
    <item>
      <title>Substitution Without Copy and Paste</title>
      <link>https://arxiv.org/abs/2510.12304</link>
      <description>arXiv:2510.12304v1 Announce Type: new 
Abstract: Defining substitution for a language with binders like the simply typed $\lambda$-calculus requires repetition, defining substitution and renaming separately. To verify the categorical properties of this calculus, we must repeat the same argument many times. We present a lightweight method that avoids repetition and that  gives rise to a simply typed category with families (CwF) isomorphic to the initial simply typed CwF. Our paper is a literate Agda script.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12304v1</guid>
      <category>cs.LO</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.431.5</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 431, 2025, pp. 65-81</arxiv:journal_reference>
      <dc:creator>Thorsten Altenkirch (University of Nottingham), Nathaniel Burke (Imperial College London), Philip Wadler (University of Edinburgh)</dc:creator>
    </item>
    <item>
      <title>Dependently Sorted Nominal Signatures</title>
      <link>https://arxiv.org/abs/2510.12305</link>
      <description>arXiv:2510.12305v1 Announce Type: new 
Abstract: We investigate an extension of nominal many-sorted signatures in which abstraction has a form of instantiation, called generalised concretion, as elimination operator (similarly to lambda-calculi). Expressions are then classified using a system of sorts and sort families that respects alpha-conversion (similarly to dependently-typed lambda-calculi) but not allowing names to carry abstraction sorts, thus constituting a first-order dependent sort system. The system can represent forms of judgement and rules of inference of several interesting calculi. We present rules and properties of the system as well as experiments of representation, and discuss how it constitutes a basis on which to build a type theory where raw expressions with alpha-equivalence are given a completely formal treatment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12305v1</guid>
      <category>cs.LO</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.431.6</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 431, 2025, pp. 82-98</arxiv:journal_reference>
      <dc:creator>Maribel Fern\'andez (King s College London, UK), Miguel Pagano (FAMAF - Universidad Nacional de C\'ordoba, Argentina), Nora Szasz (Universidad ORT Uruguay), \'Alvaro Tasistro (Universidad ORT Uruguay)</dc:creator>
    </item>
    <item>
      <title>Proceedings of the International Workshop on Verification of Scientific Software</title>
      <link>https://arxiv.org/abs/2510.12314</link>
      <description>arXiv:2510.12314v1 Announce Type: new 
Abstract: This volume contains the proceedings of the Verification of Scientific Software (VSS 2025) workshop, held on 4 May 2025 at McMaster University, Canada, as part of ETAPS 2025. VSS brings together researchers in software verification and scientific computing to address challenges in ensuring the correctness and reliability of large-scale scientific codes. The program featured five peer-reviewed papers, three invited contributions, and a set of challenge problems, covering themes such as deductive verification, floating-point error analysis, specification of coupled models, and domain-aware testing. VSS builds on the Correctness Workshop series at Supercomputing and the 2023 NSF/DOE report on scientific software correctness. It serves as yet another snapshot of this important area, showcasing a wide range of perspectives, problems and their solutions in progress, with the challenge problems having the potential to bring together separate verification tools into concerted action.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12314v1</guid>
      <category>cs.LO</category>
      <category>cs.CE</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.432</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 432, 2025</arxiv:journal_reference>
      <dc:creator>Stephen F. Siegel, Ganesh Gopalakrishnan</dc:creator>
    </item>
    <item>
      <title>Operational methods in semantics</title>
      <link>https://arxiv.org/abs/2510.12295</link>
      <description>arXiv:2510.12295v1 Announce Type: cross 
Abstract: The focus of these lecture notes is on abstract models and basic ideas and results that relate to the operational semantics of programming languages largely conceived. The approach is to start with an abstract description of the computation steps of programs and then to build on top semantic equivalences, specification languages, and static analyses. While other approaches to the semantics of programming languages are possible, it appears that the operational one is particularly effective in that it requires a moderate level of mathematical sophistication and scales reasonably well to a large variety of programming features. In practice, operational semantics is a suitable framework to build portable language implementations and to specify and test program properties. It is also used routinely to tackle more ambitious tasks such as proving the correctness of a compiler or a static analyzer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12295v1</guid>
      <category>cs.PL</category>
      <category>cs.LO</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto M. Amadio</dc:creator>
    </item>
    <item>
      <title>First Order Logic and Twin-Width in Tournaments and Dense Oriented Graphs</title>
      <link>https://arxiv.org/abs/2207.07683</link>
      <description>arXiv:2207.07683v5 Announce Type: replace 
Abstract: We characterise the classes of tournaments with tractable first-order model checking. For every hereditary class of tournaments $\mathcal T$, first-order model checking is either fixed parameter tractable or $\textrm{AW}[*]$-hard. This dichotomy coincides with the fact that $\mathcal T$ has either bounded or unbounded twin-width, and that the growth of $\mathcal T$ is either at most exponential or at least factorial. From the model-theoretic point of view, we show that NIP classes of tournaments coincide with bounded twin-width. Twin-width is also characterised by three infinite families of obstructions: $\mathcal T$ has bounded twin-width if and only if it excludes at least one tournament from each family. This generalises results of Bonnet et al.\ on ordered graphs.
  The key for these results is a polynomial time algorithm that takes as input a tournament $T$ and computes a linear order $&lt;$ on $V(T)$ such that the twin-width of the birelation $(T,&lt;)$ is at most some function of the twin-width of $T$. Since approximating twin-width can be done in polynomial time for an ordered structure $(T,&lt;)$, this provides a polynomial time approximation of twin-width for tournaments.
  Our results extend to oriented graphs with stable sets of bounded size, which may also be augmented by arbitrary binary relations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.07683v5</guid>
      <category>cs.LO</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Colin Geniet, St\'ephan Thomass\'e</dc:creator>
    </item>
    <item>
      <title>A Strongly Normalising System of Dependent Types for Transparent and Opaque Probabilistic Computation</title>
      <link>https://arxiv.org/abs/2406.17082</link>
      <description>arXiv:2406.17082v4 Announce Type: replace 
Abstract: We define an extension of lambda-calculus with dependents types that enables us to encode transparent and opaque probabilistic programs and prove a strong normalisation result for it by a reducibility technique. While transparent nondeterministic programs are formalised by rather usual techniques, opaque nondeterministic programs are formalised by introducing in the syntax oracle constants, the behaviour of which is governed by oracular functions. The generality of these functions and the fact that their values are determined by the form of the whole term inside which the relative oracle occurs also enable us to simulate learning-like behaviours. We then extend the calculus in order to define a computational trustworthiness predicate. The extension of the calculus does not only enable us to precisely formalise a notion of trustworthiness and to encode the procedures required to test it on programs, but also to reason, by means of the type system, on the behaviour of programs with respect to trustworthiness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17082v4</guid>
      <category>cs.LO</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francesco A. Genco</dc:creator>
    </item>
    <item>
      <title>Small Term Reachability and Related Problems for Terminating Term Rewriting Systems</title>
      <link>https://arxiv.org/abs/2412.06047</link>
      <description>arXiv:2412.06047v4 Announce Type: replace 
Abstract: Motivated by an application where we try to make proofs for Description Logic inferences smaller by rewriting, we consider the following decision problem, which we call the small term reachability problem: given a term rewriting system $R$, a term $s$, and a natural number $n$, decide whether there is a term $t$ of size $\leq n$ reachable from $s$ using the rules of $R$. We investigate the complexity of this problem depending on how termination of $R$ can be established. We show that the problem is in general NP-complete for length-reducing term rewriting systems. Its complexity increases to N2ExpTime-complete (NExpTime-complete) if termination is proved using a (linear) polynomial order and to PSpace-complete for systems whose termination can be shown using a restricted class of Knuth-Bendix orders. Confluence reduces the complexity to P for the length-reducing case, but has no effect on the worst-case complexity in the other two cases. Finally, we consider the large term reachability problem, a variant of the problem where we are interested in reachability of a term of size $\geq n$. It turns out that this seemingly innocuous modification in some cases changes the complexity of the problem, which may also become dependent on whether the number $n$ is is represented in unary or binary encoding, whereas this makes no difference for the complexity of the small term reachability problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06047v4</guid>
      <category>cs.LO</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Franz Baader, J\"urgen Giesl</dc:creator>
    </item>
    <item>
      <title>On the Existential Theory of the Reals Enriched with Integer Powers of a Computable Number</title>
      <link>https://arxiv.org/abs/2502.02220</link>
      <description>arXiv:2502.02220v3 Announce Type: replace 
Abstract: This paper investigates $\exists\mathbb{R}(r^{\mathbb{Z}})$, that is the extension of the existential theory of the reals by an additional unary predicate $r^{\mathbb{Z}}$ for the integer powers of a fixed computable real number $r &gt; 0$. If all we have access to is a Turing machine computing $r$, it is not possible to decide whether an input formula from this theory satisfiable. However, we show an algorithm to decide this problem when:
  1. $r$ is known to be transcendental, or
  2. $r$ is a root of some given integer polynomial (that is, $r$ is algebraic).
  In other words, knowing the algebraicity of $r$ suffices to circumvent undecidability. Furthermore, we establish complexity results under the proviso that $r$ enjoys what we call a polynomial root barrier. Using this notion, we show that the satisfiability problem of $\exists\mathbb{R}(r^{\mathbb{Z}})$ is
  1. in NEXPTIME if $r$ is a natural number,
  2. in EXPSPACE if $r$ is an algebraic number, and
  3. in 3EXP if $r$ belongs to a family of transcendental numbers including $\pi$ and Euler's $e$.
  As a by-product of our results, we are able to remove the appeal to Schanuel's conjecture from the proof of decidability of the entropic risk threshold problem for stochastic games with rational probabilities, rewards and threshold [Baier et al., MFCS'23]: when the base of the entropic risk is Euler's $e$ and the aversion factor is a fixed algebraic number, the problem is in EXP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02220v3</guid>
      <category>cs.LO</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jorge Gallego-Hern\'andez, Alessio Mansutti</dc:creator>
    </item>
    <item>
      <title>Representations</title>
      <link>https://arxiv.org/abs/2510.11419</link>
      <description>arXiv:2510.11419v2 Announce Type: replace 
Abstract: The formal analysis of automated systems is an important and growing industry. This activity routinely requires new verification frameworks to be developed to tackle new programming features, or new considerations (bugs of interest). Often, one particular property can prove frustrating to establish: completeness of the logic with respect to the semantics. In this paper, we try and make such developments easier, with a particular attention on completeness. Towards that aim, we propose a formal (meta-)model of software analysis systems (SAS), the eponymous Representations. This model requires few assumptions on the SAS being modelled, and as such is able to capture a large class of such systems. We then show how our approach can be fruitful, both to understand how existing completeness proofs can be structured, and to leverage this structure to build new systems and prove their completeness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11419v2</guid>
      <category>cs.LO</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul Brunet (UPEC UP12, LACL)</dc:creator>
    </item>
    <item>
      <title>Can ChatGPT support software verification?</title>
      <link>https://arxiv.org/abs/2311.02433</link>
      <description>arXiv:2311.02433v2 Announce Type: replace-cross 
Abstract: Large language models have become increasingly effective in software engineering tasks such as code generation, debugging and repair. Language models like ChatGPT can not only generate code, but also explain its inner workings and in particular its correctness. This raises the question whether we can utilize ChatGPT to support formal software verification.
  In this paper, we take some first steps towards answering this question. More specifically, we investigate whether ChatGPT can generate loop invariants. Loop invariant generation is a core task in software verification, and the generation of valid and useful invariants would likely help formal verifiers. To provide some first evidence on this hypothesis, we ask ChatGPT to annotate 106 C programs with loop invariants. We check validity and usefulness of the generated invariants by passing them to two verifiers, Frama-C and CPAchecker. Our evaluation shows that ChatGPT is able to produce valid and useful invariants allowing Frama-C to verify tasks that it could not solve before. Based on our initial insights, we propose ways of combining ChatGPT (or large language models in general) and software verifiers, and discuss current limitations and open issues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.02433v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.FL</category>
      <category>cs.LG</category>
      <category>cs.LO</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Christian Jan{\ss}en, Cedric Richter, Heike Wehrheim</dc:creator>
    </item>
    <item>
      <title>A Customized SAT-based Solver for Graph Coloring</title>
      <link>https://arxiv.org/abs/2504.04821</link>
      <description>arXiv:2504.04821v2 Announce Type: replace-cross 
Abstract: We introduce ZykovColor, a novel SAT-based algorithm to solve the graph coloring problem working on top of an encoding that mimics the Zykov tree. Our method is based on an approach of H\'ebrard and Katsirelos (2020) that employs a propagator to enforce transitivity constraints, incorporate lower bounds for search tree pruning, and enable inferred propagations.
  We leverage the recently introduced IPASIR-UP interface for CaDiCaL to implement these techniques with a SAT solver. Furthermore, we propose new features that take advantage of the underlying SAT solver. These include modifying the integrated decision strategy with vertex domination hints and using incremental bottom-up search that allows to reuse learned clauses from previous calls. Additionally, we integrate a more effective clique computation and an algorithm for computing the fractional chromatic number to improve the lower bounds used for pruning during the search.
  We validate the effectiveness of each new feature through an experimental analysis. ZykovColor outperforms other state-of-the-art graph coloring implementations on the DIMACS benchmark set. Further experiments on random Erd\H{o}s-R\'enyi graphs show that our new approach matches or outperforms state-of-the-art SAT-based methods for both very sparse and highly dense graphs. We give an additional configuration of ZykovColor that dominates other SAT-based methods on the Erd\H{o}s-R\'enyi graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04821v2</guid>
      <category>cs.DM</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <category>cs.LO</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Timo Brand, Daniel Faber, Stephan Held, Petra Mutzel</dc:creator>
    </item>
  </channel>
</rss>
