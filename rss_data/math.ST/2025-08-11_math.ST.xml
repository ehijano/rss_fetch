<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 12 Aug 2025 04:03:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Hybrid estimation for a mixed fractional Black-Scholes model with random effects from discrete time observations</title>
      <link>https://arxiv.org/abs/2508.07936</link>
      <description>arXiv:2508.07936v1 Announce Type: new 
Abstract: We propose a hybrid estimation procedure to estimate global fixed parameters and subject-specific random effects in a mixed fractional Black-Scholes model based on discrete time observations. Specifically, we consider $N$ independent stochastic processes, each driven by a linear combination of standard Brownian motion and an independent fractional Brownian motion, and governed by a drift term that depends on an unobserved random effect with unknown distribution. Based on discrete-time statistics of process increments, we construct parametric estimators for the Brownian motion volatility, the scaling parameter for the fractional Brownian motion, and the Hurst parameter using a generalized method of moments. We establish strong consistency and joint asymptotic normality of these estimators. Then, from one trajectory, we consistently estimate the random effects, using a plug-in approach, and we study their asymptotic behavior under different asymptotic regimes as $N$ and $n$ grow. Finally, we construct a nonparametric estimator for the distribution function of these random effects using a Lagrange interpolation at Chebyshev-Gauss nodes based method, and we analyze its asymptotic properties as both the number of subjects $N$ and the number of observations per-subject $n$ increase. A numerical simulation framework is also investigated to illustrate the theoretical results of the estimators behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07936v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nerine Chebli, Hamdi Fathallah, Yousri Slaoui</dc:creator>
    </item>
    <item>
      <title>A test statistic, $h^*$, for outlier analysis</title>
      <link>https://arxiv.org/abs/2508.06792</link>
      <description>arXiv:2508.06792v1 Announce Type: cross 
Abstract: Outlier analysis is a critical tool across diverse domains, from clinical decision-making to cybersecurity and talent identification. Traditional statistical outlier detection methods, such as Grubb's test and Dixon's Q, are predicated on the assumption of normality and often fail to reckon the meaningfulness of exceptional values within non-normal datasets. In this paper, we introduce the h* statistic, a novel parametric, frequentist approach for evaluating global outliers without the normality assumption. Unlike conventional techniques that primarily remove outliers to preserve statistical `integrity,' h* assesses the distinctiveness as phenomena worthy of investigation by quantifying a data point's extremity relative to its group as a measure of statistical significance analogous to the role of Student's t in comparing means. We detail the mathematical formulation of h* with tabulated confidence intervals of significance levels and extensions to Bayesian inference and paired analysis. The capacity of h* to discern between stable extraordinary deviations and values that merely appear extreme under conventional criteria is demonstrated using empirical data from a mood intervention study. A generalisation of h* is subsequently proposed, with individual weights assigned to differences for nuanced contextual description, and a variable sensitivity exponent for objective inference optimisation and subjective inference specification. The physical significance of an h*-recognised outlier is linked to the signature of unique occurrences. Our findings suggest that h* offers a robust alternative for outlier evaluation, enriching the analytical repertoire for researchers and practitioners by foregrounding the interpretative value of outliers within complex, real-world datasets. This paper is also a statement against the dominance of normality in celebration of the luminary and the lunatic alike.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06792v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Johan F. Hoorn, Johnny K. W. Ho</dc:creator>
    </item>
    <item>
      <title>Tight Bounds for Schr\"odinger Potential Estimation in Unpaired Image-to-Image Translation Problems</title>
      <link>https://arxiv.org/abs/2508.07392</link>
      <description>arXiv:2508.07392v1 Announce Type: cross 
Abstract: Modern methods of generative modelling and unpaired image-to-image translation based on Schr\"odinger bridges and stochastic optimal control theory aim to transform an initial density to a target one in an optimal way. In the present paper, we assume that we only have access to i.i.d. samples from initial and final distributions. This makes our setup suitable for both generative modelling and unpaired image-to-image translation. Relying on the stochastic optimal control approach, we choose an Ornstein-Uhlenbeck process as the reference one and estimate the corresponding Schr\"odinger potential. Introducing a risk function as the Kullback-Leibler divergence between couplings, we derive tight bounds on generalization ability of an empirical risk minimizer in a class of Schr\"odinger potentials including Gaussian mixtures. Thanks to the mixing properties of the Ornstein-Uhlenbeck process, we almost achieve fast rates of convergence up to some logarithmic factors in favourable scenarios. We also illustrate performance of the suggested approach with numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07392v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikita Puchkin, Denis Suchkov, Alexey Naumov, Denis Belomestny</dc:creator>
    </item>
    <item>
      <title>High-dimensional Longitudinal Inference via a De-sparsified Dantzig-Selector</title>
      <link>https://arxiv.org/abs/2508.07498</link>
      <description>arXiv:2508.07498v1 Announce Type: cross 
Abstract: In this paper, we consider statistical inference with generalized linear models in high dimensions under a longitudinal clustered data framework. Specifically, we propose a de-sparsified version of an initial Dantzig-type regularized estimator in regression settings and provide theoretical justification for both linear and generalized linear models. We present extensive numerical simulations demonstrating the effectiveness of our method for continuous and binary data. For continuous outcomes under linear models, we show that our estimator asymptotically attains an appropriate efficiency bound when the correlation structure is correctly specified. We conclude with an application of our method to a well-established genetics dataset, with bacterial riboflavin production as the outcome of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07498v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan Huey, Rajarshi Mukherjee</dc:creator>
    </item>
    <item>
      <title>Procedural Mixture Sets</title>
      <link>https://arxiv.org/abs/2508.07588</link>
      <description>arXiv:2508.07588v1 Announce Type: cross 
Abstract: The paper characterizes the Shannon (1948) and Tsallis (1988) entropies in a standard framework of decision theory, mixture sets. Procedural mixture sets are introduced as a variant of mixture sets in which it is not necessarily true that a mixture of two identical elements yields the same element. This allows the process of mixing itself to have an intrinsic value. The paper proves the surprising result that simply imposing the standard axioms of von Neumann-Morgenstern on preferences on a procedural mixture set yields the entropy as a representation of procedural value. An application of the theorem to decision processes and the relation between choice probabilities and decision times elucidates the difficulty of extending the drift-diffusion model to multi-alternative choice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07588v1</guid>
      <category>econ.TH</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hendrik Rommeswinkel</dc:creator>
    </item>
    <item>
      <title>Regularization Effects of Time Integration on Gaussian Process Functionals</title>
      <link>https://arxiv.org/abs/2508.07665</link>
      <description>arXiv:2508.07665v1 Announce Type: cross 
Abstract: In this paper, we investigate the regularization effects, in the sense of Malliavin calculus, on functionals of Gaussian processes induced by time integration, focusing on their covariance functions. We study several examples of important covariance functions classes to verify whether they satisfy the sufficient conditions proposed for regularization. Additionally, we derive a weak implication for the smoothness of level-crossing functionals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07665v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takafumi Amaba, Marie Kratz</dc:creator>
    </item>
    <item>
      <title>Stochastic dynamics learning with state-space systems</title>
      <link>https://arxiv.org/abs/2508.07876</link>
      <description>arXiv:2508.07876v1 Announce Type: cross 
Abstract: This work advances the theoretical foundations of reservoir computing (RC) by providing a unified treatment of fading memory and the echo state property (ESP) in both deterministic and stochastic settings. We investigate state-space systems, a central model class in time series learning, and establish that fading memory and solution stability hold generically -- even in the absence of the ESP -- offering a robust explanation for the empirical success of RC models without strict contractivity conditions. In the stochastic case, we critically assess stochastic echo states, proposing a novel distributional perspective rooted in attractor dynamics on the space of probability distributions, which leads to a rich and coherent theory. Our results extend and generalize previous work on non-autonomous dynamical systems, offering new insights into causality, stability, and memory in RC models. This lays the groundwork for reliable generative modeling of temporal data in both deterministic and stochastic regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07876v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.DS</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Juan-Pablo Ortega, Florian Rossmannek</dc:creator>
    </item>
    <item>
      <title>Gaussian Approximation for Two-Timescale Linear Stochastic Approximation</title>
      <link>https://arxiv.org/abs/2508.07928</link>
      <description>arXiv:2508.07928v1 Announce Type: cross 
Abstract: In this paper, we establish non-asymptotic bounds for accuracy of normal approximation for linear two-timescale stochastic approximation (TTSA) algorithms driven by martingale difference or Markov noise. Focusing on both the last iterate and Polyak-Ruppert averaging regimes, we derive bounds for normal approximation in terms of the convex distance between probability distributions. Our analysis reveals a non-trivial interaction between the fast and slow timescales: the normal approximation rate for the last iterate improves as the timescale separation increases, while it decreases in the Polyak-Ruppert averaged setting. We also provide the high-order moment bounds for the error of linear TTSA algorithm, which may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07928v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bogdan Butyrin, Artemy Rubtsov, Alexey Naumov, Vladimir Ulyanov, Sergey Samsonov</dc:creator>
    </item>
    <item>
      <title>Random Modulation: Achieving Asymptotic Replica Optimality over Arbitrary Norm-Bounded and Spectrally Convergent Channel Matrices</title>
      <link>https://arxiv.org/abs/2508.08099</link>
      <description>arXiv:2508.08099v1 Announce Type: cross 
Abstract: This paper introduces a random modulation technique that is decoupled from the channel matrix, allowing it to be applied to arbitrary norm-bounded and spectrally convergent channel matrices. The proposed random modulation constructs an equivalent dense and random channel matrix, ensuring that the signals undergo sufficient statistical channel fading. It also guarantees the asymptotic replica maximum a posteriori (MAP) bit-error rate (BER) optimality of approximate message passing (AMP)-type detectors for linear systems with arbitrary norm-bounded and spectrally convergent channel matrices when their state evolution has a unique fixed point. Then, a low-complexity cross-domain memory approximate message passing (CD-MAMP) detector is proposed for random modulation, leveraging the sparsity of the time-domain channel and the randomness of the random transform-domain channel. Furthermore, the optimal power allocation schemes are derived to minimize the replica MAP BER and maximize the replica constrained capacity of random-modulated linear systems, assuming the availability of channel state information (CSI) at the transceiver. Numerical results show that the proposed random modulation can achieve BER and block-error rate (BLER) performance gains of up to 2 - 3 dB compared to existing OFDM/OTFS/AFDM with 5G-NR LDPC codes, under both average and optimized power allocation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08099v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lei Liu, Yuhao Chi, Shunqi Huang</dc:creator>
    </item>
    <item>
      <title>Variance estimation for weighted average treatment effects</title>
      <link>https://arxiv.org/abs/2508.08167</link>
      <description>arXiv:2508.08167v1 Announce Type: cross 
Abstract: Common variance estimation methods for weighted average treatment effects (WATEs) in observational studies include nonparametric bootstrap and model-based, closed-form sandwich variance estimation. However, the computational cost of bootstrap increases with the size of the data at hand. Besides, some replicates may exhibit random violations of the positivity assumption even when the original data do not. Sandwich variance estimation relies on regularity conditions that may be structurally violated. Moreover, the sandwich variance estimation is model-dependent on the propensity score model, the outcome model, or both; thus it does not have a unified closed-form expression. Recent studies have explored the use of wild bootstrap to estimate the variance of the average treatment effect on the treated (ATT). This technique adopts a one-dimensional, nonparametric, and computationally efficient resampling strategy. In this article, we propose a "post-weighting" bootstrap approach as an alternative to the conventional bootstrap, which helps avoid random positivity violations in replicates and improves computational efficiency. We also generalize the wild bootstrap algorithm from ATT to the broader class of WATEs by providing new justification for correctly accounting for sampling variability from multiple sources under different weighting functions. We evaluate the performance of all four methods through extensive simulation studies and demonstrate their application using data from the National Health and Nutrition Examination Survey (NHANES). Our findings offer several practical recommendations for the variance estimation of WATE estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08167v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huiyue Li, Yi Liu, Yunji Zhou, Jiajun Liu, Dezhao Fu, Roland A. Matsouaka</dc:creator>
    </item>
    <item>
      <title>Optimal Test-Data Piling in HDLSS Classification with Covariance Heterogeneity</title>
      <link>https://arxiv.org/abs/2211.15562</link>
      <description>arXiv:2211.15562v3 Announce Type: replace 
Abstract: This work addresses a longstanding question in high-dimensional linear classification: Is perfect classification achievable in heterogeneous covariance structures? We focus on the phenomenon of data piling, where projected data points collapse onto discrete values. We provide a comprehensive characterization of two distinct types of data piling. The first type of data piling refers to the phenomenon where projecting the training data onto a certain direction yields exactly two distinct values-one for each class. This occurs universally when the data dimension $p$ exceeds the sample size $n$. The second type concerns independent test data and arises asymptotically as $p \to \infty$ with fixed $n$. While previous work established the existence of such double data piling under homogeneously spiked covariance structures using negatively ridged classifiers, our analysis extends to the more general and realistic case of heterogeneous covariance. We identify an optimal direction among all piling directions that maximizes the separation between test data piles, which is called the Second Maximal Data Piling direction. An algorithm based on data splitting is proposed to compute this direction using only training data. Our analysis reveals a key insight: the main obstacle to discovering this direction is the imbalance of the tail eigenvalues, rather than differences in spike count, spike magnitude, or the alignment of leading eigenspaces. Extensive simulations confirm our theoretical results and demonstrate the effectiveness of the proposed classifier across a wide range of high-dimensional scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.15562v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Taehyun Kim, Jeongyoun Ahn, Sungkyu Jung</dc:creator>
    </item>
    <item>
      <title>Exact and asymptotic distribution theory for the empirical correlation of two AR(1) processes with Gaussian increments</title>
      <link>https://arxiv.org/abs/2310.08575</link>
      <description>arXiv:2310.08575v2 Announce Type: replace 
Abstract: This paper begins with a study of the exact distribution of the empirical correlation of two independent AR(1) processes with Gaussian increments. We proceed to develop rates of convergence for the distribution of the scaled empirical correlation to the standard Gaussian distribution in both Wasserstein distance and Kolmogorov distance. Given $n$ data points, we prove the convergence rate in Wasserstein distance is $n^{-1/2}$ and the convergence rate in Kolmogorov distance is $n^{-1/2} \sqrt{\ln n}$. We conclude by extending these results to two AR(1) processes with correlated Gaussian increments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.08575v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philip A. Ernst, Dongzhou Huang</dc:creator>
    </item>
    <item>
      <title>Estimation for multistate models subject to reporting delays and incomplete event adjudication with application to disability insurance</title>
      <link>https://arxiv.org/abs/2311.04318</link>
      <description>arXiv:2311.04318v4 Announce Type: replace 
Abstract: Accurate forecasting of an insurer's outstanding liabilities is vital for the solvency of insurance companies and the financial stability of the insurance sector. For health and disability insurance, the liabilities are intimately linked with the biometric event history of the insured. Complete observation of event histories is often impossible due to sampling effects such as right-censoring and left-truncation, but also due to reporting delays and incomplete event adjudication. In this paper, we develop a parametric two-step M-estimation method that takes the aforementioned effects into account, treating the latter two as partially exogenous. The approach is valid under weak assumptions and allows for complicated dependencies between the event history, reporting delays, and adjudication while remaining relatively simple to implement. The estimation approach has desirable properties which are demonstrated by theoretical results and numerical experiments.
  In the application, we introduce and consider a large portfolio of disability insurance policies. We find that properly accounting for the sampling effects has a large impact on the number of disabilities and reactivations that an insurer would forecast, allowing for a more accurate assessment of the insurer's liabilities and improved risk management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.04318v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>K. Buchardt, C. Furrer, O. L. Sandqvist</dc:creator>
    </item>
    <item>
      <title>Asymptotic Bayes Optiamlity for Sparse Count Data</title>
      <link>https://arxiv.org/abs/2401.05693</link>
      <description>arXiv:2401.05693v2 Announce Type: replace 
Abstract: Consider a situation of analyzing high-dimensional count data containing an excess of near-zero counts with a small number of moderate or large counts. Assuming that the observations are modeled by a Poisson distribution, we are interested in simultaneous testing of whether the mean of the $i^{\text{th}}$ observation is small or large. In this work, we study some optimal properties (in terms of Bayes risk) of multiple-testing rules when the mean parameter is modeled by both two-group and a general class of one-group shrinkage priors, proposed by Polson and Scott (2010). Here, first, we model each mean by a two-group prior, and under additive $0-1$ loss function, obtain an expression for the optimal Bayes risk under some assumption similar in the spirit of Bogdan et al. (2011). Next, assuming that the observations are truly generated from a two-group mixture model and modelling each mean parameter by the broad class of one-group priors, we study the Bayes risk induced by our chosen class of priors. We have been able to show that, when the underlying level of sparsity is known, under some proposed assumptions, the Bayes risk corresponding to our broad class of priors attains the optimal Bayes risk, upto a multiplicative constant. When this sparsity pattern is unknown, motivated by Yano et al. (2021), we use an empirical Bayes estimate of the global shrinkage parameter. In this case, also, we show that the modified decision rule attains the optimal Bayes risk, upto a multiplicative constant. In this way, as an alternative solution for two-group prior, we propose a broad class of global-local priors having similar optimal properties in terms of Bayes risk for quasi-sparse count data. Finally, the theoretical results are verified using simulation studies followed by a real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.05693v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sayantan Paul, Arijit Chakrabarti</dc:creator>
    </item>
    <item>
      <title>Location and association measures for interval-valued data based on Mallows' distance</title>
      <link>https://arxiv.org/abs/2407.05105</link>
      <description>arXiv:2407.05105v2 Announce Type: replace 
Abstract: The growing demand to analyse large and complex datasets has spurred the development of Symbolic Data Analysis as a promising approach to address contemporary data challenges. Amongst these, interval-valued data introduces new theoretical and methodological questions that remain open.
  In this paper, we generalise measures of location and association for interval-valued random variables using Mallows' distance. Departing from restrictive assumptions such as uniform distributions over microdata, our proposal extends the barycentre approach to any absolutely continuous distribution with finite second moment. A key contribution is the derivation of explicit formulas for Mallows' distance in p-dimensional interval spaces. These formulas decompose into components for centres, ranges, and a novel cross-term that captures their interaction. This decomposition leads to a new theoretical symbolic covariance matrix that explicitly accounts for the dependence between centres and ranges - a relation often obscured in current definitions of symbolic covariance.
  Theoretical developments are supported by empirical studies on diverse real-world datasets, each reflecting different degrees of information about the underlying microdata. These applications highlight both the flexibility of the proposed methodology and the interpretability of its results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05105v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>M. Ros\'ario Oliveira, Diogo Pinheiro, Lina Oliveira</dc:creator>
    </item>
    <item>
      <title>Learning latent tree models with small query complexity</title>
      <link>https://arxiv.org/abs/2408.15624</link>
      <description>arXiv:2408.15624v2 Announce Type: replace 
Abstract: We consider the problem of structure recovery in a graphical model of a tree where some variables are latent. Specifically, we focus on the Gaussian case, which can be reformulated as a well-studied problem: recovering a semi-labeled tree from a distance metric. We introduce randomized procedures that achieve query complexity of optimal order. Additionally, we provide statistical analysis for scenarios where the tree distances are noisy. The Gaussian setting can be extended to other situations, including the binary case and non-paranormal distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15624v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luc Devroye, Gabor Lugosi, Piotr Zwiernik</dc:creator>
    </item>
    <item>
      <title>Adaptive Reference-Guided Estimation of Principal Component Subspace in High Dimensions</title>
      <link>https://arxiv.org/abs/2411.15899</link>
      <description>arXiv:2411.15899v3 Announce Type: replace 
Abstract: We propose a novel estimator for the principal component (PC) subspace tailored to the high-dimension, low-sample size (HDLSS) context. The method, termed Adaptive Reference-Guided (ARG) estimator, is designed for data exhibiting spiked covariance structures and seeks to improve upon the conventional sample PC subspace by leveraging auxiliary information from reference vectors, presumed to carry prior knowledge about the true PC subspace. The estimator is constructed by first identifying vectors asymptotically orthogonal to the true PC subspace within a signal subspace, the subspace spanned by the leading sample PC directions and the references, and then taking the orthogonal complement. The estimator is adaptive, as it automatically selects the subspace asymptotically closest to the true PC subspace inside the signal subspace, without requiring parameter tuning. We show that when the reference vectors carry nontrivial information, the proposed estimator asymptotically reduces all principal angles between the estimated and true PC subspaces compared to the naive sample-based estimator. Interestingly, despite being derived from a completely different rationale, the ARG estimator is theoretically equivalent to an estimator based on James-Stein shrinkage. Our results thus establish a theoretical foundation that unifies these two distinct approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15899v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1002/sta4.70081</arxiv:DOI>
      <arxiv:journal_reference>Stat 14 (2025) e70081</arxiv:journal_reference>
      <dc:creator>Dongsun Yoon, Sungkyu Jung</dc:creator>
    </item>
    <item>
      <title>Rethinking Mean Square Error: Why Information is a Superior Assessment of Estimators</title>
      <link>https://arxiv.org/abs/2412.08475</link>
      <description>arXiv:2412.08475v2 Announce Type: replace 
Abstract: The James-Stein estimator's dominance over maximum likelihood in terms of mean square error (MSE) has been one of the most celebrated results in modern statistics, suggesting that biased estimators can systematically outperform unbiased ones. We argue that this conclusion stems from using an inappropriate assessment criterion. Through simple simulations, we demonstrate that while James-Stein achieves lower MSE, it produces concerning behavior in practice: hypothesis tests based on James-Stein can have power below the significance level, exhibit severe asymmetry, and lead to conclusions that practitioners would hesitate to report. Using $\Lambda$-information (Vos and Wu, 2025), a criterion that measures how effectively estimators distinguish between distributions, we show that maximum likelihood achieves full efficiency while James-Stein performs poorly precisely where MSE suggests superiority. Our analysis reveals that MSE's fundamental flaw--assessing estimators point-wise thereby missing important aspects of estimation--creates these paradoxes. By expanding from point estimators to generalized estimators (functions over the parameter space), we obtain a parameter-invariant framework that unifies estimation and testing. These insights suggest that the statistical community should reconsider not maximum likelihood theory, but rather our reliance on MSE for comparing estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08475v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul W. Vos</dc:creator>
    </item>
    <item>
      <title>Priors for second-order unbiased Bayes estimators</title>
      <link>https://arxiv.org/abs/2412.19187</link>
      <description>arXiv:2412.19187v2 Announce Type: replace 
Abstract: Asymptotically unbiased priors, introduced by Hartigan (1965), are designed to achieve second-order unbiasedness of Bayes estimators. This paper extends Hartigan's framework to non-i.i.d. models by deriving a system of partial differential equations that characterizes asymptotically unbiased priors. Furthermore, we establish a necessary and sufficient condition for the existence of such priors and propose a simple procedure for constructing them. The proposed method is applied to the linear regression model and the nested error regression model (also known as the random effects model). Simulation studies evaluate the frequentist properties of the Bayes estimator under the asymptotically unbiased prior for the nested error regression model, highlighting its effectiveness in small-sample settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19187v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mana Sakai, Takeru Matsuda, Tatsuya Kubokawa</dc:creator>
    </item>
    <item>
      <title>A Note on Inferential Decisions, Errors and Path-Dependency</title>
      <link>https://arxiv.org/abs/2507.05634</link>
      <description>arXiv:2507.05634v2 Announce Type: replace 
Abstract: Consider the standard inferential testing of a binary outcome. Depending on the beliefs underlying such a test, the a posteriori belief process and its objectively true conditional-probability counterpart generally differ, but they converge to the same target in well-defined tests. We show that unless the two processes are 'essentially identical', differing at most by an a priori factor, time-homogeneous continuous sequential decisions based on the former must be path-dependent with respect to state-variables based on the latter or any other non-essentially-identical a posteriori belief processes. Further, total inferential errors decompose into two independent components with distinct properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05634v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kangda K. Wren</dc:creator>
    </item>
    <item>
      <title>Asymptotic universal moment matching properties of normal distributions</title>
      <link>https://arxiv.org/abs/2508.03790</link>
      <description>arXiv:2508.03790v2 Announce Type: replace 
Abstract: Moment matching is an easy-to-implement and usually effective method to reduce variance of Monte Carlo simulation estimates. On the other hand, there is no guarantee that moment matching will always reduce simulation variance for general integration problems at least asymptotically, i.e. when the number of samples is large. We study the characterization of conditions on a given underlying distribution $X$ under which asymptotic variance reduction is guaranteed for a general integration problem $\mathbb{E}[f(X)]$ when moment matching techniques are applied. We show that a sufficient and necessary condition for such asymptotic variance reduction property is $X$ being a normal distribution. Moreover, when $X$ is a normal distribution, formulae for efficient estimation of simulation variance for (first and second order) moment matching Monte Carlo are obtained. These formulae allow estimations of simulation variance as by-products of the simulation process, in a way similar to variance estimations for plain Monte Carlo. Moreover, we propose non-linear moment matching schemes for any given continuous distribution such that asymptotic variance reduction is guaranteed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03790v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>q-fin.CP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuan Liu</dc:creator>
    </item>
    <item>
      <title>Statistical inference of random graphs with a surrogate likelihood function</title>
      <link>https://arxiv.org/abs/2207.01702</link>
      <description>arXiv:2207.01702v3 Announce Type: replace-cross 
Abstract: Spectral estimators have been broadly applied to statistical network analysis, but they do not incorporate the likelihood information of the network sampling model. This paper proposes a novel surrogate likelihood function for statistical inference of a class of popular network models referred to as random dot product graphs. In contrast to the structurally complicated exact likelihood function, the surrogate likelihood function has a separable structure and is log-concave yet approximates the exact likelihood function well. From the frequentist perspective, we study the maximum surrogate likelihood estimator and establish the accompanying theory. We show its existence, uniqueness, large sample properties, and that it improves upon the baseline spectral estimator with a smaller sum of squared errors. Furthermore, we derive the second-order bias of the proposed estimator and gain insight into why it outperforms some of the existing estimators. A computationally convenient stochastic gradient descent algorithm is designed to find the maximum surrogate likelihood estimator in practice. From the Bayesian perspective, we establish the Bernstein--von Mises theorem of the posterior distribution with the surrogate likelihood function and show that the resulting credible sets have the correct frequentist coverage. The empirical performance of the proposed surrogate-likelihood-based methods is validated through the analyses of simulation examples and a real-world Wikipedia graph dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.01702v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dingbo Wu, Fangzheng Xie</dc:creator>
    </item>
    <item>
      <title>From Spikes to Heavy Tails: Unveiling the Spectral Evolution of Neural Networks</title>
      <link>https://arxiv.org/abs/2406.04657</link>
      <description>arXiv:2406.04657v3 Announce Type: replace-cross 
Abstract: Training strategies for modern deep neural networks (NNs) tend to induce a heavy-tailed (HT) empirical spectral density (ESD) in the layer weights. While previous efforts have shown that the HT phenomenon correlates with good generalization in large NNs, a theoretical explanation of its occurrence is still lacking. Especially, understanding the conditions which lead to this phenomenon can shed light on the interplay between generalization and weight spectra. Our work aims to bridge this gap by presenting a simple, rich setting to model the emergence of HT ESD. In particular, we present a theory-informed setup for 'crafting' heavy tails in the ESD of two-layer NNs and present a systematic analysis of the HT ESD emergence without any gradient noise. This is the first work to analyze a noise-free setting, and we also incorporate optimizer (GD/Adam) dependent (large) learning rates into the HT ESD analysis. Our results highlight the role of learning rates on the Bulk+Spike and HT shape of the ESDs in the early phase of training, which can facilitate generalization in the two-layer NN. These observations shed light on the behavior of large-scale NNs, albeit in a much simpler setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04657v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Vignesh Kothapalli, Tianyu Pang, Shenyang Deng, Zongmin Liu, Yaoqing Yang</dc:creator>
    </item>
    <item>
      <title>A variational Bayes approach to debiased inference for low-dimensional parameters in high-dimensional linear regression</title>
      <link>https://arxiv.org/abs/2406.12659</link>
      <description>arXiv:2406.12659v2 Announce Type: replace-cross 
Abstract: We propose a scalable variational Bayes method for statistical inference for a single or low-dimensional subset of the coordinates of a high-dimensional parameter in sparse linear regression. Our approach relies on assigning a mean-field approximation to the nuisance coordinates and carefully modelling the conditional distribution of the target given the nuisance. This requires only a preprocessing step and preserves the computational advantages of mean-field variational Bayes, while ensuring accurate and reliable inference for the target parameter, including for uncertainty quantification. We investigate the numerical performance of our algorithm, showing that it performs competitively with existing methods. We further establish accompanying theoretical guarantees for estimation and uncertainty quantification in the form of a Bernstein--von Mises theorem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12659v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Isma\"el Castillo, Alice L'Huillier, Kolyan Ray, Luke Travis</dc:creator>
    </item>
    <item>
      <title>Characterization of Exponential Families of Lumpable Stochastic Matrices</title>
      <link>https://arxiv.org/abs/2412.08400</link>
      <description>arXiv:2412.08400v2 Announce Type: replace-cross 
Abstract: It is known that the set of lumpable Markov chains over a finite state space, with respect to a fixed lumping function, generally does not form an exponential family of stochastic matrices. In this work, we explore efficiently verifiable necessary and sufficient conditions for families of lumpable transition matrices to form exponential families. To this end, we develop a broadly applicable dimension-based method for determining whether a given family of stochastic matrices forms an exponential family.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08400v2</guid>
      <category>math.PR</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shun Watanabe, Geoffrey Wolfer</dc:creator>
    </item>
    <item>
      <title>High-dimensional random landscapes: from typical to large deviations</title>
      <link>https://arxiv.org/abs/2502.14084</link>
      <description>arXiv:2502.14084v2 Announce Type: replace-cross 
Abstract: In these notes we discuss tools and concepts that emerge when studying high-dimensional random landscapes, i.e., random functions on high-dimensional spaces. As an illustrative example, we consider an inference problem in two forms: low-rank matrix estimation (Case 1) and low-rank tensor estimation (Case 2). We show how to map the inference problem onto the optimization problem of a high-dimensional landscape, which exhibits distinct geometrical properties in the two cases. We discuss methods for characterizing typical realizations of these landscapes and their optimization through local dynamics. We conclude by highlighting connections between the landscape problem and Large Deviation Theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14084v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Valentina Ros</dc:creator>
    </item>
    <item>
      <title>Decision Theory For Large Scale Outlier Detection Using Aleatoric Uncertainty: With a Note on Bayesian FDR</title>
      <link>https://arxiv.org/abs/2508.01988</link>
      <description>arXiv:2508.01988v2 Announce Type: replace-cross 
Abstract: Aleatoric and Epistemic uncertainty have achieved recent attention in the literature as different sources from which uncertainty can emerge in stochastic modeling. Epistemic being intrinsic or model based notions of uncertainty, and aleatoric being the uncertainty inherent in the data. We propose a novel decision theoretic framework for outlier detection in the context of aleatoric uncertainty; in the context of Bayesian modeling. The model incorporates bayesian false discovery rate control for multiplicty adjustment, and a new generalization of Bayesian FDR is introduced. The model is applied to simulations based on temporally fluctuating outlier detection where fixing thresholds often results in poor performance due to nonstationarity, and a case study is outlined on on a novel cybersecurity detection. Cyberthreat signals are highly nonstationary; giving a credible stress test of the model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01988v2</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ryan Warnick</dc:creator>
    </item>
  </channel>
</rss>
