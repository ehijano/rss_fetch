<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 25 Aug 2025 04:00:51 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Nonparametric regression for a circular response with error-in-covariate</title>
      <link>https://arxiv.org/abs/2508.15962</link>
      <description>arXiv:2508.15962v1 Announce Type: new 
Abstract: This study considers regression analysis of a circular response with an error-prone linear covariate. Starting with an existing estimator of the circular regression function that assumes error-free covariate, three approaches are proposed to revise this estimator, leading to three nonparametric estimators for the circular regression function accounting for measurement error. The proposed estimators are intrinsically connected through some deconvoluting operator that is exploited differently in different estimators. Moreover, a new bandwidth selection method is developed that is more computationally efficient than an existing method well-received in the context of tuning parameter selection in the presence of measurement error. The efficacy of these new estimators and their relative strengths are demonstrated through a thorough investigation of their asymptotic properties and extensive empirical study of their finite-sample performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15962v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicholas Woolsey, Xianzheng Huang</dc:creator>
    </item>
    <item>
      <title>Quasi Instrumental Variable Methods for Stable Hidden Confounding and Binary Outcome</title>
      <link>https://arxiv.org/abs/2508.16096</link>
      <description>arXiv:2508.16096v1 Announce Type: new 
Abstract: Instrumental variable (IV) methods are central to causal inference from observational data, particularly when a randomized experiment is not feasible. However, of the three conventional core IV identification conditions, only one, IV relevance, is empirically verifiable; often one or both of the other conditions, exclusion restriction and IV independence from unmeasured confounders, are unmet in real-world applications. These challenges are compounded when the outcome is binary, a setting for which robust IV methods remain underdeveloped. A fundamental contribution of this paper is the development of a general identification strategy justified under a structural equilibrium dynamic generative model of so-called stable confounding and a quasi instrumental variable (QIV), i.e. a variable that is only assumed to be predictive of the outcome. Such a model implies (a) stability of confounding on the multiplicative scale, and (b) stability of the additive average treatment effect among the treated (ATT), across levels of that QIV. The former is all that is necessary to ensure a valid test of the causal null hypothesis; together those two conditions establish nonparametric identification and estimation of the conditional and marginal ATT. To address the statistical challenges posed by the need for boundedness in binary outcomes, we introduce a generalized odds product re-parametrization of the observed data distribution, and we develop both a principled maximum likelihood estimator and a triply robust semiparametric locally efficient estimator, which we evaluate through simulations and an empirical application to the UK Biobank.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16096v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhonghua Liu, Baoluo Sun, Ting Ye, David Richardson, Eric Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Estimating the growth rate of a birth and death process using data from a small sample</title>
      <link>https://arxiv.org/abs/2508.16110</link>
      <description>arXiv:2508.16110v1 Announce Type: new 
Abstract: The problem of estimating the growth rate of a birth and death processes based on the coalescence times of a sample of $n$ individuals has been considered by several authors (\cite{stadler2009incomplete, williams2022life, mitchell2022clonal, Johnson2023}). This problem has applications, for example, to cancer research, when one is interested in determining the growth rate of a clone.
  Recently, \cite{Johnson2023} proposed an analytical method for estimating the growth rate using the theory of coalescent point processes, which has comparable accuracy to more computationally intensive methods when the sample size $n$ is large. We use a similar approach to obtain an estimate of the growth rate that is not based on the assumption that $n$ is large.
  We demonstrate, through simulations using the R-package \texttt{cloneRate}, that our estimator for the growth rate has a smaller mean squared error than previous estimates when $n$ is small.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16110v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carola Sophia Heinzel, Jason Schweinsberg</dc:creator>
    </item>
    <item>
      <title>A statistical note on extending Christensen's limits of agreement with the mean</title>
      <link>https://arxiv.org/abs/2508.16250</link>
      <description>arXiv:2508.16250v1 Announce Type: new 
Abstract: Limits of agreement with the mean (LOAM) can be used for assessing agreement of continuous measurements made by different observers. Definitions of a LOAM for measuring reproducibility has been introduced under a two-way random effects model without interaction between subject and observer. Here we extend that model framework to include a subject-observer interaction, allowing the separation of residual measurement error and systematic variation in how individual observers measure specific objects. Further, our framework extends the LOAM concept to two metrics: one quantifying reproducibility and the other repeatability. We supply estimates and confidence intervals for the reproducibility and repeatibility LOAM and discuss sample size calculations and a test to compare LOAMs between two groups. To make the text self-contained for a complete agreement analysis, we additionally provide estimates and confidence intervals for the variance components.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16250v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heidi S{\o}gaard Christensen, Martin B{\o}gsted, Jens Borgbjerg</dc:creator>
    </item>
    <item>
      <title>A regularized multi-state model for covariate selection with interval-censored survival data</title>
      <link>https://arxiv.org/abs/2508.16256</link>
      <description>arXiv:2508.16256v1 Announce Type: new 
Abstract: In population-based cohorts, disease diagnoses are typically censored by intervals as made during scheduled follow-up visits. The exact disease onset time is thus unknown, and in the presence of semi-competing risk of death, subjects may also die in between two visits before any diagnosis can be made. Illness-death models can be used to handle uncertainty about illness timing and the possible absence of diagnosis due to death. However, they are so far limited in the number of covariates. We developed a regularized estimation procedure for illness-death models with interval-censored illness diagnosis that performs variable selection in the case of high-dimensional predictors. We considered a proximal gradient hybrid algorithm maximizing the regularized likelihood with an elastic-net penalty. The algorithm simultaneously estimates the regression parameters of the three transitions under proportional transition intensities with transition-specific penalty parameters determined in an outer gridsearch. The algorithm, implemented in the R package HIDeM, shows high performances in predicting illness probability, as well as correct selection of transition-specific risk factors across different simulation scenarios. In comparison, the cause-specific competing risk model neglecting interval-censoring systematically showed worse predictive ability and tended to select irrelevant illness predictors, originally associated with death. Applied to the population-based cohort Three-City, the method identified predictors of clinical dementia onset among a large set of brain imaging, cognitive and clinical markers.
  Keywords: Interval censoring; Multi-state model; Semi-competing risk; Survival Analysis; Variable Selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16256v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ariane Bercu, Agathe Guilloux, C\'ecile Proust-Lima, H\'el\`ene Jacqmin-Gadda</dc:creator>
    </item>
    <item>
      <title>Tree-based methods for length-biased survival data</title>
      <link>https://arxiv.org/abs/2508.16312</link>
      <description>arXiv:2508.16312v1 Announce Type: new 
Abstract: Left-truncated survival data commonly arise in prevalent cohort studies, where only individuals who have experienced disease onset and survived until enrollment in the study. When the onset process follows a stationary Poisson process, the resulting data are length-biased. This sampling mechanism induces a selection bias towards longer survival individuals, and nonparametric and semiparametric methods for traditional survival data are not directly applicable. While tree-based methods developed for left-truncated data can be applied, they may be inefficient for length-biased data, as they do not account for the distribution of truncation times. To address this, we propose new survival trees and forests for length-biased right-censored data within the conditional inference framework. Our approach uses a score function derived from the full likelihood to construct permutation test statistics for unbiased variable selection. For survival prediction, we consider two estimators of the unbiased survival function, differing in statistical efficiency and computational complexity. These elements enhance efficiency in tree construction and improve accuracy of survival prediction in ensemble settings. Simulation studies demonstrate efficiency gains in both tree recovery and survival prediction, often exceeding the gains from ensembling alone. We further illustrate the utility of the proposed methods using lung cancer data from the Cancer Public Library Database.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16312v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinwoo Lee, Jiyu Sun, Hyunwoo Lee, Donghwan Lee</dc:creator>
    </item>
    <item>
      <title>Heterogeneous Quantile Treatment Effect Estimation for Longitudinal Data with High-Dimensional Confounding</title>
      <link>https://arxiv.org/abs/2508.16326</link>
      <description>arXiv:2508.16326v1 Announce Type: new 
Abstract: Causal inference plays a fundamental role in various real-world applications. However, in the motivating non-small cell lung cancer (NSCLC) study, it is challenging to estimate the treatment effect of chemotherapy on circulating tumor DNA (ctDNA). First, the heterogeneous treatment effects vary across patient subgroups defined by baseline characteristics. Second, there exists a broad set of demographic, clinical and molecular variables act as potential confounders. Third, ctDNA trajectories over time show heavy-tailed non-Gaussian behavior. Finally, repeated measurements within subjects introduce unknown correlation. Combining convolution-smoothed quantile regression and orthogonal random forest, we propose a framework to estimate heterogeneous quantile treatment effects in the presence of high-dimensional confounding, which not only captures effect heterogeneity across covariates, but also behaves robustly to nuisance parameter estimation error. We establish the theoretical properties of the proposed estimator and demonstrate its finite-sample performance through comprehensive simulations. We illustrate its practical utility in the motivated NSCLC study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16326v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhixin Qiu, Huichen Zhu, Wenjie Wang, Yanlin Tang</dc:creator>
    </item>
    <item>
      <title>Principled type I error rate inflation in two-arm clinical trial designs with external control information borrowing</title>
      <link>https://arxiv.org/abs/2508.16348</link>
      <description>arXiv:2508.16348v1 Announce Type: new 
Abstract: External information borrowing is often considered in order to improve a clinical trial's efficiency. The Bayesian approach borrows such external information by specifying an informative prior distribution. A potential issue with this procedure is that external and current information may conflict, but such inconsistency may not be predictable a priori. Robust prior choices are typically proposed to limit extreme worsening of operating characteristics (OCs) in these situations. However, trade-offs are still present and in general no power gains are possible if strict control of type I error (TIE) rate is desired. In this context, principled justifications for TIE rate inflation can be of interest. Here we investigate two-arm trials, with a focus on external/historical control information borrowing. We investigate frequentist OCs trade-offs and propose an interpretable approach for external information borrowing. The approach analytically links observed potential prior-data conflict with allowances for TIE rate inflation and power loss. The approach does not rely on a robust prior specification, but can instead be interpreted as an adaptive choice of Bayes test decision thresholds under the available informative prior. A development for both Normal and binomial outcomes is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16348v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Silvia Calderazzo, Manuel Wiesenfarth, Vivienn Weru, Annette Kopp-Schneider</dc:creator>
    </item>
    <item>
      <title>An interpretable family of projected normal distributions and a related copula model for Bayesian analysis of hypertoroidal data</title>
      <link>https://arxiv.org/abs/2508.16432</link>
      <description>arXiv:2508.16432v1 Announce Type: new 
Abstract: This paper introduces two families of probability distributions for Bayesian analysis of hypertoroidal data. The first family consists of symmetric distributions derived from the projection of multivariate normal distributions under specific parameter constraints. This family is closed under marginalization and hence any marginal distribution belongs to a lower-dimensional case of the same family. In particular the univariate marginal of the family is the unimodal case of the projected normal distribution on the circle. The second family is a flexible extension of the copula case of the first family, which can accommodate any univariate marginal distributions. Unlike existing models derived via projection, both families have the common advantage that their parameters possess a clear and intuitive interpretation. In addition, Markov Chain Monte Carlo algorithms are presented for Bayesian estimation of both families and a simulation study is used to demonstrate their performance. As a real data example, a meteorological data set is analyzed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16432v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shogo Kato, Gianluca Mastrantonio, Masayuki Ishikawa</dc:creator>
    </item>
    <item>
      <title>Scalable Bayesian inference on high-dimensional multivariate linear regression</title>
      <link>https://arxiv.org/abs/2508.16446</link>
      <description>arXiv:2508.16446v1 Announce Type: new 
Abstract: We consider jointly estimating the coefficient matrix and the error precision matrix in high-dimensional multivariate linear regression models. Bayesian methods in this context often face computational challenges, leading to previous approaches that either utilize a generalized likelihood without ensuring the positive definiteness of the precision matrix or rely on maximization algorithms targeting only the posterior mode, thus failing to address uncertainty. In this work, we propose two Bayesian methods: an exact method and an approximate two-step method. We first propose an exact method based on spike and slab priors for the coefficient matrix and DAG-Wishart prior for the error precision matrix, whose computational complexity is comparable to the state-of-the-art generalized likelihood-based Bayesian method. To further enhance scalability, a two-step approach is developed by ignoring the dependency structure among response variables. This method estimates the coefficient matrix first, followed by the calculation of the posterior of the error precision matrix based on the estimated errors. We validate the two-step method by demonstrating (i) selection consistency and posterior convergence rates for the coefficient matrix and (ii) selection consistency for the directed acyclic graph (DAG) of errors. We demonstrate the practical performance of proposed methods through synthetic and real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16446v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xuan Cao, Kyoungjae Lee</dc:creator>
    </item>
    <item>
      <title>Identifying Treatment Effect Heterogeneity with Bayesian Hierarchical Adjustable Random Partition in Adaptive Enrichment Trials</title>
      <link>https://arxiv.org/abs/2508.16523</link>
      <description>arXiv:2508.16523v1 Announce Type: new 
Abstract: Treatment effect heterogeneity refers to the systematic variation in treatment effects across subgroups. There is an increasing need for clinical trials that aim to investigate treatment effect heterogeneity and estimate subgroup-specific responses. While several statistical methods have been proposed to address this problem, existing partitioning-based methods often depend on auxiliary analysis, overlook model uncertainty, or impose inflexible borrowing strength. We propose the Bayesian Hierarchical Adjustable Random Partition (BHARP) model, a self-contained framework that applies a finite mixture model with an unknown number of components to explore the partition space accounting for model uncertainty. The BHARP model jointly estimates subgroup-specific effects and the heterogeneity patterns, and adjusts the borrowing strengths based on within-cluster cohesion without requiring manual calibration. Posterior sampling is performed via a custom reversible-jump Markov chain Monte Carlo sampler tailored to partitioning-based information borrowing in clinical trials. Simulation studies across a range of treatment effect heterogeneity patterns show that the BHARP model achieves better accuracy and precision compared to conventional and advanced methods. We showcase the utilities of the BHARP model in the context of a multi-arm adaptive enrichment trial investigating physical activity interventions in patients with type 2 diabetes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16523v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xianglin Zhao, Shirin Golchi, Jean-Philippe Gouin, Kaberi Dasgupta</dc:creator>
    </item>
    <item>
      <title>Spherical latent space models for social network analysis</title>
      <link>https://arxiv.org/abs/2508.16556</link>
      <description>arXiv:2508.16556v1 Announce Type: new 
Abstract: This article introduces a spherical latent space model for social network analysis, embedding actors on a hypersphere rather than in Euclidean space as in standard latent space models. The spherical geometry facilitates the representation of transitive relationships and community structure, naturally captures cyclical patterns, and ensures bounded distances, thereby mitigating degeneracy issues common in traditional approaches. Bayesian inference is performed via Markov chain Monte Carlo methods to estimate both latent positions and other model parameters. The approach is demonstrated using two benchmark social network datasets, yielding improved model fit and interpretability relative to conventional latent space models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16556v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan Sosa, Carlos Nosa</dc:creator>
    </item>
    <item>
      <title>A nonstationary spatial model of PM2.5 with localized transfer learning from numerical model output</title>
      <link>https://arxiv.org/abs/2508.15978</link>
      <description>arXiv:2508.15978v1 Announce Type: cross 
Abstract: Ambient air pollution measurements from regulatory monitoring networks are routinely used to support epidemiologic studies and environmental policy decision making. However, regulatory monitors are spatially sparse and preferentially located in areas with large populations. Numerical air pollution model output can be leveraged into the inference and prediction of air pollution data combining with measurements from monitors. Nonstationary covariance functions allow the model to adapt to spatial surfaces whose variability changes with location like air pollution data. In the paper, we employ localized covariance parameters learned from the numerical output model to knit together into a global nonstationary covariance, to incorporate in a fully Bayesian model. We model the nonstationary structure in a computationally efficient way to make the Bayesian model scalable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15978v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenlong Gong, Brian J. Reich, Joseph Guinness</dc:creator>
    </item>
    <item>
      <title>Data Gluttony: Epistemic Risks, Dependent Testing and Data Reuse in Large Datasets</title>
      <link>https://arxiv.org/abs/2508.16552</link>
      <description>arXiv:2508.16552v1 Announce Type: cross 
Abstract: Large-scale registries have collected vast amounts of data which has enabled investigators to efficiently conduct studies of observational data. Common practice is for investigators to use all data meeting the inclusion criteria of their study to perform their analysis. We term this common practice data gluttony. It has apparent formal justification insofar as this approach maximizes per-study power. But this comes at a cost: data reuse affects the shape of the tail distribution of inferential errors. Using the theory of risk orderings we demonstrate how positively dependent testing procedures result in strictly riskier distributions of inferential error.
  We identify two remedies to this state of affairs: research portfolio optimization and what we term data temperance. Research portfolio optimization requires that we formulate the enterprise of inference in a utility theoretic framework: associated to each hypothesis to be evaluated is some utility dependent on its truth as well as the impact of the statistical decision rendered on the basis of the data. Under certain models of data governance, this approach can be used to optimally allocate data usage across multiple inferential tasks. On the other hand, data temperance is a more flexible strategy for managing the distribution of inferential errors. Data temperance is the principle that an investigator use only as much data as is necessary to perform the task at hand. This is possible due to the diminishing marginal returns in power and precision in sample size. We analyze the effectiveness of data temperance at reducing the dependence across testing and develop a theory of the capacity of a static database to sustain large numbers of inferential tasks with low probability of inducing pairwise dependent testing procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16552v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Reid Dale, Jordan Rodu, Maria E. Currie, Mike Baiocchi</dc:creator>
    </item>
    <item>
      <title>A correlated pseudo-marginal approach to doubly intractable problems</title>
      <link>https://arxiv.org/abs/2210.02734</link>
      <description>arXiv:2210.02734v2 Announce Type: replace 
Abstract: Doubly intractable models are encountered in a number of fields, e.g. social networks, ecology and epidemiology. Inference for such models requires the evaluation of a likelihood function, whose normalising factor depends on the model parameters and is assumed to be computationally intractable. The normalising constant of the posterior distribution and the additional normalising factor of the likelihood function result in a so-called doubly intractable posterior, for which it is difficult to directly apply Markov chain Monte Carlo methods. We propose a signed pseudo-marginal Metropolis-Hastings algorithm with an unbiased block-Poisson estimator to sample from the posterior distribution of doubly intractable models. As the estimator can be negative, the algorithm targets the absolute value of the estimated posterior and uses an importance sampling estimator to ensure simulation-consistent estimates of the posterior mean of a function of the parameters. The importance sampling estimator can perform poorly when its denominator is close to zero. We derive a finite-sample concentration inequality that ensures, with high probability, that this pathological case does not occur. Our estimator for doubly intractable problems has three advantages over existing estimators. First, the estimator is well-suited for efficient parallelisation and vectorisation. Second, its structure is ideal for correlated pseudo-marginal methods, which are well known to dramatically increase sampling efficiency. Third, the estimator enables the derivation of heuristic guidelines for tuning its hyperparameters under simplifying assumptions. We demonstrate the superior performance of our method in the standard benchmark example that models correlated spatial data using the Ising model, as well as the Kent distribution model for spherical data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.02734v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Yang, Matias Quiroz, Robert Kohn, Scott A. Sisson</dc:creator>
    </item>
    <item>
      <title>The causal effects of modified treatment policies under network interference</title>
      <link>https://arxiv.org/abs/2412.02105</link>
      <description>arXiv:2412.02105v2 Announce Type: replace 
Abstract: Modified treatment policies are a widely applicable class of interventions useful for studying the causal effects of continuous exposures. Approaches to evaluating their causal effects assume no interference, meaning that such effects cannot be learned from data in settings where the exposure of one unit affects the outcomes of others, as is common in spatial or network data. We introduce a new class of intervention, induced modified treatment policies, which we show identify such causal effects in the presence of network interference. Building on recent developments for causal inference in networks, we provide flexible, semi-parametric efficient estimators of the statistical estimand. Numerical experiments demonstrate that an induced modified treatment policy can eliminate the causal, or identification, bias that results from network interference. We use the methodology developed to evaluate the effect of zero-emission vehicle uptake on air pollution in California, strengthening prior evidence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02105v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Salvador V. Balkus, Scott W. Delaney, Nima S. Hejazi</dc:creator>
    </item>
    <item>
      <title>Autocorrelation functions for point-process time series</title>
      <link>https://arxiv.org/abs/2504.08070</link>
      <description>arXiv:2504.08070v2 Announce Type: replace 
Abstract: This article introduces autocorrelograms for time series of point processes. Such time series usually arise when a longer temporal or spatio-temporal point process is sliced into smaller time units; for example, when an annual process is sliced into 365 daily replications. We assume the point processes follow a doubly-stochastic Poisson model with log-Gaussian intensity functions. The proposed autocorrelograms are computationally simple and based on binning. The asymptotic distribution of the autocorrelations is established. The ability of the method to detect the patterns of common autoregressive and moving-average time series models is shown by simulation. Two examples of application to temporal and spatial point-process time series are shown, pertaining bike demand in the Divvy bike-sharing system of Chicago and street theft in Chicago, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08070v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1111/jtsa.70018</arxiv:DOI>
      <dc:creator>Daniel Gervini</dc:creator>
    </item>
    <item>
      <title>Statistical methods: Basic concepts, interpretations, and cautions</title>
      <link>https://arxiv.org/abs/2508.10168</link>
      <description>arXiv:2508.10168v2 Announce Type: replace 
Abstract: The study of associations and their causal explanations is a central research activity whose methodology varies tremendously across fields. Even within specialized subfields, comparisons across textbooks and journals reveals that the basics are subject to considerable variation and controversy. This variation is often obscured by the singular viewpoints presented within textbooks and journal guidelines, which may be deceptively written as if the norms they adopt are unchallenged. Furthermore, human limitations and the vastness within fields imply that no one can have expertise across all subfields and that interpretations will be severely constrained by the limitations of studies of human populations.
  The present chapter outlines an approach to statistical methods that attempts to recognize these problems from the start, rather than assume they are absent as in the claims of 'statistical significance' and 'confidence' ordinarily attached to statistical tests and interval estimates. It does so by grounding models and statistics in data description, and treating inferences from them as speculations based on assumptions that cannot be fully validated or checked using the analysis data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10168v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sander Greenland</dc:creator>
    </item>
    <item>
      <title>Does fertility affect woman's labor force participation in low- and middle-income settings? Findings from a Bayesian nonparametric analysis</title>
      <link>https://arxiv.org/abs/2508.10787</link>
      <description>arXiv:2508.10787v2 Announce Type: replace 
Abstract: Estimating the causal effect of fertility on women's employment is challenging because fertility and labor decisions are jointly determined. The difficulty is amplified in low- and middle-income countries, where longitudinal data are scarce. In this study, we propose a novel approach to estimating the causal effect of fertility on employment using widely available Demographic and Health Survey (DHS) observational data. Using infecundity as an instrument for family size, our approach combines principal stratification with Bayesian Additive Regression Trees to flexibly account for covariate-dependent instrument validity, work with count-valued intermediate variables, and produce estimates of causal effects and effect heterogeneity, i.e., how effects vary with covariates in the survey population. We apply the approach to DHS data from Nigeria, Senegal, and Kenya. We find in the survey sample and general population that an additional child significantly reduces employment among women in Nigeria but has no clear average effect in Senegal or Kenya. Across all three countries, however, there is strong evidence of effect heterogeneity: younger, less-educated women experience large employment penalties, while older or more advantaged women are largely unaffected. Robustness checks confirm that these findings are not sensitive to key modeling assumptions. While limitations remain due to the cross-sectional nature of the DHS data, our results illustrate how flexible non-parametric models can uncover important effect variation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10787v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas Godoy Garraza, Leontine Alkema</dc:creator>
    </item>
    <item>
      <title>Weak Poincar\'e inequality comparisons for ideal and hybrid slice sampling</title>
      <link>https://arxiv.org/abs/2402.13678</link>
      <description>arXiv:2402.13678v2 Announce Type: replace-cross 
Abstract: Using the framework of weak Poincar\'e inequalities, we provide a general comparison between Hybrid and Ideal Slice Sampling in terms of their corresponding Dirichlet forms. In particular, under suitable assumptions Hybrid Slice Sampling inherits fast convergence from Ideal Slice Sampling and conversely. We apply our results to analyse the convergence of the Independent Metropolis-Hastings, stepping-out and shrinkage, as well as Hit-and-Run within slice sampling algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13678v2</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sam Power, Daniel Rudolf, Bj\"orn Sprungk, Andi Q. Wang</dc:creator>
    </item>
  </channel>
</rss>
