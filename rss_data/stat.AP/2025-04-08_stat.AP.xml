<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 09 Apr 2025 01:55:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The Rhythm of Aging: Stability and Drift in Human Senescence</title>
      <link>https://arxiv.org/abs/2504.04143</link>
      <description>arXiv:2504.04143v2 Announce Type: new 
Abstract: Human aging is marked by a steady rise in mortality risk with age - a process demographers describe as senescence. While life expectancy has improved dramatically over the past century, a fundamental question remains: is the rate at which mortality accelerates biologically fixed, or has it shifted across generations? Vaupel's hypothesis suggests that the pace of aging is stable - that humans are not aging more slowly, but simply starting later. To test this, we analyze cohort mortality data from France, Denmark, Italy, and Sweden. We use a two-step framework to first isolate senescent mortality, then decompose the Gompertz slope into three parts: a biological constant, a potential trend, and a cumulative period effect. The results show that most variation in the rate of aging is not biological in origin. Once non-senescent deaths and historical shocks are accounted for, the Gompertz slope is remarkably stable. The fluctuations we see are not signs of changing senescence, but echoes of shared history. Aging itself, it seems, has stayed the same. These findings suggest that while longevity has shifted, the fundamental rhythm of human aging may be biologically fixed - shaped not by evolution, but by history.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04143v2</guid>
      <category>stat.AP</category>
      <category>q-bio.PE</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Silvio Cabral Patricio</dc:creator>
    </item>
    <item>
      <title>BlockingPy: approximate nearest neighbours for blocking of records for entity resolution</title>
      <link>https://arxiv.org/abs/2504.04266</link>
      <description>arXiv:2504.04266v1 Announce Type: new 
Abstract: Entity resolution (probabilistic record linkage, deduplication) is a key step in scientific analysis and data science pipelines involving multiple data sources. The objective of entity resolution is to link records without identifiers that refer to the same entity (e.g., person, company). However, without identifiers, researchers need to specify which records to compare in order to calculate matching probability and reduce computational complexity. One solution is to deterministically block records based on some common variables, such as names, dates of birth or sex. However, this approach assumes that these variables are free of errors and completely observed, which is often not the case. To address this challenge, we have developed a Python package, BlockingPy, which utilises blocking via modern approximate nearest neighbour search and graph algorithms to significantly reduce the number of comparisons. In this paper, we present the design of the package, its functionalities and two case studies related to official statistics. We believe the presented software will be useful for researchers (i.e., social scientists, economists or statisticians) interested in linking data from various sources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04266v1</guid>
      <category>stat.AP</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.CO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tymoteusz Strojny, Maciej Ber\k{e}sewicz</dc:creator>
    </item>
    <item>
      <title>On misconceptions about the Brier score in binary prediction models</title>
      <link>https://arxiv.org/abs/2504.04906</link>
      <description>arXiv:2504.04906v1 Announce Type: new 
Abstract: The Brier score is a widely used metric evaluating overall performance of predictions for binary outcome probabilities in clinical research. However, its interpretation can be complex, as it does not align with commonly taught concepts in medical statistics. Consequently, the Brier score is often misinterpreted, sometimes to a significant extent, a fact that has not been adequately addressed in the literature. This commentary aims to explore prevalent misconceptions surrounding the Brier score and elucidate the reasons these interpretations are incorrect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04906v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Linard Hoessly</dc:creator>
    </item>
    <item>
      <title>Bayesian local clustering of age-period mortality surfaces across multiple countries</title>
      <link>https://arxiv.org/abs/2504.05240</link>
      <description>arXiv:2504.05240v1 Announce Type: new 
Abstract: Although traditional literature on mortality modeling has focused on single countries in isolation, recent contributions have progressively moved toward joint models for multiple countries. Besides favoring borrowing of information to improve age-period forecasts, this perspective has also potentials to infer local similarities among countries' mortality patterns in specific age classes and periods that could unveil unexplored demographic trends, while guiding the design of targeted policies. Advancements along this latter relevant direction are currently undermined by the lack of a multi-country model capable of incorporating the core structures of age-period mortality surfaces together with clustering patterns among countries that are not global, but rather vary locally across different combinations of ages and periods. We cover this gap by developing a novel Bayesian model for log-mortality rates that characterizes the age structure of mortality through a B-spline expansion whose country-specific dynamic coefficients encode both changes of this age structure across periods and also local clustering patterns among countries under a time-dependent random partition prior for these country-specific dynamic coefficients. While flexible, this formulation admits tractable posterior inference leveraging a suitably-designed Gibbs-sampler. The application to mortality data from 14 countries unveils local similarities highlighting both previously-recognized demographic phenomena and also yet-unexplored trends.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05240v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giovanni Roman\`o, Emanuele Aliverti, Daniele Durante</dc:creator>
    </item>
    <item>
      <title>Confirmatory Biomarker Identification via Derandomized Knockoffs for Cox Regression with k-FWER Control</title>
      <link>https://arxiv.org/abs/2504.03907</link>
      <description>arXiv:2504.03907v1 Announce Type: cross 
Abstract: Selecting important features in high-dimensional survival analysis is critical for identifying confirmatory biomarkers while maintaining rigorous error control. In this paper, we propose a derandomized knockoffs procedure for Cox regression that enhances stability in feature selection while maintaining rigorous control over the k-familywise error rate (k-FWER). By aggregating across multiple randomized knockoff realizations, our approach mitigates the instability commonly observed with conventional knockoffs. Through extensive simulations, we demonstrate that our method consistently outperforms standard knockoffs in both selection power and error control. Moreover, we apply our procedure to a clinical dataset on primary biliary cirrhosis (PBC) to identify key prognostic biomarkers associated with patient survival. The results confirm the superior stability of the derandomized knockoffs method, allowing for a more reliable identification of important clinical variables. Additionally, our approach is applicable to datasets containing both continuous and categorical covariates, broadening its utility in real-world biomedical studies. This framework provides a robust and interpretable solution for high-dimensional survival analysis, making it particularly suitable for applications requiring precise and stable variable selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03907v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rui Liu, Nan Sun</dc:creator>
    </item>
    <item>
      <title>Regression Discontinuity Design with Distribution-Valued Outcomes</title>
      <link>https://arxiv.org/abs/2504.03992</link>
      <description>arXiv:2504.03992v1 Announce Type: cross 
Abstract: This article introduces Regression Discontinuity Design (RDD) with Distribution-Valued Outcomes (R3D), extending the standard RDD framework to settings where the outcome is a distribution rather than a scalar. Such settings arise when treatment is assigned at a higher level of aggregation than the outcome-for example, when a subsidy is allocated based on a firm-level revenue cutoff while the outcome of interest is the distribution of employee wages within the firm. Since standard RDD methods cannot accommodate such two-level randomness, I propose a novel approach based on random distributions. The target estimand is a "local average quantile treatment effect", which averages across random quantiles. To estimate this target, I introduce two related approaches: one that extends local polynomial regression to random quantiles and another based on local Fr\'echet regression, a form of functional regression. For both estimators, I establish asymptotic normality and develop uniform, debiased confidence bands together with a data-driven bandwidth selection procedure. Simulations validate these theoretical properties and show existing methods to be biased and inconsistent in this setting. I then apply the proposed methods to study the effects of gubernatorial party control on within-state income distributions in the US, using a close-election design. The results suggest a classic equality-efficiency tradeoff under Democratic governorship, driven by reductions in income at the top of the distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03992v1</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Van Dijcke</dc:creator>
    </item>
    <item>
      <title>Spatially-Heterogeneous Causal Bayesian Networks for Seismic Multi-Hazard Estimation: A Variational Approach with Gaussian Processes and Normalizing Flows</title>
      <link>https://arxiv.org/abs/2504.04013</link>
      <description>arXiv:2504.04013v1 Announce Type: cross 
Abstract: Post-earthquake hazard and impact estimation are critical for effective disaster response, yet current approaches face significant limitations. Traditional models employ fixed parameters regardless of geographical context, misrepresenting how seismic effects vary across diverse landscapes, while remote sensing technologies struggle to distinguish between co-located hazards. We address these challenges with a spatially-aware causal Bayesian network that decouples co-located hazards by modeling their causal relationships with location-specific parameters. Our framework integrates sensing observations, latent variables, and spatial heterogeneity through a novel combination of Gaussian Processes with normalizing flows, enabling us to capture how same earthquake produces different effects across varied geological and topographical features. Evaluations across three earthquakes demonstrate Spatial-VCBN achieves Area Under the Curve (AUC) improvements of up to 35.2% over existing methods. These results highlight the critical importance of modeling spatial heterogeneity in causal mechanisms for accurate disaster assessment, with direct implications for improving emergency response resource allocation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04013v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xuechun Li, Shan Gao, Runyu Gao, Susu Xu</dc:creator>
    </item>
    <item>
      <title>nonprobsvy -- An R package for modern methods for non-probability surveys</title>
      <link>https://arxiv.org/abs/2504.04255</link>
      <description>arXiv:2504.04255v1 Announce Type: cross 
Abstract: The following paper presents {nonprobsvy} -- an {R} package for inference based on non-probability samples. The package implements various approaches that can be categorized into three groups: prediction-based approach, inverse probability weighting and doubly robust approach. In the package, we assume the existence of either population-level data or probability-based population information and leverage the \pkg{survey} package for inference. The package implements both analytical and bootstrap variance estimation for the proposed estimators. In the paper we present the theory behind the package, its functionalities and case study that showcases the usage of the package. The package is aimed at scientists and researchers who would like to use non-probability samples (e.g.big data, opt-in web panels, social media) to accurately estimate population characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04255v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>{\L}ukasz Chrostowski, Piotr Chlebicki, Maciej Ber\k{e}sewicz</dc:creator>
    </item>
    <item>
      <title>Beyond the Hype: Embeddings vs. Prompting for Multiclass Classification Tasks</title>
      <link>https://arxiv.org/abs/2504.04277</link>
      <description>arXiv:2504.04277v1 Announce Type: cross 
Abstract: Are traditional classification approaches irrelevant in this era of AI hype? We show that there are multiclass classification problems where predictive models holistically outperform LLM prompt-based frameworks. Given text and images from home-service project descriptions provided by Thumbtack customers, we build embeddings-based softmax models that predict the professional category (e.g., handyman, bathroom remodeling) associated with each problem description. We then compare against prompts that ask state-of-the-art LLM models to solve the same problem. We find that the embeddings approach outperforms the best LLM prompts in terms of accuracy, calibration, latency, and financial cost. In particular, the embeddings approach has 49.5% higher accuracy than the prompting approach, and its superiority is consistent across text-only, image-only, and text-image problem descriptions. Furthermore, it yields well-calibrated probabilities, which we later use as confidence signals to provide contextualized user experience during deployment. On the contrary, prompting scores are overly uninformative. Finally, the embeddings approach is 14 and 81 times faster than prompting in processing images and text respectively, while under realistic deployment assumptions, it can be up to 10 times cheaper. Based on these results, we deployed a variation of the embeddings approach, and through A/B testing we observed performance consistent with our offline analysis. Our study shows that for multiclass classification problems that can leverage proprietary datasets, an embeddings-based approach may yield unequivocally better results. Hence, scientists, practitioners, engineers, and business leaders can use our study to go beyond the hype and consider appropriate predictive models for their classification use cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04277v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marios Kokkodis, Richard Demsyn-Jones, Vijay Raghavan</dc:creator>
    </item>
    <item>
      <title>A statistical framework for analyzing activity pattern from GPS data</title>
      <link>https://arxiv.org/abs/2504.04316</link>
      <description>arXiv:2504.04316v1 Announce Type: cross 
Abstract: We introduce a novel statistical framework for analyzing the GPS data of a single individual. Our approach models daily GPS observations as noisy measurements of an underlying random trajectory, enabling the definition of meaningful concepts such as the average GPS density function. We propose estimators for this density function and establish their asymptotic properties. To study human activity patterns using GPS data, we develop a simple movement model based on mixture models for generating random trajectories. Building on this framework, we introduce several analytical tools to explore activity spaces and mobility patterns. We demonstrate the effectiveness of our approach through applications to both simulated and real-world GPS data, uncovering insightful mobility trends.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04316v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoyang Wu, Yen-Chi Chen, Adrian Dobra</dc:creator>
    </item>
    <item>
      <title>Constructing the Truth: Text Mining and Linguistic Networks in Public Hearings of Case 03 of the Special Jurisdiction for Peace (JEP)</title>
      <link>https://arxiv.org/abs/2504.04325</link>
      <description>arXiv:2504.04325v2 Announce Type: cross 
Abstract: Case 03 of the Special Jurisdiction for Peace (JEP), focused on the so-called false positives in Colombia, represents one of the most harrowing episodes of the Colombian armed conflict. This article proposes an innovative methodology based on natural language analysis and semantic co-occurrence models to explore, systematize, and visualize narrative patterns present in the public hearings of victims and appearing parties. By constructing skipgram networks and analyzing their modularity, the study identifies thematic clusters that reveal regional and procedural status differences, providing empirical evidence on dynamics of victimization, responsibility, and acknowledgment in this case. This computational approach contributes to the collective construction of both judicial and extrajudicial truth, offering replicable tools for other transitional justice cases. The work is grounded in the pillars of truth, justice, reparation, and non-repetition, proposing a critical and in-depth reading of contested memories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04325v2</guid>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan Sosa, Alejandro Urrego-L\'opez, Cesar Prieto, Emma J. Camargo-D\'iaz</dc:creator>
    </item>
    <item>
      <title>Forecasting a time series of Lorenz curves: One-way functional analysis of variance</title>
      <link>https://arxiv.org/abs/2504.04437</link>
      <description>arXiv:2504.04437v1 Announce Type: cross 
Abstract: The Lorenz curve is a fundamental tool for analysing income and wealth distribution and inequality at national and regional levels. We utilise a one-way functional analysis of variance to decompose a time series of Lorenz curves and develop a method for producing one-step-ahead point and interval forecasts. The one-way functional analysis of variance is easily interpretable by decomposing an array into a functional grand effect, a functional row effect and residual functions. We evaluate and compare the forecast accuracy between the functional analysis of variance and three non-functional methods using the Italian household income and wealth data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04437v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Han Lin Shang</dc:creator>
    </item>
    <item>
      <title>Sequential Hierarchical Regression Imputation with Variable Selection Routines</title>
      <link>https://arxiv.org/abs/2504.04539</link>
      <description>arXiv:2504.04539v1 Announce Type: cross 
Abstract: We aim to incorporate variable selection routines into variable-by-variable (or sequential) imputation in clustered data to achieve computational improvement in applications with large-scale health data. Specifically, we utilize variable selection routines using spike-and-slab priors within the Bayesian variable selection routine. The choice of these priors allows us to ``force'' variables of importance (e.g., design variables or variables known to play a role in the missingness mechanism) into the imputation models based on a class of mixed-effects models. Our ultimate goal is to improve computational speed by removing unnecessary variables. We employ Markov chain Monte Carlo techniques to sample from the implied posterior distributions for model unknowns as well as missing data. We assess the performance of our proposed methodology via simulation studies. Our results show that our proposed algorithms lead to satisfactory estimates and, in some instances, outperform some of the existing methods that are available to practitioners. We illustrate our methods using a national survey of children's health.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04539v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiushuang Li, Recai Yucel</dc:creator>
    </item>
    <item>
      <title>Variational Bayesian Multiple Imputation in High-Dimensional Regression Models With Missing Responses</title>
      <link>https://arxiv.org/abs/2504.04547</link>
      <description>arXiv:2504.04547v1 Announce Type: cross 
Abstract: Multiple imputation has become one of the standard methods in drawing inferences in many incomplete data applications. Applications of multiple imputation in relatively more complex settings, such as high-dimensional clustered data, require specialized methods to overcome the computational burden. Using linear mixed-effects models, we develop such methods that can be applied to continuous, binary, or categorical incomplete data by employing variational Bayesian inference to sample the posterior predictive distribution of the missing data. These methods specifically target high-dimensional data and work with the spike-and-slab prior, which automatically selects the variables of importance to be in the imputation model. The individual regression computation is then incorporated into a variable-by-variable imputation algorithm. Finally, we use a calibration-based algorithm to adopt these methods to multiple imputations of categorical variables. We present a simulation study and illustrate on National Survey of Children's Health data to assess the performance of these methods in a repetitive sampling framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04547v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiushuang Li, Recai Yucel</dc:creator>
    </item>
    <item>
      <title>Quantifying uncertainty of individualized treatment effects in right-censored survival data: A comparison of Bayesian additive regression trees and causal survival forest</title>
      <link>https://arxiv.org/abs/2504.04571</link>
      <description>arXiv:2504.04571v1 Announce Type: cross 
Abstract: Estimation of individualized treatment effects (ITE), also known as conditional average treatment effects (CATE), is an active area of methodology development. However, much less attention has been paid to the quantification of uncertainty of ITE/CATE estimates in right-censored survival data. Here we undertake an extensive simulation study to examine the coverage of interval estimates from two popular estimation algorithms, Bayesian additive regression trees (BART) and causal survival forest (CSF). We conducted simulation designs from 3 different settings: first, in a setting where BART was developed for an accelerated failure time model; second, where CSF was developed; and finally, a ``neutral'' simulation taken from a setting where neither BART nor CSF was developed. BART outperformed CSF in all three simulation settings. Both the BART and CSF algorithms involve multiple hyperparameters, and BART credible intervals had better coverage than the CSF confidence intervals under the default values, as well as under optimized values, of these hyperparameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04571v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daijiro Kabata, Nicholas C. Henderson, Ravi Varadhan</dc:creator>
    </item>
    <item>
      <title>How Untested Modeling Assumptions Influence the U.S. EPA's Estimates of Population-Level Ozone Exposure Risk</title>
      <link>https://arxiv.org/abs/2504.04591</link>
      <description>arXiv:2504.04591v1 Announce Type: cross 
Abstract: In recent reviews of the National Ambient Air Quality Standards (NAAQS) for ozone, the U.S. Environmental Protection Agency (U.S. EPA) has presented estimates of the health risks associated with ozone exposure. One way in which the U.S. EPA calculates population-level ozone risk estimates is through a simulation model that calculates ozone exposures and the resulting lung function decrements for a simulated population. This simulation model includes several random error terms to capture inter- and intra-individual variability in responsiveness to ozone exposure. In this manuscript we undertake a sensitivity analysis examining the influence of untested assumptions about these error terms. We show that ad hoc bounds imposed on the error terms and the frequency of redrawing the intra-individual error terms have a strong influence on the population-level ozone exposure risk reported by the U.S. EPA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04591v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Garrett Glasgow, Anne E. Smith</dc:creator>
    </item>
    <item>
      <title>3DM-WeConvene: Learned Image Compression with 3D Multi-Level Wavelet-Domain Convolution and Entropy Model</title>
      <link>https://arxiv.org/abs/2504.04658</link>
      <description>arXiv:2504.04658v1 Announce Type: cross 
Abstract: Learned image compression (LIC) has recently made significant progress, surpassing traditional methods. However, most LIC approaches operate mainly in the spatial domain and lack mechanisms for reducing frequency-domain correlations. To address this, we propose a novel framework that integrates low-complexity 3D multi-level Discrete Wavelet Transform (DWT) into convolutional layers and entropy coding, reducing both spatial and channel correlations to improve frequency selectivity and rate-distortion (R-D) performance.
  Our proposed 3D multi-level wavelet-domain convolution (3DM-WeConv) layer first applies 3D multi-level DWT (e.g., 5/3 and 9/7 wavelets from JPEG 2000) to transform data into the wavelet domain. Then, different-sized convolutions are applied to different frequency subbands, followed by inverse 3D DWT to restore the spatial domain. The 3DM-WeConv layer can be flexibly used within existing CNN-based LIC models.
  We also introduce a 3D wavelet-domain channel-wise autoregressive entropy model (3DWeChARM), which performs slice-based entropy coding in the 3D DWT domain. Low-frequency (LF) slices are encoded first to provide priors for high-frequency (HF) slices.
  A two-step training strategy is adopted: first balancing LF and HF rates, then fine-tuning with separate weights.
  Extensive experiments demonstrate that our framework consistently outperforms state-of-the-art CNN-based LIC methods in R-D performance and computational complexity, with larger gains for high-resolution images. On the Kodak, Tecnick 100, and CLIC test sets, our method achieves BD-Rate reductions of -12.24%, -15.51%, and -12.97%, respectively, compared to H.266/VVC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04658v1</guid>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haisheng Fu, Jie Liang, Feng Liang, Zhenman Fang, Guohe Zhang, Jingning Han</dc:creator>
    </item>
    <item>
      <title>SurvSurf: a partially monotonic neural network for first-hitting time prediction of intermittently observed discrete and continuous sequential events</title>
      <link>https://arxiv.org/abs/2504.04997</link>
      <description>arXiv:2504.04997v1 Announce Type: cross 
Abstract: We propose a neural-network based survival model (SurvSurf) specifically designed for direct and simultaneous probabilistic prediction of the first hitting time of sequential events from baseline. Unlike existing models, SurvSurf is theoretically guaranteed to never violate the monotonic relationship between the cumulative incidence functions of sequential events, while allowing nonlinear influence from predictors. It also incorporates implicit truths for unobserved intermediate events in model fitting, and supports both discrete and continuous time and events. We also identified a variant of the Integrated Brier Score (IBS) that showed robust correlation with the mean squared error (MSE) between the true and predicted probabilities by accounting for implied truths about the missing intermediate events. We demonstrated the superiority of SurvSurf compared to modern and traditional predictive survival models in two simulated datasets and two real-world datasets, using MSE, the more robust IBS and by measuring the extent of monotonicity violation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04997v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yichen Kelly Chen, S\"oren Dittmer, Kinga Bernatowicz, Josep Ar\'us-Pous, Kamen Bliznashki, John Aston, James H. F. Rudd, Carola-Bibiane Sch\"onlieb, James Jones, Michael Roberts</dc:creator>
    </item>
    <item>
      <title>Dominating Hyperplane Regularization for Variable Selection in Multivariate Count Regression</title>
      <link>https://arxiv.org/abs/2504.05034</link>
      <description>arXiv:2504.05034v1 Announce Type: cross 
Abstract: Identifying relevant factors that influence the multinomial counts in compositional data is difficult in high dimensional settings due to the complex associations and overdispersion. Multivariate count models such as the Dirichlet-multinomial (DM), negative multinomial, and generalized DM accommodate overdispersion but are difficult to optimize due to their non-concave likelihood functions. Further, for the class of regression models that associate covariates to the multivariate count outcomes, variable selection becomes necessary as the number of potentially relevant factors becomes large. The sparse group lasso (SGL) is a natural choice for regularizing these models. Motivated by understanding the associations between water quality and benthic macroinvertebrate compositions in Canada's Athabasca oil sands region, we develop dominating hyperplane regularization (DHR), a novel method for optimizing regularized regression models with the SGL penalty. Under the majorization-minimization framework, we show that applying DHR to a SGL penalty gives rise to a surrogate function that can be expressed as a weighted ridge penalty. Consequently, we prove that for multivariate count regression models with the SGL penalty, the optimization leads to an iteratively reweighted Poisson ridge regression. We demonstrate stable optimization and high performance of our algorithm through simulation and real world application to benthic macroinvertebrate compositions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05034v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alysha Cooper (Department of Mathematics,Statistics, University of Guelph), Zeny Feng (Department of Mathematics,Statistics, University of Guelph), Ayesha Ali (Department of Mathematics,Statistics, University of Guelph), Tim Arciszewski (Alberta Environment,Protected Areas), Lorna Deeth (Department of Mathematics,Statistics, University of Guelph)</dc:creator>
    </item>
    <item>
      <title>Flexible Estimation of the Heterogeneous Non-Parametric Component in a Relative Survival Cure Model</title>
      <link>https://arxiv.org/abs/2504.05093</link>
      <description>arXiv:2504.05093v1 Announce Type: cross 
Abstract: Estimating the cure fraction in a diseased population, especially in the presence of competing mortality causes, is crucial for both patients and clinicians. It offers a valuable measure for monitoring and interpreting trends in disease outcomes. When information on the cause of death is unavailable or unreliable, the Relative Survival (RS) framework is the preferred approach for estimating Net Survival, which represents survival in a hypothetical scenario where the disease of interest is the only possible cause of death. In the context of cancer, RS often reaches a plateau, indicating that a portion of diagnosed patients is cured, as they have the same risk of dying as a comparable group of healthy individuals with similar demographic characteristics. Classical RS cure models use logistic regression to estimate the fraction of cured patients. However, this functional form is somewhat arbitrary, and misspecifying it can severely distort the resulting cure indicators. Consequently, evaluations of the efficacy of cancer treatments at the population level could be inaccurate, leading to biased decision-making regarding patient care. In this paper, we address this issue by relaxing the parametric assumption and considering flexible functions of the covariates within the framework of \textit{Generalized Models} and \textit{Neural Networks}. We design an EM algorithm for these RS cure models and conduct a simulation study to compare our proposals with the classical approach. We apply our methodology to a real-world dataset from a historical Italian cancer registry. The results demonstrate that our proposed models outperform the classical approach and provide valuable insights into the survival outcomes of Italian colon cancer patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05093v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fabrizio Di Mari, Roberto Rocci, Silvia Rossi, Giovanna Tagliabue, Roberta De Angelis</dc:creator>
    </item>
    <item>
      <title>Eigenvalue-Based Randomness Test for Residual Diagnostics in Panel Data Models</title>
      <link>https://arxiv.org/abs/2504.05297</link>
      <description>arXiv:2504.05297v1 Announce Type: cross 
Abstract: This paper introduces the Eigenvalue-Based Randomness (EBR) test - a novel approach rooted in the Tracy-Widom law from random matrix theory - and applies it to the context of residual analysis in panel data models. Unlike traditional methods, which target specific issues like cross-sectional dependence or autocorrelation, the EBR test simultaneously examines multiple assumptions by analyzing the largest eigenvalue of a symmetrized residual matrix. Monte Carlo simulations demonstrate that the EBR test is particularly robust in detecting not only standard violations such as autocorrelation and linear cross-sectional dependence (CSD) but also more intricate non-linear and non-monotonic dependencies, making it a comprehensive and highly flexible tool for enhancing the reliability of panel data analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05297v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcell T. Kurbucz, Betsab\'e P\'erez Garrido, Antal Jakov\'ac</dc:creator>
    </item>
    <item>
      <title>Predicting Census Survey Response Rates With Parsimonious Additive Models and Structured Interactions</title>
      <link>https://arxiv.org/abs/2108.11328</link>
      <description>arXiv:2108.11328v5 Announce Type: replace-cross 
Abstract: In this paper, we consider the problem of predicting survey response rates using a family of flexible and interpretable nonparametric models. The study is motivated by the US Census Bureau's well-known ROAM application, which uses a linear regression model trained on the US Census Planning Database data to identify hard-to-survey areas. A crowdsourcing competition (Erdman and Bates, 2016) organized more than ten years ago revealed that machine learning methods based on ensembles of regression trees led to the best performance in predicting survey response rates; however, the corresponding models could not be adopted for the intended application due to their black-box nature. We consider nonparametric additive models with a small number of main and pairwise interaction effects using $\ell_0$-based penalization. From a methodological viewpoint, we study our estimator's computational and statistical aspects and discuss variants incorporating strong hierarchical interactions. Our algorithms (open-sourced on GitHub) extend the computational frontiers of existing algorithms for sparse additive models to be able to handle datasets relevant to the application we consider. We discuss and interpret findings from our model on the US Census Planning Database. In addition to being useful from an interpretability standpoint, our models lead to predictions comparable to popular black-box machine learning methods based on gradient boosting and feedforward neural networks - suggesting that it is possible to have models that have the best of both worlds: good model accuracy and interpretability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2108.11328v5</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1214/24-AOAS1929</arxiv:DOI>
      <arxiv:journal_reference>The Annals of Applied Statistics 2025, Vol. 19, No. 1, 94-120</arxiv:journal_reference>
      <dc:creator>Shibal Ibrahim, Peter Radchenko, Emanuel Ben-David, Rahul Mazumder</dc:creator>
    </item>
    <item>
      <title>CDsampling: An R Package for Constrained D-Optimal Sampling in Paid Research Studies</title>
      <link>https://arxiv.org/abs/2410.20606</link>
      <description>arXiv:2410.20606v3 Announce Type: replace-cross 
Abstract: In the context of paid research studies and clinical trials, budget considerations often require patient sampling from available populations which comes with inherent constraints. We introduce the R package CDsampling, which is the first to our knowledge to integrate optimal design theories within the framework of constrained sampling. This package offers the possibility to find both D-optimal approximate and exact allocations for samplings with or without constraints. Additionally, it provides functions to find constrained uniform sampling as a robust sampling strategy when the model information is limited. To demonstrate its efficacy, we provide simulated examples and a real-data example with datasets embedded in the package and compare them with classical sampling methods. Furthermore, the package revisits the theoretical results of the Fisher information matrix for generalized linear models (including regular linear regression model) and multinomial logistic models, offering functions for its computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20606v3</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifei Huang, Liping Tong, Jie Yang</dc:creator>
    </item>
    <item>
      <title>Feature Selection for Latent Factor Models</title>
      <link>https://arxiv.org/abs/2412.10128</link>
      <description>arXiv:2412.10128v2 Announce Type: replace-cross 
Abstract: Feature selection is crucial for pinpointing relevant features in high-dimensional datasets, mitigating the 'curse of dimensionality,' and enhancing machine learning performance. Traditional feature selection methods for classification use data from all classes to select features for each class. This paper explores feature selection methods that select features for each class separately, using class models based on low-rank generative methods and introducing a signal-to-noise ratio (SNR) feature selection criterion. This novel approach has theoretical true feature recovery guarantees under certain assumptions and is shown to outperform some existing feature selection methods on standard classification datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10128v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rittwika Kansabanik, Adrian Barbu</dc:creator>
    </item>
    <item>
      <title>Context-Alignment: Activating and Enhancing LLM Capabilities in Time Series</title>
      <link>https://arxiv.org/abs/2501.03747</link>
      <description>arXiv:2501.03747v2 Announce Type: replace-cross 
Abstract: Recently, leveraging pre-trained Large Language Models (LLMs) for time series (TS) tasks has gained increasing attention, which involves activating and enhancing LLMs' capabilities. Many methods aim to activate LLMs' capabilities based on token-level alignment but overlook LLMs' inherent strength on natural language processing -- their deep understanding of linguistic logic and structure rather than superficial embedding processing. We propose Context-Alignment, a new paradigm that aligns TS with a linguistic component in the language environments familiar to LLMs to enable LLMs to contextualize and comprehend TS data, thereby activating their capabilities. Specifically, such context-level alignment comprises structural alignment and logical alignment, which is achieved by a Dual-Scale Context-Alignment GNNs (DSCA-GNNs) applied to TS-language multimodal inputs. Structural alignment utilizes dual-scale nodes to describe hierarchical structure in TS-language, enabling LLMs treat long TS data as a whole linguistic component while preserving intrinsic token features. Logical alignment uses directed edges to guide logical relationships, ensuring coherence in the contextual semantics. Demonstration examples prompt are employed to construct Demonstration Examples based Context-Alignment (DECA) following DSCA-GNNs framework. DECA can be flexibly and repeatedly integrated into various layers of pre-trained LLMs to improve awareness of logic and structure, thereby enhancing performance. Extensive experiments show the effectiveness of DECA and the importance of Context-Alignment across tasks, particularly in few-shot and zero-shot forecasting, confirming that Context-Alignment provide powerful prior knowledge on context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03747v2</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxiao Hu, Qian Li, Dongxiao Zhang, Jinyue Yan, Yuntian Chen</dc:creator>
    </item>
  </channel>
</rss>
