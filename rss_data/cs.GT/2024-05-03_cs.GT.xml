<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 03 May 2024 04:00:34 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 03 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>LOQA: Learning with Opponent Q-Learning Awareness</title>
      <link>https://arxiv.org/abs/2405.01035</link>
      <description>arXiv:2405.01035v1 Announce Type: new 
Abstract: In various real-world scenarios, interactions among agents often resemble the dynamics of general-sum games, where each agent strives to optimize its own utility. Despite the ubiquitous relevance of such settings, decentralized machine learning algorithms have struggled to find equilibria that maximize individual utility while preserving social welfare. In this paper we introduce Learning with Opponent Q-Learning Awareness (LOQA), a novel, decentralized reinforcement learning algorithm tailored to optimizing an agent's individual utility while fostering cooperation among adversaries in partially competitive environments. LOQA assumes the opponent samples actions proportionally to their action-value function Q. Experimental results demonstrate the effectiveness of LOQA at achieving state-of-the-art performance in benchmark scenarios such as the Iterated Prisoner's Dilemma and the Coin Game. LOQA achieves these outcomes with a significantly reduced computational footprint, making it a promising approach for practical multi-agent applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01035v1</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Milad Aghajohari, Juan Agustin Duque, Tim Cooijmans, Aaron Courville</dc:creator>
    </item>
    <item>
      <title>Two competing populations with a common environmental resource</title>
      <link>https://arxiv.org/abs/2405.01437</link>
      <description>arXiv:2405.01437v1 Announce Type: new 
Abstract: Feedback-evolving games is a framework that models the co-evolution between payoff functions and an environmental state. It serves as a useful tool to analyze many social dilemmas such as natural resource consumption, behaviors in epidemics, and the evolution of biological populations. However, it has primarily focused on the dynamics of a single population of agents. In this paper, we consider the impact of two populations of agents that share a common environmental resource. We focus on a scenario where individuals in one population are governed by an environmentally "responsible" incentive policy, and individuals in the other population are environmentally "irresponsible". An analysis on the asymptotic stability of the coupled system is provided, and conditions for which the resource collapses are identified. We then derive consumption rates for the irresponsible population that optimally exploit the environmental resource, and analyze how incentives should be allocated to the responsible population that most effectively promote the environment via a sensitivity analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01437v1</guid>
      <category>cs.GT</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>q-bio.PE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keith Paarporn, James Nelson</dc:creator>
    </item>
    <item>
      <title>Pilot Contamination in Massive MIMO Systems: Challenges and Future Prospects</title>
      <link>https://arxiv.org/abs/2404.19238</link>
      <description>arXiv:2404.19238v1 Announce Type: cross 
Abstract: Massive multiple input multiple output (M-MIMO) technology plays a pivotal role in fifth-generation (5G) and beyond communication systems, offering a wide range of benefits, from increased spectral efficiency (SE) to enhanced energy efficiency and higher reliability. However, these advantages are contingent upon precise channel state information (CSI) availability at the base station (BS). Ensuring precise CSI is challenging due to the constrained size of the coherence interval and the resulting limitations on pilot sequence length. Therefore, reusing pilot sequences in adjacent cells introduces pilot contamination, hindering SE enhancement. This paper reviews recent advancements and addresses research challenges in mitigating pilot contamination and improving channel estimation, categorizing the existing research into three broader categories: pilot assignment schemes, advanced signal processing methods, and advanced channel estimation techniques. Salient representative pilot mitigation/assignment techniques are analyzed and compared in each category. Lastly, possible future research directions are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19238v1</guid>
      <category>cs.IT</category>
      <category>cs.DC</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Muhammad Kamran Saeed, Ashfaq Khokhar, Shakil Ahmed</dc:creator>
    </item>
    <item>
      <title>Optimal Pricing for Linear-Quadratic Games with Nonlinear Interaction Between Agents</title>
      <link>https://arxiv.org/abs/2405.01047</link>
      <description>arXiv:2405.01047v1 Announce Type: cross 
Abstract: This paper studies a class of network games with linear-quadratic payoffs and externalities exerted through a strictly concave interaction function. This class of game is motivated by the diminishing marginal effects with peer influences. We analyze the optimal pricing strategy for this class of network game. First, we prove the existence of a unique Nash Equilibrium (NE). Second, we study the optimal pricing strategy of a monopolist selling a divisible good to agents. We show that the optimal pricing strategy, found by solving a bilevel optimization problem, is strictly better when the monopolist knows the network structure as opposed to the best strategy agnostic to network structure. Numerical experiments demonstrate that in most cases, the maximum revenue is achieved with an asymmetric network. These results contrast with the previously studied case of linear interaction function, where a network-independent price is proven optimal with symmetric networks. Lastly, we describe an efficient algorithm to find the optimal pricing strategy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01047v1</guid>
      <category>math.OC</category>
      <category>cs.GT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiamin Cai, Chenyue Zhang, Hoi-To Wai</dc:creator>
    </item>
    <item>
      <title>Low Revenue in Display Ad Auctions: Algorithmic Collusion vs. Non-Quasilinear Preferences</title>
      <link>https://arxiv.org/abs/2312.00243</link>
      <description>arXiv:2312.00243v2 Announce Type: replace 
Abstract: The transition of display ad exchanges from second-price to first-price auctions has raised questions about its impact on revenue, but evaluating these changes empirically proves challenging. Automated bidding agents play a significant role in this transition, often employing dynamic strategies that evolve through exploration and exploitation rather than using the static game-theoretical equilibrium strategies. Thus revenue equivalence between first- and second-price auctions might not hold. Research on algorithmic collusion in display ad auctions found that first-price auctions can induce Q-learning agents to tacitly collude below the Nash equilibrium, which leads to lower revenue compared to the second-price auction. Our analysis explores widespread online learning algorithms' convergence behavior in both complete and incomplete information models but does not find systematic deviance from equilibrium behavior. Convergence for Q-learning depends on hyperparameters and initializations, and algorithmic collusion also vanishes when Q-learning agents are competing against other learning algorithms. The objective of bidding agents in these auctions is typically to maximize return-on-investment or return-on-spend, but not necessarily payoff maximization. The revenue comparison under such utility functions is an open question. Analytical derivations of equilibrium are challenging, but learning algorithms allow us to approximate equilibria and predict the outcome when agents have such non-quasilinear objectives. Our analysis shows that if learning agents aim to optimize such objectives rather than payoff, then the second-price auction achieves higher expected revenue compared to the first-price auction. Understanding the intricate interplay of auction rules, learning algorithms, and utility models is crucial in the ever-evolving world of advertising markets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.00243v2</guid>
      <category>cs.GT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martin Bichler, Alok Gupta, Laura Mathews, Matthias Oberlechner</dc:creator>
    </item>
    <item>
      <title>Near-Optimal Policy Optimization for Correlated Equilibrium in General-Sum Markov Games</title>
      <link>https://arxiv.org/abs/2401.15240</link>
      <description>arXiv:2401.15240v2 Announce Type: replace-cross 
Abstract: We study policy optimization algorithms for computing correlated equilibria in multi-player general-sum Markov Games. Previous results achieve $O(T^{-1/2})$ convergence rate to a correlated equilibrium and an accelerated $O(T^{-3/4})$ convergence rate to the weaker notion of coarse correlated equilibrium. In this paper, we improve both results significantly by providing an uncoupled policy optimization algorithm that attains a near-optimal $\tilde{O}(T^{-1})$ convergence rate for computing a correlated equilibrium. Our algorithm is constructed by combining two main elements (i) smooth value updates and (ii) the optimistic-follow-the-regularized-leader algorithm with the log barrier regularizer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.15240v2</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Cai, Haipeng Luo, Chen-Yu Wei, Weiqiang Zheng</dc:creator>
    </item>
    <item>
      <title>Age of Information Minimization using Multi-agent UAVs based on AI-Enhanced Mean Field Resource Allocation</title>
      <link>https://arxiv.org/abs/2405.00056</link>
      <description>arXiv:2405.00056v2 Announce Type: replace-cross 
Abstract: Unmanned Aerial Vehicle (UAV) swarms play an effective role in timely data collection from ground sensors in remote and hostile areas. Optimizing the collective behavior of swarms can improve data collection performance. This paper puts forth a new mean field flight resource allocation optimization to minimize age of information (AoI) of sensory data, where balancing the trade-off between the UAVs movements and AoI is formulated as a mean field game (MFG). The MFG optimization yields an expansive solution space encompassing continuous state and action, resulting in significant computational complexity. To address practical situations, we propose, a new mean field hybrid proximal policy optimization (MF-HPPO) scheme to minimize the average AoI by optimizing the UAV's trajectories and data collection scheduling of the ground sensors given mixed continuous and discrete actions. Furthermore, a long short term memory (LSTM) is leveraged in MF-HPPO to predict the time-varying network state and stabilize the training. Numerical results demonstrate that the proposed MF-HPPO reduces the average AoI by up to 45 percent and 57 percent in the considered simulation setting, as compared to multi-agent deep Q-learning (MADQN) method and non-learning random algorithm, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00056v2</guid>
      <category>eess.SY</category>
      <category>cs.GT</category>
      <category>cs.SY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yousef Emami, Hao Gao, Kai Li, Luis Almeida, Eduardo Tovar, Zhu Han</dc:creator>
    </item>
  </channel>
</rss>
