<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.LO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.LO</link>
    <description>cs.LO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.LO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 05 Dec 2025 02:32:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Hypernetwork Theory: The Structural Kernel</title>
      <link>https://arxiv.org/abs/2512.03091</link>
      <description>arXiv:2512.03091v1 Announce Type: new 
Abstract: Modelling across engineering, systems science, and formal methods remains limited by binary relations, implicit semantics, and diagram-centred notations that obscure multilevel structure and hinder mechanisation. Hypernetwork Theory (HT) addresses these gaps by treating the n-ary relation as the primary modelling construct. Each relation is realised as a typed hypersimplex - alpha (conjunctive, part-whole) or beta (disjunctive, taxonomic) - bound to a relation symbol R that fixes arity and ordered roles. Semantics are embedded directly in the construct, enabling hypernetworks to represent hierarchical and heterarchical systems without reconstruction or tool-specific interpretation.
  This paper presents the structural kernel of HT. It motivates typed n-ary relational modelling, formalises the notation and axioms (A1-A5) for vertices, simplices, hypersimplices, boundaries, and ordering, and develops a complete algebra of structural composition. Five operators - merge, meet, difference, prune, and split - are defined by deterministic conditions and decision tables that ensure semantics-preserving behaviour and reconcile the Open World Assumption with closure under rules. Their deterministic algorithms show that HT supports reproducible and mechanisable model construction, comparison, decomposition, and restructuring.
  The resulting framework elevates hypernetworks from symbolic collections to structured, executable system models, providing a rigorous and extensible foundation for mechanisable multilevel modelling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03091v1</guid>
      <category>cs.LO</category>
      <category>cs.MA</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Richard D. Charlesworth</dc:creator>
    </item>
    <item>
      <title>A Cut-Free Sequent Calculus for the Analysis of Finite-Trace Properties in Concurrent Systems</title>
      <link>https://arxiv.org/abs/2512.03164</link>
      <description>arXiv:2512.03164v1 Announce Type: new 
Abstract: We address the problem of identifying a proof-theoretic framework that enables a compositional analysis of finite-trace properties in concurrent systems, with a particular focus on those specified via prefix-closure. To this end, we investigate the interaction of a prefix-closure operator and its residual (with respect to set-theoretic inclusion) with language intersection, union, and concatenation, and introduce the variety of closure $\ell$-monoids as a minimal algebraic abstraction of finite-trace properties to be conveniently described within an analytic proof system. Closure $\ell$-monoids are division-free reducts of distributive residuated lattices equipped with a forward diamond/backward box residuated pair of unary modal operators, where the diamond is a topological closure operator satisfying $\Diamond(x \cdot y) \leq \Diamond x \cdot \Diamond y$. As a logical counterpart to these structures, we present $\mathsf{LMC}$, a Gentzen-style system based on the division-free fragment of the Distributive Full Lambek Calculus. In $\mathsf{LMC}$, structural terms are built from formulas using Belnap-style structural operators for monoid multiplication, meet, and diamond. The rules for the modalities and the structural diamond are taken from Moortgat's system $\mathsf{NL}(\Diamond)$. We show that the calculus is sound and complete with respect to the variety of closure $\ell$-monoids and that it admits cut elimination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03164v1</guid>
      <category>cs.LO</category>
      <category>math.LO</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ludovico Fusco, Alessandro Aldini</dc:creator>
    </item>
    <item>
      <title>The Seifert-van Kampen Theorem via Computational Paths: A Formalized Approach to Computing Fundamental Groups</title>
      <link>https://arxiv.org/abs/2512.03175</link>
      <description>arXiv:2512.03175v1 Announce Type: new 
Abstract: The Seifert-van Kampen theorem computes the fundamental group of a space from the fundamental groups of its constituents. We formalize this theorem within the framework of computational paths, an approach to equality where witnesses are explicit sequences of rewrites governed by the confluent, terminating LNDEQ-TRS. Our contributions are: (i) pushouts as higher-inductive types with explicit path constructors; (ii) free products and amalgamated free products as quotients of word representations; (iii) an encode-decode proof establishing pi_1(Pushout(A, B, C), f, g) cong pi_1(A) *_{pi_1(C)} pi_1(B); and (iv) applications to the figure-eight (pi_1(S^1 v S^1) cong Z * Z) and 2-sphere (pi_1(S^2) cong 1). The framework makes coherence witnesses explicit as rewrite derivations. The development is formalized in Lean 4, where the pushout axioms and the encode map are assumed, while the decode map, amalgamation compatibility, and applications are fully mechanized (2050 lines). This demonstrates that the encode-decode method for higher-inductive types becomes fully constructive when path equality is decidable via normalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03175v1</guid>
      <category>cs.LO</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arthur F. Ramos, Tiago M. L. de Veras, Ruy J. G. B. de Queiroz, Anjolina G. de Oliveira</dc:creator>
    </item>
    <item>
      <title>Formal Analysis of the Sigmoid Function and Formal Proof of the Universal Approximation Theorem</title>
      <link>https://arxiv.org/abs/2512.03635</link>
      <description>arXiv:2512.03635v1 Announce Type: new 
Abstract: This paper presents a formalized analysis of the sigmoid function and a fully mechanized proof of the Universal Approximation Theorem (UAT) in Isabelle/HOL, a higher-order logic theorem prover. The sigmoid function plays a fundamental role in neural networks; yet, its formal properties, such as differentiability, higher-order derivatives, and limit behavior, have not previously been comprehensively mechanized in a proof assistant. We present a rigorous formalization of the sigmoid function, proving its monotonicity, smoothness, and higher-order derivatives. We provide a constructive proof of the UAT, demonstrating that neural networks with sigmoidal activation functions can approximate any continuous function on a compact interval. Our work identifies and addresses gaps in Isabelle/HOL's formal proof libraries and introduces simpler methods for reasoning about the limits of real functions. By exploiting theorem proving for AI verification, our work enhances trust in neural networks and contributes to the broader goal of verified and trustworthy machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03635v1</guid>
      <category>cs.LO</category>
      <category>cs.SE</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dustin Bryant, Jim Woodcock, Simon Foster</dc:creator>
    </item>
    <item>
      <title>Approximate Optimal Active Learning of Decision Trees</title>
      <link>https://arxiv.org/abs/2512.03971</link>
      <description>arXiv:2512.03971v1 Announce Type: new 
Abstract: We consider the problem of actively learning an unknown binary decision tree using only membership queries, a setting in which the learner must reason about a large hypothesis space while maintaining formal guarantees. Rather than enumerating candidate trees or relying on heuristic impurity or entropy measures, we encode the entire space of bounded-depth decision trees symbolically in SAT formulas. We propose a symbolic method for active learning of decision trees, in which approximate model counting is used to estimate the reduction of the hypothesis space caused by each potential query, enabling near-optimal query selection without full model enumeration. The resulting learner incrementally strengthens a CNF representation based on observed query outcomes, and approximate model counter ApproxMC is invoked to quantify the remaining version space in a sound and scalable manner. Additionally, when ApproxMC stagnates, a functional equivalence check is performed to verify that all remaining hypotheses are functionally identical. Experiments on decision trees show that the method reliably converges to the correct model using only a handful of queries, while retaining a rigorous SAT-based foundation suitable for formal analysis and verification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03971v1</guid>
      <category>cs.LO</category>
      <category>cs.SE</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zunchen Huang, Chenglu Jin</dc:creator>
    </item>
    <item>
      <title>Beyond the Black Box: A Cognitive Architecture for Explainable and Aligned AI</title>
      <link>https://arxiv.org/abs/2512.03072</link>
      <description>arXiv:2512.03072v1 Announce Type: cross 
Abstract: Current AI paradigms, as "architects of experience," face fundamental challenges in explainability and value alignment. This paper introduces "Weight-Calculatism," a novel cognitive architecture grounded in first principles, and demonstrates its potential as a viable pathway toward Artificial General Intelligence (AGI). The architecture deconstructs cognition into indivisible Logical Atoms and two fundamental operations: Pointing and Comparison. Decision-making is formalized through an interpretable Weight-Calculation model (Weight = Benefit * Probability), where all values are traceable to an auditable set of Initial Weights. This atomic decomposition enables radical explainability, intrinsic generality for novel situations, and traceable value alignment. We detail its implementation via a graph-algorithm-based computational engine and a global workspace workflow, supported by a preliminary code implementation and scenario validation. Results indicate that the architecture achieves transparent, human-like reasoning and robust learning in unprecedented scenarios, establishing a practical and theoretical foundation for building trustworthy and aligned AGI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03072v1</guid>
      <category>cs.AI</category>
      <category>cs.LO</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hu Keyi</dc:creator>
    </item>
    <item>
      <title>Modal Logical Neural Networks</title>
      <link>https://arxiv.org/abs/2512.03491</link>
      <description>arXiv:2512.03491v1 Announce Type: cross 
Abstract: We propose Modal Logical Neural Networks (MLNNs), a neurosymbolic framework that integrates deep learning with the formal semantics of modal logic, enabling reasoning about necessity and possibility. Drawing on Kripke semantics, we introduce specialized neurons for the modal operators $\Box$ and $\Diamond$ that operate over a set of possible worlds, enabling the framework to act as a differentiable ``logical guardrail.'' The architecture is highly flexible: the accessibility relation between worlds can either be fixed by the user to enforce known rules or, as an inductive feature, be parameterized by a neural network. This allows the model to optionally learn the relational structure of a logical system from data while simultaneously performing deductive reasoning within that structure.
  This versatile construction is designed for flexibility. The entire framework is differentiable from end to end, with learning driven by minimizing a logical contradiction loss. This not only makes the system resilient to inconsistent knowledge but also enables it to learn nonlinear relationships that can help define the logic of a problem space. We illustrate MLNNs on four case studies: grammatical guardrailing, axiomatic detection of the unknown, multi-agent epistemic trust, and detecting constructive deception in natural language negotiation. These experiments demonstrate how enforcing or learning accessibility can increase logical consistency and interpretability without changing the underlying task architecture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03491v1</guid>
      <category>cs.LG</category>
      <category>cs.LO</category>
      <category>cs.MA</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antonin Sulc</dc:creator>
    </item>
    <item>
      <title>Tunable Automation in Automated Program Verification</title>
      <link>https://arxiv.org/abs/2512.03926</link>
      <description>arXiv:2512.03926v1 Announce Type: cross 
Abstract: Automated verification tools based on SMT solvers have made significant progress in verifying complex software systems. However, these tools face a fundamental tension between automation and performance when dealing with quantifier instantiation -- the primary source of incompleteness and verification slowdown in SMT-based verifiers. Tools choose between aggressive quantifier instantiation that provides more automation but longer verification times, or conservative instantiation that responds quickly but may require more manual proof hints.
  We present a mechanism that enables fine-grained control over the availability of quantified facts in verification contexts, allowing developers to selectively tune the level of automation. Our approach lets library authors provide different pre-defined automation levels while giving end-users the ability to further customize quantifier availability at the module, function, or proof context level.
  We implement our techniques in Verus, a Rust-based verification tool, and evaluate them on multiple openly available codebases. Our empirical analysis demonstrates the automation-performance tradeoff and that selective quantifier management enables developers to select the appropriate level of automation in different contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03926v1</guid>
      <category>cs.SE</category>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Alexander Y. Bai, Chris Hawblitzel, Andrea Lattuada</dc:creator>
    </item>
    <item>
      <title>Nested Sequents for Intuitionistic Grammar Logics via Structural Refinement</title>
      <link>https://arxiv.org/abs/2210.17139</link>
      <description>arXiv:2210.17139v2 Announce Type: replace 
Abstract: Intuitionistic grammar logics fuse constructive and multi-modal reasoning while permitting the use of converse modalities, serving as a generalization of standard intuitionistic modal logics. In this paper, we provide definitions of these logics as well as establish a suitable proof theory thereof. In particular, we show how to apply the structural refinement methodology to extract cut-free nested sequent calculi for intuitionistic grammar logics from their semantics. This method proceeds by first transforming the semantics of these logics into sound and complete labeled sequent systems, which we prove have favorable proof-theoretic properties such as syntactic cut-elimination. We then transform these labeled systems into nested sequent systems via the introduction of propagation rules and the elimination of structural rules. Our derived proof systems are then put to use, whereby we prove the conservativity of intuitionistic grammar logics over their modal counterparts, establish the general undecidability of these logics, and recognize a decidable subclass, referred to as "simple" intuitionistic grammar logics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.17139v2</guid>
      <category>cs.LO</category>
      <category>cs.DM</category>
      <category>math.LO</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tim S. Lyon</dc:creator>
    </item>
    <item>
      <title>From Interpolating Formulas to Separating Languages and Back Again</title>
      <link>https://arxiv.org/abs/2508.12805</link>
      <description>arXiv:2508.12805v2 Announce Type: replace 
Abstract: Traditionally, research on Craig interpolation is concerned with (a) establishing the Craig interpolation property (CIP) of a logic saying that every valid implication in the logic has a Craig interpolant and (b) designing algorithms that extract Craig interpolants from proofs. Logics that lack the CIP are regarded as `pathological' and excluded from consideration. In this chapter, we survey variations and generalisations of traditional Craig interpolation. First, we consider Craig interpolants for implications in logics without the CIP, focusing on the decidability and complexity of deciding their existence. We then generalise interpolation by looking for Craig interpolants in languages L' that can be weaker than the language L of the given implication. Thus, do not only we restrict the non-logical symbols of Craig interpolants but also the logical ones. The resulting L/L'-interpolation problem generalises L/L'-definability, the question whether an L-formula is equivalent to some L'-formula. After that, we move from logical languages to formal languages where interpolation disguises itself as separation: given two disjoint languages in a class C, does there exist a separating language in a smaller class C'? This question is particularly well-studied in the case when the input languages are regular and the separating language is first-order definable. Finally, we connect the different research strands by showing how the decidability of the separation problem for regular languages can be used to prove the decidability of Craig interpolant existence for linear temporal logic LTL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12805v2</guid>
      <category>cs.LO</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Agi Kurucz, Frank Wolter, Michael Zakharyaschev</dc:creator>
    </item>
  </channel>
</rss>
