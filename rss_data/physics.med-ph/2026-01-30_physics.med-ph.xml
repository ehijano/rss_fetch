<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.med-ph updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.med-ph</link>
    <description>physics.med-ph updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.med-ph" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 30 Jan 2026 05:01:57 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>AIRPET: Virtual Positron Emission Tomography</title>
      <link>https://arxiv.org/abs/2601.22059</link>
      <description>arXiv:2601.22059v1 Announce Type: cross 
Abstract: Positron Emission Tomography (PET) is a powerful medical imaging technique, but the design and evaluation of new PET scanner technologies present significant challenges. The process is typically divided into three major stages: 1. detector design and simulation, 2. image reconstruction, and 3. image interpretation. Each of these stages requires significant expertise, making it difficult for individuals or small teams to manage all three at once. AIRPET (AI-driven Revolution in Positron Emission Tomography) is a web-based platform designed to address this challenge by integrating all phases of PET design into a single, accessible, and AI-assisted workflow. AIRPET provides an interface to large language models (LLMs) for assisted geometry creation and an interface for basic PET image reconstruction with the potential for further expansion. Here we introduce AIRPET and outline its current functionality and proposed additions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22059v1</guid>
      <category>physics.ins-det</category>
      <category>physics.med-ph</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J. Renner, J. J. G\'omez-Cadenas, R. Soleti</dc:creator>
    </item>
    <item>
      <title>DeLTA-BIT: an open-source probabilistic tractography-based deep learning framework for thalamic targeting in functional neurological disorders</title>
      <link>https://arxiv.org/abs/2312.15462</link>
      <description>arXiv:2312.15462v2 Announce Type: replace 
Abstract: In the last years in-vivo tractography has assumed an important role in neurosciences, for both research and clinical applications such as non-invasive investigation of brain connectivity and presurgical planning in neurosurgery. In more recent years there has been a growing interest in the applications of diffusion tractography for target identification in functional neurological disorders for an increasingly tailored approach. The growing diffusion of well-established neurosurgical procedures, such as deep brain stimulation or trans-cranial Magnetic Resonance-guided Focused Ultrasound, favored this trend. Tractography can indeed provide more accurate, patient-specific, information about the targeted region if compared to stereotactic atlases. On the other hand, this tractography-based approach is not very physician-friendly, and its heavily time consuming since it needs several hours for Magnetic Resonance Imaging data processing. In this study we propose a novel open-source deep learning framework called DeLTA-BIT (acronym of Deep-learning Local TrActography for BraIn Targeting) for fast target predictions, based on probabilistic tractography. The proposed framework exploits a convolutional neural network (CNN) to predict the location of the Ventral Intermediate Nucleus of the thalamus (VIM). The CNN was trained on the Human Connectome Project (HCP) dataset. The model capability in predicting the VIM location was tested both on the HCP (internal validation) and clinical data (external validation). Results from the internal validation have shown good capability in predicting the VIM region (mean DSC = 0.62+- 0.15, mean sDSC=0.76+- 0.17) by using just T1 images as input, in a time scale of fraction of second per subject. As for the clinical data, results have been compared with an atlas-based method demonstrating similar performance, but within a significantly shorter timeframe.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.15462v2</guid>
      <category>physics.med-ph</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mattia Romeo, Cesare Gagliardo, Grazia Cottone, Giorgio Collura, Enrico Maggio, Claudio Runfola, Eleonora Bruno, Maria Cristina D'Oca, Massimo Midiri, Francesca Lizzi, Ian Postuma, Marco D'Amelio, Alessandro Lascialfari, Alessandra Retico, Maurizio Marrale</dc:creator>
    </item>
  </channel>
</rss>
