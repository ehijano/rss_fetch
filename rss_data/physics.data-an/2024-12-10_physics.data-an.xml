<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 11 Dec 2024 02:50:54 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Learning Symmetry-Independent Jet Representations via Jet-Based Joint Embedding Predictive Architecture</title>
      <link>https://arxiv.org/abs/2412.05333</link>
      <description>arXiv:2412.05333v1 Announce Type: cross 
Abstract: In high energy physics, self-supervised learning (SSL) methods have the potential to aid in the creation of machine learning models without the need for labeled datasets for a variety of tasks, including those related to jets -- narrow sprays of particles produced by quarks and gluons in high energy particle collisions. This study introduces an approach to learning jet representations without hand-crafted augmentations using a jet-based joint embedding predictive architecture (J-JEPA), which aims to predict various physical targets from an informative context. As our method does not require hand-crafted augmentation like other common SSL techniques, J-JEPA avoids introducing biases that could harm downstream tasks. Since different tasks generally require invariance under different augmentations, this training without hand-crafted augmentation enables versatile applications, offering a pathway toward a cross-task foundation model. We finetune the representations learned by J-JEPA for jet tagging and benchmark them against task-specific representations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05333v1</guid>
      <category>hep-ph</category>
      <category>cs.LG</category>
      <category>hep-ex</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.5281/zenodo.14251372</arxiv:DOI>
      <dc:creator>Subash Katel, Haoyang Li, Zihan Zhao, Raghav Kansal, Farouk Mokhtar, Javier Duarte</dc:creator>
    </item>
    <item>
      <title>Run 3 performance and advances in heavy-flavor jet tagging in CMS</title>
      <link>https://arxiv.org/abs/2412.05863</link>
      <description>arXiv:2412.05863v1 Announce Type: cross 
Abstract: Identification of hadronic jets originating from heavy-flavor quarks is extremely important to several physics analyses in High Energy Physics, such as studies of the properties of the top quark and the Higgs boson, and searches for new physics. Recent algorithms used in the CMS experiment were developed using state-of-the-art machine-learning techniques to distinguish jets emerging from the decay of heavy flavour (charm and bottom) quarks from those arising from light-flavor (udsg) ones. Increasingly complex deep neural network architectures, such as graphs and transformers, have helped achieve unprecedented accuracies in jet tagging. New advances in tagging algorithms, along with new calibration methods using flavour-enriched selections of proton-proton collision events, allow us to estimate flavour tagging performances with the CMS detector during early Run 3 of the LHC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05863v1</guid>
      <category>hep-ex</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Uttiya Sarkar</dc:creator>
    </item>
    <item>
      <title>Multifidelity Uncertainty Quantification for Ice Sheet Simulations</title>
      <link>https://arxiv.org/abs/2412.06110</link>
      <description>arXiv:2412.06110v1 Announce Type: cross 
Abstract: Ice sheet simulations suffer from vast parametric uncertainties, such as the basal sliding boundary condition or geothermal heat flux. Quantifying the resulting uncertainties in predictions is of utmost importance to support judicious decision-making, but high-fidelity simulations are too expensive to embed within uncertainty quantification (UQ) computations. UQ methods typically employ Monte Carlo simulation to estimate statistics of interest, which requires hundreds (or more) of ice sheet simulations. Cheaper low-fidelity models are readily available (e.g., approximated physics, coarser meshes), but replacing the high-fidelity model with a lower fidelity surrogate introduces bias, which means that UQ results generated with a low-fidelity model cannot be rigorously trusted. Multifidelity UQ retains the high-fidelity model but expands the estimator to shift computations to low-fidelity models, while still guaranteeing an unbiased estimate. Through this exploitation of multiple models, multifidelity estimators guarantee a target accuracy at reduced computational cost. This paper presents a comprehensive multifidelity UQ framework for ice sheet simulations. We present three multifidelity UQ approaches -- Multifidelity Monte Carlo, Multilevel Monte Carlo, and the Best Linear Unbiased Estimator -- that enable tractable UQ for continental-scale ice sheet simulations. We demonstrate the techniques on a model of the Greenland ice sheet to estimate the 2015-2050 ice mass loss, verify their estimates through comparison with Monte Carlo simulations, and give a comparative performance analysis. For a target accuracy equivalent to 1 mm sea level rise contribution at 95% confidence, the multifidelity estimators achieve computational speedups of two orders of magnitude.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06110v1</guid>
      <category>physics.geo-ph</category>
      <category>physics.ao-ph</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicole Aretz, Max Gunzburger, Mathieu Morlighem, Karen Willcox</dc:creator>
    </item>
    <item>
      <title>Data Unfolding with Mean Integrated Square Error Optimization</title>
      <link>https://arxiv.org/abs/2402.12990</link>
      <description>arXiv:2402.12990v3 Announce Type: replace 
Abstract: Experimental data in Particle and Nuclear physics, Particle Astrophysics and Radiation Protection Dosimetry are obtained from experimental facilities comprising a complex array of sensors, electronics and software. Computer simulation is used to study the measurement process. Probability Density Functions (PDFs) of measured physical parameters deviate from true PDFs due to resolution, bias, and efficiency effects. Good estimates of the true PDF are necessary for testing theoretical models, comparing results from different experiments, and combining results from various research endeavors. In the article, the histogram method is employed to estimate both the measured and true PDFs. The binning of histograms is determined using the K-means clustering algorithm. The true PDF is estimated through the maximization of the likelihood function with entropy regularization, utilizing a non-linear optimization algorithm specially designed for this purpose. The accuracy of the results is assessed using the Mean Integrated Square Error. To determine the optimal value for the regularization parameter, a bootstrap method is applied. Additionally, a mathematical model of the measurement system is formulated using system identification methods. This approach enhances the robustness and precision of the estimation process, providing a more reliable analysis of the system's characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.12990v3</guid>
      <category>physics.data-an</category>
      <category>hep-ex</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nikolay D. Gagunashvili</dc:creator>
    </item>
    <item>
      <title>Burst-tree structure and higher-order temporal correlations</title>
      <link>https://arxiv.org/abs/2409.01674</link>
      <description>arXiv:2409.01674v3 Announce Type: replace 
Abstract: Understanding characteristics of temporal correlations in time series is crucial for developing accurate models in natural and social sciences. The burst-tree decomposition method was recently introduced to reveal higher-order temporal correlations in time series in a form of an event sequence, in particular, the hierarchical structure of bursty trains of events for the entire range of timescales [Jo et al., Sci.~Rep.~\textbf{10}, 12202 (2020)]. Such structure has been found to be simply characterized by the burst-merging kernel governing which bursts are merged together as the timescale for defining bursts increases. In this work, we study the effects of kernels on the higher-order temporal correlations in terms of burst size distributions, memory coefficients for bursts, and the autocorrelation function. We employ several kernels, including the constant, sum, product, and diagonal kernels as well as those inspired by empirical results. We generically find that kernels with preferential merging lead to heavy-tailed burst size distributions, while kernels with assortative merging lead to positive correlations between burst sizes. The decaying exponent of the autocorrelation function depends not only on the kernel but also on the power-law exponent of the interevent time distribution. In addition, thanks to the analogy to the coagulation process, analytical solutions of burst size distributions for some kernels could be obtained. Our findings may shed light on the role of burst-merging kernels as underlying mechanisms of higher-order temporal correlations in time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01674v3</guid>
      <category>physics.data-an</category>
      <category>physics.comp-ph</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tibebe Birhanu, Hang-Hyun Jo</dc:creator>
    </item>
    <item>
      <title>GD-VAEs: Geometric Dynamic Variational Autoencoders for Learning Nonlinear Dynamics and Dimension Reductions</title>
      <link>https://arxiv.org/abs/2206.05183</link>
      <description>arXiv:2206.05183v3 Announce Type: replace-cross 
Abstract: We develop data-driven methods incorporating geometric and topological information to learn parsimonious representations of nonlinear dynamics from observations. The approaches learn nonlinear state-space models of the dynamics for general manifold latent spaces using training strategies related to Variational Autoencoders (VAEs). Our methods are referred to as Geometric Dynamic (GD) Variational Autoencoders (GD-VAEs). We learn encoders and decoders for the system states and evolution based on deep neural network architectures that include general Multilayer Perceptrons (MLPs), Convolutional Neural Networks (CNNs), and other architectures. Motivated by problems arising in parameterized PDEs and physics, we investigate the performance of our methods on tasks for learning reduced dimensional representations of the nonlinear Burgers Equations, Constrained Mechanical Systems, and spatial fields of Reaction-Diffusion Systems. GD-VAEs provide methods that can be used to obtain representations in manifold latent spaces for diverse learning tasks involving dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.05183v3</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.DS</category>
      <category>math.NA</category>
      <category>physics.data-an</category>
      <category>stat.ML</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryan Lopez, Paul J. Atzberger</dc:creator>
    </item>
    <item>
      <title>Learning the dynamics of Markovian open quantum systems from experimental data</title>
      <link>https://arxiv.org/abs/2410.17942</link>
      <description>arXiv:2410.17942v2 Announce Type: replace-cross 
Abstract: We present a Bayesian algorithm to identify generators of open quantum system dynamics, described by a Lindblad master equation, that are compatible with measured experimental data. The algorithm, based on a Markov Chain Monte Carlo approach, assumes the energy levels of the system are known and outputs a ranked list of interpretable master equation models that produce predicted measurement traces that closely match experimental data. We benchmark our algorithm on quantum optics experiments performed on single and pairs of quantum emitters. The latter case opens the possibility of cooperative emission effects and additional complexity due to the possible interplay between photon and phonon influences on the dynamics. Our algorithm retrieves various minimal models that are consistent with the experimental data, and which can provide a closer fit to measured data than previously suggested and physically expected approximate models. Our results represent an important step towards automated systems characterisation with an approach that is capable of working with diverse and tomographically incomplete input data. This may help with the development of theoretical models for unknown quantum systems as well as providing scientists with alternative interpretations of the data that they might not have originally envisioned and enabling them to challenge their original hypotheses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17942v2</guid>
      <category>quant-ph</category>
      <category>cond-mat.mes-hall</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <category>physics.optics</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stewart Wallace, Yoann Altmann, Brian D. Gerardot, Erik M. Gauger, Cristian Bonato</dc:creator>
    </item>
    <item>
      <title>Interpreting Transformers for Jet Tagging</title>
      <link>https://arxiv.org/abs/2412.03673</link>
      <description>arXiv:2412.03673v2 Announce Type: replace-cross 
Abstract: Machine learning (ML) algorithms, particularly attention-based transformer models, have become indispensable for analyzing the vast data generated by particle physics experiments like ATLAS and CMS at the CERN LHC. Particle Transformer (ParT), a state-of-the-art model, leverages particle-level attention to improve jet-tagging tasks, which are critical for identifying particles resulting from proton collisions. This study focuses on interpreting ParT by analyzing attention heat maps and particle-pair correlations on the $\eta$-$\phi$ plane, revealing a binary attention pattern where each particle attends to at most one other particle. At the same time, we observe that ParT shows varying focus on important particles and subjets depending on decay, indicating that the model learns traditional jet substructure observables. These insights enhance our understanding of the model's internal workings and learning process, offering potential avenues for improving the efficiency of transformer architectures in future high-energy physics applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03673v2</guid>
      <category>hep-ph</category>
      <category>cs.LG</category>
      <category>hep-ex</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aaron Wang, Abhijith Gandrakota, Jennifer Ngadiuba, Vivekanand Sahu, Priyansh Bhatnagar, Elham E Khoda, Javier Duarte</dc:creator>
    </item>
  </channel>
</rss>
