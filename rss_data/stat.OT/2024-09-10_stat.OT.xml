<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.OT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.OT</link>
    <description>stat.OT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.OT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 10 Sep 2024 04:02:38 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>How to survive the Squid Games using probability theory</title>
      <link>https://arxiv.org/abs/2409.05263</link>
      <description>arXiv:2409.05263v1 Announce Type: new 
Abstract: In this paper, we consider how probability theory can be used to determine the survival strategy in two of the ``Squid Game" and ``Squid Game: The Challenge" challenges: the Hopscotch and the Warships. We show how Hopscotch can be easily tackled with the knowledge of the binomial distribution, taught in introductory statistics courses, while Warships is a much more complex problem, which can be tackled at different levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05263v1</guid>
      <category>stat.OT</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elena Moltchanova, Miguel Moyers-Gonz\'alez, Geertrui Van de Voorde, Jos\'e Felipe Voloch, Philipp Wacker</dc:creator>
    </item>
    <item>
      <title>Censored Data Forecasting: Applying Tobit Exponential Smoothing with Time Aggregation</title>
      <link>https://arxiv.org/abs/2409.05412</link>
      <description>arXiv:2409.05412v1 Announce Type: cross 
Abstract: This study introduces a novel approach to forecasting by Tobit Exponential Smoothing with time aggregation constraints. This model, a particular case of the Tobit Innovations State Space system, handles censored observed time series effectively, such as sales data, with known and potentially variable censoring levels over time. The paper provides a comprehensive analysis of the model structure, including its representation in system equations and the optimal recursive estimation of states. It also explores the benefits of time aggregation in state space systems, particularly for inventory management and demand forecasting. Through a series of case studies, the paper demonstrates the effectiveness of the model across various scenarios, including hourly and daily censoring levels. The results highlight the model's ability to produce accurate forecasts and confidence bands comparable to those from uncensored models, even under severe censoring conditions. The study further discusses the implications for inventory policy, emphasizing the importance of avoiding spiral-down effects in demand estimation. The paper concludes by showcasing the superiority of the proposed model over standard methods, particularly in reducing lost sales and excess stock, thereby optimizing inventory costs. This research contributes to the field of forecasting by offering a robust model that effectively addresses the challenges of censored data and time aggregation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05412v1</guid>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Diego J. Pedregal, Juan R. Trapero</dc:creator>
    </item>
    <item>
      <title>Jackknife Empirical Likelihood Ratio Test for Cauchy Distribution</title>
      <link>https://arxiv.org/abs/2409.05764</link>
      <description>arXiv:2409.05764v1 Announce Type: cross 
Abstract: Heavy-tailed distributions, such as the Cauchy distribution, are acknowledged for providing more accurate models for financial returns, as the normal distribution is deemed insufficient for capturing the significant fluctuations observed in real-world assets. Data sets characterized by outlier sensitivity are critically important in diverse areas, including finance, economics, telecommunications, and signal processing. This article addresses a goodness-of-fit test for the Cauchy distribution. The proposed test utilizes empirical likelihood methods, including the jackknife empirical likelihood (JEL) and adjusted jackknife empirical likelihood (AJEL). Extensive Monte Carlo simulation studies are conducted to evaluate the finite sample performance of the proposed test. The application of the proposed test is illustrated through the analysing two real data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05764v1</guid>
      <category>math.ST</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Avhad Ganesh Vishnu, Ananya Lahiri, Sudheesh K. Kattumannil</dc:creator>
    </item>
    <item>
      <title>Robust estimations from distribution structures: I. Mean</title>
      <link>https://arxiv.org/abs/2403.12110</link>
      <description>arXiv:2403.12110v3 Announce Type: replace-cross 
Abstract: As the most fundamental problem in statistics, robust location estimation has many prominent solutions, such as the trimmed mean, Winsorized mean, Hodges Lehmann estimator, Huber M estimator, and median of means. Recent studies suggest that their maximum biases concerning the mean can be quite different, but the underlying mechanisms largely remain unclear. This study exploited a semiparametric method to classify distributions by the asymptotic orderliness of quantile combinations with varying breakdown points, showing their interrelations and connections to parametric distributions. Further deductions explain why the Winsorized mean typically has smaller biases compared to the trimmed mean; two sequences of semiparametric robust mean estimators emerge, particularly highlighting the superiority of the median Hodges Lehmann mean. This article sheds light on the understanding of the common nature of probability distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12110v3</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tuobang Li</dc:creator>
    </item>
  </channel>
</rss>
