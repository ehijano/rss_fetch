<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 05 Jun 2024 01:55:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 04 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Parallel Redundancy Removal in lrslib with Application to Projections</title>
      <link>https://arxiv.org/abs/2406.00065</link>
      <description>arXiv:2406.00065v1 Announce Type: new 
Abstract: We describe a parallel implementation in lrslib for removing redundant halfspaces and finding a minimum representation for an H-representation of a convex polyhedron. By a standard transformation, the same code works for V-representations. We use this approach to speed up the redundancy removal step in Fourier-Motzkin elimination. Computational results are given including a comparison with Clarkson's algorithm, which is particularly fast on highly redundant inputs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00065v1</guid>
      <category>math.OC</category>
      <category>cs.CG</category>
      <category>cs.MS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Avis, Charles Jordan</dc:creator>
    </item>
    <item>
      <title>Nonlinear synthesis of compliant mechanisms with selective compliance</title>
      <link>https://arxiv.org/abs/2406.00067</link>
      <description>arXiv:2406.00067v1 Announce Type: new 
Abstract: The synthesis of compliant mechanisms (CMs) is frequently achieved through topology optimization. Many synthesis approaches simplify implementation by assuming small distortions, but this limits their practical application since CMs typically undergo large deformations that include geometric and material nonlinearities. CMs designed to generate a desired deformation path at the output points under specific loads are known as path-generating CMs. However, these CMs face significant challenges in topology optimization, resulting in the development of only a few optimization methods. Existing approaches often include only certain load cases in the optimization process. Consequently, if a CM designed this way encounters different load cases in practice, its path-generating behavior cannot be guaranteed.
  The authors have previously contributed to the development of an approach suitable for synthesizing load case insensitive CMs. This paper extends that approach to account for nonlinearities, enabling the synthesis of path-generating CMs. The effectiveness of this extended approach is demonstrated through appropriate design examples. Additionally, the paper presents, for the first time, a shape-adaptive path-generating CM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00067v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Stephanie Seltmann, Alexander Hasse</dc:creator>
    </item>
    <item>
      <title>Universe-inspired algorithms for Control Engineering: A review</title>
      <link>https://arxiv.org/abs/2406.00084</link>
      <description>arXiv:2406.00084v1 Announce Type: new 
Abstract: Control algorithms have been proposed based on knowledge related to nature-inspired mechanisms, including those based on the behavior of living beings. This paper presents a review focused on major breakthroughs carried out in the scope of applied control inspired by the gravitational attraction between bodies. A control approach focused on Artificial Potential Fields was identified, as well as four optimization metaheuristics: Gravitational Search Algorithm, Black-Hole algorithm, Multi-Verse Optimizer, and Galactic Swarm Optimization. A thorough analysis of ninety-one relevant papers was carried out to highlight their performance and to identify the gravitational and attraction foundations, as well as the universe laws supporting them. Included are their standard formulations, as well as their improved, modified, hybrid, cascade, fuzzy, chaotic and adaptive versions. Moreover, this review also deeply delves into the impact of universe-inspired algorithms on control problems of dynamic systems, providing an extensive list of control-related applications, and their inherent advantages and limitations. Strong evidence suggests that gravitation-inspired and black-hole dynamic-driven algorithms can outperform other well-known algorithms in control engineering, even though they have not been designed according to realistic astrophysical phenomena and formulated according to astrophysics laws. Even so, they support future research directions towards the development of high-sophisticated control laws inspired by Newtonian/Einsteinian physics, such that effective control-astrophysics bridges can be established and applied in a wide range of applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00084v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.heliyon.2024.e31771</arxiv:DOI>
      <arxiv:journal_reference>Heliyon 10 (2024), no. 11, e31771, 1--24</arxiv:journal_reference>
      <dc:creator>Rodrigo M. C. Bernardo, Delfim F. M. Torres, Carlos A. R. Herdeiro, Marco P. Soares dos Santos</dc:creator>
    </item>
    <item>
      <title>solar: A solar thermal power plant simulator for blackbox optimization benchmarking</title>
      <link>https://arxiv.org/abs/2406.00140</link>
      <description>arXiv:2406.00140v1 Announce Type: new 
Abstract: This work introduces solar, a collection of ten optimization problem instances for benchmarking blackbox optimization solvers. The instances present different design aspects of a concentrated solar power plant simulated by blackbox numerical models. The type of variables (discrete or continuous), dimensionality, and number and types of constraints (including hidden constraints) differ across instances. Some are deterministic, others are stochastic with possibilities to execute several replications to control stochasticity. Most instances offer variable fidelity surrogates, two are biobjective and one is unconstrained. The solar plant model takes into account various subsystems: a heliostats field, a central cavity receiver (the receiver), a molten salt thermal energy storage, a steam generator and an idealized power block. Several numerical methods are implemented throughout the solar code and most of the executions are time-consuming. Great care was applied to guarantee reproducibility across platforms. The solar tool encompasses most of the characteristics that can be found in industrial and real-life blackbox optimization problems, all in an open-source and stand-alone code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00140v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolau Andr\'es-Thi\'o, Charles Audet, Miguel Diago, Aimen E. Gheribi, S\'ebastien Le Digabel, Xavier Lebeuf, Mathieu Lemyre Garneau, Christophe Tribes</dc:creator>
    </item>
    <item>
      <title>Hamiltonian formalism for optimal control of nonlinear loaded integro-PDEs</title>
      <link>https://arxiv.org/abs/2406.00248</link>
      <description>arXiv:2406.00248v1 Announce Type: new 
Abstract: We formulate nonlinear nonlocal integro-PDE with memory, biloaded (boundary integrals load the ambient space, and the ambient space loads the boundary), and the associated optimal control problems. We derive part of the necessary conditions for optimality in the form of Hamilton-Euler-Lagrange loaded integro-PDEs. In the process, we introduce an agglomeration of new differential operators. Our results have relevance to optimal amelioration of flooded areas, remediation of sites of contaminated groundwater, and active control methods for optimally extinguishing forest fires.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00248v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>S. A. Belbas</dc:creator>
    </item>
    <item>
      <title>Learning Preconditioners for Inverse Problems</title>
      <link>https://arxiv.org/abs/2406.00260</link>
      <description>arXiv:2406.00260v1 Announce Type: new 
Abstract: We explore the application of preconditioning in optimisation algorithms, specifically those appearing in Inverse Problems in imaging. Such problems often contain an ill-posed forward operator and are large-scale. Therefore, computationally efficient algorithms which converge quickly are desirable. To remedy these issues, learning-to-optimise leverages training data to accelerate solving particular optimisation problems. Many traditional optimisation methods use scalar hyperparameters, significantly limiting their convergence speed when applied to ill-conditioned problems. In contrast, we propose a novel approach that replaces these scalar quantities with matrices learned using data. Often, preconditioning considers only symmetric positive-definite preconditioners. However, we consider multiple parametrisations of the preconditioner, which do not require symmetry or positive-definiteness. These parametrisations include using full matrices, diagonal matrices, and convolutions. We analyse the convergence properties of these methods and compare their performance against classical optimisation algorithms. Generalisation performance of these methods is also considered, both for in-distribution and out-of-distribution data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00260v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthias J. Ehrhardt, Patrick Fahy, Mohammad Golbabaee</dc:creator>
    </item>
    <item>
      <title>A Single-Loop Robust Policy Gradient Method for Robust Markov Decision Processes</title>
      <link>https://arxiv.org/abs/2406.00274</link>
      <description>arXiv:2406.00274v1 Announce Type: new 
Abstract: Robust Markov Decision Processes (RMDPs) have recently been recognized as a valuable and promising approach to discovering a policy with creditable performance, particularly in the presence of a dynamic environment and estimation errors in the transition matrix due to limited data. Despite extensive exploration of dynamic programming algorithms for solving RMDPs, there has been a notable upswing in interest in developing efficient algorithms using the policy gradient method. In this paper, we propose the first single-loop robust policy gradient (SRPG) method with the global optimality guarantee for solving RMDPs through its minimax formulation. Moreover, we complement the convergence analysis of the nonconvex-nonconcave min-max optimization problem with the objective function's gradient dominance property, which is not explored in the prior literature. Numerical experiments validate the efficacy of SRPG, demonstrating its faster and more robust convergence behavior compared to its nested-loop counterpart.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00274v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhenwei Lin, Chenyu Xue, Qi Deng, Yinyu Ye</dc:creator>
    </item>
    <item>
      <title>State Compensation Linearization and Control</title>
      <link>https://arxiv.org/abs/2406.00285</link>
      <description>arXiv:2406.00285v1 Announce Type: new 
Abstract: The linearization method builds a bridge from mature methods for linear systems to nonlinear systems and has been widely used in various areas. There are currently two main linearization methods: Jacobian linearization and feedback linearization. However, the Jacobian linearization method has approximate and local properties, and the feedback linearization method has a singularity problem and loses the physical meaning of the obtained states. Thus, as a kind of complementation, a new linearization method named state compensation linearization is proposed in the paper. Their differences, advantages, and disadvantages are discussed in detail. Based on the state compensation linearization, a state-compensation-linearization-based control framework is proposed for a class of nonlinear systems. Under the new framework, the original problem can be simplified. The framework also allows different control methods, especially those only applicable to linear systems, to be incorporated. Three illustrative examples are also given to show the process and effectiveness of the proposed linearization method and control framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00285v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Quan Quan, Jinrui Ren</dc:creator>
    </item>
    <item>
      <title>Computation of Maximal Admissible Robust Positive Invariant Sets for Linear Systems with Parametric and Additive Uncertainties</title>
      <link>https://arxiv.org/abs/2406.00355</link>
      <description>arXiv:2406.00355v1 Announce Type: new 
Abstract: In this paper, we address the problem of computing the maximal admissible robust positive invariant (MARPI) set for discrete-time linear time-varying systems with parametric uncertainties and additive disturbances. The system state and input are subjected to hard constraints, and the system parameters and the exogenous disturbance are assumed to belong to known convex polytopes. We provide necessary and sufficient conditions for the existence of the non-empty MARPI set, and explore relevant features of the set that lead to an efficient finite-time converging algorithm with a suitable stopping criterion. The analysis hinges on backward reachable sets defined using recursively computed halfspaces and the minimal RPI set. A numerical example is used to validate the theoretical development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00355v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anchita Dey, Shubhendu Bhasin</dc:creator>
    </item>
    <item>
      <title>Large Deviations Analysis For Regret Minimizing Stochastic Approximation Algorithms</title>
      <link>https://arxiv.org/abs/2406.00414</link>
      <description>arXiv:2406.00414v1 Announce Type: new 
Abstract: Motivated by learning of correlated equilibria in non-cooperative games, we perform a large deviations analysis of a regret minimizing stochastic approximation algorithm. The regret minimization algorithm we consider comprises multiple agents that communicate over a graph to coordinate their decisions. We derive an exponential decay rate towards the algorithm's stable point using large deviations theory. Our analysis leverages the variational representation of the Laplace functionals and weak convergence methods to characterize the exponential decay rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00414v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongjiang Qian, Vikram Krishnamurthy</dc:creator>
    </item>
    <item>
      <title>Modelling Non-monotone Risk Aversion and Convex Compensation in Incomplete Markets</title>
      <link>https://arxiv.org/abs/2406.00435</link>
      <description>arXiv:2406.00435v1 Announce Type: new 
Abstract: In hedge funds, convex compensation schemes are popular to stimulate a high-profit performance for portfolio managers. In economics, non-monotone risk aversion is proposed to argue that individuals may not be risk-averse when the wealth level is low. Combining these two ingredients, we study the optimal control strategy of the manager in incomplete markets. Generally, we propose a wide class of utility functions, the Piecewise Symmetric Asymptotic Hyperbolic Absolute Risk Aversion (PSAHARA) utility, to model the two ingredients, containing both non-concavity and non-differentiability as some abnormalities. Significantly, we derive an explicit optimal control for the family of PSAHARA utilities. This control is expressed into a unified four-term structure, featuring the asymptotic Merton term and the risk adjustment term. Furthermore, we provide a detailed asymptotic analysis and numerical illustration of the optimal portfolio. We obtain the following key insights: (i) A manager with the PSAHARA utility becomes extremely risk-seeking when his/her wealth level tends to zero; (ii) The optimal investment ratio tends to the Merton constant as the wealth level approaches infinity and the negative Merton constant when the wealth falls to negative infinity, implying that such a manager takes a risk-seeking investment as the wealth falls negatively low; (iii) The convex compensation still induces a great risk-taking behavior in the case that the preference is modeled by SAHARA utility. Finally, we conduct a real-data analysis of the U.S. stock market under the above model and conclude that the PSAHARA portfolio is very risk-seeking and leads to a high return and a high volatility (two-peak Sharpe ratio).</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00435v1</guid>
      <category>math.OC</category>
      <category>q-fin.MF</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Liu, Zhenyu Shen</dc:creator>
    </item>
    <item>
      <title>A computationally efficient arc-search interior-point algorithm for nonlinear constrained optimization</title>
      <link>https://arxiv.org/abs/2406.00436</link>
      <description>arXiv:2406.00436v1 Announce Type: new 
Abstract: This paper proposes an arc-search interior-point algorithm for the nonlinear constrained optimization problem. The proposed algorithm uses the second-order derivatives to construct a search arc that approaches the optimizer. Because the arc stays in the interior set longer than any straight line, it is expected that the scheme will generate a better new iterate than a line search method. The computation of the second-order derivatives requires to solve the second linear system of equations, but the coefficient matrix of the second linear system of equations is the same as the first linear system of equations. Therefore, the matrix decomposition obtained while solving the first linear system of equations can be reused. In addition, most elements of the right-hand side vector of the second linear system of equations are already computed when the coefficient matrix is assembled. Therefore, the computation cost for solving the second linear system of equations is insignificant and the benefit of having a better search scheme is well justified. The convergence of the proposed algorithm is established. Some preliminary test results are reported to demonstrate the merit of the proposed algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00436v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yaguang Yang</dc:creator>
    </item>
    <item>
      <title>Non-geodesically-convex optimization in the Wasserstein space</title>
      <link>https://arxiv.org/abs/2406.00502</link>
      <description>arXiv:2406.00502v1 Announce Type: new 
Abstract: We study a class of optimization problems in the Wasserstein space (the space of probability measures) where the objective function is \emph{nonconvex} along generalized geodesics. When the regularization term is the negative entropy, the optimization problem becomes a sampling problem where it minimizes the Kullback-Leibler divergence between a probability measure (optimization variable) and a target probability measure whose logarithmic probability density is a nonconvex function. We derive multiple convergence insights for a novel {\em semi Forward-Backward Euler scheme} under several nonconvex (and possibly nonsmooth) regimes. Notably, the semi Forward-Backward Euler is just a slight modification of the Forward-Backward Euler whose convergence is -- to our knowledge -- still unknown in our very general non-geodesically-convex setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00502v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hoang Phuc Hau Luu, Hanlin Yu, Bernardo Williams, Petrus Mikkola, Marcelo Hartmann, Kai Puolam\"aki, Arto Klami</dc:creator>
    </item>
    <item>
      <title>Schr\"{o}dinger Bridge with Quadratic State Cost is Exactly Solvable</title>
      <link>https://arxiv.org/abs/2406.00503</link>
      <description>arXiv:2406.00503v1 Announce Type: new 
Abstract: Schr\"odinger bridge is a diffusion process that steers a given distribution to another in a prescribed time while minimizing the effort to do so. It can be seen as the stochastic dynamical version of the optimal mass transport, and has growing applications in generative diffusion models and stochastic optimal control. In this work, we propose a regularized variant of the Schr\"odinger bridge with a quadratic state cost-to-go that incentivizes the optimal sample paths to stay close to a nominal level. Unlike the conventional Schr\"odinger bridge, the regularization induces a state-dependent rate of killing and creation of probability mass, and its solution requires determining the Markov kernel of a reaction-diffusion partial differential equation. We derive this Markov kernel in closed form. Our solution recovers the heat kernel in the vanishing regularization (i.e., diffusion without reaction) limit, thereby recovering the solution of the conventional Schr\"odinger bridge. Our results enable the use of dynamic Sinkhorn recursion for computing the Schr\"odinger bridge with a quadratic state cost-to-go, which would otherwise be challenging to use in this setting. We deduce properties of the new kernel and explain its connections with certain exactly solvable models in quantum mechanics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00503v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexis M. H. Teter, Wenqing Wang, Abhishek Halder</dc:creator>
    </item>
    <item>
      <title>On the Convergence Rates of Set Membership Estimation of Linear Systems with Disturbances Bounded by General Convex Sets</title>
      <link>https://arxiv.org/abs/2406.00574</link>
      <description>arXiv:2406.00574v1 Announce Type: new 
Abstract: This paper studies the uncertainty set estimation of system parameters of linear dynamical systems with bounded disturbances, which is motivated by robust (adaptive) constrained control. Departing from the confidence bounds of least square estimation from the machine-learning literature, this paper focuses on a method commonly used in (robust constrained) control literature: set membership estimation (SME). SME tends to enjoy better empirical performance than LSE's confidence bounds when the system disturbances are bounded. However, the theoretical guarantees of SME are not fully addressed even for i.i.d. bounded disturbances. In the literature, SME's convergence has been proved for general convex supports of the disturbances, but SME's convergence rate assumes a special type of disturbance support: $l_\infty$ ball. The main contribution of this paper is relaxing the assumption on the disturbance support and establishing the convergence rates of SME for general convex supports, which closes the gap on the applicability of the convergence and convergence rates results. Numerical experiments on SME and LSE's confidence bounds are also provided for different disturbance supports.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00574v1</guid>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haonan Xu, Yingying Li</dc:creator>
    </item>
    <item>
      <title>A Universal Transfer Theorem for Convex Optimization Algorithms Using Inexact First-order Oracles</title>
      <link>https://arxiv.org/abs/2406.00576</link>
      <description>arXiv:2406.00576v1 Announce Type: new 
Abstract: Given any algorithm for convex optimization that uses exact first-order information (i.e., function values and subgradients), we show how to use such an algorithm to solve the problem with access to inexact first-order information. This is done in a ``black-box'' manner without knowledge of the internal workings of the algorithm. This complements previous work that considers the performance of specific algorithms like (accelerated) gradient descent with inexact information. In particular, our results apply to a wider range of algorithms beyond variants of gradient descent, e.g., projection-free methods, cutting-plane methods, or any other first-order methods formulated in the future. Further, they also apply to algorithms that handle structured nonconvexities like mixed-integer decision variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00576v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Phillip Kerger, Marco Molinaro, Hongyi Jiang, Amitabh Basu</dc:creator>
    </item>
    <item>
      <title>Policy Iteration for exploratory Hamilton--Jacobi--Bellman equations</title>
      <link>https://arxiv.org/abs/2406.00612</link>
      <description>arXiv:2406.00612v1 Announce Type: new 
Abstract: We study the policy iteration algorithm (PIA) for entropy-regularized stochastic control problems on an infinite time horizon with a large discount rate, focusing on two main scenarios. First, we analyze PIA with bounded coefficients where the controls applied to the diffusion term satisfy a smallness condition. We demonstrate the convergence of PIA based on a uniform $\mathcal{C}^{2,\alpha}$ estimate for the value sequence generated by PIA, and provide a quantitative convergence analysis for this scenario. Second, we investigate PIA with unbounded coefficients but no control over the diffusion term. In this scenario, we first provide the well-posedness of the exploratory Hamilton--Jacobi--Bellman equation with linear growth coefficients and polynomial growth reward function. By such a well-posedess result we achieve PIA's convergence by establishing a quantitative locally uniform $\mathcal{C}^{1,\alpha}$ estimates for the generated value sequence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00612v1</guid>
      <category>math.OC</category>
      <category>math.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hung Vinh Tran, Zhenhua Wang, Yuming Paul Zhang</dc:creator>
    </item>
    <item>
      <title>Level proximal subdifferential, variational convexity, and pointwise Lipschitz smoothness</title>
      <link>https://arxiv.org/abs/2406.00648</link>
      <description>arXiv:2406.00648v1 Announce Type: new 
Abstract: Level proximal subdifferential was introduced by Rockafellar recently as a tool for studying proximal mappings of possibly nonconvex functions. In this paper we give a systematic study of level proximal subdifferntial, characterize variational convexity of the function by locally firm nonexpansiveness of proximal mappings or locally relative monotonicity of level proximal subdifferential, and investigate pointwise Lipschitz smoothness of the function. Integration and single-valuedness of level proximal subdifferential are also examined.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00648v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Honglin Luo, Xianfu Wang, Ziyuan Wang, Xinmin Yang</dc:creator>
    </item>
    <item>
      <title>A third order dynamical system for generalized monotone equation</title>
      <link>https://arxiv.org/abs/2406.00705</link>
      <description>arXiv:2406.00705v1 Announce Type: new 
Abstract: We propose a third order dynamical system for solving a nonlinear equation in Hilbert spaces where the operator is cocoercive with respect to the solutions set. Under mild conditions on the parameters, we establish the existence and uniqueness of the generated trajectories as well as its asymptotic convergence to a solution of the equation. When the operator is strongly monotone with respect to the solutions set, we deliver an exponential convergence rate of $e^{-2t}$, which is significantly faster than the known results of second order dynamical systems. In particular, for convex optimization problems, the proposed dynamical system provides a fast convergence rate of {\bf $\mathcal{O}(\frac{1}{t^3})$} for the objective values. In addition, we discuss the applications of the proposed dynamical system to several splitting monotone inclusion problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00705v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pham Viet Hai, Phan Tu Vuong</dc:creator>
    </item>
    <item>
      <title>Singular Perturbation: When the Perturbation Parameter Becomes a State-Dependent Function</title>
      <link>https://arxiv.org/abs/2406.00753</link>
      <description>arXiv:2406.00753v1 Announce Type: new 
Abstract: This paper presents a new systematic framework for nonlinear singularly perturbed systems in which state-dependent perturbation functions are used instead of constant perturbation coefficients. Under this framework, general results are obtained for the global robust stability and input-to-state stability of nonlinear singularly perturbed systems. Interestingly, the proposed methodology provides innovative solutions beyond traditional singular perturbation theory for emerging control problems arising from nonlinear integral control, feedback optimization, and formation-based source seeking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00753v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tengfei Liu, Zhong-Ping Jiang</dc:creator>
    </item>
    <item>
      <title>Tikhonov regularization of monotone operator flows not only ensures strong convergence of the trajectories but also speeds up the vanishing of the residuals</title>
      <link>https://arxiv.org/abs/2406.00852</link>
      <description>arXiv:2406.00852v1 Announce Type: new 
Abstract: In the framework of real Hilbert spaces, we investigate first-order dynamical systems governed by monotone and continuous operators. It has been established that for these systems, only the ergodic trajectory converges to a zero of the operator. A notable example is the counterclockwise $\pi/2$-rotation operator on $\mathbb{R}^2$, which illustrates that general trajectory convergence cannot be expected. However, trajectory convergence is assured for operators with the stronger property of cocoercivity. For this class of operators, the trajectory's velocity and the opertor values along the trajectory converge in norm to zero at a rate of $o(\frac{1}{\sqrt{t}})$ as $t \rightarrow +\infty$.
  In this paper, we demonstrate that when the monotone operator flow is augmented with a Tikhonov regularization term, the resulting trajectory converges strongly to the element of the set of zeros with minimal norm. In addition, rates of convergence in norm for the trajectory's velocity and the operator along the trajectory can be derived in terms of the regularization function. In some particular cases, these rates of convergence can outperform the ones of the coercive operator flows and can be as fast as $O(\frac{1}{t})$ as $t \rightarrow +\infty$. In this way, we emphasize a surprising acceleration feature of the Tikhonov regularization. Additionally, we explore these properties for monotone operator flows that incorporate time rescaling and an anchor point. For a specific choice of the Tikhonov regularization function, these flows are closely linked to second-order dynamical systems with a vanishing damping term. The convergence and convergence rate results we achieve for these systems complement recent findings for the Fast Optimistic Gradient Descent Ascent (OGDA) dynamics, leading to surprising outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00852v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Radu Ioan Bot, Dang-Khoa Nguyen</dc:creator>
    </item>
    <item>
      <title>Exploiting cone approximations in an augmented Lagrangian method for conic optimization</title>
      <link>https://arxiv.org/abs/2406.00854</link>
      <description>arXiv:2406.00854v1 Announce Type: new 
Abstract: We propose an algorithm for general nonlinear conic programming which does not require the knowledge of the full cone, but rather a simpler, more tractable, approximation of it. We prove that the algorithm satisfies a strong global convergence property in the sense that it generates a strong sequential optimality condition. In particular, a KKT point is necessarily found when a limit point satisfies Robinson's condition. We conduct numerical experiments minimizing nonlinear functions subject to a copositive cone constraint. In order to do this, we consider a well known polyhedral approximation of this cone by means of refining the polyhedral constraints after each augmented Lagrangian iteration. We show that our strategy outperforms the standard approach of considering a close polyhedral approximation of the full copositive cone in every iteration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00854v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mituhiro Fukuda, Walter G\'omez, Gabriel Haeser, Leonardo M. Makoto</dc:creator>
    </item>
    <item>
      <title>Inverting Laguerre tessellations: Recovering tessellations from the volumes and centroids of their cells using optimal transport</title>
      <link>https://arxiv.org/abs/2406.00871</link>
      <description>arXiv:2406.00871v1 Announce Type: new 
Abstract: In this paper we study an inverse problem in convex geometry, inspired by a problem in materials science. Firstly, we consider the question of whether a Laguerre tessellation (a partition by convex polytopes) can be recovered from only the volumes and centroids of its cells. We show that this problem has a unique solution and give a constructive way of computing it using optimal transport theory and convex optimisation. Secondly, we consider the problem of fitting a Laguerre tessellation to synthetic volume and centroid data. Given some target volumes and centroids, we seek a Laguerre tessellation such that the difference between the volumes and centroids of its cells and the target volumes and centroids is minimised. For an appropriate objective function and suitable data, we prove that local minimisers of this problem can be constructed using convex optimisation. We also illustrate our results numerically. There is great interest in the computational materials science community in fitting Laguerre tessellations to electron backscatter diffraction (EBSD) and x-ray diffraction images of polycrystalline materials. As an application of our results we fit a 2D Laguerre tessellation to an EBSD image of steel.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00871v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David P. Bourne, Mason Pearce, Steven M. Roper</dc:creator>
    </item>
    <item>
      <title>Wasserstein gradient flow for optimal probability measure decomposition</title>
      <link>https://arxiv.org/abs/2406.00914</link>
      <description>arXiv:2406.00914v1 Announce Type: new 
Abstract: We examine the infinite-dimensional optimization problem of finding a decomposition of a probability measure into K probability sub-measures to minimize specific loss functions inspired by applications in clustering and user grouping. We analytically explore the structures of the support of optimal sub-measures and introduce algorithms based on Wasserstein gradient flow, demonstrating their convergence. Numerical results illustrate the implementability of our algorithms and provide further insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00914v1</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiangze Han, Christopher Thomas Ryan, Xin T. Tong</dc:creator>
    </item>
    <item>
      <title>Stochastic Variance-Reduced Forward-Reflected Methods for Root-Finding Problems</title>
      <link>https://arxiv.org/abs/2406.00937</link>
      <description>arXiv:2406.00937v1 Announce Type: new 
Abstract: We develop two novel stochastic variance-reduction methods to approximate a solution of root-finding problems applicable to both equations and inclusions. Our algorithms leverage a new combination of ideas from the forward-reflected-backward splitting method and a class of unbiased variance-reduction estimators. We construct two new stochastic estimators within this class, inspired by the well-established SVRG and SAGA estimators. These estimators differ significantly from existing approaches used for root-finding algorithms. By appropriately selecting parameters, both algorithms achieve the state-of-the-art oracle complexity of $\mathcal{O}(n + n^{2/3} \epsilon^{-2})$ for achieving an $\epsilon$-solution in terms of the operator residual norm, where $n$ represents the number of summands and $\epsilon$ signifies the desired accuracy. This complexity aligns with the best-known results in stochastic nonconvex optimization without enhancements. We test our algorithms on two numerical examples and compare them with existing methods. The results demonstrate promising improvements offered by the new methods compared to their competitors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00937v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Quoc Tran-Dinh</dc:creator>
    </item>
    <item>
      <title>Port-Hamiltonian structures in infinite-dimensional optimal control: Primal-Dual gradient method and control-by-interconnection</title>
      <link>https://arxiv.org/abs/2406.01087</link>
      <description>arXiv:2406.01087v1 Announce Type: new 
Abstract: In this note, we consider port-Hamiltonian structures in numerical optimal control of ordinary differential equations. By introducing a novel class of nonlinear monotone port-Hamiltonian (pH) systems, we show that the primal-dual gradient method may be viewed as an infinite-dimensional nonlinear pH system. The monotonicity and the particular block structure arising in the optimality system is used to prove exponential stability of the dynamics towards its equilibrium, which is a critical point of the first order optimality conditions. Leveraging the port-based modeling, we propose an optimization-based controller in a suboptimal receding horizon control fashion. To this end, the primal-dual gradient based optimizer-dynamics is coupled to a pH plant dynamics in a power-preserving manner. We show that the resulting model is again monotone pH system and prove that the closed-loop exhibits local exponential convergence towards the equilibrium.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01087v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hannes Gernandt, Manuel Schaller</dc:creator>
    </item>
    <item>
      <title>Random carbon tax policy and investment into emission abatement technologies</title>
      <link>https://arxiv.org/abs/2406.01088</link>
      <description>arXiv:2406.01088v1 Announce Type: new 
Abstract: We study the problem of a profit maximizing electricity producer who has to pay carbon taxes and who decides on investments into technologies for the abatement of carbon emissions in an environment where carbon tax policy is random and where the investment in the abatement technology is divisible, irreversible and subject to transaction costs. We consider two approaches for modelling the randomness in taxes. First we assume a precise probabilistic model for the tax process, namely a pure jump Markov process (so-called tax risk); this leads to a stochastic control problem for the investment strategy. Second, we analyze the case of an {uncertainty-averse} producer who uses a differential game to decide on optimal production and investment. We carry out a rigorous mathematical analysis of the producer's optimization problem and of the associated nonlinear PDEs in both cases. Numerical methods are used to study quantitative properties of the optimal investment strategy. We find that in the tax risk case the investment in abatement technologies is typically lower than in a benchmark scenario with deterministic taxes. However, there are a couple of interesting new twists related to production technology, divisibility of the investment, tax rebates and investor expectations. In the stochastic differential game on the other hand an increase in uncertainty might stipulate more investment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01088v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Katia Colaneri, R\"udiger Frey, Verena K\"ock</dc:creator>
    </item>
    <item>
      <title>Joint Learning of Linear Dynamical Systems under Smoothness Constraints</title>
      <link>https://arxiv.org/abs/2406.01094</link>
      <description>arXiv:2406.01094v1 Announce Type: new 
Abstract: We consider the problem of joint learning of multiple linear dynamical systems. This has received significant attention recently under different types of assumptions on the model parameters. The setting we consider involves a collection of $m$ linear systems each of which resides on a node of a given undirected graph $G = ([m], \mathcal{E})$. We assume that the system matrices are marginally stable, and satisfy a smoothness constraint w.r.t $G$ -- akin to the quadratic variation of a signal on a graph. Given access to the states of the nodes over $T$ time points, we then propose two estimators for joint estimation of the system matrices, along with non-asymptotic error bounds on the mean-squared error (MSE). In particular, we show conditions under which the MSE converges to zero as $m$ increases, typically polynomially fast w.r.t $m$. The results hold under mild (i.e., $T \sim \log m$), or sometimes, even no assumption on $T$ (i.e. $T \geq 2$).</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01094v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hemant Tyagi</dc:creator>
    </item>
    <item>
      <title>Modelling Gas Networks with Compressors: A port-Hamiltonian Approach</title>
      <link>https://arxiv.org/abs/2406.01164</link>
      <description>arXiv:2406.01164v1 Announce Type: new 
Abstract: Transient gas network simulations can significantly assist in design and operational aspects of gas networks. Models used in these simulations require a detailed framework integrating various models of the network constituents - pipes and compressor stations among others. In this context, the port-Hamiltonian modelling framework provides an energy-based modelling approach with a port-based coupling mechanism. This study investigates developing compressor models in an integrated isothermal port-Hamiltonian model for gas networks. Four different models of compressors are considered and their inclusion in a larger network model is detailed. A numerical implementation for a simple testcase is provided to confirm the validity of the proposed model and to highlight their differences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01164v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Thomas Bendokat, Peter Benner, Sara Grundel, Ashwin S. Nayak</dc:creator>
    </item>
    <item>
      <title>Tighter yet more tractable relaxations and nontrivial instance generation for sparse standard quadratic optimization</title>
      <link>https://arxiv.org/abs/2406.01239</link>
      <description>arXiv:2406.01239v1 Announce Type: new 
Abstract: The Standard Quadratic optimization Problem (StQP), arguably the simplest among all classes of NP-hard optimization problems, consists of extremizing a quadratic form (the simplest nonlinear polynomial) over the standard simplex (the simplest polytope/compact feasible set). As a problem class, StQPs may be nonconvex with an exponential number of inefficient local solutions. StQPs arise in a multitude of applications, among them mathematical finance, machine learning (clustering), and modeling in biosciences (e.g., selection and ecology). This paper deals with such StQPs under an additional sparsity or cardinality constraint, which, even for convex objectives, renders NP-hard problems. One motivation to study StQPs under such sparsity restrictions is the high-dimensional portfolio selection problem with too many assets to handle, in particular, in the presence of transaction costs. Here, relying on modern conic optimization techniques, we present tractable convex relaxations for this relevant but difficult problem. We propose novel equivalent reformulations of these relaxations with significant dimensional reduction, which is essential for the tractability of these relaxations when the problem size grows. Moreover, we propose an instance generation procedure which systematically avoids too easy instances. Our extensive computational results illustrate the high quality of the relaxation bounds in a significant number of instances. Furthermore, in contrast with exact mixed-integer quadratic programming models, the solution time of the relaxations is very robust to the choices of the problem parameters. In particular, the reduced formulations achieve significant improvements in terms of the solution time over their counterparts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01239v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Immanuel Bomze, Bo Peng, Yuzhou Qiu, E. Alper Yildirim</dc:creator>
    </item>
    <item>
      <title>Stochastic Newton Proximal Extragradient Method</title>
      <link>https://arxiv.org/abs/2406.01478</link>
      <description>arXiv:2406.01478v1 Announce Type: new 
Abstract: Stochastic second-order methods achieve fast local convergence in strongly convex optimization by using noisy Hessian estimates to precondition the gradient. However, these methods typically reach superlinear convergence only when the stochastic Hessian noise diminishes, increasing per-iteration costs over time. Recent work in [arXiv:2204.09266] addressed this with a Hessian averaging scheme that achieves superlinear convergence without higher per-iteration costs. Nonetheless, the method has slow global convergence, requiring up to $\tilde{O}(\kappa^2)$ iterations to reach the superlinear rate of $\tilde{O}((1/t)^{t/2})$, where $\kappa$ is the problem's condition number. In this paper, we propose a novel stochastic Newton proximal extragradient method that improves these bounds, achieving a faster global linear rate and reaching the same fast superlinear rate in $\tilde{O}(\kappa)$ iterations. We accomplish this by extending the Hybrid Proximal Extragradient (HPE) framework, achieving fast global and local convergence rates for strongly convex functions with access to a noisy Hessian oracle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01478v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruichen Jiang, Micha{\l} Derezi\'nski, Aryan Mokhtari</dc:creator>
    </item>
    <item>
      <title>Online Optimization Perspective on First-Order and Zero-Order Decentralized Nonsmooth Nonconvex Stochastic Optimization</title>
      <link>https://arxiv.org/abs/2406.01484</link>
      <description>arXiv:2406.01484v1 Announce Type: new 
Abstract: We investigate the finite-time analysis of finding ($\delta,\epsilon$)-stationary points for nonsmooth nonconvex objectives in decentralized stochastic optimization. A set of agents aim at minimizing a global function using only their local information by interacting over a network. We present a novel algorithm, called Multi Epoch Decentralized Online Learning (ME-DOL), for which we establish the sample complexity in various settings. First, using a recently proposed online-to-nonconvex technique, we show that our algorithm recovers the optimal convergence rate of smooth nonconvex objectives. We then extend our analysis to the nonsmooth setting, building on properties of randomized smoothing and Goldstein-subdifferential sets. We establish the sample complexity of $O(\delta^{-1}\epsilon^{-3})$, which to the best of our knowledge is the first finite-time guarantee for decentralized nonsmooth nonconvex stochastic optimization in the first-order setting (without weak-convexity), matching its optimal centralized counterpart. We further prove the same rate for the zero-order oracle setting without using variance reduction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01484v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emre Sahinoglu, Shahin Shahrampour</dc:creator>
    </item>
    <item>
      <title>Adaptive discretization algorithms for locally optimal experimental design</title>
      <link>https://arxiv.org/abs/2406.01541</link>
      <description>arXiv:2406.01541v1 Announce Type: new 
Abstract: We develop adaptive discretization algorithms for locally optimal experimental design of nonlinear prediction models. With these algorithms, we refine and improve a pertinent state-of-the-art algorithm in various respects. We establish novel termination, convergence, and convergence rate results for the proposed algorithms. In particular, we prove a sublinear convergence rate result under very general assumptions on the design criterion and, most notably, a linear convergence result under the additional assumption that the design criterion is strongly convex and the design space is finite. Additionally, we prove the finite termination at approximately optimal designs, including upper bounds on the number of iterations until termination. And finally, we illustrate the practical use of the proposed algorithms by means of two application examples from chemical engineering: one with a stationary model and one with a dynamic model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01541v1</guid>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jochen Schmid, Philipp Seufert, Michael Bortz</dc:creator>
    </item>
    <item>
      <title>Extremum Seeking Control for Scalar Maps with Distributed Diffusion PDEs</title>
      <link>https://arxiv.org/abs/2406.01564</link>
      <description>arXiv:2406.01564v1 Announce Type: new 
Abstract: This paper deals with the gradient extremum seeking control for static scalar maps with actuators governed by distributed diffusion partial differential equations (PDEs). To achieve the real-time optimization objective, we design a compensation controller for the distributed diffusion PDE via backstepping transformation in infinite dimensions. A further contribution of this paper is the appropriate motion planning design of the so-called probing (or perturbation) signal, which is more involved than in the non-distributed counterpart. Hence, with these two design ingredients, we provide an averaging-based methodology that can be implemented using the gradient and Hessian estimates. Local exponential stability for the closed-loop equilibrium of the average error dynamics is guaranteed through a Lyapunov-based analysis. By employing the averaging theory for infinite-dimensional systems, we prove that the trajectory converges to a small neighborhood surrounding the optimal point. The effectiveness of the proposed extremum seeking controller for distributed diffusion PDEs in cascade of nonlinear maps to be optimized is illustrated by means of numerical simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01564v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pedro Henrique Silva Coutinho, Tiago Roux Oliveira, Miroslav Krstic</dc:creator>
    </item>
    <item>
      <title>Stochastic Bilevel Optimization with Lower-Level Contextual Markov Decision Processes</title>
      <link>https://arxiv.org/abs/2406.01575</link>
      <description>arXiv:2406.01575v1 Announce Type: new 
Abstract: In various applications, the optimal policy in a strategic decision-making problem depends both on the environmental configuration and exogenous events. For these settings, we introduce Bilevel Optimization with Contextual Markov Decision Processes (BO-CMDP), a stochastic bilevel decision-making model, where the lower level consists of solving a contextual Markov Decision Process (CMDP). BO-CMDP can be viewed as a Stackelberg Game where the leader and a random context beyond the leader's control together decide the setup of (many) MDPs that (potentially multiple) followers best respond to. This framework extends beyond traditional bilevel optimization and finds relevance in diverse fields such as model design for MDPs, tax design, reward shaping and dynamic mechanism design. We propose a stochastic Hyper Policy Gradient Descent (HPGD) algorithm to solve BO-CMDP, and demonstrate its convergence. Notably, HPGD only utilizes observations of the followers' trajectories. Therefore, it allows followers to use any training procedure and the leader to be agnostic of the specific algorithm used, which aligns with various real-world scenarios. We further consider the setting when the leader can influence the training of followers and propose an accelerated algorithm. We empirically demonstrate the performance of our algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01575v1</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vinzenz Thoma, Barna Pasztor, Andreas Krause, Giorgia Ramponi, Yifan Hu</dc:creator>
    </item>
    <item>
      <title>Stochastic Control with Signatures</title>
      <link>https://arxiv.org/abs/2406.01585</link>
      <description>arXiv:2406.01585v1 Announce Type: new 
Abstract: This paper proposes to parameterize open loop controls in stochastic optimal control problems via suitable classes of functionals depending on the driver's path signature, a concept adopted from rough path integration theory. We rigorously prove that these controls are dense in the class of progressively measurable controls and use rough path methods to establish suitable conditions for stability of the controlled dynamics and target functional. These results pave the way for Monte Carlo methods to stochastic optimal control for generic target functionals and dynamics. We discuss the rather versatile numerical algorithms for computing approximately optimal controls and verify their accurateness in benchmark problems from Mathematical Finance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01585v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>P. Bank, C. Bayer, P. P. Hager, S. Riedel, T. Nauen</dc:creator>
    </item>
    <item>
      <title>Multi-qubit Lattice Surgery Scheduling</title>
      <link>https://arxiv.org/abs/2405.17688</link>
      <description>arXiv:2405.17688v1 Announce Type: cross 
Abstract: Fault-tolerant quantum computation using two-dimensional topological quantum error correcting codes can benefit from multi-qubit long-range operations. By using simple commutation rules, a quantum circuit can be transpiled into a sequence of solely non-Clifford multi-qubit gates. Prior work on fault-tolerant compilation avoids optimal scheduling of such gates since they reduce the parallelizability of the circuit. We observe that the reduced parallelization potential is outweighed by the significant reduction in the number of gates. We therefore devise a method for scheduling multi-qubit lattice surgery using an earliest-available-first policy, solving the associated forest packing problem using a representation of the multi-qubit gates as Steiner trees. Our extensive testing on random and application-inspired circuits demonstrates the method's scalability and performance. We show that the transpilation significantly reduces the circuit length on the set of circuits tested, and that the resulting circuit of multi-qubit gates has a further reduction in the expected circuit execution time compared to serial execution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17688v1</guid>
      <category>quant-ph</category>
      <category>cs.AR</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Allyson Silva, Xiangyi Zhang, Zak Webb, Mia Kramer, Chan Woo Yang, Xiao Liu, Jessica Lemieux, Ka-Wai Chen, Artur Scherer, Pooya Ronagh</dc:creator>
    </item>
    <item>
      <title>Multiple-arc optimization of low-thrust earth-moon orbit transfers leveraging implicit costate transformation</title>
      <link>https://arxiv.org/abs/2406.00336</link>
      <description>arXiv:2406.00336v1 Announce Type: cross 
Abstract: This work focuses on minimum-time low-thrust orbit transfers from a prescribed low Earth orbit to a specified low lunar orbit. The well-established indirect formulation of minimum-time orbit transfers is extended to a multibody dynamical framework, with initial and final orbits around two distinct primaries. To do this, different representations, useful for describing orbit dynamics, are introduced, i.e., modified equinoctial elements (MEE) and Cartesian coordinates (CC). Use of two sets of MEE, relative to either Earth or Moon, allows simple writing of the boundary conditions about the two celestial bodies, but requires the formulation of a multiple-arc trajectory optimization problem, including two legs: (a) geocentric leg and (b) selenocentric leg. In the numerical solution process, the transition between the two MEE representations uses CC, which play the role of convenient intermediate, matching variables. The multiple-arc formulation at hand leads to identifying a set of intermediate necessary conditions for optimality, at the transition between the two legs. This research proves that a closed-form solution to these intermediate conditions exists, leveraging implicit costate transformation. As a result, the parameter set for an indirect algorithm retains the reduced size of the typical set associated with a single-arc optimization problem. The indirect heuristic technique, based on the joint use of the necessary conditions and a heuristic algorithm (i.e., differential evolution in this study) is proposed as the numerical solution method, together with the definition of a layered fitness function, aimed at facilitating convergence. The minimum-time trajectory of interest is sought in a high-fidelity dynamical framework, with the use of planetary ephemeris and the inclusion of the simultaneous gravitational action of Sun, Earth, and Moon, along the entire transfer path.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00336v1</guid>
      <category>astro-ph.EP</category>
      <category>math.DS</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.actaastro.2024.04.030</arxiv:DOI>
      <arxiv:journal_reference>Acta Astronautica, 2024, 220: 330-344</arxiv:journal_reference>
      <dc:creator>Alessandro Beolchi, Mauro Pontani, Chiara Pozzi, Elena Fantino</dc:creator>
    </item>
    <item>
      <title>Performance Evaluation of Damping Systems in Civil Engineering Structures Via Minimal Sensor</title>
      <link>https://arxiv.org/abs/2406.00372</link>
      <description>arXiv:2406.00372v1 Announce Type: cross 
Abstract: To control structural responses under various actions, the growing use of supplementary damping systems in modern civil engineering structures necessitates inspecting and evaluating their operational performance postinstallation. However, due to the dispersed placement and complex nonlinearities of these devices, difficulties arise in determining minimal sensor configuration. This is inherently connected to a pivotal challenge: establishing a reliable input-output mapping, which comprises both the mathematical model and sensor arrangements. Prior work indicates this can be achieved through theoretical observability analysis or Lie symmetries analysis, both of which provide different perspectives on the existence of a way to access the solutions of a system identification problem uniquely (at least locally). The present study introduces a unified framework, enhanced by algorithm realization as an application guide, for analyzing the observability and Lie symmetries of a given input-output mapping. We demonstrate its implementation via examples of a building structure with various damping systems under different conditions such as seismic loads, wind loads, and operational vibrations. Finally, we present a case study for an isolation building with an inerter damper and minimal sensor arrangement under seismic action. The results demonstrate that the unscented Kalman filter, a system identification method, can precisely estimate structural responses and assess damping device performance once a reliable input-output mapping is established.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00372v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xinhao He, Dan Li</dc:creator>
    </item>
    <item>
      <title>Model Predictive Control and Reinforcement Learning: A Unified Framework Based on Dynamic Programming</title>
      <link>https://arxiv.org/abs/2406.00592</link>
      <description>arXiv:2406.00592v1 Announce Type: cross 
Abstract: In this paper we describe a new conceptual framework that connects approximate Dynamic Programming (DP), Model Predictive Control (MPC), and Reinforcement Learning (RL). This framework centers around two algorithms, which are designed largely independently of each other and operate in synergy through the powerful mechanism of Newton's method. We call them the off-line training and the on-line play algorithms. The names are borrowed from some of the major successes of RL involving games; primary examples are the recent (2017) AlphaZero program (which plays chess, [SHS17], [SSS17]), and the similarly structured and earlier (1990s) TD-Gammon program (which plays backgammon, [Tes94], [Tes95], [TeG96]). In these game contexts, the off-line training algorithm is the method used to teach the program how to evaluate positions and to generate good moves at any given position, while the on-line play algorithm is the method used to play in real time against human or computer opponents.
  Significantly, the synergy between off-line training and on-line play also underlies MPC (as well as other major classes of sequential decision problems), and indeed the MPC design architecture is very similar to the one of AlphaZero and TD-Gammon. This conceptual insight provides a vehicle for bridging the cultural gap between RL and MPC, and sheds new light on some fundamental issues in MPC. These include the enhancement of stability properties through rollout, the treatment of uncertainty through the use of certainty equivalence, the resilience of MPC in adaptive control settings that involve changing system parameters, and the insights provided by the superlinear performance bounds implied by Newton's method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00592v1</guid>
      <category>eess.SY</category>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dimitri P. Bertsekas</dc:creator>
    </item>
    <item>
      <title>Portfolio Optimization with Robust Covariance and Conditional Value-at-Risk Constraints</title>
      <link>https://arxiv.org/abs/2406.00610</link>
      <description>arXiv:2406.00610v1 Announce Type: cross 
Abstract: The measure of portfolio risk is an important input of the Markowitz framework. In this study, we explored various methods to obtain a robust covariance estimators that are less susceptible to financial data noise. We evaluated the performance of large-cap portfolio using various forms of Ledoit Shrinkage Covariance and Robust Gerber Covariance matrix during the period of 2012 to 2022. Out-of-sample performance indicates that robust covariance estimators can outperform the market capitalization-weighted benchmark portfolio, particularly during bull markets. The Gerber covariance with Mean-Absolute-Deviation (MAD) emerged as the top performer. However, robust estimators do not manage tail risk well under extreme market conditions, for example, Covid-19 period. When we aim to control for tail risk, we should add constraint on Conditional Value-at-Risk (CVaR) to make more conservative decision on risk exposure. Additionally, we incorporated unsupervised clustering algorithm K-means to the optimization algorithm (i.e. Nested Clustering Optimization, NCO). It not only helps mitigate numerical instability of the optimization algorithm, but also contributes to lower drawdown as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00610v1</guid>
      <category>q-fin.PM</category>
      <category>econ.EM</category>
      <category>math.OC</category>
      <category>q-fin.MF</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiqin Zhou</dc:creator>
    </item>
    <item>
      <title>Log-Scale Quantization in Distributed First-Order Methods: Gradient-based Learning from Distributed Data</title>
      <link>https://arxiv.org/abs/2406.00621</link>
      <description>arXiv:2406.00621v1 Announce Type: cross 
Abstract: Decentralized strategies are of interest for learning from large-scale data over networks. This paper studies learning over a network of geographically distributed nodes/agents subject to quantization. Each node possesses a private local cost function, collectively contributing to a global cost function, which the proposed methodology aims to minimize. In contrast to many existing literature, the information exchange among nodes is quantized. We adopt a first-order computationally-efficient distributed optimization algorithm (with no extra inner consensus loop) that leverages node-level gradient correction based on local data and network-level gradient aggregation only over nearby nodes. This method only requires balanced networks with no need for stochastic weight design. It can handle log-scale quantized data exchange over possibly time-varying and switching network setups. We analyze convergence over both structured networks (for example, training over data-centers) and ad-hoc multi-agent networks (for example, training over dynamic robotic networks). Through analysis and experimental validation, we show that (i) structured networks generally result in a smaller optimality gap, and (ii) logarithmic quantization leads to smaller optimality gap compared to uniform quantization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00621v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammadreza Doostmohammadian, Muhammad I. Qureshi, Mohammad Hossein Khalesi, Hamid R. Rabiee, Usman A. Khan</dc:creator>
    </item>
    <item>
      <title>Elementary solution to the fair division problem</title>
      <link>https://arxiv.org/abs/2406.00733</link>
      <description>arXiv:2406.00733v1 Announce Type: cross 
Abstract: A new and relatively elementary approach is proposed for solving the problem of fair division of a continuous resource (measurable space, pie, etc.) between several participants, the selection criteria of which are described by charges (signed measures). The setting of the problem with charges is considered for the first time. The problem comes down to analyzing the properties of the trajectories of a specially constructed dynamical system acting in the space of finite measurable partitions. Exponentially fast convergence to a limit solution is proved for both the case of true measures and the case of charges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00733v1</guid>
      <category>math.DS</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Blank, Maxim Polyakov</dc:creator>
    </item>
    <item>
      <title>Local Methods with Adaptivity via Scaling</title>
      <link>https://arxiv.org/abs/2406.00846</link>
      <description>arXiv:2406.00846v1 Announce Type: cross 
Abstract: The rapid development of machine learning and deep learning has introduced increasingly complex optimization challenges that must be addressed. Indeed, training modern, advanced models has become difficult to implement without leveraging multiple computing nodes in a distributed environment. Distributed optimization is also fundamental to emerging fields such as federated learning. Specifically, there is a need to organize the training process to minimize the time lost due to communication. A widely used and extensively researched technique to mitigate the communication bottleneck involves performing local training before communication. This approach is the focus of our paper. Concurrently, adaptive methods that incorporate scaling, notably led by Adam, have gained significant popularity in recent years. Therefore, this paper aims to merge the local training technique with the adaptive approach to develop efficient distributed learning methods. We consider the classical Local SGD method and enhance it with a scaling feature. A crucial aspect is that the scaling is described generically, allowing us to analyze various approaches, including Adam, RMSProp, and OASIS, in a unified manner. In addition to theoretical analysis, we validate the performance of our methods in practice by training a neural network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00846v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Savelii Chezhegov, Sergey Skorik, Nikolas Khachaturov, Danil Shalagin, Aram Avetisyan, Aleksandr Beznosikov, Martin Tak\'a\v{c}, Yaroslav Kholodov, Alexander Gasnikov</dc:creator>
    </item>
    <item>
      <title>Optimal Control of General Impulsive VS-EIAR Epidemic Models with Application to Covid-19</title>
      <link>https://arxiv.org/abs/2406.00864</link>
      <description>arXiv:2406.00864v1 Announce Type: cross 
Abstract: In this work, we are interested in a VS-EIAR epidemiological model considering vaccinated individuals ${V_i: i=1,\ldots,n}$, where $n\in \mathbb{N}^{*}$. The dynamic of the VS-EIAR model involves several ordinary differential equations that describe the changes in the vaccinated, susceptible, infected, exposed, asymptomatic, and deceased population groups. Our aim is to reduce the number of susceptible, exposed, infected, and asymptomatic individuals by administering vaccination doses to susceptible individuals and treatment to infected population. To achieve this, we utilize optimal control theory to regulate the dynamic of our considered epidemic model within a terminal optimal time $\tau^{*}$. Pontryagin's maximum principle (PMP) will be employed to establish the existence of an optimal control time $(v^{*}(t), u^{*}(t))$. We also incorporate an impulsive VS-EIAR epidemic model, with special attention given to immigration or the travel of certain population groups. Finally, we provide a numerical simulation to demonstrate the practical implementation of the theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00864v1</guid>
      <category>math.DS</category>
      <category>math.OC</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mamadou Abdoul Diop, Mohammed Elghandouri, Khalil Ezzinbi</dc:creator>
    </item>
    <item>
      <title>Demystifying SGD with Doubly Stochastic Gradients</title>
      <link>https://arxiv.org/abs/2406.00920</link>
      <description>arXiv:2406.00920v1 Announce Type: cross 
Abstract: Optimization objectives in the form of a sum of intractable expectations are rising in importance (e.g., diffusion models, variational autoencoders, and many more), a setting also known as "finite sum with infinite data." For these problems, a popular strategy is to employ SGD with doubly stochastic gradients (doubly SGD): the expectations are estimated using the gradient estimator of each component, while the sum is estimated by subsampling over these estimators. Despite its popularity, little is known about the convergence properties of doubly SGD, except under strong assumptions such as bounded variance. In this work, we establish the convergence of doubly SGD with independent minibatching and random reshuffling under general conditions, which encompasses dependent component gradient estimators. In particular, for dependent estimators, our analysis allows fined-grained analysis of the effect correlations. As a result, under a per-iteration computational budget of $b \times m$, where $b$ is the minibatch size and $m$ is the number of Monte Carlo samples, our analysis suggests where one should invest most of the budget in general. Furthermore, we prove that random reshuffling (RR) improves the complexity dependence on the subsampling noise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00920v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kyurae Kim, Joohwan Ko, Yi-An Ma, Jacob R. Gardner</dc:creator>
    </item>
    <item>
      <title>Detour Monophonic Vertex Cover Pebbling Number (DMVCPN) of Some Standard Graphs</title>
      <link>https://arxiv.org/abs/2406.01009</link>
      <description>arXiv:2406.01009v1 Announce Type: cross 
Abstract: Let $G$ be a connected graph with vertex set $V(G)$ and edge set $E(G)$. Pebbling shift is a deletion of two pebbles from a vertex and a placement of one pebble at a neighbouring vertex. The vertex cover set, $D_{vc}$ for graph $G$ is the subset of $V(G)$ such that every edge in $G$ has at least one end in $D_{vc}$. A detour monophonic path is considered to be a longest chordless path between two non adjacent vertices $x$ and $y$. A detour monophonic vertex cover pebbling number, ${\mu}_{vc}(G),$ is a minimum number of pebbles required to cover all the vertices of the vertex cover set of $G$ with at least one pebble each on them after the transformation of pebbles by using detour monophonic paths. We determine the detour monophonic vertex cover pebbling number (DMVCPN) of the cycle, path, fan, and wheel graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01009v1</guid>
      <category>math.CO</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>K. Christy Rani, I. Dhivviyanandam</dc:creator>
    </item>
    <item>
      <title>Achieving Tractable Minimax Optimal Regret in Average Reward MDPs</title>
      <link>https://arxiv.org/abs/2406.01234</link>
      <description>arXiv:2406.01234v1 Announce Type: cross 
Abstract: In recent years, significant attention has been directed towards learning average-reward Markov Decision Processes (MDPs). However, existing algorithms either suffer from sub-optimal regret guarantees or computational inefficiencies. In this paper, we present the first tractable algorithm with minimax optimal regret of $\widetilde{\mathrm{O}}(\sqrt{\mathrm{sp}(h^*) S A T})$, where $\mathrm{sp}(h^*)$ is the span of the optimal bias function $h^*$, $S \times A$ is the size of the state-action space and $T$ the number of learning steps. Remarkably, our algorithm does not require prior information on $\mathrm{sp}(h^*)$. Our algorithm relies on a novel subroutine, Projected Mitigated Extended Value Iteration (PMEVI), to compute bias-constrained optimal policies efficiently. This subroutine can be applied to various previous algorithms to improve regret bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01234v1</guid>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Victor Boone, Zihan Zhang</dc:creator>
    </item>
    <item>
      <title>Feedback Stabilization and Finite Element Error Analysis of Viscous Burgers Equation around Non-Constant Steady State</title>
      <link>https://arxiv.org/abs/2406.01553</link>
      <description>arXiv:2406.01553v1 Announce Type: cross 
Abstract: In this article, we explore the feedback stabilization of a viscous Burgers equation around a non-constant steady state using localized interior controls and then develop error estimates for the stabilized system using finite element method. The system is not only feedback stabilizable but exhibits an exponential decay $-\omega&lt;0$ for any $\omega&gt;0$. The derivation of a stabilizing control in feedback form is achieved by solving a suitable algebraic Riccati equation posed for the linearized system. In the second part of the article, we utilize a conforming finite element method to discretize the continuous system, resulting in a finite-dimensional discrete system. This approximated system is also proven to be feedback stabilizable (uniformly) with exponential decay $-\omega+\epsilon$ for any $\epsilon&gt;0$. The feedback control for this discrete system is obtained by solving a discrete algebraic Riccati equation. To validate the effectiveness of our approach, we provide error estimates for both the stabilized solutions and the stabilizing feedback controls. Numerical implementations are carried out to support and validate our theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01553v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wasim Akram</dc:creator>
    </item>
    <item>
      <title>Biased Stochastic First-Order Methods for Conditional Stochastic Optimization and Applications in Meta Learning</title>
      <link>https://arxiv.org/abs/2002.10790</link>
      <description>arXiv:2002.10790v2 Announce Type: replace 
Abstract: Conditional stochastic optimization covers a variety of applications ranging from invariant learning and causal inference to meta-learning. However, constructing unbiased gradient estimators for such problems is challenging due to the composition structure. As an alternative, we propose a biased stochastic gradient descent (BSGD) algorithm and study the bias-variance tradeoff under different structural assumptions. We establish the sample complexities of BSGD for strongly convex, convex, and weakly convex objectives under smooth and non-smooth conditions. Our lower bound analysis shows that the sample complexities of BSGD cannot be improved for general convex objectives and nonconvex objectives except for smooth nonconvex objectives with Lipschitz continuous gradient estimator. For this special setting, we propose an accelerated algorithm called biased SpiderBoost (BSpiderBoost) that matches the lower bound complexity. We further conduct numerical experiments on invariant logistic regression and model-agnostic meta-learning to illustrate the performance of BSGD and BSpiderBoost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2002.10790v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Advances in Neural Information Processing Systems 33 (NeurIPS 2020)</arxiv:journal_reference>
      <dc:creator>Yifan Hu, Siqi Zhang, Xin Chen, Niao He</dc:creator>
    </item>
    <item>
      <title>The Speed-Robustness Trade-Off for First-Order Methods with Additive Gradient Noise</title>
      <link>https://arxiv.org/abs/2109.05059</link>
      <description>arXiv:2109.05059v2 Announce Type: replace 
Abstract: We study the trade-off between convergence rate and sensitivity to stochastic additive gradient noise for first-order optimization methods. Ordinary Gradient Descent (GD) can be made fast-and-sensitive or slow-and-robust by increasing or decreasing the stepsize, respectively. However, it is not clear how such a trade-off can be navigated when working with accelerated methods such as Polyak's Heavy Ball (HB) or Nesterov's Fast Gradient (FG) methods. We consider three classes of functions: (1) smooth strongly convex quadratics, (2) smooth strongly convex functions, and (3) functions that satisfy the Polyak-Lojasiewicz property and have one-sided Lipschitz gradients. For each function class, we present a tractable way to compute the convergence rate and sensitivity to additive gradient noise for a broad family of first-order methods, and we present algorithm designs that trade off these competing performance metrics. Each design consists of a simple analytic update rule with two states of memory, similar to HB and FG. Moreover, each design has a scalar tuning parameter that explicitly trades off convergence rate and sensitivity to additive gradient noise. We numerically validate the performance of our designs by comparing their convergence rate and sensitivity to those of many other algorithms, and through simulations on Nesterov's "bad function".</description>
      <guid isPermaLink="false">oai:arXiv.org:2109.05059v2</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bryan Van Scoy, Laurent Lessard</dc:creator>
    </item>
    <item>
      <title>On the Convergence of Policy in Unregularized Policy Mirror Descent</title>
      <link>https://arxiv.org/abs/2205.08176</link>
      <description>arXiv:2205.08176v3 Announce Type: replace 
Abstract: In this short note, we give the convergence analysis of the policy in the recent famous policy mirror descent (PMD). We mainly consider the unregularized setting following [11] with generalized Bregman divergence. The difference is that we directly give the convergence rates of policy under generalized Bregman divergence. Our results are inspired by the convergence of value function in previous works and are an extension study of policy mirror descent. Though some results have already appeared in previous work, we further discover a large body of Bregman divergences could give finite-step convergence to an optimal policy, such as the classical Euclidean distance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.08176v3</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dachao Lin, Zhihua Zhang</dc:creator>
    </item>
    <item>
      <title>Synchronous deautoconvolution algorithm for discrete-time positive signals via I-divergence approximation</title>
      <link>https://arxiv.org/abs/2302.12644</link>
      <description>arXiv:2302.12644v2 Announce Type: replace 
Abstract: We pose the problem of the optimal approximation of a given nonnegative signal $y_t$ with the scalar autoconvolution $(x*x)_t$ of a nonnegative signal $x_t$, where $x_t$ and $y_t$ are signals of equal length. The $\mathcal{I}$-divergence has been adopted as optimality criterion, being well suited to incorporate nonnegativity constraints. To find a minimizer we derive an iterative descent algorithm of the alternating minimization type. The algorithm is based on the lifting of the original problem to a larger space, a relaxation technique developed by Csisz\'ar and Tusn\'ady in [Statistics \&amp; Decisions (S1) (1984), 205--237] which, in the present context, requires the solution of a hard partial minimization problem. We study the asymptotic behavior of the algorithm exploiting the optimality properties of the partial minimization problems and prove, among other results, that its limit points are Kuhn-Tucker points of the original minimization problem. Numerical experiments illustrate the results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.12644v2</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.cam.2024.116025</arxiv:DOI>
      <arxiv:journal_reference>Journal of computational and applied mathematics 451, Article No. 116025, 2024</arxiv:journal_reference>
      <dc:creator>Lorenzo Finesso, Peter Spreij</dc:creator>
    </item>
    <item>
      <title>Gradient descent in matrix factorization: Understanding large initialization</title>
      <link>https://arxiv.org/abs/2305.19206</link>
      <description>arXiv:2305.19206v2 Announce Type: replace 
Abstract: Gradient Descent (GD) has been proven effective in solving various matrix factorization problems. However, its optimization behavior with large initial values remains less understood. To address this gap, this paper presents a novel theoretical framework for examining the convergence trajectory of GD with a large initialization. The framework is grounded in signal-to-noise ratio concepts and inductive arguments. The results uncover an implicit incremental learning phenomenon in GD and offer a deeper understanding of its performance in large initialization scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.19206v2</guid>
      <category>math.OC</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hengchao Chen, Xin Chen, Mohamad Elmasri, Qiang Sun</dc:creator>
    </item>
    <item>
      <title>Communication-Efficient Gradient Descent-Accent Methods for Distributed Variational Inequalities: Unified Analysis and Local Updates</title>
      <link>https://arxiv.org/abs/2306.05100</link>
      <description>arXiv:2306.05100v2 Announce Type: replace 
Abstract: Distributed and federated learning algorithms and techniques associated primarily with minimization problems. However, with the increase of minimax optimization and variational inequality problems in machine learning, the necessity of designing efficient distributed/federated learning approaches for these problems is becoming more apparent. In this paper, we provide a unified convergence analysis of communication-efficient local training methods for distributed variational inequality problems (VIPs). Our approach is based on a general key assumption on the stochastic estimates that allows us to propose and analyze several novel local training algorithms under a single framework for solving a class of structured non-monotone VIPs. We present the first local gradient descent-accent algorithms with provable improved communication complexity for solving distributed variational inequalities on heterogeneous data. The general algorithmic framework recovers state-of-the-art algorithms and their sharp convergence guarantees when the setting is specialized to minimization or minimax optimization problems. Finally, we demonstrate the strong performance of the proposed algorithms compared to state-of-the-art methods when solving federated minimax optimization problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.05100v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siqi Zhang, Sayantan Choudhury, Sebastian U Stich, Nicolas Loizou</dc:creator>
    </item>
    <item>
      <title>Extended Zero-Gradient-Sum Approach for Constrained Distributed Optimization with Free Initialization</title>
      <link>https://arxiv.org/abs/2308.13743</link>
      <description>arXiv:2308.13743v3 Announce Type: replace 
Abstract: This paper proposes an extended zero-gradient-sum (EZGS) approach for solving constrained distributed optimization (DO) with free initialization. A Newton-based continuous-time algorithm (CTA) is first designed for general constrained optimization and then extended to solve constrained DO based on the EZGS method. It is shown that for typical consensus protocols, the EZGS CTA can achieve the performance with exponential/finite/fixed/prescribed-time convergence. Particularly, the nonlinear consensus protocols for finite-time EZGS algorithms can have heterogeneous power coefficients. The prescribed-time EZGS dynamics is continuous and uniformly bounded, which can achieve the optimal solution in one stage. Moreover, the barrier method is employed to tackle the inequality constraints. Finally, the performance of the proposed algorithms is verified by numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.13743v3</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xinli Shi, Xinghuo Yu, Guanghui Wen, Xiangping Xu</dc:creator>
    </item>
    <item>
      <title>Complexity analysis of regularization methods for implicitly constrained least squares</title>
      <link>https://arxiv.org/abs/2309.07086</link>
      <description>arXiv:2309.07086v3 Announce Type: replace 
Abstract: Optimization problems constrained by partial differential equations (PDEs) naturally arise in scientific computing, as those constraints often model physical systems or the simulation thereof. In an implicitly constrained approach, the constraints are incorporated into the objective through a reduced formulation. To this end, a numerical procedure is typically applied to solve the constraint system, and efficient numerical routines with quantifiable cost have long been developed for that purpose. Meanwhile, the field of complexity in optimization, that estimates the cost of an optimization algorithm, has received significant attention in the literature, with most of the focus being on unconstrained or explicitly constrained problems.
  In this paper, we analyze an algorithmic framework based on quadratic regularization for implicitly constrained nonlinear least squares. By leveraging adjoint formulations, we can quantify the worst-case cost of our method to reach an approximate stationary point of the optimization problem. Our definition of such points exploits the least-squares structure of the objective, and provides new complexity insights even in the unconstrained setting. Numerical experiments conducted on PDE-constrained optimization problems demonstrate the efficiency of the proposed framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.07086v3</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akwum Onwunta, Cl\'ement W. Royer</dc:creator>
    </item>
    <item>
      <title>Extragradient Type Methods for Riemannian Variational Inequality Problems</title>
      <link>https://arxiv.org/abs/2309.14155</link>
      <description>arXiv:2309.14155v2 Announce Type: replace 
Abstract: Riemannian convex optimization and minimax optimization have recently drawn considerable attention. Their appeal lies in their capacity to adeptly manage the non-convexity of the objective function as well as constraints inherent in the feasible set in the Euclidean sense. In this work, we delve into monotone Riemannian Variational Inequality Problems (RVIPs), which encompass both Riemannian convex optimization and minimax optimization as particular cases. In the context of Euclidean space, it is established that the last-iterates of both the extragradient (EG) and past extragradient (PEG) methods converge to the solution of monotone variational inequality problems at a rate of $O\left(\frac{1}{\sqrt{T}}\right)$ (Cai et al., 2022). However, analogous behavior on Riemannian manifolds remains an open question. To bridge this gap, we introduce the Riemannian extragradient (REG) and Riemannian past extragradient (RPEG) methods. We demonstrate that both exhibit $O\left(\frac{1}{\sqrt{T}}\right)$ last-iterate convergence. Additionally, we show that the average-iterate convergence of both REG and RPEG is $O\left(\frac{1}{{T}}\right)$, aligning with observations in the Euclidean case (Mokhtari et al., 2020). These results are enabled by judiciously addressing the holonomy effect so that additional complications in Riemannian cases can be reduced and the Euclidean proof inspired by the performance estimation problem (PEP) technique or the sum-of-squares (SOS) technique can be applied again.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.14155v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zihao Hu, Guanghui Wang, Xi Wang, Andre Wibisono, Jacob Abernethy, Molei Tao</dc:creator>
    </item>
    <item>
      <title>Full-Low Evaluation Methods For Bound and Linearly Constrained Derivative-Free Optimization</title>
      <link>https://arxiv.org/abs/2310.00755</link>
      <description>arXiv:2310.00755v2 Announce Type: replace 
Abstract: Derivative-free optimization (DFO) consists in finding the best value of an objective function without relying on derivatives. To tackle such problems, one may build approximate derivatives, using for instance finite-difference estimates. One may also design algorithmic strategies that perform space exploration and seek improvement over the current point. The first type of strategy often provides good performance on smooth problems but at the expense of more function evaluations. The second type is cheaper and typically handles non-smoothness or noise in the objective better. Recently, full-low evaluation methods have been proposed as a hybrid class of DFO algorithms that combine both strategies, respectively denoted as Full-Eval and Low-Eval. In the unconstrained case, these methods showed promising numerical performance.
  In this paper, we extend the full-low evaluation framework to bound and linearly constrained derivative-free optimization. We derive convergence results for an instance of this framework, that combines finite-difference quasi-Newton steps with probabilistic direct-search steps. The former are projected onto the feasible set, while the latter are defined within tangent cones identified by nearby active constraints. We illustrate the practical performance of our instance on standard linearly constrained problems, that we adapt to introduce noisy evaluations as well as non-smoothness. In all cases, our method performs favorably compared to algorithms that rely solely on Full-eval or Low-eval iterations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.00755v2</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Cl\'ement W. Royer, Oumaima Sohab, Luis Nunes Vicente</dc:creator>
    </item>
    <item>
      <title>On the Sublinear Convergence of Projected Policy Gradient for Any Constant Step Sizes</title>
      <link>https://arxiv.org/abs/2311.01104</link>
      <description>arXiv:2311.01104v5 Announce Type: replace 
Abstract: Projected policy gradient (PPG) is a basic policy optimization method in reinforcement learning. Given access to exact policy evaluations, previous studies have established the sublinear convergence of PPG for sufficiently small step sizes based on the smoothness and the gradient domination properties of the value function. However, as the step size goes to infinity, PPG reduces to the classic policy iteration method, which suggests the convergence of PPG even for large step sizes. In this paper, we fill this gap and show that PPG admits a sublinear convergence for any constant step sizes. Due to the existence of the state-wise visitation measure in the expression of policy gradient, the existing optimization-based analysis framework for a preconditioned version of PPG (i.e., projected Q-ascent) is not applicable, to the best of our knowledge. Instead, we proceed the proof by computing the state-wise improvement lower bound of PPG based on its inherent structure. In addition, the finite iteration convergence of PPG for any constant step size is further established, which is also new.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.01104v5</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiacai Liu, Wenye Li, Dachao Lin, Ke Wei, Zhihua Zhang</dc:creator>
    </item>
    <item>
      <title>A Large Deviations Perspective on Policy Gradient Algorithms</title>
      <link>https://arxiv.org/abs/2311.07411</link>
      <description>arXiv:2311.07411v3 Announce Type: replace 
Abstract: Motivated by policy gradient methods in the context of reinforcement learning, we identify a large deviation rate function for the iterates generated by stochastic gradient descent for possibly non-convex objectives satisfying a Polyak-{\L}ojasiewicz condition. Leveraging the contraction principle from large deviations theory, we illustrate the potential of this result by showing how convergence properties of policy gradient with a softmax parametrization and an entropy regularized objective can be naturally extended to a wide spectrum of other policy parametrizations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.07411v3</guid>
      <category>math.OC</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wouter Jongeneel, Daniel Kuhn, Mengmeng Li</dc:creator>
    </item>
    <item>
      <title>Optimality conditions in terms of Bouligand generalized differentials for a nonsmooth semilinear elliptic optimal control problem with distributed and boundary control pointwise constraints</title>
      <link>https://arxiv.org/abs/2311.15669</link>
      <description>arXiv:2311.15669v2 Announce Type: replace 
Abstract: This paper is concerned with an optimal control problem governed by nonsmooth semilinear elliptic partial differential equations with both distributed and boundary unilateral pointwise control constraints, in which the nonlinear coefficient in the state equation is not differentiable at one point.
  Therefore, the Bouligand subdifferential of this nonsmooth coefficient in every point consists of one or two elements that will be used to construct the two associated Bouligand generalized derivatives of the control-to-state operator in any admissible control.
  These Bouligand generalized derivatives appear in a novel optimality condition, which extends the purely primal optimality condition saying that the directional derivative of the reduced objective functional in admissible directions in nonnegative.
  We then establish the optimality conditions in the form of multiplier existence. There, in addition to the existence of the adjoint states and of the nonnegative multipliers associated with the unilateral pointwise constraints as usual, other nonnegative multipliers exist and correspond to the nondifferentiability of the control-to-state mapping.
  The latter type of optimality conditions shall be applied to an optimal control satisfying the so-called \emph{constraint qualification} to derive a \emph{strong} stationarity, where the sign of the associated adjoint state does not vary on the level set of the corresponding optimal state at the value of nondifferentiability.
  Finally, this strong stationarity is also shown to be equivalent to the purely primal optimality condition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.15669v2</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vu Huu Nhu, Nguyen Hai Son</dc:creator>
    </item>
    <item>
      <title>Linear Quadratic Dual Control</title>
      <link>https://arxiv.org/abs/2312.06014</link>
      <description>arXiv:2312.06014v2 Announce Type: replace 
Abstract: This is a draft paper originally posted on Arxiv as a documentation of a plenary lecture at CDC2023. The core material has been accepted for publication at L4DC 2024. Certainty equivalence adaptive controllers are analysed using a ``data-driven Riccati equation'', corresponding to the model-free Bellman equation used in Q-learning. The equation depends quadratically on data correlation matrices. This makes it possible to derive simple sufficient conditions for stability and robustness to unmodeled dynamics in adaptive systems. The paper is concluded by short remarks on how the bounds can be used to quantify the interplay between excitation levels and robustness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.06014v2</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anders Rantzer</dc:creator>
    </item>
    <item>
      <title>Extended Formulations for Binary Optimal Control Problems</title>
      <link>https://arxiv.org/abs/2401.03942</link>
      <description>arXiv:2401.03942v2 Announce Type: replace 
Abstract: Extended formulations are an important tool in polyhedral combinatorics. Many combinatorial optimization problems require an exponential number of inequalities when modeled as a linear program in the natural space of variables. However, by adding artificial variables, one can often find a small linear formulation, i.e., one containing a polynomial number of variables and constraints, such that the projection to the original space of variables yields a perfect linear formulation. Motivated by binary optimal control problems with switching constraints, we show that a similar approach can be useful also for optimization problems in function space, in order to model the closed convex hull of feasible controls in a compact way. More specifically, we present small extended formulations for switches with bounded variation and for dwell-time constraints. For general linear switching point constraints, we devise an extended model linearizing the problem, but show that a small extended formulation that is compatible with discretization cannot exist unless P=NP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.03942v2</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christoph Buchheim</dc:creator>
    </item>
    <item>
      <title>Meta-Learning Linear Quadratic Regulators: A Policy Gradient MAML Approach for Model-free LQR</title>
      <link>https://arxiv.org/abs/2401.14534</link>
      <description>arXiv:2401.14534v2 Announce Type: replace 
Abstract: We investigate the problem of learning linear quadratic regulators (LQR) in a multi-task, heterogeneous, and model-free setting. We characterize the stability and personalization guarantees of a policy gradient-based (PG) model-agnostic meta-learning (MAML) (Finn et al., 2017) approach for the LQR problem under different task-heterogeneity settings. We show that our MAML-LQR algorithm produces a stabilizing controller close to each task-specific optimal controller up to a task-heterogeneity bias in both model-based and model-free learning scenarios. Moreover, in the model-based setting, we show that such a controller is achieved with a linear convergence rate, which improves upon sub-linear rates from existing work. Our theoretical guarantees demonstrate that the learned controller can efficiently adapt to unseen LQR tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.14534v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leonardo F. Toso, Donglin Zhan, James Anderson, Han Wang</dc:creator>
    </item>
    <item>
      <title>Stein Boltzmann Sampling: A Variational Approach for Global Optimization</title>
      <link>https://arxiv.org/abs/2402.04689</link>
      <description>arXiv:2402.04689v5 Announce Type: replace 
Abstract: In this paper, we present a flow-based method for global optimization of continuous Sobolev functions, called Stein Boltzmann Sampling (SBS). SBS initializes uniformly a number of particles representing candidate solutions, then uses the Stein Variational Gradient Descent (SVGD) algorithm to sequentially and deterministically move those particles in order to approximate a target distribution whose mass is concentrated around promising areas of the domain of the optimized function. The target is chosen to be a properly parametrized Boltzmann distribution. For the purpose of global optimization, we adapt the generic SVGD theoretical framework allowing to address more general target distributions over a compact subset of $\mathbb{R}^d$, and we prove SBS's asymptotic convergence. In addition to the main SBS algorithm, we present two variants: the SBS-PF that includes a particle filtering strategy, and the SBS-HYBRID one that uses SBS or SBS-PF as a continuation after other particle- or distribution-based optimization methods. A detailed comparison with state-of-the-art methods on benchmark functions demonstrates that SBS and its variants are highly competitive, while the combination of the two variants provides the best trade-off between accuracy and computational cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04689v5</guid>
      <category>math.OC</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ga\"etan Serr\'e (CB), Argyris Kalogeratos (CB), Nicolas Vayatis (CB)</dc:creator>
    </item>
    <item>
      <title>Relationship between General MP and DPP for the Stochastic Recursive Optimal Control Problem With Jumps: Viscosity Solution Framework</title>
      <link>https://arxiv.org/abs/2403.09044</link>
      <description>arXiv:2403.09044v3 Announce Type: replace 
Abstract: This paper is concerned with the relationship between general maximum principle and dynamic programming principle for the stochastic recursive optimal control problem with jumps, where the control domain is not necessarily convex. Relations among the adjoint processes, the generalized Hamiltonian function and the value function are proved, under the assumption of a smooth value function and within the framework of viscosity solutions, respectively. Some examples are given to illustrate the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09044v3</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Bin Wang, Jingtao Shi</dc:creator>
    </item>
    <item>
      <title>Existence of Optimal Stationary Singular Controls and Mean Field Game Equilibria</title>
      <link>https://arxiv.org/abs/2404.07945</link>
      <description>arXiv:2404.07945v2 Announce Type: replace 
Abstract: In this paper, we examine the stationary relaxed singular control problem within a multi-dimensional framework for a single agent, as well as its Mean Field Game (MFG) equivalent. We demonstrate that optimal relaxed controls exist for both maximization and minimization cases. These relaxed controls are defined by random measures across the state and control spaces, with the state process described as a solution to the associated martingale problem. By leveraging findings from [Kurtz-Stockbridge 2001], we establish the equivalence between the martingale problem and the stationary forward equation. This allows us to reformulate the relaxed control problem into a linear programming problem within the measure space. We prove the sequential compactness of these measures, thereby confirming the feasibility of achieving an optimal solution. Subsequently, our focus shifts to Mean Field Games. Drawing on insights from the single-agent problem and employing Kakutani--Glicksberg--Fan fixed point theorem, we derive the existence of a mean field game equilibria.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07945v2</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Asaf Cohen, Chuhao Sun</dc:creator>
    </item>
    <item>
      <title>Optimal Pricing for Linear-Quadratic Games with Nonlinear Interaction Between Agents</title>
      <link>https://arxiv.org/abs/2405.01047</link>
      <description>arXiv:2405.01047v2 Announce Type: replace 
Abstract: This paper studies a class of network games with linear-quadratic payoffs and externalities exerted through a strictly concave interaction function. This class of game is motivated by the diminishing marginal effects with peer influences. We analyze the optimal pricing strategy for this class of network game. First, we prove the existence of a unique Nash Equilibrium (NE). Second, we study the optimal pricing strategy of a monopolist selling a divisible good to agents. We show that the optimal pricing strategy, found by solving a bilevel optimization problem, is strictly better when the monopolist knows the network structure as opposed to the best strategy agnostic to network structure. Numerical experiments demonstrate that in most cases, the maximum revenue is achieved with an asymmetric network. These results contrast with the previously studied case of linear interaction function, where a network-independent price is proven optimal with symmetric networks. Lastly, we describe an efficient algorithm to find the optimal pricing strategy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01047v2</guid>
      <category>math.OC</category>
      <category>cs.GT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiamin Cai, Chenyue Zhang, Hoi-To Wai</dc:creator>
    </item>
    <item>
      <title>Higher Degree Inexact Model for Optimization problems</title>
      <link>https://arxiv.org/abs/2405.16140</link>
      <description>arXiv:2405.16140v2 Announce Type: replace 
Abstract: In this paper, it was proposed a new concept of the inexact higher degree $(\delta, L, q)$-model of a function that is a generalization of the inexact $(\delta, L)$-model, $(\delta, L)$-oracle and $(\delta, L)$-oracle of degree $q \in [0,2)$. Some examples were provided to illustrate the proposed new model. Adaptive inexact gradient and fast gradient methods for convex and strongly convex functions were constructed and analyzed using the new proposed inexact model. A universal fast gradient method that allows solving optimization problems with a weaker level of smoothness, among them non-smooth problems was proposed. For convex optimization problems it was proved that the proposed gradient and fast gradient methods could be converged with rates $O\left(\frac{1}{k} + \frac{\delta}{k^{q/2}}\right)$ and $O\left(\frac{1}{k^2} + \frac{\delta}{k^{(3q-2)/2}}\right)$, respectively. For the gradient method, the coefficient of $\delta$ diminishes with $k$, and for the fast gradient method, there is no error accumulation for $q \geq 2/3$. It proposed a definition of an inexact higher degree oracle for strongly convex functions and a projected gradient method using this inexact oracle. For variational inequalities and saddle point problems, a higher degree inexact model and an adaptive method called Generalized Mirror Prox to solve such class of problems using the proposed inexact model were proposed. Some numerical experiments were conducted to demonstrate the effectiveness of the proposed inexact model, we test the universal fast gradient method to solve some non-smooth problems with a geometrical nature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16140v2</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Alkousa, Fedor Stonyakin, Alexander Gasnikov, Asmaa Abdo, Mohammad Alcheikh</dc:creator>
    </item>
    <item>
      <title>MIP-DD: A Delta Debugger for Mixed Integer Programming Solvers</title>
      <link>https://arxiv.org/abs/2405.19770</link>
      <description>arXiv:2405.19770v2 Announce Type: replace 
Abstract: The recent performance improvements in mixed-integer programming (MIP) went along with a significantly increased complexity of the codes of MIP solvers, which poses challenges in fixing implementation errors. In this paper, we introduce MIP-DD, a solver-independent tool, which is, to the best of our knowledge, the first open-source delta debugger for MIP. Delta debugging is a hypothesis-trial-result approach to isolate the cause of a solver failure. MIP-DD simplifies MIP instances while maintaining the undesired behavior and already supported and motivated fixes for many bugs in the SCIP releases 8.0.4, 8.1.0, and 9.0.0. This translates to an increase of approximately 71.% more bugfixes than in the same time period before and including some fixes of long-known issues. As we highlight in selected case studies, instances triggering fundamental bugs in SCIP can typically be reduced to a few variables and constraints in less than an hour. This makes it significantly easier to manually trace and check the solution process on the resulting simplified instances. A promising future application of MIP-DD is the analysis of performance bottlenecks, which could very well benefit from simple adversarial instances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19770v2</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Hoen, Dominik Kamp, Ambros Gleixner</dc:creator>
    </item>
    <item>
      <title>Decision Machines: An Extension of Decision Trees</title>
      <link>https://arxiv.org/abs/2101.11347</link>
      <description>arXiv:2101.11347v5 Announce Type: replace-cross 
Abstract: Here is a compact representation of binary decision trees. We can explicitly draw the dependencies between prediction and binary tests in decision trees and construct a procedure to guide the input instance from the root to its exit leaf. And we provided a connection between decision trees and error-correcting output codes. Then we built a bridge from tree-based models to attention mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2101.11347v5</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jinxiong Zhang</dc:creator>
    </item>
    <item>
      <title>Interpreting and Improving Diffusion Models from an Optimization Perspective</title>
      <link>https://arxiv.org/abs/2306.04848</link>
      <description>arXiv:2306.04848v4 Announce Type: replace-cross 
Abstract: Denoising is intuitively related to projection. Indeed, under the manifold hypothesis, adding random noise is approximately equivalent to orthogonal perturbation. Hence, learning to denoise is approximately learning to project. In this paper, we use this observation to interpret denoising diffusion models as approximate gradient descent applied to the Euclidean distance function. We then provide straight-forward convergence analysis of the DDIM sampler under simple assumptions on the projection error of the denoiser. Finally, we propose a new gradient-estimation sampler, generalizing DDIM using insights from our theoretical results. In as few as 5-10 function evaluations, our sampler achieves state-of-the-art FID scores on pretrained CIFAR-10 and CelebA models and can generate high quality samples on latent diffusion models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.04848v4</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Frank Permenter, Chenyang Yuan</dc:creator>
    </item>
    <item>
      <title>On the Communication Complexity of Decentralized Bilevel Optimization</title>
      <link>https://arxiv.org/abs/2311.11342</link>
      <description>arXiv:2311.11342v4 Announce Type: replace-cross 
Abstract: Stochastic bilevel optimization finds widespread applications in machine learning, including meta-learning, hyperparameter optimization, and neural architecture search. To extend stochastic bilevel optimization to distributed data, several decentralized stochastic bilevel optimization algorithms have been developed. However, existing methods often suffer from slow convergence rates and high communication costs in heterogeneous settings, limiting their applicability to real-world tasks. To address these issues, we propose two novel decentralized stochastic bilevel gradient descent algorithms based on simultaneous and alternating update strategies. Our algorithms can achieve faster convergence rates and lower communication costs than existing methods. Importantly, our convergence analyses do not rely on strong assumptions regarding heterogeneity. More importantly, our theoretical analysis clearly discloses how the additional communication required for estimating hypergradient under the heterogeneous setting affects the convergence rate. To the best of our knowledge, this is the first time such favorable theoretical results have been achieved with mild assumptions in the heterogeneous setting. Furthermore, we demonstrate how to establish the convergence rate for the alternating update strategy when combined with the variance-reduced gradient. Finally, experimental results confirm the efficacy of our algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.11342v4</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yihan Zhang, My T. Thai, Jie Wu, Hongchang Gao</dc:creator>
    </item>
    <item>
      <title>Sample Efficient Reinforcement Learning with Partial Dynamics Knowledge</title>
      <link>https://arxiv.org/abs/2312.12558</link>
      <description>arXiv:2312.12558v3 Announce Type: replace-cross 
Abstract: The problem of sample complexity of online reinforcement learning is often studied in the literature without taking into account any partial knowledge about the system dynamics that could potentially accelerate the learning process. In this paper, we study the sample complexity of online Q-learning methods when some prior knowledge about the dynamics is available or can be learned efficiently. We focus on systems that evolve according to an additive disturbance model of the form $S_{h+1} = f(S_h, A_h) + W_h$, where $f$ represents the underlying system dynamics, and $W_h$ are unknown disturbances independent of states and actions. In the setting of finite episodic Markov decision processes with $S$ states, $A$ actions, and episode length $H$, we present an optimistic Q-learning algorithm that achieves $\tilde{\mathcal{O}}(\text{Poly}(H)\sqrt{T})$ regret under perfect knowledge of $f$, where $T$ is the total number of interactions with the system. This is in contrast to the typical $\tilde{\mathcal{O}}(\text{Poly}(H)\sqrt{SAT})$ regret for existing Q-learning methods. Further, if only a noisy estimate $\hat{f}$ of $f$ is available, our method can learn an approximately optimal policy in a number of samples that is independent of the cardinalities of state and action spaces. The sub-optimality gap depends on the approximation error $\hat{f}-f$, as well as the Lipschitz constant of the corresponding optimal value function. Our approach does not require modeling of the transition probabilities and enjoys the same memory complexity as model-free methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.12558v3</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1609/aaai.v38i10.28953</arxiv:DOI>
      <dc:creator>Meshal Alharbi, Mardavij Roozbehani, Munther Dahleh</dc:creator>
    </item>
    <item>
      <title>A Simulation Preorder for Koopman-like Lifted Control Systems</title>
      <link>https://arxiv.org/abs/2401.14909</link>
      <description>arXiv:2401.14909v2 Announce Type: replace-cross 
Abstract: This paper introduces a simulation preorder among lifted systems, a generalization of finite-dimensional Koopman approximations (also known as approximate immersions) to systems with inputs. It is proved that this simulation relation implies the containment of both the open- and closed-loop behaviors. Optimization-based sufficient conditions are derived to verify the simulation relation in two special cases: i) a nonlinear (unlifted) system and an affine lifted system and, ii) two affine lifted systems. Numerical examples demonstrate the approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.14909v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antoine Aspeel, Necmiye Ozay</dc:creator>
    </item>
    <item>
      <title>Online Resource Allocation with Non-Stationary Customers</title>
      <link>https://arxiv.org/abs/2401.16945</link>
      <description>arXiv:2401.16945v2 Announce Type: replace-cross 
Abstract: We propose a novel algorithm for online resource allocation with non-stationary customer arrivals and unknown click-through rates. We assume multiple types of customers arrive in a nonstationary stochastic fashion, with unknown arrival rates in each period, and that customers' click-through rates are unknown and can only be learned online. By leveraging results from the stochastic contextual bandit with knapsack and online matching with adversarial arrivals, we develop an online scheme to allocate the resources to nonstationary customers. We prove that under mild conditions, our scheme achieves a ``best-of-both-world'' result: the scheme has a sublinear regret when the customer arrivals are near-stationary, and enjoys an optimal competitive ratio under general (non-stationary) customer arrival distributions. Finally, we conduct extensive numerical experiments to show our approach generates near-optimal revenues for all different customer scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.16945v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoyue Zhang, Hanzhang Qin, Mabel C. Chou</dc:creator>
    </item>
    <item>
      <title>Principled Penalty-based Methods for Bilevel Reinforcement Learning and RLHF</title>
      <link>https://arxiv.org/abs/2402.06886</link>
      <description>arXiv:2402.06886v3 Announce Type: replace-cross 
Abstract: Bilevel optimization has been recently applied to many machine learning tasks. However, their applications have been restricted to the supervised learning setting, where static objective functions with benign structures are considered. But bilevel problems such as incentive design, inverse reinforcement learning (RL), and RL from human feedback (RLHF) are often modeled as dynamic objective functions that go beyond the simple static objective structures, which pose significant challenges of using existing bilevel solutions. To tackle this new class of bilevel problems, we introduce the first principled algorithmic framework for solving bilevel RL problems through the lens of penalty formulation. We provide theoretical studies of the problem landscape and its penalty-based (policy) gradient algorithms. We demonstrate the effectiveness of our algorithms via simulations in the Stackelberg Markov game, RL from human feedback and incentive design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.06886v3</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Han Shen, Zhuoran Yang, Tianyi Chen</dc:creator>
    </item>
    <item>
      <title>Loss Symmetry and Noise Equilibrium of Stochastic Gradient Descent</title>
      <link>https://arxiv.org/abs/2402.07193</link>
      <description>arXiv:2402.07193v2 Announce Type: replace-cross 
Abstract: Symmetries exist abundantly in the loss function of neural networks. We characterize the learning dynamics of stochastic gradient descent (SGD) when exponential symmetries, a broad subclass of continuous symmetries, exist in the loss function. We establish that when gradient noises do not balance, SGD has the tendency to move the model parameters toward a point where noises from different directions are balanced. Here, a special type of fixed point in the constant directions of the loss function emerges as a candidate for solutions for SGD. As the main theoretical result, we prove that every parameter $\theta$ connects without loss function barrier to a unique noise-balanced fixed point $\theta^*$. The theory implies that the balancing of gradient noise can serve as a novel alternative mechanism for relevant phenomena such as progressive sharpening and flattening and can be applied to understand common practical problems such as representation normalization, matrix factorization, warmup, and formation of latent representations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07193v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liu Ziyin, Mingze Wang, Hongchao Li, Lei Wu</dc:creator>
    </item>
    <item>
      <title>Achieving $\tilde{O}(1/\epsilon)$ Sample Complexity for Constrained Markov Decision Process</title>
      <link>https://arxiv.org/abs/2402.16324</link>
      <description>arXiv:2402.16324v2 Announce Type: replace-cross 
Abstract: We consider the reinforcement learning problem for the constrained Markov decision process (CMDP), which plays a central role in satisfying safety or resource constraints in sequential learning and decision-making. In this problem, we are given finite resources and a MDP with unknown transition probabilities. At each stage, we take an action, collecting a reward and consuming some resources, all assumed to be unknown and need to be learned over time. In this work, we take the first step towards deriving optimal problem-dependent guarantees for the CMDP problems. We derive a logarithmic regret bound, which translates into a $O(\frac{1}{\Delta\cdot\eps}\cdot\log^2(1/\eps))$ sample complexity bound, with $\Delta$ being a problem-dependent parameter, yet independent of $\eps$. Our sample complexity bound improves upon the state-of-art $O(1/\eps^2)$ sample complexity for CMDP problems established in the previous literature, in terms of the dependency on $\eps$. To achieve this advance, we develop a new framework for analyzing CMDP problems. To be specific, our algorithm operates in the primal space and we resolve the primal LP for the CMDP problem at each period in an online manner, with \textit{adaptive} remaining resource capacities. The key elements of our algorithm are: i) a characterization of the instance hardness via LP basis, ii) an eliminating procedure that identifies one optimal basis of the primal LP, and; iii) a resolving procedure that is adaptive to the remaining resources and sticks to the characterized optimal basis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.16324v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiashuo Jiang, Yinyu Ye</dc:creator>
    </item>
    <item>
      <title>Effect of constraint relaxation on dynamic critical phenomena in minimum vertex cover problem</title>
      <link>https://arxiv.org/abs/2404.02564</link>
      <description>arXiv:2404.02564v3 Announce Type: replace-cross 
Abstract: The effects of constraint relaxation on dynamic critical phenomena in the Minimum Vertex Cover (MVC) problem on Erd\H{o}s-R\'enyi random graphs are investigated using Markov chain Monte Carlo simulations. Following our previous work that revealed the reduction of the critical temperature by constraint relaxation based on the penalty function method, this study focuses on investigating the critical properties of the relaxation time along its phase boundary. It is found that the dynamical correlation function of MVC with respect to the problem size and the constraint strength follows a universal scaling function. The analysis shows that the relaxation time decreases as the constraints are relaxed. This decrease is more pronounced for the critical amplitude than for the critical exponent, and this result is interpreted in terms of the system's microscopic energy barriers due to the constraint relaxation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02564v3</guid>
      <category>cond-mat.stat-mech</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1209/0295-5075/ad5102</arxiv:DOI>
      <dc:creator>Aki Dote, Koji Hukushima</dc:creator>
    </item>
    <item>
      <title>Optimal single threshold stopping rules and sharp prophet inequalities</title>
      <link>https://arxiv.org/abs/2404.12949</link>
      <description>arXiv:2404.12949v2 Announce Type: replace-cross 
Abstract: This paper considers a finite horizon optimal stopping problem for a sequence of independent and identically distributed random variables. The objective is to design stopping rules that attempt to select the random variable with the highest value in the sequence. The performance of any stopping rule may be benchmarked relative to the selection of a ``prophet" that has perfect foreknowledge of the largest value. Such comparisons are typically stated in the form of "prophet inequalities." In this paper we characterize sharp prophet inequalities for single threshold stopping rules as solutions to infinite two person zero sum games on the unit square with special payoff kernels. The proposed game theoretic characterization allows one to derive sharp non-asymptotic prophet inequalities for different classes of distributions. This, in turn, gives rise to a simple and computationally tractable algorithmic paradigm for deriving optimal single threshold stopping rules.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12949v2</guid>
      <category>math.PR</category>
      <category>cs.GT</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Goldenshluger, Yaakov Malinovsky, Assaf Zeevi</dc:creator>
    </item>
    <item>
      <title>Parallel and Proximal Constrained Linear-Quadratic Methods for Real-Time Nonlinear MPC</title>
      <link>https://arxiv.org/abs/2405.09197</link>
      <description>arXiv:2405.09197v2 Announce Type: replace-cross 
Abstract: Recent strides in nonlinear model predictive control (NMPC) underscore a dependence on numerical advancements to efficiently and accurately solve large-scale problems. Given the substantial number of variables characterizing typical whole-body optimal control (OC) problems - often numbering in the thousands - exploiting the sparse structure of the numerical problem becomes crucial to meet computational demands, typically in the range of a few milliseconds. Addressing the linear-quadratic regulator (LQR) problem is a fundamental building block for computing Newton or Sequential Quadratic Programming (SQP) steps in direct optimal control methods. This paper concentrates on equality-constrained problems featuring implicit system dynamics and dual regularization, a characteristic of advanced interiorpoint or augmented Lagrangian solvers. Here, we introduce a parallel algorithm for solving an LQR problem with dual regularization. Leveraging a rewriting of the LQR recursion through block elimination, we first enhanced the efficiency of the serial algorithm and then subsequently generalized it to handle parametric problems. This extension enables us to split decision variables and solve multiple subproblems concurrently. Our algorithm is implemented in our nonlinear numerical optimal control library ALIGATOR. It showcases improved performance over previous serial formulations and we validate its efficacy by deploying it in the model predictive control of a real quadruped robot.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09197v2</guid>
      <category>cs.RO</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wilson Jallet (LAAS-GEPETTO, WILLOW), Ewen Dantec (WILLOW), Etienne Arlaud (WILLOW), Justin Carpentier (WILLOW, DI-ENS), Nicolas Mansard (LAAS-GEPETTO)</dc:creator>
    </item>
  </channel>
</rss>
