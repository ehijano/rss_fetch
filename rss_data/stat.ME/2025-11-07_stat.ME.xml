<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 07 Nov 2025 05:01:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Adaptive Geometric Regression for High-Dimensional Structured Data</title>
      <link>https://arxiv.org/abs/2511.03817</link>
      <description>arXiv:2511.03817v1 Announce Type: new 
Abstract: We present a geometric framework for regression on structured high-dimensional
  data that shifts the analysis from the ambient space to a geometric object
  capturing the data's intrinsic structure. The method addresses a fundamental
  challenge in analyzing datasets with high ambient dimension but low intrinsic
  dimension, such as microbiome compositions, where traditional approaches fail
  to capture the underlying geometric structure. Starting from a k-nearest
  neighbor covering of the feature space, the geometry evolves iteratively
  through heat diffusion and response-coherence modulation, concentrating mass
  within regions where the response varies smoothly while creating diffusion
  barriers where the response changes rapidly. This iterative refinement
  produces conditional expectation estimates that respect both the intrinsic
  geometry of the feature space and the structure of the response.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03817v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pawel Gajer, Jacques Ravel</dc:creator>
    </item>
    <item>
      <title>A Pragmatic Framework for Bayesian Utility Magnitude-Based Decisions</title>
      <link>https://arxiv.org/abs/2511.03932</link>
      <description>arXiv:2511.03932v1 Announce Type: new 
Abstract: This article presents a pragmatic framework for making formal, utility-based decisions from statistical inferences. The method calculates an expected utility score for an intervention by combining Bayesian posterior probabilities of different effect magnitudes with points representing their practical value. A key innovation is a unified, non-arbitrary points scale (1-9 for small to extremely large) derived from a principle linking tangible outcomes across different effect types. This tangible scale enables a principled "trade-off" method for including values for loss aversion, side effects, and implementation cost. The framework produces a single, definitive expected utility score, and the initial decision is made by comparing the magnitude of this single score to a user-defined smallest important net benefit, a direct and intuitive comparison made possible by the scale's tangible nature. This expected utility decision is interpreted alongside clinical magnitude-based decision probabilities or credible interval coverage to assess evidence strength. Inclusion of a standard deviation representing individual responses to an intervention (or differences between settings with meta-analytic data) allows characterization of differences between individuals (or settings) in the utility score expressed as proportions expected to experience benefit, a negligible effect, and harm. These proportions provide context for the final decision about implementation. Users must perform sensitivity analyses to investigate the effects of systematic bias and of the subjective inputs on the final decision. This framework, implemented in an accessible spreadsheet, has not been empirically validated. It represents a tool in development, designed for practical decision-making from available statistical evidence and structured thinking about values of outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03932v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Will G. Hopkins</dc:creator>
    </item>
    <item>
      <title>Nonparametric Modeling of Continuous-Time Markov Chains</title>
      <link>https://arxiv.org/abs/2511.03954</link>
      <description>arXiv:2511.03954v1 Announce Type: new 
Abstract: Inferring the infinitesimal rates of continuous-time Markov chains (CTMCs) is a central challenge in many scientific domains. This task is hindered by three factors: quadratic growth in the number of rates as the CTMC state space expands, strong dependencies among rates, and incomplete information for many transitions. We introduce a new Bayesian framework that flexibly models the CTMC rates by incorporating covariates through Gaussian processes (GPs). This approach improves inference by integrating new information and contributes to the understanding of the CTMC stochastic behavior by shedding light on potential external drivers. Unlike previous approaches limited to linear covariate effects, our method captures complex non-linear relationships, enabling fuller use of covariate information and more accurate characterization of their influence. To perform efficient inference, we employ a scalable Hamiltonian Monte Carlo (HMC) sampler. We address the prohibitive cost of computing the exact likelihood gradient by integrating the HMC trajectories with a scalable gradient approximation, reducing the computational complexity from $O(K^5)$ to $O(K^2)$, where $K$ is the number of CTMC states. Finally, we demonstrate our method on Bayesian phylogeography inference -- a domain where CTMCs are central -- showing effectiveness on both synthetic and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03954v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Filippo Monti, Xiang Ji, Marc A. Suchard</dc:creator>
    </item>
    <item>
      <title>Assessing Replicability Across Dependent Studies: A Framework for Testing Partial Conjunction Hypotheses with Application to GWAS</title>
      <link>https://arxiv.org/abs/2511.04130</link>
      <description>arXiv:2511.04130v1 Announce Type: new 
Abstract: Replicability is central to scientific progress, and the partial conjunction (PC) hypothesis testing framework provides an objective tool to quantify it across disciplines. Existing PC methods assume independent studies. Yet many modern applications, such as genome-wide association studies (GWAS) with sample overlap, violate this assumption, leading to dependence among study-specific summary statistics. Failure to account for this dependence can drastically inflate type I errors when combining inferences. We propose e-Filter, a powerful procedure grounded on the theory of e-values. It involves a filtering step that retains a set of the most promising PC hypotheses, and a selection step where PC hypotheses from the filtering step are marked as discoveries whenever their e-values exceed a selection threshold. We establish the validity of e-Filter for FWER and FDR control under unknown study dependence. A comprehensive simulation study demonstrates its excellent power gains over competing methods. We apply e-Filter to a GWAS replicability study to identify consistent genetic signals for low-density lipoprotein cholesterol (LDL-C). Here, the participating studies exhibit varying levels of sample overlap, rendering existing methods unsuitable for combining inferences. A subsequent pathway enrichment analysis shows that e-Filter replicated signals achieve stronger statistical enrichment on biologically relevant LDL-C pathways than competing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04130v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Monitirtha Dey, Trambak Banerjee, Prajamitra Bhuyan, Arunabha Majumdar</dc:creator>
    </item>
    <item>
      <title>Estimation of Independent Component Analysis Systems</title>
      <link>https://arxiv.org/abs/2511.04273</link>
      <description>arXiv:2511.04273v1 Announce Type: new 
Abstract: Although approaches to Independent Component Analysis (ICA) based on characteristic function seem theoretically elegant, they may suffer from implementational challenges because of numerical integration steps or selection of tuning parameters. Extending previously considered objective functions and leveraging results from the continuum Generalized Method of Moments of Carrasco and Florens (2000), I derive an optimal estimator that can take a tractable form and thus bypass these concerns. The method shares advantages with characteristic function approaches -- it does not require the existence of higher-order moments or parametric restrictions -- while retaining computational feasibility and asymptotic efficiency. The results are adapted to handle a possible first step that delivers estimated sensors. Finally, a by-product of the approach is a specification test that is valuable in many ICA applications. The method's effectiveness is illustrated through simulations, where the estimator outperforms efficient GMM, JADE, or FastICA, and an application to the estimation of Structural Vector Autoregressions (SVAR), a workhorse of the macroeconometric time series literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04273v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vincent Starck</dc:creator>
    </item>
    <item>
      <title>Matrix-Variate Regression Model for Multivariate Spatio-Temporal Data</title>
      <link>https://arxiv.org/abs/2511.04331</link>
      <description>arXiv:2511.04331v1 Announce Type: new 
Abstract: This paper introduces a matrix-variate regression model for analyzing multivariate data observed across spatial locations and over time. The model's design incorporates a mean structure that links covariates to the response matrix and a separable covariance structure, based on a Kronecker product, to capture spatial and temporal dependencies efficiently. We derive maximum likelihood estimators for all model parameters. A simulation study validates the model, showing its effectiveness in parameter recovery across different spatial resolutions. Finally, an application to real-world data on agricultural and livestock production from Brazilian municipalities showcases the model's practical utility in revealing structured spatio-temporal patterns of variation and covariate effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04331v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Carlos A. Ribeiro Diniz, Victor E. Lachos Olivares, Victor H. Lachos Davila</dc:creator>
    </item>
    <item>
      <title>Nonparametric Robust Comparison of Solutions under Input Uncertainty</title>
      <link>https://arxiv.org/abs/2511.04457</link>
      <description>arXiv:2511.04457v1 Announce Type: new 
Abstract: We study ranking and selection under input uncertainty in settings where additional data cannot be collected. We propose the Nonparametric Input-Output Uncertainty Comparisons (NIOU-C) procedure to construct a confidence set that includes the optimal solution with a user-specified probability. We construct an ambiguity set of input distributions using empirical likelihood and approximate the mean performance of each solution using a linear functional representation of the input distributions. By solving optimization problems evaluating worst-case pairwise mean differences within the ambiguity set, we build a confidence set of solutions indistinguishable from the optimum. We characterize sample size requirements for NIOU-C to achieve the asymptotic validity under mild conditions. Moreover, we propose an extension to NIOU-C, NIOU-C:E, that mitigates conservatism and yields a smaller confidence set. In numerical experiments, NIOU-C provides a smaller confidence set that includes the optimum more frequently than a parametric procedure that takes advantage of the parametric distribution families.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04457v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jaime Gonzalez-Hodar, Johannes Milz, Eunhye Song</dc:creator>
    </item>
    <item>
      <title>Conditional Selective Inference for the Selected Groups in Panel Data</title>
      <link>https://arxiv.org/abs/2511.04466</link>
      <description>arXiv:2511.04466v1 Announce Type: new 
Abstract: We consider the problem of testing for differences in group-specific slopes between the selected groups in panel data identified via k-means clustering. In this setting, the classical Wald-type test statistic is problematic because it produces an extremely inflated type I error probability. The underlying reason is that the same dataset is used to identify the group structure and construct the test statistic, simultaneously. This creates dependence between the selection and inference stages. To address this issue, we propose a valid selective inference approach conditional on the selection event to account for the selection effect. We formally define the selective type I error and describe how to efficiently compute the correct p-values for clusters obtained using k-means clustering. Furthermore, the same idea can be extended to test for differences in coefficients due to a single covariate and can be incorporated into the GMM estimation framework. Simulation studies show that our method has satisfactory finite sample performance. We apply this method to explore the heterogeneous relationships between economic growth and the $CO_2$ emission across countries for which some new findings are discovered. An R package TestHomoPanel is provided to implement the proposed selective inference framework for panel data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04466v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chuang Wan, Jiajun Sun, Xingbai Xu</dc:creator>
    </item>
    <item>
      <title>A General Approach for Calibration Weighting under Missing at Random</title>
      <link>https://arxiv.org/abs/2511.04496</link>
      <description>arXiv:2511.04496v1 Announce Type: new 
Abstract: We propose a unified class of calibration weighting methods based on weighted generalized entropy to handle missing at random (MAR) data with improved stability and efficiency. The proposed generalized entropy calibration (GEC) formulates weight construction as a convex optimization program that unifies entropy-based approaches and generalized regression weighting. Double robustness is achieved by augmenting standard covariate balancing with a debiasing constraint tied to the propensity score model and a Neyman-orthogonal constraint that removes first-order sensitivity to nuisance estimation. Selection of the weights on the entropy function can lead to the optimal calibration estimator under a correctly specified outcome regression model. The proposed GEC weighting ha a nice geometric characterization: the GEC solution is the Bregman projection of the initial weights onto a constraint set, which yields a generalized Pythagorean identity and a nested decomposition that quantifies the incremental distance paid for additional constraints. We also develop a high-dimensional extension with soft calibration and a projection calibration constraint that preserves doubly robust inference. Two simulation studies are presented to compare the performance of the proposed method with the existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04496v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yonghyun Kwon, Jae Kwang Kim, Yumou Qiu</dc:creator>
    </item>
    <item>
      <title>Generative Bayesian Filtering and Parameter Learning</title>
      <link>https://arxiv.org/abs/2511.04552</link>
      <description>arXiv:2511.04552v1 Announce Type: new 
Abstract: Generative Bayesian Filtering (GBF) provides a powerful and flexible framework for performing posterior inference in complex nonlinear and non-Gaussian state-space models. Our approach extends Generative Bayesian Computation (GBC) to dynamic settings, enabling recursive posterior inference using simulation-based methods powered by deep neural networks. GBF does not require explicit density evaluations, making it particularly effective when observation or transition distributions are analytically intractable. To address parameter learning, we introduce the Generative-Gibbs sampler, which bypasses explicit density evaluation by iteratively sampling each variable from its implicit full conditional distribution. Such technique is broadly applicable and enables inference in hierarchical Bayesian models with intractable densities, including state-space models. We assess the performance of the proposed methodologies through both simulated and empirical studies, including the estimation of $\alpha$-stable stochastic volatility models. Our findings indicate that GBF significantly outperforms existing likelihood-free approaches in accuracy and robustness when dealing with intractable state-space models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04552v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edoardo Marcelli, Sean O'Hagan, Veronika Rockova</dc:creator>
    </item>
    <item>
      <title>Geometric Decomposition of Statistical Inference through Gradient Flow and Co-Monotonicity Measures</title>
      <link>https://arxiv.org/abs/2511.04599</link>
      <description>arXiv:2511.04599v1 Announce Type: new 
Abstract: Understanding feature-outcome associations in high-dimensional data remains
  challenging when relationships vary across subpopulations, yet standard
  methods assuming global associations miss context-dependent patterns, reducing
  statistical power and interpretability. We develop a geometric decomposition
  framework offering two strategies for partitioning inference problems into
  regional analyses on data-derived Riemannian graphs. Gradient flow
  decomposition uses path-monotonicity-validated discrete Morse theory to
  partition samples into basins where outcomes exhibit monotonic behavior.
  Co-monotonicity decomposition leverages association structure: vertex-level
  coefficients measuring directional concordance between outcome and features,
  or between feature pairs, define embeddings of samples into association space.
  These embeddings induce Riemannian k-NN graphs on which biclustering
  identifies co-monotonicity cells (coherent regions) and feature modules. This
  extends naturally to multi-modal integration across multiple feature sets.
  Both strategies apply independently or jointly, with Bayesian posterior
  sampling providing credible intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04599v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pawel Gajer, Jacques Ravel</dc:creator>
    </item>
    <item>
      <title>Where to Experiment? Site Selection Under Distribution Shift via Optimal Transport and Wasserstein DRO</title>
      <link>https://arxiv.org/abs/2511.04658</link>
      <description>arXiv:2511.04658v1 Announce Type: new 
Abstract: How should researchers select experimental sites when the deployment population differs from observed data? I formulate the problem of experimental site selection as an optimal transport problem, developing methods to minimize downstream estimation error by choosing sites that minimize the Wasserstein distance between population and sample covariate distributions. I develop new theoretical upper bounds on PATE and CATE estimation errors, and show that these different objectives lead to different site selection strategies. I extend this approach by using Wasserstein Distributionally Robust Optimization to develop a site selection procedure robust to adversarial perturbations of covariate information: a specific model of distribution shift. I also propose a novel data-driven procedure for selecting the uncertainty radius the Wasserstein DRO problem, which allows the user to benchmark robustness levels against observed variation in their data. Simulation evidence, and a reanalysis of a randomized microcredit experiment in Morocco (Cr\'epon et al.), show that these methods outperform random and stratified sampling of sites when covariates have prognostic R-squared &gt; .5, and alternative optimization methods i) for moderate-to-large size problem instances ii) when covariates are moderately informative about treatment effects, and iii) under induced distribution shift.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04658v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adam Bouyamourn</dc:creator>
    </item>
    <item>
      <title>Conditional Score Learning for Quickest Change Detection in Markov Transition Kernels</title>
      <link>https://arxiv.org/abs/2511.03953</link>
      <description>arXiv:2511.03953v1 Announce Type: cross 
Abstract: We address the problem of quickest change detection in Markov processes with unknown transition kernels. The key idea is to learn the conditional score $\nabla_{\mathbf{y}} \log p(\mathbf{y}|\mathbf{x})$ directly from sample pairs $( \mathbf{x},\mathbf{y})$, where both $\mathbf{x}$ and $\mathbf{y}$ are high-dimensional data generated by the same transition kernel. In this way, we avoid explicit likelihood evaluation and provide a practical way to learn the transition dynamics. Based on this estimation, we develop a score-based CUSUM procedure that uses conditional Hyvarinen score differences to detect changes in the kernel. To ensure bounded increments, we propose a truncated version of the statistic. With Hoeffding's inequality for uniformly ergodic Markov processes, we prove exponential lower bounds on the mean time to false alarm. We also prove asymptotic upper bounds on detection delay. These results give both theoretical guarantees and practical feasibility for score-based detection in high-dimensional Markov models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03953v1</guid>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wuxia Chen, Taposh Banerjee, Vahid Tarokh</dc:creator>
    </item>
    <item>
      <title>Score-Based Quickest Change Detection and Fault Identification for Multi-Stream Signals</title>
      <link>https://arxiv.org/abs/2511.03967</link>
      <description>arXiv:2511.03967v1 Announce Type: cross 
Abstract: This paper introduces an approach to multi-stream quickest change detection and fault isolation for unnormalized and score-based statistical models. Traditional optimal algorithms in the quickest change detection literature require explicit pre-change and post-change distributions to calculate the likelihood ratio of the observations, which can be computationally expensive for higher-dimensional data and sometimes even infeasible for complex machine learning models. To address these challenges, we propose the min-SCUSUM method, a Hyvarinen score-based algorithm that computes the difference of score functions in place of log-likelihood ratios. We provide a delay and false alarm analysis of the proposed algorithm, showing that its asymptotic performance depends on the Fisher divergence between the pre- and post-change distributions. Furthermore, we establish an upper bound on the probability of fault misidentification in distinguishing the affected stream from the unaffected ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03967v1</guid>
      <category>eess.SP</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wuxia Chen, Sean Moushegian, Vahid Tarokh, Taposh Banerjee</dc:creator>
    </item>
    <item>
      <title>Recursions on the marginals and exact computation of the normalizing constant for Gibbs processes</title>
      <link>https://arxiv.org/abs/2511.04298</link>
      <description>arXiv:2511.04298v1 Announce Type: cross 
Abstract: This paper presents different recursive formulas for computing the marginals and the normalizing constant of a Gibbs distribution $\pi$: The common thread is the use of the underlying Markov properties of such processes. The procedures are illustrated with several examples, particularly the Ising model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04298v1</guid>
      <category>math.PR</category>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>C\'ecile Hardouin, Xavier Guyon</dc:creator>
    </item>
    <item>
      <title>Riesz Regression As Direct Density Ratio Estimation</title>
      <link>https://arxiv.org/abs/2511.04568</link>
      <description>arXiv:2511.04568v1 Announce Type: cross 
Abstract: Riesz regression has garnered attention as a tool in debiased machine learning for causal and structural parameter estimation (Chernozhukov et al., 2021). This study shows that Riesz regression is closely related to direct density-ratio estimation (DRE) in important cases, including average treat- ment effect (ATE) estimation. Specifically, the idea and objective in Riesz regression coincide with the one in least-squares importance fitting (LSIF, Kanamori et al., 2009) in direct density-ratio estimation. While Riesz regression is general in the sense that it can be applied to Riesz representer estimation in a wide class of problems, the equivalence with DRE allows us to directly import exist- ing results in specific cases, including convergence-rate analyses, the selection of loss functions via Bregman-divergence minimization, and regularization techniques for flexible models, such as neural networks. Conversely, insights about the Riesz representer in debiased machine learning broaden the applications of direct density-ratio estimation methods. This paper consolidates our prior results in Kato (2025a) and Kato (2025b).</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04568v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
    <item>
      <title>Knothe-Rosenblatt maps via soft-constrained optimal transport</title>
      <link>https://arxiv.org/abs/2511.04579</link>
      <description>arXiv:2511.04579v1 Announce Type: cross 
Abstract: In the theory of optimal transport, the Knothe-Rosenblatt (KR) rearrangement provides an explicit construction to map between two probability measures by building one-dimensional transformations from the marginal conditionals of one measure to the other. The KR map has shown to be useful in different realms of mathematics and statistics, from proving functional inequalities to designing methodologies for sampling conditional distributions. It is known that the KR rearrangement can be obtained as the limit of a sequence of optimal transport maps with a weighted quadratic cost. We extend these results in this work by showing that one can obtain the KR map as a limit of maps that solve a relaxation of the weighted-cost optimal transport problem with a soft-constraint for the target distribution. In addition, we show that this procedure also applies to the construction of triangular velocity fields via dynamic optimal transport yielding optimal velocity fields. This justifies various variational methodologies for estimating KR maps in practice by minimizing a divergence between the target and pushforward measure through an approximate map. Moreover, it opens the possibilities for novel static and dynamic OT estimators for KR maps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04579v1</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ricardo Baptista, Franca Hoffmann, Minh Van Hoang Nguyen, Benjamin Zhang</dc:creator>
    </item>
    <item>
      <title>A Mixed Model Approach for Estimating Regional Functional Connectivity from Voxel-level BOLD Signals</title>
      <link>https://arxiv.org/abs/2211.02192</link>
      <description>arXiv:2211.02192v2 Announce Type: replace 
Abstract: Resting-state brain functional connectivity quantifies the synchrony between activity patterns of different brain regions. In functional magnetic resonance imaging (fMRI), each region comprises a set of spatially contiguous voxels at which blood-oxygen-level-dependent signals are acquired. The ubiquitous Correlation of Averages (CA) estimator, and other similar metrics, are computed from spatially aggregated signals within each region, and remain the quantifications of inter-regional connectivity most used by neuroscientists despite their bias that stems from intra-regional correlation and measurement error. We leverage the framework of linear mixed-effects models to isolate different sources of variability in the voxel-level signals, including both inter-regional and intra-regional correlation and measurement error. A novel computational pipeline, focused on subject-level inter-regional correlation parameters of interest, is developed to address the challenges of applying maximum (or restricted maximum) likelihood estimation to such structured, high-dimensional spatiotemporal data. Simulation results demonstrate the reliability of correlation estimates and their large sample standard error approximations, and their superiority relative to CA. The proposed method is applied to two public fMRI data sets. First, we analyze scans of a dead rat to assess false positive performance when connectivity is absent. Second, individual human brain networks are constructed for subjects from a Human Connectome Project test-retest database. Concordance between inter-regional correlation estimates for test-retest scans of the same subject are shown to be higher for the proposed method relative to CA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.02192v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruobin Liu, Chao Zhang, Chau Tran, Sophie Achard, Wendy Meiring, Alexander Petersen</dc:creator>
    </item>
    <item>
      <title>Robust and consistent model evaluation criteria in high-dimensional regression</title>
      <link>https://arxiv.org/abs/2407.16116</link>
      <description>arXiv:2407.16116v3 Announce Type: replace 
Abstract: Most of the regularization methods such as the LASSO have one (or more) regularization parameter(s), and to select the value of the regularization parameter is essentially equal to select a model. Thus, to obtain a model suitable for the data and phenomenon, we need to determine an adequate value of the regularization parameter. Regarding the determination of the regularization parameter in the linear regression model, we often apply the information criteria like the AIC and BIC, however, it has been pointed out that these criteria are sensitive to outliers and tend not to perform well in high-dimensional settings. Outliers generally have a negative effect on not only estimation but also model selection, consequently, it is important to employ a selection method with robustness against outliers. In addition, when the number of explanatory variables is quite large, most conventional criteria are prone to select unnecessary explanatory variables. In this paper, we propose model evaluation criteria based on the statistical divergence with excellence in robustness in both of parametric estimation and model selection, by applying the quasi-Bayesian procedure. Our proposed criteria achieve the selection consistency even in high-dimensional settings due to precise approximation, simultaneously with robustness. We also investigate the conditions for establishing robustness and consistency, and provide an appropriate example of the divergence and penalty term that can achieve the desirable properties. We finally report the results of some numerical examples to verify that the proposed criteria perform robust and consistent variable selection compared with the conventional selection methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16116v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>Sumito Kurata and Kei Hirose. 2026. Robust and consistent model evaluation criteria in high-dimensional regression. Journal of Statistical Planning and Inference, 242, 106358</arxiv:journal_reference>
      <dc:creator>Sumito Kurata, Kei Hirose</dc:creator>
    </item>
    <item>
      <title>A comparison between copula-based, mixed model, and estimating equation methods for regression of bivariate correlated data</title>
      <link>https://arxiv.org/abs/2410.11892</link>
      <description>arXiv:2410.11892v3 Announce Type: replace 
Abstract: This paper presents a simulation study comparing the performance of generalized joint regression models (GJRM) with generalized linear mixed models (GLMM) and generalized estimating equations (GEE) for regression of longitudinal data with two measurements per observational unit. We compare models on the basis of overall fit, coefficient accuracy and computational complexity.
  We find that for the normal model with identity link, all models provide accurate estimates of regression coefficients with comparable fit. However, for non-normal marginal distributions and when a non-identity link function is used, we highlight a major pitfall in the use of GLMMs: without significant adjustment they provide highly biased estimates of marginal coefficients and often provide extreme fits. GLMM coefficient bias and relative lack of fit is more pronounced when the marginal distributions are more skewed or highly correlated. In addition, we find major discrepancies between the estimates from different GLMM software implementations. In contrast, we find that GJRM provides unbiased estimates across all distributions with accurate standard errors when the copula is correctly specified; and the GJRM provides a model fit favourable or comparable to GLMMs and GEEs in almost all cases. We also compare the approaches for a real-world longitudinal study of doctor visits.
  We conclude that for non-normal bivariate data, the GJRM provides a superior model with more consistently accurate and interpretable coefficients than the GLMM, and better or comparable fit than both the GLMM and GEE, while providing more flexibility in choice of marginal distributions, and control over correlation structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11892v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aydin Sareff-Hibbert, Gillian Z. Heller</dc:creator>
    </item>
    <item>
      <title>Canonical Correlation Analysis: review</title>
      <link>https://arxiv.org/abs/2411.15625</link>
      <description>arXiv:2411.15625v2 Announce Type: replace 
Abstract: For over a century canonical correlations, variables, and related concepts have been studied across various fields, with contributions dating back to Jordan [1875] and Hotelling [1936]. This text surveys the evolution of canonical correlation analysis, a fundamental statistical tool, beginning with its foundational theorems and progressing to recent developments and open research problems. Along the way we introduce and review methods, notions, and fundamental concepts from linear algebra, random matrix theory, and high-dimensional statistics, placing particular emphasis on rigorous mathematical treatment.
  The survey is intended for technically proficient graduate students and other researchers with an interest in this area. The content is organized into five chapters, supplemented by six sets of exercises found in Chapter 6. These exercises introduce additional material, reinforce key concepts, and serve to bridge ideas across chapters. We recommend the following sequence: first, solve Problem Set 0, then proceed with Chapter 1, solve Problem Set 1, and so on through the text.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15625v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anna Bykhovskaya, Vadim Gorin</dc:creator>
    </item>
    <item>
      <title>Spatiotemporal causal inference with arbitrary spillover and carryover effects: Airstrikes and insurgent violence in the Iraq War</title>
      <link>https://arxiv.org/abs/2504.03464</link>
      <description>arXiv:2504.03464v2 Announce Type: replace 
Abstract: Social scientists now routinely draw on high-frequency, high-granularity ''microlevel'' data to estimate the causal effects of subnational interventions. To date, most researchers aggregate these data into panels, often tied to large-scale administrative units. This approach has two limitations. First, data (over)aggregation obscures valuable spatial and temporal information, heightening the risk of mistaken inferences. Second, existing panel approaches either ignore spatial spillover and temporal carryover effects completely or impose overly restrictive assumptions. We introduce a general methodological framework and an accompanying open-source R package, geocausal, that enable spatiotemporal causal inference with arbitrary spillover and carryover effects. Using this framework, we demonstrate how to define and estimate causal quantities of interest, explore heterogeneous treatment effects, conduct causal mediation analysis, and perform data visualization. We apply our methodology to the Iraq War (2003-11), where we reexamine long-standing questions about the effects of airstrikes on insurgent violence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03464v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mitsuru Mukaigawara, Kosuke Imai, Jason Lyall, Georgia Papadogeorgou</dc:creator>
    </item>
    <item>
      <title>SOMA: A Novel Sampler for Bayesian Inference from Privatized Data</title>
      <link>https://arxiv.org/abs/2505.00635</link>
      <description>arXiv:2505.00635v2 Announce Type: replace 
Abstract: Making valid statistical inferences from privatized data is a key challenge in modern analysis. In Bayesian settings, data augmentation MCMC (DAMCMC) methods impute unobserved confidential data given noisy privatized summaries, enabling principled uncertainty quantification. However, standard DAMCMC often suffers from slow mixing due to component-wise Metropolis-within-Gibbs updates. We propose the Single-Offer-Multiple-Attempts (SOMA) sampler. This novel algorithm improves acceptance rates by generating a single proposal and simultaneously evaluating its suitability to replace all components. By sharing proposals across components, SOMA rejects fewer proposal points. We prove lower bounds on SOMA's acceptance probability and establish convergence rates in the two-component case. Experiments on synthetic and real census data with linear regression and other models confirm SOMA's efficiency gains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00635v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yifei Xiong, Nianqiao Phyllis Ju</dc:creator>
    </item>
    <item>
      <title>Assessing Risk Heterogeneity through Heavy-Tailed Frequency and Severity Mixtures</title>
      <link>https://arxiv.org/abs/2505.04795</link>
      <description>arXiv:2505.04795v3 Announce Type: replace 
Abstract: The analysis of risk typically involves dividing a random damage-generation process into separate frequency (event-count) and severity (damage-magnitude) components. In the present article, we construct canonical families of mixture distributions for each of these components, based on a Negative Binomial kernel for frequencies and a Gamma kernel for severities. These mixtures are employed to assess the heterogeneity of risk factors underlying an empirical distribution through the shape of the implied mixing distribution. From the duality of the Negative Binomial and Gamma distributions, we first derive necessary and sufficient conditions for heavy-tailed (i.e., inverse power-law) canonical mixtures. We then formulate flexible 4-parameter families of mixing distributions for Geometric and Exponential kernels to generate heavy-tailed 4-parameter mixture models, and extend these mixtures to arbitrary Negative Binomial and Gamma kernels, respectively, yielding 5-parameter mixtures for detecting and measuring risk heterogeneity. To check the robustness of such heterogeneity inferences, we show how a fitted 5-parameter model may be re-expressed in terms of alternative Negative Binomial or Gamma kernels whose associated mixing distributions form a "calibrated" family.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04795v3</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael R. Powers, Jiaxin Xu</dc:creator>
    </item>
    <item>
      <title>A mirror descent approach to maximum likelihood estimation in latent variable models</title>
      <link>https://arxiv.org/abs/2501.15896</link>
      <description>arXiv:2501.15896v2 Announce Type: replace-cross 
Abstract: We introduce an approach based on mirror descent and sequential Monte Carlo (SMC) to perform joint parameter inference and posterior estimation in latent variable models. This approach is based on minimisation of a functional over the parameter space and the space of probability distributions and, contrary to other popular approaches, can be implemented when the latent variable takes values in discrete spaces. We provide a detailed theoretical analysis of both the mirror descent algorithm and its approximation via SMC. We experimentally show that the proposed algorithm outperforms standard expectation maximisation algorithms and is competitive with other popular methods for real-valued latent variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15896v2</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesca R. Crucinio</dc:creator>
    </item>
    <item>
      <title>Reinterpreting demand estimation</title>
      <link>https://arxiv.org/abs/2503.23524</link>
      <description>arXiv:2503.23524v3 Announce Type: replace-cross 
Abstract: This paper clarifies how and why structural demand models (Berry and Haile, 2014, 2024) predict unit-level counterfactual outcomes. We do so by casting structural assumptions equivalently as restrictions on the joint distribution of potential outcomes. Our reformulation highlights a counterfactual homogeneity assumption underlying structural demand models: The relationship between counterfactual outcomes is assumed to be identical across markets. This assumption is strong, but cannot be relaxed without sacrificing identification of market-level counterfactuals. Absent this assumption, we can interpret model-based predictions as extrapolations from certain causally identified average treatment effects. This reinterpretation provides a conceptual bridge between structural modeling and causal inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23524v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiafeng Chen</dc:creator>
    </item>
    <item>
      <title>Statistical Properties of Rectified Flow</title>
      <link>https://arxiv.org/abs/2511.03193</link>
      <description>arXiv:2511.03193v2 Announce Type: replace-cross 
Abstract: Rectified flow (Liu et al., 2022; Liu, 2022; Wu et al., 2023) is a method for defining a transport map between two distributions, and enjoys popularity in machine learning, although theoretical results supporting the validity of these methods are scant. The rectified flow can be regarded as an approximation to optimal transport, but in contrast to other transport methods that require optimization over a function space, computing the rectified flow only requires standard statistical tools such as regression or density estimation. Because of this, one can leverage standard data analysis tools for regression and density estimation to develop empirical versions of transport maps. We study some structural properties of the rectified flow, including existence, uniqueness, and regularity, as well as the related statistical properties, such as rates of convergence and central limit theorems, for some selected estimators. To do so, we analyze separately the bounded and unbounded cases as each presents unique challenges. In both cases, we are able to establish convergence at faster rates than the ones for the usual nonparametric regression and density estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03193v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gonzalo Mena, Arun Kumar Kuchibhotla, Larry Wasserman</dc:creator>
    </item>
  </channel>
</rss>
