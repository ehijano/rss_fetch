<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 08 Apr 2025 04:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>MemPool Flavors: Between Versatility and Specialization in a RISC-V Manycore Cluster</title>
      <link>https://arxiv.org/abs/2504.03675</link>
      <description>arXiv:2504.03675v1 Announce Type: new 
Abstract: As computational paradigms evolve, applications such as attention-based models, wireless telecommunications, and computer vision impose increasingly challenging requirements on computer architectures: significant memory footprints and computing resources are demanded while maintaining flexibility and programmability at a low power budget. Thanks to their advantageous trade-offs, shared-L1-memory clusters have become a common building block of massively parallel computing architectures tackling these issues. MemPool is an open-source, RISC-V-based manycore cluster scaling up to 1024 processing elements (PEs). MemPool offers a scalable, extensible, and programmable solution to the challenges of shared-L1 clusters, establishing itself as an open-source research platform for architectural variants covering a wide trade-off space between versatility and performance. As a demonstration, this paper compares the three main MemPool flavors, Baseline MemPool, Systolic MemPool, and Vectorial MemPool, detailing their architecture, targets, and achieved trade-offs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03675v1</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sergio Mazzola, Yichao Zhang, Marco Bertuletti, Diyou Shen, Luca Benini</dc:creator>
    </item>
    <item>
      <title>Fused-Tiled Layers: Minimizing Data Movement on RISC-V SoCs with Software-Managed Caches</title>
      <link>https://arxiv.org/abs/2504.03676</link>
      <description>arXiv:2504.03676v1 Announce Type: new 
Abstract: The success of DNNs and their high computational requirements pushed for large codesign efforts aiming at DNN acceleration. Since DNNs can be represented as static computational graphs, static memory allocation and tiling are two crucial optimizations. Hence, SoCs specialized for DNN acceleration commonly features a multi-level software-managed memory hierarchy. In such architecture, layer-wise tiling, i.e., splitting each layer into multiple sub-nodes, is commonly used; however, while reducing memory occupation, it can increase the total memory transfer, ultimately causing costly off-chip memory copies, which impact energy efficiency and create memory bottlenecks. This work proposes Fused-Tiled Layers, a novel algorithm for automatic fusion between tiled layers. We leverage the flexibility and efficiency of a RISC-V (RV32) heterogeneous SoC to integrate FTL in an open-source deployment framework, which we tune for RISC-V targets. We demonstrate that FTL brings up to 60.1% runtime reduction for a typical MLP stage of ViT due to the reduction of off-chip transfer and on-chip data movement by 47.1%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03676v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Victor J. B. Jung, Alessio Burrello, Francesco Conti, Luca Benini</dc:creator>
    </item>
    <item>
      <title>Work-In-Progress: Accelerating Numpy With OpenBLAS For Open-Source RISC-V Chips</title>
      <link>https://arxiv.org/abs/2504.03677</link>
      <description>arXiv:2504.03677v1 Announce Type: new 
Abstract: RISC-V allows for building general-purpose computing platforms with programmable accelerators around a single open-source ISA. However, leveraging heterogeneous SoCs within high-level applications is a tedious task. In this preliminary work, we modify the OpenBLAS library to offload selected linear kernels to a programmable manycore accelerator (PMCA) using OpenMP. By linking the Python package Numpy against this library, we enable acceleration of high-level applications. We target an open-source heterogeneous System-on-Chip with a rv64g Linux capable host and a rv32imafd PMCA. Using this platform emulated on FPGA, and the presented software stack, we can accelerate Phyton applications with linear algebra operators like matrix multiplication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03677v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cyril Koenig, Enrico Zelioli, Frank K. G\"urkaynak, Luca Benini</dc:creator>
    </item>
    <item>
      <title>A Survey of Circuit Foundation Model: Foundation AI Models for VLSI Circuit Design and EDA</title>
      <link>https://arxiv.org/abs/2504.03711</link>
      <description>arXiv:2504.03711v1 Announce Type: new 
Abstract: Artificial intelligence (AI)-driven electronic design automation (EDA) techniques have been extensively explored for VLSI circuit design applications. Most recently, foundation AI models for circuits have emerged as a new technology trend. Unlike traditional task-specific AI solutions, these new AI models are developed through two stages: 1) self-supervised pre-training on a large amount of unlabeled data to learn intrinsic circuit properties; and 2) efficient fine-tuning for specific downstream applications, such as early-stage design quality evaluation, circuit-related context generation, and functional verification. This new paradigm brings many advantages: model generalization, less reliance on labeled circuit data, efficient adaptation to new tasks, and unprecedented generative capability. In this paper, we propose referring to AI models developed with this new paradigm as circuit foundation models (CFMs). This paper provides a comprehensive survey of the latest progress in circuit foundation models, unprecedentedly covering over 130 relevant works. Over 90% of our introduced works were published in or after 2022, indicating that this emerging research trend has attracted wide attention in a short period. In this survey, we propose to categorize all existing circuit foundation models into two primary types: 1) encoder-based methods performing general circuit representation learning for predictive tasks; and 2) decoder-based methods leveraging large language models (LLMs) for generative tasks. For our introduced works, we cover their input modalities, model architecture, pre-training strategies, domain adaptation techniques, and downstream design applications. In addition, this paper discussed the unique properties of circuits from the data perspective. These circuit properties have motivated many works in this domain and differentiated them from general AI techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03711v1</guid>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Wenji Fang, Jing Wang, Yao Lu, Shang Liu, Yuchao Wu, Yuzhe Ma, Zhiyao Xie</dc:creator>
    </item>
    <item>
      <title>WebRISC-V: A 64-bit RISC-V Pipeline Simulator for Computer Architecture Classes</title>
      <link>https://arxiv.org/abs/2504.03722</link>
      <description>arXiv:2504.03722v1 Announce Type: new 
Abstract: WebRISC-V is a web-based educational tool designed to simulate the pipelined execution of assembly programs according to the RV64IM specifications (64-bit RISC-V processor). The tool allows users to investigate pipeline stalls, understand the internal state of pipeline architectural blocks, and visualize the cycle-by-cycle execution of instructions. WebRISC-V executes directly in a web browser, providing a detailed pipeline execution for RISC-V processors. This paper describes the features of WebRISC-V, compares it with similar tools, and provides an example of its usage in investigating the pipeline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03722v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Roberto Giorgi, Gianfranco Mariotti</dc:creator>
    </item>
    <item>
      <title>VFlow: Discovering Optimal Agentic Workflows for Verilog Generation</title>
      <link>https://arxiv.org/abs/2504.03723</link>
      <description>arXiv:2504.03723v1 Announce Type: new 
Abstract: Hardware design automation faces challenges in generating high-quality Verilog code efficiently. This paper introduces VFlow, an automated framework that optimizes agentic workflows for Verilog code generation. Unlike existing approaches that rely on pre-defined prompting strategies, VFlow leverages Monte Carlo Tree Search (MCTS) to discover effective sequences of Large Language Models invocations that maximize code quality while minimizing computational costs. VFlow extends the AFLOW methodology with domain-specific operators addressing hardware design requirements, including syntax validation, simulation-based verification, and synthesis optimization. Experimental evaluation on the VerilogEval benchmark demonstrates VFlow's superiority, achieving an 83.6% average pass@1 rate-a 6.1\% improvement over state-of-the-art PromptV and a 36.9\% gain compared to direct LLM invocation. Most significantly, VFlow enhances the capabilities of smaller models, enabling DeepSeek-V3 to achieve 141.2\% of GPT-4o's performance while reducing API costs to just 13\%. These findings indicate that intelligently optimized workflows enable cost-efficient LLMs to outperform larger models on hardware design tasks, potentially democratizing access to advanced digital circuit development tools and accelerating innovation in the semiconductor industry</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03723v1</guid>
      <category>cs.AR</category>
      <category>cs.MA</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yangbo Wei, Zhen Huang, Huang Li, Wei W. Xing, Ting-Jung Lin, Lei He</dc:creator>
    </item>
    <item>
      <title>SAGe: A Lightweight Algorithm-Architecture Co-Design for Alleviating Data Preparation Overheads in Large-Scale Genome Analysis</title>
      <link>https://arxiv.org/abs/2504.03732</link>
      <description>arXiv:2504.03732v1 Announce Type: new 
Abstract: There have been extensive efforts to accelerate genome analysis, given the exponentially growing volumes of genomic data. Prior works typically assume that the data is ready to be analyzed in the desired format; in real usage scenarios, however, it is common practice to store genomic data in storage systems in a compressed format. Unfortunately, preparing genomic data (i.e., accessing compressed data from storage, and decompressing and reformatting it) for an accelerator leads to large performance and energy overheads, significantly diminishing the accelerator's intended benefits. To harness the benefits of acceleration, without needing to store massive genomic data uncompressed, there is a critical need to effectively address data preparation overheads. The solution must meet three criteria: (i) high performance and energy efficiency, (ii) high compression ratios, comparable to state-of-the-art genomic compression, and (iii) be lightweight for seamless integration with a broad range of genomics systems. This is challenging, particularly due to the high decompression complexity of state-of-the-art genomic compressors and the resource constraints of a wide range of genomics systems. We propose SAGe, an algorithm-architecture co-design for highly-compressed storage and high-performance access of large-scale genomic data in desired formats. With our rigorous analysis of genomic datasets' features, we propose a co-design of a new (de)compression algorithm, hardware, storage data layout, and interface commands. SAGe encodes data in structures decodable by efficient sequential scans and lightweight hardware. To still maintain high compression ratios, SAGe exploits unique features of genomic data. SAGe improves the average performance (energy efficiency) of state-of-the-art genomics accelerators by 3.0-12.3x (18.8-49.6x), compared to when the accelerators rely on state-of-the-art decompressors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03732v1</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <category>q-bio.GN</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nika Mansouri Ghiasi, Talu G\"uloglu, Harun Mustafa, Can Firtina, Konstantina Koliogeorgi, Konstantinos Kanellopoulos, Haiyu Mao, Rakesh Nadig, Mohammad Sadrosadati, Jisung Park, Onur Mutlu</dc:creator>
    </item>
    <item>
      <title>Efficient Calibration for RRAM-based In-Memory Computing using DoRA</title>
      <link>https://arxiv.org/abs/2504.03763</link>
      <description>arXiv:2504.03763v1 Announce Type: new 
Abstract: Resistive In-Memory Computing (RIMC) offers ultra-efficient computation for edge AI but faces accuracy degradation due to RRAM conductance drift over time. Traditional retraining methods are limited by RRAM's high energy consumption, write latency, and endurance constraints. We propose a DoRA-based calibration framework that restores accuracy by compensating influential weights with minimal calibration parameters stored in SRAM, leaving RRAM weights untouched. This eliminates in-field RRAM writes, ensuring energy-efficient, fast, and reliable calibration. Experiments on RIMC-based ResNet50 (ImageNet-1K) demonstrate 69.53% accuracy restoration using just 10 calibration samples while updating only 2.34% of parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03763v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weirong Dong, Kai Zhou, Zhen Kong, Quan Cheng, Junkai Huang, Zhengke Yang, Masanori Hashimoto, Longyang Lin</dc:creator>
    </item>
    <item>
      <title>RealProbe: An Automated and Lightweight Performance Profiler for In-FPGA Execution of High-Level Synthesis Designs</title>
      <link>https://arxiv.org/abs/2504.03879</link>
      <description>arXiv:2504.03879v1 Announce Type: new 
Abstract: High-level synthesis (HLS) accelerates FPGA design by rapidly generating diverse implementations using optimization directives. However, even with cycle-accurate C/RTL co-simulation, the reported clock cycles often differ significantly from actual FPGA performance. This discrepancy hampers accurate bottleneck identification, leading to suboptimal design choices. Existing in-FPGA profiling tools, such as the Integrated Logic Analyzer (ILA), require tedious inspection of HLS-generated RTL and manual signal monitoring, reducing productivity. To address these challenges, we introduce RealProbe, the first fully automated, lightweight in-FPGA profiling tool for HLS designs. With a single directive--#pragma HLS RealProbe--the tool automatically generates all necessary code to profile cycle counts across the full function hierarchy, including submodules and loops. RealProbe extracts, records, and visualizes cycle counts with high precision, providing actionable insights into on-board performance. RealProbe is non-intrusive, implemented as independent logic to ensure minimal impact on kernel functionality or timing. It also supports automated design space exploration (DSE), optimizing resource allocation based on FPGA constraints and module complexity. By leveraging incremental synthesis and implementation, DSE runs independently of the original HLS kernel. Evaluated across 28 diverse test cases, including a large-scale design, RealProbe achieves 100% accuracy in capturing cycle counts with minimal logic overhead-just 16.98% LUTs, 43.15% FFs, and 0% BRAM usage. The tool, with full documentation and examples, is available on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03879v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiho Kim, Cong Hao</dc:creator>
    </item>
    <item>
      <title>Learning Cache Coherence Traffic for NoC Routing Design</title>
      <link>https://arxiv.org/abs/2504.04005</link>
      <description>arXiv:2504.04005v1 Announce Type: new 
Abstract: The rapid growth of multi-core systems highlights the need for efficient Network-on-Chip (NoC) design to ensure seamless communication. Cache coherence, essential for data consistency, substantially reduces task computation time by enabling data sharing among caches. As a result, routing serves two roles: facilitating data sharing (influenced by topology) and managing NoC-level communication. However, cache coherence is often overlooked in routing, causing mismatches between design expectations and evaluation outcomes. Two main challenges are the lack of specialized tools to assess cache coherence's impact and the neglect of topology selection in routing. In this work, we propose a cache coherence-aware routing approach with integrated topology selection, guided by our Cache Coherence Traffic Analyzer (CCTA). Our method achieves up to 10.52% lower packet latency, 55.51% faster execution time, and 49.02% total energy savings, underscoring the critical role of cache coherence in NoC design and enabling effective co-design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04005v1</guid>
      <category>cs.AR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guochu Xiong, Xiangzhong Luo, Weichen Liu</dc:creator>
    </item>
    <item>
      <title>Multi-Phase Coupled CMOS Ring Oscillator based Potts Machine</title>
      <link>https://arxiv.org/abs/2504.04223</link>
      <description>arXiv:2504.04223v1 Announce Type: new 
Abstract: This paper presents a coupled ring oscillator based Potts ma chine to solve NP-hard combinatorial optimization problems
  (COPs). Potts model is a generalization of the Ising model, cap turing multivalued spins in contrast to the binary-valued spins
  allowed in the Ising model. Similar to recent literature on Ising
  machines, the proposed architecture of Potts machines imple ments the Potts model with interacting spins represented by cou pled ring oscillators. Unlike Ising machines which are limited
  to two spin values, Potts machines model COPs that require a
  larger number of spin values. A major novelty of the proposed
  Potts machine is the utilization of the N-SHIL (Sub-Harmonic
  Injection Locking) mechanism, where multiple stable phases are
  obtained from a single (i.e. ring) oscillator. In evaluation, 3 coloring problems from the DIMACS SATBLIB benchmark and
  two randomly generated larger problems are mapped to the pro posed architecture. The proposed architecture is demonstrated
  to solve problems of varying size with 89% to 92% accuracy
  averaged over multiple iterations. The simulation results show
  that there is no degradation in accuracy, no significant increase
  in solution time, and only a linear increase in power dissipation
  with increasing problem sizes up to 2000 nodes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04223v1</guid>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3676536.3676720</arxiv:DOI>
      <dc:creator>Yilmaz Ege Gonul, Baris Taskin</dc:creator>
    </item>
    <item>
      <title>Virtual memory for real-time systems using hPMP</title>
      <link>https://arxiv.org/abs/2504.04498</link>
      <description>arXiv:2504.04498v1 Announce Type: new 
Abstract: To satisfy automotive safety and security requirements, memory protection mechanisms are an essential component of automotive microcontrollers. In today's available systems, either a fully physical address-based protection is implemented utilizing a memory protection unit, or a memory management unit takes care of memory protection while also mapping virtual addresses to physical addresses. The possibility to develop software using a large virtual address space, which is agnostic to the underlying physical address space, allows for easier software development and integration, especially in the context of virtualization. In this work, we showcase an extension to the current RISC-V SPMP proposal that enables address redirection for selected address regions, while maintaining the fully deterministic behavior of a memory protection unit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04498v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Konrad Walluszik, Daniel Auge, Gerhard Wirrer, Holm Rauchfuss, Thomas Roecker</dc:creator>
    </item>
    <item>
      <title>N-TORC: Native Tensor Optimizer for Real-time Constraints</title>
      <link>https://arxiv.org/abs/2504.04661</link>
      <description>arXiv:2504.04661v1 Announce Type: new 
Abstract: Compared to overlay-based tensor architectures like VTA or Gemmini, compilers that directly translate machine learning models into a dataflow architecture as HLS code, such as HLS4ML and FINN, generally can achieve lower latency by generating customized matrix-vector multipliers and memory structures tailored to the specific fundamental tensor operations required by each layer. However, this approach has significant drawbacks: the compilation process is highly time-consuming and the resulting deployments have unpredictable area and latency, making it impractical to constrain the latency while simultaneously minimizing area. Currently, no existing methods address this type of optimization. In this paper, we present N-TORC (Native Tensor Optimizer for Real-Time Constraints), a novel approach that utilizes data-driven performance and resource models to optimize individual layers of a dataflow architecture. When combined with model hyperparameter optimization, N-TORC can quickly generate architectures that satisfy latency constraints while simultaneously optimizing for both accuracy and resource cost (i.e. offering a set of optimal trade-offs between cost and accuracy). To demonstrate its effectiveness, we applied this framework to a cyber-physical application, DROPBEAR (Dynamic Reproduction of Projectiles in Ballistic Environments for Advanced Research). N-TORC's HLS4ML performance and resource models achieve higher accuracy than prior efforts, and its Mixed Integer Program (MIP)-based solver generates equivalent solutions to a stochastic search in 1000X less time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04661v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Suyash Vardhan Singh, Iftakhar Ahmad, David Andrews, Miaoqing Huang, Austin R. J. Downey, Jason D. Bakos</dc:creator>
    </item>
    <item>
      <title>FERIVer: An FPGA-assisted Emulated Framework for RTL Verification of RISC-V Processors</title>
      <link>https://arxiv.org/abs/2504.05284</link>
      <description>arXiv:2504.05284v1 Announce Type: new 
Abstract: Processor design and verification require a synergistic approach that combines instruction-level functional simulations with precise hardware emulations. The trade-off between speed and accuracy in the instruction set simulation poses a significant challenge to the efficiency of processor verification. By tapping the potentials of Field Programmable Gate Arrays (FPGAs), we propose an FPGA-assisted System-on-Chip (SoC) platform that facilitates cross-verification by the embedded CPU and the synthesized hardware in the programmable fabrics. This method accelerates the verification of the RISC-V Instruction Set Architecture (ISA) processor at a speed of 5 million instructions per second (MIPS), which is 150x faster than the vendor-specific tool (Xilinx XSim) and a 35x boost to the state-of-the-art open-source verification setup (Verilator). With less than 7\% hardware occupation on Zynq 7000 FPGA, the proposed framework enables flexible verification with high time and cost efficiency for exploring RISC-V instruction set architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05284v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kun Qin, Xiaorang Guo, Martin Schulz, Carsten Trinitis</dc:creator>
    </item>
    <item>
      <title>Achieving Dependability of AI Execution with Radiation Hardened Processors</title>
      <link>https://arxiv.org/abs/2504.03680</link>
      <description>arXiv:2504.03680v1 Announce Type: cross 
Abstract: The reliance on radiation-hardened hardware, essential for domains requiring high-dependability such as space, nuclear energy and medical applications, severely restricts the choice of components available for modern AI-intensive tasks, particularly for real-time AI-based classifications. To address this challenge, we propose leveraging the High Performance Data Processor (HPDP) as a radiation-hardened and low-power co-processor in conjunction with an optimized AI framework for efficient data processing. The HPDP's dynamic reconfiguration capabilities and dataflow-oriented architecture provide an ideal platform for executing AI-driven applications that demand low-latency, high-throughput streaming data processing. To fully utilize the co-processor's capabilities, we utilized Klepsydra's AI-runtime inference framework, which, due to its lock-free execution and efficient resource management, significantly enhances data processing throughput without increasing power consumption. Our approach entails programming the HPDP as a dedicated mathematical backend, enabling the AI framework to execute workloads directly on this co-processor without requiring additional hardware-specific coding. This paper presents the preliminary results of our implementation, describing the application domain, AI pipeline, key features of the HPDP architecture, and performance evaluation. Our solution demonstrates a significant advancement in deploying AI on radiation-hardened platforms by using the HPDP as a dependable, efficient, and reprogrammable co-processor, making it highly suitable for any application requiring dependable execution in any environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03680v1</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Carlos Rafael Tordoya Taquichiri, Hans Dermot Doran, Pablo Ghiglino, Mandar Harshe</dc:creator>
    </item>
    <item>
      <title>Exploring energy consumption of AI frameworks on a 64-core RV64 Server CPU</title>
      <link>https://arxiv.org/abs/2504.03774</link>
      <description>arXiv:2504.03774v1 Announce Type: cross 
Abstract: In today's era of rapid technological advancement, artificial intelligence (AI) applications require large-scale, high-performance, and data-intensive computations, leading to significant energy demands. Addressing this challenge necessitates a combined approach involving both hardware and software innovations. Hardware manufacturers are developing new, efficient, and specialized solutions, with the RISC-V architecture emerging as a prominent player due to its open, extensible, and energy-efficient instruction set architecture (ISA). Simultaneously, software developers are creating new algorithms and frameworks, yet their energy efficiency often remains unclear. In this study, we conduct a comprehensive benchmark analysis of machine learning (ML) applications on the 64-core SOPHON SG2042 RISC-V architecture. We specifically analyze the energy consumption of deep learning inference models across three leading AI frameworks: PyTorch, ONNX Runtime, and TensorFlow. Our findings show that frameworks using the XNNPACK back-end, such as ONNX Runtime and TensorFlow, consume less energy compared to PyTorch, which is compiled with the native OpenBLAS back-end.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03774v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giulio Malenza, Francesco Targa, Adriano Marques Garcia, Marco Aldinucci, Robert Birke</dc:creator>
    </item>
    <item>
      <title>Reconfigurable Time-Domain In-Memory Computing Marco using CAM FeFET with Multilevel Delay Calibration in 28 nm CMOS</title>
      <link>https://arxiv.org/abs/2504.03925</link>
      <description>arXiv:2504.03925v1 Announce Type: cross 
Abstract: Time-domain nonvolatile in-memory computing (TD-nvIMC) architectures enhance energy efficiency by reducing data movement and data converter power. This work presents a reconfigurable TD-nvIMC accelerator integrating on-die a ferroelectric FET content-addressable memory array, delay element chain, and time-to-digital converter. Fabricated in 28 nm CMOS, it supports binary MAC operations using XOR/AND for multiplication and Boolean logic. FeFET-based nvIMC with 550 ps step size is empirically demonstrated, almost 2000$\times$ improvement from previous works. Write-disturb prevention and multilevel state (MLS) is demonstrated using isolated bulks. Delay element mismatch is compensated through an on-die MLS calibration for robust operation with a high temporal resolution of 100 ps. The proposed architecture can achieve a throughput of 232 GOPS and energy efficiency of 1887 TOPS/W with a 0.85-V supply, making it a promising candidate for efficient in-memory computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03925v1</guid>
      <category>cs.ET</category>
      <category>cs.AR</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jeries Mattar, Mor M. Dahan, Stefan Dunkel, Halid Mulaosmanovic, Sven Beyer, Eilam Yalon, Nicol\'as Wainstein</dc:creator>
    </item>
    <item>
      <title>OffRAC: Offloading Through Remote Accelerator Calls</title>
      <link>https://arxiv.org/abs/2504.04404</link>
      <description>arXiv:2504.04404v1 Announce Type: cross 
Abstract: Modern applications increasingly demand ultra-low latency for data processing, often facilitated by host-controlled accelerators like GPUs and FPGAs. However, significant delays result from host involvement in accessing accelerators. To address this limitation, we introduce a novel paradigm we call Offloading through Remote Accelerator Calls (OffRAC), which elevates accelerators to first-class compute resources. OffRAC enables direct calls to FPGA-based accelerators without host involvement. Utilizing the stateless function abstraction of serverless computing, with applications decomposed into simpler stateless functions, offloading promotes efficient acceleration and distribution of computational loads across the network. To realize this proposal, we present a prototype design and implementation of an OffRAC platform for FPGAs that assembles diverse requests from multiple clients into complete accelerator calls with multi-tenancy performance isolation. This design minimizes the implementation complexity for accelerator users while ensuring isolation and programmability. Results show that the OffRAC approach reduces the latency of network calls to accelerators down to approximately 10.5 us, as well as sustaining high application throughput up to 85Gbps, demonstrating scalability and efficiency, making it compelling for the next generation of low-latency applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04404v1</guid>
      <category>cs.NI</category>
      <category>cs.AR</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ziyi Yang, Krishnan B. Iyer, Yixi Chen, Ran Shi, Zsolt Istv\'an, Marco Canini, Suhaib A. Fahmy</dc:creator>
    </item>
    <item>
      <title>Balancing Robustness and Efficiency in Embedded DNNs Through Activation Function Selection</title>
      <link>https://arxiv.org/abs/2504.05119</link>
      <description>arXiv:2504.05119v1 Announce Type: cross 
Abstract: Machine learning-based embedded systems for safety-critical applications, such as aerospace and autonomous driving, must be robust to perturbations caused by soft errors. As transistor geometries shrink and voltages decrease, modern electronic devices become more susceptible to background radiation, increasing the concern about failures produced by soft errors. The resilience of deep neural networks (DNNs) to these errors depends not only on target device technology but also on model structure and the numerical representation and arithmetic precision of their parameters. Compression techniques like pruning and quantization, used to reduce memory footprint and computational complexity, alter both model structure and representation, affecting soft error robustness. In this regard, although often overlooked, the choice of activation functions (AFs) impacts not only accuracy and trainability but also compressibility and error resilience. This paper explores the use of bounded AFs to enhance robustness against parameter perturbations, while evaluating their effects on model accuracy, compressibility, and computational load with a technology-agnostic approach. We focus on encoder-decoder convolutional models developed for semantic segmentation of hyperspectral images with application to autonomous driving systems. Experiments are conducted on an AMD-Xilinx's KV260 SoM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05119v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1049/ell2.70210</arxiv:DOI>
      <dc:creator>Jon Guti\'errez Zaballa, Koldo Basterretxea, Javier Echanobe</dc:creator>
    </item>
    <item>
      <title>A Survey on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms</title>
      <link>https://arxiv.org/abs/2306.15552</link>
      <description>arXiv:2306.15552v3 Announce Type: replace 
Abstract: Recent trends in deep learning (DL) have made hardware accelerators essential for various high-performance computing (HPC) applications, including image classification, computer vision, and speech recognition. This survey summarizes and classifies the most recent developments in DL accelerators, focusing on their role in meeting the performance demands of HPC applications. We explore cutting-edge approaches to DL acceleration, covering not only GPU- and TPU-based platforms but also specialized hardware such as FPGA- and ASIC-based accelerators, Neural Processing Units, open hardware RISC-V-based accelerators, and co-processors. This survey also describes accelerators leveraging emerging memory technologies and computing paradigms, including 3D-stacked Processor-In-Memory, non-volatile memories like Resistive RAM and Phase Change Memories used for in-memory computing, as well as Neuromorphic Processing Units, and Multi-Chip Module-based accelerators. Furthermore, we provide insights into emerging quantum-based accelerators and photonics. Finally, this survey categorizes the most influential architectures and technologies from recent years, offering readers a comprehensive perspective on the rapidly evolving field of deep learning acceleration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15552v3</guid>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Cristina Silvano, Daniele Ielmini, Fabrizio Ferrandi, Leandro Fiorin, Serena Curzel, Luca Benini, Francesco Conti, Angelo Garofalo, Cristian Zambelli, Enrico Calore, Sebastiano Fabio Schifano, Maurizio Palesi, Giuseppe Ascia, Davide Patti, Nicola Petra, Davide De Caro, Luciano Lavagno, Teodoro Urso, Valeria Cardellini, Gian Carlo Cardarilli, Robert Birke, Stefania Perri</dc:creator>
    </item>
    <item>
      <title>Holistic Optimization Framework for FPGA Accelerators</title>
      <link>https://arxiv.org/abs/2501.09242</link>
      <description>arXiv:2501.09242v4 Announce Type: replace 
Abstract: Customized accelerators have revolutionized modern computing by delivering substantial gains in energy efficiency and performance through hardware specialization. Field-Programmable Gate Arrays (FPGAs) play a crucial role in this paradigm, offering unparalleled flexibility and high-performance potential. High-Level Synthesis (HLS) and source-to-source compilers have simplified FPGA development by translating high-level programming languages into hardware descriptions enriched with directives. However, achieving high Quality of Results (QoR) remains a significant challenge, requiring intricate code transformations, strategic directive placement, and optimized data communication. This paper presents Prometheus, a holistic optimization framework that integrates key optimizations--including task fusion, tiling, loop permutation, computation-communication overlap, and concurrent task execution--into a unified design space. By leveraging Non-Linear Programming (NLP) methodologies, Prometheus explores the optimization space under strict resource constraints, enabling automatic bitstream generation. Unlike existing frameworks, Prometheus considers interdependent transformations and dynamically balances computation and memory access. We evaluate Prometheus across multiple benchmarks, demonstrating its ability to maximize parallelism, minimize execution stalls, and optimize data movement. The results showcase its superior performance compared to state-of-the-art FPGA optimization frameworks, highlighting its effectiveness in delivering high QoR while reducing manual tuning efforts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09242v4</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>St\'ephane Pouget, Michael Lo, Louis-No\"el Pouchet, Jason Cong</dc:creator>
    </item>
    <item>
      <title>InTAR: Inter-Task Auto-Reconfigurable Accelerator Design for High Data Volume Variation in DNNs</title>
      <link>https://arxiv.org/abs/2502.08807</link>
      <description>arXiv:2502.08807v2 Announce Type: replace 
Abstract: The rise of deep neural networks (DNNs) has driven an increased demand for computing power and memory. Modern DNNs exhibit high data volume variation (HDV) across tasks, which poses challenges for FPGA acceleration: conventional accelerators rely on fixed execution patterns (dataflow or sequential) that can lead to pipeline stalls or necessitate frequent off-chip memory accesses. To address these challenges, we introduce the Inter-Task Auto-Reconfigurable Accelerator (InTAR), a novel accelerator design methodology for HDV applications on FPGAs. InTAR combines the high computational efficiency of sequential execution with the reduced off-chip memory overhead of dataflow execution. It switches execution patterns automatically with a static schedule determined before circuit design based on resource constraints and problem sizes. Unlike previous reconfigurable accelerators, InTAR encodes reconfiguration schedules during circuit design, allowing model-specific optimizations that allocate only the necessary logic and interconnects. Thus, InTAR achieves a high clock frequency with fewer resources and low reconfiguration time. Furthermore, InTAR supports high-level tools such as HLS for fast design generation. We implement a set of multi-task HDV DNN kernels using InTAR. Compared with dataflow and sequential accelerators, InTAR exhibits $\mathbf{1.8\times}$ and $\mathbf{7.1 \times}$ speedups correspondingly. Moreover, we extend InTAR to GPT-2 medium as a more complex example, which is $\mathbf{3.65 \sim 39.14\times}$ faster and a $\mathbf{1.72 \sim 10.44\times}$ more DSP efficient than SoTA accelerators (Allo and DFX) on FPGAs. Additionally, this design demonstrates $\mathbf{1.66 \sim 7.17\times}$ better power efficiency than GPUs. Code: https://github.com/OswaldHe/InTAR</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08807v2</guid>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zifan He, Anderson Truong, Yingqi Cao, Jason Cong</dc:creator>
    </item>
    <item>
      <title>An Analytical Cost Model for Fast Evaluation of Multiple Compute-Engine CNN Accelerators</title>
      <link>https://arxiv.org/abs/2503.07242</link>
      <description>arXiv:2503.07242v3 Announce Type: replace 
Abstract: Convolutional Neural Networks (CNNs) serve various applications with diverse performance and resource requirements. Model-aware CNN accelerators best address these diverse requirements. These accelerators usually combine multiple dedicated Compute Engines (CEs). The flexibility of Field-Programmable Gate Arrays (FPGAs) enables the design of such multiple Compute-Engine (multiple-CE) accelerators. However, existing multiple-CE accelerators differ in how they arrange their CEs and distribute the FPGA resources and CNN operators among the CEs. The design space of multiple-CE accelerators comprises numerous such arrangements, which makes a systematic identification of the best ones an open challenge. This paper proposes a multiple-CE accelerator analytical Cost Model (MCCM) and an evaluation methodology built around MCCM. The model and methodology streamline the expression of any multiple-CE accelerator and provide a fast evaluation of its performance and efficiency. MCCM is in the order of 100000x faster than traditional synthesis-based evaluation and has an average accuracy of &gt; 90%. The paper presents three use cases of MCCM. The first describes an end-to-end evaluation of state-of-the-art multiple-CE accelerators considering various metrics, CNN models, and resource budgets. The second describes fine-grained evaluation that helps identify performance bottlenecks of multiple-CE accelerators. The third demonstrates that MCCM fast evaluation enables exploring the vast design space of multiple-CE accelerators. These use cases show that no unique CE arrangement achieves the best results given different metrics, CNN models, and resource budgets. They also show that fast evaluation enables design space exploration, resulting in accelerator designs that outperform state-of-the-art ones. MCCM is available at https://github.com/fqararyah/MCCM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07242v3</guid>
      <category>cs.AR</category>
      <category>cs.PF</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fareed Qararyah, Mohammad Ali Maleki, Pedro Trancoso</dc:creator>
    </item>
    <item>
      <title>ReaLM: Reliable and Efficient Large Language Model Inference with Statistical Algorithm-Based Fault Tolerance</title>
      <link>https://arxiv.org/abs/2503.24053</link>
      <description>arXiv:2503.24053v2 Announce Type: replace 
Abstract: The demand for efficient large language model (LLM) inference has propelled the development of dedicated accelerators. As accelerators are vulnerable to hardware faults due to aging, variation, etc, existing accelerator designs often reserve a large voltage margin or leverage algorithm-based fault tolerance (ABFT) techniques to ensure LLM inference correctness. However, previous methods often overlook the inherent fault tolerance of LLMs, leading to high computation and energy overhead. To enable reliable yet efficient LLM inference, in this paper, we propose a novel algorithm/circuit co-design framework, dubbed ReaLM. For the first time, we systematically characterize the fault tolerance of LLMs by performing a large-scale error injection study of representative LLMs and natural language understanding tasks. Then, we propose a statistical ABFT algorithm that fully leverages the error robustness to minimize error recovery as much as possible. We also customize the error detection circuits to enable a low-cost online collection of error statistics. Extensive experiments show that with only 1.42% circuit area and 1.79% power overhead, our ReaLM can reduce perplexity degradation from 18.54 to 0.29. Compared to existing methods, ReaLM consistently reduces recovery costs across different operating voltages and improves energy efficiency by up to 35.83% without compromising LLM performance. Our error injection code is available at https://github.com/PKU-SEC-Lab/ReaLM_DAC25/</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.24053v2</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tong Xie, Jiawang Zhao, Zishen Wan, Zuodong Zhang, Yuan Wang, Runsheng Wang, Ru Huang, Meng Li</dc:creator>
    </item>
    <item>
      <title>Dataflow Optimized Reconfigurable Acceleration for FEM-based CFD Simulations</title>
      <link>https://arxiv.org/abs/2411.16245</link>
      <description>arXiv:2411.16245v2 Announce Type: replace-cross 
Abstract: Computational Fluid Dynamics (CFD) simulations are essential for analyzing and optimizing fluid flows in a wide range of real-world applications. These simulations involve approximating the solutions of the Navier-Stokes differential equations using numerical methods, which are highly compute- and memory-intensive due to their need for high-precision iterations. In this work, we introduce a high-performance FPGA accelerator specifically designed for numerically solving the Navier-Stokes equations. We focus on the Finite Element Method (FEM) due to its ability to accurately model complex geometries and intricate setups typical of real-world applications. Our accelerator is implemented using High-Level Synthesis (HLS) on an AMD Alveo U200 FPGA, leveraging the reconfigurability of FPGAs to offer a flexible and adaptable solution. The proposed solution achieves 7.9x higher performance than optimized Vitis-HLS implementations and 45% lower latency with 3.64x less power compared to a software implementation on a high-end server CPU. This highlights the potential of our approach to solve Navier-Stokes equations more effectively, paving the way for tackling even more challenging CFD simulations in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16245v2</guid>
      <category>physics.flu-dyn</category>
      <category>cs.AR</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anastassis Kapetanakis, Aggelos Ferikoglou, George Anagnostopoulos, Sotirios Xydis</dc:creator>
    </item>
    <item>
      <title>CODEI: Resource-Efficient Task-Driven Co-Design of Perception and Decision Making for Mobile Robots Applied to Autonomous Vehicles</title>
      <link>https://arxiv.org/abs/2503.10296</link>
      <description>arXiv:2503.10296v2 Announce Type: replace-cross 
Abstract: This paper discusses the integration challenges and strategies for designing mobile robots, by focusing on the task-driven, optimal selection of hardware and software to balance safety, efficiency, and minimal usage of resources such as costs, energy, computational requirements, and weight. We emphasize the interplay between perception and motion planning in decision-making by introducing the concept of occupancy queries to quantify the perception requirements for sampling-based motion planners. Sensor and algorithm performance are evaluated using False Negative Rates (FPR) and False Positive Rates (FPR) across various factors such as geometric relationships, object properties, sensor resolution, and environmental conditions. By integrating perception requirements with perception performance, an Integer Linear Programming (ILP) approach is proposed for efficient sensor and algorithm selection and placement. This forms the basis for a co-design optimization that includes the robot body, motion planner, perception pipeline, and computing unit. We refer to this framework for solving the co-design problem of mobile robots as CODEI, short for Co-design of Embodied Intelligence. A case study on developing an Autonomous Vehicle (AV) for urban scenarios provides actionable information for designers, and shows that complex tasks escalate resource demands, with task performance affecting choices of the autonomy stack. The study demonstrates that resource prioritization influences sensor choice: cameras are preferred for cost-effective and lightweight designs, while lidar sensors are chosen for better energy and computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10296v2</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.CV</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TRO.2025.3552347</arxiv:DOI>
      <arxiv:journal_reference>10.1109/TRO.2025.3552347</arxiv:journal_reference>
      <dc:creator>Dejan Milojevic, Gioele Zardini, Miriam Elser, Andrea Censi, Emilio Frazzoli</dc:creator>
    </item>
  </channel>
</rss>
