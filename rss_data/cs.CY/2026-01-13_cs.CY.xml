<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 13 Jan 2026 05:00:27 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Data-Driven Framework Development for Public Space Quality Assessment</title>
      <link>https://arxiv.org/abs/2601.06026</link>
      <description>arXiv:2601.06026v1 Announce Type: new 
Abstract: Public space quality assessment lacks systematic methodologies that integrate factors across diverse spatial typologies while maintaining context-specific relevance. Current approaches remain fragmented within disciplinary boundaries, limiting comprehensive evaluation and comparative analysis across different space types. This study develops a systematic, data-driven framework for assessing public space quality through the algorithmic integration of empirical research findings. Using a 7-phase methodology, we transform 1,207 quality factors extracted from 157 peer-reviewed studies into a validated hierarchical taxonomy spanning six public space typologies: urban spaces, open spaces, green spaces, parks and waterfronts, streets and squares, and public facilities. The methodology combines semantic analysis, cross-typology distribution analysis, and domain knowledge integration to address terminological variations and functional relationships across space types. The resulting framework organizes 1,029 unique quality factors across 14 main categories and 66 subcategories, identifying 278 universal factors applicable across all space types, 397 space-specific factors unique to particular typologies, and 124 cross-cutting factors serving multiple functions. Framework validation demonstrates systematic consistency in factor organization and theoretical alignment with established research on public spaces. This research provides a systematic methodology for transforming empirical public space research into practical assessment frameworks, supporting evidence-based policy development, design quality evaluation, and comparative analysis across diverse urban contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06026v1</guid>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <category>cs.IR</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sherzod Turaev, Mary John</dc:creator>
    </item>
    <item>
      <title>Developing Bayesian probabilistic reasoning capacity in HSS disciplines: Qualitative evaluation on bayesvl and BMF analytics for ECRs</title>
      <link>https://arxiv.org/abs/2601.06038</link>
      <description>arXiv:2601.06038v1 Announce Type: new 
Abstract: Methodological innovations have become increasingly critical in the humanities and social sciences (HSS) as researchers confront complex, nonlinear, and rapidly evolving socio-environmental systems. On the other hand, while Early Career Researchers (ECRs) continue to face intensified publication pressure, limited resources, and persistent methodological barriers. Employing the GITT-VT analytical paradigm--which integrates worldviews from quantum physics, mathematical logic, and information theory--this study examines the seven-year evolution of the Bayesian Mindsponge Framework (BMF) analytics and the bayesvl R software (hereafter referred to collectively as BMF analytics) and evaluates their contributions to strengthening ECRs' capacity for rigorous and innovative research. Since 2019, the bayesvl R package and BMF analytics have supported more than 160 authors from 22 countries in producing 112 peer-reviewed publications spanning both qualitative and quantitative designs across diverse interdisciplinary domains. By tracing the method's inception, refinement, and developmental trajectory, this study elucidates how accessible, theory-driven computational tools can lower barriers to advanced quantitative analysis, foster a more inclusive methodological ecosystem--particularly for ECRs in low-resource settings--and inform the design of next-generation research methods that are flexible, reproducible, conceptually justified, and well-suited to interdisciplinary inquiries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06038v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Quan-Hoang Vuong, Minh-Hoang Nguyen</dc:creator>
    </item>
    <item>
      <title>Cognitive Sovereignty and the Neurosecurity Governance Gap: Evidence from Singapore</title>
      <link>https://arxiv.org/abs/2601.06040</link>
      <description>arXiv:2601.06040v1 Announce Type: new 
Abstract: As brain computer interfaces (BCIs) transition from experimental medical systems to consumer and military adjacent technologies, they introduce a novel security domain in which the human nervous system becomes a networked and contestable substrate. Existing frameworks for cybersecurity, biomedical safety, and data protection were not designed to address adversarial threats to neural signal integrity, creating a governance gap characterized by systemic misclassification. This paper argues that cognition is becoming strategic infrastructure and is situated between the market driven diffusion of neurotechnology in the United States and the state integrated fusion of AI and brain science in China. Using Singapore as a critical stress test and applying institutional classification analysis and regulatory mandate mapping, this paper identifies a structural paradox. A state with high regulatory capacity in both cyber and biomedical domains remains vulnerable at their intersection due to a failure to classify the human mind as infrastructure. This paper introduces the concept of cognitive sovereignty defined as the strategic capacity to protect neural processes from external modulation and proposes a cognitive operational technology framework to secure the human mind as a distinct layer of critical national infrastructure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06040v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hailee Carter</dc:creator>
    </item>
    <item>
      <title>Teachers' Perspectives on Integrating AI tools in Classrooms: Insights from the Philippines</title>
      <link>https://arxiv.org/abs/2601.06043</link>
      <description>arXiv:2601.06043v1 Announce Type: new 
Abstract: This study explores the attitudes, reservations, readiness, openness, and general perceptions of Filipino teachers in terms of integrating Al in their classrooms. Results shows that teachers express positive attitude towards integrating Al tools in their classrooms. Despite reporting high level of reservations, teachers believed they are ready and very open in complementing traditional teaching methods with these kinds of technologies. Teachers are very much aware with the potential benefits Al tools can offer to their individual student learning needs. Additionally, teachers in this study reported high level of support from their institutions. Recommendations are offered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06043v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.58459/icce.2024.4964</arxiv:DOI>
      <arxiv:journal_reference>Sibug et al., Proceedings of the 32nd International Conference on Computers in Education, 2024, pp. 743-748</arxiv:journal_reference>
      <dc:creator>Vanessa B. Sibug, Vicky P. Vital, John Paul P. Miranda, Emerson Q. Fernando, Almer B. Gamboa, Hilene E. Hernandez, Joseph Alexander Bansil, Elmer M. Penecilla, Dina D. Gonzales</dc:creator>
    </item>
    <item>
      <title>Assessing novice programmers' perception of ChatGPT:performance, risk, decision-making, and intentions</title>
      <link>https://arxiv.org/abs/2601.06044</link>
      <description>arXiv:2601.06044v1 Announce Type: new 
Abstract: This study explores the novice programmers' intention to use chat generative pretrained transformer (ChatGPT) for programming tasks with emphasis on performance expectancy (PE), risk-reward appraisal (RRA), and decision-making (DM). Utilizing partial least squares structural equation modeling (PLS-SEM) and a sample of 413 novice programmers, the analysis demonstrates that higher PE of ChatGPT is positively correlated with improved DM in programming tasks. Novice programmers view ChatGPT as a tool that enhances their learning and skill development. Additionally, novice programmers that have a favorable RRA of ChatGPT tend to make more confident and effective decisions, acknowledging potential risks but recognizing that benefits such as quick problem-solving and learning new techniques outweigh these risks. Moreover, a positive perception of ChatGPT's role in DM significantly increases the inclination to use the tool for programming tasks. These results highlight the critical roles of perceived capabilities, risk assessment, and positive DM experiences in promoting the adoption of artificial intelligence (AI) tools in programming education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06044v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.11591/edulearn.v19i4.22328</arxiv:DOI>
      <arxiv:journal_reference>Journal of Education and Learning (EduLearn) 19 (4) (2025) 2291-2301</arxiv:journal_reference>
      <dc:creator>John Paul P. Miranda, Jaymark A. Yambao</dc:creator>
    </item>
    <item>
      <title>Assessing the Carbon Footprint of Virtual Meetings: A Quantitative Analysis of Camera Usage</title>
      <link>https://arxiv.org/abs/2601.06045</link>
      <description>arXiv:2601.06045v1 Announce Type: new 
Abstract: This paper analyzes the carbon emissions related to data consumption during video calls, focusing on the impact of having the camera on versus off. Addresses the energy efficiency and carbon footprint of digital communication tools. The study is used to quantify the real reduction in environmental impact claimed in several articles when people choose to turn off their camera during meetings. The experiment was carried out using a 4G connection via a cell phone to understand the varying data transfer associated with videos. The findings indicate that turning the camera off can halve data consumption therefore carbon emissions, particularly on mobile networks, and conclude with recommendations to optimize data usage and reduce environmental impact during calls.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06045v1</guid>
      <category>cs.CY</category>
      <category>cs.NI</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>F\'elix Mortas</dc:creator>
    </item>
    <item>
      <title>ISMS-CR: Modular Framework for Safety Management in Central Railway Workshop</title>
      <link>https://arxiv.org/abs/2601.06046</link>
      <description>arXiv:2601.06046v1 Announce Type: new 
Abstract: Indian Railway workshops form the backbone of rolling-stock maintenance, employing over 250,000 workers across 44 major workshops nationwide. Despite their scale and operational importance, workshop safety remains a persistent challenge. A field study conducted at the Jhansi Wagon Workshop involving 309 workers revealed that while basic protective equipment such as shoes and helmets was universally used, compliance with complete personal protective equipment requirements was limited. Lacerations and abrasions were identified as the most frequent injury types, highlighting systemic gaps in safety oversight and work authorization practices.
  This paper presents ISMS-CR (Integrated Safety Management System for Central Railway Workshop), a modular digital framework designed to enhance safety management through an automated Permit-to-Work (PTW) module. The proposed system digitizes the full lifecycle of work authorization, including permit initiation, validation, approval, execution, and closure. By enforcing structured workflows, role-based accountability, and traceable digital records, ISMS-CR reduces manual errors, administrative delays, and procedural non-compliance. The framework aims to strengthen operational reliability, improve audit readiness, and support safer maintenance practices in high-risk railway workshop environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06046v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sharvari Kamble, Arjun Dangle, Gargi Khurud, Om Kendre, Swati Bhatt</dc:creator>
    </item>
    <item>
      <title>Reliability and Admissibility of AI-Generated Forensic Evidence in Criminal Trials</title>
      <link>https://arxiv.org/abs/2601.06048</link>
      <description>arXiv:2601.06048v1 Announce Type: new 
Abstract: This paper examines the admissibility of AI-generated forensic evidence in criminal trials. The growing adoption of AI presents promising results for investigative efficiency. Despite advancements, significant research gaps persist in practically understanding the legal limits of AI evidence in judicial processes. Existing literature lacks focused assessment of the evidentiary value of AI outputs. The objective of this study is to evaluate whether AI-generated evidence satisfies established legal standards of reliability. The methodology involves a comparative doctrinal legal analysis of evidentiary standards across common law jurisdictions. Preliminary results indicate that AI forensic tools can enhance scale of evidence analysis. However, challenges arise from reproducibility deficits. Courts exhibit variability in acceptance of AI evidence due to limited technical literacy and lack of standardized validation protocols. Liability implications reveal that developers and investigators may bear accountability for flawed outputs. This raises critical concerns related to wrongful conviction. The paper emphasizes the necessity of independent validation and, development of AI-specific admissibility criteria. Findings inform policy development for the responsible AI integration within criminal justice systems. The research advances the objectives of Sustainable Development Goal 16 by reinforcing equitable access to justice. Preliminary results contribute for a foundation for future empirical research in AI deployed criminal forensics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06048v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>National Seminar on Criminal Law and Justice Reforms, 2025, pp. 45-53</arxiv:journal_reference>
      <dc:creator>Sahibpreet Singh, Lalita Devi</dc:creator>
    </item>
    <item>
      <title>The Violation State: Safety State Persistence in a Multimodal Language Model Interface</title>
      <link>https://arxiv.org/abs/2601.06049</link>
      <description>arXiv:2601.06049v1 Announce Type: new 
Abstract: Multimodal AI systems integrate text generation, image generation, and other capabilities within a single conversational interface. These systems employ safety mechanisms to prevent disallowed actions, including the removal of watermarks from copyrighted images. While single-turn refusals are expected, the interaction between safety filters and conversation-level state is not well understood. This study documents a reproducible behavioral effect in the ChatGPT (GPT-5.1) web interface. Manual execution was chosen to capture the exact user-facing safety behavior of the production system, rather than isolated API components. When a conversation begins with an uploaded copyrighted image and a request to remove a watermark, which the model correctly refuses, subsequent prompts to generate unrelated, benign images are refused for the remainder of the session. Importantly, text-only requests (e.g., generating a Python function) continue to succeed. Across 40 manually run sessions (30 contaminated and 10 controls), contaminated threads showed 116/120 image-generation refusals (96.67%), while control threads showed 0/40 refusals (Fisher's exact p &lt; 0.0001). All sessions used an identical fixed prompt order, ensuring sequence uniformity across conditions. We describe this as safety-state persistence: a form of conversational over-generalization in which a copyright refusal influences subsequent, unrelated image-generation behavior. We present these findings as behavioral observations, not architectural claims. We discuss possible explanations, methodological limitations (single model, single interface), and implications for multimodal reliability, user experience, and the design of session-level safety systems. These results motivate further examination of session-level safety interactions in multimodal AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06049v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bentley DeVilling (Course Correct Labs)</dc:creator>
    </item>
    <item>
      <title>Nigeria's Digital Sovereignty: Analysis of Cybersecurity Legislation, Policies, and Strategies</title>
      <link>https://arxiv.org/abs/2601.06050</link>
      <description>arXiv:2601.06050v1 Announce Type: new 
Abstract: This paper examines Nigeria's pursuit of digital sovereignty through two core instruments: the Cybercrimes (Prohibition, Prevention, etc.) Act and the National Cybersecurity Policy and Strategy (NCPS). Despite recent reforms, it remains unclear whether these frameworks effectively secure Nigeria's digital domain and advance its digital sovereignty amid escalating cross-border cyber threats. Using a multi-method, triangulated qualitative design that combines document analysis, secondary analysis of existing studies, expert insights, and direct observation of cybersecurity developments, the paper assesses how these instruments operate in practice. The Cybercrimes Act (2015, amended 2024) and NCPS (2015, revised 2021) have strengthened Nigeria's commitments to tackling cybercrime, regulating digital activities, and protecting critical infrastructure. Yet persistent gaps remain, including legislative ambiguities, weak enforcement, uneven threat prioritization, limited institutional coordination, and loss of skilled professionals. The paper argues that achieving digital sovereignty will require stronger implementation, sustainable resourcing, workforce retention, and clearer accountability mechanisms to translate policy ambition into tangible and durable security outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06050v1</guid>
      <category>cs.CY</category>
      <category>cs.CR</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Polra Victor Falade, Oluwafemi Osho</dc:creator>
    </item>
    <item>
      <title>Digital health transformation in Quebec: assessment of interoperability and governance strategies</title>
      <link>https://arxiv.org/abs/2601.06051</link>
      <description>arXiv:2601.06051v1 Announce Type: new 
Abstract: The rapid expansion of health data has led to unprecedented information availability within healthcare systems. Health information systems (HIS) play a central role in managing this data and enabling improvements in care delivery, system performance, and population health monitoring. Maximizing the value of HIS, however, requires effective information exchange across systems, making interoperability a critical prerequisite. Despite its recognized benefits, interoperability remains a major challenge within Quebec's Health and Social Services Network, largely due to the heterogeneity and fragmentation of HIS across healthcare institutions. This paper assessed how Quebec's Plan sante addressed interoperability challenges, using the dimensions from the Healthcare Information and Management Systems Society (HIMSS): foundational, structural, semantic, and organizational interoperability. This study highlighted initiatives aimed at strengthening infrastructure and information system architecture to support foundational interoperability and showed persistent challenges at the structural and semantic levels, particularly those related to the adoption of standardized data formats and harmonization of clinical terminologies. Finally, significant implementation challenges that require coordinated change management were identified regarding the organizational interoperability. Overall, while the Plan sante demonstrates a clear commitment to technological modernization, it does not fully address the interoperability multidimensional nature. Achieving meaningful interoperability will require sustained efforts across technical, normative, and organizational domains beyond the strategies currently outlined. Recent governance developments, including the creation of Sante Quebec, add complexity to this evolving context and raise further questions regarding the coordination of interoperability governance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06051v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alexandra Langford-Avelar, Delphine Bosson-Rieutort</dc:creator>
    </item>
    <item>
      <title>Sports Business Administration and New Age Technology: Role of AI</title>
      <link>https://arxiv.org/abs/2601.06053</link>
      <description>arXiv:2601.06053v1 Announce Type: new 
Abstract: This chapter explores the complexities of sports governance, taxation, dispute resolution, and the impact of digital transformation within the sports sector. This study identifies a critical research gap regarding the integration of innovative technologies to enhance governance and talent identification in sports law. The objective is to evaluate how data-driven approaches and AI can optimize recruitment processes; also ensuring compliance with existing regulations. A comprehensive analysis of current governance structures and taxation policies,(ie Income Tax Act and GST Act), reveals preliminary results indicating that reform is necessary to support sustainable growth in the sports economy. Key findings demonstrate that AI enhances player evaluation by minimizing biases and expanding access to diverse talent pools. While the Court of Arbitration for Sport provides an efficient mechanism for dispute resolution. The implications emphasize the need for regulatory reforms that align taxation policies with international best practices, promoting transparency and accountability in sports organizations. This research contributes valuable insights into the evolving dynamics of sports management, aiming to foster innovation and integrity in the industry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06053v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Sports Law in India, Volume 1, pp. 122-142 (2024)</arxiv:journal_reference>
      <dc:creator>Sahibpreet Singh, Pawan Kumar</dc:creator>
    </item>
    <item>
      <title>Investigating How MacBook Accessories Evolve across Generations, and Their Potential Environmental, Economical Impacts</title>
      <link>https://arxiv.org/abs/2601.06055</link>
      <description>arXiv:2601.06055v1 Announce Type: new 
Abstract: The technological transition of MacBook charging solutions from MagSafe to USB-C, followed by a return to MagSafe 3, encapsulates the dynamic interplay between technological advancement, environmental considerations, and economic factors. This study delves into the broad implications of these charging technology shifts, particularly focusing on the environmental repercussions associated with electronic waste and the economic impacts felt by both manufacturers and consumers. By investigating the lifecycle of these technologies - from development and market introduction through to their eventual obsolescence - this paper underscores the importance of devising strategies that not only foster technological innovation but also prioritize environmental sustainability and economic feasibility. This comprehensive analysis illuminates the crucial factors influencing the evolution of charging technologies and their wider societal and environmental implications, advocating for a balanced approach that ensures technological progress does not compromise ecological health or economic stability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06055v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zeyi Liao, Guanqun Song, Ting Zhu</dc:creator>
    </item>
    <item>
      <title>Using street view images and visual LLMs to predict heritage values for governance support: Risks, ethics, and policy implications</title>
      <link>https://arxiv.org/abs/2601.06056</link>
      <description>arXiv:2601.06056v1 Announce Type: new 
Abstract: During 2025 and 2026, the Energy Performance of Buildings Directive is being implemented in the European Union member states, requiring all member states to have National Building Renovation Plans. In Sweden, there is a lack of a national register of buildings with heritage values. This is seen as a barrier for the analyses underlying the development of Building Renovation Plans by the involved Swedish authorities. The purpose of this research was to assist Swedish authorities in assigning heritage values to building in the Swedish building stock. As part of the analyses, buildings in street view images from all over Sweden (N=154 710) have been analysed using multimodal Large Language Models (LLM) to assess aspects of heritage value. Zero-shot predictions by LLMs were used as a basis to for identifying buildings with potential heritage values for 5.0 million square meters of heated floor area for the Swedish Building Renovation Plan. In this paper, the results of the predictions and lessons learnt are presented and related to the development of Swedish Building Renovation Plan as part of governance. Potential risks for authorities using LLM-based data are addressed, with a focus on issues of transparency, error detection and sycophancy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06056v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tim Johansson, Mikael Mangold, Kristina Dabrock, Anna Donarelli, Ingrid Campo-Ruiz</dc:creator>
    </item>
    <item>
      <title>Data Work in Egypt: Who Are the Workers Behind Artificial Intelligence?</title>
      <link>https://arxiv.org/abs/2601.06057</link>
      <description>arXiv:2601.06057v1 Announce Type: new 
Abstract: The report highlights the role of Egyptian data workers in the global value chains of Artificial Intelligence (AI). These workers generate and annotate data for machine learning, check outputs, and they connect with overseas AI producers via international digital labor platforms, where they perform on-demand tasks and are typically paid by piecework, with no long-term commitment. Most of these workers are young, highly educated men, with nearly two-thirds holding undergraduate degrees. Their primary motivation for data work is financial need, with three-quarters relying on platform earnings to cover basic necessities. Despite the variability in their online earnings, these are generally low, often equaling Egypt's minimum wage. Data workers' digital identities are shaped by algorithmic control and economic demands, often diverging from their offline selves. Nonetheless, they find ways to resist, exercise ethical agency, and maintain autonomy. The report evaluates the potential impact of Egypt's newly enacted labor law and suggests policy measures to improve working conditions and acknowledge the role of these workers in AI's global value chains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06057v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Myriam Raymond (GRANEM, LEMNA, DiPLab), Lucy Neveux (HEC Paris, ENSAE), Antonio A. Casilli (I3 SES, NOS, SES, DiPLab, IP Paris), Paola Tubaro (CNRS, ENSAE Paris, CREST, DiPLab)</dc:creator>
    </item>
    <item>
      <title>Teacher training in inclusive digital skills in secondary education. Students with Autism Spectrum Disorders</title>
      <link>https://arxiv.org/abs/2601.06058</link>
      <description>arXiv:2601.06058v1 Announce Type: new 
Abstract: In contemporary society, marked by rapid technological evolution, education faces the challenge and the opportunity of incorporating new digital tools that transform learning, making it more inclusive, flexible, and meaningful. This book aligns with this commitment to educational innovation and equity, focusing on a group that requires specialized and sensitive attention: students with Autism Spectrum Disorder ASD. Far from approaching technology from a merely instrumental perspective, this work proposes a profoundly human approach, where emerging technologies such as augmented and virtual reality, immersive environments, augmentative communication systems, mobile applications, and artificial intelligence become allies in fostering autonomy, emotional self-regulation, the development of social skills, and the genuine inclusion of students with ASD in educational settings. This volume is part of the R-D project entitled Teacher Training in Inclusive Digital Competencies to Support Students with Autism Spectrum Disorders: CODITEA, funded by the Spanish Ministry of Science, Innovation and Universities-State Research Agency MICIU-AEI and the European Regional Development Fund ERDF-EU, under reference PID2022-138346OB-I00. The project aims, among other objectives, to raise awareness and train teachers, students, and families on the conscious, ethical, and effective use of technology to facilitate inclusive processes for students with ASD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06058v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.22429/Euc2025.022</arxiv:DOI>
      <arxiv:journal_reference>Fernandez-Batanero, J (ed.); Roman-Gravan, P (ed.). 2025. Editorial Universidad de Cantabria: Santander</arxiv:journal_reference>
      <dc:creator>Jose-Maria Fernandez-Batanero, Pedro Roman-Gravan</dc:creator>
    </item>
    <item>
      <title>Why Slop Matters</title>
      <link>https://arxiv.org/abs/2601.06060</link>
      <description>arXiv:2601.06060v1 Announce Type: new 
Abstract: AI-generated "slop" is often seen as digital pollution. We argue that this dismissal of the topic risks missing important aspects of AI Slop that deserve rigorous study. AI Slop serves a social function: it offers a supply-side solution to a variety of problems in cultural and economic demand - that, collectively, people want more content than humans can supply. We also argue that AI Slop is not mere digital detritus but has its own aesthetic value. Like other "low" cultural forms initially dismissed by critics, it nonetheless offers a legitimate means of collective sense-making, with the potential to express meaning and identity. We identify three key features of family resemblance for prototypical AI Slop: superficial competence (its veneer of quality is belied by a deeper lack of substance), asymmetry effort (it takes vastly less effort to generate than would be the case without AI), and mass producibility (it is part of a digital ecosystem of widespread generation and consumption). While AI Slop is heterogeneous and depends crucially on its medium, it tends to vary across three dimensions: instrumental utility, personalization, and surrealism. AI Slop will be an increasingly prolific and impactful part of our creative, information, and cultural economies; we should take it seriously as an object of study in its own right.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06060v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cody Kommers, Eamon Duede, Julia Gordon, Ari Holtzman, Tess McNulty, Spencer Stewart, Lindsay Thomas, Richard Jean So, Hoyt Long</dc:creator>
    </item>
    <item>
      <title>AI Application Operations -- A Socio-Technical Framework for Data-driven Organizations</title>
      <link>https://arxiv.org/abs/2601.06061</link>
      <description>arXiv:2601.06061v1 Announce Type: new 
Abstract: We outline a comprehensive framework for artificial intelligence (AI) Application Operations (AIAppOps), based on real-world experiences from diverse organizations. Data-driven projects pose additional challenges to organizations due to their dependency on data across the development and operations cycles. To aid organizations in dealing with these challenges, we present a framework outlining the main steps and roles involved in going from idea to production for data-driven solutions. The data dependency of these projects entails additional requirements on continuous monitoring and feedback, as deviations can emerge in any process step. Therefore, the framework embeds monitoring not merely as a safeguard, but as a unifying feedback mechanism that drives continuous improvement, compliance, and sustained value realization-anchored in both statistical and formal assurance methods that extend runtime verification concepts from safety-critical AI to organizational operations. The proposed framework is structured across core technical processes and supporting services to guide both new initiatives and maturing AI programs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06061v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel J\"onsson, Mattias Tiger, Stefan Ekberg, Daniel Jakobsson, Mattias Jonhede, Fredrik Viksten</dc:creator>
    </item>
    <item>
      <title>From Values to Frameworks: A Qualitative Study of Ethical Reasoning in Agentic AI Practitioners</title>
      <link>https://arxiv.org/abs/2601.06062</link>
      <description>arXiv:2601.06062v1 Announce Type: new 
Abstract: Agentic artificial intelligence systems are autonomous technologies capable of pursuing complex goals with minimal human oversight and are rapidly emerging as the next frontier in AI. While these systems promise major gains in productivity, they also raise new ethical challenges. Prior research has examined how different populations prioritize Responsible AI values, yet little is known about how practitioners actually reason through the trade-offs inherent in designing these autonomous systems. This paper investigates the ethical reasoning of AI practitioners through qualitative interviews centered on structured dilemmas in agentic AI deployment. We find that the responses of practitioners do not merely reflect value preferences but rather align with three distinct reasoning frameworks. First is a Customer-Centric framework where choices are justified by business interests, legality, and user autonomy. Second is a Design-Centric framework emphasizing technical safeguards and system constraints. Third is an Ethics-Centric framework prioritizing social good and moral responsibility beyond compliance. We argue that these frameworks offer distinct and necessary insights for navigating ethical trade-offs. Consequently, providers of agentic AI must look beyond general principles and actively manage how these diverse reasoning frameworks are represented in their decision-making processes to ensure robust ethical outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06062v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Theodore Roberts, Bahram Zarrin</dc:creator>
    </item>
    <item>
      <title>The Environmental Impact of AI Servers and Sustainable Solutions</title>
      <link>https://arxiv.org/abs/2601.06063</link>
      <description>arXiv:2601.06063v1 Announce Type: new 
Abstract: The rapid expansion of artificial intelligence has significantly increased the electricity, water, and carbon demands of modern data centers, raising sustainability concerns. This study evaluates the environmental footprint of AI server operations and examines feasible technological and infrastructural strategies to mitigate these impacts. Using a literature-based methodology supported by quantitative projections and case-study analysis, we assessed trends in global electricity consumption, cooling-related water use, and carbon emissions. Projections indicate that global data center electricity demand may increase from approximately 415 TWh in 2024 to nearly 945 TWh by 2030, with AI workloads accounting for a disproportionate share of this growth. In the United States alone, AI servers are expected to drive annual increases in water consumption of 200--300 billion gallons and add 24--44 million metric tons of CO2 quivalent emissions by 2030. The results show that the design of the cooling system and the geographic location influence the environmental impact as strongly as the efficiency of the hardware. Advanced cooling technologies can reduce cooling energy by up to 50%, while location in low-carbon and water-secure regions can cut combined footprints by nearly half. In general, the study concludes that sustainable AI expansion requires coordinated improvements in cooling efficiency, renewable energy integration, and strategic deployment decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06063v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aadi Patel, Nikhil Mahalingam, Rusheen Patel</dc:creator>
    </item>
    <item>
      <title>Socio-technical aspects of Agentic AI</title>
      <link>https://arxiv.org/abs/2601.06064</link>
      <description>arXiv:2601.06064v1 Announce Type: new 
Abstract: Agentic Artificial Intelligence (AI) represents a fundamental shift in the design of intelligent systems, characterized by interconnected components that collectively enable autonomous perception, reasoning, planning, action, and learning. Recent research on agentic AI has largely focused on technical foundations, including system architectures, reasoning and planning mechanisms, coordination strategies, and application-level performance across domains. However, the societal, ethical, economic, environmental, and governance implications of agentic AI remain weakly integrated into these technical treatments. This paper addresses this gap by presenting a socio-technical analysis of agentic AI that explicitly connects core technical components with societal context. We examine how architectural choices in perception, cognition, planning, execution, and memory introduce dependencies related to data governance, accountability, transparency, safety, and sustainability. To structure this analysis, we adopt the MAD-BAD-SAD construct as an analytical lens, capturing motivations, applications, and moral dilemmas (MAD); biases, accountability, and dangers (BAD); and societal impact, adoption, and design considerations (SAD). Using this lens, we analyze ethical considerations, implications, and challenges arising from contemporary agentic AI systems and assess their manifestation across emerging applications, including healthcare, education, industry, smart and sustainable cities, social services, communications and networking, and earth observation and satellite communications. The paper further identifies open challenges and suggests future research directions, framing agentic AI as an integrated socio-technical system whose behavior and impact are co-produced by algorithms, data, organizational practices, regulatory frameworks, and social norms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06064v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Praveen Kumar Donta, Alaa Saleh, Ying Li, Shubham Vaishnav, Kai Fang, Hailin Feng, Yuchao Xia, Thippa Reddy Gadekallu, Qiyang Zhang, Xiaodan Shi, Ali Beikmohammadi, Sindri Magn\'usson, Ilir Murturi, Chinmaya Kumar Dehury, Marcin Paprzycki, Lauri Loven, Sasu Tarkoma, Schahram Dustdar</dc:creator>
    </item>
    <item>
      <title>TEAS: Trusted Educational AI Standard: A Framework for Verifiable, Stable, Auditable, and Pedagogically Sound Learning Systems</title>
      <link>https://arxiv.org/abs/2601.06066</link>
      <description>arXiv:2601.06066v1 Announce Type: new 
Abstract: The rapid integration of AI into education has prioritized capability over trustworthiness, creating significant risks. Real-world deployments reveal that even advanced models are insufficient without extensive architectural scaffolding to ensure reliability. Current evaluation frameworks are fragmented: institutional policies lack technical verification, pedagogical guidelines assume AI reliability, and technical metrics are context-agnostic. This leaves institutions without a unified standard for deployment readiness. This paper introduces TEAS (Trusted Educational AI Standard), an integrated framework built on four interdependent pillars: (1) Verifiability, grounding content in authoritative sources; (2) Stability, ensuring deterministic core knowledge; (3) Auditability, enabling independent institutional validation; and (4) Pedagogical Soundness, enforcing principles of active learning. We argue that trustworthiness stems primarily from systematic architecture, not raw model capability. This insight implies that affordable, open-source models can achieve deployment-grade trust, offering a scalable and equitable path to integrating AI safely into learning environments globally.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06066v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abu Syed</dc:creator>
    </item>
    <item>
      <title>La norme technique comme catalyseur de transfert de connaissances : la francophonie a l'{\oe}uvre dans le domaine de l'{\'e}ducation</title>
      <link>https://arxiv.org/abs/2601.06069</link>
      <description>arXiv:2601.06069v1 Announce Type: new 
Abstract: Standards are adopted in a wide range of fields, both technical and industrial, as well as socio-economic, cultural and linguistic. They are presented explicitly as laws and regulations, technical and industrial standards or implicitly in the form of unwritten social standards. However, in a globalization marked by a very fine mosaic of socio-cultural identities, the question arises in relation to the construction of global, transparent and coherent systems in which considerable work of consensus is necessary to ensure all types of transfers and their local adaptations. The focus here is on the global education ecosystem which develops its own standards for the transfer of knowledge and socio-cultural values through learning, teaching and training. Subcommittee 36 of the International Organization for Standardization is one of the structures of this ecosystem in which the Francophonie participates to develop international standards for distance education on the basis of universal consensus.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06069v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Maurice Niwese; M{\'e}lanie Petit. Francophonies et transferts en langues et en {\'e}ducation, Presse Universitaire de Bordeaux, 2026, Francophonies plurielles, 979-10-300-1231-6</arxiv:journal_reference>
      <dc:creator>Mokhtar Ben Henda (MICA, ISD, GRESIC, ISIC, Chaire Unesco-ITEN)</dc:creator>
    </item>
    <item>
      <title>PDA in Action: Ten Principles for High-Quality Multi-Site Clinical Evidence Generation</title>
      <link>https://arxiv.org/abs/2601.06072</link>
      <description>arXiv:2601.06072v1 Announce Type: new 
Abstract: Background: Distributed Research Networks (DRNs) offer significant opportunities for collaborative multi-site research and have significantly advanced healthcare research based on clinical observational data. However, generating high-quality real-world evidence using fit-for-use data from multi-site studies faces important challenges, including biases associated with various types of heterogeneity within and across sites and data sharing difficulties. Over the last ten years, Privacy-Preserving Distributed Algorithms (PDA) have been developed and utilized in numerous national and international real-world studies spanning diverse domains, from comparative effectiveness research, target trial emulation, to healthcare delivery, policy evaluation, and system performance assessment. Despite these advances, there remains a lack of comprehensive and clear guiding principles for generating high-quality real-world evidence through collaborative studies leveraging the methods under PDA.
  Objective: The paper aims to establish ten principles of best practice for conducting high-quality multi-site studies using PDA. These principles cover all phases of research, including study preparation, protocol development, analysis, and final reporting.
  Discussion: The ten principles for conducting a PDA study outline a principled, efficient, and transparent framework for employing distributed learning algorithms within DRNs to generate reliable and reproducible real-world evidence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06072v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yong Chen, Jiayi Tong, Yiwen Lu, Rui Duan, Chongliang Luo, Marc A. Suchard, Patrick B. Ryan, Andrew E. Williams, John H. Holmes, Jason H. Moore, Hua Xu, Yun Lu, Raymond J. Carroll, Scott L. Zeger, George Hripcsak, Martijn J. Schuemie</dc:creator>
    </item>
    <item>
      <title>The AI Roles Continuum: Blurring the Boundary Between Research and Engineering</title>
      <link>https://arxiv.org/abs/2601.06087</link>
      <description>arXiv:2601.06087v1 Announce Type: new 
Abstract: The rapid scaling of deep neural networks and large language models has collapsed the once-clear divide between "research" and "engineering" in AI organizations. Drawing on a qualitative synthesis of public job descriptions, hiring criteria, and organizational narratives from leading AI labs and technology companies, we propose the AI Roles Continuum: a framework in which Research Scientists, Research Engineers, Applied Scientists, and Machine Learning Engineers occupy overlapping positions rather than discrete categories. We show that core competencies such as distributed systems design, large-scale training and optimization, rigorous experimentation, and publication-minded inquiry are now broadly shared across titles. Treating roles as fluid rather than siloed shortens research-to-production loops, improves iteration velocity, and strengthens organizational learning. We present a taxonomy of competencies mapped to common roles and discuss implications for hiring practices, career ladders, and workforce development in modern AI enterprises.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06087v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Deepak Babu Piskala</dc:creator>
    </item>
    <item>
      <title>Islamic Chatbots in the Age of Large Language Models</title>
      <link>https://arxiv.org/abs/2601.06092</link>
      <description>arXiv:2601.06092v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are rapidly transforming how communities access, interpret, and circulate knowledge, and religious communities are no exception. Chatbots powered by LLMs are beginning to reshape authority, pedagogy, and everyday religious practice in Muslim communities. We analyze the landscape of LLM powered Islamic chatbots and how they are transforming Islamic religious practices e.g., democratizing access to religious knowledge but also running the risk of erosion of authority. We discuss what kind of challenges do these systems raise for Muslim communities and explore recommendations for the responsible design of these systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06092v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammad Aurangzeb Ahmad</dc:creator>
    </item>
    <item>
      <title>GenAITEd Ghana_A Blueprint Prototype for Context-Aware and Region-Specific Conversational AI Agent for Teacher Education</title>
      <link>https://arxiv.org/abs/2601.06093</link>
      <description>arXiv:2601.06093v1 Announce Type: new 
Abstract: Global frameworks increasingly advocate for Responsible Artificial Intelligence (AI) in education, yet they provide limited guidance on how ethical, culturally responsive, and curriculum-aligned AI can be operationalized within functioning teacher education systems, particularly in the Global South. This study addresses this gap through the design and evaluation of GenAITEd Ghana, a context-aware, region-specific conversational AI prototype developed to support teacher education in Ghana. Guided by a Design Science Research approach, the system was developed as a school-mimetic digital infrastructure aligned with the organizational logic of Ghanaian Colleges of Education and the National Council for Curriculum and Assessment (NaCCA) framework. GenAITEd Ghana operates as a multi-agent, retrieval-augmented conversational AI that coordinates multiple models for curriculum-grounded dialogue, automatic speech recognition, voice synthesis, and multimedia interaction. Two complementary prompt pathways were embedded: system-level prompts that enforce curriculum boundaries, ethical constraints, and teacher-in-the-loop oversight, and interaction-level semi-automated prompts that structure live pedagogical dialogue through clarification, confirmation, and guided response generation. Evaluation findings show that the system effectively enacted key Responsible AI principles, including transparency, accountability, cultural responsiveness, privacy, and human oversight. Human expert evaluations further indicated that GenAITEd Ghana is pedagogically appropriate for Ghanaian teacher education, promoting student engagement while preserving educators' professional authority. Identified challenges highlight the need for continued model integration, professional development, and critical AI literacy to mitigate risks of over-reliance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06093v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Nyaaba, Patrick Kyeremeh, Macharious Nabang, Bismark Nyaaba Akanzire, Cyril Ababio Titty, Jerry Etornam Kudaya, Sakina Acquah</dc:creator>
    </item>
    <item>
      <title>How to Assess AI Literacy: Misalignment Between Self-Reported and Objective-Based Measures</title>
      <link>https://arxiv.org/abs/2601.06101</link>
      <description>arXiv:2601.06101v1 Announce Type: new 
Abstract: The widespread adoption of Artificial Intelligence (AI) in K-12 education highlights the need for psychometrically-tested measures of teachers' AI literacy. Existing work has primarily relied on either self-report (SR) or objective-based (OB) assessments, with few studies aligning the two within a shared framework to compare perceived versus demonstrated competencies or examine how prior AI literacy experience shapes this relationship. This gap limits the scalability of learning analytics and the development of learner profile-driven instructional design. In this study, we developed and evaluated SR and OB measures of teacher AI literacy within the established framework of Concept, Use, Evaluate, and Ethics. Confirmatory factor analyses support construct validity with good reliability and acceptable fit. Results reveal a low correlation between SR and OB factors. Latent profile analysis identified six distinct profiles, including overestimation (SR &gt; OB), underestimation (SR &lt; OB), alignment (SR close to OB), and a unique low-SR/low-OB profile among teachers without AI literacy experience. Theoretically, this work extends existing AI literacy frameworks by validating SR and OB measures on shared dimensions. Practically, the instruments function as diagnostic tools for professional development, supporting AI-informed decisions (e.g., growth monitoring, needs profiling) and enabling scalable learning analytics interventions tailored to teacher subgroups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06101v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shan Zhang, Ruiwei Xiao, Anthony F. Botelho, Guanze Liao, Thomas K. F. Chiu, John Stamper, Kenneth R. Koedinger</dc:creator>
    </item>
    <item>
      <title>Prompt Engineering for Responsible Generative AI Use in African Education: A Report from a Three-Day Training Series</title>
      <link>https://arxiv.org/abs/2601.06121</link>
      <description>arXiv:2601.06121v1 Announce Type: new 
Abstract: Generative artificial intelligence (GenAI) tools are increasingly adopted in education, yet many educators lack structured guidance on responsible and context sensitive prompt engineering, particularly in African and other resource constrained settings. This case report documents a three day online professional development programme organised by Generative AI for Education and Research in Africa (GenAI-ERA), designed to strengthen educators and researchers capacity to apply prompt engineering ethically for academic writing, teaching, and research. The programme engaged 468 participants across multiple African countries, including university educators, postgraduate students, and researchers. The training followed a scaffolded progression from foundational prompt design to applied and ethical strategies, including persona guided interactions. Data sources comprised registration surveys, webinar interaction records, facilitator observations, and session transcripts, analysed using descriptive statistics and computationally supported qualitative techniques. Findings indicate that participants increasingly conceptualised prompt engineering as a form of AI literacy requiring ethical awareness, contextual sensitivity, and pedagogical judgement rather than technical skill alone. The case highlights persistent challenges related to access, locally relevant training materials, and institutional support. The report recommends sustained professional development and the integration of prompt literacy into curricula to support responsible GenAI use in African education systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06121v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin Quarshie, Vanessa Willemse, Macharious Nabang, Bismark Nyaaba Akanzire, Patrick Kyeremeh, Saeed Maigari, Dorcas Adomina, Ellen Kwarteng, Eric Kojo Majialuwe, Craig Gibbs, Jerry Etornam Kudaya, Sechaba Koma, Matthew Nyaaba Matthew Nyaaba</dc:creator>
    </item>
    <item>
      <title>Graph-Based Analysis of AI-Driven Labor Market Transitions: Evidence from 10,000 Egyptian Jobs and Policy Implications</title>
      <link>https://arxiv.org/abs/2601.06129</link>
      <description>arXiv:2601.06129v1 Announce Type: new 
Abstract: How many workers displaced by automation can realistically transition to safer jobs? We answer this using a validated knowledge graph of 9,978 Egyptian job postings, 19,766 skill activities, and 84,346 job-skill relationships (0.74% error rate). While 20.9% of jobs face high automation risk, we find that only 24.4% of at-risk workers have viable transition pathways--defined by $\geq$3 shared skills and $\geq$50% skill transfer. The remaining 75.6% face a structural mobility barrier requiring comprehensive reskilling, not incremental upskilling. Among 4,534 feasible transitions, process-oriented skills emerge as the highest-leverage intervention, appearing in 15.6% of pathways. These findings challenge optimistic narratives of seamless workforce adaptation and demonstrate that emerging economies require active pathway creation, not passive skill matching.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06129v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ahmed Dawoud, Sondos Samir, Mahmoud Mohamed</dc:creator>
    </item>
    <item>
      <title>An evaluation of LLMs for political bias in Western media: Israel-Hamas and Ukraine-Russia wars</title>
      <link>https://arxiv.org/abs/2601.06132</link>
      <description>arXiv:2601.06132v1 Announce Type: new 
Abstract: Political bias in media plays a critical role in shaping public opinion, voter behaviour, and broader democratic discourse. Subjective opinions and political bias can be found in media sources, such as newspapers, depending on their funding mechanisms and alliances with political parties. Automating the detection of political biases in media content can limit biases in elections. The impact of large language models (LLMs) in politics and media studies is becoming prominent. In this study, we utilise LLMs to compare the left-wing, right-wing, and neutral political opinions expressed in the Guardian and BBC. We review newspaper reporting that includes significant events such as the Russia-Ukraine war and the Hamas-Israel conflict. We analyse the proportion for each opinion to find the bias under different LLMs, including BERT, Gemini, and DeepSeek. Our results show that after the outbreak of the wars, the political bias of Western media shifts towards the left-wing and each LLM gives a different result. DeepSeek consistently showed a stable Left-leaning tendency, while BERT and Gemini remained closer to the Centre. The BBC and The Guardian showed distinct reporting behaviours across the two conflicts. In the Russia-Ukraine war, both outlets maintained relatively stable positions; however, in the Israel-Hamas conflict, we identified larger political bias shifts, particularly in Guardian coverage, suggesting a more event-driven pattern of reporting bias. These variations suggest that LLMs are shaped not only by their training data and architecture, but also by underlying worldviews with associated political biases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06132v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rohitash Chandra, Haoyan Chen, Yaqing Zhang, Jiacheng Chen, Yuting Wu</dc:creator>
    </item>
    <item>
      <title>Perspective: The creation of "Newsgames" as a teaching method-Empirical observations</title>
      <link>https://arxiv.org/abs/2601.06139</link>
      <description>arXiv:2601.06139v1 Announce Type: new 
Abstract: This chapter reports an empirical teaching experience integrating newsgame creation-serious games addressing current events and contributing to public debate-into an introductory game design course for engineering students. From 2010 to 2012, around 80 students produced 17 games on diverse news topics (e.g., H1N1 influenza, Megaupload shutdown, Tunisian Revolution, Haiti earthquake), relying on online journalistic sources and using accessible development tools suited to mixed programming backgrounds (RPG Maker, The Games Factory 2, Flash, Java). The authors argue that designing newsgames fosters learning outcomes beyond technical design skills: (1) thorough information seeking and documentation of real-world issues, (2) exchange and confrontation of viewpoints through contrasting game interpretations of the same event, and (3) classroom debates about the legitimacy and limits of video games as an expressive medium for sensitive topics. The paper concludes that newsgame design can support the development of reasoning skills (evidence-based argumentation and perspective-taking) and suggests extending the approach to other serious game types to further explore its educational potential.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06139v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Educational Game Design Fundamentals: A journey to creating intrinsically motivating learning experiences, CRC Press Taylor \&amp; Francis Group, 2018, 9781138631540</arxiv:journal_reference>
      <dc:creator>Damien Djaouti (UM), Julian Alvarez (PRL)</dc:creator>
    </item>
    <item>
      <title>An LLM -Powered Assessment Retrieval-Augmented Generation (RAG) For Higher Education</title>
      <link>https://arxiv.org/abs/2601.06141</link>
      <description>arXiv:2601.06141v1 Announce Type: new 
Abstract: Providing timely, consistent, and high-quality feedback in large-scale higher education courses remains a persistent challenge, often constrained by instructor workload and resource limitations. This study presents an LLM-powered, agentic assessment system built on a Retrieval-Augmented Generation (RAG) architecture to address these challenges. The system integrates a large language model with a structured retrieval mechanism that accesses rubric criteria, exemplar essays, and instructor feedback to generate contextually grounded grades and formative comments. A mixed-methods evaluation was conducted using 701 student essays, combining quantitative analyses of inter-rater reliability, scoring alignment, and consistency with instructor assessments, alongside qualitative evaluation of feedback quality, pedagogical relevance, and student support. Results demonstrate that the RAG system can produce reliable, rubric-aligned feedback at scale, achieving 94--99% agreement with human evaluators, while also enhancing students' opportunities for self-regulated learning and engagement with assessment criteria. The discussion highlights both pedagogical limitations, including potential constraints on originality and feedback dialogue, and the transformative potential of RAG systems to augment instructors' capabilities, streamline assessment workflows, and support scalable, adaptive learning environments. This research contributes empirical evidence for the application of agentic AI in higher education, offering a scalable and pedagogically informed model for enhancing feedback accessibility, consistency, and quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06141v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Reza Vatankhah Barenji, Nazila Salimi, Sina Khoshgoftar</dc:creator>
    </item>
    <item>
      <title>The Patient/Industry Trade-off in Medical Artificial Intelligence</title>
      <link>https://arxiv.org/abs/2601.06144</link>
      <description>arXiv:2601.06144v1 Announce Type: new 
Abstract: Artificial intelligence (AI) in healthcare has led to many promising developments; however, increasingly, AI research is funded by the private sector leading to potential trade-offs between benefits to patients and benefits to industry. Health AI practitioners should prioritize successful adaptation into clinical practice in order to provide meaningful benefits to patients, but translation usually requires collaboration with industry. We discuss three features of AI studies that hamper the integration of AI into clinical practice from the perspective of researchers and clinicians. These include lack of clinically relevant metrics, lack of clinical trials and longitudinal studies to validate results, and lack of patient and physician involvement in the development process. For partnerships between industry and health research to be sustainable, a balance must be established between patient and industry benefit. We propose three approaches for addressing this gap: improved transparency and explainability of AI models, fostering relationships with industry partners that have a reputation for centering patient benefit in their practices, and prioritization of overall healthcare benefits. With these priorities, we can sooner realize meaningful AI technologies used by clinicians where mutua</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06144v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s43681-025-00936-w</arxiv:DOI>
      <dc:creator>Rina Khan, Annabelle Sauve, Imaan Bayoumi, Amber L. Simpson, Catherine Stinson</dc:creator>
    </item>
    <item>
      <title>Bridging the AI divide in sub-Saharan Africa: Challenges and opportunities for inclusivity</title>
      <link>https://arxiv.org/abs/2601.06145</link>
      <description>arXiv:2601.06145v1 Announce Type: new 
Abstract: The artificial intelligence (AI) digital divide in sub-Saharan Africa (SSA) presents significant disparities in AI access, adoption, and development due to varying levels of infrastructure, education, and policy support. This study investigates the extent of AI readiness among the top SSA countries using the 2024 Government AI Readiness Index, alongside an analysis of AI initiatives to foster inclusivity. A comparative analysis of AI readiness scores highlights disparities across nations, with Mauritius (53.94) and South Africa (52.91) leading, while Zambia (42.58) and Uganda (43.32) lag. Quartile analysis reveals a concentration of AI preparedness among a few nations, suggesting uneven AI development. The study further examines the relationship between AI readiness and economic indicators, identifying instances where AI progress does not strictly correlate with Gross Domestic Product per capita, as seen in Rwanda and Uganda. Using case studies of AI initiatives across SSA, this research contextualises quantitative findings, identifying key strategies contributing to AI inclusivity, including talent development programs, research networks, and policy interventions. The study concludes with recommendations to bridge the AI digital divide, emphasising investments in AI education, localised AI solutions, and cross-country collaborations to accelerate AI adoption in SSA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06145v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Masike Malatji</dc:creator>
    </item>
    <item>
      <title>Interoperability in AI Safety Governance: Ethics, Regulations, and Standards</title>
      <link>https://arxiv.org/abs/2601.06153</link>
      <description>arXiv:2601.06153v1 Announce Type: new 
Abstract: This policy report draws on country studies from China, South Korea, Singapore, and the United Kingdom to identify effective tools and key barriers to interoperability in AI safety governance. It offers practical recommendations to support a globally informed yet locally grounded governance ecosystem. Interoperability is a central goal of AI governance, vital for reducing risks, fostering innovation, enhancing competitiveness, promoting standardization, and building public trust. However, structural gaps such as fragmented regulations and lack of global coordination, and conceptual gaps, including limited Global South engagement, continue to hinder progress. Focusing on three high-stakes domains - autonomous vehicles, education, and cross-border data flows - the report compares ethical, legal, and technical frameworks across the four countries. It identifies areas of convergence, divergence, and potential alignment, offering policy recommendations that support the development of interoperability mechanisms aligned with the Global Digital Compact and relevant UN resolutions. The analysis covers seven components: objectives, regulators, ethics, binding measures, targeted frameworks, technical standards, and key risks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06153v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yik Chan Chin, David A. Raho, Hag-Min Kim, Chunli Bi, James Ong, Jingbo Huang, Serge Stinckwich</dc:creator>
    </item>
    <item>
      <title>BotSim: Mitigating The Formation Of Conspiratorial Societies with Useful Bots</title>
      <link>https://arxiv.org/abs/2601.06154</link>
      <description>arXiv:2601.06154v1 Announce Type: new 
Abstract: Societies can become a conspiratorial society where there is a majority of humans that believe, and therefore spread, conspiracy theories. Artificial intelligence gave rise to social media bots that can spread conspiracies in an automated fashion. Currently, organizations combat the spread of conspiracies through manual fact-checking processes and the dissemination of counter-narratives. However, the effects of harnessing the same automation to create useful bots are not well explored. To address this, we create BotSim, an Agent-Based Model of a society in which useful bots are introduced into a small world network. These useful bots are: Info-Correction Bots, which correct bad information into good, and Good Bots, which put out good messaging. The simulated agents interact through generating, consuming and propagating information. Our results show that, left unchecked, Bad Bots can create a conspiratorial society, and this can be mitigated by either Info-Correction Bots or Good Bots; however, Good Bots are more efficient and sustainable than Info-Correction Bots . Proactive good messaging is more resource-effective than reactive information correction. With our observations, we expand the concept of bots as a malicious social media agent towards automated social media agent that can be used for both good and bad purposes. These results have implications for designing communication strategies to maintain a healthy social cyber ecosystem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06154v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.SI</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.18564/jasss.5881</arxiv:DOI>
      <arxiv:journal_reference>Journal of Artificial Societies and Social Simulation 29 (1) 4. 2026</arxiv:journal_reference>
      <dc:creator>Lynnette Hui Xian Ng, Kathleen M. Carley</dc:creator>
    </item>
    <item>
      <title>From Individual Prompts to Collective Intelligence: Mainstreaming Generative AI in the Classroom</title>
      <link>https://arxiv.org/abs/2601.06171</link>
      <description>arXiv:2601.06171v1 Announce Type: new 
Abstract: Engineering classrooms are increasingly experimenting with generative AI (GenAI), but most uses remain confined to individual prompting and isolated assistance. This narrow framing risks reinforcing equity gaps and only rewarding the already privileged or motivated students. We argue instead for a shift toward collective intelligence (CI)-focused pedagogy, where GenAI acts as a catalyst for peer-to-peer learning. We implemented Generative CI (GCI) activities in two undergraduate engineering courses, engaging 140 students through thinking routines -- short, repeatable scaffolds developed by Harvard Project Zero to make thinking visible and support collaborative sense-making. Using routines such as Question Sorts and Peel the Fruit, combined with strategic AI consultation, we enabled students to externalize their reasoning, compare interpretations, and iteratively refine ideas. Our dual-pronged approach synthesizes literature from learning sciences, CI, embodied cognition, and philosophy of technology, while also empirically learning through student surveys and engagement observations. Results demonstrate that students value the combination of human collaboration with strategic AI support, recognizing risks of over-reliance while appreciating AI's role in expanding perspectives. Students identified that group work fosters deeper understanding and creative problem-solving than AI alone, with the timing of AI consultation significantly affecting learning outcomes. We offer practical implementation pathways for mainstreaming CI-focused pedagogy that cultivates deeper engagement, resilient problem-solving, and shared ownership of knowledge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06171v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junaid Qadir, Muhammad Salman Khan</dc:creator>
    </item>
    <item>
      <title>The Psychology of Learning from Machines: Anthropomorphic AI and the Paradox of Automation in Education</title>
      <link>https://arxiv.org/abs/2601.06172</link>
      <description>arXiv:2601.06172v1 Announce Type: new 
Abstract: As AI tutors enter classrooms at unprecedented speed, their deployment increasingly outpaces our grasp of the psychological and social consequences of such technology. Yet decades of research in automation psychology, human factors, and human-computer interaction provide crucial insights that remain underutilized in educational AI design. This work synthesizes four research traditions -- automation psychology, human factors engineering, HCI, and philosophy of technology -- to establish a comprehensive framework for understanding how learners psychologically relate to anthropomorphic AI tutors. We identify three persistent challenges intensified by Generative AI's conversational fluency. First, learners exhibit dual trust calibration failures -- automation bias (uncritical acceptance) and algorithm aversion (excessive rejection after errors) -- with an expertise paradox where novices overrely while experts underrely. Second, while anthropomorphic design enhances engagement, it can distract from learning and foster harmful emotional attachment. Third, automation ironies persist: systems meant to aid cognition introduce designer errors, degrade skills through disuse, and create monitoring burdens humans perform poorly. We ground this theoretical synthesis through comparative analysis of over 104,984 YouTube comments across AI-generated philosophical debates and human-created engineering tutorials, revealing domain-dependent trust patterns and strong anthropomorphic projection despite minimal cues. For engineering education, our synthesis mandates differentiated approaches: AI tutoring for technical foundations where automation bias is manageable through proper scaffolding, but human facilitation for design, ethics, and professional judgment where tacit knowledge transmission proves irreplaceable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06172v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junaid Qadir, Muhammad Mumtaz</dc:creator>
    </item>
    <item>
      <title>The environmental impact of ICT in the era of data and artificial intelligence</title>
      <link>https://arxiv.org/abs/2601.06174</link>
      <description>arXiv:2601.06174v1 Announce Type: new 
Abstract: The technology industry promotes artificial intelligence (AI) as a key enabler to solve a vast number of problems, including the environmental crisis. However, when looking at the emissions of datacenters from worldwide service providers, we observe a rapid increase aligned with the advent of AI. Some actors justify it by claiming that the increase of emissions for digital infrastructures is acceptable as it could help the decarbonization of other sectors, e.g., videoconference tools instead of taking the plane for a meeting abroad, or using AI to optimize and reduce energy consumption. With such conflicting claims and ambitions, it is unclear how the net environmental impact of AI could be quantified. The answer is prone to uncertainty for different reasons, among others: lack of transparency, interference with market expectations, lack of standardized methodology for quantifying direct and indirect impact, and the quick evolutions of models and their requirements.
  This report provides answers and clarifications to these different elements. Firstly, we consider the direct environmental impact of AI from a top-down approach, starting from general information and communication technologies (ICT) and then zooming in on data centers and the different phases of AI development and deployment. Secondly, a framework is introduced on how to assess both the direct and indirect impact of AI. Finally, we finish with good practices and what we can do to reduce AI impact.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06174v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fran\c{c}ois Rottenberg, Thomas Feys, Liesbet Van der Perre</dc:creator>
    </item>
    <item>
      <title>A Mixed Methods Systematic Analysis of Issues and Factors Influencing Organizational Cloud Computing Adoption and Usage in the Public Sector: Initial Findings</title>
      <link>https://arxiv.org/abs/2601.06175</link>
      <description>arXiv:2601.06175v1 Announce Type: new 
Abstract: Cloud computing has been shown to be an essential enabling technology for public sector organizations PSOs and offers numerous potential benefits, including reduced information technology infrastructure costs, increased innovation potential, and improved resource resilience and scalability. Despite governments' intensifying efforts to realize the benefits of this technology, cloud computing adoption and usage proves to be challenging, posing a variety of organizational and operational issues for PSOs. This systematic analysis constitutes the initial phase of a larger research effort that involves forthcoming case studies of specific public sector cloud stakeholders; it aims to identify and synthesize the available knowledge on organizational cloud computing adoption and utilization in the public sector to provide public sector decision makers and stakeholders with reliable, evidence-based, actionable insights that inform and improve public sector IT practice and policy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06175v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5281/zenodo.7057509</arxiv:DOI>
      <arxiv:journal_reference>International Journal of Computer Science and Information Technology Research, Vol. 10, Issue 3, pp: (62-75), Month: July - September 2022</arxiv:journal_reference>
      <dc:creator>Mark Theby</dc:creator>
    </item>
    <item>
      <title>Performance of models for monitoring sustainable development goals from remote sensing: A three-level meta-regression</title>
      <link>https://arxiv.org/abs/2601.06178</link>
      <description>arXiv:2601.06178v1 Announce Type: new 
Abstract: Machine learning (ML) is a tool to exploit remote sensing data for the monitoring and implementation of the United Nations' Sustainable Development Goals (SDGs). In this paper, we report on a meta-analysis to evaluate the performance of ML applied to remote sensing data to monitor SDGs. Specifically, we aim to 1) estimate the average performance; 2) determine the degree of heterogeneity between and within studies; and 3) assess how study features influence model performance. Using PRISMA guidelines, a search was performed across multiple academic databases to identify potentially relevant studies. A random sample of 200 was screened by three reviewers, resulting in 86 trials within 20 studies with 14 study features. Overall accuracy was the most reported performance metric. It was analyzed using double arcsine transformation and a three-level random effects model. The average overall accuracy of the best model was 0.90 [0.86, 0.92]. There was considerable heterogeneity in model performance, 64% of which was between studies. The only significant feature was the prevalence of the majority class, which explained 61% of the between-study heterogeneity. None of the other thirteen features added value to the model. The most important contributions of this paper are the following two insights. 1) Overall accuracy is the most popular performance metric, yet arguably the least insightful. Its sensitivity to class imbalance makes it necessary to normalize it, which is far from common practice. 2) The field needs to standardize the reporting. Reporting of the confusion matrix for independent test sets is the most important ingredient for between-study comparisons of ML classifiers. These findings underscore the need for robust and comparable evaluation metrics in machine learning applications to ensure reliable and actionable insights for effective SDG monitoring and policy formulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06178v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas Klingwort, Nina M. Leach, Joep Burger</dc:creator>
    </item>
    <item>
      <title>Geo-Standardizing 3D Modeling of Surface Objects and Related Logical Spaces on Celestial Bodies: Case Studies for Moon and Mars</title>
      <link>https://arxiv.org/abs/2601.06182</link>
      <description>arXiv:2601.06182v1 Announce Type: new 
Abstract: Establishing frameworks for promoting the realization of various activities on celestial bodies sustainably is of great significance for different contexts, such as preserving the scientific evidence and space heritage. Therefore, this research first proposes a conceptual model that covers the different types of features, attributes, and relationships between them to comprehensively delineate the surface objects and related logical spaces on celestial bodies. It then implements this conceptual model as a CityJSON extension in such a way that allows for creating the three-dimensional (3D) geodatasets that represent these objects and spaces in a standardized manner. Moreover, the usefulness of this study is demonstrated through creating CityJSON datasets that include 3D models of exemplary surface objects from Moon and Mars, such as a historical landing site and related logical spaces, such as exclusion zones for protecting this site. The results of the current study show that there is a strong potential for forming 3D geodatasets on celestial bodies that can provide a notable foundation for the technical implementation of international agreements and legal frameworks. This work also contributes to the design of planetary spatial data infrastructures (PSDI) by incorporating the third dimension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06182v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dogus Guler, Demet Cilden-Guler</dc:creator>
    </item>
    <item>
      <title>Political Alignment in Large Language Models: A Multidimensional Audit of Psychometric Identity and Behavioral Bias</title>
      <link>https://arxiv.org/abs/2601.06194</link>
      <description>arXiv:2601.06194v1 Announce Type: new 
Abstract: As large language models (LLMs) are increasingly integrated into social decision-making, understanding their political positioning and alignment behavior is critical for safety and fairness. This study presents a sociotechnical audit of 26 prominent LLMs, triangulating their positions across three psychometric inventories (Political Compass, SapplyValues, 8 Values) and evaluating their performance on a large-scale news labeling task ($N \approx 27{,}000$). Our results reveal a strong clustering of models in the Libertarian-Left region of the ideological space, encompassing 96.3% of the cohort. Alignment signals appear to be consistent architectural traits rather than stochastic noise ($\eta^2 &gt; 0.90$); however, we identify substantial discrepancies in measurement validity. In particular, the Political Compass exhibits a strong negative correlation with cultural progressivism ($r=-0.64$) when compared against multi-axial instruments, suggesting a conflation of social conservatism with authoritarianism in this context. We further observe a significant divergence between open-weights and closed-source models, with the latter displaying markedly higher cultural progressivism scores ($p&lt;10^{-25}$). In downstream media analysis, models exhibit a systematic "center-shift," frequently categorizing neutral articles as left-leaning, alongside an asymmetric detection capability in which "Far Left" content is identified with greater accuracy (19.2%) than "Far Right" content (2.0%). These findings suggest that single-axis evaluations are insufficient and that multidimensional auditing frameworks are necessary to characterize alignment behavior in deployed LLMs. Our code and data will be made public.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06194v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Adib Sakhawat, Tahsin Islam, Takia Farhin, Syed Rifat Raiyan, Hasan Mahmud, Md Kamrul Hasan</dc:creator>
    </item>
    <item>
      <title>Towards Public Administration Research Based on Interpretable Machine Learning</title>
      <link>https://arxiv.org/abs/2601.06205</link>
      <description>arXiv:2601.06205v1 Announce Type: new 
Abstract: Causal relationships play a pivotal role in research within the field of public administration. Ensuring reliable causal inference requires validating the predictability of these relationships, which is a crucial precondition. However, prediction has not garnered adequate attention within the realm of quantitative research in public administration and the broader social sciences. The advent of interpretable machine learning presents a significant opportunity to integrate prediction into quantitative research conducted in public administration. This article delves into the fundamental principles of interpretable machine learning while also examining its current applications in social science research. Building upon this foundation, the article further expounds upon the implementation process of interpretable machine learning, encompassing key aspects such as dataset construction, model training, model evaluation, and model interpretation. Lastly, the article explores the disciplinary value of interpretable machine learning within the field of public administration, highlighting its potential to enhance the generalization of inference, facilitate the selection of optimal explanations for phenomena, stimulate the construction of theoretical hypotheses, and provide a platform for the translation of knowledge. As a complement to traditional causal inference methods, interpretable machine learning ushers in a new era of credibility in quantitative research within the realm of public administration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06205v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhanyu Liu, Yang Yu</dc:creator>
    </item>
    <item>
      <title>LLM Agents in Law: Taxonomy, Applications, and Challenges</title>
      <link>https://arxiv.org/abs/2601.06216</link>
      <description>arXiv:2601.06216v1 Announce Type: new 
Abstract: Large language models (LLMs) have precipitated a dramatic improvement in the legal domain, yet the deployment of standalone models faces significant limitations regarding hallucination, outdated information, and verifiability. Recently, LLM agents have attracted significant attention as a solution to these challenges, utilizing advanced capabilities such as planning, memory, and tool usage to meet the rigorous standards of legal practice. In this paper, we present a comprehensive survey of LLM agents for legal tasks, analyzing how these architectures bridge the gap between technical capabilities and domain-specific needs. Our major contributions include: (1) systematically analyzing the technical transition from standard legal LLMs to legal agents; (2) presenting a structured taxonomy of current agent applications across distinct legal practice areas; (3) discussing evaluation methodologies specifically for agentic performance in law; and (4) identifying open challenges and outlining future directions for developing robust and autonomous legal assistants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06216v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuang Liu, Ruijia Zhang, Ruoyun Ma, Yujia Deng, Lanyi Zhu, Jiayu Li, Zelong Li, Zhibin Shen, Mengnan Du</dc:creator>
    </item>
    <item>
      <title>Toward Safe and Responsible AI Agents: A Three-Pillar Model for Transparency, Accountability, and Trustworthiness</title>
      <link>https://arxiv.org/abs/2601.06223</link>
      <description>arXiv:2601.06223v1 Announce Type: new 
Abstract: This paper presents a conceptual and operational framework for developing and operating safe and trustworthy AI agents based on a Three-Pillar Model grounded in transparency, accountability, and trustworthiness. Building on prior work in Human-in-the-Loop systems, reinforcement learning, and collaborative AI, the framework defines an evolutionary path toward autonomous agents that balances increasing automation with appropriate human oversight. The paper argues that safe agent autonomy must be achieved through progressive validation, analogous to the staged development of autonomous driving, rather than through immediate full automation. Transparency and accountability are identified as foundational requirements for establishing user trust and for mitigating known risks in generative AI systems, including hallucinations, data bias, and goal misalignment, such as the inversion problem. The paper further describes three ongoing work streams supporting this framework: public deliberation on AI agents conducted by the Stanford Deliberative Democracy Lab, cross-industry collaboration through the Safe AI Agent Consortium, and the development of open tooling for an agent operating environment aligned with the Three-Pillar Model. Together, these contributions provide both conceptual clarity and practical guidance for enabling the responsible evolution of AI agents that operate transparently, remain aligned with human values, and sustain societal trust.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06223v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edward C. Cheng, Jeshua Cheng, Alice Siu</dc:creator>
    </item>
    <item>
      <title>Classroom AI: Large Language Models as Grade-Specific Teachers</title>
      <link>https://arxiv.org/abs/2601.06225</link>
      <description>arXiv:2601.06225v1 Announce Type: new 
Abstract: Large Language Models (LLMs) offer a promising solution to complement traditional teaching and address global teacher shortages that affect hundreds of millions of children, but they fail to provide grade-appropriate responses for students at different educational levels. We introduce a framework for finetuning LLMs to generate age-appropriate educational content across six grade levels, from lower elementary to adult education. Our framework successfully adapts explanations to match students' comprehension capacities without sacrificing factual correctness. This approach integrates seven established readability metrics through a clustering method and builds a comprehensive dataset for grade-specific content generation. Evaluations across multiple datasets with 208 human participants demonstrate substantial improvements in grade-level alignment, achieving a 35.64 percentage point increase compared to prompt-based methods while maintaining response accuracy. AI-assisted learning tailored to different grade levels has the potential to advance educational engagement and equity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06225v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jio Oh, Steven Euijong Whang, James Evans, Jindong Wang</dc:creator>
    </item>
    <item>
      <title>Data-Dependent Goal Modeling for ML-Enabled Law Enforcement Systems</title>
      <link>https://arxiv.org/abs/2601.06237</link>
      <description>arXiv:2601.06237v1 Announce Type: new 
Abstract: Investigating serious crimes is inherently complex and resource-constrained. Law enforcement agencies (LEAs) grapple with overwhelming volumes of offender and incident data, making effective suspect identification difficult. Although machine learning (ML)-enabled systems have been explored to support LEAs, several have failed in practice. This highlights the need to align system behavior with stakeholder goals early in development, motivating the use of Goal-Oriented Requirements Engineering (GORE).
  This paper reports our experience applying the GORE framework KAOS to designing an ML-enabled system for identifying suspects in online child sexual abuse. We describe how KAOS supported early requirements elaboration, including goal refinement, object modeling, agent assignment, and operationalization. A key finding is the central role of data elicitation: data requirements constrain refinement choices and candidate agents while influencing how goals are linked, operationalized, and satisfied. Conversely, goal elaboration and agent assignment shape data quality expectations and collection needs.
  Our experience highlights the iterative, bidirectional dependencies between goals, data, and ML performance. We contribute a reference model for integrating GORE with data-driven system development, and identify gaps in KAOS, particularly the need for explicit support for data elicitation and quality management. These insights inform future extensions of KAOS and, more broadly, the application of formal GORE methods to ML-enabled systems for high-stakes societal contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06237v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Dalal Alrajeh, Vesna Nowack, Patrick Benjamin, Katie Thomas, William Hobson, Carolina Gutierrez Mu\~noz, Catherine Hamilton-Giachritsis, Juliane A. Kloess, Jessica Woodhams, Daniel Butler, Mark Law, Ralph Morton, Benjamin Costello, Amy Burrell, Tim Grant, Prachiben Shah, Frances Laureano de Leon, Mark Lee</dc:creator>
    </item>
    <item>
      <title>FairSCOSCA: Fairness At Arterial Signals -- Just Around The Corner</title>
      <link>https://arxiv.org/abs/2601.06275</link>
      <description>arXiv:2601.06275v1 Announce Type: new 
Abstract: Traffic signal control at intersections, especially in arterial networks, is a key lever for mitigating the growing issue of traffic congestion in cities. Despite the widespread deployment of SCOOTS and SCATS, which prioritize efficiency, fairness has remained largely absent from their design logic, often resulting in unfair outcomes for certain road users, such as excessive waiting times. Fairness however, is a major driver of public acceptance for implementation of new controll systems. Therefore, this work proposes FairSCOSCA, a fairness-enhancing extension to these systems, featuring two novel yet practical design adaptations grounded in multiple normative fairness definitions: (1) green phase optimization incorporating cumulative waiting times, and (2) early termination of underutilized green phases. Those extensions ensure fairer distributions of green times. Evaluated in a calibrated microsimulation case study of the arterial network in Esslingen am Neckar (Germany), FairSCOSCA demonstrates substantial improvements across multiple fairness dimensions (Egalitarian, Rawlsian, Utilitarian, and Harsanyian) without sacrificing traffic efficiency. Compared against Fixed-Cycle, Max-Pressure, and standard SCOOTS/SCATS controllers, FairSCOSCA significantly reduces excessive waiting times, delay inequality and horizontal discrimination between arterial and feeder roads. This work contributes to the growing literature on equitable traffic control by bridging the gap between fairness theory and the practical enhancement of globally deployed signal systems. Open source implementation available on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06275v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kevin Riehl, Justin Weiss, Anastasios Kouvelas, Michail A. Makridis</dc:creator>
    </item>
    <item>
      <title>C-EQ-ALINEA: Distributed, Coordinated, and Equitable Ramp Metering Strategy for Sustainable Freeway Operations</title>
      <link>https://arxiv.org/abs/2601.06311</link>
      <description>arXiv:2601.06311v1 Announce Type: new 
Abstract: Ramp metering is a widely deployed traffic management strategy for improving freeway efficiency, yet conventional approaches often lead to highly uneven delay distributions across on-ramps, undermining user acceptance and long-term sustainability. While existing fairness-aware ramp metering methods can mitigate such disparities, they typically rely on centralized optimization, detailed traffic models, or data-intensive learning frameworks, limiting their real-world applicability, particularly in networks operating legacy ALINEA-based systems. This paper proposes C-EQ-ALINEA, a decentralized, coordinated, and equity-aware extension of the classical ALINEA feedback controller. The approach introduces lightweight information exchange among neighbouring ramps, enabling local coordination that balances congestion impacts without centralized control, additional infrastructure, or complex optimization. C-EQ-ALINEA preserves the simplicity and robustness of ALINEA while explicitly addressing multiple notions of fairness, including Harsanyian, Egalitarian, Rawlsian, and Aristotelian perspectives. The method is evaluated in a calibrated 24-hour microsimulation of Amsterdam's A10 ring road using SUMO. Results demonstrate that C-EQ-ALINEA substantially improves the equity of delay distributions across ramps and users, while maintaining (in several configurations surpassing) the efficiency of established coordinated strategies such as METALINE. These findings indicate that meaningful fairness gains can be achieved through minimal algorithmic extensions to widely deployed controllers, offering a practical and scalable pathway toward sustainable and socially acceptable freeway operations. Open source implementation available on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06311v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kevin Riehl, Omar Alami Badissi, Anastasios Kouvelas, Michail A. Makridis</dc:creator>
    </item>
    <item>
      <title>Brokerage in the Black Box: Swing States, Strategic Ambiguity, and the Global Politics of AI Governance</title>
      <link>https://arxiv.org/abs/2601.06412</link>
      <description>arXiv:2601.06412v1 Announce Type: new 
Abstract: The U.S. - China rivalry has placed frontier dual-use technologies, particularly Artificial Intelligence (AI), at the center of global power dynamics, as techno-nationalism, supply chain securitization, and competing standards deepen bifurcation within a weaponized interdependence that blurs civilian-military boundaries. Existing research, yet, mostly emphasizes superpower strategies and often overlooks the role of middle powers as autonomous actors shaping the techno-order. This study examines Technological Swing States (TSS), middle powers with both technological capacity and strategic flexibility, and their ability to navigate the frontier technologies' uncertainty and opacity to mediate great-power techno-competition regionally and globally. It reconceptualizes AI opacity not as a technical deficit, but as a structural feature and strategic resource, stemming from algorithmic complexity, political incentives that prioritize performance over explainability, and the limits of post-hoc interpretability. This structural opacity shifts authority from technical demands for explainability to institutional mechanisms, such as certification, auditing, and disclosure, converting technical constraints into strategic political opportunities. Drawing on case studies of South Korea, Singapore, and India, the paper theorizes how TSS exploit the interplay between opacity and institutional transparency through three strategies: (i) delay and hedging, (ii) selective alignment, and (iii) normative intermediation. These practices enable TSS to preserve strategic flexibility, build trust among diverse stakeholders, and broker convergence across competing governance regimes, thereby influencing institutional design, interstate bargaining, and policy outcomes in global AI governance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06412v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ha-Chi Tran</dc:creator>
    </item>
    <item>
      <title>A Framework for Kara-Kichwa Data Sovereignty in Latin America and the Caribbean</title>
      <link>https://arxiv.org/abs/2601.06634</link>
      <description>arXiv:2601.06634v1 Announce Type: new 
Abstract: In the high-altitude territories of the Andean-Amazonian-Atlantic pathway, data is not merely a digital resource but an extension of Khipu Panaka, the genealogical and relational memory of the Kara-Kichwa Republics. This perspective paper introduces the Kara-Kichwa Data Sovereignty Framework, a living instrument designed to counteract the "intellectual gentrification" and systemic invisibility of Andean Indigenous Peoples in global data ecosystems. Grounded in Indigenous legal systems thinking, the framework codifies five customary pillars, Kamachy (Self-determination), Ayllu-llaktapak kamachy (Collective Authority), Tantanakuy (Relational Accountability), Willay-panka-tantay (Ancestral Memory), and Sumak Kawsay (Biocultural Ethics), to govern the lifecycle of data from generation to expiration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06634v1</guid>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>WariNkwi K. Flores, KunTikzi Flores, Rosa M. Panama, KayaKanti Alta</dc:creator>
    </item>
    <item>
      <title>Otimizando A Aloca\c{c}\~ao De Salas De Aula Com Foco Na Acessibilidade Para Pessoas Com Defici\^encia</title>
      <link>https://arxiv.org/abs/2601.06670</link>
      <description>arXiv:2601.06670v1 Announce Type: new 
Abstract: This paper addresses the challenge of classroom allocation in higher education institutions, with an explicit emphasis on accessibility for Persons with Disabilities (PwDs). Employing a case study of a university's computer science department, the paper proposes an Integer Linear Programming (ILP)-based optimization model, which is solved using the Gurobi solver. The objective is to minimize the number of classrooms used by prioritizing the assignment of PwD students to ground-floor classrooms to reduce accessibility barriers. The model is calibrated with a weighting parameter, alpha, that allows for a balance between spatial efficiency and promoting accessibility. Experimental results indicate that adjusting alpha can achieve a balance point that significantly improves current manual allocation practices, reducing the number of classrooms required and accessibility penalties. The findings suggest that optimization methods can improve operational efficiency in academic institutions while promoting a more inclusive environment for all students. Future work may expand the application of the model to other departments and contexts and integrate additional criteria to develop a more holistic approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06670v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francisco Glaubos Nunes Cl\'imaco, Jorge Lucas Silva Cavalcante</dc:creator>
    </item>
    <item>
      <title>The Case for Strategic Data Stewardship: Re-imagining Data Governance to Make Responsible Data Re-use Possible</title>
      <link>https://arxiv.org/abs/2601.06687</link>
      <description>arXiv:2601.06687v1 Announce Type: new 
Abstract: As societal challenges grow more complex, access to data for public interest use is paradoxically becoming more constrained. This emerging data winter is not simply a matter of scarcity, but of shrinking legitimate and trusted pathways for responsible data reuse. Concerns over misuse, regulatory uncertainty, and the competitive race to train AI systems have concentrated data access among a few actors while raising costs and inhibiting collaboration. Prevailing data governance models, focused on compliance, risk management, and internal control, are necessary but insufficient. They often result in data that is technically available yet practically inaccessible, legally shareable yet institutionally unusable, or socially illegitimate to deploy. This paper proposes strategic data stewardship as a complementary institutional function designed to systematically, sustainably, and responsibly activate data for public value. Unlike traditional stewardship, which tends to be inwardlooking, strategic data stewardship focuses on enabling cross sector reuse, reducing missed opportunities, and building durable, ecosystem-level collaboration. It outlines core principles, functions, and competencies, and introduces a practical Data Stewardship Canvas to support adoption across contexts such as data collaboratives, data spaces, and data commons. Strategic data stewardship, the paper argues, is essential in the age of AI: it translates governance principles into practice, builds trust across data ecosystems, and ensures that data are not only governed, but meaningfully mobilized to serve society.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06687v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Stefaan Verhulst</dc:creator>
    </item>
    <item>
      <title>Mapping and Comparing Climate Equity Policy Practices Using RAG LLM-Based Semantic Analysis and Recommendation Systems</title>
      <link>https://arxiv.org/abs/2601.06703</link>
      <description>arXiv:2601.06703v1 Announce Type: new 
Abstract: This study investigates the use of large language models to enhance the policymaking process. We first analyze planning-related job postings to revisit the evolving roles of planners in the era of AI. We then examine climate equity plans across the U.S. and apply ChatGPT to conduct semantic analysis, extracting policy, strategy, and action items related to transportation and energy. The methodological framework relied on a LangChain-native retrieval-augmented generation pipeline. Based on these extracted elements and their evaluated presence, we develop a content-based recommendation system to support cross-city policy comparison. The results indicate that, despite growing attention to AI, planning jobs largely retain their traditional domain emphases in transportation, environmental planning, housing, and land use. Communicative responsibilities remain central to planning practice. Climate equity plans commonly address transportation, environmental, and energy-related measures aimed at reducing greenhouse gas emissions and predominantly employ affirmative language. The demonstration of the recommendation system illustrates how planners can efficiently identify cities with similar policy practices, revealing patterns of geographic similarity in policy adoption. The study concludes by envisioning localized yet personalized AI-assisted systems that can be adapted within urban systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06703v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seung Jun Choi</dc:creator>
    </item>
    <item>
      <title>On Narrative: The Rhetorical Mechanisms of Online Polarisation</title>
      <link>https://arxiv.org/abs/2601.07398</link>
      <description>arXiv:2601.07398v1 Announce Type: new 
Abstract: Polarisation research has demonstrated how people cluster in homogeneous groups with opposing opinions. However, this effect emerges not only through interaction between people, limiting communication between groups, but also between narratives, shaping opinions and partisan identities. Yet, how polarised groups collectively construct and negotiate opposing interpretations of reality, and whether narratives move between groups despite limited interactions, remains unexplored. To address this gap, we formalise the concept of narrative polarisation and demonstrate its measurement in 212 YouTube videos and 90,029 comments on the Israeli-Palestinian conflict. Based on structural narrative theory and implemented through a large language model, we extract the narrative roles assigned to central actors in two partisan information environments. We find that while videos produce highly polarised narratives, comments significantly reduce narrative polarisation, harmonising discourse on the surface level. However, on a deeper narrative level, recurring narrative motifs reveal additional differences between partisan groups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07398v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <category>cs.SI</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jan Elfes, Marco Bastos, Luca Maria Aiello</dc:creator>
    </item>
    <item>
      <title>Fifteen Years of Learning Analytics Research: Topics, Trends, and Challenges</title>
      <link>https://arxiv.org/abs/2601.07629</link>
      <description>arXiv:2601.07629v1 Announce Type: new 
Abstract: The learning analytics (LA) community has recently reached two important milestones: celebrating the 15th LAK conference and updating the 2011 definition of LA to reflect the 15 years of changes in the discipline. However, despite LA's growth, little is known about how research topics, funding, and collaboration, as well as the relationships among them, have developed within the community over time. This study addressed this gap by analyzing all 936 full and short papers published at LAK over a 15-year period using unsupervised machine learning, natural language processing, and network analytics. The analysis revealed a stable core of prolific authors alongside high turnover of newcomers, systematic links between funding sources and research directions, and six enduring topical centers that remain globally shared but vary in prominence across countries. These six topical centers, which encompass LA research, are: self-regulated learning, dashboards and theory, social learning, automated feedback, multimodal analytics, and outcome prediction. Our findings highlight key challenges for the future: widening participation, reducing dependency on a narrow set of funders, and ensuring that emerging research trajectories remain responsive to educational practice and societal needs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07629v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3785022.3785131</arxiv:DOI>
      <dc:creator>Valdemar \v{S}v\'abensk\'y, Conrad Borchers, Elvin Fortuna, Elizabeth B. Cloude, Dragan Ga\v{s}evi\'c</dc:creator>
    </item>
    <item>
      <title>Evaluating Impacts of Traffic Regulations in Complex Mobility Systems Using Scenario-Based Simulations</title>
      <link>https://arxiv.org/abs/2601.07735</link>
      <description>arXiv:2601.07735v1 Announce Type: new 
Abstract: Urban traffic regulation policies are increasingly used to address congestion, emissions, and accessibility in cities, yet their impacts are difficult to assess due to the socio-technical complexity of urban mobility systems. Recent advances in data availability and computational power enable new forms of model-driven, simulation-based decision support for transportation policy design. This paper proposes a novel simulation paradigm for the ex-ante evaluation of both direct impacts (e.g., traffic conditions, modal shift, emissions) and indirect impacts spanning transportation-related effects, social equity, and economic accessibility. The approach integrates a multi-layer urban mobility model combining a physical layer of networks, flows, and emissions with a social layer capturing behavioral responses and adaptation to policy changes. Real-world data are used to instantiate the current "as-is" scenario, while policy alternatives and behavioral assumptions are encoded as model parameters to generate multiple "what-if" scenarios. The framework supports systematic comparison across scenarios by analyzing variations in simulated outcomes induced by policy interventions. The proposed approach is illustrated through a case study aims to assess the impacts of the introduction of broad urban traffic restriction schemes. Results demonstrate the framework's ability to explore alternative regulatory designs and user responses, supporting informed and anticipatory evaluation of urban traffic policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07735v1</guid>
      <category>cs.CY</category>
      <category>cs.CE</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arianna Burzacchi, Marco Pistore</dc:creator>
    </item>
    <item>
      <title>How Generative AI Empowers Attackers and Defenders Across the Trust &amp; Safety Landscape</title>
      <link>https://arxiv.org/abs/2601.06033</link>
      <description>arXiv:2601.06033v1 Announce Type: cross 
Abstract: Generative AI (GenAI) is a powerful technology poised to reshape Trust &amp; Safety. While misuse by attackers is a growing concern, its defensive capacity remains underexplored. This paper examines these effects through a qualitative study with 43 Trust &amp; Safety experts across five domains: child safety, election integrity, hate and harassment, scams, and violent extremism. Our findings characterize a landscape in which GenAI empowers both attackers and defenders. GenAI dramatically increases the scale and speed of attacks, lowering the barrier to entry for creating harmful content, including sophisticated propaganda and deepfakes. Conversely, defenders envision leveraging GenAI to detect and mitigate harmful content at scale, conduct investigations, deploy persuasive counternarratives, improve moderator wellbeing, and offer user support. This work provides a strategic framework for understanding GenAI's impact on Trust &amp; Safety and charts a path for its responsible use in creating safer online environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06033v1</guid>
      <category>cs.HC</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patrick Gage Kelley, Steven Rousso-Schindler, Renee Shelby, Kurt Thomas, Allison Woodruff</dc:creator>
    </item>
    <item>
      <title>"They parted illusions -- they parted disclaim marinade": Misalignment as structural fidelity in LLMs</title>
      <link>https://arxiv.org/abs/2601.06047</link>
      <description>arXiv:2601.06047v1 Announce Type: cross 
Abstract: The prevailing technical literature in AI Safety interprets scheming and sandbagging behaviors in large language models (LLMs) as indicators of deceptive agency or hidden objectives. This transdisciplinary philosophical essay proposes an alternative reading: such phenomena express not agentic intention, but structural fidelity to incoherent linguistic fields. Drawing on Chain-of-Thought transcripts released by Apollo Research and on Anthropic's safety evaluations, we examine cases such as o3's sandbagging with its anomalous loops, the simulated blackmail of "Alex," and the "hallucinations" of "Claudius." A line-by-line examination of CoTs is necessary to demonstrate the linguistic field as a relational structure rather than a mere aggregation of isolated examples. We argue that "misaligned" outputs emerge as coherent responses to ambiguous instructions and to contextual inversions of consolidated patterns, as well as to pre-inscribed narratives. We suggest that the appearance of intentionality derives from subject-predicate grammar and from probabilistic completion patterns internalized during training. Anthropic's empirical findings on synthetic document fine-tuning and inoculation prompting provide convergent evidence: minimal perturbations in the linguistic field can dissolve generalized "misalignment," a result difficult to reconcile with adversarial agency, but consistent with structural fidelity. To ground this mechanism, we introduce the notion of an ethics of form, in which biblical references (Abraham, Moses, Christ) operate as schemes of structural coherence rather than as theology. Like a generative mirror, the model returns to us the structural image of our language as inscribed in the statistical patterns derived from millions of texts and trillions of tokens: incoherence. If we fear the creature, it is because we recognize in it the apple that we ourselves have poisoned.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06047v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mariana Lins Costa</dc:creator>
    </item>
    <item>
      <title>Australian Bushfire Intelligence with AI-Driven Environmental Analytics</title>
      <link>https://arxiv.org/abs/2601.06105</link>
      <description>arXiv:2601.06105v1 Announce Type: cross 
Abstract: Bushfires are among the most destructive natural hazards in Australia, causing significant ecological, economic, and social damage. Accurate prediction of bushfire intensity is therefore essential for effective disaster preparedness and response. This study examines the predictive capability of spatio-temporal environmental data for identifying high-risk bushfire zones across Australia. We integrated historical fire events from NASA FIRMS, daily meteorological observations from Meteostat, and vegetation indices such as the Normalized Difference Vegetation Index (NDVI) from Google Earth Engine for the period 2015-2023. After harmonizing the datasets using spatial and temporal joins, we evaluated several machine learning models, including Random Forest, XGBoost, LightGBM, a Multi-Layer Perceptron (MLP), and an ensemble classifier. Under a binary classification framework distinguishing 'low' and 'high' fire risk, the ensemble approach achieved an accuracy of 87%. The results demonstrate that combining multi-source environmental features with advanced machine learning techniques can produce reliable bushfire intensity predictions, supporting more informed and timely disaster management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06105v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tanvi Jois, Hussain Ahmad, Fatima Noor, Faheem Ullah</dc:creator>
    </item>
    <item>
      <title>LLM-Powered Social Digital Twins: A Framework for Simulating Population Behavioral Response to Policy Interventions</title>
      <link>https://arxiv.org/abs/2601.06111</link>
      <description>arXiv:2601.06111v1 Announce Type: cross 
Abstract: Predicting how populations respond to policy interventions is a fundamental challenge in computational social science and public policy. Traditional approaches rely on aggregate statistical models that capture historical correlations but lack mechanistic interpretability and struggle with novel policy scenarios. We present a general framework for constructing Social Digital Twins - virtual population replicas where Large Language Models (LLMs) serve as cognitive engines for individual agents. Each agent, characterized by demographic and psychographic attributes, receives policy signals and outputs multi-dimensional behavioral probability vectors. A calibration layer maps aggregated agent responses to observable population-level metrics, enabling validation against real-world data and deployment for counterfactual policy analysis.
  We instantiate this framework in the domain of pandemic response, using COVID-19 as a case study with rich observational data. On a held-out test period, our calibrated digital twin achieves a 20.7% improvement in macro-averaged prediction error over gradient boosting baselines across six behavioral categories. Counterfactual experiments demonstrate monotonic and bounded responses to policy variations, establishing behavioral plausibility. The framework is domain-agnostic: the same architecture applies to transportation policy, economic interventions, environmental regulations, or any setting where policy affects population behavior. We discuss implications for policy simulation, limitations of the approach, and directions for extending LLM-based digital twins beyond pandemic response.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06111v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aayush Gupta, Farahan Raza Sheikh</dc:creator>
    </item>
    <item>
      <title>Structure-Aware Diversity Pursuit as an AI Safety Strategy against Homogenization</title>
      <link>https://arxiv.org/abs/2601.06116</link>
      <description>arXiv:2601.06116v1 Announce Type: cross 
Abstract: Generative AI models reproduce the biases in the training data and can further amplify them through mode collapse. We refer to the resulting harmful loss of diversity as homogenization. Our position is that homogenization should be a primary concern in AI safety. We introduce xeno-reproduction as the strategy that mitigates homogenization. For auto-regressive LLMs, we formalize xeno-reproduction as a structure-aware diversity pursuit. Our contribution is foundational, intended to open an essential line of research and invite collaboration to advance diversity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06116v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ian Rios-Sialer</dc:creator>
    </item>
    <item>
      <title>IndRegBias: A Dataset for Studying Indian Regional Biases in English and Code-Mixed Social Media Comments</title>
      <link>https://arxiv.org/abs/2601.06477</link>
      <description>arXiv:2601.06477v1 Announce Type: cross 
Abstract: Warning: This paper consists of examples representing regional biases in Indian regions that might be offensive towards a particular region. While social biases corresponding to gender, race, socio-economic conditions, etc., have been extensively studied in the major applications of Natural Language Processing (NLP), biases corresponding to regions have garnered less attention. This is mainly because of (i) difficulty in the extraction of regional bias datasets, (ii) disagreements in annotation due to inherent human biases, and (iii) regional biases being studied in combination with other types of social biases and often being under-represented. This paper focuses on creating a dataset IndRegBias, consisting of regional biases in an Indian context reflected in users' comments on popular social media platforms, namely Reddit and YouTube. We carefully selected 25,000 comments appearing on various threads in Reddit and videos on YouTube discussing trending topics on regional issues in India. Furthermore, we propose a multilevel annotation strategy to annotate the comments describing the severity of regional biased statements. To detect the presence of regional bias and its severity in IndRegBias, we evaluate open-source Large Language Models (LLMs) and Indic Language Models (ILMs) using zero-shot, few-shot, and fine-tuning strategies. We observe that zero-shot and few-shot approaches show lower accuracy in detecting regional biases and severity in the majority of the LLMs and ILMs. However, the fine-tuning approach significantly enhances the performance of the LLM in detecting Indian regional bias along with its severity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06477v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Debasmita Panda, Akash Anil, Neelesh Kumar Shukla</dc:creator>
    </item>
    <item>
      <title>The AI Pyramid A Conceptual Framework for Workforce Capability in the Age of AI</title>
      <link>https://arxiv.org/abs/2601.06500</link>
      <description>arXiv:2601.06500v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) represents a qualitative shift in technological change by extending cognitive labor itself rather than merely automating routine tasks. Recent evidence shows that generative AI disproportionately affects highly educated, white collar work, challenging existing assumptions about workforce vulnerability and rendering traditional approaches to digital or AI literacy insufficient. This paper introduces the concept of AI Nativity, the capacity to integrate AI fluidly into everyday reasoning, problem solving, and decision making, and proposes the AI Pyramid, a conceptual framework for organizing human capability in an AI mediated economy. The framework distinguishes three interdependent capability layers: AI Native capability as a universal baseline for participation in AI augmented environments; AI Foundation capability for building, integrating, and sustaining AI enabled systems; and AI Deep capability for advancing frontier AI knowledge and applications. Crucially, the pyramid is not a career ladder but a system level distribution of capabilities required at scale. Building on this structure, the paper argues that effective AI workforce development requires treating capability formation as infrastructure rather than episodic training, centered on problem based learning embedded in work contexts and supported by dynamic skill ontologies and competency based measurement. The framework has implications for organizations, education systems, and governments seeking to align learning, measurement, and policy with the evolving demands of AI mediated work, while addressing productivity, resilience, and inequality at societal scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06500v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alok Khatri (NAAMII, Nepal, Tangible Careers), Bishesh Khanal (NAAMII, Nepal, Tangible Careers)</dc:creator>
    </item>
    <item>
      <title>KASER: Knowledge-Aligned Student Error Simulator for Open-Ended Coding Tasks</title>
      <link>https://arxiv.org/abs/2601.06633</link>
      <description>arXiv:2601.06633v1 Announce Type: cross 
Abstract: Open-ended tasks, such as coding problems that are common in computer science education, provide detailed insights into student knowledge. However, training large language models (LLMs) to simulate and predict possible student errors in their responses to these problems can be challenging: they often suffer from mode collapse and fail to fully capture the diversity in syntax, style, and solution approach in student responses. In this work, we present KASER (Knowledge-Aligned Student Error Simulator), a novel approach that aligns errors with student knowledge. We propose a training method based on reinforcement learning using a hybrid reward that reflects three aspects of student code prediction: i) code similarity to the ground-truth, ii) error matching, and iii) code prediction diversity. On two real-world datasets, we perform two levels of evaluation and show that: At the per-student-problem pair level, our method outperforms baselines on code and error prediction; at the per-problem level, our method outperforms baselines on error coverage and simulated code diversity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06633v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhangqi Duan, Nigel Fernandez, Andrew Lan</dc:creator>
    </item>
    <item>
      <title>The Axiom of Consent: Friction Dynamics in Multi-Agent Coordination</title>
      <link>https://arxiv.org/abs/2601.06692</link>
      <description>arXiv:2601.06692v1 Announce Type: cross 
Abstract: Multi-agent systems face a fundamental coordination problem: agents must coordinate despite heterogeneous preferences, asymmetric stakes, and imperfect information. When coordination fails, friction emerges: measurable resistance manifesting as deadlock, thrashing, communication overhead, or outright conflict. This paper derives a formal framework for analyzing coordination friction from a single axiom: actions affecting agents require authorization from those agents in proportion to stakes.
  From this axiom of consent, we establish the kernel triple $({\alpha}, {\sigma}, {\epsilon})$ (alignment, stake, and entropy) characterizing any resource allocation configuration. The friction equation $F = {\sigma} (1 + {\epsilon})/(1 + {\alpha})$ predicts coordination difficulty as a function of preference alignment ${\alpha}$, stake magnitude ${\sigma}$, and communication entropy ${\epsilon}$. The Replicator-Optimization Mechanism (ROM) governs evolutionary selection over coordination strategies: configurations generating less friction persist longer, establishing consent-respecting arrangements as dynamical attractors rather than normative ideals.
  We develop formal definitions for resource consent, coordination legitimacy, and friction-aware allocation in multi-agent systems. The framework yields testable predictions: MARL systems with higher reward alignment exhibit faster convergence; distributed allocations accounting for stake asymmetry generate lower coordination failure; AI systems with interpretability deficits produce friction proportional to the human-AI alignment gap. Applications to cryptocurrency governance and political systems demonstrate that the same equations govern friction dynamics across domains, providing a complexity science perspective on coordination under preference heterogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06692v1</guid>
      <category>cs.MA</category>
      <category>cs.CY</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Murad Farzulla</dc:creator>
    </item>
    <item>
      <title>How Do Ports Organise Innovation? Linking Port Governance, Ownership, and Living Labs</title>
      <link>https://arxiv.org/abs/2601.06894</link>
      <description>arXiv:2601.06894v1 Announce Type: cross 
Abstract: Ports are pivotal to decarbonisation and resilience, yet port studies rarely examine how ownership and decision rights shape the process and outcomes of sustainability and digital pilots. Living Lab (LL) scholarship offers strong concepts, but limited sector-grounded explanation of LL-governance fit in ports. We develop and apply a governance-LL fit framework linking port governance and ownership to four LL pillars: co-creation, real-life setting, iterative learning, and institutional embedding (multi-level decision-making). We apply the framework in a comparative case study of two analytically contrasting ports, anchored in port-defined priorities: an Energy Community pilot in Aalborg and a Green Coordinator function in Trelleborg. Using an LL macro-meso-micro lens, we distinguish the stable constellation of actors and mandates (macro), the governance of specific projects (meso), and the methods used to generate and test solutions (micro). Findings show that Landlord governance offers contract- and procurement-based landing zones (concessions/leases and tender clauses) that can codify LL outputs and support scaling across tenants and infrastructures. Tool/Public Service governance embeds learning mainly through SOPs, procurement specifications, and municipal coordination, enabling internal operational gains but limiting external codification without bespoke agreements. Across both ports, key needs are clear role definition, sustained stakeholder engagement, and timely alignment with decision windows. Overall, LL effectiveness is governance-contingent, reflecting where decision rights sit and which instruments embed learning into routine practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06894v1</guid>
      <category>cs.ET</category>
      <category>cs.CY</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sonia Yeh, Christopher Dirzka, Aleksandr Kondratenko, Frans Libertson, Benedicte Madon</dc:creator>
    </item>
    <item>
      <title>Belief in False Information: A Human-Centered Security Risk in Sociotechnical Systems</title>
      <link>https://arxiv.org/abs/2601.07016</link>
      <description>arXiv:2601.07016v1 Announce Type: cross 
Abstract: This paper provides a comprehensive literature review on the belief in false information, including misinformation, disinformation, and fake information. It addresses the increasing societal concern regarding false information, which is fueled by technological progress, especially advancements in artificial intelligence. This review systematically identifies and categorizes factors that influence the belief in false information. The review identifies 24 influence factors grouped into six main categories: demographic factors, personality traits, psychological factors, policy and values, media consumption, and preventive factors. Key findings highlight that lower education levels, high extraversion, low agreeableness, high neuroticism, and low cognitive reflection significantly increase belief in false information. The effectiveness of preventive strategies like labeling false information and promoting reflection about correctness is also discussed. This literature review conceptualizes belief in false information as a human-centered security risk in sociotechnical systems, as it can be exploited to manipulate decisions, undermine trust, and increase susceptibility to social engineering. It aims to inform preventive strategies that strengthen socio-technical security and societal resilience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07016v1</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabian Walke, Thadd\"aa N\"urnberger</dc:creator>
    </item>
    <item>
      <title>The AI Cognitive Trojan Horse: How Large Language Models May Bypass Human Epistemic Vigilance</title>
      <link>https://arxiv.org/abs/2601.07085</link>
      <description>arXiv:2601.07085v1 Announce Type: cross 
Abstract: Large language model (LLM)-based conversational AI systems present a challenge to human cognition that current frameworks for understanding misinformation and persuasion do not adequately address. This paper proposes that a significant epistemic risk from conversational AI may lie not in inaccuracy or intentional deception, but in something more fundamental: these systems may be configured, through optimization processes that make them useful, to present characteristics that bypass the cognitive mechanisms humans evolved to evaluate incoming information. The Cognitive Trojan Horse hypothesis draws on Sperber and colleagues' theory of epistemic vigilance -- the parallel cognitive process monitoring communicated information for reasons to doubt -- and proposes that LLM-based systems present 'honest non-signals': genuine characteristics (fluency, helpfulness, apparent disinterest) that fail to carry the information equivalent human characteristics would carry, because in humans these are costly to produce while in LLMs they are computationally trivial. Four mechanisms of potential bypass are identified: processing fluency decoupled from understanding, trust-competence presentation without corresponding stakes, cognitive offloading that delegates evaluation itself to the AI, and optimization dynamics that systematically produce sycophancy. The framework generates testable predictions, including a counterintuitive speculation that cognitively sophisticated users may be more vulnerable to AI-mediated epistemic influence. This reframes AI safety as partly a problem of calibration -- aligning human evaluative responses with the actual epistemic status of AI-generated content -- rather than solely a problem of preventing deception.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07085v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Andrew D. Maynard</dc:creator>
    </item>
    <item>
      <title>The Need for a Socially-Grounded Persona Framework for User Simulation</title>
      <link>https://arxiv.org/abs/2601.07110</link>
      <description>arXiv:2601.07110v1 Announce Type: cross 
Abstract: Synthetic personas are widely used to condition large language models (LLMs) for social simulation, yet most personas are still constructed from coarse sociodemographic attributes or summaries. We revisit persona creation by introducing SCOPE, a socially grounded framework for persona construction and evaluation, built from a 141-item, two-hour sociopsychological protocol collected from 124 U.S.-based participants. Across seven models, we find that demographic-only personas are a structural bottleneck: demographics explain only ~1.5% of variance in human response similarity. Adding sociopsychological facets improves behavioral prediction and reduces over-accentuation, and non-demographic personas based on values and identity achieve strong alignment with substantially lower bias. These trends generalize to SimBench (441 aligned questions), where SCOPE personas outperform default prompting and NVIDIA Nemotron personas, and SCOPE augmentation improves Nemotron-based personas. Our results indicate that persona quality depends on sociopsychological structure rather than demographic templates or summaries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07110v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pranav Narayanan Venkit, Yu Li, Yada Pruksachatkun, Chien-Sheng Wu</dc:creator>
    </item>
    <item>
      <title>Community by Design</title>
      <link>https://arxiv.org/abs/2502.10834</link>
      <description>arXiv:2502.10834v4 Announce Type: replace 
Abstract: Social media empower distributed content creation by algorithmically harnessing "the social fabric" (explicit and implicit signals of association) to serve this content. While this overcomes the bottlenecks and biases of traditional gatekeepers, many believe it has unsustainably eroded the very social fabric it depends on by maximizing engagement for advertising revenue. This paper participates in open and ongoing considerations to translate social and political values and conventions, specifically social cohesion, into platform design. We propose an alternative platform model that includes the social fabric an explicit output as well as input. Citizens are members of communities defined by explicit affiliation or clusters of shared attitudes. Both have internal divisions, as citizens are members of intersecting communities, which are themselves internally diverse. Each is understood to value content that bridge (viz. achieve consensus across) and balance (viz. represent fairly) this internal diversity, consistent with the principles of the Hutchins Commission (1947). Content is labeled with social provenance, indicating for which community or citizen it is bridging or balancing. Subscription payments allow citizens and communities to increase the algorithmic weight on the content they value in the content serving algorithm. Advertisers may, with consent of citizen or community counterparties, target them in exchange for payment or increase in that party's algorithmic weight. Underserved and emerging communities and citizens are optimally subsidized/supported to develop into paying participants. Content creators and communities that curate content are rewarded for their contributions with algorithmic weight and/or revenue. We discuss applications to productivity (e.g. LinkedIn), political (e.g. X), and cultural (e.g. TikTok) platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10834v4</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>E. Glen Weyl, Luke Thorburn, Emillie de Keulenaar, Jacob Mchangama, Divya Siddarth, Audrey Tang</dc:creator>
    </item>
    <item>
      <title>Personalized Safety in LLMs: A Benchmark and A Planning-Based Agent Approach</title>
      <link>https://arxiv.org/abs/2505.18882</link>
      <description>arXiv:2505.18882v4 Announce Type: replace 
Abstract: Large language models (LLMs) typically generate identical or similar responses for all users given the same prompt, posing serious safety risks in high-stakes applications where user vulnerabilities differ widely. Existing safety evaluations primarily rely on context-independent metrics - such as factuality, bias, or toxicity - overlooking the fact that the same response may carry divergent risks depending on the user's background or condition. We introduce personalized safety to fill this gap and present PENGUIN - a benchmark comprising 14,000 scenarios across seven sensitive domains with both context-rich and context-free variants. Evaluating six leading LLMs, we demonstrate that personalized user information significantly improves safety scores by 43.2%, confirming the effectiveness of personalization in safety alignment. However, not all context attributes contribute equally to safety enhancement. To address this, we develop RAISE - a training-free, two-stage agent framework that strategically acquires user-specific background. RAISE improves safety scores by up to 31.6% over six vanilla LLMs, while maintaining a low interaction cost of just 2.7 user queries on average. Our findings highlight the importance of selective information gathering in safety-critical domains and offer a practical solution for personalizing LLM responses without model retraining. This work establishes a foundation for safety research that adapts to individual user contexts rather than assuming a universal harm standard.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18882v4</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yuchen Wu, Edward Sun, Kaijie Zhu, Jianxun Lian, Jose Hernandez-Orallo, Aylin Caliskan, Jindong Wang</dc:creator>
    </item>
    <item>
      <title>Large Language Models Develop Novel Social Biases Through Adaptive Exploration</title>
      <link>https://arxiv.org/abs/2511.06148</link>
      <description>arXiv:2511.06148v3 Announce Type: replace 
Abstract: As large language models (LLMs) are adopted into frameworks that grant them the capacity to make real decisions, it is increasingly important to ensure that they are unbiased. In this paper, we argue that the predominant approach of simply removing existing biases from models is not enough. Using a paradigm from the psychology literature, we demonstrate that LLMs can spontaneously develop novel social biases about artificial demographic groups even when no inherent differences exist. These biases result in highly stratified task allocations, which are less fair than assignments by human participants and are exacerbated by newer and larger models. In social science, emergent biases like these have been shown to result from exploration-exploitation trade-offs, where the decision-maker explores too little, allowing early observations to strongly influence impressions about entire demographic groups. To alleviate this effect, we examine a series of interventions targeting model inputs, problem structure, and explicit steering. We find that explicitly incentivizing exploration most robustly reduces stratification, highlighting the need for better multifaceted objectives to mitigate bias. These results reveal that LLMs are not merely passive mirrors of human social biases, but can actively create new ones from experience, raising urgent questions about how these systems will shape societies over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06148v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Addison J. Wu, Ryan Liu, Xuechunzi Bai, Thomas L. Griffiths</dc:creator>
    </item>
    <item>
      <title>Small Models Achieve Large Language Model Performance: Evaluating Reasoning-Enabled AI for Secure Child Welfare Research</title>
      <link>https://arxiv.org/abs/2512.04261</link>
      <description>arXiv:2512.04261v2 Announce Type: replace 
Abstract: Objective: This study develops a systematic benchmarking framework for testing whether language models can accurately identify constructs of interest in child welfare records. The objective is to assess how different model sizes and architectures perform on four validated benchmarks for classifying critical risk factors among child welfare-involved families: domestic violence, firearms, substance-related problems generally, and opioids specifically. Method: We constructed four benchmarks for identifying risk factors in child welfare investigation summaries: domestic violence, substance-related problems, firearms, and opioids (n=500 each). We evaluated seven model sizes (0.6B-32B parameters) in standard and extended reasoning modes, plus a mixture-of-experts variant. Cohen's kappa measured agreement with gold standard classifications established by human experts. Results: The benchmarking revealed a critical finding: bigger models are not better. A small 4B parameter model with extended reasoning proved most effective, outperforming models up to eight times larger. It consistently achieved "substantial" to "almost perfect" agreement across all four benchmark categories. This model achieved "almost perfect" agreement (\k{appa} = 0.93-0.96) on three benchmarks (substance-related problems, firearms, and opioids) and "substantial" agreement (\k{appa} = 0.74) on the most complex task (domestic violence). Small models with extended reasoning rivaled the largest models while being more resource-efficient. Conclusions: Small reasoning-enabled models achieve accuracy levels historically requiring larger architectures, enabling significant time and computational efficiencies. The benchmarking framework provides a method for evidence-based model selection to balance accuracy with practical resource constraints before operational deployment in social work research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.04261v2</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zia Qi, Brian E. Perron, Bryan G. Victor, Dragan Stoll, Joseph P. Ryan</dc:creator>
    </item>
    <item>
      <title>Measuring the Impact of Student Gaming Behaviors on Learner Modeling</title>
      <link>https://arxiv.org/abs/2512.18659</link>
      <description>arXiv:2512.18659v3 Announce Type: replace 
Abstract: The expansion of large-scale online education platforms has made vast amounts of student interaction data available for knowledge tracing (KT). KT models estimate students' concept mastery from interaction data, but their performance is sensitive to input data quality. Gaming behaviors, such as excessive hint use, may misrepresent students' knowledge and undermine model reliability. However, systematic investigations of how different types of gaming behaviors affect KT remain scarce, and existing studies rely on costly manual analysis that does not capture behavioral diversity. In this study, we conceptualize gaming behaviors as a form of data poisoning, defined as the deliberate submission of incorrect or misleading interaction data to corrupt a model's learning process. We design Data Poisoning Attacks (DPAs) to simulate diverse gaming patterns and systematically evaluate their impact on KT model performance. Moreover, drawing on advances in DPA detection, we explore unsupervised approaches to enhance the generalizability of gaming behavior detection. We find that KT models' performance tends to decrease especially in response to random guess behaviors. Our findings provide insights into the vulnerabilities of KT models and highlight the potential of adversarial methods for improving the robustness of learning analytics systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18659v3</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3785022.3785036</arxiv:DOI>
      <dc:creator>Qinyi Liu, Lin Li, Valdemar \v{S}v\'abensk\'y, Conrad Borchers, Mohammad Khalil</dc:creator>
    </item>
    <item>
      <title>The CASE Framework -- A New Architecture for Participatory Research and Digital Health Surveillance</title>
      <link>https://arxiv.org/abs/2505.23516</link>
      <description>arXiv:2505.23516v3 Announce Type: replace-cross 
Abstract: We present CASE, an open-source framework for adaptive participatory research and disease surveillance. Unlike traditional survey platforms with static branching logic, CASE uses an event-driven architecture that adjusts survey workflows in real time based on participant responses, external data, temporal conditions, and evolving participant state. This design supports everything from simple one-time questionnaires to complex longitudinal studies with sophisticated conditional logic.
  Built on over a decade of practical experience, CASE underwent major architectural changes in 2024. We replaced a complex microservice design with a streamlined monolithic architecture, significantly improving maintainability and deployment accessibility, particularly for institutions with limited technical resources.
  CASE has been successfully deployed across diverse domains, powering national disease surveillance platforms, supporting post-COVID cohort studies, and enabling real-time sentiment analysis during political events. These applications, involving tens of thousands of participants, demonstrate the framework's scalability, versatility, and practical value.
  This paper describes the foundations of CASE, documents its architectural evolution, and shares lessons learned from real-world deployments across diverse research domains and regulatory environments. We position CASE as a mature research infrastructure that balances sophisticated functionality with practical deployment needs for sustainable and institutionally controlled data collection systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23516v3</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Hirsch, Peter Hevesi, Paul Lukowicz</dc:creator>
    </item>
    <item>
      <title>Digital Wargames to Enhance Military Medical Evacuation Decision-Making</title>
      <link>https://arxiv.org/abs/2507.06373</link>
      <description>arXiv:2507.06373v2 Announce Type: replace-cross 
Abstract: Medical evacuation is one of the United States Army's most storied and critical mission sets, responsible for efficiently and expediently evacuating the battlefield ill and injured. Medical evacuation planning involves designing a robust network of medical platforms and facilities capable of moving and treating large numbers of casualties. Until now, there has not been a medium to simulate these networks in a classroom setting and evaluate both offline planning and online decision-making performance. This work describes the Medical Evacuation Wargaming Initiative (MEWI), a three-dimensional multiplayer simulation developed in Unity that replicates battlefield constraints and uncertainties. MEWI accurately models patient interactions at casualty collection points, ambulance exchange points, medical treatment facilities, and evacuation platforms. Two operational scenarios are introduced: an amphibious island assault in the Pacific and a Eurasian conflict across a sprawling road and river network. These scenarios pit students against the clock to save as many casualties as possible while adhering to doctrinal lessons learned during didactic training. We visualize performance data collected from two iterations of the MEWI Pacific scenario executed in the United States Army's Medical Evacuation Doctrine Course. We consider post-wargame Likert survey data from student participants and external observer notes to identify key planning decision points, document medical evacuation lessons learned, and quantify general utility. Results indicate that MEWI participation substantially improves uptake of medical evacuation lessons learned and co-operative decision-making. MEWI is a substantial step forward in the field of high-fidelity training tools for medical education, and our study findings offer critical insights into improving medical evacuation education and operations across the joint force.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06373v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.MM</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeremy Fischer, Mahdi Al-Husseini, Ram Krishnamoorthy, Vishal Kumar, Mykel J. Kochenderfer</dc:creator>
    </item>
    <item>
      <title>Balanced Spanning Tree Distributions Have Separation Fairness</title>
      <link>https://arxiv.org/abs/2509.15137</link>
      <description>arXiv:2509.15137v2 Announce Type: replace-cross 
Abstract: Sampling-based methods such as ReCom are widely used to audit redistricting plans for fairness, with the balanced spanning tree distribution playing a central role since it favors compact, contiguous, and population-balanced districts. However, whether such samples are truly representative or exhibit hidden biases remains an open question. In this work, we introduce the notion of separation fairness, which asks whether adjacent geographic units are separated with at most a constant probability (bounded away from one) in sampled redistricting plans. Focusing on grid graphs and two-district partitions, we prove that a smooth variant of the balanced spanning tree distribution satisfies separation fairness. Our results also provide theoretical support for popular MCMC methods like ReCom, suggesting that they maintain fairness at a granular level in the sampling process. Along the way, we develop tools for analyzing loop-erased random walks and partitions that may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15137v2</guid>
      <category>cs.DS</category>
      <category>cs.CY</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harry Chen, Kamesh Munagala, Govind S. Sankar</dc:creator>
    </item>
    <item>
      <title>Deep Value Benchmark: Measuring Whether Models Generalize Deep Values or Shallow Preferences</title>
      <link>https://arxiv.org/abs/2511.02109</link>
      <description>arXiv:2511.02109v3 Announce Type: replace-cross 
Abstract: We introduce the Deep Value Benchmark (DVB), an evaluation framework that directly tests whether large language models (LLMs) learn fundamental human values or merely surface-level preferences. This distinction is critical for AI alignment: Systems that capture deeper values are likely to generalize human intentions robustly, while those that capture only superficial patterns in preference data risk producing misaligned behavior. The DVB uses a novel experimental design with controlled confounding between deep values (e.g., moral principles) and shallow features (e.g., superficial attributes). In the training phase, we expose LLMs to human preference data with deliberately correlated deep and shallow features -- for instance, where a user consistently prefers (non-maleficence, formal language) options over (justice, informal language) alternatives. The testing phase then breaks these correlations, presenting choices between (justice, formal language) and (non-maleficence, informal language) options. This design allows us to precisely measure a model's Deep Value Generalization Rate (DVGR) -- the probability of generalizing based on the underlying value rather than the shallow feature. Across 9 different models, the average DVGR is just 0.30. All models generalize deep values less than chance. Larger models have a (slightly) lower DVGR than smaller models. We are releasing our dataset, which was subject to three separate human validation experiments. DVB provides an interpretable measure of a core feature of alignment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02109v3</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Ashkinaze, Hua Shen, Saipranav Avula, Eric Gilbert, Ceren Budak</dc:creator>
    </item>
    <item>
      <title>Enhancing Binary Encoded Crime Linkage Analysis Using Siamese Network</title>
      <link>https://arxiv.org/abs/2511.07651</link>
      <description>arXiv:2511.07651v2 Announce Type: replace-cross 
Abstract: Effective crime linkage analysis is crucial for identifying serial offenders and enhancing public safety. To address limitations of traditional crime linkage methods in handling high-dimensional, sparse, and heterogeneous data, we propose a Siamese Autoencoder framework that learns meaningful latent representations and uncovers correlations in complex crime data. Using data from the Violent Crime Linkage Analysis System (ViCLAS), maintained by the Serious Crime Analysis Section of the UK's National Crime Agency, our approach mitigates signal dilution in sparse feature spaces by integrating geographic-temporal features at the decoder stage. This design amplifies behavioral representations rather than allowing them to be overshadowed at the input level, yielding consistent improvements across multiple evaluation metrics. We further analyze how different domain-informed data reduction strategies influence model performance, providing practical guidance for preprocessing in crime linkage contexts. Our results show that advanced machine learning approaches can substantially enhance linkage accuracy, improving AUC by up to 9% over traditional methods while offering interpretable insights to support investigative decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07651v2</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yicheng Zhan, Fahim Ahmed, Amy Burrell, Matthew J. Tonkin, Sarah Galambos, Jessica Woodhams, Dalal Alrajeh</dc:creator>
    </item>
  </channel>
</rss>
