<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NE</link>
    <description>cs.NE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 25 Feb 2025 05:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Weight Adaptation Trigger Mechanism in Decomposition-based Evolutionary Multi-Objective Optimisation</title>
      <link>https://arxiv.org/abs/2502.16481</link>
      <description>arXiv:2502.16481v1 Announce Type: new 
Abstract: Decomposition-based multi-objective evolutionary algorithms (MOEAs) are widely used for solving multi-objective optimisation problems. However, their effectiveness depends on the consistency between the problems Pareto front shape and the weight distribution. Decomposition-based MOEAs, with uniformly distributed weights (in a simplex), perform well on problems with a regular (simplex-like) Pareto front, but not on those with an irregular Pareto front. Previous studies have focused on adapting the weights to approximate the irregular Pareto front during the evolutionary process. However, these adaptations can actually harm the performance on the regular Pareto front via changing the weights during the search process that are eventually the best fit for the Pareto front. In this paper, we propose an algorithm called the weight adaptation trigger mechanism for decomposition-based MOEAs (ATM-MOEA/D) to tackle this issue. ATM-MOEA/D uses an archive to gradually approximate the shape of the Pareto front during the search. When the algorithm detects evolution stagnation (meaning the population no longer improves significantly), it compares the distribution of the population with that of the archive to distinguish between regular and irregular Pareto fronts. Only when an irregular Pareto front is identified, the weights are adapted. Our experimental results show that the proposed algorithm not only performs generally better than seven state-of-the-art weight-adapting methods on irregular Pareto fronts but also is able to achieve the same results as fixed-weight methods like MOEA/D on regular Pareto fronts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16481v1</guid>
      <category>cs.NE</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaofeng Han, Xiaochen Chu, Tao Chao, Ming Yang, Miqing Li</dc:creator>
    </item>
    <item>
      <title>Evolving Form and Function: Dual-Objective Optimization in Neural Symbolic Regression Networks</title>
      <link>https://arxiv.org/abs/2502.17393</link>
      <description>arXiv:2502.17393v1 Announce Type: new 
Abstract: Data increasingly abounds, but distilling their underlying relationships down to something interpretable remains challenging. One approach is genetic programming, which `symbolically regresses' a data set down into an equation.
  However, symbolic regression (SR) faces the issue of requiring training from scratch for each new dataset. To generalize across all datasets, deep learning techniques have been applied to SR.
  These networks, however, are only able to be trained using a symbolic objective: NN-generated and target equations are symbolically compared. But this does not consider the predictive power of these equations, which could be measured by a behavioral objective that compares the generated equation's predictions to actual data.
  Here we introduce a method that combines gradient descent and evolutionary computation to yield neural networks that minimize the symbolic and behavioral errors of the equations they generate from data.
  As a result, these evolved networks are shown to generate more symbolically and behaviorally accurate equations than those generated by networks trained by state-of-the-art gradient based neural symbolic regression methods.
  We hope this method suggests that evolutionary algorithms, combined with gradient descent, can improve SR results by yielding equations with more accurate form and function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17393v1</guid>
      <category>cs.NE</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3638529.3654030</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the Genetic and Evolutionary Computation Conference, pp. 277-285. 2024</arxiv:journal_reference>
      <dc:creator>Amanda Bertschinger, James Bagrow, Joshua Bongard</dc:creator>
    </item>
    <item>
      <title>A Brain-Inspired Perception-Decision Driving Model Based on Neural Pathway Anatomical Alignment</title>
      <link>https://arxiv.org/abs/2502.16027</link>
      <description>arXiv:2502.16027v1 Announce Type: cross 
Abstract: In the realm of autonomous driving, conventional approaches for vehicle perception and decision-making primarily rely on sensor input and rule-based algorithms. However, these methodologies often suffer from lack of interpretability and robustness, particularly in intricate traffic scenarios. To tackle this challenge, we propose a novel brain-inspired driving (BID) framework. Diverging from traditional methods, our approach harnesses brain-inspired perception technology to achieve more efficient and robust environmental perception. Additionally, it employs brain-inspired decision-making techniques to facilitate intelligent decision-making. The experimental results show that the performance has been significantly improved across various autonomous driving tasks and achieved the end-to-end autopilot successfully. This contribution not only advances interpretability and robustness but also offers fancy insights and methodologies for further advancing autonomous driving technology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16027v1</guid>
      <category>cs.RO</category>
      <category>cs.NE</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haidong Wang, Pengfei Xiao, Ao Liu, Qia Shan, Jianhua Zhang</dc:creator>
    </item>
    <item>
      <title>Destroy and Repair Using Hyper Graphs for Routing</title>
      <link>https://arxiv.org/abs/2502.16170</link>
      <description>arXiv:2502.16170v1 Announce Type: cross 
Abstract: Recent advancements in Neural Combinatorial Optimization (NCO) have shown promise in solving routing problems like the Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP) without handcrafted designs. Research in this domain has explored two primary categories of methods: iterative and non-iterative. While non-iterative methods struggle to generate near-optimal solutions directly, iterative methods simplify the task by learning local search steps. However, existing iterative methods are often limited by restricted neighborhood searches, leading to suboptimal results. To address this limitation, we propose a novel approach that extends the search to larger neighborhoods by learning a destroy-and-repair strategy. Specifically, we introduce a Destroy-and-Repair framework based on Hyper-Graphs (DRHG). This framework reduces consecutive intact edges to hyper-edges, allowing the model to pay more attention to the destroyed part and decrease the complexity of encoding all nodes. Experiments demonstrate that DRHG achieves stateof-the-art performance on TSP with up to 10,000 nodes and shows strong generalization to real-world TSPLib and CVRPLib problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16170v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ke Li, Fei Liu, Zhengkun Wang, Qingfu Zhang</dc:creator>
    </item>
    <item>
      <title>Brain-Model Evaluations Need the NeuroAI Turing Test</title>
      <link>https://arxiv.org/abs/2502.16238</link>
      <description>arXiv:2502.16238v1 Announce Type: cross 
Abstract: What makes an artificial system a good model of intelligence? The classical test proposed by Alan Turing focuses on behavior, requiring that an artificial agent's behavior be indistinguishable from that of a human. While behavioral similarity provides a strong starting point, two systems with very different internal representations can produce the same outputs. Thus, in modeling biological intelligence, the field of NeuroAI often aims to go beyond behavioral similarity and achieve representational convergence between a model's activations and the measured activity of a biological system. This position paper argues that the standard definition of the Turing Test is incomplete for NeuroAI, and proposes a stronger framework called the ``NeuroAI Turing Test'', a benchmark that extends beyond behavior alone and \emph{additionally} requires models to produce internal neural representations that are empirically indistinguishable from those of a brain up to measured individual variability, i.e. the differences between a computational model and the brain is no more than the difference between one brain and another brain. While the brain is not necessarily the ceiling of intelligence, it remains the only universally agreed-upon example, making it a natural reference point for evaluating computational models. By proposing this framework, we aim to shift the discourse from loosely defined notions of brain inspiration to a systematic and testable standard centered on both behavior and internal representations, providing a clear benchmark for neuroscientific modeling and AI development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16238v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jenelle Feather, Meenakshi Khosla, N. Apurva Ratan Murty, Aran Nayebi</dc:creator>
    </item>
    <item>
      <title>Evolution 6.0: Evolving Robotic Capabilities Through Generative Design</title>
      <link>https://arxiv.org/abs/2502.17034</link>
      <description>arXiv:2502.17034v1 Announce Type: cross 
Abstract: We propose a new concept, Evolution 6.0, which represents the evolution of robotics driven by Generative AI. When a robot lacks the necessary tools to accomplish a task requested by a human, it autonomously designs the required instruments and learns how to use them to achieve the goal. Evolution 6.0 is an autonomous robotic system powered by Vision-Language Models (VLMs), Vision-Language Action (VLA) models, and Text-to-3D generative models for tool design and task execution. The system comprises two key modules: the Tool Generation Module, which fabricates task-specific tools from visual and textual data, and the Action Generation Module, which converts natural language instructions into robotic actions. It integrates QwenVLM for environmental understanding, OpenVLA for task execution, and Llama-Mesh for 3D tool generation. Evaluation results demonstrate a 90% success rate for tool generation with a 10-second inference time, and action generation achieving 83.5% in physical and visual generalization, 70% in motion generalization, and 37% in semantic generalization. Future improvements will focus on bimanual manipulation, expanded task capabilities, and enhanced environmental interpretation to improve real-world adaptability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17034v1</guid>
      <category>cs.RO</category>
      <category>cs.NE</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Muhammad Haris Khan, Artyom Myshlyaev, Artyom Lykov, Miguel Altamirano Cabrera, Dzmitry Tsetserukou</dc:creator>
    </item>
    <item>
      <title>MToP: A MATLAB Optimization Platform for Evolutionary Multitasking</title>
      <link>https://arxiv.org/abs/2312.08134</link>
      <description>arXiv:2312.08134v4 Announce Type: replace 
Abstract: Evolutionary multitasking (EMT) has emerged as a popular topic of evolutionary computation over the past decade. It aims to concurrently address multiple optimization tasks within limited computing resources, leveraging inter-task knowledge transfer techniques. Despite the abundance of multitask evolutionary algorithms (MTEAs) proposed for multitask optimization (MTO), there remains a comprehensive software platform to help researchers evaluate MTEA performance on benchmark MTO problems as well as explore real-world applications. To bridge this gap, we introduce the first open-source optimization platform, named MTO-Platform (MToP), for EMT. MToP incorporates over 50 MTEAs, more than 200 MTO problem cases with real-world applications, and {over 20 performance metrics}. Moreover, to facilitate comparative analyses between MTEAs and traditional evolutionary algorithms, we adapted over 50 popular single-task evolutionary algorithms to address MTO problems. MToP boasts a user-friendly graphical interface, facilitating results analysis, data export, and schematics visualization. More importantly, MToP is designed with extensibility in mind, allowing users to develop new algorithms and tackle emerging problem domains. The source code of MToP is available at https://github.com/intLyc/MTO-Platform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.08134v4</guid>
      <category>cs.NE</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanchi Li, Wenyin Gong, Fei Ming, Tingyu Zhang, Shuijia Li, Qiong Gu</dc:creator>
    </item>
    <item>
      <title>Runtime Analysis of Evolutionary Algorithms for Multiparty Multiobjective Optimization</title>
      <link>https://arxiv.org/abs/2501.16336</link>
      <description>arXiv:2501.16336v2 Announce Type: replace 
Abstract: In scenarios where multiple decision-makers operate within a common decision space, each focusing on their own multi-objective optimization problem (e.g., bargaining games), the problem can be modeled as a multi-party multi-objective optimization problem (MPMOP). While numerous evolutionary algorithms have been proposed to solve MPMOPs, most results remain empirical. This paper presents the first theoretical analysis of the expected runtime of evolutionary algorithms on bi-party multi-objective optimization problems (BPMOPs). Our findings demonstrate that employing traditional multi-objective optimization algorithms to solve MPMOPs is both time-consuming and inefficient, as the resulting population contains many solutions that fail to achieve consensus among decision-makers. An alternative approach involves decision-makers individually solving their respective optimization problems and seeking consensus only in the final stage. While feasible for pseudo-Boolean optimization problems, this method may fail to guarantee approximate performance for one party in NP-hard problems. Finally, We propose coevolutionary multi-party multi-objective optimizers (CoEMPMO) for pseudo-Boolean optimization and shortest path problems within a multi-party multi-objective context, which maintains a common solution set among all parties through coevolution. Theoretical and experimental results demonstrate that the proposed \( \text{CoEMPMO}_{\text{random}} \) outperforms previous algorithms in terms of the expected lower bound on runtime for pseudo-Boolean optimization problems. Additionally, \( \text{CoEMPMO}_{\text{cons}}^{\text{SP}} \) achieves better efficiency and precision in solving shortest path problems compared to existing algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16336v2</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuetong Sun, Peilan Xu, Wenjian Luo</dc:creator>
    </item>
    <item>
      <title>Learning Heuristics for Transit Network Design and Improvement with Deep Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2404.05894</link>
      <description>arXiv:2404.05894v4 Announce Type: replace-cross 
Abstract: Transit agencies world-wide face tightening budgets and declining ridership. To maintain quality of service while cutting costs, efficient transit network design is essential. But planning a network of public transit routes is a challenging optimization problem. The most successful approaches to date use metaheuristic algorithms to search through the space of possible transit networks by applying low-level heuristics that randomly alter routes in a network. The design of these low-level heuristics has a major impact on the quality of the result. In this paper we use deep reinforcement learning with graph neural nets to learn low-level heuristics for an evolutionary algorithm, instead of designing them manually. These learned heuristics improve the algorithm's results on benchmark synthetic cities with 70 nodes or more, and achieve new state-of-the-art results the challenging Mumford benchmark. They also improve upon a simulation of the real transit network in the city of Laval, Canada, by as much as 52% and 25% on two key metrics, and offer cost savings of up to 19% over the city's existing transit network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05894v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Holliday, Ahmed El-Geneidy, Gregory Dudek</dc:creator>
    </item>
    <item>
      <title>Quantum-tunnelling deep neural network for optical illusion recognition</title>
      <link>https://arxiv.org/abs/2407.11013</link>
      <description>arXiv:2407.11013v2 Announce Type: replace-cross 
Abstract: The discovery of the quantum tunnelling (QT) effect -- the transmission of particles through a high potential barrier -- was one of the most impressive achievements of quantum mechanics made in the 1920s. Responding to the contemporary challenges, I introduce a deep neural network (DNN) architecture that processes information using the effect of QT. I demonstrate the ability of QT-DNN to recognise optical illusions like a human. Tasking QT-DNN to simulate human perception of the Necker cube and Rubin's vase, I provide arguments in favour of the superiority of QT-based activation functions over the activation functions optimised for modern applications in machine vision, also showing that, at the fundamental level, QT-DNN is closely related to biology-inspired DNNs and models based on the principles of quantum information processing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11013v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.NE</category>
      <category>physics.soc-ph</category>
      <category>quant-ph</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1063/5.0225771</arxiv:DOI>
      <arxiv:journal_reference>APL Mach. Learn. 2, 036107 (2024)</arxiv:journal_reference>
      <dc:creator>Ivan S. Maksymov</dc:creator>
    </item>
    <item>
      <title>Optimistic Games for Combinatorial Bayesian Optimization with Application to Protein Design</title>
      <link>https://arxiv.org/abs/2409.18582</link>
      <description>arXiv:2409.18582v2 Announce Type: replace-cross 
Abstract: Bayesian optimization (BO) is a powerful framework to optimize black-box expensive-to-evaluate functions via sequential interactions. In several important problems (e.g. drug discovery, circuit design, neural architecture search, etc.), though, such functions are defined over large $\textit{combinatorial and unstructured}$ spaces. This makes existing BO algorithms not feasible due to the intractable maximization of the acquisition function over these domains. To address this issue, we propose $\textbf{GameOpt}$, a novel game-theoretical approach to combinatorial BO. $\textbf{GameOpt}$ establishes a cooperative game between the different optimization variables, and selects points that are game $\textit{equilibria}$ of an upper confidence bound acquisition function. These are stable configurations from which no variable has an incentive to deviate$-$ analog to local optima in continuous domains. Crucially, this allows us to efficiently break down the complexity of the combinatorial domain into individual decision sets, making $\textbf{GameOpt}$ scalable to large combinatorial spaces. We demonstrate the application of $\textbf{GameOpt}$ to the challenging $\textit{protein design}$ problem and validate its performance on four real-world protein datasets. Each protein can take up to $20^{X}$ possible configurations, where $X$ is the length of a protein, making standard BO methods infeasible. Instead, our approach iteratively selects informative protein configurations and very quickly discovers highly active protein variants compared to other baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18582v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <category>q-bio.BM</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Melis Ilayda Bal, Pier Giuseppe Sessa, Mojmir Mutny, Andreas Krause</dc:creator>
    </item>
    <item>
      <title>Universal approximation results for neural networks with non-polynomial activation function over non-compact domains</title>
      <link>https://arxiv.org/abs/2410.14759</link>
      <description>arXiv:2410.14759v3 Announce Type: replace-cross 
Abstract: This paper extends the universal approximation property of single-hidden-layer feedforward neural networks beyond compact domains, which is of particular interest for the approximation within weighted $C^k$-spaces and weighted Sobolev spaces over unbounded domains. More precisely, by assuming that the activation function is non-polynomial, we establish universal approximation results within function spaces defined over non-compact subsets of a Euclidean space, including $L^p$-spaces, weighted $C^k$-spaces, and weighted Sobolev spaces, where the latter two include the approximation of the (weak) derivatives. Moreover, we provide some dimension-independent rates for approximating a function with sufficiently regular and integrable Fourier transform by neural networks with non-polynomial activation function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14759v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>math.CA</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ariel Neufeld, Philipp Schmocker</dc:creator>
    </item>
  </channel>
</rss>
