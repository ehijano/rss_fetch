<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.RM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.RM</link>
    <description>q-fin.RM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.RM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 02 Dec 2025 05:04:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Monopoly Pricing of Weather Index Insurance</title>
      <link>https://arxiv.org/abs/2512.01623</link>
      <description>arXiv:2512.01623v1 Announce Type: new 
Abstract: This study models the monopoly pricing of weather index insurance as a Bowley-type sequential game involving a profit-maximizing insurer (leader) and a farmer (follower). The farmer chooses an insurance payoff to minimize a convex distortion risk measure, while the insurer anticipates this best response and selects a premium principle and its parameters to maximize profit net of administrative costs. For the insurer, we adopt three different premium-principle parameterizations: (i) an expected premium with a single risk-loading factor, (ii) a two-parameter distortion premium based on a power transform, and (iii) a fully flexible pricing kernel drawn from the general Choquet integral representation with nondecreasing distortions. For the farmer, we model index payoffs using neural networks and compare solutions under fully connected architectures with those under convolutional neural networks (CNNs). We solve the game using a penalized bilevel programming algorithm that employs a function-value-gap penalty and delivers convergence guarantees without requiring the lower-level objective to be strongly convex. Based on Iowa's soybean yields and high-dimensional PRISM weather data, we find that CNN-based designs yield smoother, less noisy payoffs that reduce basis risk and push insurer profits closer to indemnity insurance levels. Moreover, expanding pricing flexibility from a single loading to a two-parameter distortion premium, and ultimately to a flexible pricing kernel, systematically increases equilibrium profits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01623v1</guid>
      <category>q-fin.RM</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tim J. Boonen, Wenyuan Li, Zixiao Quan</dc:creator>
    </item>
    <item>
      <title>Stochastic Dominance Constrained Optimization with S-shaped Utilities: Poor-Performance-Region Algorithm and Neural Network</title>
      <link>https://arxiv.org/abs/2512.00299</link>
      <description>arXiv:2512.00299v1 Announce Type: cross 
Abstract: We investigate the static portfolio selection problem of S-shaped and non-concave utility maximization under first-order and second-order stochastic dominance (SD) constraints. In many S-shaped utility optimization problems, one should require a liquidation boundary to guarantee the existence of a finite concave envelope function. A first-order SD (FSD) constraint can replace this requirement and provide an alternative for risk management. We explicitly solve the optimal solution under a general S-shaped utility function with a first-order stochastic dominance constraint. However, the second-order SD (SSD) constrained problem under non-concave utilities is difficult to solve analytically due to the invalidity of Sion's maxmin theorem. For this sake, we propose a numerical algorithm to obtain a plausible and sub-optimal solution for general non-concave utilities. The key idea is to detect the poor performance region with respect to the SSD constraints, characterize its structure and modify the distribution on that region to obtain (sub-)optimality. A key financial insight is that the decision maker should follow the SD constraint on the poor performance scenario while conducting the unconstrained optimal strategy otherwise. We provide numerical experiments to show that our algorithm effectively finds a sub-optimal solution in many cases. Finally, we develop an algorithm-guided piecewise-neural-network framework to learn the solution of the SSD problem, which demonstrates accelerated convergence compared to standard neural network approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00299v1</guid>
      <category>q-fin.MF</category>
      <category>cs.LG</category>
      <category>q-fin.PM</category>
      <category>q-fin.RM</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zeyun Hu, Yang Liu</dc:creator>
    </item>
    <item>
      <title>An Imbalance-Robust Evaluation Framework for Extreme Risk Forecasts</title>
      <link>https://arxiv.org/abs/2512.00916</link>
      <description>arXiv:2512.00916v1 Announce Type: cross 
Abstract: Evaluating rare-event forecasts is challenging because standard metrics collapse as event prevalence declines. Measures such as F1-score, AUPRC, MCC, and accuracy induce degenerate thresholds -- converging to zero or one -- and their values become dominated by class imbalance rather than tail discrimination. We develop a family of rare-event-stable (RES) metrics whose optimal thresholds remain strictly interior as the event probability approaches zero, ensuring coherent decision rules under extreme rarity. Simulations spanning event probabilities from 0.01 down to one in a million show that RES metrics maintain stable thresholds, consistent model rankings, and near-complete prevalence invariance, whereas traditional metrics exhibit statistically significant threshold drift and structural collapse. A credit-default application confirms these results: RES metrics yield interpretable probability-of-default cutoffs (4-9%) and remain robust under subsampling, while classical metrics fail operationally. The RES framework provides a principled, prevalence-invariant basis for evaluating extreme-risk forecasts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00916v1</guid>
      <category>stat.ME</category>
      <category>q-fin.RM</category>
      <category>stat.ML</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sotirios D. Nikolopoulos</dc:creator>
    </item>
    <item>
      <title>Autodeleveraging: Impossibilities and Optimization</title>
      <link>https://arxiv.org/abs/2512.01112</link>
      <description>arXiv:2512.01112v1 Announce Type: cross 
Abstract: Autodeleveraging (ADL) is a last-resort loss socialization mechanism for perpetual futures venues. It is triggered when solvency-preserving liquidations fail. Despite the dominance of perpetual futures in the crypto derivatives market, with over \$60 trillion of volume in 2024, there has been no formal study of ADL. In this paper, we provide the first rigorous model of ADL. We prove that ADL mechanisms face a fundamental \emph{trilemma}: no policy can simultaneously satisfy exchange \emph{solvency}, \emph{revenue}, and \emph{fairness} to traders. This impossibility theorem implies that as participation scales, a novel form of \emph{moral hazard} grows asymptotically, rendering `zero-loss' socialization impossible. Constructively, we show that three classes of ADL mechanisms can optimally navigate this trilemma to provide fairness, robustness to price shocks, and maximal exchange revenue. We analyze these mechanisms on the Hyperliquid dataset from October 10, 2025, when ADL was used repeatedly to close \$2.1 billion of positions in 12 minutes. By comparing our ADL mechanisms to the standard approaches used in practice, we demonstrate empirically that Hyperliquid's production queue overutilized ADL by approximately $8\times$ relative to our optimal policy, imposing roughly \$630 million of unnecessary haircuts on winning traders. This comparison also suggests that Binance overutilized ADL far more than Hyperliquid. Our results both theoretically and empirically demonstrate that optimized ADL mechanisms can dramatically reduce the loss of trader profits while maintaining exchange solvency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01112v1</guid>
      <category>cs.GT</category>
      <category>q-fin.RM</category>
      <category>q-fin.TR</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tarun Chitra</dc:creator>
    </item>
    <item>
      <title>DeFi Arbitrage in Hedged Liquidity Tokens</title>
      <link>https://arxiv.org/abs/2409.11339</link>
      <description>arXiv:2409.11339v3 Announce Type: replace-cross 
Abstract: Empirically, the prevailing market prices for liquidity tokens of the constant product market maker (CPMM) -- as offered in practice by companies such as Uniswap -- readily permit arbitrage opportunities by delta hedging the risk of the position. Herein, we investigate this arbitrage opportunity by treating the liquidity token as a derivative position in the prices of the underlying assets for the CPMM. In doing so, not dissimilar to the Black-Scholes result, we deduce risk-neutral pricing and hedging formulas for these liquidity tokens. Furthermore, with our novel pricing formula, we construct a method to calibrate a volatility to data which provides an updated (non-market) price which would not permit arbitrage if quoted by the CPMM. We conclude with a discussion of novel AMM designs which would bring the pricing of liquidity tokens into the modern financial era.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11339v3</guid>
      <category>q-fin.MF</category>
      <category>q-fin.PR</category>
      <category>q-fin.RM</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maxim Bichuch, Zachary Feinstein</dc:creator>
    </item>
  </channel>
</rss>
