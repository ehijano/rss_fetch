<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 06 May 2025 04:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Embedded System for Recording and Controlling Hand Hygiene in Healthcare Environments</title>
      <link>https://arxiv.org/abs/2505.01576</link>
      <description>arXiv:2505.01576v1 Announce Type: new 
Abstract: Nowadays, more effective control of hand hygiene (HH) by healthcare teams has become essential. HH control is crucial to prevent cross-contamination and healthcare-associated infections (HAI), according to Brazilian regulatory standards and WHO guidelines. The lack of widespread technology to measure acceptable hygiene rates within hospital environments leads to the practice of a manual sample audit reading, requiring more time for decision making. Thus, the present study addresses the lack of automation technologies for HH, aiming to record, measure, and provide data for internal audits in hospitals. This article introduces an embedded system for HH control and recording, comprising low-cost hardware architecture with IoT connectivity and online monitoring. Results with practical evaluation in a real hospital setting for 3 hours demonstrated the system's effectiveness in recording HH indices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01576v1</guid>
      <category>cs.AR</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Castro, Rafael Z. ROQUE, A. S. Embedded System for Recording and Controlling Hand Hygiene in Healthcare Environments. In Brazilian Conference on Automatics (Automation and Informatics) - CBA, Rio de Janeiro, RJ, 2024</arxiv:journal_reference>
      <dc:creator>Rafael Castro, Alexandre dos Santos Roque</dc:creator>
    </item>
    <item>
      <title>ForgeEDA: A Comprehensive Multimodal Dataset for Advancing EDA</title>
      <link>https://arxiv.org/abs/2505.02016</link>
      <description>arXiv:2505.02016v1 Announce Type: new 
Abstract: We introduce ForgeEDA, an open-source comprehensive circuit dataset across various categories. ForgeEDA includes diverse circuit representations such as Register Transfer Level (RTL) code, Post-mapping (PM) netlists, And-Inverter Graphs (AIGs), and placed netlists, enabling comprehensive analysis and development. We demonstrate ForgeEDA's utility by benchmarking state-of-the-art EDA algorithms on critical tasks such as Power, Performance, and Area (PPA) optimization, highlighting its ability to expose performance gaps and drive advancements. Additionally, ForgeEDA's scale and diversity facilitate the training of AI models for EDA tasks, demonstrating its potential to improve model performance and generalization. By addressing limitations in existing datasets, ForgeEDA aims to catalyze breakthroughs in modern IC design and support the next generation of innovations in EDA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02016v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhengyuan Shi, Zeju Li, Chengyu Ma, Yunhao Zhou, Ziyang Zheng, Jiawei Liu, Hongyang Pan, Lingfeng Zhou, Kezhi Li, Jiaying Zhu, Lingwei Yan, Zhiqiang He, Chenhao Xue, Wentao Jiang, Fan Yang, Guangyu Sun, Xiaoyan Yang, Gang Chen, Chuan Shi, Zhufei Chu, Jun Yang, Qiang Xu</dc:creator>
    </item>
    <item>
      <title>CircuitFusion: Multimodal Circuit Representation Learning for Agile Chip Design</title>
      <link>https://arxiv.org/abs/2505.02168</link>
      <description>arXiv:2505.02168v1 Announce Type: new 
Abstract: The rapid advancements of AI rely on the support of ICs. However, the growing complexity of digital ICs makes the traditional IC design process costly and time-consuming. In recent years, AI-assisted IC design methods have demonstrated great potential, but most methods are task-specific or focus solely on the circuit structure in graph format, overlooking other circuit modalities with rich functional information. In this paper, we introduce CircuitFusion, the first multimodal and implementation-aware circuit encoder. It encodes circuits into general representations that support different downstream circuit design tasks. To learn from circuits, we propose to fuse three circuit modalities: hardware code, structural graph, and functionality summary. More importantly, we identify four unique properties of circuits: parallel execution, functional equivalent transformation, multiple design stages, and circuit reusability. Based on these properties, we propose new strategies for both the development and application of CircuitFusion: 1) During circuit preprocessing, utilizing the parallel nature of circuits, we split each circuit into multiple sub-circuits based on sequential-element boundaries, each sub-circuit in three modalities. 2) During CircuitFusion pre-training, we introduce three self-supervised tasks that utilize equivalent transformations both within and across modalities. 3) When applying CircuitFusion to downstream tasks, we propose a new retrieval-augmented inference method, which retrieves similar known circuits as a reference for predictions. It improves fine-tuning performance and even enables zero-shot inference. Evaluated on five different circuit design tasks, CircuitFusion consistently outperforms the SOTA supervised method specifically developed for every single task, demonstrating its generalizability and ability to learn circuits' inherent properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02168v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Wenji Fang, Shang Liu, Jing Wang, Zhiyao Xie</dc:creator>
    </item>
    <item>
      <title>NeuroSim V1.5: Improved Software Backbone for Benchmarking Compute-in-Memory Accelerators with Device and Circuit-level Non-idealities</title>
      <link>https://arxiv.org/abs/2505.02314</link>
      <description>arXiv:2505.02314v1 Announce Type: new 
Abstract: The exponential growth of artificial intelligence (AI) applications has exposed the inefficiency of conventional von Neumann architectures, where frequent data transfers between compute units and memory create significant energy and latency bottlenecks. Analog Computing-in-Memory (ACIM) addresses this challenge by performing multiply-accumulate (MAC) operations directly in the memory arrays, substantially reducing data movement. However, designing robust ACIM accelerators requires accurate modeling of device- and circuit-level non-idealities. In this work, we present NeuroSim V1.5, introducing several key advances: (1) seamless integration with TensorRT's post-training quantization flow enabling support for more neural networks including transformers, (2) a flexible noise injection methodology built on pre-characterized statistical models, making it straightforward to incorporate data from SPICE simulations or silicon measurements, (3) expanded device support including emerging non-volatile capacitive memories, and (4) up to 6.5x faster runtime than NeuroSim V1.4 through optimized behavioral simulation. The combination of these capabilities uniquely enables systematic design space exploration across both accuracy and hardware efficiency metrics. Through multiple case studies, we demonstrate optimization of critical design parameters while maintaining network accuracy. By bridging high-fidelity noise modeling with efficient simulation, NeuroSim V1.5 advances the design and validation of next-generation ACIM accelerators. All NeuroSim versions are available open-source at https://github.com/neurosim/NeuroSim.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02314v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Read, Ming-Yen Lee, Wei-Hsing Huang, Yuan-Chun Luo, Anni Lu, Shimeng Yu</dc:creator>
    </item>
    <item>
      <title>MaskClip: Detachable Clip-on Piezoelectric Sensing of Mask Surface Vibrations for Real-time Noise-Robust Speech Input</title>
      <link>https://arxiv.org/abs/2505.02180</link>
      <description>arXiv:2505.02180v1 Announce Type: cross 
Abstract: Masks are essential in medical settings and during infectious outbreaks but significantly impair speech communication, especially in environments with background noise. Existing solutions often require substantial computational resources or compromise hygiene and comfort. We propose a novel sensing approach that captures only the wearer's voice by detecting mask surface vibrations using a piezoelectric sensor. Our developed device, MaskClip, employs a stainless steel clip with an optimally positioned piezoelectric sensor to selectively capture speech vibrations while inherently filtering out ambient noise. Evaluation experiments demonstrated superior performance with a low Character Error Rate of 6.1\% in noisy environments compared to conventional microphones. Subjective evaluations by 102 participants also showed high satisfaction scores. This approach shows promise for applications in settings where clear voice communication must be maintained while wearing protective equipment, such as medical facilities, cleanrooms, and industrial environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02180v1</guid>
      <category>cs.SD</category>
      <category>cs.AR</category>
      <category>cs.HC</category>
      <category>eess.AS</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hirotaka Hiraki, Jun Rekimoto</dc:creator>
    </item>
    <item>
      <title>Efficient FPGA Implementation of Time-Domain Popcount for Low-Complexity Machine Learning</title>
      <link>https://arxiv.org/abs/2505.02181</link>
      <description>arXiv:2505.02181v1 Announce Type: cross 
Abstract: Population count (popcount) is a crucial operation for many low-complexity machine learning (ML) algorithms, including Tsetlin Machine (TM)-a promising new ML method, particularly well-suited for solving classification tasks. The inference mechanism in TM consists of propositional logic-based structures within each class, followed by a majority voting scheme, which makes the classification decision. In TM, the voters are the outputs of Boolean clauses. The voting mechanism comprises two operations: popcount for each class and determining the class with the maximum vote by means of an argmax operation.
  While TMs offer a lightweight ML alternative, their performance is often limited by the high computational cost of popcount and comparison required to produce the argmax result. In this paper, we propose an innovative approach to accelerate and optimize these operations by performing them in the time domain. Our time-domain implementation uses programmable delay lines (PDLs) and arbiters to efficiently manage these tasks through delay-based mechanisms. We also present an FPGA design flow for practical implementation of the time-domain popcount, addressing delay skew and ensuring that the behavior matches that of the model's intended functionality. By leveraging the natural compatibility of the proposed popcount with asynchronous architectures, we demonstrate significant improvements in an asynchronous TM, including up to 38% reduction in latency, 43.1% reduction in dynamic power, and 15% savings in resource utilization, compared to synchronous TMs using adder-based popcount.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02181v1</guid>
      <category>cs.LG</category>
      <category>cs.AR</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shengyu Duan, Marcos L. L. Sartori, Rishad Shafik, Alex Yakovlev, Emre Ozer</dc:creator>
    </item>
    <item>
      <title>Content Addressable Memory Design with Reference Resistor for Improved Search Resolution</title>
      <link>https://arxiv.org/abs/2505.02285</link>
      <description>arXiv:2505.02285v1 Announce Type: cross 
Abstract: Despite the parallel in-memory search capabilities of content addressable memories (CAMs), their use in applications is constrained by their limited resolution that worsens as they are scaled to larger arrays or advanced nodes. In this work we present experimental results for a novel back-end-of-line compatible reference resistive device that can significantly improve the search resolution of CAMs implemented with CMOS and beyond-CMOS technologies to less than or equal to 5-bits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02285v1</guid>
      <category>cs.ET</category>
      <category>cs.AR</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siri Narla, Steven J. Koester, Rebecca A. Dawley, Ageeth A. Bol, Piyush Kumar, Azad Naeemi</dc:creator>
    </item>
    <item>
      <title>Machine-Learning-Powered Neural Interfaces for Smart Prosthetics and Diagnostics</title>
      <link>https://arxiv.org/abs/2505.02516</link>
      <description>arXiv:2505.02516v1 Announce Type: cross 
Abstract: Advanced neural interfaces are transforming applications ranging from neuroscience research to diagnostic tools (for mental state recognition, tremor and seizure detection) as well as prosthetic devices (for motor and communication recovery). By integrating complex functions into miniaturized neural devices, these systems unlock significant opportunities for personalized assistive technologies and adaptive therapeutic interventions. Leveraging high-density neural recordings, on-site signal processing, and machine learning (ML), these interfaces extract critical features, identify disease neuro-markers, and enable accurate, low-latency neural decoding. This integration facilitates real-time interpretation of neural signals, adaptive modulation of brain activity, and efficient control of assistive devices. Moreover, the synergy between neural interfaces and ML has paved the way for self-sufficient, ubiquitous platforms capable of operating in diverse environments with minimal hardware costs and external dependencies. In this work, we review recent advancements in AI-driven decoding algorithms and energy-efficient System-on-Chip (SoC) platforms for next-generation miniaturized neural devices. These innovations highlight the potential for developing intelligent neural interfaces, addressing critical challenges in scalability, reliability, interpretability, and user adaptability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02516v1</guid>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>MohammadAli Shaeri, Jinhan Liu, Mahsa Shoaran</dc:creator>
    </item>
    <item>
      <title>Open Challenges for a Production-ready Cloud Environment on top of RISC-V hardware</title>
      <link>https://arxiv.org/abs/2505.02650</link>
      <description>arXiv:2505.02650v1 Announce Type: cross 
Abstract: As part of the Vitamin-V European project, we have built a prototype of a RISC-V cluster managed by OpenStack, with the goal of realizing a functional RISC-V cloud ecosystem. In this poster we explain the hardware and software challenges encountered while porting some elements of OpenStack. We also discuss the current performance gaps that challenge a performance-ready cloud environment over such new ISA, an essential element to fulfill in order to achieve european technological sovereignty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02650v1</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aaron Call, Ramon Nou, Guillem Senabre</dc:creator>
    </item>
    <item>
      <title>PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language Model Inference</title>
      <link>https://arxiv.org/abs/2502.07578</link>
      <description>arXiv:2502.07578v3 Announce Type: replace 
Abstract: Large Language Model (LLM) inference uses an autoregressive manner to generate one token at a time, which exhibits notably lower operational intensity compared to earlier Machine Learning (ML) models such as encoder-only transformers and Convolutional Neural Networks. At the same time, LLMs possess large parameter sizes and use key-value caches to store context information. Modern LLMs support context windows with up to 1 million tokens to generate versatile text, audio, and video content. A large key-value cache unique to each prompt requires a large memory capacity, limiting the inference batch size. Both low operational intensity and limited batch size necessitate a high memory bandwidth. However, contemporary hardware systems for ML model deployment, such as GPUs and TPUs, are primarily optimized for compute throughput. This mismatch challenges the efficient deployment of advanced LLMs and makes users pay for expensive compute resources that are poorly utilized for the memory-bound LLM inference tasks.
  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which harnesses CXL memory expansion capabilities to accommodate substantial LLM sizes, and utilizes near-bank processing units to deliver high memory bandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable CXL network to support peer-to-peer and collective communication primitives across CXL devices. We implement various parallelism strategies to distribute LLMs across these devices. Compared to GPU baselines with maximum supported batch sizes and similar average power, CENT achieves 2.3$\times$ higher throughput and consumes 2.9$\times$ less energy. CENT enhances the Total Cost of Ownership (TCO), generating 5.2$\times$ more tokens per dollar than GPUs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07578v3</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3676641.3716267</arxiv:DOI>
      <dc:creator>Yufeng Gu, Alireza Khadem, Sumanth Umesh, Ning Liang, Xavier Servot, Onur Mutlu, Ravi Iyer, Reetuparna Das</dc:creator>
    </item>
    <item>
      <title>MCMComm: Hardware-Software Co-Optimization for End-to-End Communication in Multi-Chip-Modules</title>
      <link>https://arxiv.org/abs/2505.00041</link>
      <description>arXiv:2505.00041v2 Announce Type: replace 
Abstract: Increasing AI computing demands and slowing transistor scaling have led to the advent of Multi-Chip-Module (MCMs) based accelerators. MCMs enable cost-effective scalability, higher yield, and modular reuse by partitioning large chips into smaller chiplets. However, MCMs come at an increased communication cost, which requires critical analysis and optimization. This paper makes three main contributions: (i) an end-to-end, off-chip congestion-aware and packaging-adaptive analytical framework for detailed analysis, (ii) hardware software co-optimization incorporating diagonal links, on-chip redistribution, and non-uniform workload partitioning to optimize the framework, and (iii) using metaheuristics (genetic algorithms, GA) and mixed integer quadratic programming (MIQP) to solve the optimized framework. Experimental results demonstrate significant performance improvements for CNNs and Vision Transformers, showcasing up to 1.58x and 2.7x EdP (Energy delay Product) improvement using GA and MIQP, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00041v2</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ritik Raj, Shengjie Lin, William Won, Tushar Krishna</dc:creator>
    </item>
    <item>
      <title>LeapFrog: The Rowhammer Instruction Skip Attack</title>
      <link>https://arxiv.org/abs/2404.07878</link>
      <description>arXiv:2404.07878v3 Announce Type: replace-cross 
Abstract: Since its inception, Rowhammer exploits have rapidly evolved into increasingly sophisticated threats compromising data integrity and the control flow integrity of victim processes. Nevertheless, it remains a challenge for an attacker to identify vulnerable targets (i.e., Rowhammer gadgets), understand the outcome of the attempted fault, and formulate an attack that yields useful results.
  In this paper, we present a new type of Rowhammer gadget, called a LeapFrog gadget, which, when present in the victim code, allows an adversary to subvert code execution to bypass a critical piece of code (e.g., authentication check logic, encryption rounds, padding in security protocols). The LeapFrog gadget manifests when the victim code stores the Program Counter (PC) value in the user or kernel stack (e.g., a return address during a function call) which, when tampered with, repositions the return address to a location that bypasses a security-critical code pattern.
  This research also presents a systematic process to identify LeapFrog gadgets. This methodology enables the automated detection of susceptible targets and the determination of optimal attack parameters. We first show the attack on a decision tree algorithm to show the potential implications. Secondly, we employ the attack on OpenSSL to bypass the encryption and reveal the plaintext. We then use our tools to scan the Open Quantum Safe library and report on the number of LeapFrog gadgets in the code. Lastly, we demonstrate this new attack vector through a practical demonstration in a client/server TLS handshake scenario, successfully inducing an instruction skip in a client application. Our findings extend the impact of Rowhammer attacks on control flow and contribute to developing more robust defenses against these increasingly sophisticated threats.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07878v3</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Andrew Adiletta, M. Caner Tol, Kemal Derya, Berk Sunar, Saad Islam</dc:creator>
    </item>
  </channel>
</rss>
