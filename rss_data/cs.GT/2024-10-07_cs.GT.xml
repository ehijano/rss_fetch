<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 08 Oct 2024 03:21:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Harm Ratio: A Novel and Versatile Fairness Criterion</title>
      <link>https://arxiv.org/abs/2410.02977</link>
      <description>arXiv:2410.02977v1 Announce Type: new 
Abstract: Envy-freeness has become the cornerstone of fair division research. In settings where each individual is allocated a disjoint share of collective resources, it is a compelling fairness axiom which demands that no individual strictly prefer the allocation of another individual to their own. Unfortunately, in many real-life collective decision-making problems, the goal is to choose a (common) public outcome that is equally applicable to all individuals, and the notion of envy becomes vacuous. Consequently, this literature has avoided studying fairness criteria that focus on individuals feeling a sense of jealousy or resentment towards other individuals (rather than towards the system), missing out on a key aspect of fairness.
  In this work, we propose a novel fairness criterion, individual harm ratio, which is inspired by envy-freeness but applies to a broad range of collective decision-making settings. Theoretically, we identify minimal conditions under which this criterion and its groupwise extensions can be guaranteed, and study the computational complexity of related problems. Empirically, we conduct experiments with real data to show that our fairness criterion is powerful enough to differentiate between prominent decision-making algorithms for a range of tasks from voting and fair division to participatory budgeting and peer review.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02977v1</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soroush Ebadian, Rupert Freeman, Nisarg Shah</dc:creator>
    </item>
    <item>
      <title>Proportionality in Multiple Dimensions to Design Electoral Systems</title>
      <link>https://arxiv.org/abs/2410.03304</link>
      <description>arXiv:2410.03304v1 Announce Type: new 
Abstract: How to elect the representatives in legislative bodies is a question that every modern democracy has to answer. This design task has to consider various elements so as to fulfill the citizens' expectations and contribute to the maintenance of a healthy democracy. The notion of proportionality, in that the support of a given idea in the house should be nearly proportional to its support in the general public, lies at the core of this design task. In the last decades, demographic aspects beyond political support have been incorporated by requiring that they are also fairly represented in the body, giving rise to a multidimensional version of the apportionment problem. In this work, we provide an axiomatic justification for a recently proposed notion of multidimensional proportionality and extend it to encompass two relevant constraints often used in electoral systems: a threshold on the number of votes that a list needs in order to be eligible and the election of the most-voted candidate in each district. We then build upon these results to design methods based on multidimensional proportionality. We use the Chilean Constitutional Convention election (May 15-16, 2021) results as a testing ground -- where the dimensions are given by political lists, districts, and genders -- and compare the apportionment obtained under each method according to three criteria: proportionality, representativeness, and voting power. While local and global methods exhibit a natural trade-off between local and global proportionality, including the election of most-voted candidates on top of methods based on 3-dimensional proportionality allows us to incorporate both notions while ensuring higher levels of representativeness and a balanced voting power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03304v1</guid>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Javier Cembrano, Jos\'e Correa, Gonzalo D\'iaz, Victor Verdugo</dc:creator>
    </item>
    <item>
      <title>Group Fairness in Peer Review</title>
      <link>https://arxiv.org/abs/2410.03474</link>
      <description>arXiv:2410.03474v1 Announce Type: new 
Abstract: Large conferences such as NeurIPS and AAAI serve as crossroads of various AI fields, since they attract submissions from a vast number of communities. However, in some cases, this has resulted in a poor reviewing experience for some communities, whose submissions get assigned to less qualified reviewers outside of their communities. An often-advocated solution is to break up any such large conference into smaller conferences, but this can lead to isolation of communities and harm interdisciplinary research. We tackle this challenge by introducing a notion of group fairness, called the core, which requires that every possible community (subset of researchers) to be treated in a way that prevents them from unilaterally benefiting by withdrawing from a large conference.
  We study a simple peer review model, prove that it always admits a reviewing assignment in the core, and design an efficient algorithm to find one such assignment. We use real data from CVPR and ICLR conferences to compare our algorithm to existing reviewing assignment algorithms on a number of metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03474v1</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haris Aziz, Evi Micha, Nisarg Shah</dc:creator>
    </item>
    <item>
      <title>Parrondo's effects with aperiodic protocols</title>
      <link>https://arxiv.org/abs/2410.02987</link>
      <description>arXiv:2410.02987v1 Announce Type: cross 
Abstract: In this work, we study the effectiveness of employing archetypal aperiodic sequencing -- namely Fibonacci, Thue-Morse, and Rudin-Saphiro -- on the Parrondian effect. From a capital gain perspective, our results show that these series do yield a Parrondo's Paradox with the Thue-Morse based strategy outperforming not only the other two aperiodic strategies but benchmark Parrondian games with random and periodical ($AABBAABB\ldots$) switching as well. The least performing of the three aperiodic strategies is the Rudin-Shapiro. To elucidate the underlying causes of these results, we analyze the cross-correlation between the capital generated by the switching protocols and that of the isolated losing games. This analysis reveals that a pronounced anti-correlation (below -0.95) with both isolated games is typically required to achieve a robust manifestation of Parrondo's effect. We also study the influence of the sequencing on the capital using the lacunarity and persistence measures. In general, we observe that the switching protocols tend to become less performing in terms of the capital as one increases the persistence and thus approaches the features of an isolated losing game. For the (log-)lacunarity, a property related to heterogeneity, we notice that for small persistence (less than 0.5) the performance increases with the lacunarity with a maximum around 0.4. In respect of this, our work shows that the optimisation of a switching protocol is strongly dependent on a fine tune between persistence and heterogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02987v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.GT</category>
      <category>stat.AP</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcelo A. Pires, Erveton P. Pinto, Rone N. da Silva, S\'ilvio M. Duarte Queir\'os</dc:creator>
    </item>
    <item>
      <title>Impartial Selection with Additive Guarantees via Iterated Deletion</title>
      <link>https://arxiv.org/abs/2205.08979</link>
      <description>arXiv:2205.08979v2 Announce Type: replace 
Abstract: Impartial selection is the selection of an individual from a group based on nominations by other members of the group, in such a way that individuals cannot influence their own chance of selection. For this problem, we give a deterministic mechanism with an additive performance guarantee of $O(n^{(1+\kappa)/2})$ in a setting with $n$ individuals where each individual casts $O(n^{\kappa})$ nominations, where $\kappa\in[0,1]$. For $\kappa=0$, i.e., when each individual casts at most a constant number of nominations, this bound is $O(\sqrt{n})$. This matches the best-known guarantee for randomized mechanisms and a single nomination. For $\kappa=1$ the bound is $O(n)$. This is trivial, as even a mechanism that never selects provides an additive guarantee of $n-1$. We show, however, that it is also best possible: for every deterministic impartial mechanism there exists a situation in which some individual is nominated by every other individual and the mechanism either does not select or selects an individual not nominated by anyone.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.08979v2</guid>
      <category>cs.GT</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Javier Cembrano, Felix Fischer, David Hannon, Max Klimm</dc:creator>
    </item>
    <item>
      <title>Offline congestion games: How feedback type affects data coverage requirement</title>
      <link>https://arxiv.org/abs/2210.13396</link>
      <description>arXiv:2210.13396v2 Announce Type: replace 
Abstract: This paper investigates when one can efficiently recover an approximate Nash Equilibrium (NE) in offline congestion games. The existing dataset coverage assumption in offline general-sum games inevitably incurs a dependency on the number of actions, which can be exponentially large in congestion games. We consider three different types of feedback with decreasing revealed information. Starting from the facility-level (a.k.a., semi-bandit) feedback, we propose a novel one-unit deviation coverage condition and give a pessimism-type algorithm that can recover an approximate NE. For the agent-level (a.k.a., bandit) feedback setting, interestingly, we show the one-unit deviation coverage condition is not sufficient. On the other hand, we convert the game to multi-agent linear bandits and show that with a generalized data coverage assumption in offline linear bandits, we can efficiently recover the approximate NE. Lastly, we consider a novel type of feedback, the game-level feedback where only the total reward from all agents is revealed. Again, we show the coverage assumption for the agent-level feedback setting is insufficient in the game-level feedback setting, and with a stronger version of the data coverage assumption for linear bandits, we can recover an approximate NE. Together, our results constitute the first study of offline congestion games and imply formal separations between different types of feedback.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.13396v2</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Haozhe Jiang, Qiwen Cui, Zhihan Xiong, Maryam Fazel, Simon S. Du</dc:creator>
    </item>
    <item>
      <title>Private Blotto: Viewpoint Competition with Polarized Agents</title>
      <link>https://arxiv.org/abs/2302.14123</link>
      <description>arXiv:2302.14123v3 Announce Type: replace 
Abstract: Social media platforms are responsible for collecting and disseminating vast quantities of content. Recently, however, they have also begun enlisting users in helping annotate this content - for example, to provide context or label disinformation. However, users may act strategically, sometimes reflecting biases (e.g. political) about the "right" label. How can social media platforms design their systems to use human time most efficiently? Historically, competition over multiple items has been explored in the Colonel Blotto game setting (Borel, 1921). However, they were originally designed to model two centrally-controlled armies competing over zero-sum "items", a specific scenario with limited modern-day application. In this work, we propose and study the Private Blotto game, a variant with the key difference that individual agents act independently, without being coordinated by a central "Colonel". We completely characterize the Nash stability of this game and how this impacts the amount of "misallocated effort" of users on unimportant items. We show that the outcome function (aggregating multiple labels on a single item) has a critical impact, and specifically contrast a majority rule outcome (the median) as compared to a smoother outcome function (mean). In general, for median outcomes we show that instances without stable arrangements only occur for relatively few numbers of agents, but stable arrangements may have very high levels of misallocated effort. For mean outcome functions, we show that unstable arrangements can occur even for arbitrarily large numbers of agents, but when stable arrangements exist, they always have low misallocated effort. We conclude by discussing implications our results have for motivating examples in social media platforms and political competition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.14123v3</guid>
      <category>cs.GT</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kate Donahue, Jon Kleinberg</dc:creator>
    </item>
    <item>
      <title>Second-Order Algorithms for Finding Local Nash Equilibria in Zero-Sum Games</title>
      <link>https://arxiv.org/abs/2406.03565</link>
      <description>arXiv:2406.03565v2 Announce Type: replace 
Abstract: Zero-sum games arise in a wide variety of problems, including robust optimization and adversarial learning. However, algorithms deployed for finding a local Nash equilibrium in these games often converge to non-Nash stationary points. This highlights a key challenge: for any algorithm, the stability properties of its underlying dynamical system can cause non-Nash points to be potential attractors. To overcome this challenge, algorithms must account for subtleties involving the curvatures of players' costs. To this end, we leverage dynamical system theory and develop a second-order algorithm for finding a local Nash equilibrium in the smooth, possibly nonconvex-nonconcave, zero-sum game setting. First, we prove that this novel method guarantees convergence to only local Nash equilibria with a local linear convergence rate. We then interpret a version of this method as a modified Gauss-Newton algorithm with local superlinear convergence to the neighborhood of a point that satisfies first-order local Nash equilibrium conditions. In comparison, current related state-of-the-art methods do not offer convergence rate guarantees. Furthermore, we show that this approach naturally generalizes to settings with convex and potentially coupled constraints while retaining earlier guarantees of convergence to only local (generalized) Nash equilibria.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03565v2</guid>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kushagra Gupta, Xinjie Liu, Ross Allen, Ufuk Topcu, David Fridovich-Keil</dc:creator>
    </item>
    <item>
      <title>Selling to a principal and a budget-constrained agent</title>
      <link>https://arxiv.org/abs/2202.10378</link>
      <description>arXiv:2202.10378v3 Announce Type: replace-cross 
Abstract: We analyze a model of selling a single object to a principal-agent pair who want to acquire the object for a firm. The principal and the agent have different assessments of the object's value to the firm. The agent is budget-constrained while the principal is not. The agent participates in the mechanism, but she can (strategically) delegate decision-making to the principal. We derive the revenue-maximizing mechanism in a two-dimensional type space (values of the agent and the principal). We show that below a threshold budget, a mechanism involving two posted prices and three outcomes (one of which involves randomization) is the optimal mechanism for the seller. Otherwise, a single posted price mechanism is optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.10378v3</guid>
      <category>econ.TH</category>
      <category>cs.GT</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Debasis Mishra, Kolagani Paramahamsa</dc:creator>
    </item>
    <item>
      <title>Understanding the Impact of Coalitions between EV Charging Stations</title>
      <link>https://arxiv.org/abs/2404.03919</link>
      <description>arXiv:2404.03919v2 Announce Type: replace-cross 
Abstract: The rapid growth of electric vehicles (EVs) is driving the expansion of charging infrastructure globally. As charging stations become ubiquitous, their substantial electricity consumption can influence grid operation and electricity pricing. Naturally, \textit{some} groups of charging stations, which could be jointly operated by a company, may coordinate to decide their charging profile. While coordination among all charging stations is ideal, it is unclear if coordination of some charging stations is better than no coordination. In this paper, we analyze this intermediate regime between no and full coordination of charging stations. We model EV charging as a non-cooperative aggregative game, where each station's cost is determined by both monetary payments tied to reactive electricity prices on the grid and its sensitivity to deviations from a desired charging profile. We consider a solution concept that we call $\mathcal{C}$-Nash equilibrium, which is tied to a coalition $\mathcal{C}$ of charging stations coordinating to reduce their costs. We provide sufficient conditions, in terms of the demand and sensitivity of charging stations, to determine when independent (aka uncoordinated) operation of charging stations could result in lower overall costs to charging stations, coalition and charging stations outside the coalition. Somewhat counter to common intuition, we show numerical instances where allowing charging stations to operate independently is better than coordinating a subset of stations as a coalition. Jointly, these results provide operators of charging stations insights into how to coordinate their charging behavior, and open several research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03919v2</guid>
      <category>math.OC</category>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sukanya Kudva, Kshitij Kulkarni, Chinmay Maheshwari, Anil Aswani, Shankar Sastry</dc:creator>
    </item>
  </channel>
</rss>
