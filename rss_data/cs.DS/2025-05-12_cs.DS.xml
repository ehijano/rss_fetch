<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 13 May 2025 02:49:43 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>All-to-All Communication with Mobile Edge Adversary: Almost Linearly More Faults, For Free</title>
      <link>https://arxiv.org/abs/2505.05735</link>
      <description>arXiv:2505.05735v1 Announce Type: new 
Abstract: Resilient computation in all-to-all-communication models has attracted tremendous attention over the years. Most of these works assume the classical faulty model which restricts the total number of corrupted edges (or vertices) by some integer fault parameter $f$. A recent work by [Bodwin, Haeupler and Parter, SODA 2024] introduced a stronger notion of fault-tolerance, in the context of graph sparsification, which restricts the degree of the failing edge set $F$, rather than its cardinality. For a subset of faulty edges $F$, the faulty-degree $\mathrm{deg}(F)$ is the largest number of faults in $F$ incident to any given node.
  In this work, we study the communication aspects of this faulty model which allows us to handle almost linearly more edge faults (possibly quadratic), with no extra cost. Our end results are general compilers that take any Congested Clique algorithm and simulate it, in a round by round manner, in the presence of a $\alpha$-Byzantine mobile adversary that controls a $\alpha$-fraction of the edges incident to each node in the fully connected network. For every round $i$, the mobile adversary is allowed to select a distinct set of corrupted edges $F_i$ under the restriction that $\mathrm{deg}(F_i)\leq \alpha n$. In the non-adaptive setting, the $F_i$ sets are selected at the beginning of the simulation, while in the adaptive setting, these edges can be chosen based on the entire history of the protocol up to round $i$.
  We show general compilers for the non-adaptive, adaptive, and deterministic settings. A key component of our algorithms is a new resilient routing scheme which may be of independent interest. Our approach is based on a combination of techniques, including error-correcting-code, locally decodable codes, cover-free families, and sparse recovery sketches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05735v1</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Orr Fischer, Merav Parter</dc:creator>
    </item>
    <item>
      <title>persiansort: an alternative to mergesort inspired by persian rug</title>
      <link>https://arxiv.org/abs/2505.05775</link>
      <description>arXiv:2505.05775v1 Announce Type: new 
Abstract: This paper introduces persiansort, new stable sorting algorithm inspired by Persian rug. Persiansort does not have the weaknesses of mergesort under scenarios involving nearly sorted and partially sorted data, also utilizing less auxiliary memory than mergesort and take advantage of runs. Initial experimental showed, this method is flexible, powerful and works better than mergesort in almost all types of data. Persiansort offers several advantages over merge methods, make it a potential replacement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05775v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Parviz Afereidoon</dc:creator>
    </item>
    <item>
      <title>Smaller and More Flexible Cuckoo Filters</title>
      <link>https://arxiv.org/abs/2505.05847</link>
      <description>arXiv:2505.05847v1 Announce Type: new 
Abstract: Cuckoo filters are space-efficient approximate set membership data structures with a controllable false positive rate (FPR) and zero false negatives, similar to Bloom filters. In contrast to Bloom filters, Cuckoo filters store multi-bit fingerprints of keys in a hash table using variants of Cuckoo hashing, allowing each fingerprint to be stored at a small number of possible locations. Existing Cuckoo filters use fingerprints of $(k+3)$ bits per key and an additional space overhead factor of at least $1.05$ to achieve an FPR of $2^{-k}$. For $k=10$, this amounts to $1.365\, kn$ bits to store $n$ keys, which is better than $1.443\, kn$ bits for Bloom filters. The $+3$ for the fingerprint size is required to balance out the multiplied FPR caused by looking for the fingerprint at several locations. In the original Cuckoo filter, the number of hash table buckets is restricted to a power of 2, which may lead to much larger space overheads, up to $2.1\, (1+3/k)\, kn$ bits.
  We present two improvements of Cuckoo filters. First, we remove the restriction that the number of buckets must be a power of 2 by using a different placement strategy. Second, we reduce the space overhead factor of Cuckoo filters to $1.06 \, (1+2/k)$ by using overlapping windows instead of disjoint buckets to maintain the load threshold of the hash table, while reducing the number of alternative slots where any fingerprint may be found.
  A detailed evaluation demonstrates that the alternative memory layout based on overlapping windows decreases the size of Cuckoo filters not only in theory, but also in practice. A comparison with other state-of-the art filter types, Prefix filters and Vector Quotient filters (VQFs), shows that the reduced space overhead makes windowed Cuckoo filters the smallest filters supporting online insertions, with similarly fast queries, but longer insertion times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05847v1</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Johanna Elena Schmitz, Jens Zentgraf, Sven Rahmann</dc:creator>
    </item>
    <item>
      <title>A Polynomial-Time Approximation Algorithm for Complete Interval Minors</title>
      <link>https://arxiv.org/abs/2505.05997</link>
      <description>arXiv:2505.05997v1 Announce Type: new 
Abstract: As shown by Robertson and Seymour, deciding whether the complete graph $K_t$ is a minor of an input graph $G$ is a fixed parameter tractable problem when parameterized by $t$. From the approximation viewpoint, the gap to fill is quite large, as there is no PTAS for finding the largest complete minor unless $P = NP$, whereas a polytime $O(\sqrt n)$-approximation algorithm was given by Alon, Lingas and Wahl\'en.
  We investigate the complexity of finding $K_t$ as interval minor in ordered graphs (i.e. graphs with a linear order on the vertices, in which intervals are contracted to form minors). Our main result is a polytime $f(t)$-approximation algorithm, where $f$ is triply exponential in $t$ but independent of $n$. The algorithm is based on delayed decompositions and shows that ordered graphs without a $K_t$ interval minor can be constructed via a bounded number of three operations: closure under substitutions, edge union, and concatenation of a stable set. As a byproduct, graphs avoiding $K_t$ as an interval minor have bounded chromatic number.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05997v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Romain Bourneuf, Julien Cocquet, Chaoliang Tang, St\'ephan Thomass\'e</dc:creator>
    </item>
    <item>
      <title>Second Price Matching with Complete Allocation and Degree Constraints</title>
      <link>https://arxiv.org/abs/2505.06005</link>
      <description>arXiv:2505.06005v1 Announce Type: new 
Abstract: We study the Second Price Matching problem, introduced by Azar, Birnbaum, Karlin, and Nguyen in 2009. In this problem, a bipartite graph (bidders and goods) is given, and the profit of a matching is the number of matches containing a second unmatched bidder. Maximizing profit is known to be APX-hard and the current best approximation guarantee is $1/2$. APX-hardness even holds when all degrees are bounded by a constant. In this paper, we investigate the approximability of the problem under regular degree constraints. Our main result is an improved approximation guarantee of $9/10$ for Second Price Matching in $(3,2)$-regular graphs and an exact polynomial-time algorithm for $(d,2)$-regular graphs if $d\geq 4$. Our algorithm and its analysis are based on structural results in non-bipartite matching, in particular the Tutte-Berge formula coupled with novel combinatorial augmentation methods.
  We also introduce a variant of Second Price Matching where all goods have to be matched, which models the setting of expiring goods. We prove that this problem is hard to approximate within a factor better than $(1-1/e)$ and show that the problem can be approximated to a tight $(1-1/e)$ factor by maximizing a submodular function subject to a matroid constraint. We then show that our algorithm also solves this problem exactly on regular degree constrained graphs as above.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06005v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rom Pinchasi, Neta Singer, Lukas Vogl, Jiaye Wei</dc:creator>
    </item>
    <item>
      <title>Learning-Augmented Algorithms for Boolean Satisfiability</title>
      <link>https://arxiv.org/abs/2505.06146</link>
      <description>arXiv:2505.06146v1 Announce Type: new 
Abstract: Learning-augmented algorithms are a prominent recent development in beyond worst-case analysis. In this framework, a problem instance is provided with a prediction (``advice'') from a machine-learning oracle, which provides partial information about an optimal solution, and the goal is to design algorithms that leverage this advice to improve worst-case performance. We study the classic Boolean satisfiability (SAT) decision and optimization problems within this framework using two forms of advice. ``Subset advice" provides a random $\epsilon$ fraction of the variables from an optimal assignment, whereas ``label advice" provides noisy predictions for all variables in an optimal assignment.
  For the decision problem $k$-SAT, by using the subset advice we accelerate the exponential running time of the PPSZ family of algorithms due to Paturi, Pudlak, Saks and Zane, which currently represent the state of the art in the worst case. We accelerate the running time by a multiplicative factor of $2^{-c}$ in the base of the exponent, where $c$ is a function of $\epsilon$ and $k$. For the optimization problem, we show how to incorporate subset advice in a black-box fashion with any $\alpha$-approximation algorithm, improving the approximation ratio to $\alpha + (1 - \alpha)\epsilon$. Specifically, we achieve approximations of $0.94 + \Omega(\epsilon)$ for MAX-$2$-SAT, $7/8 + \Omega(\epsilon)$ for MAX-$3$-SAT, and $0.79 + \Omega(\epsilon)$ for MAX-SAT. Moreover, for label advice, we obtain near-optimal approximation for instances with large average degree, thereby generalizing recent results on MAX-CUT and MAX-$2$-LIN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06146v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Idan Attias, Xing Gao, Lev Reyzin</dc:creator>
    </item>
    <item>
      <title>Equalizing Closeness Centralities via Edge Additions</title>
      <link>https://arxiv.org/abs/2505.06222</link>
      <description>arXiv:2505.06222v1 Announce Type: new 
Abstract: Graph modification problems with the goal of optimizing some measure of a given node's network position have a rich history in the algorithms literature. Less commonly explored are modification problems with the goal of equalizing positions, though this class of problems is well-motivated from the perspective of equalizing social capital, i.e., algorithmic fairness. In this work, we study how to add edges to make the closeness centralities of a given pair of nodes more equal. We formalize two versions of this problem: Closeness Ratio Improvement, which aims to maximize the ratio of closeness centralities between two specified nodes, and Closeness Gap Minimization, which aims to minimize the absolute difference of centralities. We show that both problems are $\textsf{NP}$-hard, and for Closeness Ratio Improvement we present a quasilinear-time $\frac{6}{11}$-approximation, complemented by a bicriteria inapproximability bound. In contrast, we show that Closeness Gap Minimization admits no multiplicative approximation unless $\textsf{P} = \textsf{NP}$. We conclude with a discussion of open directions for this style of problem, including several natural generalizations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06222v1</guid>
      <category>cs.DS</category>
      <category>cs.SI</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alex Crane, Sorelle A. Friedler, Mihir Patel, Blair D. Sullivan</dc:creator>
    </item>
    <item>
      <title>Best of Both Worlds Guarantees for Equitable Allocations</title>
      <link>https://arxiv.org/abs/2505.05809</link>
      <description>arXiv:2505.05809v1 Announce Type: cross 
Abstract: Equitability is a well-studied fairness notion in fair division, where an allocation is equitable if all agents receive equal utility from their allocation. For indivisible items, an exactly equitable allocation may not exist, and a natural relaxation is EQ1, which stipulates that any inequitability should be resolved by the removal of a single item. In this paper, we study equitability in the context of randomized allocations. Specifically, we aim to achieve equitability in expectation (ex ante EQ) and require that each deterministic outcome in the support satisfies ex post EQ1. Such an allocation is commonly known as a `Best of Both Worlds' allocation, and has been studied, e.g., for envy-freeness and MMS.
  We characterize the existence of such allocations using a geometric condition on linear combinations of EQ1 allocations, and use this to give comprehensive results on both existence and computation. For two agents, we show that ex ante EQ and ex post EQ1 allocations always exist and can be computed in polynomial time. For three or more agents, however, such allocations may not exist. We prove that deciding existence of such allocations is strongly NP-complete in general, and weakly NP-complete even for three agents. We also present a pseudo-polynomial time algorithm for a constant number of agents. We show that when agents have binary valuations, best of both worlds allocations that additionally satisfy welfare guarantees exist and are efficiently computable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05809v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Umang Bhaskar, Vishwa Prakash HV, Aditi Sethia,  Rakshitha</dc:creator>
    </item>
    <item>
      <title>New Statistical and Computational Results for Learning Junta Distributions</title>
      <link>https://arxiv.org/abs/2505.05819</link>
      <description>arXiv:2505.05819v1 Announce Type: cross 
Abstract: We study the problem of learning junta distributions on $\{0, 1\}^n$, where a distribution is a $k$-junta if its probability mass function depends on a subset of at most $k$ variables. We make two main contributions:
  - We show that learning $k$-junta distributions is \emph{computationally} equivalent to learning $k$-parity functions with noise (LPN), a landmark problem in computational learning theory.
  - We design an algorithm for learning junta distributions whose statistical complexity is optimal, up to polylogarithmic factors. Computationally, our algorithm matches the complexity of previous (non-sample-optimal) algorithms.
  Combined, our two contributions imply that our algorithm cannot be significantly improved, statistically or computationally, barring a breakthrough for LPN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05819v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Beretta</dc:creator>
    </item>
    <item>
      <title>Scheduled Jacobian Chaining</title>
      <link>https://arxiv.org/abs/2505.06056</link>
      <description>arXiv:2505.06056v1 Announce Type: cross 
Abstract: This paper addresses the efficient computation of Jacobian matrices for programs composed of sequential differentiable subprograms. By representing the overall Jacobian as a chain product of the Jacobians of these subprograms, we reduce the problem to optimizing the sequence of matrix multiplications, known as the Jacobian Matrix Chain Product problem. Solutions to this problem yield "optimal bracketings", which induce a precedence-constraint scheduling problem. We investigate the inherent parallelism in the solutions and develop a new dynamic programming algorithm as a heuristic that incorporates the scheduling. To assess its performance, we benchmark it against the global optimum, which is computed via a branch-and-bound algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06056v1</guid>
      <category>cs.DM</category>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon M\"artens, Uwe Naumann</dc:creator>
    </item>
    <item>
      <title>The Power of Matching for Online Fractional Hedonic Games</title>
      <link>https://arxiv.org/abs/2505.06163</link>
      <description>arXiv:2505.06163v1 Announce Type: cross 
Abstract: We study coalition formation in the framework of fractional hedonic games (FHGs). The objective is to maximize social welfare in an online model where agents arrive one by one and must be assigned to coalitions immediately and irrevocably. For general online FHGs, it is known that computing maximal matchings achieves the optimal competitive ratio, which is, however, unbounded for unbounded agent valuations.
  We achieve a constant competitive ratio in two related settings while carving out further connections to matchings. If algorithms can dissolve coalitions, then the optimal competitive ratio of $\frac{1}{6+4\sqrt{2}}$ is achieved by a matching-based algorithm. Moreover, we perform a tight analysis for the online matching setting under random arrival with an unknown number of agents. This entails a randomized $\frac 16$-competitive algorithm for FHGs, while no algorithm can be better than $\frac 13$-competitive.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06163v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Bullinger, Ren\'e Romen, Alexander Schlenga</dc:creator>
    </item>
    <item>
      <title>Round and Communication Efficient Graph Coloring</title>
      <link>https://arxiv.org/abs/2412.12589</link>
      <description>arXiv:2412.12589v2 Announce Type: replace 
Abstract: In the context of communication complexity, we explore protocols for graph coloring, focusing on the vertex and edge coloring problems in $n$-vertex graphs $G$ with a maximum degree $\Delta$. We consider a scenario where the edges of $G$ are partitioned between two players.
  Our first contribution is a randomized protocol that efficiently finds a $(\Delta + 1)$-vertex coloring of $G$, utilizing $O(n)$ bits of communication in expectation and completing in $O(\log \log n \cdot \log \Delta)$ rounds in the worst case. This advancement represents a significant improvement over the work of Flin and Mittal [Distributed Computing 2025], who achieved the same communication cost but required $O(n)$ rounds in expectation, thereby making a significant reduction in the round complexity.
  Our second contribution is a deterministic protocol to compute a $(2\Delta - 1)$-edge coloring of $G$, which maintains the same $O(n)$ bits of communication and uses only $O(1)$ rounds. We complement the result with a tight $\Omega(n)$-bit lower bound on the communication complexity of the $(2\Delta-1)$-edge coloring problem, while a similar $\Omega(n)$ lower bound for the $(\Delta+1)$-vertex coloring problem has been established by Flin and Mittal [Distributed Computing 2025]. Our result implies a space lower bound of $\Omega(n)$ bits for $(2\Delta - 1)$-edge coloring in the $W$-streaming model, which is the first non-trivial space lower bound for edge coloring in the $W$-streaming model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12589v2</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yi-Jun Chang, Gopinath Mishra, Hung Thuan Nguyen, Farrel D Salim</dc:creator>
    </item>
    <item>
      <title>Inverting Parameterized Burrows-Wheeler Transform</title>
      <link>https://arxiv.org/abs/2503.06970</link>
      <description>arXiv:2503.06970v2 Announce Type: replace 
Abstract: The Burrows-Wheeler Transform (BWT) of a string is an invertible permutation of the string, which can be used for data compression and compact indexes for string pattern matching. Ganguly et al. [SODA, 2017] introduced the parameterized BWT (pBWT) to design compact indexes for parameterized matching (p-matching), a variant of string pattern matching with parameter symbols introduced by Baker [STOC, 1993]. Although the pBWT was inspired by the BWT, it is not obvious whether the pBWT itself is invertible or not. In this paper we show that we can retrieve the original string (up to renaming of parameter symbols) from the pBWT of length $n$ in $O(n^2)$ time and $O(n)$ space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06970v2</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shogen Kawanami, Kento Iseri, Tomohiro I</dc:creator>
    </item>
    <item>
      <title>Complexity of Local Search for Euclidean Clustering Problems</title>
      <link>https://arxiv.org/abs/2312.14916</link>
      <description>arXiv:2312.14916v3 Announce Type: replace-cross 
Abstract: We show that the simplest local search heuristics for two natural Euclidean clustering problems are PLS-complete. First, we show that the Hartigan--Wong method for $k$-Means clustering is PLS-complete, even when $k = 2$. Second, we show the same result for the Flip heuristic for Max Cut, even when the edge weights are given by the (squared) Euclidean distances between the points in some set $\mathcal{X} \subseteq \mathbb{R}^d$; a problem which is equivalent to Min Sum 2-Clustering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14916v3</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bodo Manthey, Nils Morawietz, Jesse van Rhijn, Frank Sommer</dc:creator>
    </item>
    <item>
      <title>Galled Perfect Transfer Networks</title>
      <link>https://arxiv.org/abs/2409.03935</link>
      <description>arXiv:2409.03935v3 Announce Type: replace-cross 
Abstract: Predicting horizontal gene transfers often requires comparative sequence data, but recent work has shown that character-based approaches could also be useful for this task. Notably, perfect transfer networks (PTN) explain the character diversity of a set of taxa for traits that are gained once, rarely lost, but that can be transferred laterally. Characterizing the structure of such characters is an important step towards understanding more complex characters. Although efficient algorithms can infer such networks from character data, they can sometimes predict overly complicated transfer histories. With the goal of recovering the simplest possible scenarios in this model, we introduce galled perfect transfer networks, which are PTNs that are galled trees. Such networks are useful for characters that are incompatible in terms of tree-like evolution, but that do fit in an almost-tree scenario. We provide polynomial-time algorithms for two problems: deciding whether one can add transfer edges to a tree to transform it into a galled PTN, and deciding whether a set of characters are galled-compatible, that is, they can be explained by some galled PTN. We also analyze a real dataset comprising of a bacterial species trees and KEGG functions as characters, and derive several conclusions on the difficulty of explaining characters in a galled tree, which provide several directions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03935v3</guid>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>q-bio.PE</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alitzel L\'opez S\'anchez, Manuel Lafond</dc:creator>
    </item>
    <item>
      <title>Blocked Bloom Filters with Choices</title>
      <link>https://arxiv.org/abs/2501.18977</link>
      <description>arXiv:2501.18977v2 Announce Type: replace-cross 
Abstract: Probabilistic filters are approximate set membership data structures that represent a set of keys in small space, and answer set membership queries without false negative answers, but with a certain allowed false positive probability. Such filters are widely used in database systems, networks, storage systems and in biological sequence analysis because of their fast query times and low space requirements. Starting with Bloom filters in the 1970s, many filter data structures have been developed, each with its own advantages and disadvantages, e.g., Blocked Bloom filters, Cuckoo filters, XOR filters, Ribbon filters, and more.
  We introduce Blocked Bloom filters with choices that work similarly to Blocked Bloom filters, except that for each key there are two (or more) alternative choices of blocks where the key's information may be stored. The result is a filter that partially inherits the advantages of a Blocked Bloom filter, such as the ability to insert keys rapidly online or the ability to slightly overload the filter with only a small penalty to the false positive rate. At the same time, it avoids the major disadvantage of a Blocked Bloom filter, namely the larger space consumption. Our new data structure uses less space at the same false positive rate, or has a lower false positive rate at the same space consumption as a Blocked Bloom filter. We discuss the methodology, engineered implementation, a detailed performance evaluation and use cases in bioinformatics of Blocked Bloom filters with choices, showing that they can be of practical value.
  The implementation of the evaluated filters and the workflows used are provided via Gitlab at https://gitlab.com/rahmannlab/blowchoc-filters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18977v2</guid>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Johanna Elena Schmitz, Jens Zentgraf, Sven Rahmann</dc:creator>
    </item>
  </channel>
</rss>
