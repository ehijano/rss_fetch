<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 16 Aug 2024 04:02:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 16 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>On the effect of noise on fitting linear regression models</title>
      <link>https://arxiv.org/abs/2408.07914</link>
      <description>arXiv:2408.07914v1 Announce Type: new 
Abstract: In this study, we explore the effects of including noise predictors and noise observations when fitting linear regression models. We present empirical and theoretical results that show that double descent occurs in both cases, albeit with contradictory implications: the implication for noise predictors is that complex models are often better than simple ones, while the implication for noise observations is that simple models are often better than complex ones. We resolve this contradiction by showing that it is not the model complexity but rather the implicit shrinkage by the inclusion of noise in the model that drives the double descent. Specifically, we show how noise predictors or observations shrink the estimators of the regression coefficients and make the test error asymptote, and then how the asymptotes of the test error and the ``condition number anomaly'' ensure that double descent occurs. We also show that including noise observations in the model makes the (usually unbiased) ordinary least squares estimator biased and indicates that the ridge regression estimator may need a negative ridge parameter to avoid over-shrinkage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07914v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Insha Ullah, A. H. Welsh</dc:creator>
    </item>
    <item>
      <title>Eigenvalues approximation of integral covariance operators with applications to weighted $L^2$ statistics</title>
      <link>https://arxiv.org/abs/2408.08064</link>
      <description>arXiv:2408.08064v1 Announce Type: new 
Abstract: Finding the eigenvalues connected to the covariance operator of a centred Hilbert-space valued Gaussian process is genuinely considered a hard problem in several mathematical disciplines. In statistics this problem arises for instance in the asymptotic null distribution of goodness-of-fit test statistics of weighted $L^2$-type. For this problem we present the Rayleigh-Ritz method to approximate the eigenvalues. The usefulness of these approximations is shown by high lightening implications such as critical value approximation and theoretical comparison of test statistics by means of Bahadur efficiencies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08064v1</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bruno Ebner, Mar\'ia Dolores Jim\'enez-Gamero, Bojana Milo\v{s}evi\'c</dc:creator>
    </item>
    <item>
      <title>Aliasing and Label-Independent Decomposition of Risk: Beyond the bias-variance trade-off</title>
      <link>https://arxiv.org/abs/2408.08294</link>
      <description>arXiv:2408.08294v1 Announce Type: new 
Abstract: A central problem in data science is to use potentially noisy samples of an unknown function to predict function values for unseen inputs. In classical statistics, the predictive error is understood as a trade-off between the bias and the variance that balances model simplicity with its ability to fit complex functions. However, over-parameterized models exhibit counter-intuitive behaviors, such as "double descent" in which models of increasing complexity exhibit decreasing generalization error. We introduce an alternative paradigm called the generalized aliasing decomposition. We explain the asymptotically small error of complex models as a systematic "de-aliasing" that occurs in the over-parameterized regime. In the limit of large models, the contribution due to aliasing vanishes, leaving an expression for the asymptotic total error we call the invertibility failure of very large models on few training points. Because the generalized aliasing decomposition can be explicitly calculated from the relationship between model class and samples without seeing any data labels, it can answer questions related to experimental design and model selection before collecting data or performing experiments. We demonstrate this approach using several examples, including classical regression problems and a cluster expansion model used in materials science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08294v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark K. Transtrum, Gus L. W. Hart, Tyler J. Jarvis, Jared P. Whitehead</dc:creator>
    </item>
    <item>
      <title>Invariant correlation under marginal transforms</title>
      <link>https://arxiv.org/abs/2306.11188</link>
      <description>arXiv:2306.11188v4 Announce Type: replace 
Abstract: A useful property of independent samples is that their correlation remains the same after applying marginal transforms. This invariance property plays a fundamental role in statistical inference, but does not hold in general for dependent samples. In this paper, we study this invariance property on the Pearson correlation coefficient and its applications. A multivariate random vector is said to have an invariant correlation if its pairwise correlation coefficients remain unchanged under any common marginal transforms. For a bivariate case, we characterize all models of such a random vector via a certain combination of comonotonicity -- the strongest form of positive dependence -- and independence. In particular, we show that the class of exchangeable copulas with invariant correlation is precisely described by what we call positive Fr\'echet copulas. In the general multivariate case, we characterize the set of all invariant correlation matrices via the clique partition polytope. We also propose a positive regression dependent model that admits any prescribed invariant correlation matrix. Finally, we show that all our characterization results of invariant correlation, except one special case, remain the same if the common marginal transforms are confined to the set of increasing ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.11188v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takaaki Koike, Liyuan Lin, Ruodu Wang</dc:creator>
    </item>
    <item>
      <title>Hoeffding and Bernstein inequalities for weighted sums of exchangeable random variables</title>
      <link>https://arxiv.org/abs/2404.06457</link>
      <description>arXiv:2404.06457v2 Announce Type: replace 
Abstract: The aim of this paper is to establish Hoeffding and Bernstein type concentration inequalities for weighted sums of exchangeable random variables. A special case is the i.i.d. setting, where random variables are sampled independently from some distribution (and are therefore exchangeable). In contrast to the existing literature on this problem, our results provide a natural unified view of both the i.i.d. and the exchangeable setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06457v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rina Foygel Barber</dc:creator>
    </item>
    <item>
      <title>Inverting estimating equations for causal inference on quantiles</title>
      <link>https://arxiv.org/abs/2401.00987</link>
      <description>arXiv:2401.00987v2 Announce Type: replace-cross 
Abstract: The causal inference literature frequently focuses on estimating the mean of the potential outcome, whereas quantiles of the potential outcome may carry important additional information. We propose a unified approach, based on the inverse estimating equations, to generalize a class of causal inference solutions from estimating the mean of the potential outcome to its quantiles. We assume that a moment function is available to identify the mean of the threshold-transformed potential outcome, based on which a convenient construction of the estimating equation of quantiles of potential outcome is proposed. In addition, we give a general construction of the efficient influence functions of the mean and quantiles of potential outcomes, and explicate their connection. We motivate estimators for the quantile estimands with the efficient influence function, and develop their asymptotic properties when either parametric models or data-adaptive machine learners are used to estimate the nuisance functions. A broad implication of our results is that one can rework the existing result for mean causal estimands to facilitate causal inference on quantiles. Our general results are illustrated by several analytical and numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.00987v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chao Cheng, Fan Li</dc:creator>
    </item>
  </channel>
</rss>
