<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 13 Feb 2025 05:00:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Debiasing physico-chemical models in air quality monitoring by combining different pollutant concentration measures</title>
      <link>https://arxiv.org/abs/2502.08252</link>
      <description>arXiv:2502.08252v1 Announce Type: new 
Abstract: Air quality monitoring requires to produce accurate estimation of nitrogen dioxide or fine particulate matter concentration maps, at different moments. A typical strategy is to combine different types of data. On the one hand, concentration maps produced by deterministic physicochemical models at urban scale, and on the other hand, concentration measures made at different points, different moments, and by different devices. These measures are provided first by a small number of reference stations, which give reliable measurements of the concentration, and second by a larger number of micro-sensors, which give biased and noisier measurements. The proposed approach consists in modeling the bias of the physicochemical model and estimating the parameters of this bias using all the available concentration measures. Our model relies on a partition of the geographical space of interest into different zones within which the bias is assumed to be modeled by a single affine transformation of the actual concentration. Our approach allows to improve the concentration maps provided by the deterministic models but also to understand the behavior of micro-sensors and their contribution in improving air quality monitoring. We introduce the model, detail its implementation and experiment it through numerical results using datasets collected in Grenoble (France).</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08252v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Auder (LMO), Camille Coron (MIA Paris-Saclay), Jean-Michel Poggi (LMO, IUT Paris - Rives de Seine), Emma Thulliez (LMI)</dc:creator>
    </item>
    <item>
      <title>Filtered Markovian Projection: Dimensionality Reduction in Filtering for Stochastic Reaction Networks</title>
      <link>https://arxiv.org/abs/2502.07918</link>
      <description>arXiv:2502.07918v1 Announce Type: cross 
Abstract: Stochastic reaction networks (SRNs) model stochastic effects for various applications, including intracellular chemical or biological processes and epidemiology. A typical challenge in practical problems modeled by SRNs is that only a few state variables can be dynamically observed. Given the measurement trajectories, one can estimate the conditional probability distribution of unobserved (hidden) state variables by solving a stochastic filtering problem. In this setting, the conditional distribution evolves over time according to an extensive or potentially infinite-dimensional system of coupled ordinary differential equations with jumps, known as the filtering equation. The current numerical filtering techniques, such as the Filtered Finite State Projection (DAmbrosio et al., 2022), are hindered by the curse of dimensionality, significantly affecting their computational performance. To address these limitations, we propose to use a dimensionality reduction technique based on the Markovian projection (MP), initially introduced for forward problems (Ben Hammouda et al., 2024). In this work, we explore how to adapt the existing MP approach to the filtering problem and introduce a novel version of the MP, the Filtered MP, that guarantees the consistency of the resulting estimator. The novel method combines a particle filter with reduced variance and solving the filtering equations in a low-dimensional space, exploiting the advantages of both approaches. The analysis and empirical results highlight the superior computational efficiency of projection methods compared to the existing filtered finite state projection in the large dimensional setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07918v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chiheb Ben Hammouda, Maksim Chupin, Sophia M\"unker, Ra\'ul Tempone</dc:creator>
    </item>
    <item>
      <title>Bad estimation, good prediction: the Lasso in dense regimes</title>
      <link>https://arxiv.org/abs/2502.07959</link>
      <description>arXiv:2502.07959v1 Announce Type: cross 
Abstract: For high-dimensional omics data, sparsity-inducing regularization methods such as the Lasso are widely used and often yield strong predictive performance, even in settings when the assumption of sparsity is likely violated. We demonstrate that under a specific dense model, namely the high-dimensional joint latent variable model, the Lasso produces sparse prediction rules with favorable prediction error bounds, even when the underlying regression coefficient vector is not sparse at all. We further argue that this model better represents many types of omics data than sparse linear regression models. We prove that the prediction bound under this model in fact decreases with increasing number of predictors, and confirm this through simulation examples. These results highlight the need for caution when interpreting sparse prediction rules, as strong prediction accuracy of a sparse prediction rule may not imply underlying biological significance of the individual predictors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07959v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Bratsberg, Magne Thoresen, Jelle J. Goeman</dc:creator>
    </item>
    <item>
      <title>Cost Effectiveness Analyses for Sequential Multiple Assignment Randomized Trials</title>
      <link>https://arxiv.org/abs/2502.07973</link>
      <description>arXiv:2502.07973v1 Announce Type: cross 
Abstract: Sequential multiple assignment randomized trials (SMARTs) have grown in popularity in recent years, and many of their study protocols propose conducting a cost effectiveness analysis of the adaptive strategies embedded within them. The cost effectiveness of these regimes is often proposed to be assessed using incremental cost effectiveness ratios (ICERs). In this paper, we present an estimation and inference procedure for such cost effectiveness measures for the embedded dynamic treatment regimes within a SMART design. In particular, we describe a targeted maximum likelihood estimator for the ICER of a SMART's embedded regimes with influence curve-based inference. We illustrate the performance of these methods using simulations. Throughout, we use as illustration a cost effectiveness analysis for the Adaptive Strategies for Preventing and Treating Lapses of Retention in HIV Care (ADAPT-R; NCT02338739) trial, presenting estimated ICERs (with inference) for embedded regimes aimed at increasing HIV care adherence. This manuscript is one of the first to present cost effectiveness analysis results from a SMART.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07973v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lina M. Montoya, Elvin H. Geng, Harriet F. Adhiambo, Eliud Akama, Starley B. Shade, Assurah Elly, Thomas Odeny, Maya L. Petersen</dc:creator>
    </item>
    <item>
      <title>Trend-encoded Probabilistic Multi-order Model: A Non-Machine Learning Approach for Enhanced Stock Market Forecasts</title>
      <link>https://arxiv.org/abs/2502.08144</link>
      <description>arXiv:2502.08144v1 Announce Type: cross 
Abstract: In recent years, the dominance of machine learning in stock market forecasting has been evident. While these models have shown decreasing prediction errors, their robustness across different datasets has been a concern. A successful stock market prediction model minimizes prediction errors and showcases robustness across various data sets, indicating superior forecasting performance. This study introduces a novel multiple lag order probabilistic model based on trend encoding (TeMoP) that enhances stock market predictions through a probabilistic approach. Results across different stock indexes from nine countries demonstrate that the TeMoP outperforms the state-of-the-art machine learning models in predicting accuracy and stabilization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08144v1</guid>
      <category>q-fin.CP</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peiwan Wang, Chenhao Cui, Yong Li</dc:creator>
    </item>
    <item>
      <title>Scenario analysis with multivariate Bayesian machine learning models</title>
      <link>https://arxiv.org/abs/2502.08440</link>
      <description>arXiv:2502.08440v1 Announce Type: cross 
Abstract: We present an econometric framework that adapts tools for scenario analysis, such as conditional forecasts and generalized impulse response functions, for use with dynamic nonparametric multivariate models. We demonstrate the utility of this approach through an exercise with simulated data, and three real-world applications: (i) scenario-based conditional forecasts aligned with Federal Reserve Bank stress test assumptions, (ii) measuring macroeconomic risk under varying financial conditions, and (iii) the asymmetric effects of US-based financial shocks and their international spillovers. Our results indicate the importance of nonlinearities and asymmetries in the dynamic relationship between macroeconomic and financial variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08440v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Pfarrhofer, Anna Stelzer</dc:creator>
    </item>
    <item>
      <title>Tutorial for Surrogate Endpoint Validation Using Joint modeling and Mediation Analysis</title>
      <link>https://arxiv.org/abs/2502.08443</link>
      <description>arXiv:2502.08443v1 Announce Type: cross 
Abstract: The use of valid surrogate endpoints is an important stake in clinical research to help reduce both the duration and cost of a clinical trial and speed up the evaluation of interesting treatments. Several methods have been proposed in the statistical literature to validate putative surrogate endpoints. Two main approaches have been proposed: the meta-analytic approach and the mediation analysis approach. The former uses data from meta-analyses to derive associations measures between the surrogate and the final endpoint at the individual and trial levels. The latter rather uses the proportion of the treatment effect on the final endpoint through the surrogate as a measure of surrogacy in a causal inference framework. Both approaches have remained separated as the meta-analytic approach does not estimate the treatment effect on the final endpoint through the surrogate while the mediation analysis approach have been limited to single-trial setting. However, these two approaches are complementary. In this work we propose an approach that combines the meta-analytic and mediation analysis approaches using joint modeling for surrogate validation. We focus on the cases where the final endpoint is a time-to-event endpoint (such as time-to-death) and the surrogate is either a time-to-event or a longitudinal biomarker. Two new joint models were proposed depending on the nature of the surrogate. These model are implemented in the R package frailtypack. We illustrate the developed approaches in three applications on real datasets in oncology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08443v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Quentin Le Coent, Virginie Rondeau, Catherine Legrand</dc:creator>
    </item>
    <item>
      <title>A comparison of Dirichlet kernel regression methods on the simplex</title>
      <link>https://arxiv.org/abs/2502.08461</link>
      <description>arXiv:2502.08461v1 Announce Type: cross 
Abstract: An asymmetric Dirichlet kernel version of the Gasser-M\"uller estimator is introduced for regression surfaces on the simplex, extending the univariate analog proposed by Chen [Statist. Sinica, 10(1) (2000), pp. 73-91]. Its asymptotic properties are investigated under the condition that the design points are known and fixed, including an analysis of its mean integrated squared error (MISE) and its asymptotic normality. The estimator is also applicable in a random design setting. A simulation study compares its performance with two recently proposed alternatives: the Nadaraya--Watson estimator with Dirichlet kernel and the local linear smoother with Dirichlet kernel. The results show that the local linear smoother consistently outperforms the others. To illustrate its applicability, the local linear smoother is applied to the GEMAS dataset to analyze the relationship between soil composition and pH levels across various agricultural and grazing lands in Europe.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08461v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hanen Daayeb, Christian Genest, Salah Khardani, Nicolas Klutchnikoff, Fr\'ed\'eric Ouimet</dc:creator>
    </item>
    <item>
      <title>Increasing competitiveness by imbalanced groups: The example of the 48-team FIFA World Cup</title>
      <link>https://arxiv.org/abs/2502.08565</link>
      <description>arXiv:2502.08565v1 Announce Type: cross 
Abstract: A match played in a sports tournament can be called stakeless if at least one team is indifferent to its outcome because it already has qualified or has been eliminated. Such a game threatens fairness since teams may not exert full effort without incentives. This paper suggests a novel classification for stakeless matches according to their expected outcome: they are more costly if the indifferent team is more likely to win by playing honestly. Our approach is illustrated with the 2026 FIFA World Cup, the first edition of the competition with 48 teams. We propose a novel format based on imbalanced groups, which drastically reduces the probability of stakeless matches played by the strongest teams according to Monte Carlo simulations. The new design also increases the uncertainty of match outcomes and requires fewer matches. Governing bodies in sports are encouraged to consider our innovative idea in order to enhance the competitiveness of their tournaments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08565v1</guid>
      <category>math.OC</category>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'aszl\'o Csat\'o, Andr\'as Gyimesi</dc:creator>
    </item>
    <item>
      <title>Estimating Racial and Ethnic Healthcare Quality Disparities Using Exploratory Item Response Theory and Latent Class Item Response Theory Models</title>
      <link>https://arxiv.org/abs/2305.02818</link>
      <description>arXiv:2305.02818v2 Announce Type: replace 
Abstract: Healthcare quality metrics refer to a variety of measures used to characterize what should have been done or not done for a patient or the health consequences of what was or was not done. When estimating healthcare quality, many metrics are measured and combined to provide an overall estimate either at the patient level or at higher levels, such as the provider organization or insurer. Racial and ethnic disparities are defined as the mean difference in quality be tween minorities and Whites not justified by underlying health conditions or patient preferences. Several statistical features of healthcare quality data have been ignored: quality is a theoretical construct not directly observable; quality metrics are measured on different scales or, if measured on the same scale, have different baseline rates; the construct may be multidimensional; and metrics are correlated within-individuals. Balancing health differences across race and ethnicity groups is challenging due to confounding. We provide an approach addressing these features, utilizing exploratory multidimensional item response theory (IRT) models and latent class IRT models to estimate quality, and optimization-based matching to adjust for confounding among the race and ethnicity groups. Quality metrics measured on 93,000 adults with schizophrenia residing in five U.S. states illustrate approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.02818v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sharon-Lise Normand, Katya Zelevinsky, Marcela Horvitz-Lennon</dc:creator>
    </item>
    <item>
      <title>Regression coefficient estimation from remote sensing maps</title>
      <link>https://arxiv.org/abs/2407.13659</link>
      <description>arXiv:2407.13659v4 Announce Type: replace 
Abstract: Remote sensing map products are used to estimate regression coefficients relating environmental variables, such as the effect of conservation zones on deforestation. However, the quality of map products varies, and -- because maps are outputs of complex machine learning algorithms that take in a variety of remotely sensed variables as inputs -- errors are difficult to characterize. Thus, population-level estimates from such maps may be biased. In this paper, we apply prediction-powered inference (PPI) to regression coefficient estimation. PPI generates unbiased estimates by using a small amount of randomly sampled ground truth data to correct for bias in large-scale remote sensing map products. Applying PPI across multiple remote sensing use cases in regression coefficient estimation, we find that it results in estimates that are (1) more reliable than using the map product as if it were 100% accurate and (2) have lower uncertainty than using only the ground truth and ignoring the map product. Empirically, we observe effective sample size increases of up to 17-fold using PPI compared to only using ground truth data. This is the first work to estimate remote sensing regression coefficients without assumptions on the structure of map product errors. Data and code are available at https://github.com/Earth-Intelligence-Lab/uncertainty-quantification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13659v4</guid>
      <category>stat.AP</category>
      <category>econ.GN</category>
      <category>eess.SP</category>
      <category>q-fin.EC</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kerri Lu, Dan M. Kluger, Stephen Bates, Sherrie Wang</dc:creator>
    </item>
    <item>
      <title>Targeting mediating mechanisms of social disparities with an interventional effects framework, applied to the gender pay gap in Western Germany</title>
      <link>https://arxiv.org/abs/2411.07368</link>
      <description>arXiv:2411.07368v3 Announce Type: replace 
Abstract: The Oaxaca-Blinder decomposition is a widely used method to explain social disparities. However, assigning causal meaning to its estimated components requires strong assumptions that often lack explicit justification. This paper emphasizes the importance of clearly defined estimands and their identification when targeting mediating mechanisms of social disparities. Three approaches are distinguished based on their scientific questions and assumptions: a mediation approach and two interventional approaches. The Oaxaca-Blinder decomposition and Monte Carlo simulation-based g-computation are discussed for estimation in relation to these approaches. The latter method is used in an interventional effects analysis of the observed gender pay gap in West Germany, using data from the 2017 German Socio-Economic Panel. Ten mediators, including indicators of human capital and job characteristics, are considered. Key findings indicate that the gender pay gap in log hourly wages could be reduced by up to 86% if these mediators were equally distributed between women and men. Substantial reductions could be achieved by aligning full-time employment and work experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07368v3</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christiane Didden</dc:creator>
    </item>
    <item>
      <title>Invariant Image Reparameterisation: A Unified Approach to Structural and Practical Identifiability and Model Reduction</title>
      <link>https://arxiv.org/abs/2502.04867</link>
      <description>arXiv:2502.04867v2 Announce Type: replace 
Abstract: Both structural and practical parameter non-identifiability present fundamental challenges when using mathematical models to interpret data. This issue is particularly acute in complex, applied areas such as the life sciences or engineering, where determining appropriate model complexity is challenging. While several approaches exist for diagnosing and resolving parameter non-identifiability, including symbolic methods, profile likelihood analysis, and sloppiness analysis, these approaches have distinct limitations and are rarely combined. We present an integrated approach called Invariant Image Reparameterisation (IIR) that incorporates key elements of these methods in a new way. Our approach replaces symbolic computations with numerical calculations at a single reference estimate and an invariance condition that determines when this local calculation holds globally. Parameter combinations determined by this method are naturally ordered by degree of identifiability, and this supports model reduction by replacing a practically non-identified model with a structurally non-identified approximate model. This approximate model can be further parameterised in terms of identified parameters only. By treating parameter combinations determined by our approach as interest parameters within our established likelihood-based Profile-Wise Analysis (PWA) framework, we incorporate uncertainty quantification in terms of likelihood profiles and confidence sets. We provide a Julia library on GitHub (https://github.com/omaclaren/reparam) demonstrating our methodology across a range of mathematical models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04867v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oliver J. Maclaren, Ruanui Nicholson, Joel A. Trent, Joshua Rottenberry, Matthew Simpson</dc:creator>
    </item>
    <item>
      <title>Assessing Heterogeneity of Treatment Effects</title>
      <link>https://arxiv.org/abs/2306.15048</link>
      <description>arXiv:2306.15048v3 Announce Type: replace-cross 
Abstract: Treatment effect heterogeneity is of major interest in economics, but its assessment is often hindered by the fundamental lack of identification of the individual treatment effects. For example, we may want to assess the effect of a poverty reduction measure at different levels of poverty, but the causal effects on wealth at different wealth levels are not identified. Or, we may be interested in the proportion of workers who benefit from the minimum wage increase, but the proportion is not identified in the absence of counterfactuals. This paper derives bounds useful in such situations, which only depend on the marginal distributions of the outcomes. The bounds are nonparametrically sharp, making clear the maximum extent to which the data can speak about the heterogeneity of the treatment effects. An application to microfinance shows that the bounds can be informative even when the average treatment effects are not significant. Another application to the welfare reform identifies a nonnegligible portion of workers who increased and decreased working hours due to the reform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15048v3</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tetsuya Kaji, Jianfei Cao</dc:creator>
    </item>
    <item>
      <title>Nested Instrumental Variables Design: Switcher Average Treatment Effect, Identification, Efficient Estimation and Generalizability</title>
      <link>https://arxiv.org/abs/2405.07102</link>
      <description>arXiv:2405.07102v3 Announce Type: replace-cross 
Abstract: Instrumental variables (IV) are a commonly used tool to estimate causal effects from non-randomized data. An archetype of an IV is a randomized trial with non-compliance where the randomized treatment assignment serves as an IV for the non-ignorable treatment received. Under a monotonicity assumption, a valid IV non-parametrically identifies the average treatment effect among a non-identified, latent complier subgroup, whose generalizability is often under debate. In many studies, there could exist multiple versions of an IV, for instance, different nudges to take the same treatment in different study sites in a multicentre clinical trial. These different versions of an IV may result in different compliance rates and offer a unique opportunity to study IV estimates' generalizability. In this article, we introduce a novel nested IV assumption and study identification of the average treatment effect among two latent subgroups: always-compliers and switchers, who are defined based on the joint potential treatment received under two versions of a binary IV. We derive the efficient influence function for the SWitcher Average Treatment Effect (SWATE) under a non-parametric model and propose efficient estimators. We then propose formal statistical tests of the generalizability of IV estimates under the nested IV framework. We apply the proposed method to the Prostate, Lung, Colorectal and Ovarian (PLCO) Cancer Screening Trial and study the causal effect of colorectal cancer screening and its generalizability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07102v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rui Wang, Ying-Qi Zhao, Oliver Dukes, Bo Zhang</dc:creator>
    </item>
    <item>
      <title>Robust Bayesian approach for reliability prognosis of nondestructive one-shot devices under cumulative risk model</title>
      <link>https://arxiv.org/abs/2406.08867</link>
      <description>arXiv:2406.08867v2 Announce Type: replace-cross 
Abstract: The present study aims to determine the lifetime prognosis of highly durable nondestructive one-shot devices (NOSD) units under a step-stress accelerated life testing (SSALT) experiment applying a cumulative risk model (CRM). In an SSALT experiment, CRM retains the continuity of hazard function by allowing the lag period before the effects of stress change emerge. When prior information about the model parameters is available, Bayesian inference is crucial. In a Bayesian analysis of such lifetime data, conventional likelihood-based Bayesian estimation frequently fails in the presence of outliers in the dataset. This work incorporates a robust Bayesian approach utilizing a robustified posterior based on the density power divergence measure. The order restriction on shape parameters has been incorporated as a prior assumption to reflect the decreasing expected lifetime with increasing stress levels. In testing of hypothesis, a Bayes factor is implemented based on the robustified posterior. In Bayesian estimation, we exploit Hamiltonian Monte Carlo, which has certain advantages over the conventional Metropolis-Hastings algorithms. Further, the influence functions are examined to evaluate the robust behaviour of the estimators and the Bayes factor. Finally, the analytical development is validated through a simulation study and a real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08867v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shanya Baghel, Shuvashree Mondal</dc:creator>
    </item>
    <item>
      <title>Balancing events, not patients, maximizes power of the logrank test: and other insights on unequal randomization in survival trials</title>
      <link>https://arxiv.org/abs/2407.03420</link>
      <description>arXiv:2407.03420v2 Announce Type: replace-cross 
Abstract: We revisit the question of what randomization ratio (RR) maximizes power of the logrank test in event-driven survival trials under proportional hazards (PH). By comparing three approximations of the logrank test (Schoenfeld, Freedman, Rubinstein) to empirical simulations, we find that the RR that maximizes power is the RR that balances number of events across treatment arms at the end of the trial. This contradicts the common misconception implied by Schoenfeld's approximation that 1:1 randomization maximizes power. Besides power, we consider other factors that might influence the choice of RR (accrual, trial duration, sample size, etc.). We perform simulations to better understand how unequal randomization might impact these factors in practice. Altogether, we derive 6 insights to guide statisticians in the design of survival trials considering unequal randomization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03420v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Godwin Yung, Kaspar Rufibach, Marcel Wolbers, Ray Lin, Yi Liu</dc:creator>
    </item>
    <item>
      <title>Characterization of point-source transient events with a rolling-shutter compressed sensing system</title>
      <link>https://arxiv.org/abs/2408.16868</link>
      <description>arXiv:2408.16868v2 Announce Type: replace-cross 
Abstract: Point-source transient events (PSTEs) - optical events that are both extremely fast and extremely small - pose several challenges to an imaging system. Due to their speed, accurately characterizing such events often requires detectors with very high frame rates. Due to their size, accurately detecting such events requires maintaining coverage over an extended field-of-view, often through the use of imaging focal plane arrays (FPA) with a global shutter readout. Traditional imaging systems that meet these requirements are costly in terms of price, size, weight, power consumption, and data bandwidth, and there is a need for cheaper solutions with adequate temporal and spatial coverage. To address these issues, we develop a novel compressed sensing algorithm adapted to the rolling shutter readout of an imaging system. This approach enables reconstruction of a PSTE signature at the sampling rate of the rolling shutter, offering a 1-2 order of magnitude temporal speedup and a proportional reduction in data bandwidth. We present empirical results demonstrating accurate recovery of PSTEs using measurements that are spatially undersampled by a factor of 25, and our simulations show that, relative to other compressed sensing algorithms, our algorithm is both faster and yields higher quality reconstructions. We also present theoretical results characterizing our algorithm and corroborating simulations. The potential impact of our work includes the development of much faster, cheaper sensor solutions for PSTE detection and characterization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16868v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>physics.optics</category>
      <category>stat.AP</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Frank Qiu, Joshua Michalenko, Lilian K. Casias, Cameron J. Radosevich, Jon Slater, Eric A. Shields</dc:creator>
    </item>
    <item>
      <title>Bias Mitigation in Matched Observational Studies with Continuous Treatments: Calipered Non-Bipartite Matching and Bias-Corrected Estimation and Inference</title>
      <link>https://arxiv.org/abs/2409.11701</link>
      <description>arXiv:2409.11701v2 Announce Type: replace-cross 
Abstract: In matched observational studies with continuous treatments, individuals with different treatment doses but the same or similar covariate values are paired for causal inference. While inexact covariate matching (i.e., covariate imbalance after matching) is common in practice, previous matched studies with continuous treatments have often overlooked this issue as long as post-matching covariate balance meets certain criteria. Through re-analyzing a matched observational study on the social distancing effect on COVID-19 case counts, we show that this routine practice can introduce severe bias for causal inference. Motivated by this finding, we propose a general framework for mitigating bias due to inexact matching in matched observational studies with continuous treatments, covering the matching, estimation, and inference stages. In the matching stage, we propose a carefully designed caliper that incorporates both covariate and treatment dose information to improve matching for downstream treatment effect estimation and inference. For the estimation and inference, we introduce a bias-corrected Neyman estimator paired with a corresponding bias-corrected variance estimator. The effectiveness of our proposed framework is demonstrated through numerical studies and a re-analysis of the aforementioned observational study on the effect of social distancing on COVID-19 case counts. An open-source R package for implementing our framework has also been developed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11701v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anthony Frazier, Siyu Heng, Wen Zhou</dc:creator>
    </item>
  </channel>
</rss>
