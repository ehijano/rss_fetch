<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 15 Jan 2025 02:34:40 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Simple proof of robustness for Bayesian heavy-tailed linear regression models</title>
      <link>https://arxiv.org/abs/2501.06349</link>
      <description>arXiv:2501.06349v1 Announce Type: new 
Abstract: In the Bayesian literature, a line of research called resolution of conflict is about the characterization of robustness against outliers of statistical models. The robustness characterization of a model is achieved by establishing the limiting behaviour of the posterior distribution under an asymptotic framework in which the outliers move away from the bulk of the data. The proofs of the robustness characterization results, especially the recent ones for regression models, are technical and not intuitive, limiting the accessibility and preventing the development of theory in that line of research. We highlight that the proof complexity is due to the generality of the assumptions on the prior distribution. To address the issue of accessibility, we present a significantly simpler proof for a linear regression model with a specific prior distribution corresponding to the one typically used. The proof is intuitive and uses classical results of probability theory. To promote the development of theory in resolution of conflict, we highlight which steps are only valid for linear regression and which ones are valid in greater generality. The generality of the assumption on the error distribution is also appealing; essentially, it can be any distribution with regularly varying or log-regularly varying tails. So far, there does not exist a result in such generality for models with regularly varying distributions. Finally, we analyse the necessity of the assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06349v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philippe Gagnon</dc:creator>
    </item>
    <item>
      <title>Pathwise guessing in categorical time series with unbounded alphabets</title>
      <link>https://arxiv.org/abs/2501.06547</link>
      <description>arXiv:2501.06547v2 Announce Type: new 
Abstract: The following learning problem arises naturally in various applications: Given a finite sample from a categorical or count time series, can we learn a function of the sample that (nearly) maximizes the probability of correctly guessing the values of a given portion of the data using the values from the remaining parts? Unlike the classical task of estimating conditional probabilities in a stochastic process, our approach avoids explicitly estimating these probabilities. We propose a non-parametric guessing function with a learning rate that is independent of the alphabet size. Our analysis focuses on a broad class of time series models that encompasses finite-order Markov chains, some hidden Markov chains, Poisson regression for count process, and one-dimensional Gibbs measures. Additionally, we establish a minimax lower bound for the rate of convergence of the risk associated with our guessing problem. This lower bound matches the upper bound achieved by our estimator up to a logarithmic factor, demonstrating its near-optimality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06547v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J. -R. Chazottes, S. Gallo, D. Takahashi</dc:creator>
    </item>
    <item>
      <title>Singularities in Bayesian Inference: Crucial or Overstated?</title>
      <link>https://arxiv.org/abs/2501.06618</link>
      <description>arXiv:2501.06618v1 Announce Type: new 
Abstract: Over the past two decades, shrinkage priors have become increasingly popular, and many proposals can be found in the literature. These priors aim to shrink small effects to zero while maintaining true large effects. Horseshoe-type priors have been particularly successful in various applications, mainly due to their computational advantages. However, there is no clear guidance on choosing the most appropriate prior for a specific setting. In this work, we propose a framework that encompasses a large class of shrinkage distributions, including priors with and without a singularity at zero. By reframing such priors in the context of reliability theory and wealth distributions, we provide insights into the prior parameters and shrinkage properties. The paper's key contributions are based on studying the folded version of such distributions, which we refer to as the Gambel distribution. The Gambel can be rewritten as the ratio between a Generalised Gamma and a Generalised Beta of the second kind. This representation allows us to gain insights into the behaviours near the origin and along the tails, compute measures to compare their distributional properties, derive consistency results, devise MCMC schemes for posterior inference and ultimately provide guidance on the choice of the hyperparameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06618v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maria De Iorio, Andreas Heinecke, Beatrice Franzolini, Rafael Cabral</dc:creator>
    </item>
    <item>
      <title>High-order Accurate Inference on Manifolds</title>
      <link>https://arxiv.org/abs/2501.06652</link>
      <description>arXiv:2501.06652v1 Announce Type: new 
Abstract: We present a new framework for statistical inference on Riemannian manifolds that achieves high-order accuracy, addressing the challenges posed by non-Euclidean parameter spaces frequently encountered in modern data science. Our approach leverages a novel and computationally efficient procedure to reach higher-order asymptotic precision. In particular, we develop a bootstrap algorithm on Riemannian manifolds that is both computationally efficient and accurate for hypothesis testing and confidence region construction. Although locational hypothesis testing can be reformulated as a standard Euclidean problem, constructing high-order accurate confidence regions necessitates careful treatment of manifold geometry. To this end, we establish high-order asymptotics under a fixed normal chart centered at the true parameter, thereby enabling precise expansions that incorporate curvature effects. We demonstrate the versatility of this framework across various manifold settings-including spheres, the Stiefel manifold, fixed-rank matrices manifolds, and rank-one tensor manifolds-and, for Euclidean submanifolds, introduce a class of projection-like coordinate charts with strong consistency properties. Finally, numerical studies confirm the practical merits of the proposed procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06652v1</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Chengzhu Huang, Anru R. Zhang</dc:creator>
    </item>
    <item>
      <title>Best-possible bounds on the set of copulas with a given value of Gini's gamma</title>
      <link>https://arxiv.org/abs/2501.06915</link>
      <description>arXiv:2501.06915v1 Announce Type: new 
Abstract: In this note, pointwise best-possible (lower and upper) bounds on the set of copulas with a given value of the Gini's gamma coefficient are established. It is shown that, unlike the best-possible bounds on the set of copulas with a given value of other known measures such as Kendall's tau, Spearman's rho or Blomqvist's beta, the bounds found are not necessarily copulas, but proper quasi-copulas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06915v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.22111/IJFS.2022.7155</arxiv:DOI>
      <arxiv:journal_reference>Iranian Journal of Fuzzy Systems, Volume 19, Number 5, (2022), pp. 35-45</arxiv:journal_reference>
      <dc:creator>Manuel \'Ubeda-Flores</dc:creator>
    </item>
    <item>
      <title>A Proof of Strong Consistency of Maximum Likelihood Estimator for Independent Non-Identically Distributed Data</title>
      <link>https://arxiv.org/abs/2501.07257</link>
      <description>arXiv:2501.07257v1 Announce Type: new 
Abstract: We give a general proof of the strong consistency of the Maximum Likelihood Estimator for the case of independent non-identically distributed (i.n.i.d) data, assuming that the density functions of the random variables follow a particular set of assumptions. Our proof is based on the works of Wald~\cite{wald1949note}, Goel~\cite{goel1974note}, and Ferguson~\cite{ferguson2017course}. We use this result to prove the strong consistency of a Maximum Likelihood Estimator for Orbit Determination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07257v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ricardo Ferreira, Filipa Valdeira, Marta Guimar\~aes, Cl\'audia Soares</dc:creator>
    </item>
    <item>
      <title>Statistical learnability of smooth boundaries via pairwise binary classification with deep ReLU networks</title>
      <link>https://arxiv.org/abs/2501.07571</link>
      <description>arXiv:2501.07571v1 Announce Type: new 
Abstract: The topic of nonparametric estimation of smooth boundaries is extensively studied in the conventional setting where pairs of single covariate and response variable are observed. However, this traditional setting often suffers from the cost of data collection. Recent years have witnessed the consistent development of learning algorithms for binary classification problems where one can instead observe paired covariates and binary variable representing the statistical relationship between the covariates. In this work, we theoretically study the question of whether multiple smooth boundaries are learnable if the pairwise binary classification setting is considered. We investigate the question with the statistical dependence of paired covariates to develop a learning algorithm using vector-valued functions. The main theorem shows that there is an empirical risk minimization algorithm in a class of deep ReLU networks such that it produces a consistent estimator for indicator functions defined with smooth boundaries. We also discuss how the pairwise binary classification setting is different from the conventional settings, focusing on the structural condition of function classes. As a by-product, we apply the main theorem to a multiclass nonparametric classification problem where the estimation performance is measured by the excess risk in terms of misclassification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07571v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hiroki Waida, Takafumi Kanamori</dc:creator>
    </item>
    <item>
      <title>CeViT: Copula-Enhanced Vision Transformer in multi-task learning and bi-group image covariates with an application to myopia screening</title>
      <link>https://arxiv.org/abs/2501.06540</link>
      <description>arXiv:2501.06540v1 Announce Type: cross 
Abstract: We aim to assist image-based myopia screening by resolving two longstanding problems, "how to integrate the information of ocular images of a pair of eyes" and "how to incorporate the inherent dependence among high-myopia status and axial length for both eyes." The classification-regression task is modeled as a novel 4-dimensional muti-response regression, where discrete responses are allowed, that relates to two dependent 3rd-order tensors (3D ultrawide-field fundus images). We present a Vision Transformer-based bi-channel architecture, named CeViT, where the common features of a pair of eyes are extracted via a shared Transformer encoder, and the interocular asymmetries are modeled through separated multilayer perceptron heads. Statistically, we model the conditional dependence among mixture of discrete-continuous responses given the image covariates by a so-called copula loss. We establish a new theoretical framework regarding fine-tuning on CeViT based on latent representations, allowing the black-box fine-tuning procedure interpretable and guaranteeing higher relative efficiency of fine-tuning weight estimation in the asymptotic setting. We apply CeViT to an annotated ultrawide-field fundus image dataset collected by Shanghai Eye \&amp; ENT Hospital, demonstrating that CeViT enhances the baseline model in both accuracy of classifying high-myopia and prediction of AL on both eyes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06540v1</guid>
      <category>cs.CV</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chong Zhong, Yang Li, Jinfeng Xu, Xiang Fu, Yunhao Liu, Qiuyi Huang, Danjuan Yang, Meiyan Li, Aiyi Liu, Alan H. Welsh, Xingtao Zhou, Bo Fu, Catherine C. Liu</dc:creator>
    </item>
    <item>
      <title>Doubly Robust Inference on Causal Derivative Effects for Continuous Treatments</title>
      <link>https://arxiv.org/abs/2501.06969</link>
      <description>arXiv:2501.06969v1 Announce Type: cross 
Abstract: Statistical methods for causal inference with continuous treatments mainly focus on estimating the mean potential outcome function, commonly known as the dose-response curve. However, it is often not the dose-response curve but its derivative function that signals the treatment effect. In this paper, we investigate nonparametric inference on the derivative of the dose-response curve with and without the positivity condition. Under the positivity and other regularity conditions, we propose a doubly robust (DR) inference method for estimating the derivative of the dose-response curve using kernel smoothing. When the positivity condition is violated, we demonstrate the inconsistency of conventional inverse probability weighting (IPW) and DR estimators, and introduce novel bias-corrected IPW and DR estimators. In all settings, our DR estimator achieves asymptotic normality at the standard nonparametric rate of convergence. Additionally, our approach reveals an interesting connection to nonparametric support and level set estimation problems. Finally, we demonstrate the applicability of our proposed estimators through simulations and a case study of evaluating a job training program.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06969v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yikun Zhang, Yen-Chi Chen</dc:creator>
    </item>
    <item>
      <title>Monotone Curve Estimation via Convex Duality</title>
      <link>https://arxiv.org/abs/2501.06975</link>
      <description>arXiv:2501.06975v2 Announce Type: cross 
Abstract: A principal curve serves as a powerful tool for uncovering underlying structures of data through 1-dimensional smooth and continuous representations. On the basis of optimal transport theories, this paper introduces a novel principal curve framework constrained by monotonicity with rigorous theoretical justifications. We establish statistical guarantees for our monotone curve estimate, including expected empirical and generalized mean squared errors, while proving the existence of such estimates. These statistical foundations justify adopting the popular early stopping procedure in machine learning to implement our numeric algorithm with neural networks. Comprehensive simulation studies reveal that the proposed monotone curve estimate outperforms competing methods in terms of accuracy when the data exhibits a monotonic structure. Moreover, through two real-world applications on future prices of copper, gold, and silver, and avocado prices and sales volume, we underline the robustness of our curve estimate against variable transformation, further confirming its effective applicability for noisy and complex data sets. We believe that this monotone curve-fitting framework offers significant potential for numerous applications where monotonic relationships are intrinsic or need to be imposed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06975v2</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tongseok Lim, Kyeongsik Nam, Jinwon Sohn</dc:creator>
    </item>
    <item>
      <title>Statistical and Computational Efficiency for Smooth Tensor Estimation with Unknown Permutations</title>
      <link>https://arxiv.org/abs/2111.04681</link>
      <description>arXiv:2111.04681v2 Announce Type: replace 
Abstract: We consider the problem of structured tensor denoising in the presence of unknown permutations. Such data problems arise commonly in recommendation system, neuroimaging, community detection, and multiway comparison applications. Here, we develop a general family of smooth tensor models up to arbitrary index permutations; the model incorporates the popular tensor block models and Lipschitz hypergraphon models as special cases. We show that a constrained least-squares estimator in the block-wise polynomial family achieves the minimax error bound. A phase transition phenomenon is revealed with respect to the smoothness threshold needed for optimal recovery. In particular, we find that a polynomial of degree up to $(m-2)(m+1)/2$ is sufficient for accurate recovery of order-$m$ tensors, whereas higher degree exhibits no further benefits. This phenomenon reveals the intrinsic distinction for smooth tensor estimation problems with and without unknown permutations. Furthermore, we provide an efficient polynomial-time Borda count algorithm that provably achieves optimal rate under monotonicity assumptions. The efficacy of our procedure is demonstrated through both simulations and Chicago crime data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.04681v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1080/01621459.2024.2419114</arxiv:DOI>
      <arxiv:journal_reference>Journal of the American Statistical Association (2024): 1-14</arxiv:journal_reference>
      <dc:creator>Chanwoo Lee, Miaoyan Wang</dc:creator>
    </item>
    <item>
      <title>Normal approximation for the posterior in exponential families</title>
      <link>https://arxiv.org/abs/2209.08806</link>
      <description>arXiv:2209.08806v3 Announce Type: replace 
Abstract: In this paper, we obtain quantitative, non-asymptotic, and data-dependent \textit{Bernstein-von Mises type} bounds on the normal approximation of the posterior distribution in exponential family models with arbitrary centring and scaling. Our bounds, stated in the total variation and Wasserstein distances, are valid for univariate and multivariate posteriors alike, and do not require a conjugate prior setting. They are obtained through a refined version of Stein's method of comparison of operators that allows for improved dimensional dependence in high-dimensional settings and may also be of interest in other problems. Our approach is rather flexible and, in certain settings, allows for the derivation of bounds with rates of convergence faster than the usual \( O(n^{-1/2}) \) rate (when \( n \) is the sample size). We illustrate our findings on a variety of exponential family distributions, including the Weibull, multinomial, and linear regression with unknown variance. The resulting bounds have an explicit dependence on the prior distribution and on sufficient statistics of the data from the sample, and thus provide insight into how these factors affect the quality of the normal approximation. Insights from our examples include identification of conditions under which faster \( O(n^{-1}) \) convergence rates occur for Bernoulli data, illustrations of how the quality of the normal approximation is influenced by the choice of standardisation, and dimensional dependence in high-dimensional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.08806v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adrian Fischer, Robert E. Gaunt, Gesine Reinert, Yvik Swan</dc:creator>
    </item>
    <item>
      <title>Optimality of the Right-Invariant Prior</title>
      <link>https://arxiv.org/abs/2412.12054</link>
      <description>arXiv:2412.12054v5 Announce Type: replace 
Abstract: We discuss optimal next-sample prediction for families of probability distributions with a locally compact topological group structure. The right-invariant prior was previously shown to yield a posterior predictive distribution minimizing the worst-case Kullback-Leibler risk among all predictive procedures. However, the assumptions for the proof are so strong that they rarely hold in practice and it is unclear when the density functions used in the proof exist. Therefore, we provide a measure-theoretic proof using a more appropriate set of assumptions. As an application, we show a strong optimality result for next-sample prediction for multivariate normal distributions. We also numerically evaluate prediction with the right-invariant prior against other objective priors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12054v5</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jannis Bolik, Thomas Hofmann</dc:creator>
    </item>
    <item>
      <title>Coloured Graphical Models and their Symmetries</title>
      <link>https://arxiv.org/abs/2012.01905</link>
      <description>arXiv:2012.01905v5 Announce Type: replace-cross 
Abstract: Coloured graphical models are Gaussian statistical models determined by an undirected coloured graph. These models can be described by linear spaces of symmetric matrices. We outline a relationship between the symmetries of the graph and the linear forms that vanish on the reciprocal variety of the model. In particular, we give four families for which such linear forms are completely described by symmetries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2012.01905v5</guid>
      <category>math.AG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Isobel Davies, Orlando Marigliano</dc:creator>
    </item>
    <item>
      <title>Measure transfer via stochastic slicing and matching</title>
      <link>https://arxiv.org/abs/2307.05705</link>
      <description>arXiv:2307.05705v2 Announce Type: replace-cross 
Abstract: This paper studies iterative schemes for measure transfer and approximation problems, which are defined through a slicing-and-matching procedure. Similar to the sliced Wasserstein distance, these schemes benefit from the availability of closed-form solutions for the one-dimensional optimal transport problem and the associated computational advantages. While such schemes have already been successfully utilized in data science applications, not too many results on their convergence are available. The main contribution of this paper is an almost sure convergence proof for stochastic slicing-and-matching schemes. The proof builds on an interpretation as a stochastic gradient descent scheme on the Wasserstein space. Numerical examples on step-wise image morphing are demonstrated as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.05705v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shiying Li, Caroline Moosmueller</dc:creator>
    </item>
    <item>
      <title>Double-Estimation-Friendly Inference for High-Dimensional Measurement Error Models with Non-Sparse Adaptability</title>
      <link>https://arxiv.org/abs/2409.16463</link>
      <description>arXiv:2409.16463v3 Announce Type: replace-cross 
Abstract: In this paper, we introduce an innovative testing procedure for assessing individual hypotheses in high-dimensional linear regression models with measurement errors. This method remains robust even when either the X-model or Y-model is misspecified. We develop a double robust score function that maintains a zero expectation if one of the models is incorrect, and we construct a corresponding score test. We first show the asymptotic normality of our approach in a low-dimensional setting, and then extend it to the high-dimensional models. Our analysis of high-dimensional settings explores scenarios both with and without the sparsity condition, establishing asymptotic normality and non-trivial power performance under local alternatives. Simulation studies and real data analysis demonstrate the effectiveness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16463v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shijie Cui, Xu Guo, Songshan Yang, Zhe Zhang</dc:creator>
    </item>
    <item>
      <title>Causal inference targeting a concentration index for studies of health inequalities</title>
      <link>https://arxiv.org/abs/2410.08849</link>
      <description>arXiv:2410.08849v2 Announce Type: replace-cross 
Abstract: A concentration index, a standardized covariance between a health variable and relative income ranks, is often used to quantify income-related health inequalities. There is a lack of formal approach to study the effect of an exposure, e.g., education, on such measures of inequality. In this paper we contribute by filling this gap and developing the necessary theory and method. Thus, we define a counterfactual concentration index for different levels of an exposure. We give conditions for their identification, and then deduce their efficient influence function. This allows us to propose estimators, which are regular asymptotic linear under certain conditions. In particular, these estimators are $\sqrt n$-consistent and asymptotically normal, as well as locally efficient. The implementation of the estimators is based on the fit of several nuisance functions. The estimators proposed have rate robustness properties allowing for convergence rates slower than $\sqrt{n}$-rate for some of the nuisance function fits. The relevance of the asymptotic results for finite samples is studied with simulation experiments. We also present a case study of the effect of education on income-related health inequalities for a Swedish cohort.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08849v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Ghasempour, Xavier de Luna, Per E. Gustafsson</dc:creator>
    </item>
    <item>
      <title>Learning Spectral Methods by Transformers</title>
      <link>https://arxiv.org/abs/2501.01312</link>
      <description>arXiv:2501.01312v3 Announce Type: replace-cross 
Abstract: Transformers demonstrate significant advantages as the building block of modern LLMs. In this work, we study the capacities of Transformers in performing unsupervised learning. We show that multi-layered Transformers, given a sufficiently large set of pre-training instances, are able to learn the algorithms themselves and perform statistical estimation tasks given new instances. This learning paradigm is distinct from the in-context learning setup and is similar to the learning procedure of human brains where skills are learned through past experience. Theoretically, we prove that pre-trained Transformers can learn the spectral methods and use the classification of bi-class Gaussian mixture model as an example. Our proof is constructive using algorithmic design techniques. Our results are built upon the similarities of multi-layered Transformer architecture with the iterative recovery algorithms used in practice. Empirically, we verify the strong capacity of the multi-layered (pre-trained) Transformer on unsupervised learning through the lens of both the PCA and the Clustering tasks performed on the synthetic and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01312v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yihan He, Yuan Cao, Hong-Yu Chen, Dennis Wu, Jianqing Fan, Han Liu</dc:creator>
    </item>
  </channel>
</rss>
