<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 27 Feb 2025 02:53:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A generalized dual potential for inelastic Constitutive Artificial Neural Networks: A JAX implementation at finite strains</title>
      <link>https://arxiv.org/abs/2502.17490</link>
      <description>arXiv:2502.17490v1 Announce Type: cross 
Abstract: We present a methodology for designing a generalized dual potential, or pseudo potential, for inelastic Constitutive Artificial Neural Networks (iCANNs). This potential, expressed in terms of stress invariants, inherently satisfies thermodynamic consistency for large deformations. In comparison to our previous work, the new potential captures a broader spectrum of material behaviors, including pressure-sensitive inelasticity.
  To this end, we revisit the underlying thermodynamic framework of iCANNs for finite strain inelasticity and derive conditions for constructing a convex, zero-valued, and non-negative dual potential. To embed these principles in a neural network, we detail the architecture's design, ensuring a priori compliance with thermodynamics.
  To evaluate the proposed architecture, we study its performance and limitations discovering visco-elastic material behavior, though the method is not limited to visco-elasticity. In this context, we investigate different aspects in the strategy of discovering inelastic materials. Our results indicate that the novel architecture robustly discovers interpretable models and parameters, while autonomously revealing the degree of inelasticity.
  The iCANN framework, implemented in JAX, is publicly accessible at https://doi.org/10.5281/zenodo.14894687.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17490v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.mtrl-sci</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hagen Holthusen, Kevin Linka, Ellen Kuhl, Tim Brepols</dc:creator>
    </item>
    <item>
      <title>Protein Large Language Models: A Comprehensive Survey</title>
      <link>https://arxiv.org/abs/2502.17504</link>
      <description>arXiv:2502.17504v1 Announce Type: cross 
Abstract: Protein-specific large language models (Protein LLMs) are revolutionizing protein science by enabling more efficient protein structure prediction, function annotation, and design. While existing surveys focus on specific aspects or applications, this work provides the first comprehensive overview of Protein LLMs, covering their architectures, training datasets, evaluation metrics, and diverse applications. Through a systematic analysis of over 100 articles, we propose a structured taxonomy of state-of-the-art Protein LLMs, analyze how they leverage large-scale protein sequence data for improved accuracy, and explore their potential in advancing protein engineering and biomedical research. Additionally, we discuss key challenges and future directions, positioning Protein LLMs as essential tools for scientific discovery in protein science. Resources are maintained at https://github.com/Yijia-Xiao/Protein-LLM-Survey.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17504v1</guid>
      <category>q-bio.BM</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yijia Xiao, Wanjia Zhao, Junkai Zhang, Yiqiao Jin, Han Zhang, Zhicheng Ren, Renliang Sun, Haixin Wang, Guancheng Wan, Pan Lu, Xiao Luo, Yu Zhang, James Zou, Yizhou Sun, Wei Wang</dc:creator>
    </item>
    <item>
      <title>A Mixed-Integer Linear Programming (MILP) for Garment Line Balancing</title>
      <link>https://arxiv.org/abs/2502.17508</link>
      <description>arXiv:2502.17508v1 Announce Type: cross 
Abstract: This applied research article explores the application of Mixed-Integer Linear Programming (MILP) to address line-balancing challenges in the garment industry, focusing on optimizing production processes under multiple constraints. By integrating MILP with Lean Methodology principles, the study demonstrates significant improvements in operational efficiency and cost-effectiveness. The case study, conducted in collaboration with Prof Dr Ray WM Kong, highlights the successful implementation of MILP using IBM CPLEX Studio to optimize production order quantities across online and offline operations. The results reveal a remarkable reduction in labour costs, exceeding 50%, while effectively managing resource capacity and demand constraints. This study not only validates the theoretical underpinnings of MILP in resolving line-balancing issues but also underscores its practical applicability in modernizing garment production. The findings contribute valuable insights into the potential of advanced optimization techniques to enhance competitiveness and sustainability in the garment industry. This abstract succinctly captures the essence of the research, emphasizing the methodology, results, and significance of the study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17508v1</guid>
      <category>math.OC</category>
      <category>cs.CE</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ray Wai Man Kong, Ding Ning, Theodore Ho Tin Kong</dc:creator>
    </item>
    <item>
      <title>Synergizing Deep Learning and Full-Waveform Inversion: Bridging Data-Driven and Theory-Guided Approaches for Enhanced Seismic Imaging</title>
      <link>https://arxiv.org/abs/2502.17585</link>
      <description>arXiv:2502.17585v1 Announce Type: cross 
Abstract: This review explores the integration of deep learning (DL) with full-waveform inversion (FWI) for enhanced seismic imaging and subsurface characterization. It covers FWI and DL fundamentals, geophysical applications (velocity estimation, deconvolution, tomography), and challenges (model complexity, data quality). The review also outlines future research directions, including hybrid, generative, and physics-informed models for improved accuracy, efficiency, and reliability in subsurface property estimation. The synergy between DL and FWI has the potential to transform geophysics, providing new insights into Earth's subsurface.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17585v1</guid>
      <category>physics.geo-ph</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher Zerafa, Pauline Galea, Cristiana Sebu</dc:creator>
    </item>
    <item>
      <title>Weaving the Cosmos: WASM-Powered Interchain Communication for AI Enabled Smart Contracts</title>
      <link>https://arxiv.org/abs/2502.17604</link>
      <description>arXiv:2502.17604v1 Announce Type: cross 
Abstract: In this era, significant transformations in industries and tool utilization are driven by AI/Large Language Models (LLMs) and advancements in Machine Learning. There's a growing emphasis on Machine Learning Operations(MLOps) for managing and deploying these AI models. Concurrently, the imperative for richer smart contracts and on-chain computation is escalating. Our paper introduces an innovative framework that integrates blockchain technology, particularly the Cosmos SDK, to facilitate on-chain AI inferences. This system, built on WebAssembly (WASM), enables interchain communication and deployment of WASM modules executing AI inferences across multiple blockchain nodes. We critically assess the framework from feasibility, scalability, and model security, with a special focus on its portability and engine-model agnostic deployment. The capability to support AI on-chain may enhance and expand the scope of smart contracts, and as a result enable new use cases and applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17604v1</guid>
      <category>cs.SE</category>
      <category>cs.CE</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rabimba Karanjai, Lei Xu, Weidong Shi</dc:creator>
    </item>
    <item>
      <title>Multi-Channel Currency: A Secure Method Using Semi-Quantum Tokens</title>
      <link>https://arxiv.org/abs/2502.18378</link>
      <description>arXiv:2502.18378v1 Announce Type: cross 
Abstract: Digital currencies primarily operate online, but there is growing interest in enabling offline transactions to improve digital inclusion. Existing offline methods struggle with double-spending risks, often limiting transaction amounts. In this work, we propose a quantum-state-based currency system that uses the non-cloning theorem to enable secure, multi-channel transactions without the risk of double spending. We demonstrate this system's implementation with experimental results, including use cases for currency transfers and swaps. To mitigate credit risks in swaps, we also integrate blockchain to show its wide applicability. Our approach paves the way for quantum-secure digital currencies and opens new possibilities for optimizing multi-channel tokens.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18378v1</guid>
      <category>quant-ph</category>
      <category>cs.CE</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yichi Zhang, Siyuan Jin, Yuhan Huang, Qiming Shao</dc:creator>
    </item>
    <item>
      <title>Improving Surrogate Model Robustness to Perturbations for Dynamical Systems Through Machine Learning and Data Assimilation</title>
      <link>https://arxiv.org/abs/2307.09762</link>
      <description>arXiv:2307.09762v3 Announce Type: replace 
Abstract: Many real-world systems are modelled using complex ordinary differential equations (ODEs). However, the dimensionality of these systems can make them challenging to analyze. Dimensionality reduction techniques like Proper Orthogonal Decomposition (POD) can be used in such cases. However, these reduced order models are susceptible to perturbations in the input. We propose a novel framework that combines machine learning and data assimilation techniques to improving surrogate models to handle perturbations in input data effectively. Through rigorous experiments on dynamical systems modelled on graphs, we demonstrate that our framework substantially improves the accuracy of surrogate models under input perturbations. Furthermore, we evaluate the framework's efficacy on alternative surrogate models, including neural ODEs, and the empirical results consistently show enhanced performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.09762v3</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Wed, 26 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhishek Ajayakumar, Soumyendu Raha</dc:creator>
    </item>
  </channel>
</rss>
