<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 31 Oct 2025 04:01:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Systems for Scaling Accessibility Efforts in Large Computing Courses</title>
      <link>https://arxiv.org/abs/2510.25964</link>
      <description>arXiv:2510.25964v1 Announce Type: new 
Abstract: It is critically important to make computing courses accessible for disabled students. This is particularly challenging in large computing courses, which face unique challenges due to the sheer scale of course content and staff. In this experience report, we share our attempts to scale accessibility efforts for a large university-level introductory programming course sequence, with over 3500 enrolled students and 100 teaching assistants (TAs) per year. First, we introduce our approach to auditing and remediating course materials by systematically identifying and resolving accessibility issues. However, remediating content post-hoc is purely reactive and scales poorly. We then discuss two approaches to systems that enable proactive accessibility work. We developed technical systems to manage remediation complexity at scale: redesigning other course content to be web-first and accessible by default, providing alternate accessible views for existing course content, and writing automated tests to receive instant feedback on a subset of accessibility issues. Separately, we established human systems to empower both course staff and students in accessibility best practices: developing and running various TA-targeted accessibility trainings, establishing course-wide accessibility norms, and integrating accessibility topics into core course curriculum. Preliminary qualitative feedback from both staff and students shows increased engagement in accessibility work and accessible technologies. We close by discussing limitations and lessons learned from our work, with advice for others developing similar auditing, remediation, technical, or human systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25964v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3770762.3772648</arxiv:DOI>
      <dc:creator>Ritesh Kanchi, Miya Natsuhara, Matt X. Wang</dc:creator>
    </item>
    <item>
      <title>The Quest for Reliable Metrics of Responsible AI</title>
      <link>https://arxiv.org/abs/2510.26007</link>
      <description>arXiv:2510.26007v1 Announce Type: new 
Abstract: The development of Artificial Intelligence (AI), including AI in Science (AIS), should be done following the principles of responsible AI. Progress in responsible AI is often quantified through evaluation metrics, yet there has been less work on assessing the robustness and reliability of the metrics themselves. We reflect on prior work that examines the robustness of fairness metrics for recommender systems as a type of AI application and summarise their key takeaways into a set of non-exhaustive guidelines for developing reliable metrics of responsible AI. Our guidelines apply to a broad spectrum of AI applications, including AIS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26007v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Theresia Veronika Rampisela, Maria Maistro, Tuukka Ruotsalo, Christina Lioma</dc:creator>
    </item>
    <item>
      <title>Exploring Dissatisfaction in Bus Route Reduction through LLM-Calibrated Agent-Based Modeling</title>
      <link>https://arxiv.org/abs/2510.26163</link>
      <description>arXiv:2510.26163v1 Announce Type: new 
Abstract: As emerging mobility modes continue to expand, many cities face declining bus ridership, increasing fiscal pressure to sustain underutilized routes, and growing inefficiencies in resource allocation. This study employs an agent-based modelling (ABM) approach calibrated through a large language model (LLM) using few-shot learning to examine how progressive bus route cutbacks affect passenger dissatisfaction across demographic groups and overall network resilience. Using IC-card data from Beijing's Huairou District, the LLM-calibrated ABM estimated passenger sensitivity parameters related to travel time, waiting, transfers, and crowding. Results show that the structural configuration of the bus network exerts a stronger influence on system stability than capacity or operational factors. The elimination of high-connectivity routes led to an exponential rise in total dissatisfaction, particularly among passengers with disabilities and older adults. The evolution of dissatisfaction exhibited three distinct phases - stable, transitional, and critical. Through the analysis of each stage, this study found that the continuous bus route reduction scenario exhibits three-stage thresholds. Once these thresholds are crossed, even a small reduction in routes may lead to a significant loss of passenger flow. Research highlights the nonlinear response of user sentiment to service reductions and underscore the importance of maintaining structural critical routes and providing stable services to vulnerable groups for equitable and resilient transport planning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26163v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiumeng Li, Xinxi Yang, Suhong Zhou</dc:creator>
    </item>
    <item>
      <title>Non-myopic Matching and Rebalancing in Large-Scale On-Demand Ride-Pooling Systems Using Simulation-Informed Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2510.25796</link>
      <description>arXiv:2510.25796v1 Announce Type: cross 
Abstract: Ride-pooling, also known as ride-sharing, shared ride-hailing, or microtransit, is a service wherein passengers share rides. This service can reduce costs for both passengers and operators and reduce congestion and environmental impacts. A key limitation, however, is its myopic decision-making, which overlooks long-term effects of dispatch decisions. To address this, we propose a simulation-informed reinforcement learning (RL) approach. While RL has been widely studied in the context of ride-hailing systems, its application in ride-pooling systems has been less explored. In this study, we extend the learning and planning framework of Xu et al. (2018) from ride-hailing to ride-pooling by embedding a ride-pooling simulation within the learning mechanism to enable non-myopic decision-making. In addition, we propose a complementary policy for rebalancing idle vehicles. By employing n-step temporal difference learning on simulated experiences, we derive spatiotemporal state values and subsequently evaluate the effectiveness of the non-myopic policy using NYC taxi request data. Results demonstrate that the non-myopic policy for matching can increase the service rate by up to 8.4% versus a myopic policy while reducing both in-vehicle and wait times for passengers. Furthermore, the proposed non-myopic policy can decrease fleet size by over 25% compared to a myopic policy, while maintaining the same level of performance, thereby offering significant cost savings for operators. Incorporating rebalancing operations into the proposed framework cuts wait time by up to 27.3%, in-vehicle time by 12.5%, and raises service rate by 15.1% compared to using the framework for matching decisions alone at the cost of increased vehicle minutes traveled per passenger.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25796v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Farnoosh Namdarpour, Joseph Y. J. Chow</dc:creator>
    </item>
    <item>
      <title>Industry Members' Perceptions about ABET-based Accreditation: An Exploratory Study in a Developing Country</title>
      <link>https://arxiv.org/abs/2510.26087</link>
      <description>arXiv:2510.26087v1 Announce Type: cross 
Abstract: ABET accreditation is an increasingly prominent system of global accreditation of engineering programs, and the assessment requires programs to demonstrate that they meet the needs of the program's stakeholders, typically industrial potential employers of graduates. To obtain these inputs, programs are required to assemble an advisory committee board. The views of the advisory board on the relevance of the degree outcomes are an essential part of this process. The purpose of this qualitative research study is to explore the viewpoints that industry stakeholders have on this type of process. The context for the study was an Ecuadorian engineering program which had successfully achieved the ABET accreditation. The study drew on interviews undertaken with industry members who were part of the advisory board. This study focuses on how they perceive the process and the accreditation awarded, analyzing their views of its usefulness, especially in relation to the employability of graduates. Based on the findings, we offer critical insights into this accreditation process when it takes place in contexts beyond highly industrialized countries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26087v1</guid>
      <category>physics.ed-ph</category>
      <category>cs.CY</category>
      <category>cs.SE</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TE.2024.3410996</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Education, vol. 67, no. 5, pp. 689-698, 2024</arxiv:journal_reference>
      <dc:creator>V. Sanchez Padilla, Albert Espinal, Jennifer M. Case, Jose Cordova-Garcia, Homero Murzi</dc:creator>
    </item>
    <item>
      <title>A Game-Theoretic Spatio-Temporal Reinforcement Learning Framework for Collaborative Public Resource Allocation</title>
      <link>https://arxiv.org/abs/2510.26184</link>
      <description>arXiv:2510.26184v1 Announce Type: cross 
Abstract: Public resource allocation involves the efficient distribution of resources, including urban infrastructure, energy, and transportation, to effectively meet societal demands. However, existing methods focus on optimizing the movement of individual resources independently, without considering their capacity constraints. To address this limitation, we propose a novel and more practical problem: Collaborative Public Resource Allocation (CPRA), which explicitly incorporates capacity constraints and spatio-temporal dynamics in real-world scenarios. We propose a new framework called Game-Theoretic Spatio-Temporal Reinforcement Learning (GSTRL) for solving CPRA. Our contributions are twofold: 1) We formulate the CPRA problem as a potential game and demonstrate that there is no gap between the potential function and the optimal target, laying a solid theoretical foundation for approximating the Nash equilibrium of this NP-hard problem; and 2) Our designed GSTRL framework effectively captures the spatio-temporal dynamics of the overall system. We evaluate GSTRL on two real-world datasets, where experiments show its superior performance. Our source codes are available in the supplementary materials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26184v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Songxin Lei, Qiongyan Wang, Yanchen Zhu, Hanyu Yao, Sijie Ruan, Weilin Ruan, Yuyu Luo, Huaming Wu, Yuxuan Liang</dc:creator>
    </item>
    <item>
      <title>Value Drifts: Tracing Value Alignment During LLM Post-Training</title>
      <link>https://arxiv.org/abs/2510.26707</link>
      <description>arXiv:2510.26707v1 Announce Type: cross 
Abstract: As LLMs occupy an increasingly important role in society, they are more and more confronted with questions that require them not only to draw on their general knowledge but also to align with certain human value systems. Therefore, studying the alignment of LLMs with human values has become a crucial field of inquiry. Prior work, however, mostly focuses on evaluating the alignment of fully trained models, overlooking the training dynamics by which models learn to express human values. In this work, we investigate how and at which stage value alignment arises during the course of a model's post-training. Our analysis disentangles the effects of post-training algorithms and datasets, measuring both the magnitude and time of value drifts during training. Experimenting with Llama-3 and Qwen-3 models of different sizes and popular supervised fine-tuning (SFT) and preference optimization datasets and algorithms, we find that the SFT phase generally establishes a model's values, and subsequent preference optimization rarely re-aligns these values. Furthermore, using a synthetic preference dataset that enables controlled manipulation of values, we find that different preference optimization algorithms lead to different value alignment outcomes, even when preference data is held constant. Our findings provide actionable insights into how values are learned during post-training and help to inform data curation, as well as the selection of models and algorithms for preference optimization to improve model alignment to human values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26707v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mehar Bhatia, Shravan Nayak, Gaurav Kamath, Marius Mosbach, Karolina Sta\'nczak, Vered Shwartz, Siva Reddy</dc:creator>
    </item>
    <item>
      <title>Neither Consent nor Property: A Policy Lab for Data Law</title>
      <link>https://arxiv.org/abs/2510.26727</link>
      <description>arXiv:2510.26727v1 Announce Type: cross 
Abstract: This paper makes the opaque data market in the AI economy empirically legible for the first time, constructing a computational testbed to address a core epistemic failure: regulators governing a market defined by structural opacity, fragile price discovery, and brittle technical safeguards that have paralyzed traditional empirics and fragmented policy. The pipeline begins with multi-year fieldwork to extract the market's hidden logic, and then embeds these grounded behaviors into a high-fidelity ABM, parameterized via a novel LLM-based discrete-choice experiment that captures the preferences of unsurveyable populations. The pipeline is validated against reality, reproducing observed trade patterns. This policy laboratory delivers clear, counter-intuitive results. First, property-style relief is a false promise: ''anonymous-data'' carve-outs expand trade but ignore risk, causing aggregate welfare to collapse once external harms are priced in. Second, social welfare peaks when the downstream buyer internalizes the full substantive risk. This least-cost avoider approach induces efficient safeguards, simultaneously raising welfare and sustaining trade, and provides a robust empirical foundation for the legal drift toward two-sided reachability. The contribution is a reproducible pipeline designed to end the reliance on intuition. It converts qualitative insight into testable, comparative policy experiments, obsoleting armchair conjecture by replacing it with controlled evidence on how legal rules actually shift risk and surplus. This is the forward-looking engine that moves the field from competing intuitions to direct, computational analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26727v1</guid>
      <category>econ.GN</category>
      <category>cs.CY</category>
      <category>q-fin.EC</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoyi Zhang, Tianyi Zhu</dc:creator>
    </item>
    <item>
      <title>Digital Labor and the Inconspicuous Production of Artificial Intelligence</title>
      <link>https://arxiv.org/abs/2410.05910</link>
      <description>arXiv:2410.05910v2 Announce Type: replace 
Abstract: Digital platforms capitalize on users' labor, often disguising essential contributions as casual activities or consumption, regardless of users' recognition of their efforts. Data annotation, content creation, and engagement with advertising are all aspects of this hidden productivity. Despite playing a crucial role in driving AI development, such tasks remain largely unrecognized and undercompensated. This chapter exposes the systemic devaluation of these activities in the digital economy, by drawing on historical theories about unrecognized labor, from housework to audience labor. This approach advocates for a broader understanding of digital labor by introducing the concept of ''inconspicuous production.'' It moves beyond the traditional notion of ''invisible work'' to highlight the hidden elements inherent in all job types, especially in light of growing automation and platform-based employment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05910v2</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Ergin Bulut, Julie Chen, Rafael Grohmann, Kylie Jarrett. SAGE Handbook of Digital Labour, SAGE, pp.282-292, 2025, 9781529669831</arxiv:journal_reference>
      <dc:creator>Antonio A. Casilli (DiPLab, I3 SES, NOS, IP Paris)</dc:creator>
    </item>
    <item>
      <title>Toward a Public and Secure Generative AI: A Comparative Analysis of Open and Closed LLMs</title>
      <link>https://arxiv.org/abs/2505.10603</link>
      <description>arXiv:2505.10603v2 Announce Type: replace 
Abstract: Generative artificial intelligence (Gen AI) systems represent a critical technology with far-reaching implications across multiple domains of society. However, their deployment entails a range of risks and challenges that require careful evaluation. To date, there has been a lack of comprehensive, interdisciplinary studies offering a systematic comparison between open-source and proprietary (closed) generative AI systems, particularly regarding their respective advantages and drawbacks. This study aims to: i) critically evaluate and compare the characteristics, opportunities, and challenges of open and closed generative AI models; and ii) propose foundational elements for the development of an Open, Public, and Safe Gen AI framework. As a methodology, we adopted a combined approach that integrates three methods: literature review, critical analysis, and comparative analysis. The proposed framework outlines key dimensions, openness, public governance, and security, as essential pillars for shaping the future of trustworthy and inclusive Gen AI. Our findings reveal that open models offer greater transparency, auditability, and flexibility, enabling independent scrutiny and bias mitigation. In contrast, closed systems often provide better technical support and ease of implementation, but at the cost of unequal access, accountability, and ethical oversight. The research also highlights the importance of multi-stakeholder governance, environmental sustainability, and regulatory frameworks in ensuring responsible development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10603v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jorge Machado</dc:creator>
    </item>
    <item>
      <title>AI for a Planet Under Pressure</title>
      <link>https://arxiv.org/abs/2510.24373</link>
      <description>arXiv:2510.24373v2 Announce Type: replace 
Abstract: Artificial intelligence (AI) is already driving scientific breakthroughs in a variety of research fields, ranging from the life sciences to mathematics. This raises a critical question: can AI be applied both responsibly and effectively to address complex and interconnected sustainability challenges? This report is the result of a collaboration between the Stockholm resilience Centre (Stockholm University), the Potsdam Institute for Climate Impact Research (PIK), and Google DeepMind. Our work explores the potential and limitations of using AI as a research method to help tackle eight broad sustainability challenges. The results build on iterated expert dialogues and assessments, a systematic AI-supported literature overview including over 8,500 academic publications, and expert deep-dives into eight specific issue areas. The report also includes recommendations to sustainability scientists, research funders, the private sector, and philanthropies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24373v2</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Victor Galaz, Maria Schewenius, Jonathan F. Donges, Ingo Fetzer, Erik Zhivkoplias, Wolfram Barfuss, Louis Delannoy, Lan Wang-Erlandsson, Maximilian Gelbrecht, Jobst Heitzig, Jonas Hentati-Sundberg, Christopher Kennedy, Nielja Knecht, Romi Lotcheris, Miguel Mahecha, Andrew Merrie, David Montero, Timon McPhearson, Ahmed Mustafa, Magnus Nystr\"om, Drew Purves, Juan C. Rocha, Masahiro Ryo, Claudia van der Salm, Samuel T. Segun, Anna B. Stephenson, Elizabeth Tellman, Felipe Tobar, Alice Vadrot</dc:creator>
    </item>
    <item>
      <title>Constraining Participation: Affordances of Feedback Features in Interfaces to Large Language Models</title>
      <link>https://arxiv.org/abs/2408.15066</link>
      <description>arXiv:2408.15066v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are now accessible to anyone with a computer, a web browser, and an internet connection via browser-based interfaces, shifting the dynamics of participation in AI development. This article examines how interactive feedback features in ChatGPT's interface afford user participation in LLM iteration. Drawing on a survey of early ChatGPT users and applying the mechanisms and conditions framework of affordances, we analyse how these features shape user input. Our analysis indicates that these features encourage simple, frequent, and performance-focused feedback while discouraging collective input and discussions among users. Drawing on participatory design literature, we argue such constraints, if replicated across broader user bases, risk reinforcing power imbalances between users, the public, and companies developing LLMs. Our analysis contributes to the growing literature on participatory AI by critically examining the limitations of existing feedback processes and proposing directions for redesign. Rather than focusing solely on aligning model outputs with specific user preferences, we advocate for creating infrastructure that supports sustained dialogue about the purpose and applications of LLMs. This approach requires attention to the ongoing work of "infrastructuring" - creating and sustaining the social, technical, and institutional structures necessary to address matters of concern to stakeholders impacted by LLM development and deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15066v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3748516</arxiv:DOI>
      <arxiv:journal_reference>ACM Journal on Responsible Computing, Volume 2, Issue 4, Article 16 (2025)</arxiv:journal_reference>
      <dc:creator>Ned Cooper, Alexandra Zafiroglu</dc:creator>
    </item>
    <item>
      <title>A Survey of Internet Censorship and its Measurement: Methodology, Trends, and Challenges</title>
      <link>https://arxiv.org/abs/2502.14945</link>
      <description>arXiv:2502.14945v2 Announce Type: replace-cross 
Abstract: Internet censorship limits the access of nodes residing within a specific network environment to the public Internet, and vice versa. During the last decade, techniques for conducting Internet censorship have been developed further. Consequently, methodology for measuring Internet censorship had been improved as well.
  In this paper, we firstly provide a survey of network-level Internet censorship techniques. Secondly, we survey censorship measurement methodology. We further cover the censorship of circumvention tools and its measurement, as well as available datasets. In cases where it is beneficial, we bridge the terminology and taxonomy of Internet censorship with related domains, namely traffic obfuscation and information hiding. We further extend the technical perspective with recent trends and challenges, including human aspects of Internet censorship.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14945v2</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.NI</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.cose.2025.104732</arxiv:DOI>
      <dc:creator>Steffen Wendzel, Simon Volpert, Sebastian Zillien, Julia Lenz, Philip R\"unz, Luca Caviglione</dc:creator>
    </item>
    <item>
      <title>Embracing Contradiction: Theoretical Inconsistency Will Not Impede the Road of Building Responsible AI Systems</title>
      <link>https://arxiv.org/abs/2505.18139</link>
      <description>arXiv:2505.18139v3 Announce Type: replace-cross 
Abstract: This position paper argues that the theoretical inconsistency often observed among Responsible AI (RAI) metrics, such as differing fairness definitions or tradeoffs between accuracy and privacy, should be embraced as a valuable feature rather than a flaw to be eliminated. We contend that navigating these inconsistencies, by treating metrics as divergent objectives, yields three key benefits: (1) Normative Pluralism: Maintaining a full suite of potentially contradictory metrics ensures that the diverse moral stances and stakeholder values inherent in RAI are adequately represented. (2) Epistemological Completeness: The use of multiple, sometimes conflicting, metrics allows for a more comprehensive capture of multifaceted ethical concepts, thereby preserving greater informational fidelity about these concepts than any single, simplified definition. (3) Implicit Regularization: Jointly optimizing for theoretically conflicting objectives discourages overfitting to one specific metric, steering models towards solutions with enhanced generalization and robustness under real-world complexities. In contrast, efforts to enforce theoretical consistency by simplifying or pruning metrics risk narrowing this value diversity, losing conceptual depth, and degrading model performance. We therefore advocate for a shift in RAI theory and practice: from getting trapped in inconsistency to characterizing acceptable inconsistency thresholds and elucidating the mechanisms that permit robust, approximated consistency in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18139v3</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gordon Dai, Yunze Xiao</dc:creator>
    </item>
    <item>
      <title>A Comparison of Precinct and District Voting Data Using Persistent Homology to Identify Gerrymandering in North Carolina</title>
      <link>https://arxiv.org/abs/2506.13997</link>
      <description>arXiv:2506.13997v4 Announce Type: replace-cross 
Abstract: Gerrymandering is one of the biggest threats to American democracy. By manipulating district lines, politicians effectively choose their voters rather than the other way around. Current gerrymandering identification methods (namely the Polsby-Popper and Reock scores) focus on the compactness of congressional districts, making them extremely sensitive to physical geography. To address this gap, we extend Feng and Porter's 2021 paper, which used the level-set method to turn geographic shapefiles into filtered simplicial complexes, in order to compare precinct level voting data to district level voting data. As precincts are regarded as too small to be gerrymandered, we are able to identify discrepancies between precinct and district level voting data to quantify gerrymandering in the United States. By comparing the persistent homologies of Democratic voting regions at the precinct and district levels, we detect when areas have been "cracked" (split across multiple districts) or "packed" (compressed into one district) for partisan gain. This analysis was conducted for North Carolina House of Representatives elections (2012-2024). North Carolina has been redistricted four times in the past ten years, unusually frequent as most states redistrict decennially, making it a valuable case study. By comparing persistence barcodes at the precinct and district levels (using the bottleneck distance), we show that precinct level voting patterns do not significantly fluctuate biannually, while district level patterns do, suggesting that shifts are likely a result of redistricting rather than voter behavior, providing strong evidence of gerrymandering. This research presents a novel application of topological data analysis in evaluating gerrymandering and shows persistent homology can be useful in discerning gerrymandered districts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13997v4</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ananya Shah</dc:creator>
    </item>
    <item>
      <title>Epistemic Diversity and Knowledge Collapse in Large Language Models</title>
      <link>https://arxiv.org/abs/2510.04226</link>
      <description>arXiv:2510.04226v4 Announce Type: replace-cross 
Abstract: Large language models (LLMs) tend to generate lexically, semantically, and stylistically homogenous texts. This poses a risk of knowledge collapse, where homogenous LLMs mediate a shrinking in the range of accessible information over time. Existing works on homogenization are limited by a focus on closed-ended multiple-choice setups or fuzzy semantic features, and do not look at trends across time and cultural contexts. To overcome this, we present a new methodology to measure epistemic diversity, i.e., variation in real-world claims in LLM outputs, which we use to perform a broad empirical study of LLM knowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200 prompt variations sourced from real user chats. For the topics in our study, we show that while newer models tend to generate more diverse claims, nearly all models are less epistemically diverse than a basic web search. We find that model size has a negative impact on epistemic diversity, while retrieval-augmented generation (RAG) has a positive impact, though the improvement from RAG varies by the cultural context. Finally, compared to a traditional knowledge source (Wikipedia), we find that country-specific claims reflect the English language more than the local one, highlighting a gap in epistemic representation</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04226v4</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dustin Wright, Sarah Masud, Jared Moore, Srishti Yadav, Maria Antoniak, Chan Young Park, Isabelle Augenstein</dc:creator>
    </item>
    <item>
      <title>Detecting Early and Implicit Suicidal Ideation via Longitudinal and Information Environment Signals on Social Media</title>
      <link>https://arxiv.org/abs/2510.14889</link>
      <description>arXiv:2510.14889v2 Announce Type: replace-cross 
Abstract: On social media, many individuals experiencing suicidal ideation (SI) do not disclose their distress explicitly. Instead, signs may surface indirectly through everyday posts or peer interactions. Detecting such implicit signals early is critical but remains challenging. We frame early and implicit SI as a forward-looking prediction task and develop a computational framework that models a user's information environment, consisting of both their longitudinal posting histories as well as the discourse of their socially proximal peers. We adopted a composite network centrality measure to identify top neighbors of a user, and temporally aligned the user's and neighbors' interactions -- integrating the multi-layered signals in a fine-tuned DeBERTa-v3 model. In a Reddit study of 1,000 (500 Case and 500 Control) users, our approach improves early and implicit SI detection by 15% over individual-only baselines. These findings highlight that peer interactions offer valuable predictive signals and carry broader implications for designing early detection systems that capture indirect as well as masked expressions of risk in online environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14889v2</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soorya Ram Shimgekar, Ruining Zhao, Agam Goyal, Violeta J. Rodriguez, Paul A. Bloom, Hari Sundaram, Koustuv Saha</dc:creator>
    </item>
  </channel>
</rss>
