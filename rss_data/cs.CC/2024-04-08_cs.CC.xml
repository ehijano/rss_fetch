<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 08 Apr 2024 04:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 08 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The Low-Degree Hardness of Finding Large Independent Sets in Sparse Random Hypergraphs</title>
      <link>https://arxiv.org/abs/2404.03842</link>
      <description>arXiv:2404.03842v1 Announce Type: new 
Abstract: We study the algorithmic task of finding large independent sets in Erdos-Renyi $r$-uniform hypergraphs on $n$ vertices having average degree $d$. Krivelevich and Sudakov showed that the maximum independent set has density $\left(\frac{r\log d}{(r-1)d}\right)^{1/(r-1)}$. We show that the class of low-degree polynomial algorithms can find independent sets of density $\left(\frac{\log d}{(r-1)d}\right)^{1/(r-1)}$ but no larger. This extends and generalizes earlier results of Gamarnik and Sudan, Rahman and Virag, and Wein on graphs, and answers a question of Bal and Bennett. We conjecture that this statistical-computational gap holds for this problem.
  Additionally, we explore the universality of this gap by examining $r$-partite hypergraphs. A hypergraph $H=(V,E)$ is $r$-partite if there is a partition $V=V_1\cup\cdots\cup V_r$ such that each edge contains exactly one vertex from each set $V_i$. We consider the problem of finding large balanced independent sets (independent sets containing the same number of vertices in each partition) in random $r$-partite hypergraphs with $n$ vertices in each partition and average degree $d$. We prove that the maximum balanced independent set has density $\left(\frac{r\log d}{(r-1)d}\right)^{1/(r-1)}$ asymptotically. Furthermore, we prove an analogous low-degree computational threshold of $\left(\frac{\log d}{(r-1)d}\right)^{1/(r-1)}$. Our results recover and generalize recent work of Perkins and the second author on bipartite graphs.
  While the graph case has been extensively studied, this work is the first to consider statistical-computational gaps of optimization problems on random hypergraphs. Our results suggest that these gaps persist for larger uniformities as well as across many models. A somewhat surprising aspect of the gap for balanced independent sets is that the algorithm achieving the lower bound is a simple degree-1 polynomial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03842v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abhishek Dhawan, Yuzhou Wang</dc:creator>
    </item>
    <item>
      <title>$\Pi_{2}^{P}$ vs PSpace Dichotomy for the Quantified Constraint Satisfaction Problem</title>
      <link>https://arxiv.org/abs/2404.03844</link>
      <description>arXiv:2404.03844v1 Announce Type: new 
Abstract: The Quantified Constraint Satisfaction Problem is the problem of evaluating a sentence with both quantifiers, over relations from some constraint language, with conjunction as the only connective. We show that for any constraint language on a finite domain the Quantified Constraint Satisfaction Problem is either in $\Pi_{2}^{P}$, or PSpace-complete. Additionally, we build a constraint language on a 6-element domain such that the Quantified Constraint Satisfaction Problem over this language is $\Pi_{2}^{P}$-complete.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03844v1</guid>
      <category>cs.CC</category>
      <category>cs.LO</category>
      <category>math.LO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dmitriy Zhuk</dc:creator>
    </item>
    <item>
      <title>Exploration is Harder than Prediction: Cryptographically Separating Reinforcement Learning from Supervised Learning</title>
      <link>https://arxiv.org/abs/2404.03774</link>
      <description>arXiv:2404.03774v1 Announce Type: cross 
Abstract: Supervised learning is often computationally easy in practice. But to what extent does this mean that other modes of learning, such as reinforcement learning (RL), ought to be computationally easy by extension? In this work we show the first cryptographic separation between RL and supervised learning, by exhibiting a class of block MDPs and associated decoding functions where reward-free exploration is provably computationally harder than the associated regression problem. We also show that there is no computationally efficient algorithm for reward-directed RL in block MDPs, even when given access to an oracle for this regression problem.
  It is known that being able to perform regression in block MDPs is necessary for finding a good policy; our results suggest that it is not sufficient. Our separation lower bound uses a new robustness property of the Learning Parities with Noise (LPN) hardness assumption, which is crucial in handling the dependent nature of RL data. We argue that separations and oracle lower bounds, such as ours, are a more meaningful way to prove hardness of learning because the constructions better reflect the practical reality that supervised learning by itself is often not the computational bottleneck.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03774v1</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Noah Golowich, Ankur Moitra, Dhruv Rohatgi</dc:creator>
    </item>
    <item>
      <title>Additive approximation algorithm for geodesic centers in $\delta$-hyperbolic graphs</title>
      <link>https://arxiv.org/abs/2404.03812</link>
      <description>arXiv:2404.03812v1 Announce Type: cross 
Abstract: For an integer $k\geq 1$, the objective of \textsc{$k$-Geodesic Center} is to find a set $\mathcal{C}$ of $k$ isometric paths such that the maximum distance between any vertex $v$ and $\mathcal{C}$ is minimised. Introduced by Gromov, \emph{$\delta$-hyperbolicity} measures how treelike a graph is from a metric point of view. Our main contribution in this paper is to provide an additive $O(\delta)$-approximation algorithm for \textsc{$k$-Geodesic Center} on $\delta$-hyperbolic graphs. On the way, we define a coarse version of the pairing property introduced by Gerstel \&amp; Zaks (Networks, 1994) and show it holds for $\delta$-hyperbolic graphs. This result allows to reduce the \textsc{$k$-Geodesic Center} problem to its rooted counterpart, a main idea behind our algorithm. We also adapt a technique of Dragan \&amp; Leitert, (TCS, 2017) to show that for every $k\geq 1$, $k$-\textsc{Geodesic Center} is NP-hard even on partial grids.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03812v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dibyayan Chakraborty, Yann Vax\`es</dc:creator>
    </item>
    <item>
      <title>Inapproximability of Maximum Diameter Clustering for Few Clusters</title>
      <link>https://arxiv.org/abs/2312.02097</link>
      <description>arXiv:2312.02097v2 Announce Type: replace-cross 
Abstract: In the Max-k-diameter problem, we are given a set of points in a metric space, and the goal is to partition the input points into k parts such that the maximum pairwise distance between points in the same part of the partition is minimized.
  The approximability of the Max-k-diameter problem was studied in the eighties, culminating in the work of Feder and Greene [STOC'88], wherein they showed it is NP-hard to approximate within a factor better than 2 in the $\ell_1$ and $\ell_\infty$ metrics, and NP-hard to approximate within a factor better than 1.969 in the Euclidean metric. This complements the celebrated 2 factor polynomial time approximation algorithm for the problem in general metrics (Gonzalez [TCS'85]; Hochbaum and Shmoys [JACM'86]).
  Over the last couple of decades, there has been increased interest from the algorithmic community to study the approximability of various clustering objectives when the number of clusters is fixed. In this setting, the framework of coresets has yielded PTAS for most popular clustering objectives, including k-means, k-median, k-center, k-minsum, and so on.
  In this paper, rather surprisingly, we prove that even when k=3, the Max-k-diameter problem is NP-hard to approximate within a factor of 1.5 in the $\ell_1$-metric (and Hamming metric) and NP-hard to approximate within a factor of 1.304 in the Euclidean metric.
  Our main conceptual contribution is the introduction of a novel framework called cloud systems which embed hypergraphs into $\ell_p$-metric spaces such that the chromatic number of the hypergraph is related to the quality of the Max-k-diameter clustering of the embedded pointset. Our main technical contributions are the constructions of nontrivial cloud systems in the Euclidean and $\ell_1$-metrics using extremal geometric structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.02097v2</guid>
      <category>cs.CG</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Henry Fleischmann, Kyrylo Karlov, Karthik C. S., Ashwin Padaki, Stepan Zharkov</dc:creator>
    </item>
    <item>
      <title>Parallel Play Saves Quantifiers</title>
      <link>https://arxiv.org/abs/2402.10293</link>
      <description>arXiv:2402.10293v2 Announce Type: replace-cross 
Abstract: The number of quantifiers needed to express first-order properties is captured by two-player combinatorial games called multi-structural (MS) games. We play these games on linear orders and strings, and introduce a technique we call "parallel play", that dramatically reduces the number of quantifiers needed in many cases. Linear orders and strings are the most basic representatives of ordered structures -- a class of structures that has historically been notoriously difficult to analyze. Yet, in this paper, we provide upper bounds on the number of quantifiers needed to characterize different-sized subsets of these structures, and prove that they are tight up to constant factors, including, in some cases, up to a factor of $1+\varepsilon$, for arbitrarily small $\varepsilon$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.10293v2</guid>
      <category>cs.LO</category>
      <category>cs.CC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Carmosino, Ronald Fagin, Neil Immerman, Phokion Kolaitis, Jonathan Lenchner, Rik Sengupta, Ryan Williams</dc:creator>
    </item>
  </channel>
</rss>
