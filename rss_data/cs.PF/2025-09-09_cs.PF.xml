<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PF updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PF</link>
    <description>cs.PF updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PF" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 09 Sep 2025 04:05:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Waltz: Temperature-Aware Cooperative Compression for High-Performance Compression-Based CSDs</title>
      <link>https://arxiv.org/abs/2509.05365</link>
      <description>arXiv:2509.05365v1 Announce Type: new 
Abstract: Data compression is widely adopted for modern solid-state drives (SSDs) to mitigate both storage capacity and SSD lifetime issues. Researchers have proposed compression schemes at different system layers, including device-side solutions like CCSDs ( c ompression-based c omputational SSDs) and compression supported by host-side, like F2FS (flash-friendly file system). We conduct quantitative studies to understand how host-side and device-side compression schemes affect the temperature and performance of SSD-based storage systems. From our experiments, device-side compression, facilitated by a hardware compression engine, can raise the temperature of CCSDs to intolerable levels, resulting in throttling and service shutdown. In contrast, host-side compression causes software-stack overhead, which often results in large performance degradation and resource consumption. To ensure efficient data compression with high performance and better temperature control, we propose Waltz, a temperature-aware cooperative compression method that schedules (de)compression tasks at the host and device sides by monitoring device temperature. Furthermore, we introduce two variants (Waltzs and Waltzp) for space and performance optimization, respectively. Waltz is implemented within F2FS, achieving high performance while extending SSD lifetime and preventing overheating-induced in-flight shutdowns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05365v1</guid>
      <category>cs.PF</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dingcui Yu, Yunpeng Song, Yiyang Huang, Yumiao Zhao, Yina Lv, Chundong Wang, Youtao Zhang, Liang Shi</dc:creator>
    </item>
    <item>
      <title>Efficient Fault Localization in a Cloud Stack Using End-to-End Application Service Topology</title>
      <link>https://arxiv.org/abs/2509.05511</link>
      <description>arXiv:2509.05511v1 Announce Type: new 
Abstract: Cloud application services are distributed in nature and have components across the stack working together to deliver the experience to end users. The wide adoption of microservice architecture exacerbates failure management due to increased service components. To be effective, the strategies to enhance the application service resilience need to be autonomous and developed at the service's granularity, considering its end-to-end components. However, the massive amount of observability data generated by all these components across the service stack poses a significant challenge in reacting to anomalies and restoring the service quality in real time. Identifying the most informative observability data from across the cloud service stack and timely localization of root causes of anomalies thus becomes crucial to ensure service resilience. This article presents a novel approach that considers the application service topology to select the most informative metrics across the cloud stack to support efficient, explainable, and accurate root cause identifications in case of performance anomalies. The usefulness of the selected metrics is then evaluated using the state-of-the-art Root Cause Detection (RCD) algorithm for localizing the root cause of performance anomalies. As a step towards improving the accuracy and efficiency of RCD, this article then proposes the Topology-Aware-RCD (TA-RCD) that incorporates the end-to-end application service topology in RCD. The evaluation of the failure injection studies shows that the proposed approach performs at least 2X times better on average than the state-of-the-art RCD algorithm regarding Top-3 and Top-5 recall.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05511v1</guid>
      <category>cs.PF</category>
      <category>cs.DC</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dhanya R Mathews, Mudit Verma, Pooja Aggarwal, J. Lakshmi</dc:creator>
    </item>
    <item>
      <title>Optimizing Cloud-native Services with SAGA: A Service Affinity Graph-based Approach</title>
      <link>https://arxiv.org/abs/2509.05790</link>
      <description>arXiv:2509.05790v1 Announce Type: new 
Abstract: Modern software architectures are characterized by their cloud-native, modular, and microservice-based designs. While these systems are known for their efficiency, they also face complex challenges in service optimization, especially in maintaining end-to-end quality of service across dynamically distributed services. This paper introduces a novel approach using the concept of Service Affinity to address this challenge. The proposed method, termed Service Affinity Graph-based Approach, employs a graph-based model to model the interactions among microservices. It formulates the service placement as a minimum-weight k-cut problem and utilizes an approximation algorithm for service clustering. This approach is realized through a conceptual framework that takes into account a wide range of optimization objectives, ranging from enhancing application performance and enforcing data privacy to optimizing operational costs. In addition to presenting the SAGA framework in details, this paper conducts an in-depth empirical evaluation using a prototype deployed on a Kubernetes cluster. The results demonstrate a mean latency improvement of 23.40%, validating the effectiveness of our approach. Finally, the paper comprehensively discusses various aspects of the proposed methods, including their implications, challenges, and benefits, providing a thorough analysis of the approach's impact.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05790v1</guid>
      <category>cs.PF</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/SmartNets61466.2024.10577743</arxiv:DOI>
      <dc:creator>Hai Dinh-Tuan, Franz Florian Six</dc:creator>
    </item>
    <item>
      <title>Optimizing Stateful Microservice Migration in Kubernetes with MS2M and Forensic Checkpointing</title>
      <link>https://arxiv.org/abs/2509.05794</link>
      <description>arXiv:2509.05794v1 Announce Type: new 
Abstract: The widespread adoption of microservices architecture in modern software systems has emphasized the need for efficient management of distributed services. While stateless microservices enable straightforward migration, stateful microservices introduce added complexity due to the need to preserve in-memory state during migration. However, most container orchestrators, including Kubernetes, lack native support for live stateful service migration. This paper proposes an optimized migration scheme for stateful services in Kubernetes by integrating the Message-based Stateful Microservice Migration (MS2M) framework with Kubernetes' Forensic Container Checkpointing (FCC) feature. Key enhancements include support for migrating StatefulSet-managed Pods and the introduction of a Threshold-Based Cutoff Mechanism to handle high incoming message rates. Evaluation results demonstrate that MS2M for individual Pods reduces downtime by 96.986% compared to cold migration methods, while the StatefulSet approach provides greater flexibility in managing stateful services. These insights provide practical strategies for optimizing stateful microservice migration in cloud-native environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05794v1</guid>
      <category>cs.PF</category>
      <category>cs.CE</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ICIN64016.2025.10942720</arxiv:DOI>
      <dc:creator>Hai Dinh-Tuan, Jialun Jiang</dc:creator>
    </item>
    <item>
      <title>Characterizing and Optimizing Realistic Workloads on a Commercial Compute-in-SRAM Device</title>
      <link>https://arxiv.org/abs/2509.05451</link>
      <description>arXiv:2509.05451v1 Announce Type: cross 
Abstract: Compute-in-SRAM architectures offer a promising approach to achieving higher performance and energy efficiency across a range of data-intensive applications. However, prior evaluations have largely relied on simulators or small prototypes, limiting the understanding of their real-world potential. In this work, we present a comprehensive performance and energy characterization of a commercial compute-in-SRAM device, the GSI APU, under realistic workloads. We compare the GSI APU against established architectures, including CPUs and GPUs, to quantify its energy efficiency and performance potential. We introduce an analytical framework for general-purpose compute-in-SRAM devices that reveals fundamental optimization principles by modeling performance trade-offs, thereby guiding program optimizations.
  Exploiting the fine-grained parallelism of tightly integrated memory-compute architectures requires careful data management. We address this by proposing three optimizations: communication-aware reduction mapping, coalesced DMA, and broadcast-friendly data layouts. When applied to retrieval-augmented generation (RAG) over large corpora (10GB--200GB), these optimizations enable our compute-in-SRAM system to accelerate retrieval by 4.8$\times$--6.6$\times$ over an optimized CPU baseline, improving end-to-end RAG latency by 1.1$\times$--1.8$\times$. The shared off-chip memory bandwidth is modeled using a simulated HBM, while all other components are measured on the real compute-in-SRAM device. Critically, this system matches the performance of an NVIDIA A6000 GPU for RAG while being significantly more energy-efficient (54.4$\times$-117.9$\times$ reduction). These findings validate the viability of compute-in-SRAM for complex, real-world applications and provide guidance for advancing the technology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05451v1</guid>
      <category>cs.AR</category>
      <category>cs.PF</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Niansong Zhang, Wenbo Zhu, Courtney Golden, Dan Ilan, Hongzheng Chen, Christopher Batten, Zhiru Zhang</dc:creator>
    </item>
    <item>
      <title>ProfilingAgent: Profiling-Guided Agentic Reasoning for Adaptive Model Optimization</title>
      <link>https://arxiv.org/abs/2509.05584</link>
      <description>arXiv:2509.05584v1 Announce Type: cross 
Abstract: Foundation models face growing compute and memory bottlenecks, hindering deployment on resource-limited platforms. While compression techniques such as pruning and quantization are widely used, most rely on uniform heuristics that ignore architectural and runtime heterogeneity. Profiling tools expose per-layer latency, memory, and compute cost, yet are rarely integrated into automated pipelines. We propose ProfilingAgent, a profiling-guided, agentic approach that uses large language models (LLMs) to automate compression via structured pruning and post-training dynamic quantization. Our modular multi-agent system reasons over static metrics (MACs, parameter counts) and dynamic signals (latency, memory) to design architecture-specific strategies. Unlike heuristic baselines, ProfilingAgent tailors layer-wise decisions to bottlenecks. Experiments on ImageNet-1K, CIFAR-10, and CIFAR-100 with ResNet-101, ViT-B/16, Swin-B, and DeiT-B/16 show pruning maintains competitive or improved accuracy (about 1% drop on ImageNet-1K, +2% gains for ViT-B/16 on smaller datasets), while quantization achieves up to 74% memory savings with &lt;0.5% accuracy loss. Our quantization also yields consistent inference speedups of up to 1.74 times faster. Comparative studies with GPT-4o and GPT-4-Turbo highlight the importance of LLM reasoning quality for iterative pruning. These results establish agentic systems as scalable solutions for profiling-guided model optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05584v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>cs.PF</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sadegh Jafari, Aishwarya Sarkar, Mohiuddin Bilwal, Ali Jannesari</dc:creator>
    </item>
    <item>
      <title>Toward Efficient and Scalable Design of In-Memory Graph-Based Vector Search</title>
      <link>https://arxiv.org/abs/2509.05750</link>
      <description>arXiv:2509.05750v1 Announce Type: cross 
Abstract: Vector data is prevalent across business and scientific applications, and its popularity is growing with the proliferation of learned embeddings. Vector data collections often reach billions of vectors with thousands of dimensions, thus, increasing the complexity of their analysis. Vector search is the backbone of many critical analytical tasks, and graph-based methods have become the best choice for analytical tasks that do not require guarantees on the quality of the answers. Although several paradigms (seed selection, incremental insertion, neighborhood propagation, neighborhood diversification, and divide-and-conquer) have been employed to design in-memory graph-based vector search algorithms, a systematic comparison of the key algorithmic advances is still missing. We conduct an exhaustive experimental evaluation of twelve state-of-the-art methods on seven real data collections, with sizes up to 1 billion vectors. We share key insights about the strengths and limitations of these methods; e.g., the best approaches are typically based on incremental insertion and neighborhood diversification, and the choice of the base graph can hurt scalability. Finally, we discuss open research directions, such as the importance of devising more sophisticated data adaptive seed selection and diversification strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05750v1</guid>
      <category>cs.IR</category>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <category>cs.PF</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ilias Azizi, Karima Echihab, Themis Palpanas, Vassilis Christophides</dc:creator>
    </item>
    <item>
      <title>Efficiently Ranking Software Variants with Minimal Benchmarks</title>
      <link>https://arxiv.org/abs/2509.06716</link>
      <description>arXiv:2509.06716v1 Announce Type: cross 
Abstract: Benchmarking is a common practice in software engineering to assess the qualities and performance of software variants, coming from multiple competing systems or from configurations of the same system. Benchmarks are used notably to compare and understand variant performance, fine-tune software, detect regressions, or design new software systems. The execution of benchmarks to get a complete picture of software variants is highly costly in terms of computational resources and time. In this paper, we propose a novel approach for reducing benchmarks while maintaining stable rankings, using test suite optimization techniques. That is, we remove instances from the benchmarks while trying to keep the same rankings of the variants on all tests. Our method, BISection Sampling, BISS, strategically retains the most critical tests and applies a novel divide-and-conquer approach to efficiently sample among relevant remaining tests. We experiment with datasets and use cases from LLM leaderboards, SAT competitions, and configurable systems for performance modeling. Our results show that our method outperforms baselines even when operating on a subset of variants. Using BISS, we reduce the computational cost of the benchmarks on average to 44% and on more than half the benchmarks by up to 99% without loss in ranking stability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06716v1</guid>
      <category>cs.SE</category>
      <category>cs.PF</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Th\'eo Matricon, Mathieu Acher, Helge Spieker, Arnaud Gotlieb</dc:creator>
    </item>
    <item>
      <title>Dependability of UAV-Based Networks and Computing Systems: A Survey</title>
      <link>https://arxiv.org/abs/2506.16786</link>
      <description>arXiv:2506.16786v2 Announce Type: replace 
Abstract: Uncrewed Aerial Vehicle (UAV) computing and networking are becoming a fundamental computation infrastructure for diverse cyber-physical application systems. UAVs can be empowered by AI on edge devices and can communicate with other UAVs and ground stations via wireless communication networks. Dynamic computation demands and heterogeneous computing resources are distributed in the system and need to be controlled to maintain the quality of services and to accomplish critical missions. With the evolution of UAV-based systems, dependability assurance of such systems emerges as a crucial challenge. UAV-based systems confront diverse sources of uncertainty that may threaten their dependability, such as software bugs, component failures, network disconnections, battery shortages, and disturbances from the real world. In this paper, we conduct systematic literature reviews on the dependability of UAV-based networks and computing systems. The survey report reveals emerging research trends in this field and summarizes the literature into comprehensive categories by threat types and adopted technologies. Based on our literature reviews, we identify eight research fields that require further exploration in the future to achieve dependable UAV-based systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16786v2</guid>
      <category>cs.PF</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qingyang Zhang, Mohammad Dwipa Furqan, Tasfia Nutzhat, Fumio Machida, Ermeson Andrade</dc:creator>
    </item>
    <item>
      <title>Scaling Intelligence: Designing Data Centers for Next-Gen Language Models</title>
      <link>https://arxiv.org/abs/2506.15006</link>
      <description>arXiv:2506.15006v3 Announce Type: replace-cross 
Abstract: The explosive growth of Large Language Models (LLMs), such as GPT-4 with 1.8 trillion parameters, demands a fundamental rethinking of data center architecture to ensure scalability, efficiency, and cost-effectiveness. Our work provides a comprehensive co-design framework that jointly explores FLOPS, HBM bandwidth and capacity, multiple network topologies (two-tier vs. FullFlat optical), the size of the scale-out domain, and popular parallelism/optimization strategies used in LLMs. We introduce and evaluate FullFlat network architectures, which provide uniform high-bandwidth, low-latency connectivity between all nodes, and demonstrate their transformative impact on performance and scalability. Through detailed sensitivity analyses, we quantify the benefits of overlapping compute and communication, leveraging hardware-accelerated collectives, widening the scale-out domain, and increasing memory capacity. Our study spans both sparse (mixture of experts) and dense transformer-based LLMs, revealing how system design choices affect Model FLOPS Utilization (MFU = Model FLOPS per token * Observed tokens per second / Peak FLOPS of the hardware) and overall throughput. For the co-design study, we utilized an analytical performance modeling tool capable of predicting LLM runtime within 10% of real-world measurements. Our findings offer actionable insights and a practical roadmap for designing AI data centers that can efficiently support trillion-parameter models, reduce optimization complexity, and sustain the rapid evolution of AI capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15006v3</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <category>cs.PF</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jesmin Jahan Tithi, Hanjiang Wu, Avishaii Abuhatzera, Fabrizio Petrini</dc:creator>
    </item>
  </channel>
</rss>
