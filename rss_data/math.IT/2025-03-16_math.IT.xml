<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.IT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.IT</link>
    <description>math.IT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.IT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 17 Mar 2025 04:00:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Security and Privacy: Key Requirements for Molecular Communication in Medicine and Healthcare</title>
      <link>https://arxiv.org/abs/2503.11169</link>
      <description>arXiv:2503.11169v1 Announce Type: new 
Abstract: Molecular communication (MC) is an emerging paradigm that enables data transmission through biochemical signals rather than traditional electromagnetic waves. This approach is particularly promising for environments where conventional wireless communication is impractical, such as within the human body. However, security and privacy pose significant challenges that must be addressed to ensure reliable communication. Moreover, MC is often event-triggered, making it logical to adopt goal-oriented communication strategies, similar to those used in message identification. This work explores secure identification strategies for MC, with a focus on the information-theoretic security of message identification over Poisson wiretap channels (DT-PWC).</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11169v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vida Gholamiyan, Yaning Zhao, Wafa Labidi, Holger Boche, Christian Deppe</dc:creator>
    </item>
    <item>
      <title>Finite Horizon Optimization for Large-Scale MIMO</title>
      <link>https://arxiv.org/abs/2503.11356</link>
      <description>arXiv:2503.11356v1 Announce Type: new 
Abstract: Large-scale multiple-input multiple-output (MIMO) is an emerging wireless technology that deploys thousands of transmit antennas at the base-station to boost spectral efficiency. The classic weighted minimum mean-square-error (WMMSE) algorithm for beamforming is no suited for the large-scale MIMO because each iteration of the algorithm then requires inverting a matrix whose size equals the number of transmit antennas. While the existing methods such as the reduced WMMSE algorithm seek to decrease the size of matrix to invert, this work proposes to eliminate this large matrix inversion completely by applying gradient descent method in conjunction with fractional programming. Furthermore, we optimize the step sizes for gradient descent from a finite horizon optimization perspective, aiming to maximize the performance after a limited number of iterations of gradient descent. Simulations show that the proposed algorithm is much more efficient than the WMMSE algorithm in optimizing the large-scale MIMO precoders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11356v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yi Feng, Kaiming Shen</dc:creator>
    </item>
    <item>
      <title>Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts</title>
      <link>https://arxiv.org/abs/2306.04723</link>
      <description>arXiv:2306.04723v2 Announce Type: cross 
Abstract: Rapidly increasing quality of AI-generated content makes it difficult to distinguish between human and AI-generated texts, which may lead to undesirable consequences for society. Therefore, it becomes increasingly important to study the properties of human texts that are invariant over different text domains and varying proficiency of human writers, can be easily calculated for any language, and can robustly separate natural and AI-generated texts regardless of the generation model and sampling method. In this work, we propose such an invariant for human-written texts, namely the intrinsic dimensionality of the manifold underlying the set of embeddings for a given text sample. We show that the average intrinsic dimensionality of fluent texts in a natural language is hovering around the value $9$ for several alphabet-based languages and around $7$ for Chinese, while the average intrinsic dimensionality of AI-generated texts for each language is $\approx 1.5$ lower, with a clear statistical separation between human-generated and AI-generated distributions. This property allows us to build a score-based artificial text detector. The proposed detector's accuracy is stable over text domains, generator models, and human writer proficiency levels, outperforming SOTA detectors in model-agnostic and cross-domain scenarios by a significant margin.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.04723v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.AT</category>
      <category>math.IT</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eduard Tulchinskii, Kristian Kuznetsov, Laida Kushnareva, Daniil Cherniavskii, Serguei Barannikov, Irina Piontkovskaya, Sergey Nikolenko, Evgeny Burnaev</dc:creator>
    </item>
    <item>
      <title>Disentanglement Learning via Topology</title>
      <link>https://arxiv.org/abs/2308.12696</link>
      <description>arXiv:2308.12696v4 Announce Type: cross 
Abstract: We propose TopDis (Topological Disentanglement), a method for learning disentangled representations via adding a multi-scale topological loss term. Disentanglement is a crucial property of data representations substantial for the explainability and robustness of deep learning models and a step towards high-level cognition. The state-of-the-art methods are based on VAE and encourage the joint distribution of latent variables to be factorized. We take a different perspective on disentanglement by analyzing topological properties of data manifolds. In particular, we optimize the topological similarity for data manifolds traversals. To the best of our knowledge, our paper is the first one to propose a differentiable topological loss for disentanglement learning. Our experiments have shown that the proposed TopDis loss improves disentanglement scores such as MIG, FactorVAE score, SAP score, and DCI disentanglement score with respect to state-of-the-art results while preserving the reconstruction quality. Our method works in an unsupervised manner, permitting us to apply it to problems without labeled factors of variation. The TopDis loss works even when factors of variation are correlated. Additionally, we show how to use the proposed topological loss to find disentangled directions in a trained GAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.12696v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>math.DG</category>
      <category>math.IT</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:journal_reference>ICML 2024, Proceedings of the 41st International Conference on Machine Learning</arxiv:journal_reference>
      <dc:creator>Nikita Balabin, Daria Voronkova, Ilya Trofimov, Evgeny Burnaev, Serguei Barannikov</dc:creator>
    </item>
    <item>
      <title>Robust AI-Generated Text Detection by Restricted Embeddings</title>
      <link>https://arxiv.org/abs/2410.08113</link>
      <description>arXiv:2410.08113v1 Announce Type: cross 
Abstract: Growing amount and quality of AI-generated texts makes detecting such content more difficult. In most real-world scenarios, the domain (style and topic) of generated data and the generator model are not known in advance. In this work, we focus on the robustness of classifier-based detectors of AI-generated text, namely their ability to transfer to unseen generators or semantic domains. We investigate the geometry of the embedding space of Transformer-based text encoders and show that clearing out harmful linear subspaces helps to train a robust classifier, ignoring domain-specific spurious features. We investigate several subspace decomposition and feature selection strategies and achieve significant improvements over state of the art methods in cross-domain and cross-generator transfer. Our best approaches for head-wise and coordinate-based subspace removal increase the mean out-of-distribution (OOD) classification score by up to 9% and 14% in particular setups for RoBERTa and BERT embeddings respectively. We release our code and data: https://github.com/SilverSolver/RobustATD</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08113v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kristian Kuznetsov, Eduard Tulchinskii, Laida Kushnareva, German Magai, Serguei Barannikov, Sergey Nikolenko, Irina Piontkovskaya</dc:creator>
    </item>
    <item>
      <title>Quantifying Logical Consistency in Transformers via Query-Key Alignment</title>
      <link>https://arxiv.org/abs/2502.17017</link>
      <description>arXiv:2502.17017v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated impressive performance in various natural language processing tasks, yet their ability to perform multi-step logical reasoning remains an open challenge. Although Chain-of-Thought prompting has improved logical reasoning by enabling models to generate intermediate steps, it lacks mechanisms to assess the coherence of these logical transitions. In this paper, we propose a novel, lightweight evaluation strategy for logical reasoning that uses query-key alignments inside transformer attention heads. By computing a single forward pass and extracting a "QK-score" from carefully chosen heads, our method reveals latent representations that reliably separate valid from invalid inferences, offering a scalable alternative to traditional ablation-based techniques. We also provide an empirical validation on multiple logical reasoning benchmarks, demonstrating improved robustness of our evaluation method against distractors and increased reasoning depth. The experiments were conducted on a diverse set of models, ranging from 1.5B to 70B parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17017v1</guid>
      <category>cs.CL</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.LO</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eduard Tulchinskii, Anastasia Voznyuk, Laida Kushnareva, Andrei Andriiainen, Irina Piontkovskaya, Evgeny Burnaev, Serguei Barannikov</dc:creator>
    </item>
    <item>
      <title>Feature-Level Insights into Artificial Text Detection with Sparse Autoencoders</title>
      <link>https://arxiv.org/abs/2503.03601</link>
      <description>arXiv:2503.03601v1 Announce Type: cross 
Abstract: Artificial Text Detection (ATD) is becoming increasingly important with the rise of advanced Large Language Models (LLMs). Despite numerous efforts, no single algorithm performs consistently well across different types of unseen text or guarantees effective generalization to new LLMs. Interpretability plays a crucial role in achieving this goal. In this study, we enhance ATD interpretability by using Sparse Autoencoders (SAE) to extract features from Gemma-2-2b residual stream. We identify both interpretable and efficient features, analyzing their semantics and relevance through domain- and model-specific statistics, a steering approach, and manual or LLM-based interpretation. Our methods offer valuable insights into how texts from various models differ from human-written content. We show that modern LLMs have a distinct writing style, especially in information-dense domains, even though they can produce human-like outputs with personalized prompts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03601v1</guid>
      <category>cs.CL</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kristian Kuznetsov, Laida Kushnareva, Polina Druzhinina, Anton Razzhigaev, Anastasia Voznyuk, Irina Piontkovskaya, Evgeny Burnaev, Serguei Barannikov</dc:creator>
    </item>
    <item>
      <title>Decode and Forward Relaying for SC-FDE Systems with a Multi-Antenna Relay</title>
      <link>https://arxiv.org/abs/2503.10921</link>
      <description>arXiv:2503.10921v1 Announce Type: cross 
Abstract: In this paper, a cooperative relay network consisting of a single-antenna source, a multi-antenna relay, and a multi-antenna destination is considered. The relay operates in decode-and-forward (DF) mode under frequency-selective fading. To combat intersymbol interference (ISI), single-carrier frequency-domain equalization (SC-FDE) with or without decision feedback is deployed at the relay and the destination. The equalization coefficients are obtained using minimum mean squared error (MMSE) criterion. Both equal and optimum power allocations for a constant total transmit power at the relay are considered. While, the optimum power allocation is a non-convex problem, the solution is obtained using strong duality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10921v1</guid>
      <category>eess.SP</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Farshad Dizani, Ali Olfat</dc:creator>
    </item>
    <item>
      <title>Non Line-of-Sight Optical Wireless Communication using Neuromorphic Cameras</title>
      <link>https://arxiv.org/abs/2503.11226</link>
      <description>arXiv:2503.11226v1 Announce Type: cross 
Abstract: Neuromorphic or event cameras, inspired by biological vision systems, capture changes in illumination with high temporal resolution and efficiency, producing streams of events rather than traditional images. In this paper, we explore the use of neuromorphic cameras for passive optical wireless communication (OWC), leveraging their asynchronous detection of illumination changes to decode data transmitted through reflections of light from objects. We propose a novel system that utilizes neuromorphic cameras for passive visible light communication (VLC), extending the concept to Non Line-of-Sight (NLoS) scenarios through passive reflections from everyday objects. Our experiments demonstrate the feasibility and advantages of using neuromorphic cameras for VLC, characterizing the performance of various modulation schemes, including traditional On-Off Keying (OOK) and advanced N-pulse modulation. We introduce an adaptive N-pulse modulation scheme that dynamically adjusts encoding based on the packet's bit composition, achieving higher data rates and robustness in different scenarios. Our results show that lighter-colored, glossy objects are better for NLoS communication, while larger objects and those with matte finishes experience higher error rates due to multipath reflections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11226v1</guid>
      <category>cs.CV</category>
      <category>cs.ET</category>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abbaas Alif Mohamed Nishar, Alireza Marefat, Ashwin Ashok</dc:creator>
    </item>
    <item>
      <title>The time scale of redundancy between prosody and linguistic context</title>
      <link>https://arxiv.org/abs/2503.11630</link>
      <description>arXiv:2503.11630v1 Announce Type: cross 
Abstract: In spoken language, speakers transmit information not only using words, but also via a rich array of non-verbal signals, which include prosody -- the auditory features of speech. However, previous studies have shown that prosodic features exhibit significant redundancy with both past and future words. Here, we examine the time scale of this relationship: How many words in the past (or future) contribute to predicting prosody? We find that this scale differs for past and future words. Prosody's redundancy with past words extends across approximately 3-8 words, whereas redundancy with future words is limited to just 1-2 words. These findings indicate that the prosody-future relationship reflects local word dependencies or short-scale processes such as next word prediction, while the prosody-past relationship unfolds over a longer time scale. The latter suggests that prosody serves to emphasize earlier information that may be challenging for listeners to process given limited cognitive resources in real-time communication. Our results highlight the role of prosody in shaping efficient communication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11630v1</guid>
      <category>cs.CL</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tamar I. Regev, Chiebuka Ohams, Shaylee Xie, Lukas Wolf, Evelina Fedorenko, Alex Warstadt, Ethan Wilcox, Tiago Pimentel</dc:creator>
    </item>
    <item>
      <title>On the permutation automorphisms of binary cubic codes</title>
      <link>https://arxiv.org/abs/2402.10667</link>
      <description>arXiv:2402.10667v2 Announce Type: replace 
Abstract: We investigate the structural properties of binary cubic codes. We prove that up to dimension or codimension $4$, there is no binary linear code whose permutation automorphism group is generated by a fixed point free permutation of order $3$. We also prove that there is no $5$-dimensional binary code whose length is at least $30$ and whose permutation automorphism group is generated by a fixed point free permutation of order $3$. We also provide some computational results for the five dimensional binary cubic codes of length smaller than $30$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.10667v2</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Murat Altunbulak, Fatma Altunbulak Aksu, Roghayeh Hafezieh, \.Ipek Tuvay</dc:creator>
    </item>
    <item>
      <title>On the Impact of Uncertainty and Calibration on Likelihood-Ratio Membership Inference Attacks</title>
      <link>https://arxiv.org/abs/2402.10686</link>
      <description>arXiv:2402.10686v3 Announce Type: replace 
Abstract: In a membership inference attack (MIA), an attacker exploits the overconfidence exhibited by typical machine learning models to determine whether a specific data point was used to train a target model. In this paper, we analyze the performance of the likelihood ratio attack (LiRA) within an information-theoretical framework that allows the investigation of the impact of the aleatoric uncertainty in the true data generation process, of the epistemic uncertainty caused by a limited training data set, and of the calibration level of the target model. We compare three different settings, in which the attacker receives decreasingly informative feedback from the target model: confidence vector (CV) disclosure, in which the output probability vector is released; true label confidence (TLC) disclosure, in which only the probability assigned to the true label is made available by the model; and decision set (DS) disclosure, in which an adaptive prediction set is produced as in conformal prediction. We derive bounds on the advantage of an MIA adversary with the aim of offering insights into the impact of uncertainty and calibration on the effectiveness of MIAs. Simulation results demonstrate that the derived analytical bounds predict well the effectiveness of MIAs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.10686v3</guid>
      <category>cs.IT</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Meiyi Zhu, Caili Guo, Chunyan Feng, Osvaldo Simeone</dc:creator>
    </item>
    <item>
      <title>Capacity Bounds for Broadcast Channels with Bidirectional Conferencing Decoders</title>
      <link>https://arxiv.org/abs/2406.20019</link>
      <description>arXiv:2406.20019v2 Announce Type: replace 
Abstract: The two-user broadcast channel (BC) with receivers connected by bidirectional cooperation links of finite capacities, known as conferencing decoders, is considered. A novel capacity region outer bound is established based on multiple applications of the Csisz\'{a}r-K\"{o}rner identity. Achievable rate regions are derived by using Marton's coding as the transmission scheme, together with different combinations of decode-and-forward and quantize-bin-and-forward strategies at the receivers. It is shown that the outer bound coincides with the achievable rate region for a new class of semi-deterministic BCs with degraded message sets; for this class of channels, one-round cooperation is sufficient to achieve the capacity. Capacity result is also derived for a class of more capable semi-deterministic BCs with both common and private messages and one-sided conferencing. For the Gaussian BC with conferencing decoders, if the noises at the decoders are perfectly correlated (i.e., the correlation is either 1 or -1), the new outer bound yields exact capacity region for two cases: i) BC with degraded message sets; ii) BC with one-sided conferencing from the weaker receiver to the stronger receiver. When the noises have arbitrary correlation, the outer bound is shown to be within half a bit from the capacity region for these same two cases. Finally, for the general Gaussian BC, a one-sided cooperation scheme based on decode-and-forward from the stronger receiver to the weaker receiver is shown to achieve the capacity region to within $\frac{1}{2}\log (\frac{2}{1-|\lambda|})$ bits, where $\lambda$ is the noise correlation. An interesting implication of these results is that for a Gaussian BC with perfectly negatively correlated noises and conferencing decoders with finite cooperation link capacities, it is possible to achieve a strictly positive rate using only infinitesimal amount of transmit power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.20019v2</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Reza K. Farsani, Wei Yu</dc:creator>
    </item>
    <item>
      <title>About the generalized Hamming weights of matrix-product codes</title>
      <link>https://arxiv.org/abs/2407.11810</link>
      <description>arXiv:2407.11810v2 Announce Type: replace 
Abstract: We derive a general lower bound for the generalized Hamming weights of nested matrix-product codes, with a particular emphasis on the cases with two and three constituent codes. We also provide an upper bound which is reminiscent of the bounds used for the minimum distance of matrix-product codes. When the constituent codes are two Reed-Solomon codes, we obtain an explicit formula for the generalized Hamming weights of the resulting matrix-product code. We also deal with the non-nested case for the case of two constituent codes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11810v2</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s40314-025-03141-x</arxiv:DOI>
      <dc:creator>Rodrigo San-Jos\'e</dc:creator>
    </item>
    <item>
      <title>A criterion for determining whether multiple shells support a $t$-design</title>
      <link>https://arxiv.org/abs/2310.15856</link>
      <description>arXiv:2310.15856v3 Announce Type: replace-cross 
Abstract: In this paper, we provide a criterion for determining whether multiple shells support a $t$-design. We construct as a corollary an infinite series of $2$-designs using power residue codes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.15856v3</guid>
      <category>math.CO</category>
      <category>cs.IT</category>
      <category>math.GR</category>
      <category>math.IT</category>
      <category>math.NT</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Madoka Awada, Reina Ishikawa, Tsuyoshi Miezaki, Yuuho Tanaka</dc:creator>
    </item>
    <item>
      <title>On the phase diagram of extensive-rank symmetric matrix denoising beyond rotational invariance</title>
      <link>https://arxiv.org/abs/2411.01974</link>
      <description>arXiv:2411.01974v2 Announce Type: replace-cross 
Abstract: Matrix denoising is central to signal processing and machine learning. Its statistical analysis when the matrix to infer has a factorised structure with a rank growing proportionally to its dimension remains a challenge, except when it is rotationally invariant. In this case the information theoretic limits and an efficient Bayes-optimal denoising algorithm, called rotational invariant estimator [1,2], are known. Beyond this setting few results can be found. The reason is that the model is not a usual spin system because of the growing rank dimension, nor a matrix model (as appearing in high-energy physics) due to the lack of rotation symmetry, but rather a hybrid between the two. Here we make progress towards the understanding of Bayesian matrix denoising when the signal is a factored matrix $XX^\intercal$ that is not rotationally invariant. Monte Carlo simulations suggest the existence of a \emph{denoising-factorisation transition} separating a phase where denoising using the rotational invariant estimator remains Bayes-optimal due to universality properties of the same nature as in random matrix theory, from one where universality breaks down and better denoising is possible, though algorithmically hard. We argue that it is only beyond the transition that factorisation, i.e., estimating $X$ itself, becomes possible up to irresolvable ambiguities. On the theory side, we combine mean-field techniques in an interpretable multiscale fashion in order to access the minimum mean-square error and mutual information. Interestingly, our alternative method yields equations reproducible by the replica approach of [3]. Using numerical insights, we delimit the portion of phase diagram where we conjecture the mean-field theory to be exact, and correct it using universality when it is not. Our complete ansatz matches well the numerics in the whole phase diagram when considering finite size effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01974v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jean Barbier, Francesco Camilli, Justin Ko, Koki Okajima</dc:creator>
    </item>
    <item>
      <title>Improving Pre-trained Self-Supervised Embeddings Through Effective Entropy Maximization</title>
      <link>https://arxiv.org/abs/2411.15931</link>
      <description>arXiv:2411.15931v2 Announce Type: replace-cross 
Abstract: A number of different architectures and loss functions have been applied to the problem of self-supervised learning (SSL), with the goal of developing embeddings that provide the best possible pre-training for as-yet-unknown, lightly supervised downstream tasks. One of these SSL criteria is to maximize the entropy of a set of embeddings in some compact space. But the goal of maximizing the embedding entropy often depends -- whether explicitly or implicitly -- upon high dimensional entropy estimates, which typically perform poorly in more than a few dimensions. In this paper, we motivate an effective entropy maximization criterion (E2MC), defined in terms of easy-to-estimate, low-dimensional constraints. We demonstrate that using it to continue training an already-trained SSL model for only a handful of epochs leads to a consistent and, in some cases, significant improvement in downstream performance. We perform careful ablation studies to show that the improved performance is due to the proposed add-on criterion. We also show that continued pre-training with alternative criteria does not lead to notable improvements, and in some cases, even degrades performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15931v2</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Deep Chakraborty, Yann LeCun, Tim G. J. Rudner, Erik Learned-Miller</dc:creator>
    </item>
  </channel>
</rss>
