<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 06 Jun 2024 01:51:34 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 05 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Long-term foehn reconstruction combining unsupervised and supervised learning</title>
      <link>https://arxiv.org/abs/2406.01818</link>
      <description>arXiv:2406.01818v1 Announce Type: new 
Abstract: Foehn winds, characterized by abrupt temperature increases and wind speed changes, significantly impact regions on the leeward side of mountain ranges, e.g., by spreading wildfires. Understanding how foehn occurrences change under climate change is crucial. Unfortunately, foehn cannot be measured directly but has to be inferred from meteorological measurements employing suitable classification schemes. Hence, this approach is typically limited to specific periods for which the necessary data are available. We present a novel approach for reconstructing historical foehn occurrences using a combination of unsupervised and supervised probabilistic statistical learning methods. We utilize in-situ measurements (available for recent decades) to train an unsupervised learner (finite mixture model) for automatic foehn classification. These labeled data are then linked to reanalysis data (covering longer periods) using a supervised learner (lasso or boosting). This allows to reconstruct past foehn probabilities based solely on reanalysis data. Applying this method to ERA5 reanalysis data for six stations across Switzerland and Austria achieves accurate hourly reconstructions of north and south foehn occurrence, respectively, dating back to 1940. This paves the way for investigating how seasonal foehn patterns have evolved over the past 83 years, providing valuable insights into climate change impacts on these critical wind events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01818v1</guid>
      <category>stat.AP</category>
      <category>physics.ao-ph</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Reto Stauffer, Achim Zeileis, Georg J. Mayr</dc:creator>
    </item>
    <item>
      <title>Variable importance measure for spatial machine learning models with application to air pollution exposure prediction</title>
      <link>https://arxiv.org/abs/2406.01982</link>
      <description>arXiv:2406.01982v1 Announce Type: new 
Abstract: Exposure assessment is fundamental to air pollution cohort studies. The objective is to predict air pollution exposures for study subjects at locations without data in order to optimize our ability to learn about health effects of air pollution. In addition to generating accurate predictions to minimize exposure measurement error, understanding the mechanism captured by the model is another crucial aspect that may not always be straightforward due to the complex nature of machine learning methods, as well as the lack of unifying notions of variable importance. This is further complicated in air pollution modeling by the presence of spatial correlation. We tackle these challenges in two datasets: sulfur (S) from regulatory United States national PM2.5 sub-species data and ultrafine particles (UFP) from a new Seattle-area traffic-related air pollution dataset. Our key contribution is a leave-one-out approach for variable importance that leads to interpretable and comparable measures for a broad class of models with separable mean and covariance components. We illustrate our approach with several spatial machine learning models, and it clearly highlights the difference in model mechanisms, even for those producing similar predictions. We leverage insights from this variable importance measure to assess the relative utilities of two exposure models for S and UFP that have similar out-of-sample prediction accuracies but appear to draw on different types of spatial information to make predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01982v1</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Si Cheng, Magali N. Blanco, Lianne Sheppard, Ali Shojaie, Adam Szpiro</dc:creator>
    </item>
    <item>
      <title>Impacts of Climate Change on Mortality: An extrapolation of temperature effects based on time series data in France</title>
      <link>https://arxiv.org/abs/2406.02054</link>
      <description>arXiv:2406.02054v1 Announce Type: new 
Abstract: Most contemporary mortality models rely on extrapolating trends or past events. However, population dynamics will be significantly impacted by climate change, notably the influence of temperatures on mortality. In this paper, we introduce a novel approach to incorporate temperature effects on projected mortality using a multi-population mortality model. This method combines a stochastic mortality model with a climate epidemiology model, predicting mortality variations due to daily temperature fluctuations, be it excesses or insufficiencies. The significance of this approach lies in its ability to disrupt mortality projections by utilizing temperature forecasts from climate models and to assess the impact of this unaccounted risk factor in conventional mortality models. We illustrate this proposed mortality model using French data stratified by gender, focusing on past temperatures and mortality. Utilizing climate model predictions across various IPCC scenarios, we investigate gains and loss in life expectancy linked to temperatures and the additional mortality induced by extreme heatwaves, and quantify them by assessing this new risk factor in prediction intervals. Furthermore, we analyze the geographical differences across the Metropolitan France.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02054v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Quentin Guibert (CEREMADE), Ga\"elle Pincemin (SAF), Fr\'ed\'eric Planchet (SAF)</dc:creator>
    </item>
    <item>
      <title>A Practical Approach for Exploring Granger Connectivity in High-Dimensional Networks of Time Series</title>
      <link>https://arxiv.org/abs/2406.02360</link>
      <description>arXiv:2406.02360v1 Announce Type: new 
Abstract: This manuscript presents a novel method for discovering effective connectivity between specified pairs of nodes in a high-dimensional network of time series. To accurately perform Granger causality analysis from the first node to the second node, it is essential to eliminate the influence of all other nodes within the network. The approach proposed is to create a low-dimensional representation of all other nodes in the network using frequency-domain-based dynamic principal component analysis (spectral DPCA). The resulting scores are subsequently removed from the first and second nodes of interest, thus eliminating the confounding effect of other nodes within the high-dimensional network. To conduct hypothesis testing on Granger causality, we propose a permutation-based causality test. This test enhances the accuracy of our findings when the error structures are non-Gaussian. The approach has been validated in extensive simulation studies, which demonstrate the efficacy of the methodology as a tool for causality analysis in complex time series networks. The proposed methodology has also been demonstrated to be both expedient and viable on real datasets, with particular success observed on multichannel EEG networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02360v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sipan Aslan, Hernando Ombao</dc:creator>
    </item>
    <item>
      <title>Markov Chain Monte Carlo with Gaussian Process Emulation for a 1D Hemodynamics Model of CTEPH</title>
      <link>https://arxiv.org/abs/2406.01599</link>
      <description>arXiv:2406.01599v1 Announce Type: cross 
Abstract: Microvascular disease is a contributor to persistent pulmonary hypertension in those with chronic thromboembolic pulmonary hypertension (CTEPH). The heterogenous nature of the micro and macrovascular defects motivates the use of personalized computational models, which can predict flow dynamics within multiple generations of the arterial tree and into the microvasculature. Our study uses computational hemodynamics models and Gaussian processes for rapid, subject-specific calibration using retrospective data from a large animal model of CTEPH. Our subject-specific predictions shed light on microvascular dysfunction and arterial wall shear stress changes in CTEPH.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01599v1</guid>
      <category>q-bio.QM</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amirreza Kachabi, Mitchel J. Colebank, Sofia Altieri Correa, Naomi C. Chesler</dc:creator>
    </item>
    <item>
      <title>An efficient Wasserstein-distance approach for reconstructing jump-diffusion processes using parameterized neural networks</title>
      <link>https://arxiv.org/abs/2406.01653</link>
      <description>arXiv:2406.01653v1 Announce Type: cross 
Abstract: We analyze the Wasserstein distance ($W$-distance) between two probability distributions associated with two multidimensional jump-diffusion processes. Specifically, we analyze a temporally decoupled squared $W_2$-distance, which provides both upper and lower bounds associated with the discrepancies in the drift, diffusion, and jump amplitude functions between the two jump-diffusion processes. Then, we propose a temporally decoupled squared $W_2$-distance method for efficiently reconstructing unknown jump-diffusion processes from data using parameterized neural networks. We further show its performance can be enhanced by utilizing prior information on the drift function of the jump-diffusion process. The effectiveness of our proposed reconstruction method is demonstrated across several examples and applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01653v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingtao Xia, Xiangting Li, Qijing Shen, Tom Chou</dc:creator>
    </item>
    <item>
      <title>Diffusion Boosted Trees</title>
      <link>https://arxiv.org/abs/2406.01813</link>
      <description>arXiv:2406.01813v1 Announce Type: cross 
Abstract: Combining the merits of both denoising diffusion probabilistic models and gradient boosting, the diffusion boosting paradigm is introduced for tackling supervised learning problems. We develop Diffusion Boosted Trees (DBT), which can be viewed as both a new denoising diffusion generative model parameterized by decision trees (one single tree for each diffusion timestep), and a new boosting algorithm that combines the weak learners into a strong learner of conditional distributions without making explicit parametric assumptions on their density forms. We demonstrate through experiments the advantages of DBT over deep neural network-based diffusion models as well as the competence of DBT on real-world regression tasks, and present a business application (fraud detection) of DBT for classification on tabular data with the ability of learning to defer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01813v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xizewen Han, Mingyuan Zhou</dc:creator>
    </item>
    <item>
      <title>Development of Bayesian Component Failure Models in E1 HEMP Grid Analysis</title>
      <link>https://arxiv.org/abs/2406.01923</link>
      <description>arXiv:2406.01923v1 Announce Type: cross 
Abstract: Combined electric power system and High-Altitude Electromagnetic Pulse (HEMP) models are being developed to determine the effect of a HEMP on the US power grid. The work relies primarily on deterministic methods; however, it is computationally untenable to evaluate the E1 HEMP response of large numbers of grid components distributed across a large interconnection. Further, the deterministic assessment of these components' failures are largely unachievable. E1 HEMP laboratory testing of the components is accomplished, but is expensive, leaving few data points to construct failure models of grid components exposed to E1 HEMP. The use of Bayesian priors, developed using the subject matter expertise, combined with the minimal test data in a Bayesian inference process, provides the basis for the development of more robust and cost-effective statistical component failure models. These can be used with minimal computational burden in a simulation environment such as sampling of Cumulative Distribution Functions (CDFs).</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01923v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Niladri Das, Ross Guttromson, Tommie A. Catanach</dc:creator>
    </item>
    <item>
      <title>Optimal Stock Portfolio Selection with a Multivariate Hidden Markov Model</title>
      <link>https://arxiv.org/abs/2406.02297</link>
      <description>arXiv:2406.02297v1 Announce Type: cross 
Abstract: The underlying market trends that drive stock price fluctuations are often referred to in terms of bull and bear markets. Optimal stock portfolio selection methods need to take into account these market trends; however, the bull and bear market states tend to be unobserved and can only be assigned retrospectively. We fit a linked hidden Markov model (LHMM) to relative stock price changes for S&amp;P 500 stocks from 2011--2016 based on weekly closing values. The LHMM consists of a multivariate state process whose individual components correspond to HMMs for each of the 12 sectors of the S\&amp;P 500 stocks. The state processes are linked using a Gaussian copula so that the states of the component chains are correlated at any given time point. The LHMM allows us to capture more heterogeneity in the underlying market dynamics for each sector. In this study, stock performances are evaluated in terms of capital gains using the LHMM by utilizing historical stock price data. Based on the fitted LHMM, optimal stock portfolios are constructed to maximize capital gain while balancing reward and risk. Under out-of-sample testing, the annual capital gain for the portfolios for 2016--2017 are calculated. Portfolios constructed using the LHMM are able to generate returns comparable to the S&amp;P 500 index.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02297v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s13571-022-00290-5</arxiv:DOI>
      <arxiv:journal_reference>Sankhya B85 (Suppl 1), 177-198 (2023)</arxiv:journal_reference>
      <dc:creator>Reetam Majumder, Qing Ji, Nagaraj K. Neerchal</dc:creator>
    </item>
    <item>
      <title>Compositional dynamic modelling for causal prediction in multivariate time series</title>
      <link>https://arxiv.org/abs/2406.02320</link>
      <description>arXiv:2406.02320v1 Announce Type: cross 
Abstract: Theoretical developments in sequential Bayesian analysis of multivariate dynamic models underlie new methodology for causal prediction. This extends the utility of existing models with computationally efficient methodology, enabling routine exploration of Bayesian counterfactual analyses with multiple selected time series as synthetic controls. Methodological contributions also define the concept of outcome adaptive modelling to monitor and inferentially respond to changes in experimental time series following interventions designed to explore causal effects. The benefits of sequential analyses with time-varying parameter models for causal investigations are inherited in this broader setting. A case study in commercial causal analysis-- involving retail revenue outcomes related to marketing interventions-- highlights the methodological advances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02320v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kevin Li, Graham Tierney, Christoph Hellmayr, Mike West</dc:creator>
    </item>
    <item>
      <title>A Bayesian nonlinear stationary model with multiple frequencies for business cycle analysis</title>
      <link>https://arxiv.org/abs/2406.02321</link>
      <description>arXiv:2406.02321v1 Announce Type: cross 
Abstract: We design a novel, nonlinear single-source-of-error model for analysis of multiple business cycles. The model's specification is intended to capture key empirical characteristics of business cycle data by allowing for simultaneous cycles of different types and lengths, as well as time-variable amplitude and phase shift. The model is shown to feature relevant theoretical properties, including stationarity and pseudo-cyclical autocovariance function, and enables a decomposition of overall cyclic fluctuations into separate frequency-specific components. We develop a Bayesian framework for estimation and inference in the model, along with an MCMC procedure for posterior sampling, combining the Gibbs sampler and the Metropolis-Hastings algorithm, suitably adapted to address encountered numerical issues. Empirical results obtained from the model applied to the Polish GDP growth rates imply co-existence of two types of economic fluctuations: the investment and inventory cycles, and support the stochastic variability of the amplitude and phase shift, also capturing some business cycle asymmetries. Finally, the Bayesian framework enables a fully probabilistic inference on the business cycle clocks and dating, which seems the most relevant approach in view of economic uncertainties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02321v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>{\L}ukasz Lenart, {\L}ukasz Kwiatkowski, Justyna Wr\'oblewska</dc:creator>
    </item>
    <item>
      <title>Monitoring overall survival in pivotal trials in indolent cancers</title>
      <link>https://arxiv.org/abs/2310.20658</link>
      <description>arXiv:2310.20658v3 Announce Type: replace 
Abstract: Indolent cancers are characterized by long overall survival (OS) times. Therefore, powering a clinical trial to provide definitive assessment of the effects of an experimental intervention on OS in a reasonable timeframe is generally infeasible. Instead, the primary outcome in many pivotal trials is an intermediate clinical response such as progression-free survival (PFS). In several recently reported pivotal trials of interventions for indolent cancers that yielded promising results on an intermediate outcome, however, more mature data or post-approval trials showed concerning OS trends. These problematic results have prompted a keen interest in quantitative approaches for monitoring OS that can support regulatory decision-making related to the risk of an unacceptably large detrimental effect on OS. For example, the US Food and Drug Administration, the American Association for Cancer Research, and the American Statistical Association recently organized a one-day multi-stakeholder workshop entitled 'Overall Survival in Oncology Clinical Trials'. In this paper, we propose OS monitoring guidelines tailored for the setting of indolent cancers. Our pragmatic approach is modeled, in part, on the monitoring guidelines the FDA has used in cardiovascular safety trials conducted in Type 2 Diabetes Mellitus. We illustrate proposals through application to several examples informed by actual case studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.20658v3</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Thomas R Fleming, Lisa V Hampson, Bharani Bharani-Dharan, Frank Bretz, Arunava Chakravartty, Thibaud Coroller, Evanthia Koukouli, Janet Wittes, Nigel Yateman, Emmanuel Zuber</dc:creator>
    </item>
    <item>
      <title>Analyzing trends for agricultural decision support system using twitter data</title>
      <link>https://arxiv.org/abs/2406.00577</link>
      <description>arXiv:2406.00577v2 Announce Type: replace 
Abstract: The trends and reactions of the general public towards global events can be analyzed using data from social platforms, including Twitter. The number of tweets has been reported to help detect variations in communication traffic within subsets like countries, age groups and industries. Similarly, publicly accessible data and (in particular) data from social media about agricultural issues provide a great opportunity for obtaining instantaneous snapshots of farmer opinions and a method to track changes in opinion through temporal analysis. In this paper we hypothesize that the presence of keywords like precision agriculture, digital agriculture, Internet of Things (IoT), BigData, remote sensing, GPS, etc., in tweets could serve as an indicator of discussions centered around interest in modern farming practices. We extracted relevant tweets using keywords such as IoT, BigData and Geographical Information System (GIS), and then analyzed their geographical origin and frequency of their mention. We analyzed the Twitter data for the period of 1st -11th January 2018 to understand these trends and the factors affecting them. These factors, such as special events, projects, biogeography, etc., were further analyzed using tweet sources and trending hashtags from the database. The regions with the highest interest in the keywords were United States, Egypt, Brazil, Japan and China. A comparison of frequency of keywords revealed IoT as the most tweeted word (77.6%) in the downloaded data. The most used language was English followed by Spanish, Japanese and French. Periodical tweets on IoT from an account handled by IoT project on Twitter and Seminars on IoT in January in Santa Catarina (Brazil) were found to be the underlying factors for the observed trends.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00577v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sneha Jha, Dharmendra Saraswat, Mark D. Ward</dc:creator>
    </item>
    <item>
      <title>Adaptive Online Experimental Design for Causal Discovery</title>
      <link>https://arxiv.org/abs/2405.11548</link>
      <description>arXiv:2405.11548v2 Announce Type: replace-cross 
Abstract: Causal discovery aims to uncover cause-and-effect relationships encoded in causal graphs by leveraging observational, interventional data, or their combination. The majority of existing causal discovery methods are developed assuming infinite interventional data. We focus on data interventional efficiency and formalize causal discovery from the perspective of online learning, inspired by pure exploration in bandit problems. A graph separating system, consisting of interventions that cut every edge of the graph at least once, is sufficient for learning causal graphs when infinite interventional data is available, even in the worst case. We propose a track-and-stop causal discovery algorithm that adaptively selects interventions from the graph separating system via allocation matching and learns the causal graph based on sampling history. Given any desired confidence value, the algorithm determines a termination condition and runs until it is met. We analyze the algorithm to establish a problem-dependent upper bound on the expected number of required interventional samples. Our proposed algorithm outperforms existing methods in simulations across various randomly generated causal graphs. It achieves higher accuracy, measured by the structural hamming distance (SHD) between the learned causal graph and the ground truth, with significantly fewer samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11548v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammad Qasim Elahi, Lai Wei, Murat Kocaoglu, Mahsa Ghasemi</dc:creator>
    </item>
  </channel>
</rss>
