<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 23 Jul 2024 04:00:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 23 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Statistical Models for Outbreak Detection of Measles in North Cotabato, Philippines</title>
      <link>https://arxiv.org/abs/2407.15028</link>
      <description>arXiv:2407.15028v1 Announce Type: cross 
Abstract: A measles outbreak occurs when the number of cases of measles in the population exceeds the typical level. Outbreaks that are not detected and managed early can increase mortality and morbidity and incur costs from activities responding to these events. The number of measles cases in the Province of North Cotabato, Philippines, was used in this study. Weekly reported cases of measles from January 2016 to December 2021 were provided by the Epidemiology and Surveillance Unit of the North Cotabato Provincial Health Office. Several integer-valued autoregressive (INAR) time series models were used to explore the possibility of detecting and identifying measles outbreaks in the province along with the classical ARIMA model. These models were evaluated based on goodness of fit, measles outbreak detection accuracy, and timeliness. The results of this study confirmed that INAR models have the conceptual advantage over ARIMA since the latter produces non-integer forecasts, which are not realistic for count data such as measles cases. Among the INAR models, the ZINGINAR (1) model was recommended for having a good model fit and timely and accurate detection of outbreaks. Furthermore, policymakers and decision-makers from relevant government agencies can use the ZINGINAR (1) model to improve disease surveillance and implement preventive measures against contagious diseases beforehand.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15028v1</guid>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:journal_reference>Mindanao Journal of Science and Technology, 22(1) (2024)</arxiv:journal_reference>
      <dc:creator>Julienne Kate N. Kintanar, Roel F. Ceballos</dc:creator>
    </item>
    <item>
      <title>Estimating Monte Carlo variance from multiple Markov chains</title>
      <link>https://arxiv.org/abs/2007.04229</link>
      <description>arXiv:2007.04229v4 Announce Type: replace-cross 
Abstract: Modern computational advances have enabled easy parallel implementations of Markov chain Monte Carlo (MCMC). However, almost all work in estimating the variance of Monte Carlo averages, including the efficient batch means (BM) estimator, focuses on a single-chain MCMC run. We demonstrate that simply averaging covariance matrix estimators from multiple chains can yield critical underestimates in small Monte Carlo sample sizes, especially for slow-mixing Markov chains. We extend the work of \cite{arg:and:2006} and propose a multivariate replicated batch means (RBM) estimator that utilizes information from parallel chains, thereby correcting for the underestimation. Under weak conditions on the mixing rate of the process, RBM is strongly consistent and exhibits similar large-sample bias and variance to the BM estimator. We also exhibit superior theoretical properties of RBM by showing that the (negative) bias in the RBM estimator is less than the average BM estimator in the presence of positive correlation in MCMC. Consequently, in small runs, the RBM estimator can be dramatically superior and this is demonstrated through a variety of examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2007.04229v4</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kushagra Gupta, Dootika Vats</dc:creator>
    </item>
    <item>
      <title>Credibility Theory Based on Winsorizing</title>
      <link>https://arxiv.org/abs/2306.09507</link>
      <description>arXiv:2306.09507v4 Announce Type: replace-cross 
Abstract: The classical B\"{u}hlmann credibility model has been widely applied to premium estimation for group insurance contracts and other insurance types. In this paper, we develop a robust B\"{u}hlmann credibility model using the winsorized version of loss data, also known as the winsorized mean (a robust alternative to the traditional individual mean). This approach assumes that the observed sample data come from a contaminated underlying model with a small percentage of contaminated sample data. This framework provides explicit formulas for the structural parameters in credibility estimation for scale-shape distribution families, location-scale distribution families, and their variants, commonly used in insurance risk modeling. Using the theory of \(L\)-estimators (different from the influence function approach), we derive the asymptotic properties of the proposed method and validate them through a comprehensive simulation study, comparing their performance to credibility based on the trimmed mean. By varying the winsorizing/trimming thresholds in several parametric models, we find that all structural parameters derived from the winsorized approach are less volatile than those from the trimmed approach. Using the winsorized mean as a robust risk measure can reduce the influence of parametric loss assumptions on credibility estimation. Additionally, we discuss non-parametric estimations in credibility. Finally, a numerical illustration from the Wisconsin Local Government Property Insurance Fund indicates that the proposed robust credibility approach mitigates the impact of model mis-specification and captures the risk behavior of loss data from a broader perspective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.09507v4</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s13385-024-00391-7</arxiv:DOI>
      <arxiv:journal_reference>European Actuarial Journal, 2024</arxiv:journal_reference>
      <dc:creator>Qian Zhao, Chudamani Poudyal</dc:creator>
    </item>
    <item>
      <title>Large-scale Bayesian Structure Learning for Gaussian Graphical Models using Marginal Pseudo-likelihood</title>
      <link>https://arxiv.org/abs/2307.00127</link>
      <description>arXiv:2307.00127v3 Announce Type: replace-cross 
Abstract: Bayesian methods for learning Gaussian graphical models offer a comprehensive framework that addresses model uncertainty and incorporates prior knowledge. Despite their theoretical strengths, the applicability of Bayesian methods is often constrained by computational demands, especially in modern contexts involving thousands of variables. To overcome this issue, we introduce two novel Markov chain Monte Carlo (MCMC) search algorithms with a significantly lower computational cost than leading Bayesian approaches. Our proposed MCMC-based search algorithms use the marginal pseudo-likelihood approach to bypass the complexities of computing intractable normalizing constants and iterative precision matrix sampling. These algorithms can deliver reliable results in mere minutes on standard computers, even for large-scale problems with one thousand variables. Furthermore, our proposed method efficiently addresses model uncertainty by exploring the full posterior graph space. We establish the consistency of graph recovery, and our extensive simulation study indicates that the proposed algorithms, particularly for large-scale sparse graphs, outperform leading Bayesian approaches in terms of computational efficiency and accuracy. We also illustrate the practical utility of our methods on medium and large-scale applications from human and mice gene expression studies. The implementation supporting the new approach is available through the R package BDgraph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.00127v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Reza Mohammadi, Marit Schoonhoven, Lucas Vogels, S. Ilker Birbil</dc:creator>
    </item>
    <item>
      <title>Degrees of Freedom: Search Cost and Self-consistency</title>
      <link>https://arxiv.org/abs/2308.13630</link>
      <description>arXiv:2308.13630v2 Announce Type: replace-cross 
Abstract: Model degrees of freedom ($\df$) is a fundamental concept in statistics because it quantifies the flexibility of a fitting procedure and is indispensable in model selection. To investigate the gap between $\df$ and the number of independent variables in the fitting procedure, \textcite{tibshiraniDegreesFreedomModel2015} introduced the \emph{search degrees of freedom} ($\sdf$) concept to account for the search cost during model selection. However, this definition has two limitations: it does not consider fitting procedures in augmented spaces and does not use the same fitting procedure for $\sdf$ and $\df$. We propose a \emph{modified search degrees of freedom} ($\msdf$) to directly account for the cost of searching in either original or augmented spaces. We check this definition for various fitting procedures, including classical linear regressions, spline methods, adaptive regressions (the best subset and the lasso), regression trees, and multivariate adaptive regression splines (MARS). In many scenarios when $\sdf$ is applicable, $\msdf$ reduces to $\sdf$. However, for certain procedures like the lasso, $\msdf$ offers a fresh perspective on search costs. For some complex procedures like MARS, the $\df$ has been pre-determined during model fitting, but the $\df$ of the final fitted procedure might differ from the pre-determined one. To investigate this discrepancy, we introduce the concepts of \emph{nominal} $\df$ and \emph{actual} $\df$, and define the property of \emph{self-consistency}, which occurs when there is no gap between these two $\df$'s. We propose a correction procedure for MARS to align these two $\df$'s, demonstrating improved fitting performance through extensive simulations and two real data applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.13630v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lijun Wang, Hongyu Zhao, Xiaodan Fan</dc:creator>
    </item>
    <item>
      <title>Bayesian identification of nonseparable Hamiltonians with multiplicative noise using deep learning and reduced-order modeling</title>
      <link>https://arxiv.org/abs/2401.12476</link>
      <description>arXiv:2401.12476v3 Announce Type: replace-cross 
Abstract: This paper presents a structure-preserving Bayesian approach for learning nonseparable Hamiltonian systems using stochastic dynamic models allowing for statistically-dependent, vector-valued additive and multiplicative measurement noise. The approach is comprised of three main facets. First, we derive a Gaussian filter for a statistically-dependent, vector-valued, additive and multiplicative noise model that is needed to evaluate the likelihood within the Bayesian posterior. Second, we develop a novel algorithm for cost-effective application of Bayesian system identification to high-dimensional systems. Third, we demonstrate how structure-preserving methods can be incorporated into the proposed framework, using nonseparable Hamiltonians as an illustrative system class. We assess the method's performance based on the forecasting accuracy of a model estimated from single-trajectory data. We compare the Bayesian method to a state-of-the-art machine learning method on a canonical nonseparable Hamiltonian model and a chaotic double pendulum model with small, noisy training datasets. The results show that using the Bayesian posterior as a training objective can yield upwards of 724 times improvement in Hamiltonian mean squared error using training data with up to 10% multiplicative noise compared to a standard training objective. Lastly, we demonstrate the utility of the novel algorithm for parameter estimation of a 64-dimensional model of the spatially-discretized nonlinear Schr\"odinger equation with data corrupted by up to 20% multiplicative noise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.12476v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.DS</category>
      <category>physics.data-an</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicholas Galioto, Harsh Sharma, Boris Kramer, Alex Arkady Gorodetsky</dc:creator>
    </item>
    <item>
      <title>Hyperparameter Optimization for Randomized Algorithms: A Case Study for Random Features</title>
      <link>https://arxiv.org/abs/2407.00584</link>
      <description>arXiv:2407.00584v2 Announce Type: replace-cross 
Abstract: Randomized algorithms exploit stochasticity to reduce computational complexity. One important example is random feature regression (RFR) that accelerates Gaussian process regression (GPR). RFR approximates an unknown function with a random neural network whose hidden weights and biases are sampled from a probability distribution. Only the final output layer is fit to data. In randomized algorithms like RFR, the hyperparameters that characterize the sampling distribution greatly impact performance, yet are not directly accessible from samples. This makes optimization of hyperparameters via standard (gradient-based) optimization tools inapplicable. Inspired by Bayesian ideas from GPR, this paper introduces a random objective function that is tailored for hyperparameter tuning of vector-valued random features. The objective is minimized with ensemble Kalman inversion (EKI). EKI is a gradient-free particle-based optimizer that is scalable to high-dimensions and robust to randomness in objective functions. A numerical study showcases the new black-box methodology to learn hyperparameter distributions in several problems that are sensitive to the hyperparameter selection: two global sensitivity analyses, integrating a chaotic dynamical system, and solving a Bayesian inverse problem from atmospheric dynamics. The success of the proposed EKI-based algorithm for RFR suggests its potential for automated optimization of hyperparameters arising in other randomized algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00584v2</guid>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oliver R. A. Dunbar, Nicholas H. Nelsen, Maya Mutic</dc:creator>
    </item>
  </channel>
</rss>
