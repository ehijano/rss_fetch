<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Sep 2024 03:13:46 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Generative Aerodynamic Design with Diffusion Probabilistic Models</title>
      <link>https://arxiv.org/abs/2409.13328</link>
      <description>arXiv:2409.13328v1 Announce Type: new 
Abstract: The optimization of geometries for aerodynamic design often relies on a large number of expensive simulations to evaluate and iteratively improve the geometries. It is possible to reduce the number of simulations by providing a starting geometry that has properties close to the desired requirements, often in terms of lift and drag, aerodynamic moments and surface areas. We show that generative models have the potential to provide such starting geometries by generalizing geometries over a large dataset of simulations. In particular, we leverage diffusion probabilistic models trained on XFOIL simulations to synthesize two-dimensional airfoil geometries conditioned on given aerodynamic features and constraints. The airfoils are parameterized with Bernstein polynomials, ensuring smoothness of the generated designs. We show that the models are able to generate diverse candidate designs for identical requirements and constraints, effectively exploring the design space to provide multiple starting points to optimization procedures. However, the quality of the candidate designs depends on the distribution of the simulated designs in the dataset. Importantly, the geometries in this dataset must satisfy other requirements and constraints that are not used in conditioning of the diffusion model, to ensure that the generated geometries are physical.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13328v1</guid>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Wagenaar, Simone Mancini, Andr\'es Mateo-Gab\'in</dc:creator>
    </item>
    <item>
      <title>An adapted large language model facilitates multiple medical tasks in diabetes care</title>
      <link>https://arxiv.org/abs/2409.13191</link>
      <description>arXiv:2409.13191v1 Announce Type: cross 
Abstract: Diabetes is a chronic disease that poses a significant global health burden, and optimizing diabetes management requires multi-stakeholder collaboration. Large language models (LLMs) have shown promise in various healthcare scenarios, but their effectiveness across a diverse range of diabetes tasks remains unproven. In this study, we introduced a framework to train and validate diabetes-specific LLMs. We first developed a comprehensive data processing pipeline that includes data collection, filtering, augmentation and refinement. This approach contributes to creating a high-quality, diabetes-specific dataset, and several evaluation benchmarks entirely from scratch. Utilizing the collected training dataset, we fine-tuned a diabetes-specific LLM family that demonstrated state-of-the-art proficiency in understanding and processing various diabetes tasks compared to other LLMs. Furthermore, clinical studies showed the potential applications of our models in diabetes care, including providing personalized healthcare, assisting medical education, and streamlining clinical tasks. In conclusion, our study introduced a framework to develop and evaluate a diabetes-specific LLM family, and highlighted its potential to enhance clinical practice and provide personalized, data-driven support for diabetes support when facing different end users. The code is provided via GitHub at https://github.com/waltonfuture/Diabetica.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13191v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lai Wei, Zhen Ying, Muyang He, Yutong Chen, Qian Yang, Yanzhe Hong, Jiaping Lu, Xiaoying Li, Weiran Huang, Ying Chen</dc:creator>
    </item>
    <item>
      <title>Analysis of Gene Regulatory Networks from Gene Expression Using Graph Neural Networks</title>
      <link>https://arxiv.org/abs/2409.13664</link>
      <description>arXiv:2409.13664v1 Announce Type: cross 
Abstract: Unraveling the complexities of Gene Regulatory Networks (GRNs) is crucial for understanding cellular processes and disease mechanisms. Traditional computational methods often struggle with the dynamic nature of these networks. This study explores the use of Graph Neural Networks (GNNs), a powerful approach for modeling graph-structured data like GRNs. Utilizing a Graph Attention Network v2 (GATv2), our study presents a novel approach to the construction and interrogation of GRNs, informed by gene expression data and Boolean models derived from literature. The model's adeptness in accurately predicting regulatory interactions and pinpointing key regulators is attributed to advanced attention mechanisms, a hallmark of the GNN framework. These insights suggest that GNNs are primed to revolutionize GRN analysis, addressing traditional limitations and offering richer biological insights. The success of GNNs, as highlighted by our model's reliance on high-quality data, calls for enhanced data collection methods to sustain progress. The integration of GNNs in GRN research is set to pioneer developments in personalized medicine, drug discovery, and our grasp of biological systems, bolstered by the structural analysis of networks for improved node and edge prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13664v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>cs.SI</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hakan T. Otal, Abdulhamit Subasi, Furkan Kurt, M. Abdullah Canbaz, Yasin Uzun</dc:creator>
    </item>
    <item>
      <title>Data-driven methods for computational mechanics: A fair comparison between neural networks based and model-free approaches</title>
      <link>https://arxiv.org/abs/2409.06727</link>
      <description>arXiv:2409.06727v2 Announce Type: replace 
Abstract: We present a comparison between two approaches to modelling hyperelastic material behaviour using data. The first approach is a novel approach based on Data-driven Computational Mechanics (DDCM) that completely bypasses the definition of a material model by using only data from simulations or real-life experiments to perform computations. The second is a neural network (NN) based approach, where a neural network is used as a constitutive model. It is trained on data to learn the underlying material behaviour and is implemented in the same way as conventional models. The DDCM approach has been extended to include strategies for recovering isotropic behaviour and local smoothing of data. These have proven to be critical in certain cases and increase accuracy in most cases. The NN approach contains certain elements to enforce principles such as material symmetry, thermodynamic consistency, and convexity. In order to provide a fair comparison between the approaches, they use the same data and solve the same numerical problems with a selection of problems highlighting the advantages and disadvantages of each approach. Both the DDCM and the NNs have shown acceptable performance. The DDCM performed better when applied to cases similar to those from which the data is gathered from, albeit at the expense of generality, whereas NN models were more advantageous when applied to wider range of applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06727v2</guid>
      <category>cs.CE</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.cma.2024.117289</arxiv:DOI>
      <arxiv:journal_reference>Computer Methods in Applied Mechanics and Engineering (2024)</arxiv:journal_reference>
      <dc:creator>Martin Zlati\'c, Felipe Rocha, Laurent Stainier, Marko \v{C}ana{\dj}ija</dc:creator>
    </item>
    <item>
      <title>Investigation on domain adaptation of additive manufacturing monitoring systems to enhance digital twin reusability</title>
      <link>https://arxiv.org/abs/2409.12785</link>
      <description>arXiv:2409.12785v2 Announce Type: replace 
Abstract: Powder bed fusion (PBF) is an emerging metal additive manufacturing (AM) technology that enables rapid fabrication of complex geometries. However, defects such as pores and balling may occur and lead to structural unconformities, thus compromising the mechanical performance of the part. This has become a critical challenge for quality assurance as the nature of some defects is stochastic during the process and invisible from the exterior. To address this issue, digital twin (DT) using machine learning (ML)-based modeling can be deployed for AM process monitoring and control. Melt pool is one of the most commonly observed physical phenomena for process monitoring, usually by high-speed cameras. Once labeled and preprocessed, the melt pool images are used to train ML-based models for DT applications such as process anomaly detection and print quality evaluation. Nonetheless, the reusability of DTs is restricted due to the wide variability of AM settings, including AM machines and monitoring instruments. The performance of the ML models trained using the dataset collected from one setting is usually compromised when applied to other settings. This paper proposes a knowledge transfer pipeline between different AM settings to enhance the reusability of AM DTs. The source and target datasets are collected from the National Institute of Standards and Technology and National Cheng Kung University with different cameras, materials, AM machines, and process parameters. The proposed pipeline consists of four steps: data preprocessing, data augmentation, domain alignment, and decision alignment. Compared with the model trained only using the source dataset, this pipeline increased the melt pool anomaly detection accuracy by 31% without any labeled training data from the target dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12785v2</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jiarui Xie, Zhuo Yang, Chun-Chun Hu, Haw-Ching Yang, Yan Lu, Yaoyao Fiona Zhao</dc:creator>
    </item>
  </channel>
</rss>
