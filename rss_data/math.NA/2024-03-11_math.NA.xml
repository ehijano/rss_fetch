<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.NA</link>
    <description>math.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 11 Mar 2024 04:00:22 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 11 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Towards Robust Data-Driven Automated Recovery of Symbolic Conservation Laws from Limited Data</title>
      <link>https://arxiv.org/abs/2403.04889</link>
      <description>arXiv:2403.04889v1 Announce Type: new 
Abstract: Conservation laws are an inherent feature in many systems modeling real world phenomena, in particular, those modeling biological and chemical systems. If the form of the underlying dynamical system is known, linear algebra and algebraic geometry methods can be used to identify the conservation laws. Our work focuses on using data-driven methods to identify the conservation law(s) in the absence of the knowledge of system dynamics. Building in part upon the ideas proposed in [arXiv:1811.00961], we develop a robust data-driven computational framework that automates the process of identifying the number and type of the conservation law(s) while keeping the amount of required data to a minimum. We demonstrate that due to relative stability of singular vectors to noise we are able to reconstruct correct conservation laws without the need for excessive parameter tuning. While we focus primarily on biological examples, the framework proposed herein is suitable for a variety of data science applications and can be coupled with other machine learning approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04889v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tracey Oellerich, Maria Emelianenko</dc:creator>
    </item>
    <item>
      <title>Numerical solution of FDE-IVPs by using Fractional HBVMs: the fhbvm code</title>
      <link>https://arxiv.org/abs/2403.04916</link>
      <description>arXiv:2403.04916v1 Announce Type: new 
Abstract: In this paper we describe the efficient numerical implementation of Fractional HBVMs, a class of methods recently introduced for solving systems of fractional differential equations. The reported arguments are implemented in the Matlab code fhbvm, which is made available on the web. An extensive experimentation of the code is reported, to give evidence of its effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04916v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luigi Brugnano, Gianmarco Gurioli, Felice Iavernaro</dc:creator>
    </item>
    <item>
      <title>Tensor approximation of functional differential equations</title>
      <link>https://arxiv.org/abs/2403.04946</link>
      <description>arXiv:2403.04946v1 Announce Type: new 
Abstract: Functional Differential Equations (FDEs) play a fundamental role in many areas of mathematical physics, including fluid dynamics (Hopf characteristic functional equation), quantum field theory (Schwinger-Dyson equation), and statistical physics. Despite their significance, computing solutions to FDEs remains a longstanding challenge in mathematical physics. In this paper we address this challenge by introducing new approximation theory and high-performance computational algorithms designed for solving FDEs on tensor manifolds. Our approach involves approximating FDEs using high-dimensional partial differential equations (PDEs), and then solving such high-dimensional PDEs on a low-rank tensor manifold leveraging high-performance parallel tensor algorithms. The effectiveness of the proposed approach is demonstrated through its application to the Burgers-Hopf FDE, which governs the characteristic functional of the stochastic solution to the Burgers equation evolving from a random initial state.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04946v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abram Rodgers, Daniele Venturi</dc:creator>
    </item>
    <item>
      <title>Multirate Time-Integration based on Dynamic ODE Partitioning through Adaptively Refined Meshes for Compressible Fluid Dynamics</title>
      <link>https://arxiv.org/abs/2403.05144</link>
      <description>arXiv:2403.05144v1 Announce Type: new 
Abstract: In this paper, we apply the Paired-Explicit Runge-Kutta (P-ERK) schemes by Vermeire et. al. (2019, 2022) to dynamically partitioned systems arising from adaptive mesh refinement. The P-ERK schemes enable multirate time-integration with no changes in the spatial discretization methodology, making them readily implementable in existing codes that employ a method-of-lines approach.
  We show that speedup compared to a range of state of the art Runge-Kutta methods can be realized, despite additional overhead due to the dynamic re-assignment of flagging variables and restricting nonlinear stability properties. The effectiveness of the approach is demonstrated for a range of simulation setups for viscous and inviscid convection-dominated compressible flows for which we provide a reproducibility repository.
  In addition, we perform a thorough investigation of the nonlinear stability properties of the Paired-Explicit Runge-Kutta schemes regarding limitations due to the violation of monotonicity properties of the underlying spatial discretization. Furthermore, we present a novel approach for estimating the relevant eigenvalues of large Jacobians required for the optimization of stability polynomials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05144v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Doehring, Michael Schlottke-Lakemper, Gregor J. Gassner, Manuel Torrilhon</dc:creator>
    </item>
    <item>
      <title>A fully discretization, unconditionally energy stable finite element method solving the thermodynamically consistent diffuse interface model for incompressible two-phase MHD flows with large density ratios</title>
      <link>https://arxiv.org/abs/2403.05200</link>
      <description>arXiv:2403.05200v1 Announce Type: new 
Abstract: A diffusion interface two-phase magnetohydrodynamic model has been used for matched densities in our previous work [1,2], which may limit the applications of the model. In this work, we derive a thermodynamically consistent diffuse interface model for diffusion interface two-phase magnetohydrodynamic fluids with large density ratios by Onsager's variational principle and conservation law for the first time. The finite element method for spatial discretization and the first order semi-implicit scheme linked with convect splitting method for temporal discretization, is proposed to solve this new model. The mass conservation, unconditionally energy stability and convergence of the scheme can be proved. Then we derive the existence of weak solutions of governing system employing the above properties of the scheme and compactness method. Finally, we show some numerical results to test the effectiveness and well behavior of proposed scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05200v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ke Zhang</dc:creator>
    </item>
    <item>
      <title>A note on the singular value decomposition of idempotent and involutory matrices</title>
      <link>https://arxiv.org/abs/2403.05214</link>
      <description>arXiv:2403.05214v1 Announce Type: new 
Abstract: It is known that singular values of idempotent matrices are either zero or larger or equal to one \cite{HouC63}. We state exactly how many singular values greater than one, equal to one, and equal to zero there are. Moreover, we derive a singular value decomposition of idempotent matrices which reveals a tight relationship between its left and right singular vectors. The same idea is used to augment a discovery regarding the singular values of involutory matrices as presented in \cite{FasH20}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05214v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Heike Fa{\ss}bender, Martin Halwa{\ss}</dc:creator>
    </item>
    <item>
      <title>A one-dimensional model for aspiration therapy in blood vessels</title>
      <link>https://arxiv.org/abs/2403.05494</link>
      <description>arXiv:2403.05494v1 Announce Type: new 
Abstract: Aspiration thrombectomy is a treatment option for ischemic stroke due to occlusions in large vessels. During the therapy a device is inserted into the vessel and suction is applied. A new one-dimensional model is introduced that is capable of simulating this procedure while accounting for the fluid-structure interactions in blood flow. To solve the coupling problem at the tip of the device a problem-suited Riemann solver is constructed based on relaxation of the hyperbolic model. Numerical experiments investigating the role of the catheter size and the suction forces are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05494v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Michael Herty, Niklas Kolbe, Michael Neidlin</dc:creator>
    </item>
    <item>
      <title>Nodal finite element approximation of peridynamics</title>
      <link>https://arxiv.org/abs/2403.05501</link>
      <description>arXiv:2403.05501v1 Announce Type: new 
Abstract: This work considers the nodal finite element approximation of peridynamics, in which the nodal displacements satisfy the peridynamics equation at each mesh node. For the nonlinear bond-based peridynamics model, it is shown that, under the suitable assumptions on an exact solution, the discretized solution associated with the central-in-time and nodal finite element discretization converges to the exact solution in $L^2$ norm at the rate $C_1 \Delta t + C_2 h^2/\epsilon^2$. Here, $\Delta t$, $h$, and $\epsilon$ are time step size, mesh size, and the size of the horizon or nonlocal length scale, respectively. Constants $C_1$ and $C_2$ are independent of $h$ and $\Delta t$ and depend on the norms of the exact solution. Several numerical examples involving pre-crack, void, and notch are considered, and the efficacy of the proposed nodal finite element discretization is analyzed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05501v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Prashant K. Jha, Patrick Diehl, Robert Lipton</dc:creator>
    </item>
    <item>
      <title>Gradient-free neural topology optimization</title>
      <link>https://arxiv.org/abs/2403.04937</link>
      <description>arXiv:2403.04937v1 Announce Type: cross 
Abstract: Gradient-free optimizers allow for tackling problems regardless of the smoothness or differentiability of their objective function, but they require many more iterations to converge when compared to gradient-based algorithms. This has made them unviable for topology optimization due to the high computational cost per iteration and high dimensionality of these problems. We propose a pre-trained neural reparameterization strategy that leads to at least one order of magnitude decrease in iteration count when optimizing the designs in latent space, as opposed to the conventional approach without latent reparameterization. We demonstrate this via extensive computational experiments in- and out-of-distribution with the training data. Although gradient-based topology optimization is still more efficient for differentiable problems, such as compliance optimization of structures, we believe this work will open up a new path for problems where gradient information is not readily available (e.g. fracture).</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04937v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gawel Kus, Miguel A. Bessa</dc:creator>
    </item>
    <item>
      <title>Greedy feature selection: Classifier-dependent feature selection via greedy methods</title>
      <link>https://arxiv.org/abs/2403.05138</link>
      <description>arXiv:2403.05138v1 Announce Type: cross 
Abstract: The purpose of this study is to introduce a new approach to feature ranking for classification tasks, called in what follows greedy feature selection. In statistical learning, feature selection is usually realized by means of methods that are independent of the classifier applied to perform the prediction using that reduced number of features. Instead, greedy feature selection identifies the most important feature at each step and according to the selected classifier. In the paper, the benefits of such scheme are investigated theoretically in terms of model capacity indicators, such as the Vapnik-Chervonenkis (VC) dimension or the kernel alignment, and tested numerically by considering its application to the problem of predicting geo-effective manifestations of the active Sun.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05138v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabiana Camattari, Sabrina Guastavino, Francesco Marchetti, Michele Piana, Emma Perracchione</dc:creator>
    </item>
    <item>
      <title>Numerical simulations of a stochastic dynamics leading to cascades and loss of regularity: applications to fluid turbulence and generation of fractional Gaussian fields</title>
      <link>https://arxiv.org/abs/2403.05401</link>
      <description>arXiv:2403.05401v1 Announce Type: cross 
Abstract: Motivated by the modeling of the spatial structure of the velocity field of three-dimensional turbulent flows, and the phenomenology of cascade phenomena, a linear dynamics has been recently proposed able to generate high velocity gradients from a smooth-in-space forcing term. It is based on a linear Partial Differential Equation (PDE) stirred by an additive random forcing term which is delta-correlated in time. The underlying proposed deterministic mechanism corresponds to a transport in Fourier space which aims at transferring energy injected at large scales towards small scales. The key role of the random forcing is to realize these transfers in a statistically homogeneous way. Whereas at finite times and positive viscosity the solutions are smooth, a loss of regularity is observed for the statistically stationary state in the inviscid limit. We here present novel simulations, based on finite volume methods in the Fourier domain and a splitting method in time, which are more accurate than the pseudo-spectral simulations. We show that the novel algorithm is able to reproduce accurately the expected local and statistical structure of the predicted solutions. We conduct numerical simulations in one, two and three spatial dimensions, and we display the solutions both in physical and Fourier spaces. We additionally display key statistical quantities such as second-order structure functions and power spectral densities at various viscosities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05401v1</guid>
      <category>physics.flu-dyn</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Geoffrey Beck, Charles-Edouard Br\'ehier, Laurent Chevillard, Ricardo Grande, Wandrille Ruffenach</dc:creator>
    </item>
    <item>
      <title>The dependency of spectral gaps on the convergence of the inverse iteration for a nonlinear eigenvector problem</title>
      <link>https://arxiv.org/abs/2202.07593</link>
      <description>arXiv:2202.07593v4 Announce Type: replace 
Abstract: In this paper we consider the generalized inverse iteration for computing ground states of the Gross-Pitaevskii eigenvector problem (GPE). For that we prove explicit linear convergence rates that depend on the maximum eigenvalue in magnitude of a weighted linear eigenvalue problem. Furthermore, we show that this eigenvalue can be bounded by the first spectral gap of a linearized Gross-Pitaevskii operator, recovering the same rates as for linear eigenvector problems. With this we establish the first local convergence result for the basic inverse iteration for the GPE without damping. We also show how our findings directly generalize to extended inverse iterations, such as the Gradient Flow Discrete Normalized (GFDN) proposed in [W. Bao, Q. Du, SIAM J. Sci. Comput., 25 (2004)] or the damped inverse iteration suggested in [P. Henning, D. Peterseim, SIAM J. Numer. Anal., 53 (2020)]. Our analysis also reveals why the inverse iteration for the GPE does not react favourably to spectral shifts. This empirical observation can now be explained with a blow-up of a weighting function that crucially contributes to the convergence rates. Our findings are illustrated by numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.07593v4</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patrick Henning</dc:creator>
    </item>
    <item>
      <title>Greedy Recombination Interpolation Method (GRIM)</title>
      <link>https://arxiv.org/abs/2205.07495</link>
      <description>arXiv:2205.07495v5 Announce Type: replace 
Abstract: In this paper we develop the Greedy Recombination Interpolation Method (GRIM) for finding sparse approximations of functions initially given as linear combinations of some (large) number of simpler functions. In a similar spirit to the CoSaMP algorithm, GRIM combines dynamic growth-based interpolation techniques and thinning-based reduction techniques. The dynamic growth-based aspect is a modification of the greedy growth utilised in the Generalised Empirical Interpolation Method (GEIM). A consequence of the modification is that our growth is not restricted to being one-per-step as it is in GEIM. The thinning-based aspect is carried out by recombination, which is the crucial component of the recent ground-breaking convex kernel quadrature method. GRIM provides the first use of recombination outside the setting of reducing the support of a measure. The sparsity of the approximation found by GRIM is controlled by the geometric concentration of the data in a sense that is related to a particular packing number of the data. We apply GRIM to a kernel quadrature task for the radial basis function kernel, and verify that its performance matches that of other contemporary kernel quadrature techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.07495v5</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Terry Lyons, Andrew D. McLeod</dc:creator>
    </item>
    <item>
      <title>Cooperative data-driven modeling</title>
      <link>https://arxiv.org/abs/2211.12971</link>
      <description>arXiv:2211.12971v2 Announce Type: replace 
Abstract: Data-driven modeling in mechanics is evolving rapidly based on recent machine learning advances, especially on artificial neural networks. As the field matures, new data and models created by different groups become available, opening possibilities for cooperative modeling. However, artificial neural networks suffer from catastrophic forgetting, i.e. they forget how to perform an old task when trained on a new one. This hinders cooperation because adapting an existing model for a new task affects the performance on a previous task trained by someone else. The authors developed a continual learning method that addresses this issue, applying it here for the first time to solid mechanics. In particular, the method is applied to recurrent neural networks to predict history-dependent plasticity behavior, although it can be used on any other architecture (feedforward, convolutional, etc.) and to predict other phenomena. This work intends to spawn future developments on continual learning that will foster cooperative strategies among the mechanics community to solve increasingly challenging problems. We show that the chosen continual learning strategy can sequentially learn several constitutive laws without forgetting them, using less data to achieve the same error as standard (non-cooperative) training of one law per model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.12971v2</guid>
      <category>math.NA</category>
      <category>cond-mat.mtrl-sci</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.cma.2023.116432</arxiv:DOI>
      <arxiv:journal_reference>Computer Methods in Applied Mechanics and Engineering, 417, 116432 (2023)</arxiv:journal_reference>
      <dc:creator>Aleksandr Dekhovich, O. Taylan Turan, Jiaxiang Yi, Miguel A. Bessa</dc:creator>
    </item>
    <item>
      <title>A Bayesian Approach to Modeling Finite Element Discretization Error</title>
      <link>https://arxiv.org/abs/2306.05993</link>
      <description>arXiv:2306.05993v2 Announce Type: replace 
Abstract: In this work, the uncertainty associated with the finite element discretization error is modeled following the Bayesian paradigm. First, a continuous formulation is derived, where a Gaussian process prior over the solution space is updated based on observations from a finite element discretization. To avoid the computation of intractable integrals, a second, finer, discretization is introduced that is assumed sufficiently dense to represent the true solution field. A prior distribution is assumed over the fine discretization, which is then updated based on observations from the coarse discretization. This yields a posterior distribution with a mean that serves as an estimate of the solution, and a covariance that models the uncertainty associated with this estimate. Two particular choices of prior are investigated: a prior defined implicitly by assigning a white noise distribution to the right-hand side term, and a prior whose covariance function is equal to the Green's function of the partial differential equation. The former yields a posterior distribution with a mean close to the reference solution, but a covariance that contains little information regarding the finite element discretization error. The latter, on the other hand, yields posterior distribution with a mean equal to the coarse finite element solution, and a covariance with a close connection to the discretization error. For both choices of prior a contradiction arises, since the discretization error depends on the right-hand side term, but the posterior covariance does not. We demonstrate how, by rescaling the eigenvalues of the posterior covariance, this independence can be avoided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.05993v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anne Poot, Pierre Kerfriden, Iuri Rocha, Frans van der Meer</dc:creator>
    </item>
    <item>
      <title>Implicit Boundary Conditions in Partial Differential Equations Discretizations: Identifying Spurious Modes and Model Reduction</title>
      <link>https://arxiv.org/abs/2306.15802</link>
      <description>arXiv:2306.15802v2 Announce Type: replace 
Abstract: We revisit the problem of spurious modes that are sometimes encountered in partial differential equations discretizations. It is generally suspected that one of the causes for spurious modes is due to how boundary conditions are treated, and we use this as the starting point of our investigations. By regarding boundary conditions as algebraic constraints on a differential equation, we point out that any differential equation with homogeneous boundary conditions also admits a typically infinite number of hidden or implicit boundary conditions. In most discretization schemes, these additional implicit boundary conditions are violated, and we argue that this is what leads to the emergence of spurious modes. These observations motivate two definitions of the quality of computed eigenvalues based on violations of derivatives of boundary conditions on the one hand, and on the Grassmann distance between subspaces associated with computed eigenspaces on the other. Both of these tests are based on a standardized treatment of boundary conditions and do not require a priori knowledge of eigenvalue locations. The effectiveness of these tests is demonstrated on several examples known to have spurious modes. In addition, these quality tests show that in most problems, about half the computed spectrum of a differential operator is of low quality. The tests also specifically identify the low accuracy modes, which can then be projected out as a type of model reduction scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15802v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jcp.2023.112743</arxiv:DOI>
      <arxiv:journal_reference>J. Comput. Phys. 500 (2024)</arxiv:journal_reference>
      <dc:creator>Pascal R Karam, Bassam Bamieh</dc:creator>
    </item>
    <item>
      <title>Localized implicit time stepping for the wave equation</title>
      <link>https://arxiv.org/abs/2306.17056</link>
      <description>arXiv:2306.17056v2 Announce Type: replace 
Abstract: This work proposes a discretization of the acoustic wave equation with possibly oscillatory coefficients based on a superposition of discrete solutions to spatially localized subproblems computed with an implicit time discretization. Based on exponentially decaying entries of the global system matrices and an appropriate partition of unity, it is proved that the superposition of localized solutions is appropriately close to the solution of the (global) implicit scheme. It is thereby justified that the localized (and especially parallel) computation on multiple overlapping subdomains is reasonable. Moreover, a re-start is introduced after a certain amount of time steps to maintain a moderate overlap of the subdomains. Overall, the approach may be understood as a domain decomposition strategy in space on successive short time intervals that completely avoids inner iterations. Numerical examples are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.17056v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dietmar Gallistl, Roland Maier</dc:creator>
    </item>
    <item>
      <title>Modified memoryless spectral-scaling Broyden family on Riemannian manifolds</title>
      <link>https://arxiv.org/abs/2307.08986</link>
      <description>arXiv:2307.08986v2 Announce Type: replace 
Abstract: This paper presents modified memoryless quasi-Newton methods based on the spectral-scaling Broyden family on Riemannian manifolds. The method involves adding one parameter to the search direction of the memoryless self-scaling Broyden family on the manifold. Moreover, it uses a general map instead of vector transport. This idea has already been proposed within a general framework of Riemannian conjugate gradient methods where one can use vector transport, scaled vector transport, or an inverse retraction. We show that the search direction satisfies the sufficient descent condition under some assumptions on the parameters. In addition, we show global convergence of the proposed method under the Wolfe conditions. We numerically compare it with existing methods, including Riemannian conjugate gradient methods and the memoryless spectral-scaling Broyden family. The numerical results indicate that the proposed method with the BFGS formula is suitable for solving an off-diagonal cost function minimization problem on an oblique manifold.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.08986v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hiroyuki Sakai, Hideaki Iiduka</dc:creator>
    </item>
    <item>
      <title>A Riemannian optimization method to compute the nearest singular pencil</title>
      <link>https://arxiv.org/abs/2308.12781</link>
      <description>arXiv:2308.12781v2 Announce Type: replace 
Abstract: Given a square pencil $A+ \lambda B$, where $A$ and $B$ are $n\times n$ complex (resp. real) matrices, we consider the problem of finding the singular complex (resp. real) pencil nearest to it in the Frobenius distance. This problem is known to be very difficult, and the few algorithms available in the literature can only deal efficiently with pencils of very small size. We show that the problem is equivalent to minimizing a certain objective function $f$ over the Riemannian manifold $SU(n) \times SU(n)$ (resp. $SO(n) \times SO(n)$ if the nearest real singular pencil is sought), where $SU(n)$ denotes the special unitary group (resp. $SO(n)$ denotes the special orthogonal group). This novel perspective is based on the generalized Schur form of pencils, and yields competitive numerical methods, by pairing it with { algorithms} capable of doing optimization on { Riemannian manifolds. We propose one algorithm that directly minimizes the (almost everywhere, but not everywhere, differentiable) function $f$, as well as a smoothed alternative and a third algorithm that is smooth and can also solve the problem} of finding a nearest singular pencil with a specified minimal index. We provide numerical experiments that show that the resulting methods allow us to deal with pencils of much larger size than alternative techniques, yielding candidate minimizers of comparable or better quality. In the course of our analysis, we also obtain a number of new theoretical results related to the generalized Schur form of a (regular or singular) square pencil and to the minimal index of a singular square pencil whose nullity is $1$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.12781v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Froil\'an Dopico, Vanni Noferini, Lauri Nyman</dc:creator>
    </item>
    <item>
      <title>From low-rank retractions to dynamical low-rank approximation and back</title>
      <link>https://arxiv.org/abs/2309.06125</link>
      <description>arXiv:2309.06125v2 Announce Type: replace 
Abstract: In algorithms for solving optimization problems constrained to a smooth manifold, retractions are a well-established tool to ensure that the iterates stay on the manifold. More recently, it has been demonstrated that retractions are a useful concept for other computational tasks on manifold as well, including interpolation tasks. In this work, we consider the application of retractions to the numerical integration of differential equations on fixed-rank matrix manifolds. This is closely related to dynamical low-rank approximation (DLRA) techniques. In fact, any retraction leads to a numerical integrator and, vice versa, certain DLRA techniques bear a direct relation with retractions. As an example for the latter, we introduce a new retraction, called KLS retraction, that is derived from the so-called unconventional integrator for DLRA. We also illustrate how retractions can be used to recover known DLRA techniques and to design new ones. In particular, this work introduces two novel numerical integration schemes that apply to differential equations on general manifolds: the accelerated forward Euler (AFE) method and the Projected Ralston--Hermite (PRH) method. Both methods build on retractions by using them as a tool for approximating curves on manifolds. The two methods are proven to have local truncation error of order three. Numerical experiments on classical DLRA examples highlight the advantages and shortcomings of these new methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.06125v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Axel S\'eguin, Gianluca Ceruti, Daniel Kressner</dc:creator>
    </item>
    <item>
      <title>A Unified Framework for the Error Analysis of Physics-Informed Neural Networks</title>
      <link>https://arxiv.org/abs/2311.00529</link>
      <description>arXiv:2311.00529v2 Announce Type: replace 
Abstract: We prove a priori and a posteriori error estimates for physics-informed neural networks (PINNs) for linear PDEs. We analyze elliptic equations in primal and mixed form, elasticity, parabolic, hyperbolic and Stokes equations; and a PDE constrained optimization problem. For the analysis, we propose an abstract framework in the common language of bilinear forms, and we show that coercivity and continuity lead to error estimates. The obtained estimates are sharp and reveal that the $L^2$ penalty approach for initial and boundary conditions in the PINN formulation weakens the norm of the error decay. Finally, utilizing recent advances in PINN optimization, we present numerical examples that illustrate the ability of the method to achieve accurate solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.00529v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marius Zeinhofer, Rami Masri, Kent-Andr\'e Mardal</dc:creator>
    </item>
    <item>
      <title>Improved Convergence Rates of Windowed Anderson Acceleration for Symmetric Fixed-Point Iterations</title>
      <link>https://arxiv.org/abs/2311.02490</link>
      <description>arXiv:2311.02490v2 Announce Type: replace 
Abstract: This paper studies the commonly utilized windowed Anderson acceleration (AA) algorithm for fixed-point methods, $x^{(k+1)}=q(x^{(k)})$. It provides the first proof that when the operator $q$ is linear and symmetric the windowed AA, which uses a sliding window of prior iterates, improves the root-linear convergence factor over the fixed-point iterations. When $q$ is nonlinear, yet has a symmetric Jacobian at a fixed point, a slightly modified AA algorithm is proved to have an analogous root-linear convergence factor improvement over fixed-point iterations. Simulations verify our observations. Furthermore, experiments with different data models demonstrate AA is significantly superior to the standard fixed-point methods for Tyler's M-estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.02490v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Casey Garner, Gilad Lerman, Teng Zhang</dc:creator>
    </item>
    <item>
      <title>Eigenmatrix for unstructured sparse recovery</title>
      <link>https://arxiv.org/abs/2311.16609</link>
      <description>arXiv:2311.16609v4 Announce Type: replace 
Abstract: This note considers the unstructured sparse recovery problems in a general form. Examples include rational approximation, spectral function estimation, Fourier inversion, Laplace inversion, and sparse deconvolution. The main challenges are the noise in the sample values and the unstructured nature of the sample locations. This note proposes the eigenmatrix, a data-driven construction with desired approximate eigenvalues and eigenvectors. The eigenmatrix offers a new way for these sparse recovery problems. Numerical results are provided to demonstrate the efficiency of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.16609v4</guid>
      <category>math.NA</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lexing Ying</dc:creator>
    </item>
    <item>
      <title>Adaptive multiplication of $\mathcal{H}^2$-matrices with block-relative error control</title>
      <link>https://arxiv.org/abs/2403.01566</link>
      <description>arXiv:2403.01566v2 Announce Type: replace 
Abstract: The discretization of non-local operators, e.g., solution operators of partial differential equations or integral operators, leads to large densely populated matrices. $\mathcal{H}^2$-matrices take advantage of local low-rank structures in these matrices to provide an efficient data-sparse approximation that allows us to handle large matrices efficiently, e.g., to reduce the storage requirements to $\mathcal{O}(n k)$ for $n$-dimensional matrices with local rank $k$, and to reduce the complexity of the matrix-vector multiplication to $\mathcal{O}(n k)$ operations.
  In order to perform more advanced operations, e.g., to construct efficient preconditioners or evaluate matrix functions, we require algorithms that take $\mathcal{H}^2$-matrices as input and approximate the result again by $\mathcal{H}^2$-matrices, ideally with controllable accuracy. In this manuscript, we introduce an algorithm that approximates the product of two $\mathcal{H}^2$-matrices and guarantees block-relative error estimates for the submatrices of the result. It uses specialized tree structures to represent the exact product in an intermediate step, thereby allowing us to apply mathematically rigorous error control strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01566v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Steffen B\"orm</dc:creator>
    </item>
    <item>
      <title>Heuristic Approaches to Obtain Low-Discrepancy Point Sets via Subset Selection</title>
      <link>https://arxiv.org/abs/2306.15276</link>
      <description>arXiv:2306.15276v2 Announce Type: replace-cross 
Abstract: Building upon the exact methods presented in our earlier work [J. Complexity, 2022], we introduce a heuristic approach for the star discrepancy subset selection problem. The heuristic gradually improves the current-best subset by replacing one of its elements at a time. While we prove that the heuristic does not necessarily return an optimal solution, we obtain very promising results for all tested dimensions. For example, for moderate point set sizes $30 \leq n \leq 240$ in dimension 6, we obtain point sets with $L_{\infty}$ star discrepancy up to 35% better than that of the first $n$ points of the Sobol' sequence. Our heuristic works in all dimensions, the main limitation being the precision of the discrepancy calculation algorithms.
  We also provide a comparison with a recent energy functional introduced by Steinerberger [J. Complexity, 2019], showing that our heuristic performs better on all tested instances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15276v2</guid>
      <category>cs.CG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fran\c{c}ois Cl\'ement, Carola Doerr, Lu\'is Paquete</dc:creator>
    </item>
    <item>
      <title>The Moving Discontinuous Galerkin Method with Interface Condition Enforcement for the Simulation of Hypersonic, Viscous Flows</title>
      <link>https://arxiv.org/abs/2311.00701</link>
      <description>arXiv:2311.00701v2 Announce Type: replace-cross 
Abstract: The moving discontinuous Galerkin method with interface condition enforcement (MDG-ICE) is a high-order, r-adaptive method that treats the grid as a variable and weakly enforces the conservation law, constitutive law, and corresponding interface conditions in order to implicitly fit high-gradient flow features. In this paper, we develop an optimization solver based on the Levenberg-Marquardt algorithm that features an anisotropic, locally adaptive penalty method to enhance robustness and prevent cell degeneration in the computation of hypersonic, viscous flows. Specifically, we incorporate an anisotropic grid regularization based on the mesh-implied metric that inhibits grid motion in directions with small element length scales, an element shape regularization that inhibits nonlinear deformations of the high-order elements, and a penalty regularization that penalizes degenerate elements. Additionally, we introduce a procedure for locally scaling the regularization operators in an adaptive, elementwise manner in order to maintain grid validity. We apply the proposed MDG-ICE formulation to two- and three-dimensional test cases involving viscous shocks and/or boundary layers, including Mach 17.6 hypersonic viscous flow over a circular cylinder and Mach 5 hypersonic viscous flow over a sphere, which are very challenging test cases for conventional numerical schemes on simplicial grids. Even without artificial dissipation, the computed solutions are free from spurious oscillations and yield highly symmetric surface heat-flux profiles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.00701v2</guid>
      <category>physics.flu-dyn</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Eric J. Ching, Andrew D. Kercher, Andrew Corrigan</dc:creator>
    </item>
    <item>
      <title>DPOT: Auto-Regressive Denoising Operator Transformer for Large-Scale PDE Pre-Training</title>
      <link>https://arxiv.org/abs/2403.03542</link>
      <description>arXiv:2403.03542v3 Announce Type: replace-cross 
Abstract: Pre-training has been investigated to improve the efficiency and performance of training neural operators in data-scarce settings. However, it is largely in its infancy due to the inherent complexity and diversity, such as long trajectories, multiple scales and varying dimensions of partial differential equations (PDEs) data. In this paper, we present a new auto-regressive denoising pre-training strategy, which allows for more stable and efficient pre-training on PDE data and generalizes to various downstream tasks. Moreover, by designing a flexible and scalable model architecture based on Fourier attention, we can easily scale up the model for large-scale pre-training. We train our PDE foundation model with up to 0.5B parameters on 10+ PDE datasets with more than 100k trajectories. Extensive experiments show that we achieve SOTA on these benchmarks and validate the strong generalizability of our model to significantly enhance performance on diverse downstream PDE tasks like 3D data. Code is available at \url{https://github.com/thu-ml/DPOT}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03542v3</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhongkai Hao, Chang Su, Songming Liu, Julius Berner, Chengyang Ying, Hang Su, Anima Anandkumar, Jian Song, Jun Zhu</dc:creator>
    </item>
  </channel>
</rss>
