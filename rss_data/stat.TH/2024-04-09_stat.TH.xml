<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 10 Apr 2024 04:02:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 10 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Centrality Estimators for Probability Density Functions</title>
      <link>https://arxiv.org/abs/2404.05816</link>
      <description>arXiv:2404.05816v1 Announce Type: new 
Abstract: In this report, we explore the data selection leading to a family of estimators maximizing a centrality. The family allows a nice properties leading to accurate and robust probability density function fitting according to some criteria we define. We establish a link between the centrality estimator and the maximum likelihood, showing that the latter is a particular case. Therefore, a new probability interpretation of Fisher maximum likelihood is provided. We will introduce and study two specific centralities that we have named H\"older and Lehmer estimators. A numerical simulation is provided showing the effectiveness of the proposed families of estimators opening the door to development of new concepts and algorithms in machine learning, data mining, statistics, and data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05816v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Djemel Ziou</dc:creator>
    </item>
    <item>
      <title>Integrated empirical measures and generalizations of classical goodness-of-fit statistics</title>
      <link>https://arxiv.org/abs/2404.06040</link>
      <description>arXiv:2404.06040v1 Announce Type: new 
Abstract: Based on $m$-fold integrated empirical measures, we study three new classes of goodness-of-fits tests, generalizing Anderson-Darling, Cram\'er-von Mises, and Watson statistics, respectively, and examine the corresponding limiting stochastic processes. The limiting null distributions of the statistics all lead to explicitly solvable cases with closed-form expressions for the corresponding Karhunen-Lo\`{e}ve expansions and covariance kernels. In particular, the eigenvalues are shown to be $\frac1{k(k+1)\cdots (k+2m-1)}$ for the generalized Anderson-Darling, $\frac1{(\pi k)^{2m}}$ for the generalized Cram\'er-von Mises, and $\frac1{2\pi\lceil k/2\rceil^{2m}}$ for the generalized Watson statistics, respectively. The infinite products of the resulting moment generating functions are further simplified to finite ones so as to facilitate efficient numerical calculations. These statistics are capable of detecting different features of the distributions and thus provide a useful toolbox for goodness-of-fit testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06040v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hsien-Kuei Hwang, Satoshi Kuriki</dc:creator>
    </item>
    <item>
      <title>Further Understanding of a Local Gaussian Process Approximation: Characterising Convergence in the Finite Regime</title>
      <link>https://arxiv.org/abs/2404.06200</link>
      <description>arXiv:2404.06200v1 Announce Type: new 
Abstract: We show that common choices of kernel functions for a highly accurate and massively scalable nearest-neighbour based GP regression model (GPnn: \cite{GPnn}) exhibit gradual convergence to asymptotic behaviour as dataset-size $n$ increases. For isotropic kernels such as Mat\'{e}rn and squared-exponential, an upper bound on the predictive MSE can be obtained as $O(n^{-\frac{p}{d}})$ for input dimension $d$, $p$ dictated by the kernel (and $d&gt;p$) and fixed number of nearest-neighbours $m$ with minimal assumptions on the input distribution. Similar bounds can be found under model misspecification and combined to give overall rates of convergence of both MSE and an important calibration metric. We show that lower bounds on $n$ can be given in terms of $m$, $l$, $p$, $d$, a tolerance $\varepsilon$ and a probability $\delta$. When $m$ is chosen to be $O(n^{\frac{p}{p+d}})$ minimax optimal rates of convergence are attained. Finally, we demonstrate empirical performance and show that in many cases convergence occurs faster than the upper bounds given here.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06200v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anthony Stephenson, Robert Allison, Edward Pyzer-Knapp</dc:creator>
    </item>
    <item>
      <title>Least Squares-Based Permutation Tests in Time Series</title>
      <link>https://arxiv.org/abs/2404.06238</link>
      <description>arXiv:2404.06238v1 Announce Type: new 
Abstract: This paper studies permutation tests for regression parameters in a time series setting, where the time series is assumed stationary but may exhibit an arbitrary (but weak) dependence structure. In such a setting, it is perhaps surprising that permutation tests can offer any type of inference guarantees, since permuting of covariates can destroy its relationship with the response. Indeed, the fundamental assumption of exchangeability of errors required for the finite-sample exactness of permutation tests, can easily fail. However, we show that permutation tests may be constructed which are asymptotically valid for a wide class of stationary processes, but remain exact when exchangeability holds. We also consider the problem of testing for no monotone trend and we construct asymptotically valid permutation tests in this setting as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06238v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joseph P. Romano, Marius A. Tirlea</dc:creator>
    </item>
    <item>
      <title>Permutation Testing for Monotone Trend</title>
      <link>https://arxiv.org/abs/2404.06239</link>
      <description>arXiv:2404.06239v1 Announce Type: new 
Abstract: In this paper, we consider the fundamental problem of testing for monotone trend in a time series. While the term "trend" is commonly used and has an intuitive meaning, it is first crucial to specify its exact meaning in a hypothesis testing context. A commonly used well-known test is the Mann-Kendall test, which we show does not offer Type 1 error control even in large samples. On the other hand, by an appropriate studentization of the Mann-Kendall statistic, we construct permutation tests that offer asymptotic error control quite generally, but retain the exactness property of permutation tests for i.i.d. observations. We also introduce "local" Mann-Kendall statistics as a means of testing for local rather than global trend in a time series. Similar properties of permutation tests are obtained for these tests as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06239v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joseph P. Romano, Marius A. Tirlea</dc:creator>
    </item>
    <item>
      <title>New variances for various kappa coefficients based on the unbiased estimator of the expected index of agreements</title>
      <link>https://arxiv.org/abs/2404.06295</link>
      <description>arXiv:2404.06295v1 Announce Type: new 
Abstract: Recently Mart\'in Andr\'es and \'Alvarez Hern\'andez (2024) have proposed new estimators of various kappa coefficients. These estimators are based on the unbiased estimator of the expected index of agreement of each population coefficient. In their article, these authors propose variance formulas based on the univariate delta method. Here new formulas are proposed that are based on the multivariate delta method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06295v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Antonio Mart\'in Andr\'es, Mar\'ia \'Alvarez Hern\'andez</dc:creator>
    </item>
    <item>
      <title>Hoeffding and Bernstein inequalities for weighted sums of exchangeable random variables</title>
      <link>https://arxiv.org/abs/2404.06457</link>
      <description>arXiv:2404.06457v1 Announce Type: new 
Abstract: The aim of this paper is to establish Hoeffding and Bernstein type concentration inequalities for weighted sums of exchangeable random variables. A special case is the i.i.d. setting, where random variables are sampled independently from some distribution (and are therefore exchangeable). In contrast to the existing literature on this problem, our results provide a natural unified view of both the i.i.d. and the exchangeable setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06457v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rina Foygel Barber</dc:creator>
    </item>
    <item>
      <title>Just Wing It: Optimal Estimation of Missing Mass in a Markovian Sequence</title>
      <link>https://arxiv.org/abs/2404.05819</link>
      <description>arXiv:2404.05819v1 Announce Type: cross 
Abstract: We study the problem of estimating the stationary mass -- also called the unigram mass -- that is missing from a single trajectory of a discrete-time, ergodic Markov chain. This problem has several applications -- for example, estimating the stationary missing mass is critical for accurately smoothing probability estimates in sequence models. While the classical Good--Turing estimator from the 1950s has appealing properties for i.i.d. data, it is known to be biased in the Markov setting, and other heuristic estimators do not come equipped with guarantees. Operating in the general setting in which the size of the state space may be much larger than the length $n$ of the trajectory, we develop a linear-runtime estimator called \emph{Windowed Good--Turing} (\textsc{WingIt}) and show that its risk decays as $\widetilde{\mathcal{O}}(\mathsf{T_{mix}}/n)$, where $\mathsf{T_{mix}}$ denotes the mixing time of the chain in total variation distance. Notably, this rate is independent of the size of the state space and minimax-optimal up to a logarithmic factor in $n / \mathsf{T_{mix}}$. We also present a bound on the variance of the missing mass random variable, which may be of independent interest. We extend our estimator to approximate the stationary mass placed on elements occurring with small frequency in $X^n$. Finally, we demonstrate the efficacy of our estimators both in simulations on canonical chains and on sequences constructed from a popular natural language corpus.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05819v1</guid>
      <category>stat.ML</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ashwin Pananjady, Vidya Muthukumar, Andrew Thangaraj</dc:creator>
    </item>
    <item>
      <title>Beyond the delta method</title>
      <link>https://arxiv.org/abs/2207.13954</link>
      <description>arXiv:2207.13954v2 Announce Type: replace 
Abstract: We give an asymptotic development of the maximum likelihood estimator (MLE), or any other estimator defined implicitly, in a way which involves the limiting behavior of the score and its higher-order derivatives. This development, which is explicitly computable, gives some insights about the non-asymptotic behavior of the renormalized MLE and its departure from its limit. We highlight that the results hold whenever the score and its derivative converge, including to non Gaussian limits. Our approach is based on an asymptotic implicit function theorem, inspired from perturbative approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.13954v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Antoine Lejay (PASTA, IECL, UL), Sara Mazzonetto (PASTA, IECL, UL)</dc:creator>
    </item>
    <item>
      <title>The out-of-sample prediction error of the square-root-LASSO and related estimators</title>
      <link>https://arxiv.org/abs/2211.07608</link>
      <description>arXiv:2211.07608v3 Announce Type: replace 
Abstract: We study the classical problem of predicting an outcome variable, $Y$, using a linear combination of a $d$-dimensional covariate vector, $\mathbf{X}$. We are interested in linear predictors whose coefficients solve: % \begin{align*} \inf_{\boldsymbol{\beta} \in \mathbb{R}^d} \left( \mathbb{E}_{\mathbb{P}_n} \left[ \left(Y-\mathbf{X}^{\top}\beta \right)^r \right] \right)^{1/r} +\delta \, \rho\left(\boldsymbol{\beta}\right), \end{align*} where $\delta&gt;0$ is a regularization parameter, $\rho:\mathbb{R}^d\to \mathbb{R}_+$ is a convex penalty function, $\mathbb{P}_n$ is the empirical distribution of the data, and $r\geq 1$. We present three sets of new results. First, we provide conditions under which linear predictors based on these estimators % solve a \emph{distributionally robust optimization} problem: they minimize the worst-case prediction error over distributions that are close to each other in a type of \emph{max-sliced Wasserstein metric}. Second, we provide a detailed finite-sample and asymptotic analysis of the statistical properties of the balls of distributions over which the worst-case prediction error is analyzed. Third, we use the distributionally robust optimality and our statistical analysis to present i) an oracle recommendation for the choice of regularization parameter, $\delta$, that guarantees good out-of-sample prediction error; and ii) a test-statistic to rank the out-of-sample performance of two different linear estimators. None of our results rely on sparsity assumptions about the true data generating process; thus, they broaden the scope of use of the square-root lasso and related estimators in prediction problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.07608v3</guid>
      <category>math.ST</category>
      <category>math.OC</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jos\'e Luis Montiel Olea, Cynthia Rush, Amilcar Velez, Johannes Wiesel</dc:creator>
    </item>
    <item>
      <title>Selecting informative conformal prediction sets with false coverage rate control</title>
      <link>https://arxiv.org/abs/2403.12295</link>
      <description>arXiv:2403.12295v2 Announce Type: replace 
Abstract: In supervised learning, including regression and classification, conformal methods provide prediction sets for the outcome/label with finite sample coverage for any machine learning predictor. We consider here the case where such prediction sets come after a selection process. The selection process requires that the selected prediction sets be `informative' in a well defined sense. We consider both the classification and regression settings where the analyst may consider as informative only the sample with prediction sets small enough, excluding null values, or obeying other appropriate `monotone' constraints. We develop a unified framework for building such informative conformal prediction sets while controlling the false coverage rate (FCR) on the selected sample. While conformal prediction sets after selection have been the focus of much recent literature in the field, the new introduced procedures, called InfoSP and InfoSCOP, are to our knowledge the first ones providing FCR control for informative prediction sets. We show the usefulness of our resulting procedures on real and simulated data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12295v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ulysse Gazin, Ruth Heller, Ariane Marandon, Etienne Roquain</dc:creator>
    </item>
    <item>
      <title>Enhanced Cauchy Schwarz inequality and some of its statistical applications</title>
      <link>https://arxiv.org/abs/2403.13964</link>
      <description>arXiv:2403.13964v2 Announce Type: replace 
Abstract: We present a general refinement of the Cauchy-Schwarz inequality over complete inner product spaces and show that it can be of interest for some statistical applications. This generalizes and simplifies previous results on the same subject.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13964v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sergio Scarlatti</dc:creator>
    </item>
    <item>
      <title>Mutual Information Density of Massive MIMO Systems over Rayleigh-Product Channels</title>
      <link>https://arxiv.org/abs/2210.08832</link>
      <description>arXiv:2210.08832v5 Announce Type: replace-cross 
Abstract: The Rayleigh-product channel model is utilized to characterize the rank deficiency caused by keyhole effects. However, the finite blocklength analysis for Rayleigh product channels is not available in the literature. In this paper, we will characterize the mutual information density (MID) and perform the FBL analysis to reveal the impact of rank-deficiency in Rayleigh-product channels. To this end, we first set up a central limit theorem for the MID over Rayleigh-product MIMO channels in the asymptotic regime where the number of scatterers, number of antennas, and blocklength go to infinity at the same pace. Then, we utilize the CLT to obtain the upper and lower bounds for the packet error probability, whose approximations in the high and low signal to noise ratio regimes are then derived to illustrate the impact of rank deficiency. One interesting observation is that rank-deficiency degrades the performance of MIMO systems with FBL and the fundamental limits of Rayleigh-product channels degenerate to those of the Rayleigh case when the number of scatterers approaches infinity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.08832v5</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Zhang, Shenghui Song</dc:creator>
    </item>
    <item>
      <title>Empirical Bayes When Estimation Precision Predicts Parameters</title>
      <link>https://arxiv.org/abs/2212.14444</link>
      <description>arXiv:2212.14444v4 Announce Type: replace-cross 
Abstract: Empirical Bayes methods usually maintain a prior independence assumption: The unknown parameters of interest are independent from the known standard errors of the estimates. This assumption is often theoretically questionable and empirically rejected. This paper instead models the conditional distribution of the parameter given the standard errors as a flexibly parametrized family of distributions, leading to a family of methods that we call CLOSE. This paper establishes that (i) CLOSE is rate-optimal for squared error Bayes regret, (ii) squared error regret control is sufficient for an important class of economic decision problems, and (iii) CLOSE is worst-case robust when our assumption on the conditional distribution is misspecified. Empirically, using CLOSE leads to sizable gains for selecting high-mobility Census tracts. Census tracts selected by CLOSE are substantially more mobile on average than those selected by the standard shrinkage method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.14444v4</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiafeng Chen</dc:creator>
    </item>
    <item>
      <title>Adaptive functional principal components analysis</title>
      <link>https://arxiv.org/abs/2306.16091</link>
      <description>arXiv:2306.16091v2 Announce Type: replace-cross 
Abstract: Functional data analysis almost always involves smoothing discrete observations into curves, because they are never observed in continuous time and rarely without error. Although smoothing parameters affect the subsequent inference, data-driven methods for selecting these parameters are not well-developed, frustrated by the difficulty of using all the information shared by curves while being computationally efficient. On the one hand, smoothing individual curves in an isolated, albeit sophisticated way, ignores useful signals present in other curves. On the other hand, bandwidth selection by automatic procedures such as cross-validation after pooling all the curves together quickly become computationally unfeasible due to the large number of data points. In this paper we propose a new data-driven, adaptive kernel smoothing, specifically tailored for functional principal components analysis through the derivation of sharp, explicit risk bounds for the eigen-elements. The minimization of these quadratic risk bounds provide refined, yet computationally efficient bandwidth rules for each eigen-element separately. Both common and independent design cases are allowed. Rates of convergence for the estimators are derived. An extensive simulation study, designed in a versatile manner to closely mimic the characteristics of real data sets supports our methodological contribution. An illustration on a real data application is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16091v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sunny G. W. Wang, Valentin Patilea, Nicolas Klutchnikoff</dc:creator>
    </item>
    <item>
      <title>Optimally weighted average derivative effects</title>
      <link>https://arxiv.org/abs/2308.05456</link>
      <description>arXiv:2308.05456v2 Announce Type: replace-cross 
Abstract: Weighted average derivative effects (WADEs) are nonparametric estimands with uses in economics and causal inference. Debiased WADE estimators typically require learning the conditional mean outcome as well as a Riesz representer (RR) that characterises the requisite debiasing corrections. RR estimators for WADEs often rely on kernel estimators, introducing complicated bandwidth-dependant biases. In our work we propose a new class of RRs that are isomorphic to the class of WADEs and we derive the WADE weight that is optimal, in the sense of having minimum nonparametric efficiency bound. Our optimal WADE estimators require estimating conditional expectations only (e.g. using machine learning), thus overcoming the limitations of kernel estimators. Moreover, we connect our optimal WADE to projection parameters in partially linear models. We ascribe a causal interpretation to WADE and projection parameters in terms of so-called incremental effects. We propose efficient estimators for two WADE estimands in our class, which we evaluate in a numerical experiment and use to determine the effect of Warfarin dose on blood clotting function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.05456v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oliver Hines, Karla Diaz-Ordaz, Stijn Vansteelandt</dc:creator>
    </item>
    <item>
      <title>Extremal graphical modeling with latent variables</title>
      <link>https://arxiv.org/abs/2403.09604</link>
      <description>arXiv:2403.09604v2 Announce Type: replace-cross 
Abstract: Extremal graphical models encode the conditional independence structure of multivariate extremes and provide a powerful tool for quantifying the risk of rare events. Prior work on learning these graphs from data has focused on the setting where all relevant variables are observed. For the popular class of H\"usler-Reiss models, we propose the \texttt{eglatent} method, a tractable convex program for learning extremal graphical models in the presence of latent variables. Our approach decomposes the H\"usler-Reiss precision matrix into a sparse component encoding the graphical structure among the observed variables after conditioning on the latent variables, and a low-rank component encoding the effect of a few latent variables on the observed variables. We provide finite-sample guarantees of \texttt{eglatent} and show that it consistently recovers the conditional graph as well as the number of latent variables. We highlight the improved performances of our approach on synthetic and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09604v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sebastian Engelke, Armeen Taeb</dc:creator>
    </item>
    <item>
      <title>Federated Transfer Learning with Differential Privacy</title>
      <link>https://arxiv.org/abs/2403.11343</link>
      <description>arXiv:2403.11343v2 Announce Type: replace-cross 
Abstract: Federated learning is gaining increasing popularity, with data heterogeneity and privacy being two prominent challenges. In this paper, we address both issues within a federated transfer learning framework, aiming to enhance learning on a target data set by leveraging information from multiple heterogeneous source data sets while adhering to privacy constraints. We rigorously formulate the notion of \textit{federated differential privacy}, which offers privacy guarantees for each data set without assuming a trusted central server. Under this privacy constraint, we study three classical statistical problems, namely univariate mean estimation, low-dimensional linear regression, and high-dimensional linear regression. By investigating the minimax rates and identifying the costs of privacy for these problems, we show that federated differential privacy is an intermediate privacy model between the well-established local and central models of differential privacy. Our analyses incorporate data heterogeneity and privacy, highlighting the fundamental costs of both in federated learning and underscoring the benefit of knowledge transfer across data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11343v2</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mengchu Li, Ye Tian, Yang Feng, Yi Yu</dc:creator>
    </item>
    <item>
      <title>On the rates of convergence for learning with convolutional neural networks</title>
      <link>https://arxiv.org/abs/2403.16459</link>
      <description>arXiv:2403.16459v2 Announce Type: replace-cross 
Abstract: We study approximation and learning capacities of convolutional neural networks (CNNs) with one-side zero-padding and multiple channels. Our first result proves a new approximation bound for CNNs with certain constraint on the weights. Our second result gives new analysis on the covering number of feed-forward neural networks with CNNs as special cases. The analysis carefully takes into account the size of the weights and hence gives better bounds than the existing literature in some situations. Using these two results, we are able to derive rates of convergence for estimators based on CNNs in many learning problems. In particular, we establish minimax optimal convergence rates of the least squares based on CNNs for learning smooth functions in the nonparametric regression setting. For binary classification, we derive convergence rates for CNN classifiers with hinge loss and logistic loss. It is also shown that the obtained rates for classification are minimax optimal in some common settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16459v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunfei Yang, Han Feng, Ding-Xuan Zhou</dc:creator>
    </item>
    <item>
      <title>A simple lower bound for the complexity of estimating partition functions on a quantum computer</title>
      <link>https://arxiv.org/abs/2404.02414</link>
      <description>arXiv:2404.02414v2 Announce Type: replace-cross 
Abstract: We study the complexity of estimating the partition function $\mathsf{Z}(\beta)=\sum_{x\in\chi} e^{-\beta H(x)}$ for a Gibbs distribution characterized by the Hamiltonian $H(x)$. We provide a simple and natural lower bound for quantum algorithms that solve this task by relying on reflections through the coherent encoding of Gibbs states. Our primary contribution is a $\varOmega(1/\epsilon)$ lower bound for the number of reflections needed to estimate the partition function with a quantum algorithm. The proof is based on a reduction from the problem of estimating the Hamming weight of an unknown binary string.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02414v2</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zherui Chen, Giacomo Nannicini</dc:creator>
    </item>
  </channel>
</rss>
