<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 09 Sep 2024 04:03:58 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Estimation of service value parameters for a queue with unobserved balking</title>
      <link>https://arxiv.org/abs/2409.04090</link>
      <description>arXiv:2409.04090v1 Announce Type: new 
Abstract: In Naor's model [16], customers decide whether or not to join a queue after observing its length. We suppose that customers are heterogeneous in their service value (reward) $R$ from completed service and homogeneous in the cost of staying in the system per unit of time. It is assumed that the values of customers are independent random variables generated from a common parametric distribution. The manager observes the queue length process, but not the balking customers. Based on the queue length data, an MLE is constructed for the underlying parameters of $R$. We provide verifiable conditions for which the estimator is consistent and asymptotically normal. A dynamic pricing scheme is constructed that starts from some arbitrary price and iteratively updates the price using the estimated parameters. The performance of the estimator and the pricing algorithm are studied through a series of simulation experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04090v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Podorojnyi, Liron Ravner</dc:creator>
    </item>
    <item>
      <title>Estimation of Proportion of Null Hypotheses Under Dependence</title>
      <link>https://arxiv.org/abs/2409.04100</link>
      <description>arXiv:2409.04100v1 Announce Type: new 
Abstract: Estimation of the proportion of null hypotheses in a multiple testing problem can greatly enhance the performance of the existing algorithms. Although various estimators for the proportion of null hypotheses have been proposed, most are designed for independent samples, and their effectiveness in dependent scenarios is not well explored. This article investigates the asymptotic behavior of the BH estimator and evaluates its performance across different types of dependence. Additionally, we assess Storey's estimator and another estimator proposed by Patra and Sen (2016) to understand their effectiveness in these settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04100v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nabaneet Das</dc:creator>
    </item>
    <item>
      <title>Improved Catoni-Type Confidence Sequences for Estimating the Mean When the Variance Is Infinite</title>
      <link>https://arxiv.org/abs/2409.04198</link>
      <description>arXiv:2409.04198v1 Announce Type: new 
Abstract: We consider a discrete time stochastic model with infinite variance and study the mean estimation problem as in Wang and Ramdas (2023). We refine the Catoni-type confidence sequence (abbr. CS) and use an idea of Bhatt et al. (2022) to achieve notable improvements of some currently existing results for such model.
  Specifically, for given $\alpha \in (0, 1]$, we assume that there is a known upper bound $\nu_{\alpha} &gt; 0$ for the $(1 + \alpha)$-th central moment of the population distribution that the sample follows. Our findings replicate and `optimize' results in the above references for $\alpha = 1$ (i.e., in models with finite variance) and enhance the results for $\alpha &lt; 1$. Furthermore, by employing the stitching method, we derive an upper bound on the width of the CS as $\mathcal{O} \left(((\log \log t)/t)^{\frac{\alpha}{1+\alpha}}\right)$ for the shrinking rate as $t$ increases, and $\mathcal{O}(\left(\log (1/\delta)\right)^{\frac{\alpha }{1+\alpha}})$ for the growth rate as $\delta$ decreases. These bounds are improving upon the bounds found in Wang and Ramdas (2023). Our theoretical results are illustrated by results from a series of simulation experiments. Comparing the performance of our improved $\alpha$-Catoni-type CS with the bound in the above cited paper indicates that our CS achieves tighter width.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04198v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chengfu Wei, Jordan Stoyanov, Yiming Chen, Zijun Chen</dc:creator>
    </item>
    <item>
      <title>Generative Modelling via Quantile Regression</title>
      <link>https://arxiv.org/abs/2409.04231</link>
      <description>arXiv:2409.04231v1 Announce Type: new 
Abstract: We link conditional generative modelling to quantile regression. We propose a suitable loss function and derive minimax convergence rates for the associated risk under smoothness assumptions imposed on the conditional distribution. To establish the lower bound, we show that nonparametric regression can be seen as a sub-problem of the considered generative modelling framework. Finally, we discuss extensions of our work to generate data from multivariate distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04231v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Johannes Schmidt-Hieber, Petr Zamolodtchikov</dc:creator>
    </item>
    <item>
      <title>Random effects estimation in a fractional diffusion model based on continuous observations</title>
      <link>https://arxiv.org/abs/2409.04331</link>
      <description>arXiv:2409.04331v1 Announce Type: new 
Abstract: The purpose of the present work is to construct estimators for the random effects in a fractional diffusion model using a hybrid estimation method where we combine parametric and nonparametric thechniques. We precisely consider $n$ stochastic processes $\left\{X_t^j,\ 0\leq t\leq T\right\}$, $j=1,\ldots, n$ continuously observed over the time interval $[0,T]$, where the dynamics of each process are described by fractional stochastic differential equations with drifts depending on random effects. We first construct a parametric estimator for the random effects using the techniques of maximum likelihood estimation and we study its asymptotic properties when the time horizon $T$ is sufficiently large. Then by taking into account the obtained estimator for the random effects, we build a nonparametric estimator for their common unknown density function using Bernstein polynomials approximation. Some asymptotic properties of the density estimator, such as its asymptotic bias, variance and mean integrated squared error, are studied for an infinite time horizon $T$ and a fixed sample size $n$. The asymptotic normality and the uniform convergence of the estimator are investigated for an infinite time horizon $T$, a high frequency and as the order of Bernstein polynomials is sufficiently large. Some numerical simulations are also presented to illustrate the performance of the Bernstein polynomials based estimator compared to standard Kernel estimator for the random effects density function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04331v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nesrine Chebli, Hamdi Fathallah, Yousri Slaoui</dc:creator>
    </item>
    <item>
      <title>Approximate D-optimal design and equilibrium measure *</title>
      <link>https://arxiv.org/abs/2409.04058</link>
      <description>arXiv:2409.04058v1 Announce Type: cross 
Abstract: We introduce a variant of the D-optimal design of experiments problem with a more general information matrix that takes into account the representation of the design space S. The main motivation is that if S $\subset$ R d is the unit ball, the unit box or the canonical simplex, then remarkably, for every dimension d and every degree n, the equilibrium measure of S (in pluripotential theory) is an optimal solution. Equivalently, for each degree n, the unique optimal solution is the vector of moments (up to degree 2n) of the equilibrium measure of S. Hence nding an optimal design reduces to nding a cubature for the equilibrium measure, with atoms in S, positive weights, and exact up to degree 2n. In addition, any resulting sequence of atomic D-optimal measures converges to the equilibrium measure of S for the weak-star topology, as n increases. Links with Fekete sets of points are also discussed. More general compact basic semialgebraic sets are also considered, and a previously developed two-step design algorithm is easily adapted to this new variant of D-optimal design problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04058v1</guid>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Didier Henrion (LAAS-POP), Jean Bernard Lasserre (LAAS-POP, TSE-R)</dc:creator>
    </item>
    <item>
      <title>Optimal Fidelity Estimation from Binary Measurements for Discrete and Continuous Variable Systems</title>
      <link>https://arxiv.org/abs/2409.04189</link>
      <description>arXiv:2409.04189v1 Announce Type: cross 
Abstract: Estimating the fidelity between a desired target quantum state and an actual prepared state is essential for assessing the success of experiments. For pure target states, we use functional representations that can be measured directly and determine the number of copies of the prepared state needed for fidelity estimation. In continuous variable (CV) systems, we utilise the Wigner function, which can be measured via displaced parity measurements. We provide upper and lower bounds on the sample complexity required for fidelity estimation, considering the worst-case scenario across all possible prepared states. For target states of particular interest, such as Fock and Gaussian states, we find that this sample complexity is characterised by the $L^1$-norm of the Wigner function, a measure of Wigner negativity widely studied in the literature, in particular in resource theories of quantum computation. For discrete variable systems consisting of $n$ qubits, we explore fidelity estimation protocols using Pauli string measurements. Similarly to the CV approach, the sample complexity is shown to be characterised by the $L^1$-norm of the characteristic function of the target state for both Haar random states and stabiliser states. Furthermore, in a general black box model, we prove that, for any target state, the optimal sample complexity for fidelity estimation is characterised by the smoothed $L^1$-norm of the target state. To the best of our knowledge, this is the first time the $L^1$-norm of the Wigner function provides a lower bound on the cost of some information processing task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04189v1</guid>
      <category>quant-ph</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Omar Fawzi, Aadil Oufkir, Robert Salzmann</dc:creator>
    </item>
    <item>
      <title>Minimax Optimal Algorithms with Fixed-$k$-Nearest Neighbors</title>
      <link>https://arxiv.org/abs/2202.02464</link>
      <description>arXiv:2202.02464v3 Announce Type: replace 
Abstract: This paper presents how to perform minimax optimal classification, regression, and density estimation based on fixed-$k$ nearest neighbor (NN) searches. We consider a distributed learning scenario, in which a massive dataset is split into smaller groups, where the $k$-NNs are found for a query point with respect to each subset of data. We propose \emph{optimal} rules to aggregate the fixed-$k$-NN information for classification, regression, and density estimation that achieve minimax optimal rates for the respective problems. We show that the distributed algorithm with a fixed $k$ over a sufficiently large number of groups attains a minimax optimal error rate up to a multiplicative logarithmic factor under some regularity conditions. Roughly speaking, distributed $k$-NN rules with $M$ groups has a performance comparable to the standard $\Theta(kM)$-NN rules even for fixed $k$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.02464v3</guid>
      <category>math.ST</category>
      <category>cs.DC</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J. Jon Ryu, Young-Han Kim</dc:creator>
    </item>
    <item>
      <title>Clustering Consistency of General Nonparametric Classification Methods in Cognitive Diagnosis</title>
      <link>https://arxiv.org/abs/2312.11437</link>
      <description>arXiv:2312.11437v2 Announce Type: replace 
Abstract: Cognitive diagnosis models have been popularly used in fields such as education, psychology, and social sciences. While parametric likelihood estimation is a prevailing method for fitting cognitive diagnosis models, nonparametric methodologies are attracting increasing attention due to their ease of implementation and robustness, particularly when sample sizes are relatively small. However, existing clustering consistency results of the nonparametric estimation methods often rely on certain restrictive conditions, which may not be easily satisfied in practice. In this article, the clustering consistency of the general nonparametric classification method is reestablished under weaker and more practical conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.11437v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengyu Cui, Yanlong Liu, Gongjun Xu</dc:creator>
    </item>
    <item>
      <title>Adaptive Bayesian Regression on Data with Low Intrinsic Dimensionality</title>
      <link>https://arxiv.org/abs/2407.09286</link>
      <description>arXiv:2407.09286v2 Announce Type: replace 
Abstract: We study how the posterior contraction rate under a Gaussian process (GP) prior depends on the intrinsic dimension of the predictors and smoothness of the regression function. An open question is whether a generic GP prior that does not incorporate knowledge of the intrinsic lower-dimensional structure of the predictors can attain an adaptive rate for a broad class of such structures. We show that this is indeed the case, establishing conditions under which the posterior contraction rates become adaptive to the intrinsic dimension $\varrho$ in terms of the covering number of the data domain $X$ (the Minkowski dimension), and prove the optimal posterior contraction rate $O(n^{-s/(2s +\varrho)})$, up to a logarithmic factor, assuming an approximation order $s$ of the reproducing kernel Hilbert space (RKHS) on ${X}$. When ${X}$ is a $\varrho$-dimensional compact smooth manifold, we study RKHS approximations to intrinsically defined $s$-order H\"older functions on the manifold for any positive $s$ by a novel analysis of kernel approximations on manifolds, leading to the optimal adaptive posterior contraction rate. We propose an empirical Bayes prior on the kernel bandwidth using kernel affinity and $k$-nearest neighbor statistics, eliminating the need for prior knowledge of the intrinsic dimension. The efficiency of the proposed Bayesian regression approach is demonstrated on various numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09286v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tao Tang, Nan Wu, Xiuyuan Cheng, David Dunson</dc:creator>
    </item>
    <item>
      <title>Estimation of Over-parameterized Models from an Auto-Modeling Perspective</title>
      <link>https://arxiv.org/abs/2206.01824</link>
      <description>arXiv:2206.01824v4 Announce Type: replace-cross 
Abstract: From a model-building perspective, we propose a paradigm shift for fitting over-parameterized models. Philosophically, the mindset is to fit models to future observations rather than to the observed sample. Technically, given an imputation method to generate future observations, we fit over-parameterized models to these future observations by optimizing an approximation of the desired expected loss function based on its sample counterpart and an adaptive $\textit{duality function}$. The required imputation method is also developed using the same estimation technique with an adaptive $m$-out-of-$n$ bootstrap approach. We illustrate its applications with the many-normal-means problem, $n &lt; p$ linear regression, and neural network-based image classification of MNIST digits. The numerical results demonstrate its superior performance across these diverse applications. While primarily expository, the paper conducts an in-depth investigation into the theoretical aspects of the topic. It concludes with remarks on some open problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.01824v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiran Jiang, Chuanhai Liu</dc:creator>
    </item>
    <item>
      <title>Optimizing Noise for $f$-Differential Privacy via Anti-Concentration and Stochastic Dominance</title>
      <link>https://arxiv.org/abs/2308.08343</link>
      <description>arXiv:2308.08343v2 Announce Type: replace-cross 
Abstract: In this paper, we establish anti-concentration inequalities for additive noise mechanisms which achieve $f$-differential privacy ($f$-DP), a notion of privacy phrased in terms of a tradeoff function $f$ which limits the ability of an adversary to determine which individuals were in the database. We show that canonical noise distributions (CNDs), proposed by Awan and Vadhan (2023), match the anti-concentration bounds at half-integer values, indicating that their tail behavior is near-optimal. We also show that all CNDs are sub-exponential, regardless of the $f$-DP guarantee. In the case of log-concave CNDs, we show that they are the stochastically smallest noise compared to any other noise distributions with the same privacy guarantee. In terms of integer-valued noise, we propose a new notion of discrete CND and prove that a discrete CND always exists, can be constructed by rounding a continuous CND, and that the discrete CND is unique when designed for a statistic with sensitivity 1. We further show that the discrete CND at sensitivity 1 is stochastically smallest compared to other integer-valued noises. Our theoretical results shed light on the different types of privacy guarantees possible in the $f$-DP framework and can be incorporated in more complex mechanisms to optimize performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.08343v2</guid>
      <category>cs.CR</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jordan Awan, Aishwarya Ramasethu</dc:creator>
    </item>
    <item>
      <title>Sharper dimension-free bounds on the Frobenius distance between sample covariance and its expectation</title>
      <link>https://arxiv.org/abs/2308.14739</link>
      <description>arXiv:2308.14739v2 Announce Type: replace-cross 
Abstract: We study properties of a sample covariance estimate $\widehat \Sigma$ given a finite sample of $n$ i.i.d. centered random elements in $\R^d$ with the covariance matrix $\Sigma$. We derive dimension-free bounds on the squared Frobenius norm of $(\widehat\Sigma - \Sigma)$ under reasonable assumptions. For instance, we show that $\smash{\|\widehat\Sigma - \Sigma\|_{\rm F}^2}$ differs from its expectation by at most $\smash{\mathcal O({\rm{Tr}}(\Sigma^2) / n)}$ with overwhelming probability, which is a significant improvement over the existing results. This allows us to establish the concentration phenomenon for the squared Frobenius distance between the covariance and its empirical counterpart in the case of moderately large effective rank of $\Sigma$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.14739v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikita Puchkin, Fedor Noskov, Vladimir Spokoiny</dc:creator>
    </item>
    <item>
      <title>Bayesian Cram\'er-Rao Bound Estimation with Score-Based Models</title>
      <link>https://arxiv.org/abs/2309.16076</link>
      <description>arXiv:2309.16076v3 Announce Type: replace-cross 
Abstract: The Bayesian Cram\'er-Rao bound (CRB) provides a lower bound on the mean square error of any Bayesian estimator under mild regularity conditions. It can be used to benchmark the performance of statistical estimators, and provides a principled metric for system design and optimization. However, the Bayesian CRB depends on the underlying prior distribution, which is often unknown for many problems of interest. This work introduces a new data-driven estimator for the Bayesian CRB using score matching, i.e., a statistical estimation technique that models the gradient of a probability distribution from a given set of training data. The performance of the proposed estimator is analyzed in both the classical parametric modeling regime and the neural network modeling regime. In both settings, we develop novel non-asymptotic bounds on the score matching error and our Bayesian CRB estimator based on the results from empirical process theory, including classical bounds and recently introduced techniques for characterizing neural networks. We illustrate the performance of the proposed estimator with two application examples: a signal denoising problem and a dynamic phase offset estimation problem in communication systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.16076v3</guid>
      <category>stat.ML</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TIT.2024.3447552</arxiv:DOI>
      <dc:creator>Evan Scope Crafts, Xianyang Zhang, Bo Zhao</dc:creator>
    </item>
    <item>
      <title>Jeffreys-prior penalty for high-dimensional logistic regression: A conjecture about aggregate bias</title>
      <link>https://arxiv.org/abs/2311.11290</link>
      <description>arXiv:2311.11290v2 Announce Type: replace-cross 
Abstract: Firth (1993, Biometrika) shows that the maximum Jeffreys' prior penalized likelihood estimator in logistic regression has asymptotic bias decreasing with the square of the number of observations when the number of parameters is fixed, which is an order faster than the typical rate from maximum likelihood. The widespread use of that estimator in applied work is supported by the results in Kosmidis and Firth (2021, Biometrika), who show that it takes finite values, even in cases where the maximum likelihood estimate does not exist. Kosmidis and Firth (2021, Biometrika) also provide empirical evidence that the estimator has good bias properties in high-dimensional settings where the number of parameters grows asymptotically linearly but slower than the number of observations. We design and carry out a large-scale computer experiment covering a wide range of such high-dimensional settings and produce strong empirical evidence for a simple rescaling of the maximum Jeffreys' prior penalized likelihood estimator that delivers high accuracy in signal recovery, in terms of aggregate bias, in the presence of an intercept parameter. The rescaled estimator is effective even in cases where estimates from maximum likelihood and other recently proposed corrective methods based on approximate message passing do not exist.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.11290v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ioannis Kosmidis, Patrick Zietkiewicz</dc:creator>
    </item>
    <item>
      <title>LASPATED: a Library for the Analysis of SPAtio-TEmporal Discrete data</title>
      <link>https://arxiv.org/abs/2401.04156</link>
      <description>arXiv:2401.04156v3 Announce Type: replace-cross 
Abstract: We describe methods, tools, and a software library called LASPATED, available on GitHub (at https://github.com/vguigues/) to fit models using spatio-temporal data and space-time discretization. A video tutorial for this library is available on YouTube. We consider two types of methods to estimate a non-homogeneous Poisson process in space and time. The methods approximate the arrival intensity function of the Poisson process by discretizing space and time, and estimating arrival intensity as a function of subregion and time interval. With such methods, it is typical that the dimension of the estimator is large relative to the amount of data, and therefore the performance of the estimator can be improved by using additional data. The first method uses additional data to add a regularization term to the likelihood function for calibrating the intensity of the Poisson process. The second method uses additional data to estimate arrival intensity as a function of covariates. We describe a Python package to perform various types of space and time discretization. We also describe two packages for the calibration of the models, one in Matlab and one in C++. We demonstrate the advantages of our methods compared to basic maximum likelihood estimation with simulated and real data. The experiments with real data calibrate models of the arrival process of emergencies to be handled by the Rio de Janeiro emergency medical service.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.04156v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vincent Guigues, Anton Kleywegt, Giovanni Amorim, Andr\'e Mazal Krauss, Victor Hugo Nascimento</dc:creator>
    </item>
    <item>
      <title>Structural adaptation via directional regularity: rate accelerated estimation in multivariate functional data</title>
      <link>https://arxiv.org/abs/2409.00817</link>
      <description>arXiv:2409.00817v2 Announce Type: replace-cross 
Abstract: We introduce directional regularity, a new definition of anisotropy for multivariate functional data. Instead of taking the conventional view which determines anisotropy as a notion of smoothness along a dimension, directional regularity additionally views anisotropy through the lens of directions. We show that faster rates of convergence can be obtained through a change-of-basis by adapting to the directional regularity of a multivariate process. An algorithm for the estimation and identification of the change-of-basis matrix is constructed, made possible due to the unique replication structure of functional data. Non-asymptotic bounds are provided for our algorithm, supplemented by numerical evidence from an extensive simulation study. We discuss two possible applications of the directional regularity approach, and advocate its consideration as a standard pre-processing step in multivariate functional data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00817v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Omar Kassi, Sunny G. W. Wang</dc:creator>
    </item>
  </channel>
</rss>
