<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 11 Feb 2025 04:03:55 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Tight Bounds for Noisy Computation of High-Influence Functions, Connectivity, and Threshold</title>
      <link>https://arxiv.org/abs/2502.04632</link>
      <description>arXiv:2502.04632v1 Announce Type: new 
Abstract: In the noisy query model, the (binary) return value of every query (possibly repeated) is independently flipped with some fixed probability $p \in (0, 1/2)$. In this paper, we obtain tight bounds on the noisy query complexity of several fundamental problems.
  Our first contribution is to show that any Boolean function with total influence $\Omega(n)$ has noisy query complexity $\Theta(n\log n)$. Previous works often focus on specific problems, and it is of great interest to have a characterization of noisy query complexity for general functions. Our result is the first noisy query complexity lower bound of this generality, beyond what was known for random Boolean functions [Reischuk and Schmeltz, FOCS 1991].
  Our second contribution is to prove that Graph Connectivity has noisy query complexity $\Theta(n^2 \log n)$. In this problem, the goal is to determine whether an undirected graph is connected using noisy edge queries. While the upper bound can be achieved by a simple algorithm, no non-trivial lower bounds were known prior to this work.
  Last but not least, we determine the exact number of noisy queries (up to lower order terms) needed to solve the $k$-Threshold problem and the Counting problem. The $k$-Threshold problem asks to decide whether there are at least $k$ ones among $n$ bits, given noisy query access to the bits. We prove that $(1\pm o(1)) \frac{n\log (\min\{k,n-k+1\}/\delta)}{(1-2p)\log \frac{1-p}p}$ queries are both sufficient and necessary to achieve error probability $\delta = o(1)$. Previously, such a result was only known when $\min\{k,n-k+1\}=o(n)$ [Wang, Ghaddar, Zhu and Wang, arXiv 2024]. We also show a similar $(1\pm o(1)) \frac{n\log (\min\{k+1,n-k+1\}/\delta)}{(1-2p)\log \frac{1-p}p}$ bound for the Counting problem, where one needs to count the number of ones among $n$ bits given noisy query access and $k$ denotes the answer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04632v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuzhou Gu, Xin Li, Yinzhan Xu</dc:creator>
    </item>
    <item>
      <title>LLM Query Scheduling with Prefix Reuse and Latency Constraints</title>
      <link>https://arxiv.org/abs/2502.04677</link>
      <description>arXiv:2502.04677v1 Announce Type: new 
Abstract: The efficient deployment of large language models (LLMs) in online settings requires optimizing inference performance under stringent latency constraints, particularly the time-to-first-token (TTFT) and time-per-output-token (TPOT). This paper focuses on the query scheduling problem for LLM inference with prefix reuse, a technique that leverages shared prefixes across queries to reduce computational overhead. Our work reveals previously unknown limitations of the existing first-come-first-serve (FCFS) and longest-prefix-match (LPM) scheduling strategies with respect to satisfying latency constraints. We present a formal theoretical framework for LLM query scheduling under RadixAttention, a prefix reuse mechanism that stores and reuses intermediate representations in a radix tree structure. Our analysis establishes the NP-hardness of the scheduling problem with prefix reuse under TTFT constraints and proposes a novel scheduling algorithm, $k$-LPM, which generalizes existing methods by balancing prefix reuse and fairness in query processing. Theoretical guarantees demonstrate that $k$-LPM achieves improved TTFT performance under realistic traffic patterns captured by a data generative model. Empirical evaluations in a realistic serving setting validates our findings, showing significant reductions in P99 TTFT compared to baseline methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04677v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gregory Dexter, Shao Tang, Ata Fatahi Baarzi, Qingquan Song, Tejas Dharamsi, Aman Gupta</dc:creator>
    </item>
    <item>
      <title>Exact Algorithms for Distance to Unique Vertex Cover</title>
      <link>https://arxiv.org/abs/2502.05059</link>
      <description>arXiv:2502.05059v1 Announce Type: new 
Abstract: Horiyama et al. (AAAI 2024) studied the problem of generating graph instances that possess a unique minimum vertex cover under specific conditions. Their approach involved pre-assigning certain vertices to be part of the solution or excluding them from it. Notably, for the \textsc{Vertex Cover} problem, pre-assigning a vertex is equivalent to removing it from the graph. Horiyama et al.~focused on maintaining the size of the minimum vertex cover after these modifications. In this work, we extend their study by relaxing this constraint: our goal is to ensure a unique minimum vertex cover, even if the removal of a vertex may not incur a decrease on the size of said cover.
  Surprisingly, our relaxation introduces significant theoretical challenges. We observe that the problem is $\Sigma^2_P$-complete, and remains so even for planar graphs of maximum degree 5. Nevertheless, we provide a linear time algorithm for trees, which is then further leveraged to show that MU-VC is in \textsf{FPT} when parameterized by the combination of treewidth and maximum degree. Finally, we show that MU-VC is in \textsf{XP} when parameterized by clique-width while it is fixed-parameter tractable (FPT) if we add the size of the solution as part of the parameter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05059v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Foivos Fioravantes, Du\v{s}an Knop, Nikolaos Melissinos, Michal Opler, Manolis Vasilakis</dc:creator>
    </item>
    <item>
      <title>A Randomised Approach to Distributed Sorting</title>
      <link>https://arxiv.org/abs/2502.05082</link>
      <description>arXiv:2502.05082v1 Announce Type: new 
Abstract: We introduce and analyse a new, extremely simple, randomised sorting algorithm:
  - choose a pair of indices $\{i, j\}$ according to some distribution $q$;
  - sort the elements in positions $i$ and $j$ of the array in ascending order.
  Choosing $q_{\{i,j\}} \propto 1/|j - i|$ yields an order-$n (\log n)^2$ sorting time. We call it the harmonic sorter.
  The sorter trivially parallelises in the asynchronous setting, yielding a linear speed-up. We also exhibit a low-communication, synchronous version with a linear speed-up.
  We compare and contrast this algorithm with other sorters, and discuss some of its benefits, particularly its robustness and amenability to parallelisation and distributed computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05082v1</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <category>cs.DM</category>
      <category>math.PR</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Sam Olesker-Taylor</dc:creator>
    </item>
    <item>
      <title>Near-Optimal Sample Complexity for MDPs via Anchoring</title>
      <link>https://arxiv.org/abs/2502.04477</link>
      <description>arXiv:2502.04477v1 Announce Type: cross 
Abstract: We study a new model-free algorithm to compute $\varepsilon$-optimal policies for average reward Markov decision processes, in the weakly communicating case. Given a generative model, our procedure combines a recursive sampling technique with Halpern's anchored iteration, and computes an $\varepsilon$-optimal policy with sample and time complexity $\widetilde{O}(|\mathcal{S}||\mathcal{A}|\|h^*\|_{\text{sp}}^{2}/\varepsilon^{2})$ both in high probability and in expectation. To our knowledge, this is the best complexity among model-free algorithms, matching the known lower bound up to a factor $\|h^*\|_{\text{sp}}$. Although the complexity bound involves the span seminorm $\|h^*\|_{\text{sp}}$ of the unknown bias vector, the algorithm requires no prior knowledge and implements a stopping rule which guarantees with probability 1 that the procedure terminates in finite time. We also analyze how these techniques can be adapted for discounted MDPs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04477v1</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jongmin Lee, Mario Bravo, Roberto Cominetti</dc:creator>
    </item>
    <item>
      <title>Computing and Learning on Combinatorial Data</title>
      <link>https://arxiv.org/abs/2502.05063</link>
      <description>arXiv:2502.05063v1 Announce Type: cross 
Abstract: The twenty-first century is a data-driven era where human activities and behavior, physical phenomena, scientific discoveries, technology advancements, and almost everything that happens in the world resulting in massive generation, collection, and utilization of data.
  Connectivity in data is a crucial property. A straightforward example is the World Wide Web, where every webpage is connected to other web pages through hyperlinks, providing a form of directed connectivity. Combinatorial data refers to combinations of data items based on certain connectivity rules. Other forms of combinatorial data include social networks, meshes, community clusters, set systems, and molecules.
  This Ph.D. dissertation focuses on learning and computing with combinatorial data. We study and examine topological and connectivity features within and across connected data to improve the performance of learning and achieve high algorithmic efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05063v1</guid>
      <category>cs.AI</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Zhang</dc:creator>
    </item>
    <item>
      <title>Efficient distributional regression trees learning algorithms for calibrated non-parametric probabilistic forecasts</title>
      <link>https://arxiv.org/abs/2502.05157</link>
      <description>arXiv:2502.05157v1 Announce Type: cross 
Abstract: The perspective of developing trustworthy AI for critical applications in science and engineering requires machine learning techniques that are capable of estimating their own uncertainty. In the context of regression, instead of estimating a conditional mean, this can be achieved by producing a predictive interval for the output, or to even learn a model of the conditional probability $p(y|x)$ of an output $y$ given input features $x$. While this can be done under parametric assumptions with, e.g. generalized linear model, these are typically too strong, and non-parametric models offer flexible alternatives. In particular, for scalar outputs, learning directly a model of the conditional cumulative distribution function of $y$ given $x$ can lead to more precise probabilistic estimates, and the use of proper scoring rules such as the weighted interval score (WIS) and the continuous ranked probability score (CRPS) lead to better coverage and calibration properties.
  This paper introduces novel algorithms for learning probabilistic regression trees for the WIS or CRPS loss functions. These algorithms are made computationally efficient thanks to an appropriate use of known data structures - namely min-max heaps, weight-balanced binary trees and Fenwick trees. Through numerical experiments, we demonstrate that the performance of our methods is competitive with alternative approaches. Additionally, our methods benefit from the inherent interpretability and explainability of trees. As a by-product, we show how our trees can be used in the context of conformal prediction and explain why they are particularly well-suited for achieving group-conditional coverage guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05157v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Duchemin Quentin, Obozinski Guillaume</dc:creator>
    </item>
    <item>
      <title>A linear-time algorithm for $(1+\epsilon)\Delta$-edge-coloring</title>
      <link>https://arxiv.org/abs/2407.04887</link>
      <description>arXiv:2407.04887v2 Announce Type: replace 
Abstract: We present a randomized algorithm that, given a constant $\epsilon &gt; 0$, outputs a proper $(1+\epsilon)\Delta$-edge-coloring of an $m$-edge simple graph $G$ of maximum degree $\Delta \geq 1/\epsilon$ in $O(m)$ time with high probability. This is the first linear-time algorithm for this problem covering the full range of possible values of $\Delta$. Indeed, even for edge-coloring with $2\Delta - 1$ colors (i.e., meeting the "greedy" bound), no such linear-time algorithm has been previously known.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04887v2</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anton Bernshteyn, Abhishek Dhawan</dc:creator>
    </item>
    <item>
      <title>Local Sherman's Algorithm for Multi-commodity Flow</title>
      <link>https://arxiv.org/abs/2501.10632</link>
      <description>arXiv:2501.10632v2 Announce Type: replace 
Abstract: We give the first local algorithm for computing multi-commodity flow and apply it to obtain a $(1+\epsilon)$-approximate algorithm for computing a $k$-commodity flow on an expander with $m$ edges in $(m+\epsilon^{-3}k^3D)n^{o(1)}$ time, where $D$ is the total demand. This is the first $(1+\epsilon)$-approximate algorithm that breaks the $km$ multi-commodity flow barrier, albeit only on expanders. All previous algorithms either require $\Omega(km)$ time or a big constant approximation.
  Our approach is by localizing Sherman's flow algorithm when put into the Multiplicative Weight Update (MWU) framework. We show that, on each round of MWU, the oracle could instead work with the *rounded weights* where all polynomially small weights are rounded to zero. Since there are only few large weights, one can implement the oracle call with respect to the rounded weights in sublinear time. This insight is generic and may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10632v2</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jason Li, Thatchaphol Saranurak</dc:creator>
    </item>
    <item>
      <title>On the on-line coloring of unit interval graphs with proper interval representation</title>
      <link>https://arxiv.org/abs/2401.05648</link>
      <description>arXiv:2401.05648v5 Announce Type: replace-cross 
Abstract: We define the problem as a two-player game between Algorithm and Builder. The game is played in rounds. Each round, Builder presents an interval that is neither contained in nor contains any previously presented interval. Algorithm immediately and irrevocably assigns the interval a color that has not been assigned to any interval intersecting it. The set of intervals form an interval representation for a unit interval graph and the colors form a proper coloring of that graph. For every positive integer $\omega$, we define the value $R(\omega)$ as the maximum number of colors for which Builder has a strategy that forces Algorithm to use $R(\omega)$ colors with the restriction that the unit interval graph constructed cannot contain a clique of size $\omega$. In 1981, Chrobak and \'{S}lusarek showed that $R(\omega)\leq2\omega -1$. In 2005, Epstein and Levy showed that $R(\omega)\geq\lfloor{3\omega/2\rfloor}$. This problem remained unsolved for $\omega\geq 3$. In 2022, Bir\'o and Curbelo showed that $R(3)=5$. In this paper, we show that $R(4)=7$</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.05648v5</guid>
      <category>math.CO</category>
      <category>cs.DS</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Israel R. Curbelo, Hannah R. Malko</dc:creator>
    </item>
    <item>
      <title>Regularized Robustly Reliable Learners and Instance Targeted Attacks</title>
      <link>https://arxiv.org/abs/2410.10572</link>
      <description>arXiv:2410.10572v2 Announce Type: replace-cross 
Abstract: Instance-targeted data poisoning attacks, where an adversary corrupts a training set to induce errors on specific test points, have raised significant concerns. Balcan et al (2022) proposed an approach to addressing this challenge by defining a notion of robustly-reliable learners that provide per-instance guarantees of correctness under well-defined assumptions, even in the presence of data poisoning attacks. They then give a generic optimal (but computationally inefficient) robustly reliable learner as well as a computationally efficient algorithm for the case of linear separators over log-concave distributions.
  In this work, we address two challenges left open by Balcan et al (2022). The first is that the definition of robustly-reliable learners in Balcan et al (2022) becomes vacuous for highly-flexible hypothesis classes: if there are two classifiers h_0, h_1 \in H both with zero error on the training set such that h_0(x) \neq h_1(x), then a robustly-reliable learner must abstain on x. We address this problem by defining a modified notion of regularized robustly-reliable learners that allows for nontrivial statements in this case. The second is that the generic algorithm of Balcan et al (2022) requires re-running an ERM oracle (essentially, retraining the classifier) on each test point x, which is generally impractical even if ERM can be implemented efficiently. To tackle this problem, we show that at least in certain interesting cases we can design algorithms that can produce their outputs in time sublinear in training time, by using techniques from dynamic algorithm design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10572v2</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Avrim Blum, Donya Saless</dc:creator>
    </item>
    <item>
      <title>Towards counterfactual fairness through auxiliary variables</title>
      <link>https://arxiv.org/abs/2412.04767</link>
      <description>arXiv:2412.04767v2 Announce Type: replace-cross 
Abstract: The challenge of balancing fairness and predictive accuracy in machine learning models, especially when sensitive attributes such as race, gender, or age are considered, has motivated substantial research in recent years. Counterfactual fairness ensures that predictions remain consistent across counterfactual variations of sensitive attributes, which is a crucial concept in addressing societal biases. However, existing counterfactual fairness approaches usually overlook intrinsic information about sensitive features, limiting their ability to achieve fairness while simultaneously maintaining performance. To tackle this challenge, we introduce EXOgenous Causal reasoning (EXOC), a novel causal reasoning framework motivated by exogenous variables. It leverages auxiliary variables to uncover intrinsic properties that give rise to sensitive attributes. Our framework explicitly defines an auxiliary node and a control node that contribute to counterfactual fairness and control the information flow within the model. Our evaluation, conducted on synthetic and real-world datasets, validates EXOC's superiority, showing that it outperforms state-of-the-art approaches in achieving counterfactual fairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04767v2</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bowei Tian, Ziyao Wang, Shwai He, Wanghao Ye, Guoheng Sun, Yucong Dai, Yongkai Wu, Ang Li</dc:creator>
    </item>
    <item>
      <title>Tree independence number V. Walls and claws</title>
      <link>https://arxiv.org/abs/2501.14658</link>
      <description>arXiv:2501.14658v2 Announce Type: replace-cross 
Abstract: Given a family $\mathcal{H}$ of graphs, we say that a graph $G$ is $\mathcal{H}$-free if no induced subgraph of $G$ is isomorphic to a member of $\mathcal{H}$. Let $S_{t,t,t}$ be the graph obtained from $K_{1,3}$ by subdividing each edge $t-1$ times, and let $W_{t\times t}$ be the $t$-by-$t$ hexagonal grid. Let $\mathcal{L}_t$ be the family of all graphs $G$ such that $G$ is the line graph of some subdivision of $W_{t \times t}$. We prove that for every positive integer $t$ there exists $c(t)$ such that every $\mathcal{L}_t \cup \{S_{t,t,t}, K_{t,t}\}$-free $n$-vertex graph admits a tree decomposition in which the maximum size of an independent set in each bag is at most $c(t)\log^4n$. This is a variant of a conjecture of Dallard, Krnc, Kwon, Milani\v{c}, Munaro, \v{S}torgel, and Wiederrecht from 2024. This implies that the Maximum Weight Independent Set problem, as well as many other natural algorithmic problems, that are known to be NP-hard in general, can be solved in quasi-polynomial time if the input graph is $\mathcal{L}_t \cup \{S_{t,t,t},K_{t,t}\}$-free. As part of our proof, we show that for every positive integer $t$ there exists an integer $d$ such that every $\mathcal{L}_t \cup \{S_{t,t,t}\}$-free graph admits a balanced separator that is contained in the neighborhood of at most $d$ vertices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14658v2</guid>
      <category>math.CO</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maria Chudnovsky, Julien Codsi, Daniel Lokshtanov, Martin Milani\v{c}, Varun Sivashankar</dc:creator>
    </item>
    <item>
      <title>Algorithms with Calibrated Machine Learning Predictions</title>
      <link>https://arxiv.org/abs/2502.02861</link>
      <description>arXiv:2502.02861v2 Announce Type: replace-cross 
Abstract: The field of algorithms with predictions incorporates machine learning advice in the design of online algorithms to improve real-world performance. While this theoretical framework often assumes uniform reliability across all predictions, modern machine learning models can now provide instance-level uncertainty estimates. In this paper, we propose calibration as a principled and practical tool to bridge this gap, demonstrating the benefits of calibrated advice through two case studies: the ski rental and online job scheduling problems. For ski rental, we design an algorithm that achieves optimal prediction-dependent performance and prove that, in high-variance settings, calibrated advice offers more effective guidance than alternative methods for uncertainty quantification. For job scheduling, we demonstrate that using a calibrated predictor leads to significant performance improvements over existing methods. Evaluations on real-world data validate our theoretical findings, highlighting the practical impact of calibration for algorithms with predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02861v2</guid>
      <category>stat.ML</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Judy Hanwen Shen, Ellen Vitercik, Anders Wikum</dc:creator>
    </item>
  </channel>
</rss>
