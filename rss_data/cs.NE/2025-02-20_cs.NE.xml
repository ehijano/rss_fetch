<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NE</link>
    <description>cs.NE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 20 Feb 2025 05:00:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>An Enhancement of Cuckoo Search Algorithm for Optimal Earthquake Evacuation Space Allocation in Intramuros, Manila City</title>
      <link>https://arxiv.org/abs/2502.13477</link>
      <description>arXiv:2502.13477v1 Announce Type: new 
Abstract: The Cuckoo Search Algorithm (CSA), while effective in solving complex optimization problems, faces limitations in random population initialization and reliance on fixed parameters. Random initialization of the population often results in clustered solutions, resulting in uneven exploration of the search space and hindering effective global optimization. Furthermore, the use of fixed values for discovery rate and step size creates a trade-off between solution accuracy and convergence speed. To address these limitations, an Enhanced Cuckoo Search Algorithm (ECSA) is proposed. This algorithm utilizes the Sobol Sequence to generate a more uniformly distributed initial population and incorporates Cosine Annealing with Warm Restarts to dynamically adjust the parameters. The performance of the algorithms was evaluated on 13 benchmark functions (7 unimodal, 6 multimodal). Statistical analyses were conducted to determine the significance and consistency of the results. The ECSA outperforms the CSA in 11 out of 13 benchmark functions with a mean fitness improvement of 30% across all functions, achieving 35% for unimodal functions and 24% for multimodal functions. The enhanced algorithm demonstrated increased convergence efficiency, indicating its superiority to the CSA in solving a variety of optimization problems. The ECSA is subsequently applied to optimize earthquake evacuation space allocation in Intramuros, Manila.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13477v1</guid>
      <category>cs.NE</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marcus Andre Villanueva, Charles Matthew Ching, Khatalyn Mata</dc:creator>
    </item>
    <item>
      <title>Cascading CMA-ES Instances for Generating Input-diverse Solution Batches</title>
      <link>https://arxiv.org/abs/2502.13730</link>
      <description>arXiv:2502.13730v1 Announce Type: new 
Abstract: Rather than obtaining a single good solution for a given optimization problem, users often seek alternative design choices, because the best-found solution may perform poorly with respect to additional objectives or constraints that are difficult to capture into the modeling process.
  Aiming for batches of diverse solutions of high quality is often desirable, as it provides flexibility to accommodate post-hoc user preferences. At the same time, it is crucial that the quality of the best solution found is not compromised.
  One particular problem setting balancing high quality and diversity is fixing the required minimum distance between solutions while simultaneously obtaining the best possible fitness. Recent work by Santoni et al. [arXiv 2024] revealed that this setting is not well addressed by state-of-the-art algorithms, performing in par or worse than pure random sampling.
  Driven by this important limitation, we propose a new approach, where parallel runs of the covariance matrix adaptation evolution strategy (CMA-ES) inherit tabu regions in a cascading fashion. We empirically demonstrate that our CMA-ES-Diversity Search (CMA-ES-DS) algorithm generates trajectories that allow to extract high-quality solution batches that respect a given minimum distance requirement, clearly outperforming those obtained from off-the-shelf random sampling, multi-modal optimization algorithms, and standard CMA-ES.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13730v1</guid>
      <category>cs.NE</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maria Laura Santoni, Christoph D\"urr, Carola Doerr, Mike Preuss, Elena Raponi</dc:creator>
    </item>
    <item>
      <title>Emergence of the Primacy Effect in Structured State-Space Models</title>
      <link>https://arxiv.org/abs/2502.13729</link>
      <description>arXiv:2502.13729v1 Announce Type: cross 
Abstract: Human and animal memory for sequentially presented items is well-documented to be more accurate for those at the beginning and end of a sequence, phenomena known as the primacy and recency effects, respectively. By contrast, artificial neural network (ANN) models are typically designed with a memory that decays monotonically over time. Accordingly, ANNs are expected to show the recency effect but not the primacy effect. Contrary to this theoretical expectation, however, the present study reveals a counterintuitive finding: a recently developed ANN architecture, called structured state-space models, exhibits the primacy effect when trained and evaluated on a synthetic task that mirrors psychological memory experiments. Given that this model was originally designed for recovering neuronal activity patterns observed in biological brains, this result provides a novel perspective on the psychological primacy effect while also posing a non-trivial puzzle for the current theories in machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13729v1</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takashi Morita</dc:creator>
    </item>
    <item>
      <title>CLIF: Complementary Leaky Integrate-and-Fire Neuron for Spiking Neural Networks</title>
      <link>https://arxiv.org/abs/2402.04663</link>
      <description>arXiv:2402.04663v5 Announce Type: replace 
Abstract: Spiking neural networks (SNNs) are promising brain-inspired energy-efficient models. Compared to conventional deep Artificial Neural Networks (ANNs), SNNs exhibit superior efficiency and capability to process temporal information. However, it remains a challenge to train SNNs due to their undifferentiable spiking mechanism. The surrogate gradients method is commonly used to train SNNs, but often comes with an accuracy disadvantage over ANNs counterpart. We link the degraded accuracy to the vanishing of gradient on the temporal dimension through the analytical and experimental study of the training process of Leaky Integrate-and-Fire (LIF) Neuron-based SNNs. Moreover, we propose the Complementary Leaky Integrate-and-Fire (CLIF) Neuron. CLIF creates extra paths to facilitate the backpropagation in computing temporal gradient while keeping binary output. CLIF is hyperparameter-free and features broad applicability. Extensive experiments on a variety of datasets demonstrate CLIF's clear performance advantage over other neuron models. Furthermore, the CLIF's performance even slightly surpasses superior ANNs with identical network structure and training conditions. The code is available at https://github.com/HuuYuLong/Complementary-LIF.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04663v5</guid>
      <category>cs.NE</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yulong Huang, Xiaopeng Lin, Hongwei Ren, Haotian Fu, Yue Zhou, Zunchang Liu, Biao Pan, Bojun Cheng</dc:creator>
    </item>
    <item>
      <title>QUBE: Enhancing Automatic Heuristic Design via Quality-Uncertainty Balanced Evolution</title>
      <link>https://arxiv.org/abs/2412.20694</link>
      <description>arXiv:2412.20694v3 Announce Type: replace 
Abstract: Solving NP-hard problems traditionally relies on heuristics, yet manually designing effective heuristics for complex problems remains a significant challenge. While recent advancements like FunSearch have shown that large language models (LLMs) can be integrated into evolutionary algorithms (EAs) for heuristic design, their potential is hindered by limitations in balancing exploitation and exploration. We introduce Quality-Uncertainty Balanced Evolution (QUBE), a novel approach that enhances LLM+EA methods by redefining the priority criterion within the FunSearch framework. QUBE employs the Quality-Uncertainty Trade-off Criterion (QUTC), based on our proposed Uncertainty-Inclusive Quality metric, to evaluate and guide the evolutionary process. Through extensive experiments on challenging NP-complete problems, QUBE demonstrates significant performance improvements over FunSearch and baseline methods. Our code are available at https://github.com/zzjchen/QUBE\_code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20694v3</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zijie Chen, Zhanchao Zhou, Yu Lu, Renjun Xu, Lili Pan, Zhenzhong Lan</dc:creator>
    </item>
    <item>
      <title>LayerAct: Advanced Activation Mechanism for Robust Inference of CNNs</title>
      <link>https://arxiv.org/abs/2306.04940</link>
      <description>arXiv:2306.04940v5 Announce Type: replace-cross 
Abstract: In this work, we propose a novel activation mechanism called LayerAct for CNNs. This approach is motivated by our theoretical and experimental analyses, which demonstrate that Layer Normalization (LN) can mitigate a limitation of existing activation functions regarding noise robustness. However, LN is known to be disadvantageous in CNNs due to its tendency to make activation outputs homogeneous. The proposed method is designed to be more robust than existing activation functions by reducing the upper bound of influence caused by input shifts without inheriting LN's limitation. We provide analyses and experiments showing that LayerAct functions exhibit superior robustness compared to ElementAct functions. Experimental results on three clean and noisy benchmark datasets for image classification tasks indicate that LayerAct functions outperform other activation functions in handling noisy datasets while achieving superior performance on clean datasets in most cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.04940v5</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>cs.NE</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kihyuk Yoon, Chiehyeon Lim</dc:creator>
    </item>
    <item>
      <title>Words are not Wind -- How Public Joint Commitment and Reputation Solve the Prisoner's Dilemma</title>
      <link>https://arxiv.org/abs/2307.06898</link>
      <description>arXiv:2307.06898v2 Announce Type: replace-cross 
Abstract: To achieve common goals, we often use joint commitments. Our commitment helps us to coordinate with our partners and assures them that their cooperative efforts will benefit themselves. However, if one of us can exploit the other's cooperation (as in the Prisoner's Dilemma), our commitment appears less useful. It cannot remove the temptation for our partners to exploit us. Using methods from evolutionary game theory, we study the function of joint commitments in the Prisoner's Dilemma. We propose a reputation system akin to indirect reciprocity, wherein agents observe interactions even when not directly involved. They judge cooperation as good and defection as bad, but, crucially, only if the parties involved had committed to cooperate. This results in stable cooperation even though judgments are made privately, which had been a weakness in previous models of indirect reciprocity. Our work shows that joint commitments have utility beyond coordination problems, which could explain their prevalence. The proposed link between joint commitments and reputation could also explain why some joint commitments are pointedly public, like wedding vows. A reputation-based mechanism might have been particularly relevant in our distant past, in which no institutions existed to enforce commitments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.06898v2</guid>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <category>cs.NE</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcus Krellner, The Anh Han</dc:creator>
    </item>
  </channel>
</rss>
