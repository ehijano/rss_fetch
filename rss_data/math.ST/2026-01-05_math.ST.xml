<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 05 Jan 2026 05:00:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Characterizing Finite-Dimensional Posterior Marginals in High-Dimensional GLMs via Leave-One-Out</title>
      <link>https://arxiv.org/abs/2601.00091</link>
      <description>arXiv:2601.00091v1 Announce Type: new 
Abstract: We investigate Bayes posterior distributions in high-dimensional generalized linear models (GLMs) under the proportional asymptotics regime, where the number of features and samples diverge at a comparable rate. Specifically, we characterize the limiting behavior of finite-dimensional marginals of the posterior. We establish that the posterior does not contract in this setting. Yet, the finite-dimensional posterior marginals converge to Gaussian tilts of the prior, where the mean of the Gaussian depends on the true signal coordinates of interest. Notably, the effect of the prior survives even in the limit of large samples and dimensions. We further characterize the behavior of the posterior mean and demonstrate that the posterior mean can strictly outperform the maximum likelihood estimate in mean-squared error in natural examples. Importantly, our results hold regardless of the sparsity level of the underlying signal. On the technical front, we introduce leave-one-out strategies for studying these marginals that may be of independent interest for analyzing low-dimensional functionals of high-dimensional signals in other Bayesian inference problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.00091v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manuel S\'aenz, Pragya Sur</dc:creator>
    </item>
    <item>
      <title>Geometric extremal graphical models and coefficients of extremal dependence on block graphs</title>
      <link>https://arxiv.org/abs/2601.00239</link>
      <description>arXiv:2601.00239v1 Announce Type: new 
Abstract: We introduce the concept of geometric extremal graphical models, which are defined through the gauge function of the limit set obtained from suitably scaled random vectors in light-tailed margins. For block graphs, we prove results relating to the propagation of various extremal dependence coefficients along the graph. A particular focus is placed on coefficients that link to the framework of conditional extreme value theory, which are especially interesting when variables do not all attain their most extreme values simultaneously. We also consider results related to the case when variables do exhibit joint extreme behaviour. Through the recent translation of the geometric approach for multivariate extremes to a statistical modelling framework, geometric extremal graphical models, and results relating to them, pave the way for an approach to modelling of high dimensional extremes with complex extremal dependence structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.00239v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ioannis Papastathopoulos, Jennifer Wadsworth</dc:creator>
    </item>
    <item>
      <title>Sparse Tucker Decomposition and Graph Regularization for High-Dimensional Time Series Forecasting</title>
      <link>https://arxiv.org/abs/2601.00377</link>
      <description>arXiv:2601.00377v1 Announce Type: new 
Abstract: Existing methods of vector autoregressive model for multivariate time series analysis make use of low-rank matrix approximation or Tucker decomposition to reduce the dimension of the over-parameterization issue. In this paper, we propose a sparse Tucker decomposition method with graph regularization for high-dimensional vector autoregressive time series. By stacking the time-series transition matrices into a third-order tensor, the sparse Tucker decomposition is employed to characterize important interactions within the transition third-order tensor and reduce the number of parameters. Moreover, the graph regularization is employed to measure the local consistency of the response, predictor and temporal factor matrices in the vector autoregressive model.The two proposed regularization techniques can be shown to more accurate parameters estimation. A non-asymptotic error bound of the estimator of the proposed method is established, which is lower than those of the existing matrix or tensor based methods. A proximal alternating linearized minimization algorithm is designed to solve the resulting model and its global convergence is established under very mild conditions. Extensive numerical experiments on synthetic data and real-world datasets are carried out to verify the superior performance of the proposed method over existing state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.00377v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sijia Xia, Michael K. Ng, Xiongjun Zhang</dc:creator>
    </item>
    <item>
      <title>Counterfactual Spaces</title>
      <link>https://arxiv.org/abs/2601.00507</link>
      <description>arXiv:2601.00507v1 Announce Type: new 
Abstract: We mathematically axiomatise the stochastics of counterfactuals, by introducing two related frameworks, called counterfactual probability spaces and counterfactual causal spaces, which we collectively term counterfactual spaces. They are, respectively, probability and causal spaces whose underlying measurable spaces are products of world-specific measurable spaces. In contrast to more familiar accounts of counterfactuals founded on causal models, we do not view interventions as a necessary component of a theory of counterfactuals. As an alternative to Pearl's celebrated ladder of causation, we view counterfactuals and interventions are orthogonal concepts, respectively mathematised in counterfactual probability spaces and causal spaces. The two concepts are then combined to form counterfactual causal spaces. At the heart of our theory is the notion of shared information between the worlds, encoded completely within the probability measure and causal kernels, and whose extremes are characterised by independence and synchronisation of worlds. Compared to existing frameworks, counterfactual spaces enable the mathematical treatment of a strictly broader spectrum of counterfactuals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.00507v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junhyung Park, Fanny Yang, Thomas Icard</dc:creator>
    </item>
    <item>
      <title>Asymptotic Distribution-Free Tests for Ultra-high Dimensional Parametric Regressions via Projected Empirical Processes and $p$-value Combination</title>
      <link>https://arxiv.org/abs/2601.00541</link>
      <description>arXiv:2601.00541v1 Announce Type: new 
Abstract: This paper develops a novel methodology for testing the goodness-of-fit of sparse parametric regression models based on projected empirical processes and p-value combination, where the covariate dimension may substantially exceed the sample size. In such ultra-high dimensional settings, traditional empirical process-based tests often fail due to the curse of dimensionality or their reliance on the asymptotic linearity and normality of parameter estimators--properties that may not hold under ultra-high dimensional scenarios. To overcome these challenges, we first extend the classic martingale transformation to ultra-high dimensional settings under mild conditions and construct a Cramer-von Mises type test based on a martingale-transformed, projected residual-marked empirical process for any projection on the unit sphere. The martingale transformation renders this projected test asymptotically distribution-free and enables us to derive its limiting distribution using only standard convergence rates of parameter estimators. While the projected test is consistent for almost all projections on the unit sphere under mild conditions, it may still suffer from power loss for specific projections. Therefore, we further employ powerful p-value combination procedures, such as the Cauchy combination, to aggregate p-values across multiple projections, thereby enhancing overall robustness. Furthermore, recognizing that empirical process-based tests excel at detecting low-frequency signals while local smoothing tests are generally superior for high-frequency alternatives, we propose a novel hybrid test that aggregates both approaches using Cauchy combination. The resulting hybrid test is powerful against both low-frequency and high-frequency alternatives. $\cdots$</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.00541v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Falong Tan, Shan Tang, Lixing Zhu</dc:creator>
    </item>
    <item>
      <title>Exploration in the Limit</title>
      <link>https://arxiv.org/abs/2601.00084</link>
      <description>arXiv:2601.00084v1 Announce Type: cross 
Abstract: In fixed-confidence best arm identification (BAI), the objective is to quickly identify the optimal option while controlling the probability of error below a desired threshold. Despite the plethora of BAI algorithms, existing methods typically fall short in practical settings, as stringent exact error control requires using loose tail inequalities and/or parametric restrictions. To overcome these limitations, we introduce a relaxed formulation that requires valid error control asymptotically with respect to a minimum sample size. This aligns with many real-world settings that often involve weak signals, high desired significance, and post-experiment inference requirements, all of which necessitate long horizons. This allows us to achieve tighter optimality, while better handling flexible nonparametric outcome distributions and fully leveraging individual-level contexts. We develop a novel asymptotic anytime-valid confidence sequences over arm indices, and we use it to design a new BAI algorithm for our asymptotic framework. Our method flexibly incorporates covariates for variance reduction and ensures approximate error control in fully nonparametric settings. Under mild convergence assumptions, we provide asymptotic bounds on the sample complexity and show the worst-case sample complexity of our approach matches the best-case sample complexity of Gaussian BAI under exact error guarantees and known variances. Experiments suggest our approach reduces average sample complexities while maintaining error control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.00084v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brian M. Cho, Nathan Kallus</dc:creator>
    </item>
    <item>
      <title>An exact unbiased semi-parametric L2 quasi-likelihood framework, complete in the presence of ties</title>
      <link>https://arxiv.org/abs/2601.00188</link>
      <description>arXiv:2601.00188v1 Announce Type: cross 
Abstract: Maximum likelihood style estimators possesses a number of ideal characteristics, but require prior identification of the distribution of errors to ensure exact unbiasedness. Independent of the focus of the primary statistical analysis, the estimation of a covariance matrix \(S^{P \times P}\approx \Sigma^{P \times P}\) must possess a specific structure and regularity constraints. The need to estimate a linear Gaussian covariance models appear in various applications as a formal precondition for scientific investigation and predictive analytics. In this work, we construct an \(\ell_{2}\)-norm based quasi-likelihood framework, identified by binomial comparisons between all pairs \(X_{n},Y_{n}, \forall {n}\). Our work here focuses upon the quasi-likelihood basis for estimation of an exactly unbiased linear regression H\'ajek projection, within which the Kemeny metric space is operationalised via Whitney embedding to obtain exact unbiased minimum variance multivariate covariance estimators upon both discrete and continuous random variables (i.e., exact unbiased identification in the presence of ties upon finite samples). While the covariance estimator is inherently useful, expansion of the Wilcoxon rank-sum testing framework to handle multiple covariates with exact unbiasedness upon finite samples is a currently unresolved research problem, as it maintains identification in the presence of linear surjective mappings onto common points: this model space, by definition, expands our likelihood framework into a consistent non-parametric form of the standard general linear model, which we extend to address both unknown heterogeneity and the problem of weak inferential instruments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.00188v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Landon Hurley</dc:creator>
    </item>
    <item>
      <title>Deep learning estimation of the spectral density of functional time series on large domains</title>
      <link>https://arxiv.org/abs/2601.00284</link>
      <description>arXiv:2601.00284v1 Announce Type: cross 
Abstract: We derive an estimator of the spectral density of a functional time series that is the output of a multilayer perceptron neural network. The estimator is motivated by difficulties with the computation of existing spectral density estimators for time series of functions defined on very large grids that arise, for example, in climate compute models and medical scans. Existing estimators use autocovariance kernels represented as large $G \times G$ matrices, where $G$ is the number of grid points on which the functions are evaluated. In many recent applications, functions are defined on 2D and 3D domains, and $G$ can be of the order $G \sim 10^5$, making the evaluation of the autocovariance kernels computationally intensive or even impossible. We use the theory of spectral functional principal components to derive our deep learning estimator and prove that it is a universal approximator to the spectral density under general assumptions. Our estimator can be trained without computing the autocovariance kernels and it can be parallelized to provide the estimates much faster than existing approaches. We validate its performance by simulations and an application to fMRI images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.00284v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Neda Mohammadi, Soham Sarkar, Piotr Kokoszka</dc:creator>
    </item>
    <item>
      <title>Algebraic Study of Discrete Imsetal Models</title>
      <link>https://arxiv.org/abs/2601.00432</link>
      <description>arXiv:2601.00432v1 Announce Type: cross 
Abstract: The method of imsets, introduced by Studen\'y, provides a geometric and combinatorial description of conditional independence statements. Elementary conditional independence statements over a finite set of discrete random variables correspond to column vectors of a matrix generating a polyhedral cone, and the associated toric ideals encode algebraic relations among these statements. In this paper, we study discrete probability distributions on sets of three and four random variables, including both binary variables and combinations of binary and ternary variables. We investigate the structure of conditional independence ideals arising from elementary and non-elementary CI relations and analyze the algebraic properties of imsetal models induced by faces of the elementary imset cone. Our results highlight connections between combinatorial CI relations, their associated ideals, and the geometry of imset cones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.00432v1</guid>
      <category>math.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amira Alkeswani</dc:creator>
    </item>
    <item>
      <title>Diffusion on the circle and a stochastic correlation model</title>
      <link>https://arxiv.org/abs/2412.06343</link>
      <description>arXiv:2412.06343v4 Announce Type: replace 
Abstract: We develop diffusion models for time-varying correlation using stochastic processes defined on the unit circle. Specifically, we study Brownian motion on the circle and the von Mises diffusion, and propose their use as continuous-time models for correlation dynamics. The von Mises process, introduced by Kent (1975) as a characterization of the von Mises distribution in circular statistics, does not have a known closed-form transition density, which has limited its use in likelihood-based inference. We derive an accurate analytical approximation to the transition density of the von Mises diffusion, enabling practical likelihood-based estimation. We study inference for discretely observed circular diffusions, establish consistency and asymptotic normality of the resulting estimators, and propose a stochastic correlation model for financial applications. The methodology is illustrated through simulation studies and empirical applications to equity-foreign exchange market data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06343v4</guid>
      <category>math.ST</category>
      <category>q-fin.MF</category>
      <category>stat.TH</category>
      <pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sourav Majumdar, Arnab Kumar Laha</dc:creator>
    </item>
    <item>
      <title>Logarithmic Accuracy in Importance Sampling via Large Deviations</title>
      <link>https://arxiv.org/abs/2509.00343</link>
      <description>arXiv:2509.00343v2 Announce Type: replace 
Abstract: Importance sampling (IS) is a widely used simulation method for estimating rare event probabilities. In IS, the relative variance of an estimator is the most common measure of estimator accuracy, and the focus of existing literature is on constructing an importance measure under which the relative variance of the estimator grows sub-exponentially as the parameter increases. In practice, constructing such an estimator is not easy. In this work, we study the behavior of IS estimators under an importance measure which is not necessarily optimal using large deviations theory. This provides new insights into asymptotic efficiency of IS estimators and the required sample size. Based on the study, we also propose new diagnostics of IS for rare event simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00343v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julie Choi, Peter Glynn</dc:creator>
    </item>
    <item>
      <title>A multivariate extension of Azadkia-Chatterjee's rank coefficient</title>
      <link>https://arxiv.org/abs/2512.07443</link>
      <description>arXiv:2512.07443v2 Announce Type: replace 
Abstract: The Azadkia-Chatterjee coefficient is a rank-based measure of dependence between a random variable $Y \in \mathbb{R}$ and a random vector ${\boldsymbol Z} \in \mathbb{R}^{d_Z}$. In this paper, we propose a multivariate extension that measures the dependence between random vectors ${\boldsymbol Y} \in \mathbb{R}^{d_Y}$ and ${\boldsymbol Z} \in \mathbb{R}^{d_Z}$, based on $n$ i.i.d. samples. The proposed coefficient converges almost surely to a limit with the following properties: i) it lies in $[0, 1]$; ii) it is equal to zero if and only if ${\boldsymbol Y}$ and ${\boldsymbol Z}$ are independent; and iii) it is equal to one if and only if ${\boldsymbol Y}$ is almost surely a function of ${\boldsymbol Z}$. Remarkably, the only assumption required by this convergence is that ${\boldsymbol Y}$ is not almost surely a constant vector. We further prove that under the same mild condition and after a proper scaling, this coefficient converges in distribution to a standard normal random variable when ${\boldsymbol Y}$ and ${\boldsymbol Z}$ are independent. This asymptotic normality result allows us to construct a Wald-type hypothesis test of independence based on this coefficient. To compute this coefficient, we propose a merge sort based algorithm that runs in $O(n (\log n)^{d_Y})$. Finally, we show that it can be used to measure the conditional dependence between ${\boldsymbol Y}$ and ${\boldsymbol Z}$ conditional on a third random vector ${\boldsymbol X}$, and prove that the measure is monotonic with respect to the deviation from an independence distribution under certain model restrictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07443v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenjie Huang, Zonghan Li, Yuhao Wang</dc:creator>
    </item>
    <item>
      <title>A Sieve M-Estimator for Entropic Optimal Transport</title>
      <link>https://arxiv.org/abs/2512.21981</link>
      <description>arXiv:2512.21981v3 Announce Type: replace 
Abstract: The entropically regularized optimal transport problem between probability measures on compact Euclidean subsets can be represented as an information projection with moment inequality constraints. This allows its Fenchel dual to be approximated by a sequence of convex, finite-dimensional problems using sieve methods, enabling tractable estimation of the primal value and dual optimizers from samples. Assuming only continuity of the cost function, I establish almost sure consistency of these estimators. I derive a finite-sample convergence rate for the primal value estimator, showing logarithmic dependence on sieve complexity, and quantify uncertainty for the dual optimal value estimator via matching stochastic bounds involving suprema of centered Gaussian processes. These results provide the first statistical guarantees for sieve-based estimators of entropic optimal transport, extending beyond the empirical Sinkhorn approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21981v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rami V. Tabri</dc:creator>
    </item>
    <item>
      <title>Distributed Sparse Linear Regression under Communication Constraints</title>
      <link>https://arxiv.org/abs/2301.04022</link>
      <description>arXiv:2301.04022v2 Announce Type: replace-cross 
Abstract: In multiple domains, statistical tasks are performed in distributed settings, with data split among several end machines that are connected to a fusion center. In various applications, the end machines have limited bandwidth and power, and thus a tight communication budget. In this work we focus on distributed learning of a sparse linear regression model, under severe communication constraints. We propose several two round distributed schemes, whose communication per machine is sublinear in the data dimension. In our schemes, individual machines compute debiased lasso estimators, but send to the fusion center only very few values. On the theoretical front, we analyze one of these schemes and prove that with high probability it achieves exact support recovery at low signal to noise ratios, where individual machines fail to recover the support. We show in simulations that our scheme works as well as, and in some cases better, than more communication intensive approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.04022v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rodney Fonseca, Boaz Nadler</dc:creator>
    </item>
    <item>
      <title>No-prior Bayes reIMagined: probabilistic approximations of inferential models</title>
      <link>https://arxiv.org/abs/2503.19748</link>
      <description>arXiv:2503.19748v3 Announce Type: replace-cross 
Abstract: When prior information is lacking, the go-to strategy for probabilistic inference is to combine a "default prior" and the likelihood via Bayes's theorem. Objective Bayes, (generalized) fiducial inference, etc. fall under this umbrella. This construction is natural, but the corresponding posterior distributions generally only offer limited, approximately valid uncertainty quantification. The present paper takes a reimagined approach that yields posterior distributions with stronger reliability properties. The proposed construction starts with an inferential model (IM), one that takes the mathematical form of a data-driven possibility measure and features exactly valid uncertainty quantification, and then returns a so-called inner probabilistic approximation thereof. This inner probabilistic approximation inherits many of the original IM's desirable properties, including credible sets with exact coverage and asymptotic efficiency. The approximation also agrees with the familiar Bayes/fiducial solution in applications where the model has a group invariance structure. A Monte Carlo method for evaluating the probabilistic approximation is presented, along with numerical illustrations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19748v3</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan Martin</dc:creator>
    </item>
    <item>
      <title>Nonparametric Instrumental Variable Inference with Many Weak Instruments</title>
      <link>https://arxiv.org/abs/2505.07729</link>
      <description>arXiv:2505.07729v3 Announce Type: replace-cross 
Abstract: We study inference on linear functionals in the nonparametric instrumental variable (NPIV) problem with a discretely-valued instrument under a many-weak-instruments asymptotic regime, where the number of instrument values grows with the sample size. A key motivating example is estimating long-term causal effects in a new experiment with only short-term outcomes, using past experiments to instrument for the effect of short- on long-term outcomes. Here, the assignment to a past experiment serves as the instrument: we have many past experiments but only a limited number of units in each. Since the structural function is nonparametric but constrained by only finitely many moment restrictions, point identification typically fails. To address this, we consider linear functionals of the minimum-norm solution to the moment restrictions, which is always well-defined. As the number of instrument levels grows, these functionals define an approximating sequence to a target functional, replacing point identification with a weaker asymptotic notion suited to discrete instruments. Extending the Jackknife Instrumental Variable Estimator (JIVE) beyond the classical parametric setting, we propose npJIVE, a nonparametric estimator for solutions to linear inverse problems with many weak instruments. We construct automatic debiased machine learning estimators for linear functionals of both the structural function and its minimum-norm projection, and establish their efficiency in the many-weak-instruments regime. To do so, we develop a general semiparametric efficiency theory for regular estimators under weak identification and many-weak-instrument asymptotics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07729v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lars van der Laan, Nathan Kallus, Aur\'elien Bibaut</dc:creator>
    </item>
    <item>
      <title>Bringing Closure to False Discovery Rate Control: A General Principle for Multiple Testing</title>
      <link>https://arxiv.org/abs/2509.02517</link>
      <description>arXiv:2509.02517v2 Announce Type: replace-cross 
Abstract: We present a novel necessary and sufficient principle for multiple testing methods controlling an expected loss. This principle asserts that every such multiple testing method is a special case of a general closed testing procedure based on e-values. It generalizes the Closure Principle, known to underlie all methods controlling familywise error and tail probabilities of false discovery proportions, to a large class of error rates -- in particular to the false discovery rate (FDR). By writing existing methods as special cases of this procedure, we can achieve uniform improvements, as we demonstrate for the e-Benjamini-Hochberg and the Benjamini-Yekutieli procedures, and the self-consistent method of Su (2018). We also show that methods derived using our novel e-Closure Principle generally control their error rate not just for one rejected set, but simultaneously over many, allowing post hoc flexibility for the researcher.
  Moreover, we show that because all multiple testing methods for all error metrics are derived from the same procedure, researchers may even choose the error metric post hoc. Under certain conditions, this flexibility even extends to post hoc choice of the nominal error rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02517v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyu Xu, Aldo Solari, Lasse Fischer, Rianne de Heide, Aaditya Ramdas, Jelle Goeman</dc:creator>
    </item>
  </channel>
</rss>
