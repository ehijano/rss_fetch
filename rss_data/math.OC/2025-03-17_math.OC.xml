<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Mar 2025 04:00:34 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Two Innovations in Inexact Augmented Lagrangian Methods for Convex Optimization</title>
      <link>https://arxiv.org/abs/2503.11809</link>
      <description>arXiv:2503.11809v1 Announce Type: new 
Abstract: This paper presents two new techniques relating to inexact solution of subproblems in augmented Lagrangian methods for convex programming. The first involves combining a relative error criterion for solution of the subproblems with over- or under-relaxation of the multiplier update step. In one interpretation of our proposed iterative scheme, a predetermined amount of relaxation effects the criterion for an acceptably accurate solution value. Alternatively, the amount of multiplier step relaxation can be adapted to the accuracy of the subproblem subject to a viability test employing the discriminant of a certain quadratic function. The second innovation involves solution of augmented Lagrangian subproblems for problems posed in standard Fenchel-Rockafellar form. We show that applying alternating minimization to this subproblem, as in the first two steps of the ADMM, is equivalent to executing the classical proximal gradient method on a dual formulation of the subproblem. By substituting more sophisticated variants of the proximal gradient method for the classical one, it is possible to construct new ADMM-like methods with better empirical performance than using ordinary alternating minimization within an inexact augmented Lagrangian framework. The paper concludes by describing some computational experiments exploring using these two innovations, both separately and jointly, to solve LASSO problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11809v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan Eckstein, Chang Yu</dc:creator>
    </item>
    <item>
      <title>Adaptive Stochastic Gradient Descents on Manifolds with an Application on Weighted Low-Rank Approximation</title>
      <link>https://arxiv.org/abs/2503.11833</link>
      <description>arXiv:2503.11833v1 Announce Type: new 
Abstract: We prove a convergence theorem for stochastic gradient descents on manifolds with adaptive learning rate and apply it to the weighted low-rank approximation problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11833v1</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peiqi Yang, Conglong Xu, Hao Wu</dc:creator>
    </item>
    <item>
      <title>Entropic optimal transport with congestion aversion Application to relocation of drones</title>
      <link>https://arxiv.org/abs/2503.11843</link>
      <description>arXiv:2503.11843v1 Announce Type: new 
Abstract: We present a mathematical framework for tempo-spatial entropic optimal transport, motivated by the problem of efficiently routing drones back to logistics centers. To address collision risk, we incorporate a convex penalty term into the transport model. We propose the Sinkhorn-Frank-Wolfe algorithm, a numerically efficient method with theoretical convergence guarantees, and demonstrate its effectiveness through experiments on synthetic datasets. Our approach provides a foundation for optimizing large-scale autonomous drone logistics while ensuring safe and efficient transportation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11843v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anna Kazeykina, Zhenjie Ren, Xiaozhen Wang, Yufei Zhang</dc:creator>
    </item>
    <item>
      <title>Quantization Of Probability Measures In Maximum~Mean~Discrepancy Distance</title>
      <link>https://arxiv.org/abs/2503.11868</link>
      <description>arXiv:2503.11868v1 Announce Type: new 
Abstract: Accurate approximation of probability measures is essential in numerical applications. This paper explores the quantization of probability measures using the maximum mean discrepancy (MMD) distance as a guiding metric. We first investigate optimal approximations by determining the best weights, followed by addressing the problem of optimal facility locations.
  To facilitate efficient computation, we reformulate the nonlinear objective as expectations over a product space, enabling the use of stochastic approximation methods. For the Gaussian kernel, we derive closed-form expressions to develop a deterministic optimization approach. By integrating stochastic approximation with deterministic techniques, our framework achieves precise and efficient quantization of continuous distributions, with significant implications for machine learning and signal processing applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11868v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zahra Mehraban, Alois Pichler</dc:creator>
    </item>
    <item>
      <title>Controllability of degenerate parabolic equations in non-cylindrical domains</title>
      <link>https://arxiv.org/abs/2503.11929</link>
      <description>arXiv:2503.11929v1 Announce Type: new 
Abstract: This paper is devoted to a study of the controllability for one-dimensional degenerate parabolic equations in time-dependent domains. First, we demonstrate that the problem is well-posed in appropriate weighted function spaces. To address the controllability problem, we establish a Carleman estimate for degenerate parabolic equations in non-cylindrical domains. The proof relies on the selection of weighted functions and Hardy-type inequalities. As a consequence, we derive an inequality for the adjoint problem, which enables us to obtain controllability results and establish a cost estimate for the control function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11929v1</guid>
      <category>math.OC</category>
      <category>math.AP</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lingyang Liu, Hang Gao</dc:creator>
    </item>
    <item>
      <title>Local convergence analysis of a stabilized sequential quadratic programming method for optimization problems in Banach spaces</title>
      <link>https://arxiv.org/abs/2503.11998</link>
      <description>arXiv:2503.11998v1 Announce Type: new 
Abstract: This paper presents a stabilized sequential quadratic programming (SQP) method for solving optimization problems in Banach spaces. The optimization problem considered in this study has a general form that enables us to represent various types of optimization problems. Several SQP methods have been proposed for optimization problems in Banach spaces with specific structures; however, research on the local analysis of SQP-type methods for general problems, such as those considered in this study, is limited. We focus on the local behavior of the proposed stabilized SQP method and prove its local quadratic convergence under reasonable assumptions, without including any constraint qualifications. Finally, numerical experiments are performed to confirm the theoretical properties shown in the local analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11998v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuya Yamakawa</dc:creator>
    </item>
    <item>
      <title>Safety for Time-Varying Parameterized Sets Using Control Barrier Function Methods</title>
      <link>https://arxiv.org/abs/2503.12003</link>
      <description>arXiv:2503.12003v1 Announce Type: new 
Abstract: A fundamental and classical problem in mobile autonomous systems is maintaining the safety of autonomous agents during deployment. Prior literature has presented techniques using control barrier functions (CBFs) to achieve this goal. These prior techniques utilize CBFs to keep an isolated point in state space away from the unsafe set. However, various situations require a non-singleton set of states to be kept away from an unsafe set. Prior literature has addressed this problem using nonsmooth CBF methods, but no prior work has solved this problem using only "smooth" CBF methods. This paper addresses this gap by presenting a novel method of applying CBF methods to non-singleton parameterized convex sets. The method ensures differentiability of the squared distance function between ego and obstacle sets by leveraging a form of the log-sum-exp function to form strictly convex, arbitrarily tight overapproximations of these sets. Safety-preserving control inputs can be computed via convex optimization formulations. The efficacy of our results is demonstrated through multi-agent simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12003v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Usevitch, Jackson Sahleen</dc:creator>
    </item>
    <item>
      <title>A strongly polynomial-time algorithm for the general linear programming problem</title>
      <link>https://arxiv.org/abs/2503.12041</link>
      <description>arXiv:2503.12041v1 Announce Type: new 
Abstract: This article presents a strongly polynomial-time algorithm for the general linear programming problem. This algorithm is an implicit reduction procedure that works as follows. Primal and dual problems are combined into a special system of linear equations constrained by complementarity relations and non-negative variables. Each iteration of the algorithm consists of applying a pair of complementary Gauss-Jordan pivoting operations, guided by a necessary-condition lemma. The algorithm requires no more than k+n iterations, as there are only k+n complementary pairs of columns to compare one-pair-at-a-time, where k is the number of constraints and n is the number of variables of given general linear programming problem. Numerical illustration is given that includes an instance of a classical problem of Klee and Minty and a problem of Beale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12041v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Samuel Awoniyi</dc:creator>
    </item>
    <item>
      <title>A stochastic maximum principle of mean-field type with monotonicity conditions</title>
      <link>https://arxiv.org/abs/2503.12158</link>
      <description>arXiv:2503.12158v1 Announce Type: new 
Abstract: The objective of this paper is to weaken the Lipschitz condition to a monotonicity condition and to study the corresponding Pontryagin stochastic maximum principle (SMP) for a mean-field optimal control problem under monotonicity conditions.The dynamics of the controlled state process is governed by a mean-field stochastic differential equation (SDE) whose coefficients depend not only on the control, the controlled state process itself but also on its law, and in particular, these coefficients satisfy the monotonicity condition with respect to both the controlled state process and its distribution. The associated cost functional is also of mean-field type. Under the assumption of a convex control domain we derive the SMP, which provides a necessary optimality condition for control processes. Under additional convexity assumptions on the Hamiltonian, we further prove that this necessary condition is also a sufficient one. To achieve this, we first address the challenges related to the existence and the uniqueness of solutions for mean-field backward stochastic differential equations and mean-field SDEs whose coefficients satisfy monotonicity conditions with respect to both the solution as well as its distribution. On the other hand we also construct several illustrative examples demonstrating the generality of our results compared to existing literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12158v1</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Bowen He, Juan Li, Zhanxin Li</dc:creator>
    </item>
    <item>
      <title>A Single-loop Proximal Subgradient Algorithm for A Class Structured Fractional Programs</title>
      <link>https://arxiv.org/abs/2503.12176</link>
      <description>arXiv:2503.12176v1 Announce Type: new 
Abstract: In this paper, we investigate a class of nonconvex and nonsmooth fractional programming problems, where the numerator composed of two parts: a convex, nonsmooth function and a differentiable, nonconvex function, and the denominator consists of a convex, nonsmooth function composed of a linear operator. These structured fractional programming problems have broad applications, including CT reconstruction, sparse signal recovery, the single-period optimal portfolio selection problem and standard Sharpe ratio minimization problem. We develop a single-loop proximal subgradient algorithm that alleviates computational complexity by decoupling the evaluation of the linear operator from the nonsmooth component. We prove the global convergence of the proposed single-loop algorithm to an exact lifted stationary point under the Kurdyka-\L ojasiewicz assumption. Additionally, we present a practical variant incorporating a nonmonotone line search to improve computational efficiency. Finally, through extensive numerical simulations, we showcase the superiority of the proposed approach over the existing state-of-the-art methods for three applications: $L_{1}/S_{\kappa}$ sparse signal recovery, limited-angle CT reconstruction, and optimal portfolio selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12176v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Deren Han, Min Tao, Zihao Xia</dc:creator>
    </item>
    <item>
      <title>Multiple-response agents: Fast, feasible, approximate primal recovery for dual optimization methods</title>
      <link>https://arxiv.org/abs/2503.12221</link>
      <description>arXiv:2503.12221v1 Announce Type: new 
Abstract: We consider the problem of minimizing the sum of agent functions subject to affine coupling constraints. Dual methods are attractive for such problems because they allow the agent-level subproblems to be solved in parallel. However, achieving primal feasibility with dual methods is a challenge; it can take many iterations to find sufficiently precise prices that recover a primal feasible solution, and even with exact prices primal feasibility is not guaranteed, unless special conditions like strict convexity hold. This behavior can limit the usefulness of dual decomposition methods. To overcome this limitation, we propose a novel primal recovery method, multiple-response agents (MRA), that is able to rapidly reduce primal infeasibility, tolerating some degree of suboptimality, and can be used with any dual algorithm. Rather than returning a single primal response to each price query, MRA requires agents to generate multiple primal responses, each of which has bounded suboptimality. These multiple responses can be computed in parallel, so there is no increase in the wall clock time of the underlying dual algorithm. Then a convex combination of the multiple responses is formed by minimizing the sum of the primal and complementary slackness residuals. We test MRA using both a price localization method and a dual subgradient method and show that it typically converges to a feasible, approximate solution in a few tens of iterations. Moreover, hyperparameters can be tuned to control the trade-off among speed, computational budget, and degree of suboptimality of the feasible solutions returned.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12221v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tetiana Parshakova, Yicheng Bai, Garrett van Ryzin, Stephen Boyd</dc:creator>
    </item>
    <item>
      <title>Generalized transition uncertainties in constrained Markov decision processes</title>
      <link>https://arxiv.org/abs/2503.12238</link>
      <description>arXiv:2503.12238v1 Announce Type: new 
Abstract: We examine a constrained Markov decision process under uncertain transition probabilities, with the uncertainty modeled as deviations from observed transition probabilities. We construct the uncertainty set associated with the deviations using polyhedral and second-order cone constraints and employ a robust optimization framework. We demonstrate that each inner optimization problem of the robust model can be equivalently transformed into a second-order cone programming problem. Using strong duality arguments, we show that the resulting robust problem can be equivalently reformulated into a non-convex programming problem that includes bilinear and second-order cone constraints. In the numerical experiments, we study a machine replacement problem and explore potential sources of uncertainty in the transition probabilities. We examine how the optimal values and solutions differ as we vary the feasible region of the uncertainty set, considering only polyhedral constraints and a combination of polyhedral and second-order cone constraints. Furthermore, we analyze the impact of the number of states, the discount factor, and variations in the feasible region of the uncertainty set on the optimal values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12238v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>V Varagapriya</dc:creator>
    </item>
    <item>
      <title>Towards Optimal Offline Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2503.12283</link>
      <description>arXiv:2503.12283v1 Announce Type: new 
Abstract: We study offline reinforcement learning problems with a long-run average reward objective. The state-action pairs generated by any fixed behavioral policy thus follow a Markov chain, and the {\em empirical} state-action-next-state distribution satisfies a large deviations principle. We use the rate function of this large deviations principle to construct an uncertainty set for the unknown {\em true} state-action-next-state distribution. We also construct a distribution shift transformation that maps any distribution in this uncertainty set to a state-action-next-state distribution of the Markov chain generated by a fixed evaluation policy, which may differ from the unknown behavioral policy. We prove that the worst-case average reward of the evaluation policy with respect to all distributions in the shifted uncertainty set provides, in a rigorous statistical sense, the least conservative estimator for the average reward under the unknown true distribution. This guarantee is available even if one has only access to one single trajectory of serially correlated state-action pairs. The emerging robust optimization problem can be viewed as a robust Markov decision process with a non-rectangular uncertainty set. We adapt an efficient policy gradient algorithm to solve this problem. Numerical experiments show that our methods compare favorably against state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12283v1</guid>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mengmeng Li, Daniel Kuhn, Tobias Sutter</dc:creator>
    </item>
    <item>
      <title>Intraday Battery Dispatch for Hybrid Renewable Energy Assets</title>
      <link>https://arxiv.org/abs/2503.12305</link>
      <description>arXiv:2503.12305v1 Announce Type: new 
Abstract: We develop a mathematical model for intraday dispatch of co-located wind-battery energy assets. Focusing on the primary objective of firming grid-side actual production vis-a-vis the preset day-ahead hourly generation targets, we conduct a comprehensive study of the resulting stochastic control problem across different firming formulations and wind generation dynamics. Among others, we provide a closed-form solution in the special case of a quadratic objective and linear dynamics, as well as design a novel adaptation of a Gaussian Process-based Regression Monte Carlo algorithm for our setting. Extensions studied include an asymmetric loss function for peak shaving, capturing the cost of battery cycling, and the role of battery duration. In the applied portion of our work, we calibrate our model to a collection of 140+ wind-battery assets in Texas, benchmarking the economic benefits of firming based on outputs of a realistic unit commitment and economic dispatch solver.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12305v1</guid>
      <category>math.OC</category>
      <category>q-fin.CP</category>
      <category>q-fin.MF</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thiha Aung, Mike Ludkovski</dc:creator>
    </item>
    <item>
      <title>A Review on Intermodal Transportation and Decarbonization: An Operations Research Perspective</title>
      <link>https://arxiv.org/abs/2503.12322</link>
      <description>arXiv:2503.12322v1 Announce Type: new 
Abstract: This paper reviews intermodal transportation systems and their role in decarbonizing freight networks from an operations research perspective, analyzing over a decade of studies (2010-2024). We present a chronological analysis of the literature, illustrating how the field evolved over time while highlighting the emergence of new research avenues. We observe a significant increase in research addressing decarbonization since 2018, driven by regulatory pressures and technological advancements. Our integrated analysis is organized around three themes: a) modality, b) sustainability, and c) solution techniques. Key recommendations include the development of multistage stochastic models to better manage uncertainties and disruptions within intermodal transportation systems. Further research could leverage innovative technologies like machine learning and blockchain to improve decision-making and resource use through stakeholder collaboration. Life cycle assessment models are also suggested to better understand emissions across transportation stages and support the transition to alternative energy sources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12322v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Madelaine Martinez Ferguson, Aliza Sharmin, Mustafa Can Camur, Xueping Li</dc:creator>
    </item>
    <item>
      <title>Hybrid Optimization Methods for Parameter Estimation of Reactive Transport Systems</title>
      <link>https://arxiv.org/abs/2503.12469</link>
      <description>arXiv:2503.12469v1 Announce Type: new 
Abstract: This paper presents a hybrid optimization methodology for parameter estimation of reactive transport systems. Using reduced-order advection-diffusion-reaction (ADR) models, the computational requirements of global optimization with dynamic PDE constraints are addressed by combining metaheuristics with gradient-based optimizers. A case study in preparative liquid chromatography shows that the method achieves superior computational efficiency compared to traditional multi-start methods, demonstrating the potential of hybrid strategies to advance parameter estimation in large-scale, dynamic chemical engineering applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12469v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marcus Johan Schytt, Halld\'or Gauti P\'etursson, John Bagterp J{\o}rgensen</dc:creator>
    </item>
    <item>
      <title>Ensemble Kalman-Bucy filtering for nonlinear model predictive control</title>
      <link>https://arxiv.org/abs/2503.12474</link>
      <description>arXiv:2503.12474v1 Announce Type: new 
Abstract: We consider the problem of optimal control for partially observed dynamical systems. Despite its prevalence in practical applications, there are still very few algorithms available, which take uncertainties in the current state estimates and future observations into account. In other words, most current approaches separate state estimation from the optimal control problem. In this paper, we extend the popular ensemble Kalman filter to receding horizon optimal control problems in the spirit of nonlinear model predictive control. We provide an interacting particle approximation to the forward-backward stochastic differential equations arising from Pontryagin's maximum principle with the forward stochastic differential equation provided by the time-continuous ensemble Kalman-Bucy filter equations. The receding horizon control laws are approximated as linear and are continuously updated as in nonlinear model predictive control. We illustrate the performance of the proposed methodology for an inverted pendulum example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12474v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.NA</category>
      <category>stat.CO</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sebastian Reich</dc:creator>
    </item>
    <item>
      <title>A new perspective on Willems' fundamental lemma: Universality of persistently exciting inputs</title>
      <link>https://arxiv.org/abs/2503.12489</link>
      <description>arXiv:2503.12489v1 Announce Type: new 
Abstract: In this letter, we provide new insight into Willems et al.'s fundamental lemma by studying the concept of universal inputs. An input is called universal if, when applied to any controllable system, it leads to input-output data that parametrizes all finite trajectories of the system. By the fundamental lemma, inputs that are persistently exciting of sufficiently high order are universal. The main contribution of this work is to prove the converse. Therefore, universality and persistency of excitation are equivalent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12489v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amir Shakouri, Henk J. van Waarde, M. Kanat Camlibel</dc:creator>
    </item>
    <item>
      <title>Polytope Volume Monitoring Problem: Formulation and Solution via Parametric Linear Program Based Control Barrier Function</title>
      <link>https://arxiv.org/abs/2503.12546</link>
      <description>arXiv:2503.12546v1 Announce Type: new 
Abstract: Motivated by the latest research on feasible space monitoring of multiple control barrier functions (CBFs) as well as polytopic collision avoidance, this paper studies the Polytope Volume Monitoring (PVM) problem, whose goal is to design a control law for inputs of nonlinear systems to prevent the volume of some state-dependent polytope from decreasing to zero. Recent studies have explored the idea of applying Chebyshev ball method in optimization theory to solve the case study of PVM; however, the underlying difficulties caused by nonsmoothness have not been addressed. This paper continues the study on this topic, where our main contribution is to establish the relationship between nonsmooth CBF and parametric optimization theory through directional derivatives for the first time, so as to solve PVM problems more conveniently. In detail, inspired by Chebyshev ball approach, a parametric linear program (PLP) based nonsmooth barrier function candidate is established for PVM, and then, sufficient conditions for it to be a nonsmooth CBF are proposed, based on which a quadratic program (QP) based safety filter with guaranteed feasibility is proposed to address PVM problems. Finally, a numerical simulation example is given to show the efficiency of the proposed safety filter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12546v1</guid>
      <category>math.OC</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shizhen Wu, Jinyang Dong, Xu Fang, Ning Sun, Yongchun Fang</dc:creator>
    </item>
    <item>
      <title>Integration Error Regularization in Direct Optimal Control using Embedded Runge Kutta Methods</title>
      <link>https://arxiv.org/abs/2503.12621</link>
      <description>arXiv:2503.12621v1 Announce Type: new 
Abstract: In order to solve continuous-time optimal control problems, direct methods transcribe the infinite-dimensional problem to a nonlinear program (NLP) using numerical integration methods. In cases where the integration error can be manipulated by the chosen control trajectory, the transcription might produce spurious local NLP solutions as a by-product. While often this issue can be addressed by increasing the accuracy of the integration method, this is not always computationally acceptable, e.g., in the case of embedded optimization. Therefore, alternatively, we propose to estimate the integration error using established embedded Runge-Kutta methods and to regularize this estimate in the NLP cost function, using generalized norms. While this regularization is effective at eliminating spurious solutions, it inherently comes with a loss of optimality of valid solutions. The regularization can be tuned to minimize this loss, using a single parameter that can be intuitively interpreted as the maximum allowable estimated local integration error. In a numerical example based on a system with stiff dynamics, we show how this methodology enables the use of a computationally cheap explicit integration method, achieving a speedup of a factor of 3 compared to an otherwise more suitable implicit method, with a loss of optimality of only 3\%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12621v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jakob Harzer, Jochem De Schutter, Moritz Diehl</dc:creator>
    </item>
    <item>
      <title>Fast filtering of non-Gaussian models using Amortized Optimal Transport Maps</title>
      <link>https://arxiv.org/abs/2503.12633</link>
      <description>arXiv:2503.12633v1 Announce Type: new 
Abstract: In this paper, we present the amortized optimal transport filter (A-OTF) designed to mitigate the computational burden associated with the real-time training of optimal transport filters (OTFs). OTFs can perform accurate non-Gaussian Bayesian updates in the filtering procedure, but they require training at every time step, which makes them expensive. The proposed A-OTF framework exploits the similarity between OTF maps during an initial/offline training stage in order to reduce the cost of inference during online calculations. More precisely, we use clustering algorithms to select relevant subsets of pre-trained maps whose weighted average is used to compute the A-OTF model akin to a mixture of experts. A series of numerical experiments validate that A-OTF achieves substantial computational savings during online inference while preserving the inherent flexibility and accuracy of OTF.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12633v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Al-Jarrah, Bamdad Hosseini, Amirhossein Taghvaei</dc:creator>
    </item>
    <item>
      <title>Custom Solver Generation for Quadratic Objective Second-Order Cone Programs</title>
      <link>https://arxiv.org/abs/2503.12658</link>
      <description>arXiv:2503.12658v1 Announce Type: new 
Abstract: Second-order cone programs (SOCPs) with quadratic objective functions are common in optimal control and other fields. Most SOCP solvers which use interior-point methods are designed for linear objectives and convert quadratic objectives into linear ones via slack variables and extra constraints, despite the computational advantages of handling quadratic objectives directly. In applications like model-predictive control and online trajectory optimization, these SOCPs have known sparsity structures and require rapid solutions. When solving these problems, most solvers use sparse linear algebra routines, which introduce computational overhead and hinder performance. In contrast, custom linear algebra routines can exploit the known sparsity structure of problem data and be significantly faster. This work makes two key contributions: (1) the development of QOCO, an open-source C-based solver for quadratic objective SOCPs, and (2) the introduction of QOCOGEN, an open-source custom solver generator for quadratic objective SOCPs, which generates a solver written in C that leverages custom linear algebra. Both implement a primal-dual interior-point method with Mehrotra's predictor-corrector. Our benchmarks show that QOCO is faster and more robust than many commonly used solvers, and solvers generated by QOCOGEN are significantly faster than QOCO and are free of dynamic memory allocation making them an attractive option for real-time optimization on resource-constrained embedded systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12658v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Govind M Chari, Beh\c{c}et A\c{c}ikme\c{s}e</dc:creator>
    </item>
    <item>
      <title>Exploiting Multistage Optimization Structure in Proximal Solvers</title>
      <link>https://arxiv.org/abs/2503.12664</link>
      <description>arXiv:2503.12664v1 Announce Type: new 
Abstract: This paper presents an efficient structure-exploiting algorithm for multistage optimization problems, implemented as a new backend in the PIQP solver. The proposed method extends existing approaches by supporting full coupling between stages and global decision variables in the cost as well as equality and inequality constraints. The solver leverages a specialized block-tri-diagonal-arrow Cholesky factorization within a proximal interior-point framework to handle the underlying problem structure efficiently. The implementation features automatic structure detection and seamless integration with existing interfaces. Numerical experiments demonstrate significant performance improvements, achieving up to 13x speed-up compared to a generic sparse backend and matching/exceeding the performance of the state-of-the-art specialized solver HPIPM. The solver is particularly effective for applications such as model predictive control, robust scenario optimization, and periodic optimization problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12664v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roland Schwan, Daniel Kuhn, Colin N. Jones</dc:creator>
    </item>
    <item>
      <title>Constrained Optimization From a Control Perspective via Feedback Linearization</title>
      <link>https://arxiv.org/abs/2503.12665</link>
      <description>arXiv:2503.12665v1 Announce Type: new 
Abstract: Tools from control and dynamical systems have proven valuable for analyzing and developing optimization methods. In this paper, we establish rigorous theoretical foundations for using feedback linearization -- a well-established nonlinear control technique -- to solve constrained optimization problems. For equality-constrained optimization, we establish global convergence rates to first-order Karush-Kuhn-Tucker (KKT) points and uncover the close connection between the FL method and the Sequential Quadratic Programming (SQP) algorithm. Building on this relationship, we extend the FL approach to handle inequality-constrained problems. Furthermore, we introduce a momentum-accelerated feedback linearization algorithm that achieves faster convergence, and provides a rigorous convergence guarantee.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12665v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator> Runyu (Cathy),  Zhang, Arvind Raghunathan, Jeff Shamma, Na Li</dc:creator>
    </item>
    <item>
      <title>Intrinsic Successive Convexification: Trajectory Optimization on Smooth Manifolds</title>
      <link>https://arxiv.org/abs/2503.12711</link>
      <description>arXiv:2503.12711v1 Announce Type: new 
Abstract: A fundamental issue at the core of trajectory optimization on smooth manifolds is handling the implicit manifold constraint within the dynamics. The conventional approach is to enforce the dynamic model as a constraint. However, we show this approach leads to significantly redundant operations, as well as being heavily dependent on the state space representation. Specifically, we propose an intrinsic successive convexification methodology for optimal control on smooth manifolds. This so-called iSCvx is then applied to a representative example involving attitude trajectory optimization for a spacecraft subject to non-convex constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12711v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Spencer Kraisler, Mehran Mesbahi, Behcet Acikmese</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Weighted Sample Average Approximation in Contextual Stochastic Optimization</title>
      <link>https://arxiv.org/abs/2503.12747</link>
      <description>arXiv:2503.12747v1 Announce Type: new 
Abstract: Contextual stochastic optimization provides a framework for decision-making under uncertainty incorporating observable contextual information through covariates. We analyze statistical inference for weighted sample average approximation (wSAA), a widely-used method for solving contextual stochastic optimization problems. We first establish central limit theorems for wSAA estimates of optimal values when problems can be solved exactly, characterizing how estimation uncertainty scales with covariate sample size. We then investigate practical scenarios with computational budget constraints, revealing a fundamental tradeoff between statistical accuracy and computational cost as sample sizes increase. Through central limit theorems for budget-constrained wSAA estimates, we precisely characterize this statistical-computational tradeoff. We also develop "over-optimizing" strategies for solving wSAA problems that ensure valid statistical inference. Extensive numerical experiments on both synthetic and real-world datasets validate our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12747v1</guid>
      <category>math.OC</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yanyuan Wang, Xiaowei Zhang</dc:creator>
    </item>
    <item>
      <title>Smoothing Accelerated Proximal Gradient Method with Backtracking for Nonsmooth Multiobjective Optimization</title>
      <link>https://arxiv.org/abs/2503.12915</link>
      <description>arXiv:2503.12915v1 Announce Type: new 
Abstract: For the composite multi-objective optimization problem composed of two nonsmooth terms, a smoothing method is used to overcome the nonsmoothness of the objective function, making the objective function contain at most one nonsmooth term. Then, inspired by the design idea of the aforementioned backtracking strategy, an update rule is proposed by constructing a relationship between an estimation sequence of the Lipschitz constant and a smoothing factor, which results in a backtracking strategy suitable for this problem, allowing the estimation sequence to be updated in a non-increasing manner. On this basis, a smoothing accelerated proximal gradient algorithm based on the backtracking strategy is further proposed. Under appropriate conditions, it is proven that all accumulation points of the sequence generated by this algorithm are weak Pareto optimal solutions. Additionally, the convergence rate of the algorithm under different parameters is established using a utility function. Numerical experiments show that, compared with the subgradient algorithm, the proposed algorithm demonstrates significant advantages in terms of runtime, iteration count, and function evaluations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12915v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huang Chengzhi</dc:creator>
    </item>
    <item>
      <title>Time-Varying Distributed Optimization for A Class of Stochastic Multi-Agent Systems</title>
      <link>https://arxiv.org/abs/2503.12934</link>
      <description>arXiv:2503.12934v1 Announce Type: new 
Abstract: Distributed optimization problems have received much attention due to their privacy preservation, parallel computation, less communication, and strong robustness. This paper presents and studies the time-varying distributed optimization problem for a class of stochastic multi-agent systems for the first time. For this, we initially propose a protocol in the centralized case that allows the tracking error of the agent with respect to the optimal trajectory to be exponentially ultimately bounded in a mean-square sense by stochastic Lyapunov theory. We then generalize this to the distributed case. Therein, the global variable can be accurately estimated in a fixed-time by our proposed estimator. Based on this estimator, we design a new distributed protocol, and the results demonstrate that the tracking error of all agents with respect to the optimal trajectory is exponentially ultimately bound in a mean-square sense by stochastic Lyapunov theory. Finally, simulation experiments are conducted to validate the findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12934v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wan-ying Li, Nan-jing Huang</dc:creator>
    </item>
    <item>
      <title>Solving unbounded optimal control problems with the moment-SOS hierarchy *</title>
      <link>https://arxiv.org/abs/2503.12987</link>
      <description>arXiv:2503.12987v1 Announce Type: new 
Abstract: The behaviour of the moment-sums-of-squares (moment-SOS) hierarchy for polynomial optimal control problems on compact sets has been explored to a large extent. Our contribution focuses on the case of non-compact control sets. We describe a new approach to optimal control problems with unbounded controls, using compactification by partial homogenization, leading to an equivalent infinite dimensional linear program with compactly supported measures. Our results are closely related to the results of a previous approach using DiPerna-Majda measures. However, our work provides a sound proof of the absence of relaxation gap, which was conjectured in the previous work, and thereby enables the design of a moment-sum-of-squares relaxation with guaranteed convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12987v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Karol{\i}na Sehnalov\'a (LAAS-POP), Didier Henrion (LAAS-POP), Milan Korda (LAAS-POP), Martin Kru\v{z}\'ik (UTIA / CAS)</dc:creator>
    </item>
    <item>
      <title>Optimal mixed fleet and charging infrastructure planning to electrify demand responsive feeder services with target CO2 emission constraints</title>
      <link>https://arxiv.org/abs/2503.13085</link>
      <description>arXiv:2503.13085v1 Announce Type: new 
Abstract: Electrifying demand-responsive transport systems need to plan the charging infrastructure carefully, considering the trade-offs of charging efficiency and charging infrastructure costs. Earlier studies assume a fully electrified fleet and overlook the planning issue in the transition period. This study addresses the joint fleet size and charging infrastructure planning for a demand-responsive feeder service under stochastic demand, given a user-defined targeted CO2 emission reduction policy. We propose a bi-level optimization model where the upper-level determines charging station configuration given stochastic demand patterns, whereas the lower-level solves a mixed fleet dial-a-ride routing problem under the CO2 emission and capacitated charging station constraints. An efficient deterministic annealing metaheuristic is proposed to solve the CO2-constrained mixed fleet routing problem. The performance of the algorithm is validated by a series of numerical test instances with up to 500 requests. We apply the model for a real-world case study in Bettembourg, Luxembourg, with different demand and customised CO2 reduction targets. The results show that the proposed method provides a flexible tool for joint charging infrastructure and fleet size planning under different levels of demand and CO2 emission reduction targets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13085v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haruko Nakao, Tai-Yu Ma, Richard D. Connors, Francesco Viti</dc:creator>
    </item>
    <item>
      <title>Semi-Monotone Goldstein Line Search Strategy with Application in Sparse Recovery</title>
      <link>https://arxiv.org/abs/2503.13099</link>
      <description>arXiv:2503.13099v1 Announce Type: new 
Abstract: Line search methods are a prominent class of iterative methods to solve unconstrained minimization problems. These methods produce new iterates utilizing a suitable step size after determining proper directions for minimization. In this paper we propose a semi-monotone line search technique based on the Goldstein quotient for dealing with convex non-smooth optimization problems. The method allows to employ large step sizes away from the optimum thus improving the efficacy compared to standard Goldstein approach. For the presented line search method, we prove global convergence to a stationary point and local R-linear convergence rate in strongly convex cases. We report on some experiments in compressed sensing. By comparison with several state-of-the-art algorithms in the field, we demonstrate the competitive performance of the proposed approach and specifically its high efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13099v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shima Shabani, Michael Breu{\ss}</dc:creator>
    </item>
    <item>
      <title>Actively learning equilibria in Nash games with misleading information</title>
      <link>https://arxiv.org/abs/2503.13167</link>
      <description>arXiv:2503.13167v1 Announce Type: new 
Abstract: We develop an active learning-based scheme to compute equilibria for a population of selfish agents taking part to a GNEP. Specifically, an external observer (or entity) with little knowledge on the multi-agent process at hand, collects sensible data by probing the agents' BR mappings, which are then used to recursively update local parametric estimates of these mappings. Unlike [1], we consider here the more realistic case in which the agents share noisy information with the external entity, being them malicious or simply for protecting their privacy. Inspired by a popular approach in stochastic optimization, we endow the external observer with an inexact proximal scheme for updating the local BR proxies. This technique will prove key to establishing the convergence of our scheme under standard assumptions, thereby enabling the external observer to predict an equilibrium strategy even when relying on masked information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13167v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Barbara Franci, Filippo Fabiani, Alberto Bemporad</dc:creator>
    </item>
    <item>
      <title>A method to determine the minimal null control time of 1D linear hyperbolic balance laws</title>
      <link>https://arxiv.org/abs/2503.13186</link>
      <description>arXiv:2503.13186v1 Announce Type: new 
Abstract: In this paper we introduce a method to find the minimal control time for the null controllability of 1D first-order linear hyperbolic systems by one-sided boundary controls when the coefficients are regular enough.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13186v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Long Hu, Guillaume Olive</dc:creator>
    </item>
    <item>
      <title>A convexity preserving nonconvex regularization for inverse problems under non-Gaussian noise</title>
      <link>https://arxiv.org/abs/2503.13287</link>
      <description>arXiv:2503.13287v1 Announce Type: new 
Abstract: We propose a nonconvexly regularized convex model for linear regression problems under non-Gaussian noise. The cost function of the proposed model is designed with a possibly non-quadratic data fidelity term and a nonconvex regularizer via the generalized Moreau enhancement of a seed convex regularizer. We present sufficient conditions (i) for the cost function of the proposed model to be convex over the entire space, and (ii) for the existence of a minimizer of the proposed model. Under such conditions, we propose a proximal splitting type algorithm with guaranteed convergence to a global minimizer of the proposed model. As an application, we enhance nonconvexly a convex sparsity-promoting regularizer in a scenario of simultaneous declipping and denoising.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13287v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wataru Yata, Keita Kume, Isao Yamada</dc:creator>
    </item>
    <item>
      <title>From Few-Shot Optimal Control to Few-Shot Learning</title>
      <link>https://arxiv.org/abs/2503.13298</link>
      <description>arXiv:2503.13298v1 Announce Type: new 
Abstract: We present an approach to solving unconstrained nonlinear optimal control problems for a broad class of dynamical systems. This approach involves lifting the nonlinear problem to a linear ``super-problem'' on a dual Banach space, followed by a non-standard ``exact'' variational analysis, -- culminating in a descent method that achieves rapid convergence with minimal iterations. We investigate the applicability of this framework to mean-field control and discuss its perspectives for the analysis of information propagation in self-interacting neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13298v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roman Chertovskih, Nikolay Pogodaev, Maxim Staritsyn, A. Pedro Aguiar</dc:creator>
    </item>
    <item>
      <title>Optimal intrinsic formation using exogenous systems</title>
      <link>https://arxiv.org/abs/2503.13359</link>
      <description>arXiv:2503.13359v1 Announce Type: new 
Abstract: This paper investigates the intrinsic formation problem of a multi-agent system using an exogenous system. The problem is formulated as an intrinsic infinite time-horizon linear quadratic optimal control problem, namely, no formation error information is incorporated in the performance index. Convergence to the formation is achieved by utilizing an exogenous system, thus expanding the steady-state formation space of the system. For the forward problem, we provide the existence condition for a nonzero steady state and characterize the steady-state space. For the inverse problem, we design both the input matrix and the exogenous system so that the desired formation can be achieved. Finally, numerical simulations are provided to illustrate the effectiveness of the proposed results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13359v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yueyue Xu, Panpan Zhou, Lin Wang, Xiaoming Hu</dc:creator>
    </item>
    <item>
      <title>Mixtures of ensembles: System separation and identification via optimal transport</title>
      <link>https://arxiv.org/abs/2503.13362</link>
      <description>arXiv:2503.13362v1 Announce Type: new 
Abstract: Crowd dynamics and many large biological systems can be described as populations of agents or particles, which can only be observed on aggregate population level. Identifying the dynamics of agents is crucial for understanding these large systems. However, the population of agents is typically not homogeneous, and thus the aggregate observations consist of the superposition of multiple ensembles each governed by individual dynamics. In this work, we propose an optimal transport framework to jointly separate the population into several ensembles and identify each ensemble's dynamical system, based on aggregate observations of the population. We propose a bi-convex optimization problem, which we solve using a block coordinate descent with convergence guarantees. In numerical experiments, we demonstrate that the proposed approach exhibits close-to-oracle performance also in noisy settings, yielding accurate estimates of both the ensembles and the parameters governing their dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13362v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Filip Elvander, Isabel Haasler</dc:creator>
    </item>
    <item>
      <title>Mixed Small Gain and Phase Theorem: A new view using Scale Relative Graphs</title>
      <link>https://arxiv.org/abs/2503.13367</link>
      <description>arXiv:2503.13367v1 Announce Type: new 
Abstract: We introduce a novel approach to feedback stability analysis for linear time-invariant (LTI) systems, overcoming the limitations of the sectoriality assumption in the small phase theorem. While phase analysis for single-input single-output (SISO) systems is well-established, multi-input multi-output (MIMO) systems lack a comprehensive phase analysis until recent advances introduced with the small-phase theorem.
  A limitation of the small-phase theorem is the sectorial condition, which states that an operator's eigenvalues must lie within a specified angle sector of the complex plane. We propose a framework based on Scaled Relative Graphs (SRGs) to remove this assumption. We derive two main results: a graphical set-based stability condition using SRGs and a small-phase theorem with no sectorial assumption. These results broaden the scope of phase analysis and feedback stability for MIMO systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13367v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eder Baron-Prada, Adolfo Anta, Alberto Padoan, Florian D\"orfler</dc:creator>
    </item>
    <item>
      <title>Decentralized Sensor Network Localization using Matrix-Parametrized Proximal Splittings</title>
      <link>https://arxiv.org/abs/2503.13403</link>
      <description>arXiv:2503.13403v1 Announce Type: new 
Abstract: We present a novel application of a recently-proposed matrix-parametrized proximal splitting method to sensor network localization, the problem of estimating the locations of a set of sensors using only noisy pairwise distance information between the sensors. The decentralized computation required by our approach respects the communication structure between sensors specified by the noisy SNL problem, thereby allowing individual sensors to estimate their location using only local computations and communication with their neighbors. Our proposed method experimentally outperforms a competing method for decentralized computation -- the alternating direction method of multipliers (ADMM) -- with respect to convergence rate and memory use. As an independent methodological contribution, we propose using the Sinkhorn-Knopp algorithm in a completely decentralized manner to construct the matrices which parametrize our proposed splitting method. We show that parameters selected using this method perform similarly to those selected via existing parameter selection methods while requiring far less computation. Unlike centralized interior point solution methods, our first order splitting method allows for efficient warm starting, and we demonstrate improvements in convergence using rough estimates of sensor location to warm start our algorithm. We also find that early termination of the algorithm provides more accurate location estimates than the minimizer of the node-based SDP relaxation of the SNL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13403v1</guid>
      <category>math.OC</category>
      <category>eess.SP</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peter Barkley, Robert L. Bassett</dc:creator>
    </item>
    <item>
      <title>A Smooth Analytical Formulation of Collision Detection and Rigid Body Dynamics With Contact</title>
      <link>https://arxiv.org/abs/2503.11736</link>
      <description>arXiv:2503.11736v1 Announce Type: cross 
Abstract: Generating intelligent robot behavior in contact-rich settings is a research problem where zeroth-order methods currently prevail. A major contributor to the success of such methods is their robustness in the face of non-smooth and discontinuous optimization landscapes that are characteristic of contact interactions, yet zeroth-order methods remain computationally inefficient. It is therefore desirable to develop methods for perception, planning and control in contact-rich settings that can achieve further efficiency by making use of first and second order information (i.e., gradients and Hessians). To facilitate this, we present a joint formulation of collision detection and contact modelling which, compared to existing differentiable simulation approaches, provides the following benefits: i) it results in forward and inverse dynamics that are entirely analytical (i.e. do not require solving optimization or root-finding problems with iterative methods) and smooth (i.e. twice differentiable), ii) it supports arbitrary collision geometries without needing a convex decomposition, and iii) its runtime is independent of the number of contacts. Through simulation experiments, we demonstrate the validity of the proposed formulation as a "physics for inference" that can facilitate future development of efficient methods to generate intelligent contact-rich behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11736v1</guid>
      <category>cs.RO</category>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Onur Beker, Nico G\"urtler, Ji Shi, A. Ren\'e Geist, Amirreza Razmjoo, Georg Martius, Sylvain Calinon</dc:creator>
    </item>
    <item>
      <title>Ranking and Selection with Simultaneous Input Data Collection</title>
      <link>https://arxiv.org/abs/2503.11773</link>
      <description>arXiv:2503.11773v1 Announce Type: cross 
Abstract: In this paper, we propose a general and novel formulation of ranking and selection with the existence of streaming input data. The collection of multiple streams of such data may consume different types of resources, and hence can be conducted simultaneously. To utilize the streaming input data, we aggregate simulation outputs generated under heterogeneous input distributions over time to form a performance estimator. By characterizing the asymptotic behavior of the performance estimators, we formulate two optimization problems to optimally allocate budgets for collecting input data and running simulations. We then develop a multi-stage simultaneous budget allocation procedure and provide its statistical guarantees such as consistency and asymptotic normality. We conduct several numerical studies to demonstrate the competitive performance of the proposed procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11773v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuhao Wang, Enlu Zhou</dc:creator>
    </item>
    <item>
      <title>Formally Proving Invariant Systemic Properties of Control Programs Using Ghost Code and Integral Quadratic Constraints</title>
      <link>https://arxiv.org/abs/2503.11916</link>
      <description>arXiv:2503.11916v1 Announce Type: cross 
Abstract: This paper focuses on formally verifying invariant properties of control programs both at the model and code levels. The physical process is described by an uncertain discrete-time state-space system, where the dependence of the state-space matrix-valued functions defining the system on the uncertainties can be rational. The proposed approaches make use of pointwise integral quadratic constraints (IQCs) to characterize the uncertainties affecting the behavior of the system. Various uncertainties can be characterized by pointwise IQCs, including static linear time-varying perturbations and sector-bounded nonlinearities. Using the IQC framework, a sound overapproximation of the uncertain system, which is expressible at the code level, is constructed. Tools such as Frama-C, ACSL, WP, and an Alt-Ergo plugin are employed to ensure the validity of the state and output invariant properties across both real and float models. The first proposed approach can be used to formally verify (local) invariant properties of the control code. This capability is demonstrated in a couple of examples involving gain-scheduled path-following controllers designed for an uncrewed aircraft system and an autonomous underwater vehicle. The second approach enables the verification of closed-loop invariant properties, i.e., invariant properties of the controlled system as a whole, in both real and float models, while preserving the integrity of the executable controller code. This is achieved by using ghost code attached to the control code for all elements related to the plant model with uncertainties, as the ghost code does not interfere with the executable code. The effectiveness of this approach is demonstrated in two examples on the control of a four-thruster hovercraft and the control of a two-mass rotational system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11916v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elias Khalife, Pierre-Loic Garoche, Mazen Farhood</dc:creator>
    </item>
    <item>
      <title>Non-Normalized Solutions of Generalized Nash Equilibrium in Autonomous Racing</title>
      <link>https://arxiv.org/abs/2503.12002</link>
      <description>arXiv:2503.12002v1 Announce Type: cross 
Abstract: In dynamic games with shared constraints, Generalized Nash Equilibria (GNE) are often computed using the normalized solution concept, which assumes identical Lagrange multipliers for shared constraints across all players. While widely used, this approach excludes other potentially valuable GNE. This paper addresses the limitations of normalized solutions in racing scenarios through three key contributions. First, we highlight the shortcomings of normalized solutions with a simple racing example. Second, we propose a novel method based on the Mixed Complementarity Problem (MCP) formulation to compute non-normalized Generalized Nash Equilibria (GNE). Third, we demonstrate that our proposed method overcomes the limitations of normalized GNE solutions and enables richer multi-modal interactions in realistic racing scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12002v1</guid>
      <category>cs.RO</category>
      <category>cs.GT</category>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark Pustilnik, Francesco Borrelli</dc:creator>
    </item>
    <item>
      <title>Mixed-feature Logistic Regression Robust to Distribution Shifts</title>
      <link>https://arxiv.org/abs/2503.12012</link>
      <description>arXiv:2503.12012v1 Announce Type: cross 
Abstract: Logistic regression models are widely used in the social and behavioral sciences and in high-stakes domains, due to their simplicity and interpretability properties. At the same time, such domains are permeated by distribution shifts, where the distribution generating the data changes between training and deployment. In this paper, we study a distributionally robust logistic regression problem that seeks the model that will perform best against adversarial realizations of the data distribution drawn from a suitably constructed Wasserstein ambiguity set. Our model and solution approach differ from prior work in that we can capture settings where the likelihood of distribution shifts can vary across features, significantly broadening the applicability of our model relative to the state-of-the-art. We propose a graph-based solution approach that can be integrated into off-the-shelf optimization solvers. We evaluate the performance of our model and algorithms on numerous publicly available datasets. Our solution achieves a 408x speed-up relative to the state-of-the-art. Additionally, compared to the state-of-the-art, our model reduces average calibration error by up to 36.19% and worst-case calibration error by up to 41.70%, while increasing the average area under the ROC curve (AUC) by up to 18.02% and worst-case AUC by up to 48.37%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12012v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qingshi Sun, Nathan Justin, Andres Gomez, Phebe Vayanos</dc:creator>
    </item>
    <item>
      <title>qReduMIS: A Quantum-Informed Reduction Algorithm for the Maximum Independent Set Problem</title>
      <link>https://arxiv.org/abs/2503.12551</link>
      <description>arXiv:2503.12551v1 Announce Type: cross 
Abstract: We propose and implement a quantum-informed reduction algorithm for the maximum independent set problem that integrates classical kernelization techniques with information extracted from quantum devices. Our larger framework consists of dedicated application, algorithm, and hardware layers, and easily generalizes to the maximum weight independent set problem. In this hybrid quantum-classical framework, which we call qReduMIS, the quantum computer is used as a co-processor to inform classical reduction logic about frozen vertices that are likely (or unlikely) to be in large independent sets, thereby opening up the reduction space after removal of targeted subgraphs. We systematically assess the performance of qReduMIS based on experiments with up to 231 qubits run on Rydberg quantum hardware available through Amazon Braket. Our experiments show that qReduMIS can help address fundamental performance limitations faced by a broad set of (quantum) solvers including Rydberg quantum devices. We outline implementations of qReduMIS with alternative platforms, such as superconducting qubits or trapped ions, and we discuss potential future extensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12551v1</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.quant-gas</category>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin J. A. Schuetz, Romina Yalovetzky, Ruben S. Andrist, Grant Salton, Yue Sun, Rudy Raymond, Shouvanik Chakrabarti, Atithi Acharya, Ruslan Shaydulin, Marco Pistoia, Helmut G. Katzgraber</dc:creator>
    </item>
    <item>
      <title>Understanding Gradient Orthogonalization for Deep Learning via Non-Euclidean Trust-Region Optimization</title>
      <link>https://arxiv.org/abs/2503.12645</link>
      <description>arXiv:2503.12645v1 Announce Type: cross 
Abstract: Optimization with matrix gradient orthogonalization has recently demonstrated impressive results in the training of deep neural networks (Jordan et al., 2024; Liu et al., 2025). In this paper, we provide a theoretical analysis of this approach. In particular, we show that the orthogonalized gradient method can be seen as a first-order trust-region optimization method, where the trust-region is defined in terms of the matrix spectral norm. Motivated by this observation, we provide the first theoretical analysis of the stochastic non-Euclidean trust-region gradient method with momentum, which recovers the Muon optimizer (Jordan et al., 2024) as a special case. In addition, we establish the convergence of the normalized SGD with momentum (Cutkosky and Mehta, 2020) in the constrained and composite setting, show that its iteration complexity of finding an $\varepsilon$-accurate solution can be improved from $\mathcal{O}(\varepsilon^{-3.5})$ to $\mathcal{O}(\varepsilon^{-3})$ under the star-convexity assumption, and obtain similar results for the Muon algorithm. Finally, our theoretical findings provide an explanation for the practical superiority of Muon compared to the Orthogonal-SGDM algorithm of Tuddenham et al. (2022).</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12645v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dmitry Kovalev</dc:creator>
    </item>
    <item>
      <title>The infimal convolution structure of the Hellinger-Kantorovich distance</title>
      <link>https://arxiv.org/abs/2503.12939</link>
      <description>arXiv:2503.12939v1 Announce Type: cross 
Abstract: We show that the Hellinger-Kantorovich distance can be expressed as the metric infimal convolution of the Hellinger and the Wasserstein distances, as conjectured by Liero, Mielke, and Savar\'e. To prove it, we study with the tools of Unbalanced Optimal Transport the so called Marginal Entropy-Transport problem that arises as a single minimization step in the definition of infimal convolution. Careful estimates and results when the number of minimization steps diverges are also provided, both in the specific case of the Hellinger-Kantorovich setting and in the general one of abstract distances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12939v1</guid>
      <category>math.MG</category>
      <category>math.FA</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicol\`o De Ponti, Giacomo Enrico Sodini, Luca Tamanini</dc:creator>
    </item>
    <item>
      <title>Observation estimates for a semilinear heat equation in \mathbb{R}^n</title>
      <link>https://arxiv.org/abs/2503.12951</link>
      <description>arXiv:2503.12951v1 Announce Type: cross 
Abstract: This paper studies the state observation problems for the semilinear heat equation in R^n. We derive observation estimates for the equation using the logarithmic convexity property of the frequency function (see [12]). As an application, we show that if two solutions coincide on a nonempty open subset \omega\subset\Omega at some time T&gt;0, then they must be identical.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12951v1</guid>
      <category>math.AP</category>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guojie Zheng, Xin Yu</dc:creator>
    </item>
    <item>
      <title>Some linear feedback laws for stabilisation of sterile insect technique control system</title>
      <link>https://arxiv.org/abs/2503.12986</link>
      <description>arXiv:2503.12986v1 Announce Type: cross 
Abstract: The implementation of the Sterile Insect Technique (SIT) to manage a target population has been the focus of numerous recent scientific studies. The present work focuses on a feedback law that depends linearly on the state variables of the SIT control system. We provide both mathematical proof and numerical illustrations demonstrating the global asymptotic stability of the population to zero when releasing a number of sterile insects proportional to different state variables of the SIT model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12986v1</guid>
      <category>math.DS</category>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kala Agbo Bidi (LJLL, CaGE, SU, UPCit\'e)</dc:creator>
    </item>
    <item>
      <title>Innovation diffusion dynamics toward long-term behavioral shifts</title>
      <link>https://arxiv.org/abs/2503.13044</link>
      <description>arXiv:2503.13044v1 Announce Type: cross 
Abstract: Sustainable technologies and services can play a pivotal role in the transition to "greener" habits. Their widespread adoption is thus crucial, and understanding how to foster this phenomenon in a systematic way could have a major impact on our future. With this in mind, in this work we propose an extension of the Friedkin-Johnsen opinion dynamics model toward characterizing the long-term impact of (structural) fostering policies. We then propose alternative nudging strategies that target a trade-off between widespread adoption and investments under budget constraints, showing the impact of our modeling and design choices on inclination shifts over a set of numerical tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13044v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lisa Piccinin, Valentina Breschi, Chiara Ravazzi, Fabrizio Dabbene, Mara Tanelli</dc:creator>
    </item>
    <item>
      <title>The deep multi-FBSDE method: a robust deep learning method for coupled FBSDEs</title>
      <link>https://arxiv.org/abs/2503.13193</link>
      <description>arXiv:2503.13193v1 Announce Type: cross 
Abstract: We introduce the deep multi-FBSDE method for robust approximation of coupled forward-backward stochastic differential equations (FBSDEs), focusing on cases where the deep BSDE method of Han, Jentzen, and E (2018) fails to converge. To overcome the convergence issues, we consider a family of FBSDEs that are equivalent to the original problem in the sense that they satisfy the same associated partial differential equation (PDE). Our algorithm proceeds in two phases: first, we approximate the initial condition for the FBSDE family, and second, we approximate the original FBSDE using the initial condition approximated in the first phase. Numerical experiments show that our method converges even when the standard deep BSDE method does not.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13193v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <category>q-fin.CP</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kristoffer Andersson, Adam Andersson, Cornelis W. Oosterlee</dc:creator>
    </item>
    <item>
      <title>Robust Decision-Making Via Free Energy Minimization</title>
      <link>https://arxiv.org/abs/2503.13223</link>
      <description>arXiv:2503.13223v1 Announce Type: cross 
Abstract: Despite their groundbreaking performance, state-of-the-art autonomous agents can misbehave when training and environmental conditions become inconsistent, with minor mismatches leading to undesirable behaviors or even catastrophic failures. Robustness towards these training/environment ambiguities is a core requirement for intelligent agents and its fulfillment is a long-standing challenge when deploying agents in the real world. Here, departing from mainstream views seeking robustness through training, we introduce DR-FREE, a free energy model that installs this core property by design. It directly wires robustness into the agent decision-making mechanisms via free energy minimization. By combining a robust extension of the free energy principle with a novel resolution engine, DR-FREE returns a policy that is optimal-yet-robust against ambiguity. Moreover, for the first time, it reveals the mechanistic role of ambiguity on optimal decisions and requisite Bayesian belief updating. We evaluate DR-FREE on an experimental testbed involving real rovers navigating an ambiguous environment filled with obstacles. Across all the experiments, DR-FREE enables robots to successfully navigate towards their goal even when, in contrast, standard free energy minimizing agents that do not use DR-FREE fail. In short, DR-FREE can tackle scenarios that elude previous methods: this milestone may inspire both deployment in multi-agent settings and, at a perhaps deeper level, the quest for a biologically plausible explanation of how natural agents - with little or no training - survive in capricious environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13223v1</guid>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Allahkaram Shafiei, Hozefa Jesawada, Karl Friston, Giovanni Russo</dc:creator>
    </item>
    <item>
      <title>Queues with inspection cost: To see or not to see?</title>
      <link>https://arxiv.org/abs/2503.13232</link>
      <description>arXiv:2503.13232v1 Announce Type: cross 
Abstract: Consider an M/M/1-type queue where joining attains a known reward, but a known waiting cost is paid per time unit spent queueing. In the 1960s, Naor showed that any arrival optimally joins the queue if its length is less than a known threshold. Yet acquiring knowledge of the queue length often brings an additional cost, e.g., website loading time or data roaming charge. Therefore, our model presents any arrival with three options: join blindly, balk blindly, or pay a known inspection cost to make the optimal joining decision by comparing the queue length to Naor's threshold. In a recent paper, Hassin and Roet-Green prove that a unique Nash equilibrium always exists and classify regions where the equilibrium probabilities are non-zero. We complement these findings with new closed-form expressions for the equilibrium probabilities in the majority of cases. Further, Hassin and Roet-Green show that minimizing inspection cost maximises social welfare. Envisaging a queue operator choosing where to invest, we compare the effects of lowering inspection cost and increasing the queue-joining reward on social welfare. We prove that the former dominates and that the latter can even have a detrimental effect on social welfare.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13232v1</guid>
      <category>cs.GT</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s11134-025-09940-7</arxiv:DOI>
      <dc:creator>Jake Clarkson, Konstantin Avrachenkov, Eitan Altman</dc:creator>
    </item>
    <item>
      <title>Follow-the-Regularized-Leader with Adversarial Constraints</title>
      <link>https://arxiv.org/abs/2503.13366</link>
      <description>arXiv:2503.13366v1 Announce Type: cross 
Abstract: Constrained Online Convex Optimization (COCO) can be seen as a generalization of the standard Online Convex Optimization (OCO) framework. At each round, a cost function and constraint function are revealed after a learner chooses an action. The goal is to minimize both the regret and cumulative constraint violation (CCV) against an adaptive adversary. We show for the first time that is possible to obtain the optimal $O(\sqrt{T})$ bound on both regret and CCV, improving the best known bounds of $O \left( \sqrt{T} \right)$ and $\~{O} \left( \sqrt{T} \right)$ for the regret and CCV, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13366v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ricardo N. Ferreira, Cl\'audia Soares</dc:creator>
    </item>
    <item>
      <title>Kernel-based error bounds of bilinear Koopman surrogate models for nonlinear data-driven control</title>
      <link>https://arxiv.org/abs/2503.13407</link>
      <description>arXiv:2503.13407v1 Announce Type: cross 
Abstract: We derive novel deterministic bounds on the approximation error of data-based bilinear surrogate models for unknown nonlinear systems. The surrogate models are constructed using kernel-based extended dynamic mode decomposition to approximate the Koopman operator in a reproducing kernel Hilbert space. Unlike previous methods that require restrictive assumptions on the invariance of the dictionary, our approach leverages kernel-based dictionaries that allow us to control the projection error via pointwise error bounds, overcoming a significant limitation of existing theoretical guarantees. The derived state- and input-dependent error bounds allow for direct integration into Koopman-based robust controller designs with closed-loop guarantees for the unknown nonlinear system. Numerical examples illustrate the effectiveness of the proposed framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13407v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robin Str\"asser, Manuel Schaller, Julian Berberich, Karl Worthmann, Frank Allg\"ower</dc:creator>
    </item>
    <item>
      <title>Data-Driven Estimation of Structured Singular Values</title>
      <link>https://arxiv.org/abs/2503.13410</link>
      <description>arXiv:2503.13410v1 Announce Type: cross 
Abstract: Estimating the size of the modeling error is crucial for robust control. Over the years, numerous metrics have been developed to quantify the model error in a control relevant manner. One of the most important such metrics is the structured singular value, as it leads to necessary and sufficient conditions for ensuring stability and robustness in feedback control under structured model uncertainty. Although the computation of the structured singular value is often intractable, lower and upper bounds for it can often be obtained if a model of the system is known. In this paper, we introduce a fully data-driven method to estimate a lower bound for the structured singular value, by conducting experiments on the system and applying power iterations to the collected data. Our numerical simulations demonstrate that this method effectively lower bounds the structured singular value, yielding results comparable to those obtained using the Robust Control toolbox of MATLAB.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13410v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Margarita A. Guerrero, Braghadeesh Lakshminarayanan, Cristian R. Rojas</dc:creator>
    </item>
    <item>
      <title>Stochastic Primal-Dual Three Operator Splitting Algorithm with Extension to Equivariant Regularization-by-Denoising</title>
      <link>https://arxiv.org/abs/2208.01631</link>
      <description>arXiv:2208.01631v3 Announce Type: replace 
Abstract: In this work we propose a stochastic primal-dual three-operator splitting algorithm (TOS-SPDHG) for solving a class of convex three-composite optimization problems. Our proposed scheme is a direct three-operator splitting extension of the SPDHG algorithm [Chambolle et al. 2018]. We provide theoretical convergence analysis showing ergodic $O(1/K)$ convergence rate, and demonstrate the effectiveness of our approach in imaging inverse problems. Moreover, we further propose TOS-SPDHG-RED and TOS-SPDHG-eRED which utilizes the regularization-by-denoising (RED) framework to leverage pretrained deep denoising networks as priors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.01631v3</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junqi Tang, Matthias Ehrhardt, Carola-Bibiane Sch\"onlieb</dc:creator>
    </item>
    <item>
      <title>Distributed Random Reshuffling Methods with Improved Convergence</title>
      <link>https://arxiv.org/abs/2306.12037</link>
      <description>arXiv:2306.12037v3 Announce Type: replace 
Abstract: This paper proposes two distributed random reshuffling methods, namely Gradient Tracking with Random Reshuffling (GT-RR) and Exact Diffusion with Random Reshuffling (ED-RR), to solve the distributed optimization problem over a connected network, where a set of agents aim to minimize the average of their local cost functions. Both algorithms invoke random reshuffling (RR) update for each agent, inherit favorable characteristics of RR for minimizing smooth nonconvex objective functions, and improve the performance of previous distributed random reshuffling methods both theoretically and empirically. Specifically, both GT-RR and ED-RR achieve the convergence rate of $O(1/[(1-\lambda)^{1/3}m^{1/3}T^{2/3}])$ in driving the (minimum) expected squared norm of the gradient to zero, where $T$ denotes the number of epochs, $m$ is the sample size for each agent, and $1-\lambda$ represents the spectral gap of the mixing matrix. When the objective functions further satisfy the Polyak-{\L}ojasiewicz (PL) condition, we show GT-RR and ED-RR both achieve $O(1/[(1-\lambda)mT^2])$ convergence rate in terms of the averaged expected differences between the agents' function values and the global minimum value. Notably, both results are comparable to the convergence rates of centralized RR methods (up to constant factors depending on the network topology) and outperform those of previous distributed random reshuffling algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.12037v3</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kun Huang, Linli Zhou, Shi Pu</dc:creator>
    </item>
    <item>
      <title>Combined Parameter and Shape Optimization of Electric Machines with Isogeometric Analysis</title>
      <link>https://arxiv.org/abs/2311.06046</link>
      <description>arXiv:2311.06046v2 Announce Type: replace 
Abstract: In structural optimization, both parameters and shape are relevant for the model performance. Yet, conventional optimization techniques usually consider either parameters or the shape separately. This work addresses this problem by proposing a simple yet powerful approach to combine parameter and shape optimization in a framework using Isogeometric Analysis (IGA). The optimization employs sensitivity analysis by determining the gradients of an objective function with respect to parameters and control points that represent the geometry. The gradients with respect to the control points are calculated in an analytical way using the adjoint method, which enables straightforward shape optimization by altering of these control points. Given that a change in a single geometry parameter corresponds to modifications in multiple control points, the chain rule is employed to obtain the gradient with respect to the parameters in an efficient semi-analytical way. The presented method is exemplarily applied to nonlinear 2D magnetostatic simulations featuring a permanent magnet synchronous motor and compared to designs, which were optimized using parameter and shape optimization separately. It is numerically shown that the permanent magnet mass can be reduced and the torque ripple can be eliminated almost completely by simultaneously adjusting rotor parameters and shape. The approach allows for novel designs to be created with the potential to reduce the optimization time substantially.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.06046v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Wiesheu, Theodor Komann, Melina Merkel, Sebastian Sch\"ops, Stefan Ulbrich, Idoia Cortes Garcia</dc:creator>
    </item>
    <item>
      <title>On a Generalization of Wasserstein Distance and the Beckmann Problem to Connection Graphs</title>
      <link>https://arxiv.org/abs/2312.10295</link>
      <description>arXiv:2312.10295v3 Announce Type: replace 
Abstract: We propose a model of optimal parallel transport between vector fields on a connection graph, which consists of a weighted graph along with a map from its edges to an orthogonal group. Inspired by the well-known equivalence of 1-Wasserstein distance and minimum cost flows on standard graphs, we consider two versions of this problem: a minimum norm vector-valued flow problem with divergence constraints reflective of the connection structure of the graph; and a modified version which incorporates both quadratic regularization and a relaxation of the divergence constraint. Our theoretical contributions include: conditions for feasibility and computation of the Lagrangian dual problem for both problems, and duality correspondence for the relaxed-regularized version. Example applications of the model including transport between color images, vector field interpolation, and unsupervised clustering of vector field-valued data (in this case hurricane trajectory data) are also considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.10295v3</guid>
      <category>math.OC</category>
      <category>cs.DM</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sawyer Robertson, Dhruv Kohli, Gal Mishne, Alexander Cloninger</dc:creator>
    </item>
    <item>
      <title>A full splitting algorithm for fractional programs with structured numerators and denominators</title>
      <link>https://arxiv.org/abs/2312.14341</link>
      <description>arXiv:2312.14341v2 Announce Type: replace 
Abstract: In this paper, we consider a class of nonconvex and nonsmooth fractional programming problems, that involve the sum of a convex, possibly nonsmooth function composed with a linear operator and a differentiable, possibly nonconvex function in the numerator and a convex, possibly nonsmooth function composed with a linear operator in the denominator. These problems have applications in various fields. We propose an adaptive full-splitting proximal subgradient algorithm that addresses the challenge of decoupling the composition of the nonsmooth component with the linear operator in the numerator. We specifically evaluate the nonsmooth function in the numerator using its proximal operator of its conjugate function. Furthermore, the smooth component in the numerator is evaluated through its gradient, and the nonsmooth in the denominator is managed using its subgradient. We demonstrate subsequential convergence toward an approximate lifted stationary point and ensure global convergence under the Kurdyka-\L ojasiewicz property, all achieved without full-row rank assumptions on the linear operators. We provide further discussions on {\it the tightness of the convergence results of the proposed algorithm and its related variants, and the reasoning behind aiming for an approximate lifted stationary point}. We construct a series of counter-examples to show that the proposed algorithm and its variant might diverge when seeking exact solutions. A practical version incorporating a nonmonotone line search is also developed to enhance its performance significantly. Our theoretical findings are validated through simulations involving limited-angle CT reconstruction and the robust sharp-ratio-type minimization problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14341v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Radu Ioan Bo\c{t}, Guoyin Li, Min Tao</dc:creator>
    </item>
    <item>
      <title>Exact asymptotic order for generalised adaptive approximations</title>
      <link>https://arxiv.org/abs/2312.16644</link>
      <description>arXiv:2312.16644v2 Announce Type: replace 
Abstract: In this note, we present an abstract approach to study asymptotic orders for adaptive approximations with respect to a monotone set function $\mathfrak{J}$ defined on dyadic cubes. We determine the exact upper order in terms of the critical value of the corresponding $\mathfrak{J}$-partition function, and we are able to provide upper and lower bounds in term of fractal-geometric quantities. With properly chosen $\mathfrak{J}$, our new approach has applications in many different areas of mathematics, including the spectral theory of Krein-Feller operators, quantization dimensions of compactly supported probability measures, and the exact asymptotic order for Kolmogorov, Gelfand and linear widths for Sobolev embeddings into $L_{\mu}^p$-spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.16644v2</guid>
      <category>math.OC</category>
      <category>cs.IT</category>
      <category>math.FA</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marc Kesseb\"ohmer, Aljoscha Niemann</dc:creator>
    </item>
    <item>
      <title>Constraint-Generation Policy Optimization (CGPO): Nonlinear Programming for Policy Optimization in Mixed Discrete-Continuous MDPs</title>
      <link>https://arxiv.org/abs/2401.12243</link>
      <description>arXiv:2401.12243v2 Announce Type: replace 
Abstract: We propose the Constraint-Generation Policy Optimization (CGPO) framework to optimize policy parameters within compact and interpretable policy classes for mixed discrete-continuous Markov Decision Processes (DC-MDP). CGPO can not only provide bounded policy error guarantees over an infinite range of initial states for many DC-MDPs with expressive nonlinear dynamics, but it can also provably derive optimal policies in cases where it terminates with zero error. Furthermore, CGPO can generate worst-case state trajectories to diagnose policy deficiencies and provide counterfactual explanations of optimal actions. To achieve such results, CGPO proposes a bilevel mixed-integer nonlinear optimization framework for optimizing policies in defined expressivity classes (e.g. piecewise linear) and reduces it to an optimal constraint generation methodology that adversarially generates worst-case state trajectories. Furthermore, leveraging modern nonlinear optimizers, CGPO can obtain solutions with bounded optimality gap guarantees. We handle stochastic transitions through chance constraints, providing high-probability performance guarantees. We also present a roadmap for understanding the computational complexities of different expressivity classes of policy, reward, and transition dynamics. We experimentally demonstrate the applicability of CGPO across various domains, including inventory control, management of a water reservoir system, and physics control. In summary, CGPO provides structured, compact and explainable policies with bounded performance guarantees, enabling worst-case scenario generation and counterfactual policy diagnostics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.12243v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <category>cs.SC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Gimelfarb, Ayal Taitler, Scott Sanner</dc:creator>
    </item>
    <item>
      <title>Alternating Stochastic Variance-Reduced Algorithms with Optimal Complexity for Bilevel Optimization</title>
      <link>https://arxiv.org/abs/2404.11377</link>
      <description>arXiv:2404.11377v2 Announce Type: replace 
Abstract: This paper studies the unconstrained nonconvex-strongly-convex bilevel optimization problem. A common approach to solving this problem is to alternately update the upper-level and lower-level variables using (biased) stochastic gradients or their variants, with the lower-level variable updated either one step or multiple steps. In this context, we propose two alternating stochastic variance-reduced algorithms, namely ALS-SPIDER and ALS-STORM, which introduce an auxiliary variable to estimate the hypergradient for updating the upper-level variable. ALS-SPIDER employs the SPIDER estimator for updating variables, while ALS-STORM is a modification of ALS-SPIDER designed to avoid using large batch sizes in every iteration. Theoretically, both algorithms can find an $\epsilon$-stationary point of the bilevel problem with a sample complexity of $O(\epsilon^{-1.5})$ for arbitrary constant number of lower-level variable updates. To the best of our knowledge, they are the first algorithms to achieve the optimal complexity of $O(\epsilon^{-1.5})$ when performing multiple updates on the lower-level variable. Numerical experiments are conducted to illustrate the efficiency of our algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11377v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haimei Huo, Zhixun Su</dc:creator>
    </item>
    <item>
      <title>Subdifferentially polynomially bounded functions and Gaussian smoothing-based zeroth-order optimization</title>
      <link>https://arxiv.org/abs/2405.04150</link>
      <description>arXiv:2405.04150v3 Announce Type: replace 
Abstract: We study the class of subdifferentially polynomially bounded (SPB) functions, which is a rich class of locally Lipschitz functions that encompasses all Lipschitz functions, all gradient- or Hessian-Lipschitz functions, and even some non-smooth locally Lipschitz functions. We show that SPB functions are compatible with Gaussian smoothing (GS), in the sense that the GS of any SPB function is well-defined and satisfies a descent lemma akin to gradient-Lipschitz functions, with the Lipschitz constant replaced by a polynomial function. Leveraging this descent lemma, we propose GS-based zeroth-order optimization algorithms with an adaptive stepsize strategy for minimizing SPB functions, and analyze their convergence rates with respect to both relative and absolute stationarity measures. Finally, we also establish the iteration complexity for achieving a $(\delta, \epsilon)$-approximate stationary point, based on a novel quantification of Goldstein stationarity via the GS gradient that could be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04150v3</guid>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ming Lei, Ting Kei Pong, Shuqin Sun, Man-Chung Yue</dc:creator>
    </item>
    <item>
      <title>Control in the Coefficients of an Obstacle Problem</title>
      <link>https://arxiv.org/abs/2405.20074</link>
      <description>arXiv:2405.20074v2 Announce Type: replace 
Abstract: In this work, we consider optimality conditions of an optimal control problem governed by an obstacle problem. Here, we focus on introducing a, matrix valued, control variable as the coefficients of the obstacle problem. As it is well known, obstacle problems can be formulated as a complementarity system and consequently the associated solution operator is not Gateaux differentiable. As a consequence, we utilize a regularization approach to obtain optimality conditions as the limit of optimality conditions of a family of regularized problems.
  Due to the coupling of the controlled coefficient with the gradients of the solution to the obstacle problem, weak convergence arguments can not be applied and we need to argue by $H$-convergence. We show, that, based on initial $H$-convergence, a bootstrapping argument can be utilized to prove strong $L^p$-convergence of the control and thus enable the passage to the limit in the optimality conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20074v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolai Simon, Winnifried Wollner</dc:creator>
    </item>
    <item>
      <title>Orthogonal Constrained Minimization with Tensor $\ell_{2,p}$ Regularization for HSI Denoising and Destriping</title>
      <link>https://arxiv.org/abs/2407.03605</link>
      <description>arXiv:2407.03605v2 Announce Type: replace 
Abstract: Hyperspectral images (HSIs) are often contaminated by a mixture of noises such as Gaussian noise, dead lines, stripes, and so on. In this paper, we propose a novel approach for HSI denoising and destriping, called NLTL2p, which consists of an orthogonal constrained minimization model and an iterative algorithm with convergence guarantees. The model of the proposed NLTL2p approach is built based on a new sparsity-enhanced Nonlocal Low-rank Tensor regularization and a tensor $\ell_{2,p}$ norm with $p\in(0,1)$. The low-rank constraints for HSI denoising utilize the spatial nonlocal self-similarity and spectral correlation of HSIs and are formulated based on independent higher-order singular value decomposition with sparsity enhancement on its core tensor to prompt more low-rankness. The tensor $\ell_{2,p}$ norm for HSI destriping is extended from the matrix $\ell_{2,p}$ norm. A proximal block coordinate descent algorithm is proposed in the NLTL2p approach to solve the resulting nonconvex nonsmooth minimization with orthogonal constraints. We show any accumulation point of the sequence generated by the proposed algorithm converges to a first-order stationary point, which is defined using three equalities of substationarity, symmetry, and feasibility for orthogonal constraints. In the numerical experiments, we compare the proposed method with state-of-the-art methods including a deep learning based method, and test the methods on both simulated and real HSI datasets. Our proposed NLTL2p method demonstrates outperformance in terms of metrics such as mean peak signal-to-noise ratio as well as visual quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03605v2</guid>
      <category>math.OC</category>
      <category>cs.CV</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoxia Liu, Shijie Yu, Jian Lu, Xiaojun Chen</dc:creator>
    </item>
    <item>
      <title>Two Typical Implementable Semismooth* Newton Methods for Generalized Equations are G-Semismooth Newton Methods</title>
      <link>https://arxiv.org/abs/2407.14215</link>
      <description>arXiv:2407.14215v2 Announce Type: replace 
Abstract: Semismooth* Newton methods have been proposed in recent years targeting multi-valued inclusion problems and have been successfully implemented to deal with several concrete generalized equations. In this paper, we show that two typical implementations of them that are available are exactly the applications of G-semismooth Newton methods for solving nonsmooth equations localized from these generalized equations. This new understanding expands the breadth of G-semismooth Newton methods in theory, results in a few interesting problems regarding the two categories of nonsmooth Newton methods, and more importantly, provides informative observations in facilitating the design and implementation of practical Newton-type algorithms for solving generalized equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14215v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liang Chen, Defeng Sun, Wangyongquan Zhang</dc:creator>
    </item>
    <item>
      <title>Numerical Discrete-Time Implementation of Continuous-Time Linear-Quadratic Model Predictive Control</title>
      <link>https://arxiv.org/abs/2407.18825</link>
      <description>arXiv:2407.18825v2 Announce Type: replace 
Abstract: This study presents the design, discretization and implementation of the continuous-time linear-quadratic model predictive control (CT-LMPC). The control model of the CT-LMPC is parameterized as transfer functions with time delays, and they are separated into deterministic and stochastic parts for relevant control and filtering algorithms. We formulate time-delay, finite-horizon CT linear-quadratic optimal control problems (LQ-OCPs) for the CT-LMPC. By assuming piece-wise constant inputs and constraints, we present the numerical discretization of the proposed LQ-OCPs and show how to convert the discrete-time (DT) equivalent into a standard quadratic program. The performance of the CT-LMPC is compared with the conventional DT-LMPC algorithm. Our numerical experiments show that, under fixed tunning parameters, the CT-LMPC shows better closed-loop performance as the sampling time increases than the conventional DT-LMPC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18825v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhanhao Zhang, Anders Hilmar Damm Christensen, Steen H{\o}rsholt, John Bagterp J{\o}rgensen</dc:creator>
    </item>
    <item>
      <title>HPR-LP: An implementation of an HPR method for solving linear programming</title>
      <link>https://arxiv.org/abs/2408.12179</link>
      <description>arXiv:2408.12179v2 Announce Type: replace 
Abstract: In this paper, we introduce an HPR-LP solver, an implementation of a Halpern Peaceman-Rachford (HPR) method with semi-proximal terms for solving linear programming (LP). The HPR method enjoys the iteration complexity of $O(1/k)$ in terms of the Karush-Kuhn-Tucker residual and the objective error. Based on the complexity results, we design an adaptive strategy of restart and penalty parameter update to improve the efficiency and robustness of the HPR method. We conduct extensive numerical experiments on different LP benchmark datasets using NVIDIA A100-SXM4-80GB GPU in different stopping tolerances. Our solver's Julia version achieves a $\textbf{2.39x}$ to $\textbf{5.70x}$ speedup measured by SGM10 on benchmark datasets with presolve ($\textbf{2.03x}$ to $\textbf{4.06x}$ without presolve) over the award-winning solver PDLP with the tolerance of $10^{-8}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12179v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaihuang Chen, Defeng Sun, Yancheng Yuan, Guojun Zhang, Xinyuan Zhao</dc:creator>
    </item>
    <item>
      <title>On the risk levels of distributionally robust chance constrained problems</title>
      <link>https://arxiv.org/abs/2409.01177</link>
      <description>arXiv:2409.01177v2 Announce Type: replace 
Abstract: In this paper, we discuss the utilization of perturbed risk levels (PRLs) for the solution of chance-constrained problems via sampling-based approaches. PRLs allow the consideration of distributional ambiguity by rescaling the risk level of the nominal chance constraint. Explicit expressions of the PRL exist for some discrepancy-based ambiguity sets.
  We propose a discrepancy functional not included in previous comparisons of different PRLs based on the likelihood ratio, which we term ,,relative variation distance" (RVD). If the ambiguity set can be described by the RVD, the rescaling of the risk level with the PRL is in contrast to other discrepancy functionals possible even for very low risk levels. We derive distributionally robust one- and two-level guarantees for the solution of chance-constrained problems with randomized methods. We demonstrate the viability of the derived guarantees for a randomized MPC under distributional ambiguity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01177v2</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Moritz Heinlein, Teodoro Alamo, Sergio Lucia</dc:creator>
    </item>
    <item>
      <title>Improving the Solution of Indefinite Quadratic Programs and Linear Programs with Complementarity Constraints by a Progressive MIP Method</title>
      <link>https://arxiv.org/abs/2409.09964</link>
      <description>arXiv:2409.09964v2 Announce Type: replace 
Abstract: Indefinite quadratic programs (QPs) are known to be very difficult to be solved to global optimality, so are linear programs with linear complementarity constraints. Treating the former as a subclass of the latter, this paper presents a progressive mixed integer linear programming method for solving a general linear program with linear complementarity constraints (LPCC). Instead of solving the LPCC with a full set of integer variables expressing the complementarity conditions, the presented method solves a finite number of mixed integer subprograms by starting with a small fraction of integer variables and progressively increasing this fraction. After describing the PIP (for progressive integer programming) method and its various implementations, we demonstrate, via an extensive set of computational experiments, the superior performance of the progressive approach over the direct solution of the full-integer formulation of the LPCCs. It is also shown that the solution obtained at the termination of the PIP method is a local minimizer of the LPCC, a property that cannot be claimed by any known non-enumerative method for solving this nonconvex program. In all the experiments, the PIP method is initiated at a feasible solution of the LPCC obtained from a nonlinear programming solver, and with high likelihood, can successfully improve it. Thus, the PIP method can improve a stationary solution of an indefinite QP, something that is not likely to be achievable by a nonlinear programming method. Finally, some analysis is presented that provides a better understanding of the roles of the LPCC suboptimal solutions in the local optimality of the indefinite QP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09964v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyao Zhang, Shaoning Han, Jong-Shi Pang</dc:creator>
    </item>
    <item>
      <title>Distributed Optimization Algorithm with Superlinear Convergence Rate</title>
      <link>https://arxiv.org/abs/2409.12392</link>
      <description>arXiv:2409.12392v2 Announce Type: replace 
Abstract: This paper considers distributed optimization problems, where each agent cooperatively minimizes the sum of local objective functions through the communication with its neighbors. The widely adopted distributed gradient method in solving this problem suffers from slow convergence rates, which motivates us to incorporate the second-order information of the objective functions. However, the challenge arises from the unique structure of the inverse of the Hessian matrix, which prevents the direct distributed implementation of the second-order method. We overcome this challenge by proposing a novel optimization framework. The key idea is to transform the distributed optimization problem into an optimal control problem. Using Pontryagin's maximum principle and the associated forward-backward difference equations (FBDEs), we derive a new distributed optimization algorithm that incorporates the second-order information without requiring the computation of the inverse of the Hessian matrix. Furthermore, the superlinear convergence of the proposed algorithm is proved under some mild assumptions. Finally, we also propose a variant of the algorithm to balance the number of iterations and communication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12392v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yeming Xu, Ziyuan Guo, Kaihong Lu, Huanshui Zhang</dc:creator>
    </item>
    <item>
      <title>Inpatient Overflow Management with Proximal Policy Optimization</title>
      <link>https://arxiv.org/abs/2410.13767</link>
      <description>arXiv:2410.13767v3 Announce Type: replace 
Abstract: Overflowing patients to non-primary wards can effectively alleviate congestion in hospitals, while undesired overflow also leads to issues like mismatched service quality. Therefore, we need to trade off between congestion and undesired overflow. This overflow management problem is modeled as a discrete-time Markov Decision Process with large state and action space. To overcome the curse-of-dimensionality, we decompose the action at each time into a sequence of atomic actions and use an actor-critic algorithm, Proximal Policy Optimization (PPO), to guide the atomic actions. Moreover, we tailor the design of neural network which represents policy to account for the daily periodic pattern of the system flows. Under hospital settings of different scales, the PPO policies consistently outperform commonly used state-of-art policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13767v3</guid>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jingjing Sun, Jim Dai, Pengyi Shi</dc:creator>
    </item>
    <item>
      <title>Online optimisation for dynamic electrical impedance tomography</title>
      <link>https://arxiv.org/abs/2412.12944</link>
      <description>arXiv:2412.12944v2 Announce Type: replace 
Abstract: Online optimisation studies the convergence of optimisation methods as the data embedded in the problem changes. Based on this idea, we propose a primal dual online method for nonlinear time-discrete inverse problems. We analyse the method through regret theory and demonstrate its performance in real-time monitoring of moving bodies in a fluid with Electrical Impedance Tomography (EIT). To do so, we also prove the second-order differentiability of the Complete Electrode Model (CEM) solution operator on $L^\infty$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12944v2</guid>
      <category>math.OC</category>
      <category>cs.CV</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Neil Dizon, Jyrki Jauhiainen, Tuomo Valkonen</dc:creator>
    </item>
    <item>
      <title>Data-driven $H_{\infty}$ predictive control for constrained systems: a Lagrange duality approach</title>
      <link>https://arxiv.org/abs/2412.18831</link>
      <description>arXiv:2412.18831v2 Announce Type: replace 
Abstract: This article proposes a data-driven $H_{\infty}$ control scheme for time-domain constrained systems based on model predictive control formulation. The scheme combines $H_{\infty}$ control and minimax model predictive control, enabling more effective handling of external disturbances and time-domain constraints. First, by leveraging input-output-disturbance data, the scheme ensures $H_{\infty}$ performance of the closed-loop system. Then, a minimax optimization problem is converted into a more manageable minimization problem employing Lagrange duality, which reduces conservatism typically associated with ellipsoidal evaluations of time-domain constraints. The study examines key closed-loop properties, including stability, disturbance attenuation, and constraint satisfaction, achieved by the proposed data-driven moving horizon predictive control algorithm. The effectiveness and advantages of the proposed method are demonstrated through numerical simulations involving a batch reactor system, confirming its robustness and feasibility under noisy conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18831v2</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenhuang Wu, Lulu Guo, Nan Li, Hong Chen</dc:creator>
    </item>
    <item>
      <title>Alternating minimization for square root principal component pursuit</title>
      <link>https://arxiv.org/abs/2501.00471</link>
      <description>arXiv:2501.00471v2 Announce Type: replace 
Abstract: Recently, the square root principal component pursuit (SRPCP) model has garnered significant research interest. It is shown in the literature that the SRPCP model guarantees robust matrix recovery with a universal, constant penalty parameter. While its statistical advantages are well-documented, the computational aspects from an optimization perspective remain largely unexplored. In this paper, we focus on developing efficient optimization algorithms for solving the SRPCP problem. Specifically, we propose a tuning-free alternating minimization (AltMin) algorithm, where each iteration involves subproblems enjoying closed-form optimal solutions. Additionally, we introduce techniques based on the variational formulation of the nuclear norm and Burer-Monteiro decomposition to further accelerate the AltMin method. Extensive numerical experiments confirm the efficiency and robustness of our algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00471v2</guid>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shengxiang Deng, Xudong Li, Yangjing Zhang</dc:creator>
    </item>
    <item>
      <title>Bayesian Optimization by Kernel Regression and Density-based Exploration</title>
      <link>https://arxiv.org/abs/2502.06178</link>
      <description>arXiv:2502.06178v3 Announce Type: replace 
Abstract: Bayesian optimization is highly effective for optimizing expensive-to-evaluate black-box functions, but it faces significant computational challenges due to the high computational complexity of Gaussian processes, which results in a total time complexity that is quartic with respect to the number of iterations. To address this limitation, we propose the Bayesian Optimization by Kernel regression and density-based Exploration (BOKE) algorithm. BOKE uses kernel regression for efficient function approximation, kernel density for exploration, and integrates them into the confidence bound criteria to guide the optimization process, thus reducing computational costs to quadratic. Our theoretical analysis rigorously establishes the global convergence of BOKE and ensures its robustness in noisy settings. Through extensive numerical experiments on both synthetic and real-world optimization tasks, we demonstrate that BOKE not only performs competitively compared to Gaussian process-based methods but also exhibits superior computational efficiency. These results highlight BOKE's effectiveness in resource-constrained environments, providing a practical approach for optimization problems in engineering applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06178v3</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tansheng Zhu, Hongyu Zhou, Ke Jin, Xusheng Xu, Qiufan Yuan, Lijie Ji</dc:creator>
    </item>
    <item>
      <title>Rough Stochastic Pontryagin Maximum Principle and an Indirect Shooting Method</title>
      <link>https://arxiv.org/abs/2502.06726</link>
      <description>arXiv:2502.06726v2 Announce Type: replace 
Abstract: We derive first-order Pontryagin optimality conditions for stochastic optimal control with deterministic controls for systems modeled by rough differential equations (RDE) driven by Gaussian rough paths. This Pontryagin Maximum Principle (PMP) applies to systems following stochastic differential equations (SDE) driven by Brownian motion, yet it does not rely on forward-backward SDEs and involves the same Hamiltonian as the deterministic PMP. The proof consists of first deriving various integrable error bounds for solutions to nonlinear and linear RDEs by leveraging recent results on Gaussian rough paths. The PMP then follows using standard techniques based on needle-like variations. As an application, we propose the first indirect shooting method for nonlinear stochastic optimal control and show that it converges 10x faster than a direct method on a stabilization task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06726v2</guid>
      <category>math.OC</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.PR</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Thomas Lew</dc:creator>
    </item>
    <item>
      <title>Provable and Practical Online Learning Rate Adaptation with Hypergradient Descent</title>
      <link>https://arxiv.org/abs/2502.11229</link>
      <description>arXiv:2502.11229v2 Announce Type: replace 
Abstract: This paper investigates the convergence properties of the hypergradient descent method (HDM), a 25-year-old heuristic originally proposed for adaptive stepsize selection in stochastic first-order methods. We provide the first rigorous convergence analysis of HDM using the online learning framework of [Gao24] and apply this analysis to develop new state-of-the-art adaptive gradient methods with empirical and theoretical support. Notably, HDM automatically identifies the optimal stepsize for the local optimization landscape and achieves local superlinear convergence. Our analysis explains the instability of HDM reported in the literature and proposes efficient strategies to address it. We also develop two HDM variants with heavy-ball and Nesterov momentum. Experiments on deterministic convex problems show HDM with heavy-ball momentum (HDM-HB) exhibits robust performance and significantly outperforms other adaptive first-order methods. Moreover, HDM-HB often matches the performance of L-BFGS, an efficient and practical quasi-Newton method, using less memory and cheaper iterations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11229v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ya-Chi Chu, Wenzhi Gao, Yinyu Ye, Madeleine Udell</dc:creator>
    </item>
    <item>
      <title>Cauchy-Schwarz Regularizers</title>
      <link>https://arxiv.org/abs/2503.01639</link>
      <description>arXiv:2503.01639v3 Announce Type: replace 
Abstract: We introduce a novel class of regularization functions, called Cauchy-Schwarz (CS) regularizers, which can be designed to induce a wide range of properties in solution vectors of optimization problems. To demonstrate the versatility of CS regularizers, we derive regularization functions that promote discrete-valued vectors, eigenvectors of a given matrix, and orthogonal matrices. The resulting CS regularizers are simple, differentiable, and can be free of spurious stationary points, making them suitable for gradient-based solvers and large-scale optimization problems. In addition, CS regularizers automatically adapt to the appropriate scale, which is, for example, beneficial when discretizing the weights of neural networks. To demonstrate the efficacy of CS regularizers, we provide results for solving underdetermined systems of linear equations and weight quantization in neural networks. Furthermore, we discuss specializations, variations, and generalizations, which lead to an even broader class of new and possibly more powerful regularizers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01639v3</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sueda Taner, Ziyi Wang, Christoph Studer</dc:creator>
    </item>
    <item>
      <title>Controlled Invariance in Fully Actuated Max-plus Linear Systems with Precedence Semimodules</title>
      <link>https://arxiv.org/abs/2503.03357</link>
      <description>arXiv:2503.03357v2 Announce Type: replace 
Abstract: Given a max-plus linear system and a semimodule, the problem of computing the maximal controlled invariant subsemimodule is still open to this day. In this paper, we consider this problem for the specific class of fully actuated systems and constraints in the form of precedence semimodules. The assumption of full actuation corresponds to the existence of an input for each component of the system state. A precedence semimodule is the set of solutions of inequalities typically used to represent time-window constraints. We prove that, in this setting, it is possible to (i) compute the maximal controlled invariant subsemimodule and (ii) decide the convergence of a fixed-point algorithm introduced by R.D. Katz in strongly polynomial time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03357v2</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Davide Zorzenon, J\"org Raisch</dc:creator>
    </item>
    <item>
      <title>Modeling of Rumor Propagation in Large Populations with Network via Graphon Games</title>
      <link>https://arxiv.org/abs/2503.09107</link>
      <description>arXiv:2503.09107v2 Announce Type: replace 
Abstract: In this paper, we propose a graphon game model to understand how rumor (such as fake news) propagates in large populations that are interacting on a network and how different policies affect the spread. We extend the SKIR model that is used to model rumor propagation and implement individual controls and weighted interactions with other agents to have controlled dynamics. The agents aim to minimize their own expected costs non-cooperatively. We give the finite player game model and the limiting graphon game model to approximate the Nash equilibrium in the population. We give the graphon game Nash equilibrium as a solution to a continuum of ordinary differential equations (ODEs) and give existence results. Finally, we give a numerical approach and analyze examples where we use piecewise constant graphon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09107v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Huaning Liu, Gokce Dayanikli</dc:creator>
    </item>
    <item>
      <title>Exploiting Edited Large Language Models as General Scientific Optimizers</title>
      <link>https://arxiv.org/abs/2503.09620</link>
      <description>arXiv:2503.09620v2 Announce Type: replace 
Abstract: Large language models (LLMs) have been widely adopted in mathematical optimization in scientific scenarios for their extensive knowledge and advanced reasoning capabilities. Existing methods mainly focus on utilizing LLMs to solve optimization problems in a prompt-based manner, which takes observational feedback as additional textual descriptions. However, due to LLM's \textbf{high sensitivity to the prompts} and \textbf{tendency to get lost in lengthy prompts}, these methods struggle to effectively utilize the {observational} feedback from each optimization step, which severely hinders the applications for real-world scenarios. To address these challenges, we propose a conceptually simple and general {bi-level} optimization method, namely \textbf{G}eneral \textbf{S}cientific \textbf{O}ptimizers (GSO). Specifically, GSO first utilizes inner-level simulators as experimental platforms to evaluate the current solution and provide observational feedback. Then, LLMs serve as knowledgeable and versatile scientists, generating new solutions by refining potential errors from the feedback as the outer-level optimization. Finally, simulations together with the expert knowledge in LLMs are jointly updated with bi-level interactions via model editing. Extensive experiments show that GSO consistently outperforms existing state-of-the-art methods using \textit{six} different LLM backbones on \textit{seven} different tasks, demonstrating the effectiveness and a wide range of applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09620v2</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qitan Lv, Tianyu Liu, Hong Wang</dc:creator>
    </item>
    <item>
      <title>A scalable sequential adaptive cubic regularization algorithm for optimization with general equality constraints</title>
      <link>https://arxiv.org/abs/2503.11254</link>
      <description>arXiv:2503.11254v2 Announce Type: replace 
Abstract: The scalable adaptive cubic regularization method ($\mathrm{ARC_{q}K}$: Dussault et al. in Math. Program. Ser. A 207(1-2):191-225, 2024) has been recently proposed for unconstrained optimization. It has excellent convergence properties, complexity, and promising numerical performance. In this paper, we extend $\mathrm{ARC_{q}K}$ to large scale nonlinear optimization with general equality constraints and propose a scalable sequential adaptive cubic regularization algorithm named $\mathrm{SSARC_{q}K}$. In each iteration, we construct an ARC subproblem with linearized constraints inspired by sequential quadratic optimization methods. Then composite-step approach is used to decompose the trial step into the sum of the vertical step and the horizontal step. By means of reduced-Hessian approach, we rewrite the linearity constrained ARC subproblem as a standard unconstrained ARC subproblem to compute the horizontal step. A CG-Lanczos procedure with shifts is employed to solve this subproblem approximately. We provide a new global convergence analysis of the inexact ARC method. Preliminary numerical results are reported to show the performance of $\mathrm{SSARC_{q}K}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11254v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yonggang Pei, Shuai Shao, Mauricio Silva Louzeiro, Detong Zhu</dc:creator>
    </item>
    <item>
      <title>Scenario Reduction for Distributionally Robust Optimization</title>
      <link>https://arxiv.org/abs/2503.11484</link>
      <description>arXiv:2503.11484v2 Announce Type: replace 
Abstract: Stochastic and (distributionally) robust optimization problems often become computationally challenging as the number of scenarios increases. Scenario reduction is therefore a key technique for improving tractability. We introduce a general scenario reduction method for distributionally robust optimization (DRO), which includes stochastic and robust optimization as special cases. Our approach constructs the reduced DRO problem by projecting the original ambiguity set onto a reduced set of scenarios. Under mild conditions, we establish bounds on the relative quality of the reduction. The methodology is applicable to random variables following either discrete or continuous probability distributions, with representative scenarios appropriately selected in both cases. Given the relevance of optimization problems with linear and quadratic objectives, we further refine our approach for these settings. Finally, we demonstrate its effectiveness through numerical experiments on mixed-integer benchmark instances from MIPLIB and portfolio optimization problems. Our results show that the proposed approximation significantly reduces solution time while maintaining high solution quality with only minor errors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11484v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kevin-Martin Aigner, Sebastian Denzler, Frauke Liers, Sebastian Pokutta, Kartikey Sharma</dc:creator>
    </item>
    <item>
      <title>ShieldNN: A Provably Safe NN Filter for Unsafe NN Controllers</title>
      <link>https://arxiv.org/abs/2006.09564</link>
      <description>arXiv:2006.09564v3 Announce Type: replace-cross 
Abstract: In this paper, we develop a novel closed-form Control Barrier Function (CBF) and associated controller shield for the Kinematic Bicycle Model (KBM) with respect to obstacle avoidance. The proposed CBF and shield -- designed by an algorithm we call ShieldNN -- provide two crucial advantages over existing methodologies. First, ShieldNN considers steering and velocity constraints directly with the non-affine KBM dynamics; this is in contrast to more general methods, which typically consider only affine dynamics and do not guarantee invariance properties under control constraints. Second, ShieldNN provides a closed-form set of safe controls for each state unlike more general methods, which typically rely on optimization algorithms to generate a single instantaneous for each state. Together, these advantages make ShieldNN uniquely suited as an efficient Multi-Obstacle Safe Actions (i.e. multiple-barrier-function shielding) during training time of a Reinforcement Learning (RL) enabled Neural Network controller. We show via experiments that ShieldNN dramatically increases the completion rate of RL training episodes in the presence of multiple obstacles, thus establishing the value of ShieldNN in training RL-based controllers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2006.09564v3</guid>
      <category>cs.RO</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>James Ferlez, Mahmoud Elnaggar, Yasser Shoukry, Cody Fleming</dc:creator>
    </item>
    <item>
      <title>An Information-Theoretic Analysis of Discrete-Time Control and Filtering Limitations by the I-MMSE Relationships</title>
      <link>https://arxiv.org/abs/2304.09274</link>
      <description>arXiv:2304.09274v2 Announce Type: replace-cross 
Abstract: Fundamental limitations or performance trade-offs/limits are important properties and constraints of both control and filtering systems. Among various trade-off metrics, total information rate that characterizes the sensitivity trade-offs and time-averaged performance of control and filtering systems was conventionally studied by using the differential entropy rate and Kolmogorov-Bode formula. In this paper, by extending the famous I-MMSE (mutual information -- minimum mean-square error) relationships to the discrete-time additive white Gaussian channels with and without feedback, a new paradigm is introduced to estimate and analyze total information rate as a control and filtering trade-off metric. Under this framework, we explore the trade-off properties of total information rate for a variety of the discrete-time control and filtering systems, e.g., LTI, LTV, and nonlinear, and propose an alternative approach to investigate total information rate via optimal estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.09274v2</guid>
      <category>eess.SY</category>
      <category>cs.IT</category>
      <category>cs.SY</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Neng Wan, Dapeng Li, Naira Hovakimyan, Petros G. Voulgaris</dc:creator>
    </item>
    <item>
      <title>Robust Model Reductions for the Boundary Feedback Stabilization of Magnetizable Piezoelectric Beams</title>
      <link>https://arxiv.org/abs/2309.07492</link>
      <description>arXiv:2309.07492v2 Announce Type: replace-cross 
Abstract: Magnetizable piezoelectric beams exhibit strong couplings between mechanical, electric, and magnetic fields, significantly affecting their high-frequency vibrational behavior. Ensuring exponential stability under boundary feedback controllers is challenging due to the uneven distribution of high-frequency eigenvalues in standard Finite Difference models. While numerical filtering can mitigate instability as the discretization parameter tends to zero, its reliance on explicit spectral computations is computationally demanding. This work introduces two novel model reduction techniques for stabilizing magnetizable piezoelectric beams. First, a Finite Element discretization using linear splines is developed, improving numerical stability over standard Finite Differences. However, this method still requires numerical filtering to eliminate spurious high-frequency modes, necessitating full spectral decomposition. Numerical investigations further reveal a direct dependence of the optimal filtering threshold on feedback amplifiers. To overcome these limitations, an alternative order-reduction Finite Difference scheme is proposed, eliminating the need for numerical filtering. Using a Lyapunov-based framework, we establish exponential stability with decay rates independent of the discretization parameter. The reduced model also exhibits exponential error decay and uniform energy convergence to the original system. Numerical simulations validate the effectiveness of the proposed methods, and we construct an algorithm for separating eigenpairs for the proper application of the numerical filtering. Comparative spectral analyses and energy decay results confirm the superior stability and efficiency of the proposed approach, providing a robust framework for model reduction in coupled partial differential equation systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.07492v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ahmet Kaan Aydin, Ahmet Ozkan Ozer, Jacob Walterman</dc:creator>
    </item>
    <item>
      <title>A Coefficient Makes SVRG Effective</title>
      <link>https://arxiv.org/abs/2311.05589</link>
      <description>arXiv:2311.05589v2 Announce Type: replace-cross 
Abstract: Stochastic Variance Reduced Gradient (SVRG), introduced by Johnson &amp; Zhang (2013), is a theoretically compelling optimization method. However, as Defazio &amp; Bottou (2019) highlight, its effectiveness in deep learning is yet to be proven. In this work, we demonstrate the potential of SVRG in optimizing real-world neural networks. Our empirical analysis finds that, for deeper neural networks, the strength of the variance reduction term in SVRG should be smaller and decrease as training progresses. Inspired by this, we introduce a multiplicative coefficient $\alpha$ to control the strength and adjust it through a linear decay schedule. We name our method $\alpha$-SVRG. Our results show $\alpha$-SVRG better optimizes models, consistently reducing training loss compared to the baseline and standard SVRG across various model architectures and multiple image classification datasets. We hope our findings encourage further exploration into variance reduction techniques in deep learning. Code is available at github.com/davidyyd/alpha-SVRG.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.05589v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yida Yin, Zhiqiu Xu, Zhiyuan Li, Trevor Darrell, Zhuang Liu</dc:creator>
    </item>
    <item>
      <title>Critical Influence of Overparameterization on Sharpness-aware Minimization</title>
      <link>https://arxiv.org/abs/2311.17539</link>
      <description>arXiv:2311.17539v4 Announce Type: replace-cross 
Abstract: Training overparameterized neural networks often yields solutions with varying generalization capabilities, even when achieving similar training losses. Recent evidence indicates a strong correlation between the sharpness of a minimum and its generalization error, leading to increased interest in optimization methods that explicitly seek flatter minima for improved generalization. Despite its contemporary relevance to overparameterization, however, this sharpness-aware minimization (SAM) strategy has not been studied much yet as to exactly how it is affected by overparameterization. In this work, we analyze SAM under varying degrees of overparameterization, presenting both empirical and theoretical findings that reveal its critical influence on SAM's effectiveness. First, we conduct extensive numerical experiments across diverse domains, demonstrating that SAM consistently benefits from overparameterization. Next, we attribute this phenomenon to the interplay between the enlarged solution space and increased implicit bias resulting from overparameterization. Furthermore, we show that this effect is particularly pronounced in practical settings involving label noise and sparsity, and yet, sufficient regularization is necessary. Last but not least, we provide other theoretical insights into how overparameterization helps SAM achieve minima with more uniform Hessian moments compared to SGD, and much faster convergence at a linear rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.17539v4</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sungbin Shin, Dongyeop Lee, Maksym Andriushchenko, Namhoon Lee</dc:creator>
    </item>
    <item>
      <title>Almost sure convergence rates of stochastic gradient methods under gradient domination</title>
      <link>https://arxiv.org/abs/2405.13592</link>
      <description>arXiv:2405.13592v3 Announce Type: replace-cross 
Abstract: Stochastic gradient methods are among the most important algorithms in training machine learning problems. While classical assumptions such as strong convexity allow a simple analysis they are rarely satisfied in applications. In recent years, global and local gradient domination properties have shown to be a more realistic replacement of strong convexity. They were proved to hold in diverse settings such as (simple) policy gradient methods in reinforcement learning and training of deep neural networks with analytic activation functions. We prove almost sure convergence rates $f(X_n)-f^*\in o\big( n^{-\frac{1}{4\beta-1}+\epsilon}\big)$ of the last iterate for stochastic gradient descent (with and without momentum) under global and local $\beta$-gradient domination assumptions. The almost sure rates get arbitrarily close to recent rates in expectation. Finally, we demonstrate how to apply our results to the training task in both supervised and reinforcement learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13592v3</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simon Weissmann, Sara Klein, Wa\"iss Azizian, Leif D\"oring</dc:creator>
    </item>
    <item>
      <title>Quasi-Steady-State Approach for Efficient Multiscale Simulation and Optimization of mAb Glycosylation in CHO Cell Culture</title>
      <link>https://arxiv.org/abs/2409.00281</link>
      <description>arXiv:2409.00281v2 Announce Type: replace-cross 
Abstract: Glycosylation is a critical quality attribute for monoclonal antibody (mAb) production, influenced by both process conditions and cellular mechanisms. Multiscale mechanistic models, spanning from the bioreactor to the Golgi apparatus, have been proposed for analyzing the glycosylation process. However, these models are computationally intensive to solve when using traditional methods, making optimization and control challenging. In this work, we propose a quasi-steady-state (QSS) approach for efficiently solving the multiscale glycosylation model. By introducing the QSS assumption and assuming negligible nucleotide sugar donor (NSD) flux for glycosylation in the Golgi, the large-scale partial differential algebraic equation system is converted into a series of independent differential algebraic equation systems. Based on that representation, we develop a three-step QSS simulation method and further reduce computational time through parallel computing and nonuniform time grid strategies. Case studies in simulation, parameter estimation, and dynamic optimization demonstrate that the QSS approach can be more than 300-fold faster than the method of lines, with less than 1.6% relative errors. This work establishes a solid foundation for multiscale model-based optimization and control of the glycosylation process, supporting the implementation of quality by design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00281v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yingjie Ma, Jing Guo, Andrew Maloney, Richard Braatz</dc:creator>
    </item>
    <item>
      <title>Three-dimensional Nonlinear Path-following Guidance with Bounded Input Constraints</title>
      <link>https://arxiv.org/abs/2409.08507</link>
      <description>arXiv:2409.08507v2 Announce Type: replace-cross 
Abstract: In this paper, we consider the tracking of arbitrary curvilinear geometric paths in three-dimensional output spaces of unmanned aerial vehicles (UAVs) without pre-specified timing requirements, commonly referred to as path-following problems, subjected to bounded inputs. Specifically, we propose a novel nonlinear path-following guidance law for a UAV that enables it to follow any smooth curvilinear path in three dimensions while accounting for the bounded control authority in the design. The proposed solution offers a general treatment of the path-following problem by removing the dependency on the path's geometry, which makes it applicable to paths with varying levels of complexity and smooth curvatures. Additionally, the proposed strategy draws inspiration from the pursuit guidance approach, which is known for its simplicity and ease of implementation. Theoretical analysis guarantees that the UAV converges to its desired path within a fixed time and remains on it irrespective of its initial configuration with respect to the path. Finally, the simulations demonstrate the merits and effectiveness of the proposed guidance strategy through a wide range of engagement scenarios, showcasing the UAV's ability to follow diverse curvilinear paths accurately.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08507v2</guid>
      <category>eess.SY</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>math.DS</category>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Saurabh Kumar, Shashi Ranjan Kumar, Abhinav Sinha</dc:creator>
    </item>
    <item>
      <title>Stochastic Data-Driven Predictive Control: Chance-Constraint Satisfaction with Identified Multi-step Predictors</title>
      <link>https://arxiv.org/abs/2409.10405</link>
      <description>arXiv:2409.10405v2 Announce Type: replace-cross 
Abstract: We propose a novel data-driven stochastic model predictive control framework for uncertain linear systems with noisy output measurements. Our approach leverages multi-step predictors to efficiently propagate uncertainty, ensuring chance constraint satisfaction. In particular, we present a strategy to identify multi-step predictors and quantify the associated uncertainty using a surrogate (data-driven) state space model. Then, we utilize the derived distribution to formulate a constraint tightening that ensures chance constraint satisfaction despite the parametric uncertainty. A numerical example highlights the reduced conservatism of handling parametric uncertainty in the proposed method compared to state-of-the-art solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10405v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Haldun Balim, Andrea Carron, Melanie N. Zeilinger, Johannes K\"ohler</dc:creator>
    </item>
    <item>
      <title>Two-person Positive Shortest Path Games Have Nash Equilibria in Pure Stationary Strategies</title>
      <link>https://arxiv.org/abs/2410.09257</link>
      <description>arXiv:2410.09257v3 Announce Type: replace-cross 
Abstract: We prove that every finite two-person shortest path game, where the local cost of every move is positive for each player, has a Nash equilibrium (NE) in pure stationary strategies, which can be computed in polynomial time. We also extend the existence result to infinite graphs with finite out-degrees. Moreover, our proof gives that a terminal NE (in which the play is a path from the initial position to a terminal) exists provided at least one of the two players can guarantee reaching a terminal. If none of the players can do it, in other words, if each of the two players has a strategy that separates all terminals from the initial position $s$, then, obviously, a cyclic NE exists, although its cost is infinite for both players, since we restrict ourselves to positive games. We conjecture that a terminal NE exists too, provided there exists a directed path from $s$ to a terminal. However, this is open.
  We extend our result to short paths interdiction games, where at each vertex, we allow one player to block some of the arcs and the other player to choose one of the non-blocked arcs. Assuming that blocking sets are chosen from an independence system given by an oracle, we give an algorithm for computing a NE in time $O(|E|(\log|V|+\tau))$, where $V$ is the set of vertices, $E$ is the set of arcs, and $\tau$ is the maximum time taken by the oracle on any input.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09257v3</guid>
      <category>cs.DM</category>
      <category>cs.MA</category>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Endre Boros, Khaled Elbassioni, Vladimir Gurvich, Mikhail Vyalyi</dc:creator>
    </item>
    <item>
      <title>Numerical computation of generalized Wasserstein distances with applications to traffic model analysis</title>
      <link>https://arxiv.org/abs/2410.11441</link>
      <description>arXiv:2410.11441v2 Announce Type: replace-cross 
Abstract: Generalized Wasserstein distances allow to quantitatively compare two continuous or atomic mass distributions with equal or different total mass. In this paper, we propose four numerical methods for the approximation of three different generalized Wasserstein distances introduced in the last years, giving some insights about their physical meaning. After that, we explore their usage in the context of the sensitivity analysis of differential models for traffic flow. The quantification of models sensitivity is obtained by computing the generalized Wasserstein distances between two (numerical) solutions corresponding to different inputs, including different boundary conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11441v2</guid>
      <category>math.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maya Briani, Emiliano Cristiani, Giovanni Franzina, Francesca L. Ignoto</dc:creator>
    </item>
    <item>
      <title>Trajectory Optimization for Spatial Microstructure Control in Electron Beam Metal Additive Manufacturing</title>
      <link>https://arxiv.org/abs/2410.18207</link>
      <description>arXiv:2410.18207v2 Announce Type: replace-cross 
Abstract: Metal additive manufacturing (AM) opens the possibility for spatial control of as-fabricated microstructure and properties. However, since the solid state diffusional transformations that drive microstructure outcomes are governed by nonlinear ODEs in terms of temperature, which is itself governed by PDEs over the entire part domain, solving for the system inputs needed to achieve desired microstructure distributions has proven difficult. In this work, we present a trajectory optimization approach for spatial control of microstructure in metal AM, which we demonstrate by controlling the hardness of a low-alloy steel in electron beam powder bed fusion (EB-PBF). To this end, we present models for thermal and microstructural dynamics. Next, we use experimental data to identify the parameters of the microstructure transformation dynamics. We then pose spatial microstructure control as a finite-horizon optimal control problem. The optimal power field trajectory is computed using an augmented Lagrangian differential dynamic programming (AL-DDP) method with GPU acceleration. The resulting time-varying power fields are then realized on an EB-PBF machine through an approximation scheme. Measurements of the resultant hardness shows that the optimized power field trajectory is able to closely produce the desired hardness distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18207v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mikhail Khrenov, Moon Tan, Lauren Fitzwater, Michelle Hobdari, Sneha Prabha Narra</dc:creator>
    </item>
    <item>
      <title>Convex Formulations for Training Two-Layer ReLU Neural Networks</title>
      <link>https://arxiv.org/abs/2410.22311</link>
      <description>arXiv:2410.22311v2 Announce Type: replace-cross 
Abstract: Solving non-convex, NP-hard optimization problems is crucial for training machine learning models, including neural networks. However, non-convexity often leads to black-box machine learning models with unclear inner workings. While convex formulations have been used for verifying neural network robustness, their application to training neural networks remains less explored. In response to this challenge, we reformulate the problem of training infinite-width two-layer ReLU networks as a convex completely positive program in a finite-dimensional (lifted) space. Despite the convexity, solving this problem remains NP-hard due to the complete positivity constraint. To overcome this challenge, we introduce a semidefinite relaxation that can be solved in polynomial time. We then experimentally evaluate the tightness of this relaxation, demonstrating its competitive performance in test accuracy across a range of classification tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22311v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Karthik Prakhya, Tolga Birdal, Alp Yurtsever</dc:creator>
    </item>
    <item>
      <title>Adaptive Batch Size Schedules for Distributed Training of Language Models with Data and Model Parallelism</title>
      <link>https://arxiv.org/abs/2412.21124</link>
      <description>arXiv:2412.21124v2 Announce Type: replace-cross 
Abstract: An appropriate choice of batch sizes in large-scale model training is crucial, yet it involves an intrinsic yet inevitable dilemma: large-batch training improves training efficiency in terms of memory utilization, while generalization performance often deteriorates due to small amounts of gradient noise. Despite this dilemma, the common practice of choosing batch sizes in language model training often prioritizes training efficiency -- employing either constant large sizes with data parallelism or implementing batch size warmup schedules. However, such batch size schedule designs remain heuristic and often fail to adapt to training dynamics, presenting the challenge of designing adaptive batch size schedules. Given the abundance of available datasets and the data-hungry nature of language models, data parallelism has become an indispensable distributed training paradigm, enabling the use of larger batch sizes for gradient computation. However, vanilla data parallelism requires replicas of model parameters, gradients, and optimizer states at each worker, which prohibits training larger models with billions of parameters. To optimize memory usage, more advanced parallelism strategies must be employed. In this work, we propose general-purpose and theoretically principled adaptive batch size schedules compatible with data parallelism and model parallelism. We develop a practical implementation with PyTorch Fully Sharded Data Parallel, facilitating the pretraining of language models of different sizes. We empirically demonstrate that our proposed approaches outperform constant batch sizes and heuristic batch size warmup schedules in the pretraining of models in the Llama 2 family, with particular focus on smaller models with up to 3 billion parameters. We also establish theoretical convergence guarantees for such adaptive batch size schedules with Adam for general smooth nonconvex objectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.21124v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tim Tsz-Kit Lau, Weijian Li, Chenwei Xu, Han Liu, Mladen Kolar</dc:creator>
    </item>
    <item>
      <title>When GNNs meet symmetry in ILPs: an orbit-based feature augmentation approach</title>
      <link>https://arxiv.org/abs/2501.14211</link>
      <description>arXiv:2501.14211v2 Announce Type: replace-cross 
Abstract: A common characteristic in integer linear programs (ILPs) is symmetry, allowing variables to be permuted without altering the underlying problem structure. Recently, GNNs have emerged as a promising approach for solving ILPs. However, a significant challenge arises when applying GNNs to ILPs with symmetry: classic GNN architectures struggle to differentiate between symmetric variables, which limits their predictive accuracy. In this work, we investigate the properties of permutation equivariance and invariance in GNNs, particularly in relation to the inherent symmetry of ILP formulations. We reveal that the interaction between these two factors contributes to the difficulty of distinguishing between symmetric variables. To address this challenge, we explore the potential of feature augmentation and propose several guiding principles for constructing augmented features. Building on these principles, we develop an orbit-based augmentation scheme that first groups symmetric variables and then samples augmented features for each group from a discrete uniform distribution. Empirical results demonstrate that our proposed approach significantly enhances both training efficiency and predictive performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14211v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qian Chen, Lei Li, Qian Li, Jianghua Wu, Akang Wang, Ruoyu Sun, Xiaodong Luo, Tsung-Hui Chang, Qingjiang Shi</dc:creator>
    </item>
    <item>
      <title>Learning Spatially Adaptive $\ell_1$-Norms Weights for Convolutional Synthesis Regularization</title>
      <link>https://arxiv.org/abs/2503.09483</link>
      <description>arXiv:2503.09483v2 Announce Type: replace-cross 
Abstract: We propose an unrolled algorithm approach for learning spatially adaptive parameter maps in the framework of convolutional synthesis-based $\ell_1$ regularization. More precisely, we consider a family of pre-trained convolutional filters and estimate deeply parametrized spatially varying parameters applied to the sparse feature maps by means of unrolling a FISTA algorithm to solve the underlying sparse estimation problem. The proposed approach is evaluated for image reconstruction of low-field MRI and compared to spatially adaptive and non-adaptive analysis-type procedures relying on Total Variation regularization and to a well-established model-based deep learning approach. We show that the proposed approach produces visually and quantitatively comparable results with the latter approaches and at the same time remains highly interpretable. In particular, the inferred parameter maps quantify
  the local contribution of each filter in the reconstruction, which provides valuable insight into the algorithm mechanism and could potentially be used to discard unsuited filters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09483v2</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>math.OC</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andreas Kofler, Luca Calatroni, Christoph Kolbitsch, Kostas Papafitsoros</dc:creator>
    </item>
  </channel>
</rss>
