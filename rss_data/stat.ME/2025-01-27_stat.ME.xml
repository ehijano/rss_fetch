<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 28 Jan 2025 03:45:49 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Improved subsample-and-aggregate via the private modified winsorized mean</title>
      <link>https://arxiv.org/abs/2501.14095</link>
      <description>arXiv:2501.14095v1 Announce Type: new 
Abstract: We develop a univariate, differentially private mean estimator, called the private modified winsorized mean designed to be used as the aggregator in subsample-and-aggregate. We demonstrate, via real data analysis, that common differentially private multivariate mean estimators may not perform well as the aggregator, even with a dataset with 8000 observations, motivating our developments. We show that the modified winsorized mean is minimax optimal for several, large classes of distributions, even under adversarial contamination. We also demonstrate that, empirically, the modified winsorized mean performs well compared to other private mean estimates. We consider the modified winsorized mean as the aggregator in subsample-and-aggregate, deriving a finite sample deviations bound for a subsample-and-aggregate estimate generated with the new aggregator. This result yields two important insights: (i) the optimal choice of subsamples depends on the bias of the estimator computed on the subsamples, and (ii) the rate of convergence of the subsample-and-aggregate estimator depends on the robustness of the estimator computed on the subsamples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14095v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kelly Ramsay, Dylan Spicker</dc:creator>
    </item>
    <item>
      <title>Assessing treatment efficacy for interval-censored endpoints using multistate semi-Markov models fit to multiple data streams</title>
      <link>https://arxiv.org/abs/2501.14097</link>
      <description>arXiv:2501.14097v1 Announce Type: new 
Abstract: We introduce a computationally efficient and general approach for utilizing multiple, possibly interval-censored, data streams to study complex biomedical endpoints using multistate semi-Markov models. Our motivating application is the REGEN-2069 trial, which investigated the protective efficacy (PE) of the monoclonal antibody combination REGEN-COV against SARS-CoV-2 when administered prophylactically to individuals in households at high risk of secondary transmission. Using data on symptom onset, episodic RT-qPCR sampling, and serological testing, we estimate the PE of REGEN-COV for asymptomatic infection, its effect on seroconversion following infection, and the duration of viral shedding. We find that REGEN-COV reduced the risk of asymptomatic infection and the duration of viral shedding, and led to lower rates of seroconversion among asymptomatically infected participants. Our algorithm for fitting semi-Markov models to interval-censored data employs a Monte Carlo expectation maximization (MCEM) algorithm combined with importance sampling to efficiently address the intractability of the marginal likelihood when data are intermittently observed. Our algorithm provide substantial computational improvements over existing methods and allows us to fit semi-parametric models despite complex coarsening of the data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14097v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Raphael Morsomme, C. Jason Liang, Allyson Mateja, Dean A. Follmann, Meagan P. O'Brien, Chenguang Wang, Jonathan Fintzi</dc:creator>
    </item>
    <item>
      <title>Gaussian Rank Verification</title>
      <link>https://arxiv.org/abs/2501.14142</link>
      <description>arXiv:2501.14142v1 Announce Type: new 
Abstract: Statistical experiments often seek to identify random variables with the largest population means. This inferential task, known as rank verification, has been well-studied on Gaussian data with equal variances. This work provides the first treatment of the unequal variances case, utilizing ideas from the selective inference literature. We design a hypothesis test that verifies the rank of the largest observed value without losing power due to multiple testing corrections. This test is subsequently extended for two procedures: Identifying some number of correctly-ordered Gaussian means, and validating the top-K set. The testing procedures are validated on NHANES survey data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14142v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeremy Goldwasser, Will Fithian, Giles Hooker</dc:creator>
    </item>
    <item>
      <title>Inverse Norm Weighted Maxsum Test for High Dimensional Location Parameters</title>
      <link>https://arxiv.org/abs/2501.14168</link>
      <description>arXiv:2501.14168v1 Announce Type: new 
Abstract: In the context of high-dimensional data, we investigate the one-sample location testing problem. We introduce a max-type test based on the weighted spatial sign, which exhibits exceptional performance, particularly in the presence of sparse alternatives. Notably, we find that the inverse norm test significantly enhances the power of the test compared to several existing max-type tests. Next, we prove the asymptotic independence between the newly proposed max-type test statistic and the sum-type test statistic based on the weighted spatial sign. Then, we propose an innovative max-sum type testing procedure that integrates both test statistics. This novel procedure demonstrates remarkable robustness and effectiveness across a wide range of signal sparsity levels and heavy-tailed distributions. Through extensive simulation studies, we highlight the superior performance of the proposed method, showcasing its robustness and efficiency compared to traditional alternatives in various high-dimensional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14168v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guowei Yan, Ping Zhao, Long Feng</dc:creator>
    </item>
    <item>
      <title>A decomposition of Fisher's information to inform sample size for developing fair and precise clinical prediction models -- Part 2: time-to-event outcomes</title>
      <link>https://arxiv.org/abs/2501.14482</link>
      <description>arXiv:2501.14482v1 Announce Type: new 
Abstract: Background: When developing a clinical prediction model using time-to-event data, previous research focuses on the sample size to minimise overfitting and precisely estimate the overall risk. However, instability of individual-level risk estimates may still be large. Methods: We propose a decomposition of Fisher's information matrix to examine and calculate the sample size required for developing a model that aims for precise and fair risk estimates. We propose a six-step process which can be used before data collection or when an existing dataset is available. Steps (1) to (5) require researchers to specify the overall risk in the target population at a key time-point of interest; an assumed pragmatic 'core model' in the form of an exponential regression model; the (anticipated) joint distribution of core predictors included in that model; and the distribution of any censoring. Results: We derive closed-form solutions that decompose the variance of an individual's estimated event rate into Fisher's unit information matrix, predictor values and total sample size; this allows researchers to calculate and examine uncertainty distributions around individual risk estimates and misclassification probabilities for specified sample sizes. We provide an illustrative example in breast cancer and emphasise the importance of clinical context, including risk thresholds for decision making, and examine fairness concerns for pre- and post-menopausal women. Lastly, in two empirical evaluations, we provide reassurance that uncertainty interval widths based on our approach are close to using more flexible models. Conclusions: Our approach allows users to identify the (target) sample size required to develop a prediction model for time-to-event outcomes, via the pmstabilityss module. It aims to facilitate models with improved trust, reliability and fairness in individual-level predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14482v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Richard D Riley, Gary S Collins, Lucinda Archer, Rebecca Whittle, Amardeep Legha, Laura Kirton, Paula Dhiman, Mohsen Sadatsafavi, Nicola J Adderley, Joseph Alderman, Glen P Martin, Joie Ensor</dc:creator>
    </item>
    <item>
      <title>Minimax Optimal Design with Spillover and Carryover Effects</title>
      <link>https://arxiv.org/abs/2501.14602</link>
      <description>arXiv:2501.14602v1 Announce Type: new 
Abstract: In various applications, the potential outcome of a unit may be influenced by the treatments received by other units, a phenomenon known as interference, as well as by prior treatments, referred to as carryover effects. These phenomena violate the stable unit treatment value assumption and pose significant challenges in causal inference. To address these complexities, we propose a minimax optimal experimental design that simultaneously accounts for both spillover and carryover effects, enhancing the precision of estimates for direct and spillover effects. This method is particularly applicable to multi-unit experiments, reducing sample size requirements and experimental costs. We also investigate the asymptotic properties of the Horvitz--Thompson estimators of direct and spillover effects, demonstrating their consistency and asymptotic normality under the minimax optimal design. To facilitate valid inferences, we propose conservative variance estimators. Furthermore, we tackle the challenges associated with potential misspecifications in the order of carryover effects. Our approach is validated by comprehensive numerical studies that demonstrate superior performance compared to existing experimental designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14602v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoyang Yu, Wei Ma, Hanzhong Liu</dc:creator>
    </item>
    <item>
      <title>Hierarchical Count Echo State Network Models with Application to Graduate Student Enrollments</title>
      <link>https://arxiv.org/abs/2501.14698</link>
      <description>arXiv:2501.14698v1 Announce Type: new 
Abstract: Poisson autoregressive count models have evolved into a time series staple for correlated count data. This paper proposes an alternative to Poisson autoregressions: count echo state networks. Echo state networks can be statistically analyzed in frequentist manners via optimizing penalized likelihoods, or in Bayesian manners via MCMC sampling. This paper develops Poisson echo state techniques for count data and applies them to a massive count data set containing the number of graduate students from 1,758 United States universities during the years 1972-2021 inclusive. Negative binomial models are also implemented to better handle overdispersion in the counts. Performance of the proposed models are compared via their forecasting performance as judged by several methods. In the end, a hierarchical negative binomial based echo state network is judged as the superior model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14698v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qi Wang, Paul A. Parker, Robert B. Lund</dc:creator>
    </item>
    <item>
      <title>Normalization and selecting non-differentially expressed genes improve machine learning modelling of cross-platform transcriptomic data</title>
      <link>https://arxiv.org/abs/2501.14248</link>
      <description>arXiv:2501.14248v1 Announce Type: cross 
Abstract: Normalization is a critical step in quantitative analyses of biological processes. Recent works show that cross-platform integration and normalization enable machine learning (ML) training on RNA microarray and RNA-seq data, but no independent datasets were used in their studies. Therefore, it is unclear how to improve ML modelling performance on independent RNA array and RNA-seq based datasets. Inspired by the house-keeping genes that are commonly used in experimental biology, this study tests the hypothesis that non-differentially expressed genes (NDEG) may improve normalization of transcriptomic data and subsequently cross-platform modelling performance of ML models. Microarray and RNA-seq datasets of the TCGA breast cancer were used as independent training and test datasets, respectively, to classify the molecular subtypes of breast cancer. NDEG (p&gt;0.85) and differentially expressed genes (DEG, p&lt;0.05) were selected based on the p values of ANOVA analysis and used for subsequent data normalization and classification, respectively. Models trained based on data from one platform were used for testing on the other platform. Our data show that NDEG and DEG gene selection could effectively improve the model classification performance. Normalization methods based on parametric statistical analysis were inferior to those based on nonparametric statistics. In this study, the LOG_QN and LOG_QNZ normalization methods combined with the neural network classification model seem to achieve better performance. Therefore, NDEG-based normalization appears useful for cross-platform testing on completely independent datasets. However, more studies are required to examine whether NDEG-based normalization can improve ML classification performance in other datasets and other omic data types.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14248v1</guid>
      <category>q-bio.QM</category>
      <category>q-bio.GN</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fei Deng (Department of Chemical Biology, Ernest Mario School of Pharmacy, Rutgers University, Piscataway, NJ), Catherine H Feng (Department of Chemical Biology, Ernest Mario School of Pharmacy, Rutgers University, Piscataway, NJ, Harvard University, Cambridge, MA), Nan Gao (Department of Biological Sciences, School of Arts &amp; Sciences, Rutgers University, Newark, NJ, Department of Pharmacology, Physiology, and Neuroscience, New Jersey Medical School, Rutgers University, Newark, NJ), Lanjing Zhang (Department of Chemical Biology, Ernest Mario School of Pharmacy, Rutgers University, Piscataway, NJ, Rutgers Cancer Institute of New Jersey, New Brunswick, NJ)</dc:creator>
    </item>
    <item>
      <title>Concentration of discrepancy-based approximate Bayesian computation via Rademacher complexity</title>
      <link>https://arxiv.org/abs/2206.06991</link>
      <description>arXiv:2206.06991v5 Announce Type: replace 
Abstract: There has been increasing interest on summary-free solutions for approximate Bayesian computation (ABC) which replace distances among summaries with discrepancies between the empirical distributions of the observed data and the synthetic samples generated under the proposed parameter values. The success of these strategies has motivated theoretical studies on the limiting properties of the induced posteriors. However, there is still the lack of a theoretical framework for summary-free ABC that (i) is unified, instead of discrepancy-specific, (ii) does not require to constrain the analysis to data generating processes and statistical models meeting specific regularity conditions, but rather facilitates the derivation of limiting properties that hold uniformly, and (iii) relies on verifiable assumptions that provide explicit concentration bounds clarifying which factors govern the limiting behavior of the ABC posterior. We address this gap via a novel theoretical framework that introduces the concept of Rademacher complexity in the analysis of the limiting properties for discrepancy-based ABC posteriors, including in non-i.i.d. and misspecified settings. This yields a unified theory that relies on constructive arguments and provides more informative asymptotic results and uniform concentration bounds, even in settings not covered by current studies. These advancements are obtained by relating the asymptotic properties of summary-free ABC posteriors to the behavior of the Rademacher complexity associated with the chosen discrepancy in the family of integral probability semimetrics (IPS). The IPS class extends summary-based distances, and includes the Wasserstein distance and maximum mean discrepancy, among others. As clarified in specialized theoretical analyses of popular IPS discrepancies and via illustrative simulations, this perspective improves the understanding of summary-free ABC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.06991v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sirio Legramanti, Daniele Durante, Pierre Alquier</dc:creator>
    </item>
    <item>
      <title>Stratified distance space improves the efficiency of sequential samplers for approximate Bayesian computation</title>
      <link>https://arxiv.org/abs/2401.00324</link>
      <description>arXiv:2401.00324v2 Announce Type: replace 
Abstract: Approximate Bayesian computation (ABC) methods are standard tools for inferring parameters of complex models when the likelihood function is analytically intractable. A popular approach to improving the poor acceptance rate of the basic rejection sampling ABC algorithm is to use sequential Monte Carlo (ABC SMC) to produce a sequence of proposal distributions adapting towards the posterior, instead of generating values from the prior distribution of the model parameters. Proposal distribution for the subsequent iteration is typically obtained from a weighted set of samples, often called particles, of the current iteration of this sequence. Current methods for constructing these proposal distributions treat all the particles equivalently, regardless of the corresponding value generated by the sampler, which may lead to inefficiency when propagating the information across iterations of the algorithm. To improve sampler efficiency, we introduce a modified approach called stratified distance ABC SMC. Our algorithm stratifies particles based on their distance between the corresponding synthetic and observed data, and then constructs distinct proposal distributions for all the strata. Taking into account the distribution of distances across the particle space leads to substantially improved acceptance rate of the rejection sampling. We further show that efficiency can be gained by introducing a novel stopping rule for the sequential process based on the stratified posterior samples and demonstrate these advances by several examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.00324v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Henri Pesonen, Jukka Corander</dc:creator>
    </item>
    <item>
      <title>A Scalable Variational Bayes Approach to Fit High-dimensional Spatial Generalized Linear Mixed Models</title>
      <link>https://arxiv.org/abs/2402.15705</link>
      <description>arXiv:2402.15705v3 Announce Type: replace 
Abstract: Gaussian and discrete non-Gaussian spatial datasets are common across fields like public health, ecology, geosciences, and social sciences. Bayesian spatial generalized linear mixed models (SGLMMs) are a flexible class of models for analyzing such data, but they struggle to scale to large datasets. Many scalable Bayesian methods, built upon basis representations or sparse covariance matrices, still rely on posterior sampling via Markov chain Monte Carlo (MCMC). Variational Bayes (VB) methods have been applied to SGLMMs, but only for small areal datasets. We propose two computationally efficient VB approaches for analyzing moderately sized and massive (millions of locations) Gaussian and discrete non-Gaussian spatial data in the continuous spatial domain. Our methods leverage semi-parametric approximations of latent spatial processes and parallel computing to ensure computational efficiency. The proposed methods deliver inferential and predictive performance comparable to gold-standard MCMC methods while achieving computational speedups of up to 3600 times. In most cases, our VB approaches outperform state-of-the-art alternatives such as INLA and Hamiltonian Monte Carlo. We validate our methods through a comparative numerical study and applications to real-world datasets. These VB approaches can enable practitioners to model millions of discrete non-Gaussian spatial observations on standard laptops, significantly expanding access to advanced spatial modeling tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.15705v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin Hyung Lee, Ben Seiyon Lee</dc:creator>
    </item>
    <item>
      <title>Defining Dispersion: A Fundamental Order for Univariate Discrete Distributions</title>
      <link>https://arxiv.org/abs/2406.02124</link>
      <description>arXiv:2406.02124v2 Announce Type: replace 
Abstract: The measurement of dispersion is one of the most fundamental and ubiquitous statistical concepts, in both applied and theoretical contexts. For dispersion measures, such as the standard deviation, to effectively capture the variability of a given distribution, they must, by definition, preserve some stochastic order of dispersion. The so-called dispersive order is the most basic order that serves as a foundation underneath the concept of dispersion measures. However, this order is incompatible with almost all discrete distributions, including lattice and most empirical distributions. As a result, popular measures may fail to accurately capture the dispersion of such distributions.
  In this paper, discrete adaptations of the dispersive order are defined and analyzed. They are shown to be a compromise between being equivalent to the original dispersive order on their joint area of applicability and other crucial properties. Moreover, they share many characteristic properties with the dispersive order, validating their role as a foundation for measuring discrete dispersion in a manner closely aligned with the continuous setting. Their behaviour on well-known families of lattice distribution is generally as expected when parameter differences are sufficiently large. Most popular dispersion measures preserve both discrete dispersive orders, rigorously ensuring that they are also meaningful in discrete settings. However, the interquantile range fails to preserve either discrete order, indicating that it is unsuitable for measuring the dispersion of discrete distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02124v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andreas Eberl, Bernhard Klar</dc:creator>
    </item>
    <item>
      <title>A decomposition of Fisher's information to inform sample size for developing fair and precise clinical prediction models -- part 1: binary outcomes</title>
      <link>https://arxiv.org/abs/2407.09293</link>
      <description>arXiv:2407.09293v2 Announce Type: replace 
Abstract: When developing a clinical prediction model, the sample size of the development dataset is a key consideration. Small sample sizes lead to greater concerns of overfitting, instability, poor performance and lack of fairness. Previous research has outlined minimum sample size calculations to minimise overfitting and precisely estimate the overall risk. However even when meeting these criteria, the uncertainty (instability) in individual-level risk estimates may be considerable. In this article we propose how to examine and calculate the sample size required for developing a model with acceptably precise individual-level risk estimates to inform decisions and improve fairness. We outline a five-step process to be used before data collection or when an existing dataset is available. It requires researchers to specify the overall risk in the target population, the (anticipated) distribution of key predictors in the model, and an assumed 'core model' either specified directly (i.e., a logistic regression equation is provided) or based on specified C-statistic and relative effects of (standardised) predictors. We produce closed-form solutions that decompose the variance of an individual's risk estimate into Fisher's unit information matrix, predictor values and total sample size; this allows researchers to quickly calculate and examine individual-level uncertainty interval widths and classification instability for specified sample sizes. Such information can be presented to key stakeholders (e.g., health professionals, patients, funders) using prediction and classification instability plots to help identify the (target) sample size required to improve trust, reliability and fairness in individual predictions. Our proposal is implemented in software module pmstabilityss. We provide real examples and emphasise the importance of clinical context including any risk thresholds for decision making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09293v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Richard D Riley, Gary S Collins, Rebecca Whittle, Lucinda Archer, Kym IE Snell, Paula Dhiman, Laura Kirton, Amardeep Legha, Xiaoxuan Liu, Alastair Denniston, Frank E Harrell Jr, Laure Wynants, Glen P Martin, Joie Ensor</dc:creator>
    </item>
    <item>
      <title>PCM Selector: Penalized Covariate-Mediator Selection Operator for Evaluating Linear Causal Effects</title>
      <link>https://arxiv.org/abs/2412.18180</link>
      <description>arXiv:2412.18180v2 Announce Type: replace 
Abstract: For a data-generating process for random variables that can be described with a linear structural equation model, we consider a situation in which (i) a set of covariates satisfying the back-door criterion cannot be observed or (ii) such a set can be observed, but standard statistical estimation methods cannot be applied to estimate causal effects because of multicollinearity/high-dimensional data problems. We propose a novel two-stage penalized regression approach, the penalized covariate-mediator selection operator (PCM Selector), to estimate the causal effects in such scenarios. Unlike existing penalized regression analyses, when a set of intermediate variables is available, PCM Selector provides a consistent or less biased estimator of the causal effect. In addition, PCM Selector provides a variable selection procedure for intermediate variables to obtain better estimation accuracy of the causal effects than does the back-door criterion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18180v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hisayoshi Nanmo, Manabu Kuroki</dc:creator>
    </item>
    <item>
      <title>Isotonic propensity score matching</title>
      <link>https://arxiv.org/abs/2207.08868</link>
      <description>arXiv:2207.08868v3 Announce Type: replace-cross 
Abstract: We propose a one-to-many matching estimator of the average treatment effect based on propensity scores estimated by isotonic regression. This approach is predicated on the assumption of monotonicity in the propensity score function, a condition that can be justified in many economic applications. We show that the nature of the isotonic estimator can help us to fix many problems of existing matching methods, including efficiency, choice of the number of matches, choice of tuning parameters, robustness to propensity score misspecification, and bootstrap validity. As a by-product, a uniformly consistent isotonic estimator is developed for our proposed matching method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.08868v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mengshan Xu, Taisuke Otsu</dc:creator>
    </item>
    <item>
      <title>A Note on the Prediction-Powered Bootstrap</title>
      <link>https://arxiv.org/abs/2405.18379</link>
      <description>arXiv:2405.18379v3 Announce Type: replace-cross 
Abstract: We introduce PPBoot: a bootstrap-based method for prediction-powered inference. PPBoot is applicable to arbitrary estimation problems and is very simple to implement, essentially only requiring one application of the bootstrap. Through a series of examples, we demonstrate that PPBoot often performs nearly identically to (and sometimes better than) the earlier PPI(++) method based on asymptotic normality$\unicode{x2013}$when the latter is applicable$\unicode{x2013}$without requiring any asymptotic characterizations. Given its versatility, PPBoot could simplify and expand the scope of application of prediction-powered inference to problems where central limit theorems are hard to prove.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18379v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tijana Zrnic</dc:creator>
    </item>
    <item>
      <title>Detecting Cointegrating Relations in Non-stationary Matrix-Valued Time Series</title>
      <link>https://arxiv.org/abs/2411.05601</link>
      <description>arXiv:2411.05601v2 Announce Type: replace-cross 
Abstract: This paper proposes a Matrix Error Correction Model to identify cointegration relations in matrix-valued time series. We hereby allow separate cointegrating relations along the rows and columns of the matrix-valued time series and use information criteria to select the cointegration ranks. Through Monte Carlo simulations and a macroeconomic application, we demonstrate that our approach provides a reliable estimation of the number of cointegrating relationships.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05601v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alain Hecq, Ivan Ricardo, Ines Wilms</dc:creator>
    </item>
  </channel>
</rss>
