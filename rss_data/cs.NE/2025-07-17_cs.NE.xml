<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NE</link>
    <description>cs.NE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 18 Jul 2025 04:00:47 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Adversarial attacks to image classification systems using evolutionary algorithms</title>
      <link>https://arxiv.org/abs/2507.13136</link>
      <description>arXiv:2507.13136v1 Announce Type: new 
Abstract: Image classification currently faces significant security challenges due to adversarial attacks, which consist of intentional alterations designed to deceive classification models based on artificial intelligence. This article explores an approach to generate adversarial attacks against image classifiers using a combination of evolutionary algorithms and generative adversarial networks. The proposed approach explores the latent space of a generative adversarial network with an evolutionary algorithm to find vectors representing adversarial attacks. The approach was evaluated in two case studies corresponding to the classification of handwritten digits and object images. The results showed success rates of up to 35% for handwritten digits, and up to 75% for object images, improving over other search methods and reported results in related works. The applied method proved to be effective in handling data diversity on the target datasets, even in problem instances that presented additional challenges due to the complexity and richness of information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13136v1</guid>
      <category>cs.NE</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3712256.3726429</arxiv:DOI>
      <dc:creator>Sergio Nesmachnow, Jamal Toutouh</dc:creator>
    </item>
    <item>
      <title>Multi-population GAN Training: Analyzing Co-Evolutionary Algorithms</title>
      <link>https://arxiv.org/abs/2507.13157</link>
      <description>arXiv:2507.13157v1 Announce Type: new 
Abstract: Generative adversarial networks (GANs) are powerful generative models but remain challenging to train due to pathologies suchas mode collapse and instability. Recent research has explored co-evolutionary approaches, in which populations of generators and discriminators are evolved, as a promising solution. This paper presents an empirical analysis of different coevolutionary GAN training strategies, focusing on the impact of selection and replacement mechanisms. We compare (mu,lambda), (mu+lambda) with elitism, and (mu+lambda) with tournament selection coevolutionary schemes, along with a non-evolutionary population based multi-generator multi-discriminator GAN baseline, across both synthetic low-dimensional datasets (blob and gaussian mixtures) and an image-based benchmark (MNIST). Results show that full generational replacement, i.e., (mu,lambda), consistently outperforms in terms of both sample quality and diversity, particularly when combined with larger offspring sizes. In contrast, elitist approaches tend to converge prematurely and suffer from reduced diversity. These findings highlight the importance of balancing exploration and exploitation dynamics in coevolutionary GAN training and provide guidance for designing more effective population-based generative models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13157v1</guid>
      <category>cs.NE</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3712255.3734362</arxiv:DOI>
      <dc:creator>Walter P. Casas, Jamal Toutouh</dc:creator>
    </item>
    <item>
      <title>The Generalist Brain Module: Module Repetition in Neural Networks in Light of the Minicolumn Hypothesis</title>
      <link>https://arxiv.org/abs/2507.12473</link>
      <description>arXiv:2507.12473v1 Announce Type: cross 
Abstract: While modern AI continues to advance, the biological brain remains the pinnacle of neural networks in its robustness, adaptability, and efficiency. This review explores an AI architectural path inspired by the brain's structure, particularly the minicolumn hypothesis, which views the neocortex as a distributed system of repeated modules - a structure we connect to collective intelligence (CI). Despite existing work, there is a lack of comprehensive reviews connecting the cortical column to the architectures of repeated neural modules. This review aims to fill that gap by synthesizing historical, theoretical, and methodological perspectives on neural module repetition. We distinguish between architectural repetition - reusing structure - and parameter-shared module repetition, where the same functional unit is repeated across a network. The latter exhibits key CI properties such as robustness, adaptability, and generalization. Evidence suggests that the repeated module tends to converge toward a generalist module: simple, flexible problem solvers capable of handling many roles in the ensemble. This generalist tendency may offer solutions to longstanding challenges in modern AI: improved energy efficiency during training through simplicity and scalability, and robust embodied control via generalization. While empirical results suggest such systems can generalize to out-of-distribution problems, theoretical results are still lacking. Overall, architectures featuring module repetition remain an emerging and unexplored architectural strategy, with significant untapped potential for both efficiency, robustness, and adaptiveness. We believe that a system that adopts the benefits of CI, while adhering to architectural and functional principles of the minicolumns, could challenge the modern AI problems of scalability, energy consumption, and democratization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12473v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mia-Katrin Kvalsund, Mikkel Elle Lepper{\o}d</dc:creator>
    </item>
    <item>
      <title>Cognitive Modelling Aspects of Neurodevelopmental Disorders Using Standard and Oscillating Neighbourhood SOM Neural Networks</title>
      <link>https://arxiv.org/abs/2507.12567</link>
      <description>arXiv:2507.12567v1 Announce Type: cross 
Abstract: Background/Introduction: In this paper, the neural network class of Self-Organising Maps (SOMs) is investigated in terms of its theoretical and applied validity for cognitive modelling, particularly of neurodevelopmental disorders.
  Methods: A modified SOM network type, with increased biological plausibility, incorporating a type of cortical columnar oscillation in the form of an oscillating Topological Neighbourhood (TN), is introduced and applied alongside the standard SOM. Aspects of two neurodevelopmental disorders, autism and schizophrenia, are modelled using SOM networks, based on existing neurocomputational theories. Both standard and oscillating-TN SOM training is employed with targeted modifications in the TN width function. Computer simulations are conducted using revised versions of a previously introduced model (IPSOM) based on a new modelling hypothesis.
  Results/Conclusions: The results demonstrate that there is strong similarity between standard and oscillating-TN SOM modelling in terms of map formation behaviour, output and structure, while the oscillating version offers a more realistic computational analogue of brain function. Neuroscientific and computational arguments are presented to validate the proposed SOM modification within a cognitive modelling framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12567v1</guid>
      <category>q-bio.NC</category>
      <category>cs.NE</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Spyridon Revithis, Nadine Marcus</dc:creator>
    </item>
    <item>
      <title>Topology-Aware Activation Functions in Neural Networks</title>
      <link>https://arxiv.org/abs/2507.12874</link>
      <description>arXiv:2507.12874v1 Announce Type: cross 
Abstract: This study explores novel activation functions that enhance the ability of neural networks to manipulate data topology during training. Building on the limitations of traditional activation functions like $\mathrm{ReLU}$, we propose $\mathrm{SmoothSplit}$ and $\mathrm{ParametricSplit}$, which introduce topology "cutting" capabilities. These functions enable networks to transform complex data manifolds effectively, improving performance in scenarios with low-dimensional layers. Through experiments on synthetic and real-world datasets, we demonstrate that $\mathrm{ParametricSplit}$ outperforms traditional activations in low-dimensional settings while maintaining competitive performance in higher-dimensional ones. Our findings highlight the potential of topology-aware activation functions in advancing neural network architectures. The code is available via https://github.com/Snopoff/Topology-Aware-Activations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12874v1</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.14428/esann/2025.ES2025-211</arxiv:DOI>
      <arxiv:journal_reference>ESANN 2025, European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, Bruges, Belgium, April 23-25, 2025</arxiv:journal_reference>
      <dc:creator>Pavel Snopov, Oleg R. Musin</dc:creator>
    </item>
    <item>
      <title>The late-stage training dynamics of (stochastic) subgradient descent on homogeneous neural networks</title>
      <link>https://arxiv.org/abs/2502.05668</link>
      <description>arXiv:2502.05668v3 Announce Type: replace-cross 
Abstract: We analyze the implicit bias of constant step stochastic subgradient descent (SGD). We consider the setting of binary classification with homogeneous neural networks - a large class of deep neural networks with ReLU-type activation functions such as MLPs and CNNs without biases. We interpret the dynamics of normalized SGD iterates as an Euler-like discretization of a conservative field flow that is naturally associated to the normalized classification margin. Owing to this interpretation, we show that normalized SGD iterates converge to the set of critical points of the normalized margin at late-stage training (i.e., assuming that the data is correctly classified with positive normalized margin). Up to our knowledge, this is the first extension of the analysis of Lyu and Li (2020) on the discrete dynamics of gradient descent to the nonsmooth and stochastic setting. Our main result applies to binary classification with exponential or logistic losses. We additionally discuss extensions to more general settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05668v3</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sholom Schechtman, Nicolas Schreuder</dc:creator>
    </item>
    <item>
      <title>Multiple-Frequencies Population-Based Training</title>
      <link>https://arxiv.org/abs/2506.03225</link>
      <description>arXiv:2506.03225v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning's high sensitivity to hyperparameters is a source of instability and inefficiency, creating significant challenges for practitioners. Hyperparameter Optimization (HPO) algorithms have been developed to address this issue, among them Population-Based Training (PBT) stands out for its ability to generate hyperparameters schedules instead of fixed configurations. PBT trains a population of agents, each with its own hyperparameters, frequently ranking them and replacing the worst performers with mutations of the best agents. These intermediate selection steps can cause PBT to focus on short-term improvements, leading it to get stuck in local optima and eventually fall behind vanilla Random Search over longer timescales. This paper studies how this greediness issue is connected to the choice of evolution frequency, the rate at which the selection is done. We propose Multiple-Frequencies Population-Based Training (MF-PBT), a novel HPO algorithm that addresses greediness by employing sub-populations, each evolving at distinct frequencies. MF-PBT introduces a migration process to transfer information between sub-populations, with an asymmetric design to balance short and long-term optimization. Extensive experiments on the Brax suite demonstrate that MF-PBT improves sample efficiency and long-term performance, even without actually tuning hyperparameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03225v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wa\"el Doulazmi, Auguste Lehuger, Marin Toromanoff, Valentin Charraut, Thibault Buhet, Fabien Moutarde</dc:creator>
    </item>
  </channel>
</rss>
