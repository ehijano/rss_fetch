<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 08 Nov 2024 02:44:58 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Statistical analysis to assess porosity equivalence with uncertainty across additively manufactured parts for fatigue applications</title>
      <link>https://arxiv.org/abs/2411.03401</link>
      <description>arXiv:2411.03401v1 Announce Type: new 
Abstract: Previous work on fatigue prediction in Powder Bed Fusion - Laser Beam has shown that the estimate of the largest pore size within the stressed volume is correlated with the resulting fatigue behavior in porosity-driven failures. However, single value estimates for the largest pore size are insufficient to capture the experimentally observed scatter in fatigue properties. To address this gap, in this work, we incorporate uncertainty quantification into extreme value statistics to estimate the largest pore size distribution in a given volume of material by capturing uncertainty in the number of pores present and the upper tail parameters. We then applied this statistical framework to compare the porosity equivalence between two geometries: a 4-point bend fatigue specimen and an axial fatigue specimen in the gauge section. Both geometries were manufactured with the same process conditions using Ti-6Al-4V, followed by porosity characterization via X-ray Micro CT. The results show that the largest pore size distribution of the 4-point bend specimen is insufficient to accurately capture the largest pore size observed in the axial fatigue specimen, despite similar dimensions. Based on our findings, we provide insight into the design of witness coupons that exhibit part-to-coupon porosity equivalence for fatigue.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03401v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Justin P. Miner, Sneha Prabha Narra</dc:creator>
    </item>
    <item>
      <title>Performance-Based Risk Assessment for Large-Scale Transportation Networks Using the Transitional Markov Chain Monte Carlo Method</title>
      <link>https://arxiv.org/abs/2411.03580</link>
      <description>arXiv:2411.03580v1 Announce Type: new 
Abstract: Accurately assessing failure risk due to asset deterioration and/or extreme events is essential for efficient transportation asset management. Traditional risk assessment is conducted for individual assets by either focusing on the economic risk to asset owners or relying on empirical proxies of systemwide consequences. Risk assessment directly based on system performance (e.g., network capacity) is largely limited due to (1) an exponentially increasing number of system states for accurate performance evaluation, (2) potential contribution of system states with low likelihood yet high consequences (i.e., "gray swan" events) to system state, and (3) lack of actionable information for asset management from risk assessment results. To address these challenges, this paper introduces a novel approach to performance-based risk assessment for large-scale transportation networks. The new approach is underpinned by the Transitional Markov Chain Monte Carlo (TMCMC) method, a sequential sampling technique originally developed for Bayesian updating. The risk assessment problem is reformulated such that (1) the system risk becomes the normalizing term (i.e., evidence) of a high-dimensional posterior distribution, and (2) the final posterior samples from TMCMC yield risk-based importance measures for different assets. Two types of analytical examples are developed to demonstrate the effectiveness and efficiency of the proposed approach as the number of assets increases and the influence of gray swan events grows. The new approach is further applied in a case study on the Oregon highway network, serving as a real-world example of large-scale transportation networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03580v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1061/AJRUA6/RUENG-1402</arxiv:DOI>
      <dc:creator>Anteneh Z. Deriba, David Y. Yang</dc:creator>
    </item>
    <item>
      <title>Towards pandemic preparedness: ability to estimate high-resolution social contact patterns from longitudinal surveys</title>
      <link>https://arxiv.org/abs/2411.03774</link>
      <description>arXiv:2411.03774v1 Announce Type: new 
Abstract: Social contact surveys are an important tool to assess infection risks within populations, and the effect of non-pharmaceutical interventions on social behaviour during disease outbreaks, epidemics, and pandemics. Numerous longitudinal social contact surveys were conducted during the COVID-19 era, however data analysis is plagued by reporting fatigue, a phenomenon whereby the average number of social contacts reported declines with the number of repeat participations and as participants' engagement decreases over time. Using data from the German COVIMOD Study between April 2020 to December 2021, we demonstrate that reporting fatigue varied considerably by sociodemographic factors and was consistently strongest among parents reporting children contacts (parental proxy reporting), students, middle-aged individuals, those in full-time employment and those self-employed. We find further that, when using data from first-time participants as gold standard, statistical models incorporating a simple logistic function to control for reporting fatigue were associated with substantially improved estimation accuracy relative to models with no reporting fatigue adjustments, and that no cap on the number of repeat participations was required. These results indicate that existing longitudinal contact survey data can be meaningfully interpreted under an easy-to-implement statistical approach adressing reporting fatigue confounding, and that longitudinal designs including repeat participants are a viable option for future social contact survey designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03774v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shozen Dan (on behalf of the Machine Learning,Global Health network), Joshua Tegegne (on behalf of the Machine Learning,Global Health network), Yu Chen (on behalf of the Machine Learning,Global Health network), Zhi Ling (on behalf of the Machine Learning,Global Health network), Veronika K. Jaeger (on behalf of the Machine Learning,Global Health network), Andr\'e Karch (on behalf of the Machine Learning,Global Health network), Swapnil Mishra (on behalf of the Machine Learning,Global Health network), Oliver Ratmann (on behalf of the Machine Learning,Global Health network)</dc:creator>
    </item>
    <item>
      <title>Multilingual hierarchical classification of job advertisements for job vacancy statistics</title>
      <link>https://arxiv.org/abs/2411.03779</link>
      <description>arXiv:2411.03779v1 Announce Type: new 
Abstract: The goal of this paper is to develop a multilingual classifier and conditional probability estimator of occupation codes for online job advertisements according in accordance with the International Standard Classification of Occupations (ISCO) extended with the Polish Classification of Occupations and Specializations (KZiS), which is analogous to the European Classification of Occupations. In this paper, we utilise a range of data sources, including a novel one, namely the Central Job Offers Database, which is a register of all vacancies submitted to Public Employment Offices. Their staff members code the vacancies according to the ISCO and KZiS. A hierarchical multi-class classifier has been developed based on the transformer architecture. The classifier begins by encoding the jobs found in advertisements to the widest 1-digit occupational group, and then narrows the assignment to a 6-digit occupation code. We show that incorporation of the hierarchical structure of occupations improves prediction accuracy by 1-2 percentage points, particularly for the hand-coded online job advertisements. Finally, a bilingual (Polish and English) and multilingual (24 languages) model is developed based on data translated using closed and open-source software. The open-source software is provided for the benefit of the official statistics community, with a particular focus on international comparability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03779v1</guid>
      <category>stat.AP</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.ML</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Maciej Ber\k{e}sewicz, Marek Wydmuch, Herman Cherniaiev, Robert Pater</dc:creator>
    </item>
    <item>
      <title>Bayesian algorithmic perfumery: A Hierarchical Relevance Vector Machine for the Estimation of Personalized Fragrance Preferences based on Three Sensory Layers and Jungian Personality Archetypes</title>
      <link>https://arxiv.org/abs/2411.03965</link>
      <description>arXiv:2411.03965v1 Announce Type: new 
Abstract: This study explores a Bayesian algorithmic approach to personalized fragrance recommendation by integrating hierarchical Relevance Vector Machines (RVM) and Jungian personality archetypes. The paper proposes a structured model that links individual scent preferences for top, middle, and base notes to personality traits derived from Jungian archetypes, such as the Hero, Caregiver, and Explorer, among others. The algorithm utilizes Bayesian updating to dynamically refine predictions as users interact with each fragrance note. This iterative process allows for the personalization of fragrance experiences based on prior data and personality assessments, leading to adaptive and interpretable recommendations. By combining psychological theory with Bayesian machine learning, this approach addresses the complexity of modeling individual preferences while capturing user-specific and population-level trends. The study highlights the potential of hierarchical Bayesian frameworks in creating customized olfactory experiences, informed by psychological and demographic factors, contributing to advancements in personalized product design and machine learning applications in sensory-based industries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03965v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rolando Gonzales Martinez</dc:creator>
    </item>
    <item>
      <title>A Surrogate Model for Quay Crane Scheduling Problem</title>
      <link>https://arxiv.org/abs/2411.03324</link>
      <description>arXiv:2411.03324v1 Announce Type: cross 
Abstract: In ports, a variety of tasks are carried out, and scheduling these tasks is crucial due to its significant impact on productivity, making the generation of precise plans essential. This study proposes a method to solve the Quay Crane Scheduling Problem (QCSP), a representative task scheduling problem in ports known to be NP-Hard, more quickly and accurately. First, the study suggests a method to create more accurate work plans for Quay Cranes (QCs) by learning from actual port data to accurately predict the working speed of QCs. Next, a Surrogate Model is proposed by combining a Machine Learning (ML) model with a Genetic Algorithm (GA), which is widely used to solve complex optimization problems, enabling faster and more precise exploration of solutions. Unlike methods that use fixed-dimensional chromosome encoding, the proposed methodology can provide solutions for encodings of various dimensions. To validate the performance of the newly proposed methodology, comparative experiments were conducted, demonstrating faster search speeds and improved fitness scores. The method proposed in this study can be applied not only to QCSP but also to various NP-Hard problems, and it opens up possibilities for the further development of advanced search algorithms by combining heuristic algorithms with ML models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03324v1</guid>
      <category>cs.NE</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kikun Park, Hyerim Bae</dc:creator>
    </item>
    <item>
      <title>Analysis of Bipartite Networks in Anime Series: Textual Analysis, Topic Clustering, and Modeling</title>
      <link>https://arxiv.org/abs/2411.03333</link>
      <description>arXiv:2411.03333v1 Announce Type: cross 
Abstract: This article analyzes a specific bipartite network that shows the relationships between users and anime, examining how the descriptions of anime influence the formation of user communities. In particular, we introduce a new variable that quantifies the frequency with which words from a description appear in specific word clusters. These clusters are generated from a bigram analysis derived from all descriptions in the database. This approach fully characterizes the dynamics of these communities and shows how textual content affect the cohesion and structure of the social network among anime enthusiasts. Our findings suggest that there may be significant implications for the design of recommendation systems and the enhancement of user experience on anime platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03333v1</guid>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan Sosa, Alejandro Urrego-Lopez, Cesar Prieto</dc:creator>
    </item>
    <item>
      <title>Pedestrian Volume Prediction Using a Diffusion Convolutional Gated Recurrent Unit Model</title>
      <link>https://arxiv.org/abs/2411.03360</link>
      <description>arXiv:2411.03360v1 Announce Type: cross 
Abstract: Effective models for analysing and predicting pedestrian flow are important to ensure the safety of both pedestrians and other road users. These tools also play a key role in optimising infrastructure design and geometry and supporting the economic utility of interconnected communities. The implementation of city-wide automatic pedestrian counting systems provides researchers with invaluable data, enabling the development and training of deep learning applications that offer better insights into traffic and crowd flows. Benefiting from real-world data provided by the City of Melbourne pedestrian counting system, this study presents a pedestrian flow prediction model, as an extension of Diffusion Convolutional Grated Recurrent Unit (DCGRU) with dynamic time warping, named DCGRU-DTW. This model captures the spatial dependencies of pedestrian flow through the diffusion process and the temporal dependency captured by Gated Recurrent Unit (GRU). Through extensive numerical experiments, we demonstrate that the proposed model outperforms the classic vector autoregressive model and the original DCGRU across multiple model accuracy metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03360v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiwei Dong, Tingjin Chu, Lele Zhang, Hadi Ghaderi, Hanfang Yang</dc:creator>
    </item>
    <item>
      <title>Hamiltonian Monte Carlo methods for spectroscopy data analysis</title>
      <link>https://arxiv.org/abs/2411.03523</link>
      <description>arXiv:2411.03523v1 Announce Type: cross 
Abstract: We present a scalable Bayesian framework for the analysis of confocal fluorescence spectroscopy data, addressing key limitations in traditional fluorescence correlation spectroscopy methods. Our framework captures molecular motion, microscope optics, and photon detection with high fidelity, enabling statistical inference of molecule trajectories from raw photon count data, introducing a superresolution parameter which further enhances trajectory estimation beyond the native time resolution of data acquisition. To handle the high dimensionality of the arising posterior distribution, we develop a family of Hamiltonian Monte Carlo (HMC) algorithms that leverages the unique characteristics inherent to spectroscopy data analysis. Here, due to the highly-coupled correlation structure of the target posterior distribution, HMC requires the numerical solution of a stiff ordinary differential equation containing a two-scale discrete Laplacian. By considering the spectral properties of this operator, we produce a CFL-type integrator stability condition for the standard St\"ormer-Verlet integrator used in HMC. To circumvent this instability we introduce a semi-implicit (IMEX) method which treats the stiff and non-stiff parts differently, while leveraging the sparse structure of the discrete Laplacian for computational efficiency. Detailed numerical experiments demonstrate that this method improves upon fully explicit approaches, allowing larger HMC step sizes and maintaining second-order accuracy in position and energy. Our framework provides a foundation for extensions to more complex models such as surface constrained molecular motion or motion with multiple diffusion modes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03523v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>q-bio.BM</category>
      <category>stat.AP</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel McBride, Ioannis Sgouralis</dc:creator>
    </item>
    <item>
      <title>Zero-Coupon Treasury Yield Curve with VIX as Stochastic Volatility</title>
      <link>https://arxiv.org/abs/2411.03699</link>
      <description>arXiv:2411.03699v1 Announce Type: cross 
Abstract: We apply Principal Component Analysis for zero-coupon Treasury bonds to get level, slope, and curvature series. We model these as autoregressions of order 1, and analyze their innovations. For slope, but not for level and curvature, dividing these innovations by the Volatility Index VIX made for Standard \&amp; Poor 500 makes them closer to independent identically distributed normal. We state and prove stability results for bond returns based on this observation. We chose zero-coupon as opposed to classic coupon Treasury bonds because it is much easier to compute returns for these.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03699v1</guid>
      <category>q-fin.ST</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jihyun Park, Andrey Sarantsev</dc:creator>
    </item>
    <item>
      <title>Human-in-the-Loop Feature Selection Using Interpretable Kolmogorov-Arnold Network-based Double Deep Q-Network</title>
      <link>https://arxiv.org/abs/2411.03740</link>
      <description>arXiv:2411.03740v1 Announce Type: cross 
Abstract: Feature selection is critical for improving the performance and interpretability of machine learning models, particularly in high-dimensional spaces where complex feature interactions can reduce accuracy and increase computational demands. Existing approaches often rely on static feature subsets or manual intervention, limiting adaptability and scalability. However, dynamic, per-instance feature selection methods and model-specific interpretability in reinforcement learning remain underexplored. This study proposes a human-in-the-loop (HITL) feature selection framework integrated into a Double Deep Q-Network (DDQN) using a Kolmogorov-Arnold Network (KAN). Our novel approach leverages simulated human feedback and stochastic distribution-based sampling, specifically Beta, to iteratively refine feature subsets per data instance, improving flexibility in feature selection. The KAN-DDQN achieved notable test accuracies of 93% on MNIST and 83% on FashionMNIST, outperforming conventional MLP-DDQN models by up to 9%. The KAN-based model provided high interpretability via symbolic representation while using 4 times fewer neurons in the hidden layer than MLPs did. Comparatively, the models without feature selection achieved test accuracies of only 58% on MNIST and 64% on FashionMNIST, highlighting significant gains with our framework. Pruning and visualization further enhanced model transparency by elucidating decision pathways. These findings present a scalable, interpretable solution for feature selection that is suitable for applications requiring real-time, adaptive decision-making with minimal human oversight.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03740v1</guid>
      <category>cs.LG</category>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Abrar Jahin, M. F. Mridha, Nilanjan Dey</dc:creator>
    </item>
    <item>
      <title>A Temporal Playbook for Multiple Wave Dengue Pandemic from Latin America and Asia to Italy</title>
      <link>https://arxiv.org/abs/2411.03837</link>
      <description>arXiv:2411.03837v1 Announce Type: cross 
Abstract: We show that the epidemiological Renormalization Group (eRG) framework is a useful and minimal tool to effectively describe the temporal evolution of the Dengue multi-wave pandemics. We test the framework on the Dengue history of several countries located in both Latin America and Asia. We also observe a strong correlation between the total number of infected individuals and the changes in the local temperature. Our results further support the expectation that global warming is bound to increase the cases of Dengue worldwide. We then move to investigate, via the eRG, the recent outbreak in Fano, Italy and offer our projections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03837v1</guid>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessandra D'Alise, Davide Iacobacci, Francesco Sannino</dc:creator>
    </item>
    <item>
      <title>Estimating breakpoints between climate states in the Cenozoic Era</title>
      <link>https://arxiv.org/abs/2404.08336</link>
      <description>arXiv:2404.08336v4 Announce Type: replace 
Abstract: This study presents a statistical time-domain approach for identifying transitions between climate states, referred to as breakpoints, using well-established econometric tools. We analyze a 67.1 million year record of the oxygen isotope ratio delta-O-18 derived from benthic foraminifera. The dataset is presented in Westerhold et al. (2020), where the authors use recurrence analysis to identify six climate states. Fixing the number of breakpoints to five, our procedure results in breakpoint estimates that closely align with those identified by Westerhold et al. (2020). By treating the number of breakpoints as a parameter to be estimated, we provide the statistical justification for more than five breakpoints in the time series. Further, our approach offers the advantage of constructing confidence intervals for the breakpoints, and it allows for testing the number of breakpoints present in the time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08336v4</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mikkel Bennedsen, Eric Hillebrand, Siem Jan Koopman, Kathrine By Larsen</dc:creator>
    </item>
    <item>
      <title>Long-term foehn reconstruction combining unsupervised and supervised learning</title>
      <link>https://arxiv.org/abs/2406.01818</link>
      <description>arXiv:2406.01818v2 Announce Type: replace 
Abstract: Foehn winds, characterized by abrupt temperature increases and wind speed changes, significantly impact regions on the leeward side of mountain ranges, e.g., by spreading wildfires. Understanding how foehn occurrences change under climate change is crucial. Unfortunately, foehn cannot be measured directly but has to be inferred from meteorological measurements employing suitable classification schemes. Hence, this approach is typically limited to specific periods for which the necessary data are available. We present a novel approach for reconstructing historical foehn occurrences using a combination of unsupervised and supervised probabilistic statistical learning methods. We utilize in-situ measurements (available for recent decades) to train an unsupervised learner (finite mixture model) for automatic foehn classification. These labeled data are then linked to reanalysis data (covering longer periods) using a supervised learner (lasso or boosting). This allows to reconstruct past foehn probabilities based solely on reanalysis data. Applying this method to ERA5 reanalysis data for six stations across Switzerland and Austria achieves accurate hourly reconstructions of north and south foehn occurrence, respectively, dating back to 1940. This paves the way for investigating how seasonal foehn patterns have evolved over the past 83 years, providing valuable insights into climate change impacts on these critical wind events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01818v2</guid>
      <category>stat.AP</category>
      <category>physics.ao-ph</category>
      <category>stat.ML</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1002/joc.8673</arxiv:DOI>
      <dc:creator>Reto Stauffer, Achim Zeileis, Georg J. Mayr</dc:creator>
    </item>
    <item>
      <title>Kairosis: A method for dynamical probability forecast aggregation informed by Bayesian change point detection</title>
      <link>https://arxiv.org/abs/2408.00785</link>
      <description>arXiv:2408.00785v2 Announce Type: replace 
Abstract: We present a new method, "kairosis", for aggregating probability forecasts made over a time period of a single outcome determined at the end of that period. Informed by work on Bayesian change-point detection, we begin by constructing for each time during the period a posterior probability that the forecasts before and after this time are distributed differently. The distribution of these probabilities is then integrated to give a cumulative mass function, which is then used to calculate a weighted median forecast. The effect is to construct a pool in which those forecasts are most heavily weighted which have been made since the likely most recent change in the forecasts' distribution. Kairosis outperforms standard methods, and is especially suitable for geopolitical forecasting tournaments because it is observed to be robust across disparate questions and forecaster distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00785v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zane Hassoun, Ben Powell, Niall MacKay</dc:creator>
    </item>
    <item>
      <title>Profile Monitoring via Eigenvector Perturbation</title>
      <link>https://arxiv.org/abs/2205.15422</link>
      <description>arXiv:2205.15422v2 Announce Type: replace-cross 
Abstract: In Statistical Process Control, control charts are often used to detect undesirable behavior of sequentially observed quality characteristics. Designing a control chart with desirably low False Alarm Rate (FAR) and detection delay ($ARL_1$) is an important challenge especially when the sampling rate is high and the control chart has an In-Control Average Run Length, called $ARL_0$, of 200 or more, as commonly found in practice. Unfortunately, arbitrary reduction of the FAR typically increases the $ARL_1$. Motivated by eigenvector perturbation theory, we propose the Eigenvector Perturbation Control Chart for computationally fast nonparametric profile monitoring. Our simulation studies show that it outperforms the competition and achieves both $ARL_1 \approx 1$ and $ARL_0 &gt; 10^6$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.15422v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takayuki Iguchi, Andr\'es F. Barrientos, Eric Chicken, Debajyoti Sinha</dc:creator>
    </item>
    <item>
      <title>The VIX as Stochastic Volatility for Corporate Bonds</title>
      <link>https://arxiv.org/abs/2410.22498</link>
      <description>arXiv:2410.22498v3 Announce Type: replace-cross 
Abstract: Classic stochastic volatility models assume volatility is unobservable. We use the VIX for consider it observable, and use the Volatility Index: S\&amp;P 500 VIX. This index was designed to measure volatility of S&amp;P 500. We apply it to a different segment: Corporate bond markets. We fit time series models for spreads between corporate and 10-year Treasury bonds. Next, we divide residuals by VIX. Our main idea is such division makes residuals closer to the ideal case of a Gaussian white noise. This is remarkable, since these residuals and VIX come from separate market segments. We also discuss total returns of Bank of America corporate bonds. We conclude with the analysis of long-term behavior of these models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22498v3</guid>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Thu, 07 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jihyun Park, Andrey Sarantsev</dc:creator>
    </item>
  </channel>
</rss>
