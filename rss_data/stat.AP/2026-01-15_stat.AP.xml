<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 16 Jan 2026 02:38:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>3D-SONAR: Self-Organizing Network for 3D Anomaly Ranking</title>
      <link>https://arxiv.org/abs/2601.09294</link>
      <description>arXiv:2601.09294v1 Announce Type: new 
Abstract: Surface anomaly detection using 3D point cloud data has gained increasing attention in industrial inspection. However, most existing methods rely on deep learning techniques that are highly dependent on large-scale datasets for training, which are difficult and expensive to acquire in real-world applications. To address this challenge, we propose a novel method based on self-organizing network for 3D anomaly ranking, also named 3D-SONAR. The core idea is to model the 3D point cloud as a dynamic system, where the points are represented as an undirected graph and interact via attractive and repulsive forces. The energy distribution induced by these forces can reveal surface anomalies. Experimental results show that our method achieves superior anomaly detection performance in both open surface and closed surface without training. This work provides a new perspective on unsupervised inspection and highlights the potential of physics-inspired models in industrial anomaly detection tasks with limited data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09294v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Guodong Xu, Juan Du, Hui Yang</dc:creator>
    </item>
    <item>
      <title>Kladia Liquidity Deflator (KLD): A Debt-Indexed Deflationary Token on XRPL</title>
      <link>https://arxiv.org/abs/2601.08853</link>
      <description>arXiv:2601.08853v1 Announce Type: cross 
Abstract: Kladia Liquidity Deflator (KLD) is an XRPL-based, debt-indexed token whose supply dynamics respond directly to a debt index derived from macroeconomic data sources. The model links indebtedness to deterministic adjustments in issuance, burns, and escrow release caps, creating a rule-based deflationary mechanism that strengthens as debt rises. With a fixed maximum supply of 10 billion KLD, the mechanism is implemented through XRPL oracles and governance. Escrow locking depends on the TokenEscrow amendment; until it is active network-wide, allocations will be secured in a multi-signature vault with published rules and public monitoring. KLD provides a transparent and mathematically grounded framework for a macro-responsive digital asset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.08853v1</guid>
      <category>q-fin.GN</category>
      <category>stat.AP</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kiarash Firouzi, Parham Pajouhi</dc:creator>
    </item>
    <item>
      <title>From Hawkes Processes to Attention: Time-Modulated Mechanisms for Event Sequences</title>
      <link>https://arxiv.org/abs/2601.09220</link>
      <description>arXiv:2601.09220v1 Announce Type: cross 
Abstract: Marked Temporal Point Processes (MTPPs) arise naturally in medical, social, commercial, and financial domains. However, existing Transformer-based methods mostly inject temporal information only via positional encodings, relying on shared or parametric decay structures, which limits their ability to capture heterogeneous and type-specific temporal effects. Inspired by this observation, we derive a novel attention operator called Hawkes Attention from the multivariate Hawkes process theory for MTPP, using learnable per-type neural kernels to modulate query, key and value projections, thereby replacing the corresponding parts in the traditional attention. Benefited from the design, Hawkes Attention unifies event timing and content interaction, learning both the time-relevant behavior and type-specific excitation patterns from the data. The experimental results show that our method achieves better performance compared to the baselines. In addition to the general MTPP, our attention mechanism can also be easily applied to specific temporal structures, such as time series forecasting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09220v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinzi Tan, Kejian Zhang, Junhan Yu, Doudou Zhou</dc:creator>
    </item>
    <item>
      <title>Sparse covariate-driven factorization of high-dimensional brain connectivity with application to site effect correction</title>
      <link>https://arxiv.org/abs/2601.09525</link>
      <description>arXiv:2601.09525v1 Announce Type: cross 
Abstract: Large-scale neuroimaging studies often collect data from multiple scanners across different sites, where variations in scanners, scanning procedures, and other conditions across sites can introduce artificial site effects. These effects may bias brain connectivity measures, such as functional connectivity (FC), which quantify functional network organization derived from functional magnetic resonance imaging (fMRI). How to leverage high-dimensional network structures to effectively mitigate site effects has yet to be addressed. In this paper, we propose SLACC (Sparse LAtent Covariate-driven Connectome) factorization, a multivariate method that explicitly parameterizes covariate effects in latent subject scores corresponding to sparse rank-1 latent patterns derived from brain connectivity. The proposed method identifies localized site-driven variability within and across brain networks, enabling targeted correction. We develop a penalized Expectation-Maximization (EM) algorithm for parameter estimation, incorporating the Bayesian Information Criterion (BIC) to guide optimization. Extensive simulations validate SLACC's robustness in recovering the true parameters and underlying connectivity patterns. Applied to the Autism Brain Imaging Data Exchange (ABIDE) dataset, SLACC demonstrates its ability to reduce site effects. The R package to implement our method is publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09525v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rongqian Zhang, Elena Tuzhilina, Jun Young Park</dc:creator>
    </item>
    <item>
      <title>A probabilistic match classification model for sports tournaments</title>
      <link>https://arxiv.org/abs/2601.09673</link>
      <description>arXiv:2601.09673v1 Announce Type: cross 
Abstract: Existing match classification models in the tournament design literature have two major limitations: a contestant is considered indifferent only if uncertain future results do never affect its prize, and competitive matches are not distinguished with respect to the incentives of the contestants. We propose a probabilistic framework to address both issues. For each match, our approach relies on simulating all other matches played simultaneously or later to compute the qualifying probabilities under the three main outcomes (win, draw, loss), which allows the classification of each match into six different categories. The suggested model is applied to the previous group stage and the new incomplete round-robin league, introduced in the 2024/25 season of UEFA club competitions. An incomplete round-robin tournament is found to contain fewer stakeless matches where both contestants are indifferent, and substantially more matches where both contestants should play offensively. However, the robustly higher proportion of potentially collusive matches can threaten with serious scandals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09673v1</guid>
      <category>physics.soc-ph</category>
      <category>econ.GN</category>
      <category>math.OC</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'aszl\'o Csat\'o, Andr\'as Gyimesi</dc:creator>
    </item>
    <item>
      <title>A practical scenario generation method for electricity prices on day-ahead and intraday spot markets</title>
      <link>https://arxiv.org/abs/2405.14403</link>
      <description>arXiv:2405.14403v2 Announce Type: replace 
Abstract: The increasing interest in demand-side management (DSM) as part of the energy cost optimization calls for effective methods to determine representative electricity prices for energy optimization and scheduling investigations. We propose a practical method to construct price profiles of day-ahead (DA) and intraday (ID) electricity spot markets. We construct single-day and single-week price profiles based on historical market time series to provide ready-to-use price data sets. Our method accounts for dominant mechanisms in price variation to preserve critical statistical features (e.g., mean and standard deviation) and transient patterns in the constructed profiles. Unlike common scenario generation approaches, the method is deterministic, with few degrees of freedom and minimal application effort. Our method ensures consistency between ID and DA price profiles when both are considered and introduces profile scaling to enable multiple scenario generation. Finally, we compare the constructed profiles to clustering techniques in a DSM case study, noting similar cost results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14403v2</guid>
      <category>stat.AP</category>
      <category>cs.CE</category>
      <category>physics.soc-ph</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.compchemeng.2025.109118</arxiv:DOI>
      <arxiv:journal_reference>Computers &amp; Chemical Engineering, 109118 (2025)</arxiv:journal_reference>
      <dc:creator>Chrysanthi Papadimitriou, Jan C. Schulze, Alexander Mitsos</dc:creator>
    </item>
    <item>
      <title>Controlling Ensemble Variance in Diffusion Models: An Application for Reanalyses Downscaling</title>
      <link>https://arxiv.org/abs/2501.14822</link>
      <description>arXiv:2501.14822v2 Announce Type: replace 
Abstract: In recent years, diffusion models have emerged as powerful tools for generating ensemble members in meteorology. In this work, we demonstrate how a Denoising Diffusion Implicit Model (DDIM) can effectively control ensemble variance by varying the number of diffusion steps. Introducing a theoretical framework, we relate diffusion steps to the variance expressed by the reverse diffusion process. Focusing on reanalysis downscaling, we propose an ensemble diffusion model for the full ERA5-to-CERRA domain, generating variance-calibrated ensemble members for wind speed at full spatial and temporal resolution. Our method aligns global mean variance with a reference ensemble dataset and ensures spatial variance is distributed in accordance with observed meteorological variability. Additionally, we address the lack of ensemble information in the CARRA dataset, showcasing the utility of our approach for efficient, high-resolution ensemble generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14822v2</guid>
      <category>stat.AP</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fabio Merizzi, Davide Evangelista, Harilaos Loukos</dc:creator>
    </item>
    <item>
      <title>On the retraining frequency of global models in retail demand forecasting</title>
      <link>https://arxiv.org/abs/2505.00356</link>
      <description>arXiv:2505.00356v3 Announce Type: replace 
Abstract: In an era of increasing computational capabilities and growing environmental consciousness, organizations face a critical challenge in balancing the accuracy of forecasting models with computational efficiency and sustainability. Global forecasting models, lowering the computational time, have gained significant attention over the years. However, the common practice of retraining these models with new observations raises important questions about the costs of forecasting. Using ten different machine learning and deep learning models, we analyzed various retraining scenarios, ranging from continuous updates to no retraining at all, across two large retail demand datasets. We showed that less frequent retraining strategies maintain the forecast accuracy while reducing the computational costs, providing a more sustainable approach to large-scale forecasting. We also found that machine learning models are a marginally better choice to reduce the costs of forecasting when coupled with less frequent model retraining strategies as the frequency of the data increases. Our findings challenge the conventional belief that frequent retraining is essential for maintaining forecasting accuracy. Instead, periodic retraining offers a good balance between predictive performance and efficiency, both in the case of point and probabilistic forecasting. These insights provide actionable guidelines for organizations seeking to optimize forecasting pipelines while reducing costs and energy consumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00356v3</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.OT</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.mlwa.2025.100769</arxiv:DOI>
      <arxiv:journal_reference>Machine Learning with Applications, 22 , 100769 (2025)</arxiv:journal_reference>
      <dc:creator>Marco Zanotti</dc:creator>
    </item>
    <item>
      <title>Choosing Covariate Balancing Methods for Causal Inference: Practical Insights from a Simulation Study</title>
      <link>https://arxiv.org/abs/2412.00280</link>
      <description>arXiv:2412.00280v2 Announce Type: replace-cross 
Abstract: Background: Inverse probability of treatment weighting (IPTW) is used for confounding adjustment in observational studies. Newer weighting methods include energy balancing (EB), kernel optimal matching (KOM), and tailored-loss covariate balancing propensity scores (TLF), but practical guidance remains limited. We evaluate their performance when implemented according to published recommendations.
  Methods: We conducted Monte Carlo simulations across 36 scenarios varying sample size, treatment prevalence, and a complexity factor increasing confounding and reducing overlap. Data generation used predominantly categorical covariates with some correlation. Average treatment effect and average treatment effect on the treated were estimated using IPTW, EB, KOM, and TLF combined with weighted least squares and, when supported, a doubly robust (DR) estimators. Inference followed published recommendations for each method when feasible, using standard alternatives otherwise. \textsc{PROBITsim} dataset used for illustration.
  Results: DR reduced sensitivity to the weighting scheme with an outcome regression adjusted for all confounders, despite functional-form misspecification. EB and KOM were most reliable; EB was tuning-free but scale dependent, whereas KOM required kernel and penalty choices. IPTW was variance sensitive when treatment prevalence was far from 50\%. TLF traded lower variance for higher bias, producing an RMSE plateau and sub-nominal confidence interval coverage. \textsc{PROBITsim} results mirrored these patterns.
  Conclusions: Rather than identifying a best method, our findings highlight failure modes and tuning choices to monitor. When the outcome regression adjusts for all confounders, DR estimation can be dependable across weighting schemes. Incorporating weight-estimation uncertainty into confidence intervals remains a key challenge for newer approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00280v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Etienne Peyrot, Rapha\"el Porcher, Francois Petit</dc:creator>
    </item>
    <item>
      <title>Residual lifetime prediction for heterogeneous degradation data by Bayesian semi-parametric method</title>
      <link>https://arxiv.org/abs/2504.15794</link>
      <description>arXiv:2504.15794v2 Announce Type: replace-cross 
Abstract: Degradation data are considered for assessing reliability in highly reliable systems. The usual assumption is that degradation units come from a homogeneous population. But in presence of high variability in the manufacturing process, this assumption is not true in general; that is different sub-populations are involved in the study. Predicting residual lifetime of a functioning unit is a major challenge in the degradation modeling especially in heterogeneous environment. To account for heterogeneous degradation data, we have proposed a Bayesian semi-parametric approach to relax the conventional modeling assumptions. We model the degradation path using Dirichlet process mixture of normal distributions. Based on the samples obtained from posterior distribution of model parameters we obtain residual lifetime distribution for individual unit. Transformation based MCMC technique is used for simulating values from the derived residual lifetime distribution for prediction of residual lifetime. A simulation study is undertaken to check performance of the proposed semi-parametric model compared with parametric model. Fatigue Crack Size data is analyzed to illustrate the proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15794v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Barin Karmakar, Biswabrata Pradhan</dc:creator>
    </item>
    <item>
      <title>Trust the process: mapping data-driven reconstructions to informed models using stochastic processes</title>
      <link>https://arxiv.org/abs/2506.05153</link>
      <description>arXiv:2506.05153v2 Announce Type: replace-cross 
Abstract: Gravitational-wave astronomy has entered a regime where it can extract information about the population properties of the observed binary black holes. The steep increase in the number of detections will offer deeper insights, but it will also significantly raise the computational cost of testing multiple models. To address this challenge, we propose a procedure that first performs a non-parametric (data-driven) reconstruction of the underlying distribution, and then remaps these results onto a posterior for the parameters of a parametric (informed) model. The computational cost is primarily absorbed by the initial non-parametric step, while the remapping procedure is both significantly easier to perform and computationally cheaper. In addition to yielding the posterior distribution of the model parameters, this method also provides a measure of the model's goodness-of-fit, opening for a new quantitative comparison across models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05153v2</guid>
      <category>gr-qc</category>
      <category>astro-ph.IM</category>
      <category>stat.AP</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stefano Rinaldi, Alexandre Toubiana, Jonathan R. Gair</dc:creator>
    </item>
    <item>
      <title>Smooth surface reconstruction of earthquake faults from distributed moment-potency-tensor solutions</title>
      <link>https://arxiv.org/abs/2506.14082</link>
      <description>arXiv:2506.14082v2 Announce Type: replace-cross 
Abstract: Earthquake faults as observed by seismic motions primarily manifest as displacement discontinuities within elastic continua. The displacement discontinuity and the surface normal vector (n-vector) of such an idealized earthquake source are measured by the tensor of potency, which is seismic moment normalized by stiffness. This study formulates an inverse problem to reconstruct a smooth 3D fault surface from an areal density field of the potency tensor. Here, the surface is represented by an elevation field, while nodal planes of the potency density represent the surface normal (n-vector) field, reducing the problem to an n-vector-to-elevation transform. Although this transform is a one-to-one mapping in 2D, it becomes overdetermined in 3D because the n-vector has two degrees of freedom while the scalar elevation has only one, admitting no solution in general. This overdeterminacy originates from modeling the potency density, the inelastic strain with six degrees of freedom, as a displacement discontinuity of five degrees of freedom. Whereas this overdeterminacy is the violation of the determinant-free constraint in point potency sources, it appears as a conflict with the global consistency of the n-vector field in areal potency densities. Recognizing this capacity of the potency density to describe inelastic strain incompatible with displacement discontinuity, we introduce an a priori constraint to define the fault as the smooth surface that best approximates inelastic strain as displacement discontinuity. We derive an analytical solution for this formulation and demonstrate its ability to reproduce 3D surfaces from noisy synthetic n-vectors. We integrate this formula into potency density tensor inversion and apply it to the 2013 Balochistan earthquake. The estimated 3D geometry shows better agreement with observed fault traces than previous quasi-2D methods, validating our proposal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14082v2</guid>
      <category>physics.geo-ph</category>
      <category>stat.AP</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dye SK Sato, Yuji Yagi, Ryo Okuwaki, Yukitoshi Fukahata</dc:creator>
    </item>
    <item>
      <title>Trustworthy scientific inference with generative models</title>
      <link>https://arxiv.org/abs/2508.02602</link>
      <description>arXiv:2508.02602v3 Announce Type: replace-cross 
Abstract: Generative artificial intelligence (AI) excels at producing complex data structures (text, images, videos) by learning patterns from training examples. Across scientific disciplines, researchers are now applying generative models to "inverse problems" to directly predict hidden parameters from observed data along with measures of uncertainty. While these predictive or posterior-based methods can handle intractable likelihoods and large-scale studies, they can also produce biased or overconfident conclusions even without model misspecifications. We present a solution with Frequentist-Bayes (FreB), a mathematically rigorous protocol that reshapes AI-generated posterior probability distributions into (locally valid) confidence regions that consistently include true parameters with the expected probability, while achieving minimum size when training and target data align. We demonstrate FreB's effectiveness by tackling diverse case studies in the physical sciences: identifying unknown sources under dataset shift, reconciling competing theoretical models, and mitigating selection bias and systematics in observational studies. By providing validity guarantees with interpretable diagnostics, FreB enables trustworthy scientific inference across fields where direct likelihood evaluation remains impossible or prohibitively expensive.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02602v3</guid>
      <category>stat.ML</category>
      <category>astro-ph.IM</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Carzon, Luca Masserano, Joshua D. Ingram, Alex Shen, Antonio Carlos Herling Ribeiro Junior, Tommaso Dorigo, Michele Doro, Joshua S. Speagle, Rafael Izbicki, Ann B. Lee</dc:creator>
    </item>
    <item>
      <title>In-Context Learning Enhanced Credibility Transformer</title>
      <link>https://arxiv.org/abs/2509.08122</link>
      <description>arXiv:2509.08122v2 Announce Type: replace-cross 
Abstract: The starting point of our network architecture is the Credibility Transformer which extends the classical Transformer architecture by a credibility mechanism to improve model learning and predictive performance. This Credibility Transformer learns credibilitized CLS tokens that serve as learned representations of the original input features. In this paper we present a new paradigm that augments this architecture by an in-context learning mechanism, i.e., we increase the information set by a context batch consisting of similar instances. This allows the model to enhance the CLS token representations of the instances by additional in-context information and fine-tuning. We empirically verify that this in-context learning enhances predictive accuracy by adapting to similar risk patterns. Moreover, this in-context learning also allows the model to generalize to new instances which, e.g., have feature levels in the categorical covariates that have not been present when the model was trained -- for a relevant example, think of a new vehicle model which has just been developed by a car manufacturer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08122v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kishan Padayachy, Ronald Richman, Salvatore Scognamiglio, Mario V. W\"uthrich</dc:creator>
    </item>
    <item>
      <title>Multivariable Bidirectional Mendelian Randomization via Bayesian Directed Cyclic Graphical Models with Correlated Errors</title>
      <link>https://arxiv.org/abs/2510.09991</link>
      <description>arXiv:2510.09991v2 Announce Type: replace-cross 
Abstract: Mendelian randomization (MR) is a pivotal tool in genetics, genomics, and epidemiology, leveraging genetic variants as instrumental variables to infer causal relationships between exposures and outcomes. Traditional MR methods, while powerful, often rely on stringent assumptions such as the absence of feedback loops, which are frequently violated in complex biological networks. In addition, many popular MR approaches focus on only two variables (i.e., one exposure and one outcome), whereas our motivating applications of gene regulatory networks have many variables. In this article, we introduce a novel Bayesian framework for multivariable MR that concurrently addresses unmeasured confounding and feedback loops. Central to our approach is a sparse conditional cyclic graphical model with a sparse error variance-covariance matrix. Two structural priors are employed to enable the modeling and inference of causal relationships as well as latent confounding structures. Our method is designed to operate effectively with summary-level data, facilitating its application in contexts where individual-level data are inaccessible, e.g., due to privacy concerns. It can also account for horizontal pleiotropy, under which we establish the sufficient identifiability conditions. Through extensive simulations and applications to the GTEx and OneK1K data, we demonstrate the superior performance of our approach in recovering biologically plausible causal relationships in the presence of possible feedback loops and unmeasured confounding. Using posterior samples, we further quantify uncertainty in inferred network motifs by computing their posterior probabilities. The R package MR.RGM that implements the proposed method is available on CRAN (https://cran.r-project.org/package=MR.RGM).</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09991v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bitan Sarkar, Yuchao Jiang, Tian Ge, Yang Ni</dc:creator>
    </item>
    <item>
      <title>Penalized Fair Regression for Multiple Groups in Chronic Kidney Disease</title>
      <link>https://arxiv.org/abs/2512.17340</link>
      <description>arXiv:2512.17340v2 Announce Type: replace-cross 
Abstract: Fair regression methods have the potential to mitigate societal bias concerns in health care, but there has been little work on penalized fair regression when multiple groups experience such bias. We propose a general regression framework that addresses this gap with unfairness penalties for multiple groups. Our approach is demonstrated for binary outcomes with true positive rate disparity penalties. It can be efficiently implemented through reduction to a cost-sensitive classification problem. We additionally introduce novel score functions for automatically selecting penalty weights. Our penalized fair regression methods are empirically studied in simulations, where they achieve a fairness-accuracy frontier beyond that of existing comparison methods. Finally, we apply these methods to a national multi-site primary care study of chronic kidney disease to develop a fair classifier for end-stage renal disease. There we find substantial improvements in fairness for multiple race and ethnicity groups who experience societal bias in the health care system without any appreciable loss in overall fit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17340v2</guid>
      <category>stat.ME</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carter H. Nakamoto, Lucia Lushi Chen, Agata Foryciarz, Sherri Rose</dc:creator>
    </item>
    <item>
      <title>Integrating Computational Methods and AI into Qualitative Studies of Aging and Later Life</title>
      <link>https://arxiv.org/abs/2512.17850</link>
      <description>arXiv:2512.17850v2 Announce Type: replace-cross 
Abstract: This chapter demonstrates how computational social science (CSS) tools are extending and expanding research on aging. The depth and context from traditionally qualitative methods such as participant observation, in-depth interviews, and historical documents are increasingly employed alongside scalable data management, computational text analysis, and open-science practices. Machine learning (ML) and natural language processing (NLP), provide resources to aggregate and systematically index large volumes of qualitative data, identify patterns, and maintain clear links to in-depth accounts. Drawing on case studies of projects that examine later life--including examples with original data from the DISCERN study (a team-based ethnography of life with dementia) and secondary analyses of the American Voices Project (nationally representative interview)--the chapter highlights both uses and challenges of bringing CSS tools into more meaningful dialogue with qualitative aging research. The chapter argues such work has potential for (1) streamlining and augmenting existing workflows, (2) scaling up samples and projects, and (3) generating multi-method approaches to address important questions in new ways, before turning to practices useful for individuals and teams seeking to understand current possibilities or refine their workflow processes. The chapter concludes that current developments are not without peril, but offer potential for new insights into aging and the life course by broadening--rather than replacing--the methodological foundations of qualitative research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17850v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Corey M. Abramson</dc:creator>
    </item>
    <item>
      <title>Making Absence Visible: The Roles of Reference and Prompting in Recognizing Missing Information</title>
      <link>https://arxiv.org/abs/2601.07234</link>
      <description>arXiv:2601.07234v2 Announce Type: replace-cross 
Abstract: Interactive systems that explain data, or support decision making often emphasize what is present while overlooking what is expected but missing. This presence bias limits users' ability to form complete mental models of a dataset or situation. Detecting absence depends on expectations about what should be there, yet interfaces rarely help users form such expectations. We present an experimental study examining how reference framing and prompting influence people's ability to recognize expected but missing categories in datasets. Participants compared distributions across three domains (energy, wealth, and regime) under two reference conditions: Global, presenting a unified population baseline, and Partial, showing several concrete exemplars. Results indicate that absence detection was higher with Partial reference than with Global reference, suggesting that partial, samples-based framing can support expectation formation and absence detection. When participants were prompted to look for what was missing, absence detection rose sharply. We discuss implications for interactive user interfaces and expectation-based visualization design, while considering cognitive trade-offs of reference structures and guided attention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07234v2</guid>
      <category>cs.HC</category>
      <category>cs.IR</category>
      <category>stat.AP</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hagit Ben Shoshan, Joel Lanir, Pavel Goldstein, Osnat Mokryn</dc:creator>
    </item>
  </channel>
</rss>
