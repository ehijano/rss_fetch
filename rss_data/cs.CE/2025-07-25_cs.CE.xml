<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 25 Jul 2025 04:00:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Digital Twin Technologies in Predictive Maintenance: Enabling Transferability via Sim-to-Real and Real-to-Sim Transfer</title>
      <link>https://arxiv.org/abs/2507.18449</link>
      <description>arXiv:2507.18449v1 Announce Type: new 
Abstract: The advancement of the Internet of Things (IoT) and Artificial Intelligence has catalyzed the evolution of Digital Twins (DTs) from conceptual ideas to more implementable realities. Yet, transitioning from academia to industry is complex due to the absence of standardized frameworks. This paper builds upon the authors' previously established functional and informational requirements supporting standardized DT development, focusing on a crucial aspect: transferability. While existing DT research primarily centers on asset transfer, the significance of "sim-to-real transfer" and "real-to-sim transfer"--transferring knowledge between simulations and real-world operations--is vital for comprehensive lifecycle management in DTs. A key challenge in this process is calibrating the "reality gap," the discrepancy between simulated predictions and actual outcomes. Our research investigates the impact of integrating a single Reality Gap Analysis (RGA) module into an existing DT framework to effectively manage both sim-to-real and real-to-sim transfers. This integration is facilitated by data pipelines that connect the RGA module with the existing components of the DT framework, including the historical repository and the simulation model. A case study on a pedestrian bridge at Carnegie Mellon University showcases the performance of different levels of integration of our approach with an existing framework. With full implementation of an RGA module and a complete data pipeline, our approach is capable of bidirectional knowledge transfer between simulations and real-world operations without compromising efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18449v1</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sizhe Ma, Katherine A. Flanigan, Mario Berg\'es</dc:creator>
    </item>
    <item>
      <title>Multiscale Neural PDE Surrogates for Prediction and Downscaling: Application to Ocean Currents</title>
      <link>https://arxiv.org/abs/2507.18067</link>
      <description>arXiv:2507.18067v1 Announce Type: cross 
Abstract: Accurate modeling of physical systems governed by partial differential equations is a central challenge in scientific computing. In oceanography, high-resolution current data are critical for coastal management, environmental monitoring, and maritime safety. However, available satellite products, such as Copernicus data for sea water velocity at ~0.08 degrees spatial resolution and global ocean models, often lack the spatial granularity required for detailed local analyses. In this work, we (a) introduce a supervised deep learning framework based on neural operators for solving PDEs and providing arbitrary resolution solutions, and (b) propose downscaling models with an application to Copernicus ocean current data. Additionally, our method can model surrogate PDEs and predict solutions at arbitrary resolution, regardless of the input resolution. We evaluated our model on real-world Copernicus ocean current data and synthetic Navier-Stokes simulation datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18067v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdessamad El-Kabid, Loubna Benabbou, Redouane Lguensat, Alex Hern\'andez-Garc\'ia</dc:creator>
    </item>
    <item>
      <title>On zero-order consistency residue and background pressure for the conservative SPH fluid dynamics</title>
      <link>https://arxiv.org/abs/2507.18210</link>
      <description>arXiv:2507.18210v1 Announce Type: cross 
Abstract: As one of the major challenges for the conservative smoothed particle hydrodynamics (SPH) method, the zero-order consistency issue, although thought to be mitigated by the particle regularization scheme, such as the transport velocity formulation, significantly damps the flow in a long channel for both laminar and turbulent simulations. Building on this finding, this paper not only thoroughly analyzes the damping reason in this pressure-driven channel flow, but also relates this problem with the excessive numerical dissipation in the gravity-driven free-surface flow. The common root cause of the non-physical numerical damping in the two typical flow scenarios, the zero-order gradient consistency residue, is exposed. The adverse influence of the background pressure on the residue for the two scenarios is revealed and discussed. To comprehensively understand the behavior of the residue and mitigate its potential adverse effects, we conduct both theoretical analysis and numerical experiments focusing on the key sensitive factors. For studying the residue-induced non-physical energy dissipation in the gravity-driven free-surface flow, the water depth and input dynamic pressure in the inviscid standing wave case are tested. To investigate the velocity loss in the pressure-driven channel flow, we examine the effects of the channel length, resolution, and outlet pressure. The state-of-the-art reverse kernel gradient correction technique is introduced for the two typical flows, and proved to be effective in reducing the residue effect, but we find its correction capability is fundamentally limited. Finally, the FDA nozzle, an engineering benchmark, is tested to demonstrate the residue influence in a complex geometry, highlighting the necessity of correction schemes in scenarios with unavoidable high background pressure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18210v1</guid>
      <category>physics.flu-dyn</category>
      <category>cs.CE</category>
      <category>physics.comp-ph</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Feng Wang, Xiangyu Hu</dc:creator>
    </item>
    <item>
      <title>A stabilized Two-Step Formulation of Maxwell's Equations in the time-domain</title>
      <link>https://arxiv.org/abs/2507.18235</link>
      <description>arXiv:2507.18235v1 Announce Type: cross 
Abstract: Simulating electromagnetic fields across broad frequency ranges is challenging due to numerical instabilities at low frequencies. This work extends a stabilized two-step formulation of Maxwell's equations to the time-domain. Using a Galerkin discretization in space, we apply two different time-discretization schemes that are tailored to the first- and second-order in time partial differential equations of the two-step solution procedure used here. To address the low-frequency instability, we incorporate a generalized tree-cotree gauge that removes the singularity of the curl-curl operator, ensuring robustness even in the static limit. Numerical results on academic and application-oriented 3D problems confirm stability, accuracy, and the method's applicability to nonlinear, temperature-dependent materials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18235v1</guid>
      <category>math.NA</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leon Herles, Mario Mally, J\"org Ostrowski, Sebastian Sch\"ops, Melina Merkel</dc:creator>
    </item>
    <item>
      <title>Select2Drive: Pragmatic Communications for Real-Time Collaborative Autonomous Driving</title>
      <link>https://arxiv.org/abs/2501.12040</link>
      <description>arXiv:2501.12040v2 Announce Type: replace 
Abstract: Vehicle-to-everything communications-assisted autonomous driving has witnessed remarkable advancements in recent years, with pragmatic communications (PragComm) emerging as a promising paradigm for real-time collaboration among vehicles and other agents. Simultaneously, extensive research has explored the interplay between collaborative perception and decision-making in end-to-end driving frameworks. In this work, we revisit the collaborative driving problem and propose the Select2Drive framework to optimize the utilization of limited computational and communication resources. Particularly, to mitigate cumulative latency in perception and decision-making, Select2Drive introduces distributed predictive perception by formulating an active prediction paradigm and simplifying high-dimensional semantic feature prediction into a computation cost-efficient, motion-aware reconstruction. Given the ``less is more" principle that an over-broadened perceptual horizon possibly confuses the decision module rather than contributing to it, Select2Drive utilizes area-of-importance-based PragComm to prioritize the communications of critical regions, thus boosting both communication efficiency and decision-making efficacy. Empirical evaluations on the V2Xverse and real-world DAIR-V2X demonstrate that Select2Drive achieves a $2.60$\% and $1.99$\% improvement in offline perception tasks under limited bandwidth (resp., pose error conditions). Moreover, it delivers at most $8.35$\% and $2.65$\% enhancement in closed-loop driving scores and route completion rates, particularly in scenarios characterized by dense traffic and high-speed dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12040v2</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiahao Huang, Jianhang Zhu, Rongpeng Li, Zhifeng Zhao, Honggang Zhang</dc:creator>
    </item>
    <item>
      <title>Agentar-DeepFinance-100K: A Large-Scale Financial Dataset via Systematic Chain-of-Thought Synthesis Optimization</title>
      <link>https://arxiv.org/abs/2507.12901</link>
      <description>arXiv:2507.12901v2 Announce Type: replace 
Abstract: Recent advancements in large language models (LLMs) have demonstrated remarkable general reasoning capabilities, holding significant potential for applications in the financial domain, a field that requires robust and reliable reasoning. It has been demonstrated that distilling high-quality chain-of-thought (CoT) rationales from advanced general reasoning models offers a promising and efficient path to the financial reasoning model. However, existing CoT synthesis methods suffer from shallow CoT sampling, leaving the question of how to construct a well-designed knowledge space for finance reasoning unexplored. In this paper, we present Agentar-DeepFinance-100K, a large-scale financial reasoning dataset characterized by its systematic CoT synthesis optimization. We first introduce a comprehensive CoT synthesis pipeline featuring Multi-perspective Knowledge Extraction (MKE) and Self-Corrective Rewriting (SCR) to generate exhaustive and deep financial reasoning trajectories. Furthermore, a systematic investigation, termed CoT Cube, is conducted to analyze critical factors that influence CoT effectiveness, such as necessity, length and synthesizer, yielding valuable insights for high-quality financial CoT construction. Experiments demonstrate that models trained on our Agentar-DeepFinance-100K achieve significant improvements on financial benchmarks. We publicly release Agentar-DeepFinance-100K , hoping to advance the research in financial reasoning models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12901v2</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoke Zhao, Zhaowen Zhou, Lin Chen, Lihong Wang, Zhiyi Huang, Kaiyuan Zheng, Yanjun Zheng, Xiyang Du, Longfei Liao, Jiawei Liu, Xiang Qi, Bo Zhang, Peng Zhang, Wei Wang, Zhe Li</dc:creator>
    </item>
    <item>
      <title>EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent Dialogue Framework</title>
      <link>https://arxiv.org/abs/2504.14928</link>
      <description>arXiv:2504.14928v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) increasingly serve as educational tools, yet evaluating their teaching capabilities remains challenging due to the resource-intensive, context-dependent, and methodologically complex nature of teacher-student interactions. We introduce EducationQ, a multi-agent dialogue framework that efficiently assesses teaching capabilities through simulated dynamic educational scenarios, featuring specialized agents for teaching, learning, and evaluation. Testing 14 LLMs across major AI Organizations (OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13 disciplines and 10 difficulty levels reveals that teaching effectiveness does not correlate linearly with model scale or general reasoning capabilities - with some smaller open-source models outperforming larger commercial counterparts in teaching contexts. This finding highlights a critical gap in current evaluations that prioritize knowledge recall over interactive pedagogy. Our mixed-methods evaluation, combining quantitative metrics with qualitative analysis and expert case studies, identifies distinct pedagogical strengths employed by top-performing models (e.g., sophisticated questioning strategies, adaptive feedback mechanisms). Human expert evaluations show 78% agreement with our automated qualitative analysis of effective teaching behaviors, validating our methodology. EducationQ demonstrates that LLMs-as-teachers require specialized optimization beyond simple scaling, suggesting next-generation educational AI prioritize targeted enhancement of specific pedagogical effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14928v2</guid>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.18653/v1/2025.acl-long.1576</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (2025) 32799-32828</arxiv:journal_reference>
      <dc:creator>Yao Shi, Rongkeng Liang, Yong Xu</dc:creator>
    </item>
  </channel>
</rss>
