<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 26 Feb 2026 05:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Evaluation of Minimal Residual Disease as a Surrogate for Progression-Free Survival in Hematology Oncology Trials: A Meta-Analytic Review</title>
      <link>https://arxiv.org/abs/2602.21370</link>
      <description>arXiv:2602.21370v1 Announce Type: new 
Abstract: Traditional health authority approval for oncology drugs is based on a clinical benefit endpoint, or a valid surrogate. In 1992 the FDA created the Accelerated Approval pathway to allow for earlier approval of therapies in serious conditions with an unmet medical need. This is accomplished typically by granting accelerated approval based on a surrogate endpoint that can be measured earlier than a traditional approval endpoint. Minimal residual disease (MRD) is a sensitive measure of residual cancer cells in hematology oncology after treatment, and is increasingly considered as a secondary or exploratory endpoint due to its prognostic potential for traditional clinical trial endpoints such as progression-free survival (PFS) and overall survival (OS). This work aims to evaluate MRD's surrogacy potential across several hematologic cancer indications while keeping the focus on follicular lymphoma (FL), using data from published studies. We examine individual-level and trial-level correlations extracted from previously published studies to elucidate the potential role of MRD in accelerating the drug approval process in hematology oncology trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21370v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jane She, Xiaofei Chen, Malini Iyengar, Judy Li</dc:creator>
    </item>
    <item>
      <title>Comparative Evaluation of Machine Learning Models for Predicting Donor Kidney Discard</title>
      <link>https://arxiv.org/abs/2602.21876</link>
      <description>arXiv:2602.21876v1 Announce Type: new 
Abstract: A kidney transplant can improve the life expectancy and quality of life of patients with end-stage renal failure. Even more patients could be helped with a transplant if the rate of kidneys that are discarded and not transplanted could be reduced. Machine learning (ML) can support decision-making in this context by early identification of donor organs at high risk of discard, for instance to enable timely interventions to improve organ utilization such as rescue allocation. Although various ML models have been applied, their results are difficult to compare due to heterogenous datasets and differences in feature engineering and evaluation strategies. This study aims to provide a systematic and reproducible comparison of ML models for donor kidney discard prediction. We trained five commonly used ML models: Logistic Regression, Decision Tree, Random Forest, Gradient Boosting, and Deep Learning along with an ensemble model on data from 4,080 deceased donors (death determined by neurologic criteria) in Germany. A unified benchmarking framework was implemented, including standardized feature engineering and selection, and Bayesian hyperparameter optimization. Model performance was assessed for discrimination (MCC, AUC, F1), calibration (Brier score), and explainability (SHAP). The ensemble achieved the highest discrimination performance (MCC=0.76, AUC=0.87, F1=0.90), while individual models such as Logistic Regression, Random Forest, and Deep Learning performed comparably and better than Decision Trees. Platt scaling improved calibration for tree-and neural network-based models. SHAP consistently identified donor age and renal markers as dominant predictors across models, reflecting clinical plausibility. This study demonstrates that consistent data preprocessing, feature selection, and evaluation can be more decisive for predictive success than the choice of the ML algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21876v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peer Schliephacke, Hannah Schult, Leon Mizera, Judith W\"urfel, Gunter Grieser, Axel Rahmel, Carl-Ludwig Fischer-Fr\"ohlich, Antje Jahn-Eimermacher</dc:creator>
    </item>
    <item>
      <title>Generative Bayesian Computation as a Scalable Alternative to Gaussian Process Surrogates</title>
      <link>https://arxiv.org/abs/2602.21408</link>
      <description>arXiv:2602.21408v1 Announce Type: cross 
Abstract: Gaussian process (GP) surrogates are the default tool for emulating expensive computer experiments, but cubic cost, stationarity assumptions, and Gaussian predictive distributions limit their reach. We propose Generative Bayesian Computation (GBC) via Implicit Quantile Networks (IQNs) as a surrogate framework that targets all three limitations. GBC learns the full conditional quantile function from input--output pairs; at test time, a single forward pass per quantile level produces draws from the predictive distribution.
  Across fourteen benchmarks we compare GBC to four GP-based methods. GBC improves CRPS by 11--26\% on piecewise jump-process benchmarks, by 14\% on a ten-dimensional Friedman function, and scales linearly to 90,000 training points where dense-covariance GPs are infeasible. A boundary-augmented variant matches or outperforms Modular Jump GPs on two-dimensional jump datasets (up to 46\% CRPS improvement). In active learning, a randomized-prior IQN ensemble achieves nearly three times lower RMSE than deep GP active learning on Rocket LGBB. Overall, GBC records a favorable point estimate in 12 of 14 comparisons. GPs retain an edge on smooth surfaces where their smoothness prior provides effective regularization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21408v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nick Polson, Vadim Sokolov</dc:creator>
    </item>
    <item>
      <title>Fair Model-based Clustering</title>
      <link>https://arxiv.org/abs/2602.21509</link>
      <description>arXiv:2602.21509v1 Announce Type: cross 
Abstract: The goal of fair clustering is to find clusters such that the proportion of sensitive attributes (e.g., gender, race, etc.) in each cluster is similar to that of the entire dataset. Various fair clustering algorithms have been proposed that modify standard K-means clustering to satisfy a given fairness constraint. A critical limitation of several existing fair clustering algorithms is that the number of parameters to be learned is proportional to the sample size because the cluster assignment of each datum should be optimized simultaneously with the cluster center, and thus scaling up the algorithms is difficult. In this paper, we propose a new fair clustering algorithm based on a finite mixture model, called Fair Model-based Clustering (FMC). A main advantage of FMC is that the number of learnable parameters is independent of the sample size and thus can be scaled up easily. In particular, mini-batch learning is possible to obtain clusters that are approximately fair. Moreover, FMC can be applied to non-metric data (e.g., categorical data) as long as the likelihood is well-defined. Theoretical and empirical justifications for the superiority of the proposed algorithm are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21509v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinwon Park, Kunwoong Kim, Jihu Lee, Yongdai Kim</dc:creator>
    </item>
    <item>
      <title>Adaptive Penalized Doubly Robust Regression for Longitudinal Data</title>
      <link>https://arxiv.org/abs/2602.21711</link>
      <description>arXiv:2602.21711v1 Announce Type: cross 
Abstract: Longitudinal data often involve heterogeneity, sparse signals, and contamination from response outliers or high-leverage observations especially in biomedical science. Existing methods usually address only part of this problem, either emphasizing penalized mixed effects modeling without robustness or robust mixed effects estimation without high-dimensional variable selection. We propose a doubly adaptive robust regression (DAR-R) framework for longitudinal linear mixed effects models. It combines a robust pilot fit, doubly adaptive observation weights for residual outliers and leverage points, and folded concave penalization for fixed effect selection, together with weighted updates of random effects and variance components. We develop an iterative reweighting algorithm and establish estimation and prediction error bounds, support recovery consistency, and oracle-type asymptotic normality. Simulations show that DAR-R improves estimation accuracy, false-positive control, and covariance estimation under both vertical outliers and bad leverage contamination. In the TADPOLE/ADNI Alzheimer's disease application, DAR-R achieves accurate and stable prediction of ADAS13 while selecting clinically meaningful predictors with strong resampling stability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21711v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuyao Wang, Yu Lu, Tianni Zhang, Mengfei Ran</dc:creator>
    </item>
    <item>
      <title>The Hammock Plot: Where Categorical and Numerical Data Relax Together</title>
      <link>https://arxiv.org/abs/2506.13630</link>
      <description>arXiv:2506.13630v3 Announce Type: replace 
Abstract: Effective methods for visualizing data involving multiple variables, including categorical ones, are limited. The hammock plot (Schonlau 2003) visualizes both categorical and numerical variables using parallel coordinates. We introduce the Stata implementation hammock. We give numerous examples that explore highlighting, missing values, putting axes on the same scale, and tracing an observation across variables. Further, we discuss parallel univariate plots as an edge case of hammock plots. We also present and make publicly available a new dataset on the 2020 Tour de France.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13630v3</guid>
      <category>stat.AP</category>
      <category>cs.HC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthias Schonlau, Tiancheng Yang</dc:creator>
    </item>
    <item>
      <title>A Corrected Welch Satterthwaite Equation. And: What You Always Wanted to Know About Kish's Effective Sample but Were Afraid to Ask</title>
      <link>https://arxiv.org/abs/2602.20912</link>
      <description>arXiv:2602.20912v2 Announce Type: replace 
Abstract: This article presents a corrected version of the Satterthwaite (1941, 1946) approximation for the degrees of freedom of a weighted sum of independent variance components. The original formula is known to yield biased estimates when component degrees of freedom are small. The correction, derived from exact moment matching, adjusts for the bias by incorporating a factor that accounts for the estimation of fourth moments. We show that Kish's (1965) effective sample size formula emerges as a special case when all variance components are equal, and component degrees of freedom are ignored. Simulation studies demonstrate that the corrected estimator closely matches the expected degrees of freedom even for small component sizes, while the original Satterthwaite estimator exhibits substantial downward bias. Additional applications are discussed, including jackknife variance estimation, multiple imputation total variance, and the Welch test for unequal variances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20912v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matthias von Davier</dc:creator>
    </item>
    <item>
      <title>Modi linear failure rate distribution with application to survival time data</title>
      <link>https://arxiv.org/abs/2509.20831</link>
      <description>arXiv:2509.20831v2 Announce Type: replace-cross 
Abstract: A new lifetime model, named the Modi linear failure rate distribution, is suggested. This flexible model is capable of accommodating a wide range of hazard rate shapes, including decreasing, increasing, bathtub, upside-down bathtub, and modified bathtub forms, making it particularly suitable for modeling diverse survival and reliability data. Our proposed model contains the Modi exponential distribution and the Modi Rayleigh distribution as sub-models. Numerous mathematical and reliability properties are derived, including the $r^{th}$ moment, moment generating function, $r^{th}$ conditional moment, quantile function, order statistics, mean deviations, R\'{e}nyi entropy, and reliability function. The method of maximum likelihood is employed to estimate the model parameters. Monte Carlo simulations are presented to examine how these estimators perform. The superior fit of our newly introduced model is proved through two real-world survival data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20831v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.64389/mjs.2026.02145</arxiv:DOI>
      <arxiv:journal_reference>Modern Journal of Statistics 2026</arxiv:journal_reference>
      <dc:creator>Lazhar Benkhelifa</dc:creator>
    </item>
    <item>
      <title>How segmented is my network?</title>
      <link>https://arxiv.org/abs/2602.10125</link>
      <description>arXiv:2602.10125v3 Announce Type: replace-cross 
Abstract: Network segmentation is a popular security practice for limiting lateral movement, yet practitioners lack a metric to measure how segmented a network actually is. We introduce the first statistically principled metric for network segmentedness based on global edge density, enabling practitioners to quantify what has previously been assessed only qualitatively. Then, we derive a normalized estimator for segmentedness and evaluate its uncertainty using confidence intervals. For a 95\% confidence interval with a margin-of-error of $\pm 0.1$, we show that a minimum of $M=97$ sampled node pairs is sufficient. This result is independent of the total number of nodes in the network, provided that node pairs are sampled uniformly at random. We evaluate the estimator through Monte Carlo simulations on Erd\H{o}s--R\'enyi, stochastic block models, and real-world enterprise network datasets, demonstrating accurate estimation and well-behaved coverage. Finally, we discuss applications of the estimator, such as baseline tracking, zero trust assessment, and merger integration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10125v3</guid>
      <category>cs.SI</category>
      <category>cs.NI</category>
      <category>stat.AP</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rohit Dube</dc:creator>
    </item>
    <item>
      <title>Error-Controlled Borrowing from External Data Using Wasserstein Ambiguity Sets</title>
      <link>https://arxiv.org/abs/2602.20503</link>
      <description>arXiv:2602.20503v2 Announce Type: replace-cross 
Abstract: Incorporating external data can improve the efficiency of clinical trials, but distributional mismatches between current and external populations threaten the validity of inference. While numerous dynamic borrowing methods exist, the calibration of their borrowing parameters relies mainly on ad hoc, simulation-based tuning. To overcome this, we propose BOND (Borrowing under Optimal Nonparametric Distributional robustness), a framework that formalizes data noncommensurability through Wasserstein ambiguity sets centered at the current-trial distribution. By deriving sharp, closed-form bounds on the worst-case mean drift for both continuous and binary outcomes, we construct a distributionally robust, bias-corrected Wald statistic that ensures asymptotic type I error control uniformly over the ambiguity set. Importantly, BOND determines the optimal borrowing strength by maximizing a worst-case power proxy, converting heuristic parameter tuning into a transparent, analytically tractable optimization problem. Furthermore, we demonstrate that many prominent borrowing methods can be reparameterized via an effective borrowing weight, rendering our calibration framework broadly applicable. Simulation studies and a real-world clinical trial application confirm that BOND preserves the nominal size under unmeasured heterogeneity while achieving efficiency gains over standard borrowing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20503v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yui Kimura, Shu Tamano</dc:creator>
    </item>
  </channel>
</rss>
