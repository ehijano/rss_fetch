<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Dec 2024 02:42:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Decision Support System for daily scheduling and routing of home healthcare workers with a lunch break consideration</title>
      <link>https://arxiv.org/abs/2412.06797</link>
      <description>arXiv:2412.06797v1 Announce Type: new 
Abstract: This study examines a home healthcare scheduling and routing problem (HHSRP) with a lunch break requirement. This problem especially consists of lunch break constraints for caregivers in addition to other typical features of the HHSRP in literature such as hard time window constraints for both patients and caregivers and patient preferences. The objective is to minimize both travel distance in a route and unvisited patient (penalty) cost. For this NP-Hard problem, we developed an effective Adaptive Large Neighborhood Search algorithm to provide high-quality solutions in a short amount of time. We tested the proposed four variants of the algorithm with the selected problem instances from the literature. The algorithms provided nearly all optimal solutions for 30-patient problem instances in 12 seconds on average. Additionally, they provided better solutions to 36 problem instances up to 36% improvement in some instance classes. Moreover, the improved solutions achieved to visit up to 10 more patients. The algorithms are also shown to be very robust due to their low coefficient variance of 0.3 on average. The algorithm also requires a very reasonable amount of time to generate solutions up to 54 seconds for solving 100-patient instances. A decision support system, namely Home Healthcare Decision Support System (HHCSS) was also designed to play a positive role in preventing the COVID-19 global pandemic. The system employs the proposed ALNS algorithm to solve various instances of approximately generated COVID-19 patient data from Turkey. The main aim of developing HHCSS is to support the administrative staff of home healthcare from the tedious task of scheduling and routing of caregivers and to increase service responsiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06797v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>\"Omer \"Ozt\"urko\u{g}lu, G\"okberk \"Ozsakall{\i}, Syed Shah Sultan Mohiuddin Qadri</dc:creator>
    </item>
    <item>
      <title>Secondary Use of Health Data: Centralized Structure and Information Security Frameworks in Finland</title>
      <link>https://arxiv.org/abs/2412.06800</link>
      <description>arXiv:2412.06800v1 Announce Type: new 
Abstract: The utilization of health data for secondary purposes, such as research, sta-tistics, and development, has become increasingly significant in advancing healthcare systems. To foster the above, Finland has established a framework for the secondary use of health and social data through legislative measures and the creation of specialized institutions, which are the first of their kind in the world. In this paper, we give an overview of our implementation for using secondary health and social data in a centralized fashion. As a technical contribution, we also address key implementation aspects related to implementing the framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06800v1</guid>
      <category>cs.CY</category>
      <category>cs.SE</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hannu Vilpponen, Antti Piirainen, Miikka Kallberg, Tommi Mikkonen</dc:creator>
    </item>
    <item>
      <title>Creating a Cooperative AI Policymaking Platform through Open Source Collaboration</title>
      <link>https://arxiv.org/abs/2412.06936</link>
      <description>arXiv:2412.06936v1 Announce Type: new 
Abstract: Advances in artificial intelligence (AI) present significant risks and opportunities, requiring improved governance to mitigate societal harms and promote equitable benefits. Current incentive structures and regulatory delays may hinder responsible AI development and deployment, particularly in light of the transformative potential of large language models (LLMs). To address these challenges, we propose developing the following three contributions: (1) a large multimodal text and economic-timeseries foundation model that integrates economic and natural language policy data for enhanced forecasting and decision-making, (2) algorithmic mechanisms for eliciting diverse and representative perspectives, enabling the creation of data-driven public policy recommendations, and (3) an AI-driven web platform for supporting transparent, inclusive, and data-driven policymaking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06936v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aiden Lewington, Alekhya Vittalam, Anshumaan Singh, Anuja Uppuluri, Arjun Ashok, Ashrith Mandayam Athmaram, Austin Milt, Benjamin Smith, Charlie Weinberger, Chatanya Sarin, Christoph Bergmeir, Cliff Chang, Daivik Patel, Daniel Li, David Bell, Defu Cao, Donghwa Shin, Edward Kang, Edwin Zhang, Enhui Li, Felix Chen, Gabe Smithline, Haipeng Chen, Henry Gasztowtt, Hoon Shin, Jiayun Zhang, Joshua Gray, Khai Hern Low, Kishan Patel, Lauren Hannah Cooke, Marco Burstein, Maya Kalapatapu, Mitali Mittal, Raymond Chen, Rosie Zhao, Sameen Majid, Samya Potlapalli, Shang Wang, Shrenik Patel, Shuheng Li, Siva Komaragiri, Song Lu, Sorawit Siangjaeo, Sunghoo Jung, Tianyu Zhang, Valery Mao, Vikram Krishnakumar, Vincent Zhu, Wesley Kam, Xingzhe Li, Yumeng Liu</dc:creator>
    </item>
    <item>
      <title>Generative AI Impact on Labor Market: Analyzing ChatGPT's Demand in Job Advertisements</title>
      <link>https://arxiv.org/abs/2412.07042</link>
      <description>arXiv:2412.07042v1 Announce Type: new 
Abstract: The rapid advancement of Generative AI (Gen AI) technologies, particularly tools like ChatGPT, is significantly impacting the labor market by reshaping job roles and skill requirements. This study examines the demand for ChatGPT-related skills in the U.S. labor market by analyzing job advertisements collected from major job platforms between May and December 2023. Using text mining and topic modeling techniques, we extracted and analyzed the Gen AI-related skills that employers are hiring for. Our analysis identified five distinct ChatGPT-related skill sets: general familiarity, creative content generation, marketing, advanced functionalities (such as prompt engineering), and product development. In addition, the study provides insights into job attributes such as occupation titles, degree requirements, salary ranges, and other relevant job characteristics. These findings highlight the increasing integration of Gen AI across various industries, emphasizing the growing need for both foundational knowledge and advanced technical skills. The study offers valuable insights into the evolving demands of the labor market, as employers seek candidates equipped to leverage generative AI tools to improve productivity, streamline processes, and drive innovation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07042v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mahdi Ahmadi, Neda Khosh Kheslat, Adebola Akintomide</dc:creator>
    </item>
    <item>
      <title>The Mirage of Artificial Intelligence Terms of Use Restrictions</title>
      <link>https://arxiv.org/abs/2412.07066</link>
      <description>arXiv:2412.07066v1 Announce Type: new 
Abstract: Artificial intelligence (AI) model creators commonly attach restrictive terms of use to both their models and their outputs. These terms typically prohibit activities ranging from creating competing AI models to spreading disinformation. Often taken at face value, these terms are positioned by companies as key enforceable tools for preventing misuse, particularly in policy dialogs. But are these terms truly meaningful? There are myriad examples where these broad terms are regularly and repeatedly violated. Yet except for some account suspensions on platforms, no model creator has actually tried to enforce these terms with monetary penalties or injunctive relief. This is likely for good reason: we think that the legal enforceability of these licenses is questionable.
  This Article systematically assesses of the enforceability of AI model terms of use and offers three contributions. First, we pinpoint a key problem: the artifacts that they protect, namely model weights and model outputs, are largely not copyrightable, making it unclear whether there is even anything to be licensed. Second, we examine the problems this creates for other enforcement. Recent doctrinal trends in copyright preemption may further undermine state-law claims, while other legal frameworks like the DMCA and CFAA offer limited recourse. Anti-competitive provisions likely fare even worse than responsible use provisions. Third, we provide recommendations to policymakers. There are compelling reasons for many provisions to be unenforceable: they chill good faith research, constrain competition, and create quasi-copyright ownership where none should exist. There are, of course, downsides: model creators have fewer tools to prevent harmful misuse. But we think the better approach is for statutory provisions, not private fiat, to distinguish between good and bad uses of AI, restricting the latter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07066v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peter Henderson, Mark A. Lemley</dc:creator>
    </item>
    <item>
      <title>Reconciling Human Development and Giant Panda Protection Goals: Cost-efficiency Evaluation of Farmland Reverting and Energy Substitution Programs in Wolong National Reserve</title>
      <link>https://arxiv.org/abs/2412.07275</link>
      <description>arXiv:2412.07275v1 Announce Type: new 
Abstract: Ecological policies that balance human development with conservation must consider cost-effectiveness and local impacts when identifying optimal policy scenarios that maximize outcomes within limited budgets is essential. This study employs the Socio-Econ-Ecosystem Multipurpose Simulator (SEEMS), an Agent-Based Model (ABM) designed for simulating small-scale Coupled Human and Nature Systems (CHANS), and analyzes the cost-effectiveness of two major conversation programs: Grain-to-Green (G2G) and Firewood-to-Electricity (F2E) in terms of financial budget, habitat conservation performance, and local economic impacts with the example of China's Wolong National Reserve, a worldwide hot spot for flagship species conservation. The findings are as follows: (1) The G2G program achieves optimal financial efficiency at approximately 500 CNY/Mu, with diminishing returns observed beyond 1000 CNY/Mu; (2) For the F2E program, the most fiscally cost-efficient option arises when the subsidized electricity price is at 0.4-0.5 CNY/kWh, while further reductions of the prices to below 0.1 CNY/kWh result in a diminishing cost-benefit ratio; (3) Comprehensive cost-efficiency analysis reveals no significant link between financial burden and carbon emissions, but a positive correlation with habitat quality and an inverted U-shaped relationship with total economic income; (4) Pareto analysis identifies 18 optimal dual-policy combinations for balancing carbon footprint, habitat quality, and gross financial benefits; (5) Posterior Pareto optimization further refines the selection of a specific policy scheme for a given realistic scenario. The analytical framework of this paper helps policymakers design economically viable and environmentally sustainable policies, addressing global conservation challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07275v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Keyi Liu, Yufeng Chen, Liyan Xua, Xiao Zhang, Zilin Wang, Hailong Li, Yansheng Yang, Hong You, Dihua Li</dc:creator>
    </item>
    <item>
      <title>Beyond Search Engines: Can Large Language Models Improve Curriculum Development?</title>
      <link>https://arxiv.org/abs/2412.07422</link>
      <description>arXiv:2412.07422v1 Announce Type: new 
Abstract: While Online Learning is growing and becoming widespread, the associated curricula often suffer from a lack of coverage and outdated content. In this regard, a key question is how to dynamically define the topics that must be covered to thoroughly learn a subject (e.g., a course). Large Language Models (LLMs) are considered candidates that can be used to address curriculum development challenges. Therefore, we developed a framework and a novel dataset, built on YouTube, to evaluate LLMs' performance when it comes to generating learning topics for specific courses. The experiment was conducted across over 100 courses and nearly 7,000 YouTube playlists in various subject areas. Our results indicate that GPT-4 can produce more accurate topics for the given courses than extracted topics from YouTube video playlists in terms of BERTScore</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07422v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-72312-4_17</arxiv:DOI>
      <dc:creator>Mohammad Moein, Mohammadreza Molavi Hajiagha, Abdolali Faraji, Mohammadreza Tavakoli, G\`abor Kismih\`ok</dc:creator>
    </item>
    <item>
      <title>Identity theft and societal acceptability of electronic identity in Europe and in the United States</title>
      <link>https://arxiv.org/abs/2412.07445</link>
      <description>arXiv:2412.07445v1 Announce Type: new 
Abstract: This paper addresses critical questions surrounding the security of government-issued identity documents and their potential misuse, with an emphasis on understanding the perspectives of ordinary citizens across Europe and the United States of America. Drawing upon research on technology acceptance and diffusion, the research focuses on understanding the factors that influence users' adoption of novel identity management solutions. Our methodology includes a comprehensive, census-representative survey spanning citizens from France, Germany, Italy, Spain, the United Kingdom, and the USA. The paper's findings underscore a robust confidence in government-issued identity documents, contrasted by a lower trust in private sector services, including social media platforms and email accounts. The adoption of artificial intelligence for identity verification remains contested, with a significant percentage of respondents undecided, indicating a need for explicit explanation and transparency about its implementation and related risks. Public sentiment leans towards acceptance of government data collection for identification purposes; however, the sharing of this data with private entities elicits more apprehension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07445v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marek Tiits, Tarmo Kalvet, David McBee</dc:creator>
    </item>
    <item>
      <title>Access to care improves EHR reliability and clinical risk prediction model performance</title>
      <link>https://arxiv.org/abs/2412.07712</link>
      <description>arXiv:2412.07712v1 Announce Type: new 
Abstract: Disparities in access to healthcare have been well-documented in the United States, but their effects on electronic health record (EHR) data reliability and resulting clinical models is poorly understood. Using an All of Us dataset of 134,513 participants, we investigate the effects of access to care on the medical machine learning pipeline, including medical condition rates, data quality, outcome label accuracy, and prediction performance. Our findings reveal that patients with cost constrained or delayed care have worse EHR reliability as measured by patient self-reported conditions for 78% of examined medical conditions. We demonstrate in a prediction task of Type II diabetes incidence that clinical risk predictive performance can be worse for patients without standard care, with balanced accuracy gaps of 3.6 and sensitivity gaps of 9.4 percentage points for those with cost-constrained or delayed care. We evaluate solutions to mitigate these disparities and find that including patient self-reported conditions improved performance for patients with lower access to care, with 11.2 percentage points higher sensitivity, effectively decreasing the performance gap between standard versus delayed or cost-constrained care. These findings provide the first large-scale evidence that healthcare access systematically affects both data reliability and clinical prediction performance. By revealing how access barriers propagate through the medical machine learning pipeline, our work suggests that improving model equity requires addressing both data collection biases and algorithmic limitations. More broadly, this analysis provides an empirical foundation for developing clinical prediction systems that work effectively for all patients, regardless of their access to care.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07712v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Zink, Hongzhou Luan, Irene Y. Chen</dc:creator>
    </item>
    <item>
      <title>AI Expands Scientists' Impact but Contracts Science's Focus</title>
      <link>https://arxiv.org/abs/2412.07727</link>
      <description>arXiv:2412.07727v1 Announce Type: new 
Abstract: The rapid rise of AI in science presents a paradox. Analyzing 67.9 million research papers across six major fields using a validated language model (F1=0.876), we explore AI's impact on science. Scientists who adopt AI tools publish 67.37% more papers, receive 3.16 times more citations, and become team leaders 4 years earlier than non-adopters. This individual success correlates with concerning on collective effects: AI-augmented research contracts the diameter of scientific topics studied, and diminishes follow-on scientific engagement. Rather than catalyzing the exploration of new fields, AI accelerates work in established, data-rich domains. This pattern suggests that while AI enhances individual scientific productivity, it may simultaneously reduce scientific diversity and broad engagement, highlighting a tension between personal advancement and collective scientific progress.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07727v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qianyue Hao, Fengli Xu, Yong Li, James Evans</dc:creator>
    </item>
    <item>
      <title>Demand Modeling for Advanced Air Mobility</title>
      <link>https://arxiv.org/abs/2412.06807</link>
      <description>arXiv:2412.06807v1 Announce Type: cross 
Abstract: In recent years, the rapid pace of urbanization has posed profound challenges globally, exacerbating environmental concerns and escalating traffic congestion in metropolitan areas. To mitigate these issues, Advanced Air Mobility (AAM) has emerged as a promising transportation alternative. However, the effective implementation of AAM requires robust demand modeling. This study delves into the demand dynamics of AAM by analyzing employment based trip data across Tennessee's census tracts, employing statistical techniques and machine learning models to enhance accuracy in demand forecasting. Drawing on datasets from the Bureau of Transportation Statistics (BTS), the Internal Revenue Service (IRS), the Federal Aviation Administration (FAA), and additional sources, we perform cost, time, and risk assessments to compute the Generalized Cost of Trip (GCT). Our findings indicate that trips are more likely to be viable for AAM if air transportation accounts for over 70\% of the GCT and the journey spans more than 250 miles. The study not only refines the understanding of AAM demand but also guides strategic planning and policy formulation for sustainable urban mobility solutions. The data and code can be accessed on GitHub.{https://github.com/lotussavy/IEEEBigData-2024.git }</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06807v1</guid>
      <category>stat.AP</category>
      <category>cs.CY</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kamal Acharya, Mehul Lad, Liang Sun, Houbing Song</dc:creator>
    </item>
    <item>
      <title>Investigating social alignment via mirroring in a system of interacting language models</title>
      <link>https://arxiv.org/abs/2412.06834</link>
      <description>arXiv:2412.06834v1 Announce Type: cross 
Abstract: Alignment is a social phenomenon wherein individuals share a common goal or perspective. Mirroring, or mimicking the behaviors and opinions of another individual, is one mechanism by which individuals can become aligned. Large scale investigations of the effect of mirroring on alignment have been limited due to the scalability of traditional experimental designs in sociology. In this paper, we introduce a simple computational framework that enables studying the effect of mirroring behavior on alignment in multi-agent systems. We simulate systems of interacting large language models in this framework and characterize overall system behavior and alignment with quantitative measures of agent dynamics. We find that system behavior is strongly influenced by the range of communication of each agent and that these effects are exacerbated by increased rates of mirroring. We discuss the observed simulated system behavior in the context of known human social dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06834v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Harvey McGuinness, Tianyu Wang, Carey E. Priebe, Hayden Helm</dc:creator>
    </item>
    <item>
      <title>Framework to coordinate ubiquitous devices with SOA standards</title>
      <link>https://arxiv.org/abs/2412.06908</link>
      <description>arXiv:2412.06908v1 Announce Type: cross 
Abstract: Context: Ubiquitous devices and pervasive environments are in permanent interaction in people's daily lives. In today's hyper-connected environments, it is necessary for these devices to interact with each other, transparently to the users. The problem is analyzed from the different perspectives that compose it: SOA, service composition, interaction, and the capabilities of ubiquitous devices. Problem: Currently, ubiquitous devices can interact in a limited way due to the proprietary mechanisms and protocols available on the market. The few proposals from academia have hardly achieved an impact in practice. This is not in harmony with the situation of the Internet environment and web services, which have standardized mechanisms for service composition. Aim: Apply the principles of SOA, currently standardized and tested in the information systems industry, for the connectivity of ubiquitous devices in pervasive environments. For this, a coordination framework based on these technologies is proposed. Methodology: We apply an adaptation of Design Science in our environment to allow the iterative construction and evaluation of prototypes. For this, a proof of concept is developed on which this methodology and its cycles are based. Results: We built and put into operation a coordination framework for ubiquitous devices based on WS-CDL, along with a proof of concept. In addition, we contribute to the WS-CDL language in order to support the characteristics of specific ubiquitous devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06908v1</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <category>cs.DC</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oscar A. Testa, Efrain R. Fonseca C., Germ\'an Montejano, Oscar Dieste</dc:creator>
    </item>
    <item>
      <title>Machine Unlearning Doesn't Do What You Think: Lessons for Generative AI Policy, Research, and Practice</title>
      <link>https://arxiv.org/abs/2412.06966</link>
      <description>arXiv:2412.06966v1 Announce Type: cross 
Abstract: We articulate fundamental mismatches between technical methods for machine unlearning in Generative AI, and documented aspirations for broader impact that these methods could have for law and policy. These aspirations are both numerous and varied, motivated by issues that pertain to privacy, copyright, safety, and more. For example, unlearning is often invoked as a solution for removing the effects of targeted information from a generative-AI model's parameters, e.g., a particular individual's personal data or in-copyright expression of Spiderman that was included in the model's training data. Unlearning is also proposed as a way to prevent a model from generating targeted types of information in its outputs, e.g., generations that closely resemble a particular individual's data or reflect the concept of "Spiderman." Both of these goals--the targeted removal of information from a model and the targeted suppression of information from a model's outputs--present various technical and substantive challenges. We provide a framework for thinking rigorously about these challenges, which enables us to be clear about why unlearning is not a general-purpose solution for circumscribing generative-AI model behavior in service of broader positive impact. We aim for conceptual clarity and to encourage more thoughtful communication among machine learning (ML), law, and policy experts who seek to develop and apply technical methods for compliance with policy objectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06966v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>A. Feder Cooper, Christopher A. Choquette-Choo, Miranda Bogen, Matthew Jagielski, Katja Filippova, Ken Ziyu Liu, Alexandra Chouldechova, Jamie Hayes, Yangsibo Huang, Niloofar Mireshghallah, Ilia Shumailov, Eleni Triantafillou, Peter Kairouz, Nicole Mitchell, Percy Liang, Daniel E. Ho, Yejin Choi, Sanmi Koyejo, Fernando Delgado, James Grimmelmann, Vitaly Shmatikov, Christopher De Sa, Solon Barocas, Amy Cyphert, Mark Lemley, danah boyd, Jennifer Wortman Vaughan, Miles Brundage, David Bau, Seth Neel, Abigail Z. Jacobs, Andreas Terzis, Hanna Wallach, Nicolas Papernot, Katherine Lee</dc:creator>
    </item>
    <item>
      <title>Learning about algorithm auditing in five steps: scaffolding how high school youth can systematically and critically evaluate machine learning applications</title>
      <link>https://arxiv.org/abs/2412.06989</link>
      <description>arXiv:2412.06989v1 Announce Type: cross 
Abstract: While there is widespread interest in supporting young people to critically evaluate machine learning-powered systems, there is little research on how we can support them in inquiring about how these systems work and what their limitations and implications may be. Outside of K-12 education, an effective strategy in evaluating black-boxed systems is algorithm auditing-a method for understanding algorithmic systems' opaque inner workings and external impacts from the outside in. In this paper, we review how expert researchers conduct algorithm audits and how end users engage in auditing practices to propose five steps that, when incorporated into learning activities, can support young people in auditing algorithms. We present a case study of a team of teenagers engaging with each step during an out-of-school workshop in which they audited peer-designed generative AI TikTok filters. We discuss the kind of scaffolds we provided to support youth in algorithm auditing and directions and challenges for integrating algorithm auditing into classroom activities. This paper contributes: (a) a conceptualization of five steps to scaffold algorithm auditing learning activities, and (b) examples of how youth engaged with each step during our pilot study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06989v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Luis Morales-Navarro, Yasmin B. Kafai, Lauren Vogelstein, Evelyn Yu, Dana\"e Metaxa</dc:creator>
    </item>
    <item>
      <title>Assessing the Impact of Conspiracy Theories Using Large Language Models</title>
      <link>https://arxiv.org/abs/2412.07019</link>
      <description>arXiv:2412.07019v1 Announce Type: cross 
Abstract: Measuring the relative impact of CTs is important for prioritizing responses and allocating resources effectively, especially during crises. However, assessing the actual impact of CTs on the public poses unique challenges. It requires not only the collection of CT-specific knowledge but also diverse information from social, psychological, and cultural dimensions. Recent advancements in large language models (LLMs) suggest their potential utility in this context, not only due to their extensive knowledge from large training corpora but also because they can be harnessed for complex reasoning. In this work, we develop datasets of popular CTs with human-annotated impacts. Borrowing insights from human impact assessment processes, we then design tailored strategies to leverage LLMs for performing human-like CT impact assessments. Through rigorous experiments, we textit{discover that an impact assessment mode using multi-step reasoning to analyze more CT-related evidence critically produces accurate results; and most LLMs demonstrate strong bias, such as assigning higher impacts to CTs presented earlier in the prompt, while generating less accurate impact assessments for emotionally charged and verbose CTs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07019v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bohan Jiang, Dawei Li, Zhen Tan, Xinyi Zhou, Ashwin Rao, Kristina Lerman, H. Russell Bernard, Huan Liu</dc:creator>
    </item>
    <item>
      <title>Deep Learning for Cross-Border Transaction Anomaly Detection in Anti-Money Laundering Systems</title>
      <link>https://arxiv.org/abs/2412.07027</link>
      <description>arXiv:2412.07027v1 Announce Type: cross 
Abstract: In the context of globalization and the rapid expansion of the digital economy, anti-money laundering (AML) has become a crucial aspect of financial oversight, particularly in cross-border transactions. The rising complexity and scale of international financial flows necessitate more intelligent and adaptive AML systems to combat increasingly sophisticated money laundering techniques. This paper explores the application of unsupervised learning models in cross-border AML systems, focusing on rule optimization through contrastive learning techniques. Five deep learning models, ranging from basic convolutional neural networks (CNNs) to hybrid CNNGRU architectures, were designed and tested to assess their performance in detecting abnormal transactions. The results demonstrate that as model complexity increases, so does the system's detection accuracy and responsiveness. In particular, the self-developed hybrid Convolutional-Recurrent Neural Integration Model (CRNIM) model showed superior performance in terms of accuracy and area under the receiver operating characteristic curve (AUROC). These findings highlight the potential of unsupervised learning models to significantly improve the intelligence, flexibility, and real-time capabilities of AML systems. By optimizing detection rules and enhancing adaptability to emerging money laundering schemes, this research provides both theoretical and practical contributions to the advancement of AML technologies, which are essential for safeguarding the global financial system against illicit activities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07027v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <category>q-fin.RM</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qian Yu, Zhen Xu, Zong Ke</dc:creator>
    </item>
    <item>
      <title>A Review of Challenges in Speech-based Conversational AI for Elderly Care</title>
      <link>https://arxiv.org/abs/2412.07388</link>
      <description>arXiv:2412.07388v1 Announce Type: cross 
Abstract: Artificially intelligent systems optimized for speech conversation are appearing at a fast pace. Such models are interesting from a healthcare perspective, as these voice-controlled assistants may support the elderly and enable remote health monitoring. The bottleneck for efficacy, however, is how well these devices work in practice and how the elderly experience them, but research on this topic is scant. We review elderly use of voice-controlled AI and highlight various user- and technology-centered issues, that need to be considered before effective speech-controlled AI for elderly care can be realized.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07388v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Willemijn Klaassen, Bram van Dijk, Marco Spruit</dc:creator>
    </item>
    <item>
      <title>Machine Learning Algorithms for Detecting Mental Stress in College Students</title>
      <link>https://arxiv.org/abs/2412.07415</link>
      <description>arXiv:2412.07415v1 Announce Type: cross 
Abstract: In today's world, stress is a big problem that affects people's health and happiness. More and more people are feeling stressed out, which can lead to lots of health issues like breathing problems, feeling overwhelmed, heart attack, diabetes, etc. This work endeavors to forecast stress and non-stress occurrences among college students by applying various machine learning algorithms: Decision Trees, Random Forest, Support Vector Machines, AdaBoost, Naive Bayes, Logistic Regression, and K-nearest Neighbors. The primary objective of this work is to leverage a research study to predict and mitigate stress and non-stress based on the collected questionnaire dataset. We conducted a workshop with the primary goal of studying the stress levels found among the students. This workshop was attended by Approximately 843 students aged between 18 to 21 years old. A questionnaire was given to the students validated under the guidance of the experts from the All India Institute of Medical Sciences (AIIMS) Raipur, Chhattisgarh, India, on which our dataset is based. The survey consists of 28 questions, aiming to comprehensively understand the multidimensional aspects of stress, including emotional well-being, physical health, academic performance, relationships, and leisure. This work finds that Support Vector Machines have a maximum accuracy for Stress, reaching 95\%. The study contributes to a deeper understanding of stress determinants. It aims to improve college student's overall quality of life and academic success, addressing the multifaceted nature of stress.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07415v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/I2CT61223.2024.10544243</arxiv:DOI>
      <arxiv:journal_reference>2024 IEEE 9th International Conference for Convergence in Technology (I2CT)</arxiv:journal_reference>
      <dc:creator>Ashutosh Singh, Khushdeep Singh, Amit Kumar, Abhishek Shrivastava, Santosh Kumar</dc:creator>
    </item>
    <item>
      <title>Automating Governing Knowledge Commons and Contextual Integrity (GKC-CI) Privacy Policy Annotations with Large Language Models</title>
      <link>https://arxiv.org/abs/2311.02192</link>
      <description>arXiv:2311.02192v3 Announce Type: replace 
Abstract: Identifying contextual integrity (CI) and governing knowledge commons (GKC) parameters in privacy policy texts can facilitate normative privacy analysis. However, GKC-CI annotation has heretofore required manual or crowdsourced effort. This paper demonstrates that high-accuracy GKC-CI parameter annotation of privacy policies can be performed automatically using large language models. We fine-tune 50 open-source and proprietary models on 21,588 ground truth GKC-CI annotations from 16 privacy policies. Our best performing model has an accuracy of 90.65%, which is comparable to the accuracy of experts on the same task. We apply our best performing model to 456 privacy policies from a variety of online services, demonstrating the effectiveness of scaling GKC-CI annotation for privacy policy exploration and analysis. We publicly release our model training code, training and testing data, an annotation visualizer, and all annotated policies for future GKC-CI research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.02192v3</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jake Chanenson, Madison Pickering, Noah Apthorpe</dc:creator>
    </item>
    <item>
      <title>Investigating Bias in LLM-Based Bias Detection: Disparities between LLMs and Human Perception</title>
      <link>https://arxiv.org/abs/2403.14896</link>
      <description>arXiv:2403.14896v2 Announce Type: replace 
Abstract: The pervasive spread of misinformation and disinformation in social media underscores the critical importance of detecting media bias. While robust Large Language Models (LLMs) have emerged as foundational tools for bias prediction, concerns about inherent biases within these models persist. In this work, we investigate the presence and nature of bias within LLMs and its consequential impact on media bias detection. Departing from conventional approaches that focus solely on bias detection in media content, we delve into biases within the LLM systems themselves. Through meticulous examination, we probe whether LLMs exhibit biases, particularly in political bias prediction and text continuation tasks. Additionally, we explore bias across diverse topics, aiming to uncover nuanced variations in bias expression within the LLM framework. Importantly, we propose debiasing strategies, including prompt engineering and model fine-tuning. Extensive analysis of bias tendencies across different LLMs sheds light on the broader landscape of bias propagation in language models. This study advances our understanding of LLM bias, offering critical insights into its implications for bias detection tasks and paving the way for more robust and equitable AI systems</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14896v2</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luyang Lin, Lingzhi Wang, Jinsong Guo, Kam-Fai Wong</dc:creator>
    </item>
    <item>
      <title>SOMONITOR: Combining Explainable AI &amp; Large Language Models for Marketing Analytics</title>
      <link>https://arxiv.org/abs/2407.13117</link>
      <description>arXiv:2407.13117v2 Announce Type: replace 
Abstract: Online marketing faces formidable challenges in managing and interpreting immense volumes of data necessary for competitor analysis, content research, and strategic branding. It is impossible to review hundreds to thousands of transient online content items by hand, and partial analysis often leads to suboptimal outcomes and poorly performing campaigns. We introduce an explainable AI framework SOMONITOR that aims to synergize human intuition with AI-based efficiency, helping marketers across all stages of the marketing funnel, from strategic planning to content creation and campaign execution. SOMONITOR incorporates a CTR prediction and ranking model for advertising content and uses large language models (LLMs) to process high-performing competitor content, identifying core content pillars such as target audiences, customer needs, and product features. These pillars are then organized into broader categories, including communication themes and targeted customer personas. By integrating these insights with data from the brand's own advertising campaigns, SOMONITOR constructs a narrative for addressing new customer personas and simultaneously generates detailed content briefs in the form of user stories that, as shown in the conducted case study, can be directly applied by marketing teams to streamline content production and campaign execution. The adoption of SOMONITOR in daily operations allows digital marketers to quickly parse through extensive datasets, offering actionable insights that significantly enhance campaign effectiveness and overall job satisfaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13117v2</guid>
      <category>cs.CY</category>
      <category>cs.MM</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aleksandr Farseev, Qi Yang, Marlo Ongpin, Ilia Gossoudarev, Yu-Yi Chu-Farseeva, Sergey Nikolenko</dc:creator>
    </item>
    <item>
      <title>From Text to Multimodality: Exploring the Evolution and Impact of Large Language Models in Medical Practice</title>
      <link>https://arxiv.org/abs/2410.01812</link>
      <description>arXiv:2410.01812v5 Announce Type: replace 
Abstract: Large Language Models (LLMs) have rapidly evolved from text-based systems to multimodal platforms, significantly impacting various sectors including healthcare. This comprehensive review explores the progression of LLMs to Multimodal Large Language Models (MLLMs) and their growing influence in medical practice. We examine the current landscape of MLLMs in healthcare, analyzing their applications across clinical decision support, medical imaging, patient engagement, and research. The review highlights the unique capabilities of MLLMs in integrating diverse data types, such as text, images, and audio, to provide more comprehensive insights into patient health. We also address the challenges facing MLLM implementation, including data limitations, technical hurdles, and ethical considerations. By identifying key research gaps, this paper aims to guide future investigations in areas such as dataset development, modality alignment methods, and the establishment of ethical guidelines. As MLLMs continue to shape the future of healthcare, understanding their potential and limitations is crucial for their responsible and effective integration into medical practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01812v5</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Qian Niu, Keyu Chen, Ming Li, Pohsun Feng, Ziqian Bi, Lawrence KQ Yan, Yichao Zhang, Caitlyn Heqi Yin, Cheng Fei, Junyu Liu, Benji Peng, Tianyang Wang, Yunze Wang, Silin Chen, Ming Liu</dc:creator>
    </item>
    <item>
      <title>Chatbots im Schulunterricht: Wir testen das Fobizz-Tool zur automatischen Bewertung von Hausaufgaben</title>
      <link>https://arxiv.org/abs/2412.06651</link>
      <description>arXiv:2412.06651v2 Announce Type: replace 
Abstract: [Study in German language.] This study examines the AI-powered grading tool "AI Grading Assistant" by the German company Fobizz, designed to support teachers in evaluating and providing feedback on student assignments. Against the societal backdrop of an overburdened education system and rising expectations for artificial intelligence as a solution to these challenges, the investigation evaluates the tool's functional suitability through two test series. The results reveal significant shortcomings: The tool's numerical grades and qualitative feedback are often random and do not improve even when its suggestions are incorporated. The highest ratings are achievable only with texts generated by ChatGPT. False claims and nonsensical submissions frequently go undetected, while the implementation of some grading criteria is unreliable and opaque. Since these deficiencies stem from the inherent limitations of large language models (LLMs), fundamental improvements to this or similar tools are not immediately foreseeable. The study critiques the broader trend of adopting AI as a quick fix for systemic problems in education, concluding that Fobizz's marketing of the tool as an objective and time-saving solution is misleading and irresponsible. Finally, the study calls for systematic evaluation and subject-specific pedagogical scrutiny of the use of AI tools in educational contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06651v2</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rainer Muehlhoff, Marte Henningsen</dc:creator>
    </item>
    <item>
      <title>Classification of the lunar surface pattern by AI architectures: Does AI see a rabbit in the Moon?</title>
      <link>https://arxiv.org/abs/2308.11107</link>
      <description>arXiv:2308.11107v2 Announce Type: replace-cross 
Abstract: In Asian countries, there is a tradition that a rabbit, known as the Moon rabbit, lives on the Moon. Typically, two reasons are mentioned for the origin of this tradition. The first reason is that the color pattern of the lunar surface resembles the shape of a rabbit. The second reason is that both the Moon and rabbits are symbols of fertility, as the Moon appears and disappears (i.e., waxing and waning) cyclically and rabbits are known for their high fertility. Considering the latter reason, is the color pattern of the lunar surface not similar to a rabbit? Here, the similarity between rabbit and the lunar surface pattern was evaluated using seven AI architectures. In the test conducted with Contrastive Language-Image Pre-Training (CLIP), which can classify images based on given words, it was assumed that people frequently observe the Moon in the early evening. Under this condition, the lunar surface pattern was found to be more similar to a rabbit than a face in low-latitude regions, while it could also be classified as a face as the latitude increases. This result is consistent with that the oldest literatures about the Moon rabbit were written in India and that a tradition of seeing a human face in the Moon exists in Europe. In a 1000-class test using seven AI architectures, ConvNeXt and CLIP sometimes classified the lunar surface pattern as a rabbit with relatively high probabilities. Cultures are generated by our attitude to the environment. Both dynamic and static similarities may be essential to induce our imagination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.11107v2</guid>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Daigo Shoji</dc:creator>
    </item>
    <item>
      <title>SCORPION Cyber Range: Fully Customizable Cyberexercises, Gamification, and Learning Analytics to Train Cybersecurity Competencies</title>
      <link>https://arxiv.org/abs/2401.12594</link>
      <description>arXiv:2401.12594v4 Announce Type: replace-cross 
Abstract: It is undeniable that we are witnessing an unprecedented digital revolution. However, recent years have been characterized by the explosion of cyberattacks, making cybercrime one of the most profitable businesses on the planet. That is why training in cybersecurity is increasingly essential to protect the assets of cyberspace. One of the most vital tools to train cybersecurity competencies is the Cyber Range, a virtualized environment that simulates realistic networks. The paper at hand introduces SCORPION, a fully functional and virtualized Cyber Range, which manages the authoring and automated deployment of scenarios. In addition, SCORPION includes several elements to improve student motivation, such as a gamification system with medals, points, or rankings, among other elements. Such a gamification system includes an adaptive learning module that is able to adapt the cyberexercise based on the users' performance. Moreover, SCORPION leverages learning analytics that collects and processes telemetric and biometric user data, including heart rate through a smartwatch, which is available through a dashboard for instructors. Finally, we developed a case study where SCORPION obtained 82.10% in usability and 4.57 out of 5 in usefulness from the viewpoint of a student and an instructor. The positive evaluation results are promising, indicating that SCORPION can become an effective, motivating, and advanced cybersecurity training tool to help fill current gaps in this context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.12594v4</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pantaleone Nespoli, Mariano Albaladejo-Gonz\'alez, Jos\'e A. Ruip\'erez-Valiente, Joaquin Garcia-Alfaro</dc:creator>
    </item>
    <item>
      <title>Is Self-knowledge and Action Consistent or Not: Investigating Large Language Model's Personality</title>
      <link>https://arxiv.org/abs/2402.14679</link>
      <description>arXiv:2402.14679v2 Announce Type: replace-cross 
Abstract: In this study, we delve into the validity of conventional personality questionnaires in capturing the human-like personality traits of Large Language Models (LLMs). Our objective is to assess the congruence between the personality traits LLMs claim to possess and their demonstrated tendencies in real-world scenarios. By conducting an extensive examination of LLM outputs against observed human response patterns, we aim to understand the disjunction between self-knowledge and action in LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.14679v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiming Ai, Zhiwei He, Ziyin Zhang, Wenhong Zhu, Hongkun Hao, Kai Yu, Lingjun Chen, Rui Wang</dc:creator>
    </item>
    <item>
      <title>EARN Fairness: Explaining, Asking, Reviewing, and Negotiating Artificial Intelligence Fairness Metrics Among Stakeholders</title>
      <link>https://arxiv.org/abs/2407.11442</link>
      <description>arXiv:2407.11442v2 Announce Type: replace-cross 
Abstract: Numerous fairness metrics have been proposed and employed by artificial intelligence (AI) experts to quantitatively measure bias and define fairness in AI models. Recognizing the need to accommodate stakeholders' diverse fairness understandings, efforts are underway to solicit their input. However, conveying AI fairness metrics to stakeholders without AI expertise, capturing their personal preferences, and seeking a collective consensus remain challenging and underexplored. To bridge this gap, we propose a new framework, EARN Fairness, which facilitates collective metric decisions among stakeholders without requiring AI expertise. The framework features an adaptable interactive system and a stakeholder-centered EARN Fairness process to Explain fairness metrics, Ask stakeholders' personal metric preferences, Review metrics collectively, and Negotiate a consensus on metric selection. To gather empirical results, we applied the framework to a credit rating scenario and conducted a user study involving 18 decision subjects without AI knowledge. We identify their personal metric preferences and their acceptable level of unfairness in individual sessions. Subsequently, we uncovered how they reached metric consensus in team sessions. Our work shows that the EARN Fairness framework enables stakeholders to express personal preferences and reach consensus, providing practical guidance for implementing human-centered AI fairness in high-risk contexts. Through this approach, we aim to harmonize fairness expectations of diverse stakeholders, fostering more equitable and inclusive AI fairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11442v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lin Luo, Yuri Nakao, Mathieu Chollet, Hiroya Inakoshi, Simone Stumpf</dc:creator>
    </item>
    <item>
      <title>From Measurement Instruments to Data: Leveraging Theory-Driven Synthetic Training Data for Classifying Social Constructs</title>
      <link>https://arxiv.org/abs/2410.12622</link>
      <description>arXiv:2410.12622v3 Announce Type: replace-cross 
Abstract: Computational text classification is a challenging task, especially for multi-dimensional social constructs. Recently, there has been increasing discussion that synthetic training data could enhance classification by offering examples of how these constructs are represented in texts. In this paper, we systematically examine the potential of theory-driven synthetic training data for improving the measurement of social constructs. In particular, we explore how researchers can transfer established knowledge from measurement instruments in the social sciences, such as survey scales or annotation codebooks, into theory-driven generation of synthetic data. Using two studies on measuring sexism and political topics, we assess the added value of synthetic training data for fine-tuning text classification models. Although the results of the sexism study were less promising, our findings demonstrate that synthetic data can be highly effective in reducing the need for labeled data in political topic classification. With only a minimal drop in performance, synthetic data allows for substituting large amounts of labeled data. Furthermore, theory-driven synthetic data performed markedly better than data generated without conceptual information in mind.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12622v3</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lukas Birkenmaier, Matthias Roth, Indira Sen</dc:creator>
    </item>
    <item>
      <title>Quantum computing inspired paintings: reinterpreting classical masterpieces</title>
      <link>https://arxiv.org/abs/2411.09549</link>
      <description>arXiv:2411.09549v2 Announce Type: replace-cross 
Abstract: We aim to apply a quantum computing technique to compose artworks. The main idea is to revisit three paintings of different styles and historical periods: ''Narciso'', painted circa 1597-1599 by Michelangelo Merisi (Caravaggio), ''Les fils de l'homme'', painted in 1964 by Rene Magritte and ''192 Farben'', painted in 1966 by Gerard Richter. We utilize the output of a quantum computation to change the composition in the paintings, leading to a paintings series titled ''Quantum Transformation I, II, III''. In particular, the figures are discretized into square lattices and the order of the pieces is changed according to the result of the quantum simulation. We consider an Ising Hamiltonian as the observable in the quantum computation and its time evolution as the final outcome. From a classical subject to abstract forms, we seek to combine classical and quantum aesthetics through these three art pieces. Besides experimenting with hardware runs and circuit noise, our goal is to reproduce these works as physical oil paintings on wooden panels. With this process, we complete a full circle between classical and quantum techniques and contribute to rethinking Art practice in the era of quantum computing technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09549v2</guid>
      <category>quant-ph</category>
      <category>cs.CY</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arianna Crippa, Yahui Chai, Omar Costa Hamido, Paulo Itaborai, Karl Jansen</dc:creator>
    </item>
    <item>
      <title>Argumentative Experience: Reducing Confirmation Bias on Controversial Issues through LLM-Generated Multi-Persona Debates</title>
      <link>https://arxiv.org/abs/2412.04629</link>
      <description>arXiv:2412.04629v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are enabling designers to give life to exciting new user experiences for information access. In this work, we present a system that generates LLM personas to debate a topic of interest from different perspectives. How might information seekers use and benefit from such a system? Can centering information access around diverse viewpoints help to mitigate thorny challenges like confirmation bias in which information seekers over-trust search results matching existing beliefs? How do potential biases and hallucinations in LLMs play out alongside human users who are also fallible and possibly biased?
  Our study exposes participants to multiple viewpoints on controversial issues via a mixed-methods, within-subjects study. We use eye-tracking metrics to quantitatively assess cognitive engagement alongside qualitative feedback. Compared to a baseline search system, we see more creative interactions and diverse information-seeking with our multi-persona debate system, which more effectively reduces user confirmation bias and conviction toward their initial beliefs. Overall, our study contributes to the emerging design space of LLM-based information access systems, specifically investigating the potential of simulated personas to promote greater exposure to information diversity, emulate collective intelligence, and mitigate bias in information seeking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04629v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.IR</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Li Shi, Houjiang Liu, Yian Wong, Utkarsh Mujumdar, Dan Zhang, Jacek Gwizdka, Matthew Lease</dc:creator>
    </item>
  </channel>
</rss>
