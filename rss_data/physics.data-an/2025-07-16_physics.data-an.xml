<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 17 Jul 2025 01:29:34 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Optimal Calibration of Qubit Detuning and Crosstalk</title>
      <link>https://arxiv.org/abs/2507.10661</link>
      <description>arXiv:2507.10661v1 Announce Type: cross 
Abstract: Characterizing and calibrating physical qubits is essential for maintaining the performance of quantum processors. A key challenge in this process is the presence of crosstalk that complicates the estimation of individual qubit detunings. In this work, we derive optimal strategies for estimating detuning and crosstalk parameters by optimizing Ramsey interference experiments using Fisher information and the Cramer-Rao bound. We compare several calibration protocols, including measurements of a single quadrature at multiple times and of two quadratures at a single time, for a fixed number of total measurements. Our results predict that the latter approach yields the highest precision and robustness in both cases of isolated and coupled qubits. We validate experimentally our approach using a single NV center as well as superconducting transmons. Our approach enables accurate parameter extraction with significantly fewer measurements, resulting in up to a 50\% reduction in calibration time while maintaining estimation accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10661v1</guid>
      <category>quant-ph</category>
      <category>cond-mat.other</category>
      <category>cond-mat.stat-mech</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Shnaiderov, Matan Ben Dov, Yoav Woldiger, Assaf Hamo, Eugene Demler, Emanuele G. Dalla Torre</dc:creator>
    </item>
    <item>
      <title>Information Field Theory based Event Reconstruction for Cosmic Ray Radio Detectors</title>
      <link>https://arxiv.org/abs/2507.10738</link>
      <description>arXiv:2507.10738v1 Announce Type: cross 
Abstract: Detection of extensive air showers with radio antennas is an appealing technique in cosmic ray physics. However, because of the high level of measurement noise, current reconstruction methods still leave room for improvement. Furthermore, reconstruction efforts typically focus only on a single aspect of the signal, such as the energy fluence or arrival time. Bayesian inference is then a natural choice for a holistic approach to reconstruction, yet, this problem would be ill-posed, since the electric field is a continuous quantity. Information Field Theory provides the solution for this by providing a statistical framework to deal with discretised fields in the continuum limit. We are currently developing models for this novel approach to reconstructing extensive air showers. The model described here is based on the best current understanding of the emission mechanisms: It uses parametrisations of the lateral signal strength distribution, charge-excess contribution and spectral shape. Shower-to-shower fluctuations and narrowband RFI are modelled using Gaussian processes. Combined with a detailed detector description, this model can infer not only the electric field, but also the shower geometry, electromagnetic energy and position of shower maximum. Another big achievement of this approach is its ability to naturally provide uncertainties for the reconstruction, which has been shown to be difficult in more traditional methods. With such an open framework and robust computational methods based in Information Field Theory, it will also be easy to incorporate new insights and additional data, such as timing distributions or particle detector data, in the future. This approach has a high potential to exploit the full information content of a complex detector with rigorous statistical methods, in a way that directly includes domain knowledge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10738v1</guid>
      <category>astro-ph.IM</category>
      <category>astro-ph.HE</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Simon Str\"ahnz, Tim Huege, Torsten En{\ss}lin, Karen Terveer, Anna Nelles</dc:creator>
    </item>
    <item>
      <title>Universal self-similarity of hierarchical communities formed through a general self-organizing principle</title>
      <link>https://arxiv.org/abs/2507.11159</link>
      <description>arXiv:2507.11159v1 Announce Type: cross 
Abstract: Emergence of self-similarity in hierarchical community structures is ubiquitous in complex systems. Yet, there is a dearth of universal quantification and general principles describing the formation of such structures. Here, we discover universality in scaling laws describing self-similar hierarchical community structure in multiple real-world networks including biological, infrastructural, and social networks. We replicate these scaling relations using a phenomenological model, where nodes with higher similarity in their properties have greater probability of forming a connection. A large difference in their properties forces two nodes into different communities. Smaller communities are formed owing to further differences in node properties within a larger community. We discover that the general self-organizing principle is in agreement with Hakens principle; nodes self-organize into groups such that the diversity or differences between properties of nodes in the same community is minimized at each scale and the organizational entropy decreases with increasing complexity of the organized structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11159v1</guid>
      <category>physics.soc-ph</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shruti Tandon (equal), Nidhi Dilip Sonwane (equal), Tobias Braun, Norbert Marwan, Juergen Kurths, R. I. Sujith</dc:creator>
    </item>
    <item>
      <title>FlexCAST: Enabling Flexible Scientific Data Analyses</title>
      <link>https://arxiv.org/abs/2507.11528</link>
      <description>arXiv:2507.11528v1 Announce Type: cross 
Abstract: The development of scientific data analyses is a resource-intensive process that often yields results with untapped potential for reuse and reinterpretation. In many cases, a developed analysis can be used to measure more than it was designed for, by changing its input data or parametrization. Existing reinterpretation frameworks, such as RECAST, enable analysis reinterpretation by preserving the analysis implementation to allow for changes of particular parts of the input data. We introduce FlexCAST, which generalizes this concept by preserving the analysis design itself, supporting changes to the entire input data and analysis parametrization. FlexCAST is based on three core principles: modularity, validity, and robustness. Modularity enables a change of the input data and parametrization, while validity ensures that the obtained results remain meaningful, and robustness ensures that as many configurations as possible yield meaningful results. While not being limited to data-driven machine learning techniques, FlexCAST is particularly valuable for the reinterpretation of analyses in this context, where changes in the input data can significantly impact the parametrization of the analysis. Using a state-of-the-art anomaly detection analysis on LHC-like data, we showcase FlexCAST's core principles and demonstrate how it can expand the reach of scientific data analysis through flexible reuse and reinterpretation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11528v1</guid>
      <category>hep-ex</category>
      <category>hep-ph</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Nachman, Dennis Noll</dc:creator>
    </item>
    <item>
      <title>Entropy-based models to randomize real-world hypergraphs</title>
      <link>https://arxiv.org/abs/2207.12123</link>
      <description>arXiv:2207.12123v3 Announce Type: replace-cross 
Abstract: Network theory has often disregarded many-body relationships, solely focusing on pairwise interactions: neglecting them, however, can lead to misleading representations of complex systems. Hypergraphs represent a suitable framework for describing polyadic interactions. Here, we leverage the representation of hypergraphs based on the incidence matrix for extending the entropy-based approach to higher-order structures: in analogy with the Exponential Random Graphs, we introduce the Exponential Random Hypergraphs (ERHs). After exploring the asymptotic behaviour of thresholds generalising the percolation one, we apply ERHs to study real-world data. First, we generalise key network metrics to hypergraphs; then, we compute their expected value and compare it with the empirical one, in order to detect deviations from random behaviours. Our method is analytically tractable, scalable and capable of revealing structural patterns of real-world hypergraphs that differ significantly from those emerging as a consequence of simpler constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.12123v3</guid>
      <category>cs.SI</category>
      <category>cond-mat.stat-mech</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1038/s42005-025-02182-2</arxiv:DOI>
      <arxiv:journal_reference>Comm. Phys. 8 (284) (2025)</arxiv:journal_reference>
      <dc:creator>Fabio Saracco, Giovanni Petri, Renaud Lambiotte, Tiziano Squartini</dc:creator>
    </item>
    <item>
      <title>Graph-based Full Event Interpretation: a graph neural network for event reconstruction in Belle II</title>
      <link>https://arxiv.org/abs/2503.09401</link>
      <description>arXiv:2503.09401v3 Announce Type: replace-cross 
Abstract: In this work we present the Graph-based Full Event Interpretation (GraFEI), a machine learning model based on graph neural networks to inclusively reconstruct events in the Belle~II experiment. Belle~II is well suited to perform measurements of $B$ meson decays involving invisible particles (e.g. neutrinos) in the final state. The kinematical properties of such particles can be deduced from the energy-momentum imbalance obtained after reconstructing the companion $B$ meson produced in the event. This task is performed by reconstructing it either from all the particles in an event but the signal tracks, or using the Full Event Interpretation, an algorithm based on Boosted Decision Trees and limited to specific, hard-coded decay processes. A recent example involving the use of the aforementioned techniques is the search for the $B^+ \to K^+ \nu \bar \nu$ decay, that provided an evidence for this process at about 3 standard deviations. The GraFEI model is trained to predict the structure of the decay chain by exploiting the information from the detected final state particles only, without making use of any prior assumptions about the underlying event. By retaining only signal-like decay topologies, the model considerably reduces the amount of background while keeping a relatively high signal efficiency. The performances of the model when applied to the search for $B^+ \to K^+ \nu \bar \nu$ are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09401v3</guid>
      <category>hep-ex</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Merna Abumusabh, Jacopo Cerasoli, Giulio Dujany, Corentin Santos</dc:creator>
    </item>
  </channel>
</rss>
