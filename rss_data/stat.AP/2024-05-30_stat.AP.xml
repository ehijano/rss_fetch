<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 31 May 2024 01:52:55 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 30 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Continuously Optimizing Radar Placement with Model Predictive Path Integrals</title>
      <link>https://arxiv.org/abs/2405.18999</link>
      <description>arXiv:2405.18999v2 Announce Type: new 
Abstract: Continuously optimizing sensor placement is essential for precise target localization in various military and civilian applications. While information theory has shown promise in optimizing sensor placement, many studies oversimplify sensor measurement models or neglect dynamic constraints of mobile sensors. To address these challenges, we employ a range measurement model that incorporates radar parameters and radar-target distance, coupled with Model Predictive Path Integral (MPPI) control to manage complex environmental obstacles and dynamic constraints. We compare the proposed approach against stationary radars or simplified range measurement models based on the root mean squared error (RMSE) of the Cubature Kalman Filter (CKF) estimator for the targets' state. Additionally, we visualize the evolving geometry of radars and targets over time, highlighting areas of highest measurement information gain, demonstrating the strengths of the approach. The proposed strategy outperforms stationary radars and simplified range measurement models in target localization, achieving a 38-74% reduction in mean RMSE and a 33-79% reduction in the upper tail of the 90% Highest Density Interval (HDI) over 500 Monte Carl (MC) trials across all time steps.
  Code will be made publicly available upon acceptance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18999v2</guid>
      <category>stat.AP</category>
      <category>cs.AI</category>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Potter, Shuo Tang, Paul Ghanem, Milica Stojanovic, Pau Closas, Murat Akcakaya, Ben Wright, Marius Necsoiu, Deniz Erdogmus, Michael Everett, Tales Imbiriba</dc:creator>
    </item>
    <item>
      <title>Examining the development of attitude scales using Large Language Models (LLMs)</title>
      <link>https://arxiv.org/abs/2405.19011</link>
      <description>arXiv:2405.19011v1 Announce Type: new 
Abstract: For nearly a century, social researchers and psychologists have debated the efficacy of psychometric scales for attitude measurement, focusing on Thurstone's equal appearing interval scales and Likert's summated rating scales. Thurstone scales fell out of favour due to the labour intensive process of gathering judges' opinions on the initial items. However, advancements in technology have mitigated these challenges, nullifying the simplicity advantage of Likert scales, which have their own methodological issues. This study explores a methodological experiment to develop a Thurstone scale for assessing attitudes towards individuals living with AIDS. An electronic questionnaire was distributed to a group of judges, including undergraduate, postgraduate, and PhD students from disciplines such as social policy, law, medicine, and computer engineering, alongside established social researchers, and their responses were statistically analysed. The primary innovation of this study is the incorporation of an Artificial Intelligence (AI) Large Language Model (LLM) to evaluate the initial 63 items, comparing its assessments with those of the human judges. Interestingly, the AI provided also detailed explanations for its categorisation. Results showed no significant difference between AI and human judges for 35 items, minor differences for 23 items, and major differences for 5 items. This experiment demonstrates the potential of integrating AI with traditional psychometric methods to enhance the development of attitude measurement scales.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19011v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Maria Symeonaki, Giorgos Stamou, Aggeliki Kazani, Eva Tsouparopoulou, Glykeria Stamatopoulou</dc:creator>
    </item>
    <item>
      <title>Discovering deposition process regimes: leveraging unsupervised learning for process insights, surrogate modeling, and sensitivity analysis</title>
      <link>https://arxiv.org/abs/2405.18444</link>
      <description>arXiv:2405.18444v1 Announce Type: cross 
Abstract: This work introduces a comprehensive approach utilizing data-driven methods to elucidate the deposition process regimes in Chemical Vapor Deposition (CVD) reactors and the interplay of physical mechanism that dominate in each one of them. Through this work, we address three key objectives. Firstly, our methodology relies on process outcomes, derived by a detailed CFD model, to identify clusters of "outcomes" corresponding to distinct process regimes, wherein the relative influence of input variables undergoes notable shifts. This phenomenon is experimentally validated through Arrhenius plot analysis, affirming the efficacy of our approach. Secondly, we demonstrate the development of an efficient surrogate model, based on Polynomial Chaos Expansion (PCE), that maintains accuracy, facilitating streamlined computational analyses. Finally, as a result of PCE, sensitivity analysis is made possible by means of Sobol' indices, that quantify the impact of process inputs across identified regimes. The insights gained from our analysis contribute to the formulation of hypotheses regarding phenomena occurring beyond the transition regime. Notably, the significance of temperature even in the diffusion-limited regime, as evidenced by the Arrhenius plot, suggests activation of gas phase reactions at elevated temperatures. Importantly, our proposed methods yield insights that align with experimental observations and theoretical principles, aiding decision-making in process design and optimization. By circumventing the need for costly and time-consuming experiments, our approach offers a pragmatic pathway towards enhanced process efficiency. Moreover, this study underscores the potential of data-driven computational methods for innovating reactor design paradigms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18444v1</guid>
      <category>physics.chem-ph</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Geremy Loacham\'in Suntaxi, Paris Papavasileiou, Eleni D. Koronaki, Dimitrios G. Giovanis, Georgios Gakis, Ioannis G. Aviziotis, Martin Kathrein, Gabriele Pozzetti, Christoph Czettl, St\'ephane P. A. Bordas, Andreas G. Boudouvis</dc:creator>
    </item>
    <item>
      <title>Improving Harmonic Analysis using Multitapering: Precise frequency estimation of stellar oscillations using the harmonic F-test</title>
      <link>https://arxiv.org/abs/2405.18509</link>
      <description>arXiv:2405.18509v1 Announce Type: cross 
Abstract: In Patil et. al 2024a, we developed a multitaper power spectrum estimation method, mtNUFFT, for analyzing time-series with quasi-regular spacing, and showed that it not only improves upon the statistical issues of the Lomb-Scargle periodogram, but also provides a factor of three speed up in some applications. In this paper, we combine mtNUFFT with the harmonic F-test to test the hypothesis that a strictly periodic signal or its harmonic (as opposed to e.g. a quasi-periodic signal) is present at a given frequency. This mtNUFFT/F-test combination shows that multitapering allows detection of periodic signals and precise estimation of their frequencies, thereby improving both power spectrum estimation and harmonic analysis. Using asteroseismic time-series data for the Kepler-91 red giant, we show that the F-test automatically picks up the harmonics of its transiting exoplanet as well as certain dipole ($l=1$) mixed modes. We use this example to highlight that we can distinguish between different types of stellar oscillations, e.g., transient (damped, stochastically-excited) and strictly periodic (undamped, heat-driven). We also illustrate the technique of dividing a time-series into chunks to further examine the transient versus periodic nature of stellar oscillations. The harmonic F-test combined with mtNUFFT is implemented in the public Python package tapify (https://github.com/aaryapatil/tapify), which opens opportunities to perform detailed investigations of periodic signals in time-domain astronomy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18509v1</guid>
      <category>astro-ph.SR</category>
      <category>astro-ph.EP</category>
      <category>astro-ph.IM</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Aarya A. Patil, Gwendolyn M. Eadie, Joshua S. Speagle, David J. Thomson</dc:creator>
    </item>
    <item>
      <title>Difference-in-Discontinuities: Estimation, Inference and Validity Tests</title>
      <link>https://arxiv.org/abs/2405.18531</link>
      <description>arXiv:2405.18531v1 Announce Type: cross 
Abstract: This paper investigates the econometric theory behind the newly developed difference-in-discontinuities design (DiDC). Despite its increasing use in applied research, there are currently limited studies of its properties. The method combines elements of regression discontinuity (RDD) and difference-in-differences (DiD) designs, allowing researchers to eliminate the effects of potential confounders at the discontinuity. We formalize the difference-in-discontinuity theory by stating the identification assumptions and proposing a nonparametric estimator, deriving its asymptotic properties and examining the scenarios in which the DiDC has desirable bias properties when compared to the standard RDD. We also provide comprehensive tests for one of the identification assumption of the DiDC. Monte Carlo simulation studies show that the estimators have good performance in finite samples. Finally, we revisit Grembi et al. (2016), that studies the effects of relaxing fiscal rules on public finance outcomes in Italian municipalities. The results show that the proposed estimator exhibits substantially smaller confidence intervals for the estimated effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18531v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pedro Picchetti, Cristine C. X. Pinto, Stephanie T. Shinoki</dc:creator>
    </item>
    <item>
      <title>Causal inference in the closed-loop: marginal structural models for sequential excursion effects</title>
      <link>https://arxiv.org/abs/2405.18597</link>
      <description>arXiv:2405.18597v1 Announce Type: cross 
Abstract: Optogenetics is widely used to study the effects of neural circuit manipulation on behavior. However, the paucity of causal inference methodological work on this topic has resulted in analysis conventions that discard information, and constrain the scientific questions that can be posed. To fill this gap, we introduce a nonparametric causal inference framework for analyzing "closed-loop" designs, which use dynamic policies that assign treatment based on covariates. In this setting, standard methods can introduce bias and occlude causal effects. Building on the sequentially randomized experiments literature in causal inference, our approach extends history-restricted marginal structural models for dynamic regimes. In practice, our framework can identify a wide range of causal effects of optogenetics on trial-by-trial behavior, such as, fast/slow-acting, dose-response, additive/antagonistic, and floor/ceiling. Importantly, it does so without requiring negative controls, and can estimate how causal effect magnitudes evolve across time points. From another view, our work extends "excursion effect" methods--popular in the mobile health literature--to enable estimation of causal contrasts for treatment sequences greater than length one, in the presence of positivity violations. We derive rigorous statistical guarantees, enabling hypothesis testing of these causal effects. We demonstrate our approach on data from a recent study of dopaminergic activity on learning, and show how our method reveals relevant effects obscured in standard analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18597v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander W. Levis, Gabriel Loewinger, Francisco Pereira</dc:creator>
    </item>
    <item>
      <title>Provable Contrastive Continual Learning</title>
      <link>https://arxiv.org/abs/2405.18756</link>
      <description>arXiv:2405.18756v1 Announce Type: cross 
Abstract: Continual learning requires learning incremental tasks with dynamic data distributions. So far, it has been observed that employing a combination of contrastive loss and distillation loss for training in continual learning yields strong performance. To the best of our knowledge, however, this contrastive continual learning framework lacks convincing theoretical explanations. In this work, we fill this gap by establishing theoretical performance guarantees, which reveal how the performance of the model is bounded by training losses of previous tasks in the contrastive continual learning framework. Our theoretical explanations further support the idea that pre-training can benefit continual learning. Inspired by our theoretical analysis of these guarantees, we propose a novel contrastive continual learning algorithm called CILA, which uses adaptive distillation coefficients for different tasks. These distillation coefficients are easily computed by the ratio between average distillation losses and average contrastive losses from previous tasks. Our method shows great improvement on standard benchmarks and achieves new state-of-the-art performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18756v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yichen Wen, Zhiquan Tan, Kaipeng Zheng, Chuanlong Xie, Weiran Huang</dc:creator>
    </item>
    <item>
      <title>Categorization of 31 computational methods to detect spatially variable genes from spatially resolved transcriptomics data</title>
      <link>https://arxiv.org/abs/2405.18779</link>
      <description>arXiv:2405.18779v1 Announce Type: cross 
Abstract: In the analysis of spatially resolved transcriptomics data, detecting spatially variable genes (SVGs) is crucial. Numerous computational methods exist, but varying SVG definitions and methodologies lead to incomparable results. We review 31 state-of-the-art methods, categorizing SVGs into three types: overall, cell-type-specific, and spatial-domain-marker SVGs. Our review explains the intuitions underlying these methods, summarizes their applications, and categorizes the hypothesis tests they use in the trade-off between generality and specificity for SVG detection. We discuss challenges in SVG detection and propose future directions for improvement. Our review offers insights for method developers and users, advocating for category-specific benchmarking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18779v1</guid>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Guanao Yan, Shuo Harper Hua, Jingyi Jessica Li</dc:creator>
    </item>
    <item>
      <title>How to Simulate Realistic Survival Data? A Simulation Study to Compare Realistic Simulation Models</title>
      <link>https://arxiv.org/abs/2308.07842</link>
      <description>arXiv:2308.07842v2 Announce Type: replace 
Abstract: In statistics, it is important to have realistic data sets available for a particular context to allow an appropriate and objective method comparison. For many use cases, benchmark data sets for method comparison are already available online. However, in most medical applications and especially for clinical trials in oncology, there is a lack of adequate benchmark data sets, as patient data can be sensitive and therefore cannot be published. A potential solution for this are simulation studies. However, it is sometimes not clear, which simulation models are suitable for generating realistic data. A challenge is that potentially unrealistic assumptions have to be made about the distributions. Our approach is to use reconstructed benchmark data sets %can be used as a basis for the simulations, which has the following advantages: the actual properties are known and more realistic data can be simulated. There are several possibilities to simulate realistic data from benchmark data sets. We investigate simulation models based upon kernel density estimation, fitted distributions, case resampling and conditional bootstrapping. In order to make recommendations on which models are best suited for a specific survival setting, we conducted a comparative simulation study. Since it is not possible to provide recommendations for all possible survival settings in a single paper, we focus on providing realistic simulation models for two-armed phase III lung cancer studies. To this end we reconstructed benchmark data sets from recent studies. We used the runtime and different accuracy measures (effect sizes and p-values) as criteria for comparison.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.07842v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Maria Thurow, Ina Dormuth, Christina Sauer, Marc Ditzhaus, Markus Pauly</dc:creator>
    </item>
    <item>
      <title>Bayesian Time-Varying Tensor Vector Autoregressive Models for Dynamic Effective Connectivity</title>
      <link>https://arxiv.org/abs/2106.14083</link>
      <description>arXiv:2106.14083v2 Announce Type: replace-cross 
Abstract: In contemporary neuroscience, a key area of interest is dynamic effective connectivity, which is crucial for understanding the dynamic interactions and causal relationships between different brain regions. Dynamic effective connectivity can provide insights into how brain network interactions are altered in neurological disorders such as dyslexia. Time-varying vector autoregressive (TV-VAR) models have been employed to draw inferences for this purpose. However, their significant computational requirements pose challenges, since the number of parameters to be estimated increases quadratically with the number of time series. In this paper, we propose a computationally efficient Bayesian time-varying VAR approach. For dealing with large-dimensional time series, the proposed framework employs a tensor decomposition for the VAR coefficient matrices at different lags. Dynamically varying connectivity patterns are captured by assuming that at any given time only a subset of components in the tensor decomposition is active. Latent binary time series select the active components at each time via an innovative and parsimonious Ising model in the time-domain. Furthermore, we propose parsity-inducing priors to achieve global-local shrinkage of the VAR coefficients, determine automatically the rank of the tensor decomposition and guide the selection of the lags of the auto-regression. We show the performances of our model formulation via simulation studies and data from a real fMRI study involving a book reading experiment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2106.14083v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wei Zhang, Ivor Cribben, sonia Petrone, Michele Guindani</dc:creator>
    </item>
    <item>
      <title>Improving Power Spectrum Estimation using Multitapering: Efficient asteroseismic analyses for understanding stars, the Milky Way, and beyond</title>
      <link>https://arxiv.org/abs/2209.15027</link>
      <description>arXiv:2209.15027v2 Announce Type: replace-cross 
Abstract: Asteroseismic time-series data have imprints of stellar oscillation modes, whose detection and characterization through time-series analysis allows us to probe stellar interior physics. Such analyses usually occur in the Fourier domain by computing the Lomb-Scargle (LS) periodogram, an estimator of the power spectrum underlying unevenly-sampled time-series data. However, the LS periodogram suffers from the statistical problems of (1) inconsistency (or noise) and (2) bias due to high spectral leakage. Here, we develop a multitaper power spectrum estimator using the Non-Uniform Fast Fourier Transform (mtNUFFT) to tackle the inconsistency and bias problems of the LS periodogram. Using a simulated light curve, we show that the mtNUFFT power spectrum estimate of solar-like oscillations has lower variance and bias than the LS estimate. We also apply our method to the Kepler-91 red giant, and combine it with PBjam peakbagging to obtain mode parameters and a derived age estimate of $3.97 \pm 0.52$ Gyr. PBjam allows the improvement of age precision relative to the $4.27 \pm 0.75$ Gyr APOKASC-2 (uncorrected) estimate, whereas partnering mtNUFFT with PBjam speeds up peakbagging thrice as much as LS. This increase in efficiency has promising implications for Galactic archaeology, in addition to stellar structure and evolution studies. Our new method generally applies to time-domain astronomy and is implemented in the public Python package tapify, available at https://github.com/aaryapatil/tapify.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.15027v2</guid>
      <category>astro-ph.IM</category>
      <category>astro-ph.EP</category>
      <category>astro-ph.SR</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Aarya A. Patil, Gwendolyn M. Eadie, Joshua S. Speagle, David J. Thomson</dc:creator>
    </item>
    <item>
      <title>The Multi-Range Theory of Translation Quality Measurement: MQM scoring models and Statistical Quality Control</title>
      <link>https://arxiv.org/abs/2405.16969</link>
      <description>arXiv:2405.16969v2 Announce Type: replace-cross 
Abstract: The year 2024 marks the 10th anniversary of the Multidimensional Quality Metrics (MQM) framework for analytic translation quality evaluation. The MQM error typology has been widely used by practitioners in the translation and localization industry and has served as the basis for many derivative projects. The annual Conference on Machine Translation (WMT) shared tasks on both human and automatic translation quality evaluations used the MQM error typology.
  The metric stands on two pillars: error typology and the scoring model. The scoring model calculates the quality score from annotation data, detailing how to convert error type and severity counts into numeric scores to determine if the content meets specifications. Previously, only the raw scoring model had been published. This April, the MQM Council published the Linear Calibrated Scoring Model, officially presented herein, along with the Non-Linear Scoring Model, which had not been published before.
  This paper details the latest MQM developments and presents a universal approach to translation quality measurement across three sample size ranges. It also explains why Statistical Quality Control should be used for very small sample sizes, starting from a single sentence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16969v2</guid>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Arle Lommel, Serge Gladkoff, Alan Melby, Sue Ellen Wright, Ingemar Strandvik, Katerina Gasova, Angelika Vaasa, Andy Benzo, Romina Marazzato Sparano, Monica Foresi, Johani Innis, Lifeng Han, Goran Nenadic</dc:creator>
    </item>
  </channel>
</rss>
