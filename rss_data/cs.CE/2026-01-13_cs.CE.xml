<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 14 Jan 2026 02:37:48 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Style-constrained inverse design of microstructures with tailored mechanical properties using unconditional diffusion models</title>
      <link>https://arxiv.org/abs/2601.06469</link>
      <description>arXiv:2601.06469v1 Announce Type: new 
Abstract: Deep generative models, particularly denoising diffusion models, have achieved remarkable success in high-fidelity generation of architected microstructures with desired properties and styles. Nevertheless, these recent methods typically rely on conditional training mechanisms and demand substantial computational effort to prepare the labeled training dataset, which makes them inflexible since any change in the governing equations or boundary conditions requires a complete retraining process. In this study, we propose a new inverse design framework that integrates unconditional denoising diffusion models with differentiable programming techniques for architected microstructure generation. Our approach eliminates the need for expensive labeled dataset preparation and retraining for different problem settings. By reinterpreting the noise input to the diffusion model as an optimizable design variable, we formulate the design task as an optimization problem over the noise input, enabling control over the reverse denoising trajectory to guide the generated microstructure toward the desired mechanical properties while preserving the stylistic constraints encoded in the training dataset. A unified differentiation pipeline via vector-Jacobian product concatenations is developed to enable end-to-end gradient evaluation through backpropagation. Several numerical examples, ranging from the design of microstructures with specified homogenized properties to those with targeted hyperelastic and elasto-plastic behaviors, showcase the effectiveness of the framework and its potential for advanced design tasks involving diverse performance and style requirements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06469v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weipeng Xu, Ziyuan Xie, Haoju Lin, Xinyu Wang, Guangjin Mou, Tianju Xue</dc:creator>
    </item>
    <item>
      <title>Physics-constrained Gaussian Processes for Predicting Shockwave Hugoniot Curves</title>
      <link>https://arxiv.org/abs/2601.06655</link>
      <description>arXiv:2601.06655v1 Announce Type: new 
Abstract: A physics-constrained Gaussian Process regression framework is developed for predicting shocked material states along the Hugoniot curve using data from a small number of shockwave simulations. The proposed Gaussian process employs a probabilistic Taylor series expansion in conjunction with the Rankine-Hugoniot jump conditions between the various shocked material states to construct a thermodynamically consistent covariance function. This leads to the formulation of an optimization problem over a small number of interpretable hyperparameters and enables the identification of regime transitions, from a leading elastic wave to trailing plastic and phase transformation waves. This work is motivated by the need to investigate shock-driven material response for materials discovery and for offering mechanistic insights in regimes where experimental characterizations and simulations are costly. The proposed methodology relies on large-scale molecular dynamics which are an accurate but expensive computational alternative to experiments. Under these constraints, the proposed methodology establishes Hugoniot curves from a limited number of molecular dynamics simulations. We consider silicon carbide as a representative material and atomic-level simulations are performed using a reverse ballistic approach together with appropriate interatomic potentials. The framework reproduces the Hugoniot curve with satisfactory accuracy while also quantifying the uncertainty in the predictions using the Gaussian Process posterior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06655v1</guid>
      <category>cs.CE</category>
      <category>physics.comp-ph</category>
      <category>stat.ML</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>George D. Pasparakis, Himanshu Sharma, Rushik Desai, Chunyu Li, Alejandro Strachan, Lori Graham-Brady, Michael D. Shields</dc:creator>
    </item>
    <item>
      <title>AdaField: Generalizable Surface Pressure Modeling with Physics-Informed Pre-training and Flow-Conditioned Adaptation</title>
      <link>https://arxiv.org/abs/2601.07139</link>
      <description>arXiv:2601.07139v1 Announce Type: new 
Abstract: The surface pressure field of transportation systems, including cars, trains, and aircraft, is critical for aerodynamic analysis and design. In recent years, deep neural networks have emerged as promising and efficient methods for modeling surface pressure field, being alternatives to computationally expensive CFD simulations. Currently, large-scale public datasets are available for domains such as automotive aerodynamics. However, in many specialized areas, such as high-speed trains, data scarcity remains a fundamental challenge in aerodynamic modeling, severely limiting the effectiveness of standard neural network approaches. To address this limitation, we propose the Adaptive Field Learning Framework (AdaField), which pre-trains the model on public large-scale datasets to improve generalization in sub-domains with limited data. AdaField comprises two key components. First, we design the Semantic Aggregation Point Transformer (SAPT) as a high-performance backbone that efficiently handles large-scale point clouds for surface pressure prediction. Second, regarding the substantial differences in flow conditions and geometric scales across different aerodynamic subdomains, we propose Flow-Conditioned Adapter (FCA) and Physics-Informed Data Augmentation (PIDA). FCA enables the model to flexibly adapt to different flow conditions with a small set of trainable parameters, while PIDA expands the training data distribution to better cover variations in object scale and velocity. Our experiments show that AdaField achieves SOTA performance on the DrivAerNet++ dataset and can be effectively transferred to train and aircraft scenarios with minimal fine-tuning. These results highlight AdaField's potential as a generalizable and transferable solution for surface pressure field modeling, supporting efficient aerodynamic design across a wide range of transportation systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07139v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junhong Zou, Wei Qiu, Zhenxu Sun, Xiaomei Zhang, Zhaoxiang Zhang, Xiangyu Zhu</dc:creator>
    </item>
    <item>
      <title>An adjoint method for training data-driven reduced-order models</title>
      <link>https://arxiv.org/abs/2601.07579</link>
      <description>arXiv:2601.07579v1 Announce Type: new 
Abstract: Reduced-order modeling lies at the interface of numerical analysis and data-driven scientific computing, providing principled ways to compress high-fidelity simulations in science and engineering. We propose a training framework that couples a continuous-time form of operator inference with the adjoint-state method to obtain robust data-driven reduced-order models. This method minimizes a trajectory-based loss between reduced-order solutions and projected snapshot data, which removes the need to estimate time derivatives from noisy measurements and provides intrinsic temporal regularization through time integration. We derive the corresponding continuous adjoint equations to compute gradients efficiently and implement a gradient based optimizer to update the reduced model parameters. Each iteration only requires one forward reduced order solve and one adjoint solve, followed by inexpensive gradient assembly, making the method attractive for large-scale simulations. We validate the proposed method on three partial differential equations: viscous Burgers' equation, the two-dimensional Fisher-KPP equation, and an advection-diffusion equation. We perform systematic comparisons against standard operator inference under two perturbation regimes, namely reduced temporal snapshot density and additive Gaussian noise. For clean data, both approaches deliver similar accuracy, but in situations with sparse sampling and noise, the proposed adjoint-based training provides better accuracy and enhanced roll-out stability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07579v1</guid>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Donglin Liu, Francisco Garc\'ia Atienza, Mengwu Guo</dc:creator>
    </item>
    <item>
      <title>AI-Assisted Authoring for Transparent, Data-Driven Documents</title>
      <link>https://arxiv.org/abs/2601.06027</link>
      <description>arXiv:2601.06027v1 Announce Type: cross 
Abstract: We introduce _transparent documents_, interactive web-based scholarly articles which allow readers to explore the relationship to the underlying data by hovering over fragments of text, and present an LLM-based tool for authoring transparent documents, building on recent developments in data provenance for general-purpose programming languages. As a target platform, our implementation uses Fluid, an open source programming language with a provenance-tracking runtime. Our agent-based tool supports a human author during the creation of transparent documents, identifying fragments of text which can be computed from data, such as numerical values selected from records or computed by aggregations like sum and mean, comparatives and superlatives like _better than_ and _largest_, trend-adjectives like _growing_, and similar quantitative or semi-quantitative phrases, and then attempts to synthesise a suitable Fluid query over the data which generates the target string. The resulting expression is inserted into the article's web page, turning the static text fragment into an interactable data-driven element able to reveal the data that underwrites the natural language claim. We evaluate our approach on a subset of SciGen, an open source dataset consisting of tables from scientific articles and their corresponding descriptions, which we extend with hand-generated counterfactual test cases to evaluate how well machine-generated expressions generalise. Our results show that gpt4o is often able to synthesise compound expressions extensionally compatible with our gold solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06027v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.IR</category>
      <category>cs.PL</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alfonso Piscitelli, Cristina David, Mattia De Rosa, Ali Mohammed, Federico Nanni, Jacob Pake, Roly Perera, Jessy Sodimu, Chenyiqiu Zheng</dc:creator>
    </item>
    <item>
      <title>Efficient GPU-computing simulation platform JAX-PF for differentiable phase field model</title>
      <link>https://arxiv.org/abs/2601.06079</link>
      <description>arXiv:2601.06079v1 Announce Type: cross 
Abstract: We present JAX-PF, an open-source, GPU-accelerated, and differentiable Phase Field (PF) software package, supporting both explicit and implicit time stepping schemes. Leveraging the modern computing architecture JAX, JAX-PF achieves high performance through array programming and GPU acceleration, delivering ~5x speedup over PRISMS-PF with MPI (24 CPU cores) for systems with ~4.19 million degrees of freedom using explicit schemes, and scaling efficiently with implicit schemes for large-size problems. Furthermore, a key feature of JAX-PF is automatic differentiation (AD), eliminating manual derivations of free-energy functionals and Jacobians. Beyond forward simulations, JAX-PF demonstrates its potential in inverse design by providing sensitivities for gradient-based optimization. We demonstrate, for the first time, the calibration of PF material parameters using AD-based sensitivities, highlighting its capability for high-dimensional inverse problems. By combining efficiency, flexibility, and full differentiability, JAX-PF offers a fast, practical, and integrated tool for forward simulation and inverse design, advancing co-designing of material and manufacturing processes and supporting the goals of the Materials Genome Initiative.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06079v1</guid>
      <category>physics.comp-ph</category>
      <category>cs.CE</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Fanglei Hu, Jiachen Guo, Stephen Niezgoda, Wing Kam Liu, Jian Cao</dc:creator>
    </item>
    <item>
      <title>Evaluating Impacts of Traffic Regulations in Complex Mobility Systems Using Scenario-Based Simulations</title>
      <link>https://arxiv.org/abs/2601.07735</link>
      <description>arXiv:2601.07735v1 Announce Type: cross 
Abstract: Urban traffic regulation policies are increasingly used to address congestion, emissions, and accessibility in cities, yet their impacts are difficult to assess due to the socio-technical complexity of urban mobility systems. Recent advances in data availability and computational power enable new forms of model-driven, simulation-based decision support for transportation policy design. This paper proposes a novel simulation paradigm for the ex-ante evaluation of both direct impacts (e.g., traffic conditions, modal shift, emissions) and indirect impacts spanning transportation-related effects, social equity, and economic accessibility. The approach integrates a multi-layer urban mobility model combining a physical layer of networks, flows, and emissions with a social layer capturing behavioral responses and adaptation to policy changes. Real-world data are used to instantiate the current "as-is" scenario, while policy alternatives and behavioral assumptions are encoded as model parameters to generate multiple "what-if" scenarios. The framework supports systematic comparison across scenarios by analyzing variations in simulated outcomes induced by policy interventions. The proposed approach is illustrated through a case study aims to assess the impacts of the introduction of broad urban traffic restriction schemes. Results demonstrate the framework's ability to explore alternative regulatory designs and user responses, supporting informed and anticipatory evaluation of urban traffic policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07735v1</guid>
      <category>cs.CY</category>
      <category>cs.CE</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arianna Burzacchi, Marco Pistore</dc:creator>
    </item>
    <item>
      <title>A Copula-based variational autoencoder for uncertainty quantification in inverse problems: application to damage identification in an offshore wind turbine</title>
      <link>https://arxiv.org/abs/2510.02013</link>
      <description>arXiv:2510.02013v3 Announce Type: replace 
Abstract: Structural Health Monitoring of Floating Offshore Wind Turbines (FOWTs) is critical for ensuring operational safety and efficiency. However, identifying damage in components like mooring systems from limited sensor data poses a challenging inverse problem, often characterized by multimodal solutions where various damage states could explain the observed response. To overcome it, we propose a Variational Autoencoder (VAE) architecture, where the encoder approximates the inverse operator, while the decoder approximates the forward. The posterior distribution of the latent space variables is probabilistically modeled, describing the uncertainties in the estimates. This work tackles the limitations of conventional Gaussian Mixtures used within VAEs, which can be either too restrictive or computationally prohibitive for high-dimensional spaces. We propose a novel Copula-based VAE architecture that decouples the marginal distribution of the variables from their dependence structure, offering a flexible method for representing complex, correlated posterior distributions. We provide a comprehensive comparison of three different approaches for approximating the posterior: a Gaussian Mixture with a diagonal covariance matrix, a Gaussian Mixture with a full covariance matrix, and a Gaussian Copula. Our analysis, conducted on a high-fidelity synthetic dataset, demonstrates that the Copula VAE offers a promising and tractable solution in high-dimensional spaces. Although the present work remains in the two-dimensional space, the results suggest efficient scalability to higher dimensions. It achieves superior performance with significantly fewer parameters than the Gaussian Mixture alternatives, whose parametrization grows prohibitively with the dimensionality. The results underscore the potential of Copula-based VAEs as a tool for uncertainty-aware damage identification in FOWT mooring systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02013v3</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ana Fernandez-Navamuel, Martin Alberto Diaz Viera, Matteo Croci</dc:creator>
    </item>
    <item>
      <title>SoK: Market Microstructure for Decentralized Prediction Markets (DePMs)</title>
      <link>https://arxiv.org/abs/2510.15612</link>
      <description>arXiv:2510.15612v2 Announce Type: replace 
Abstract: Decentralized prediction markets (DePMs) allow open participation in event-based wagering without fully relying on centralized intermediaries. We review the history of DePMs which date back to 2011 and includes hundreds of proposals. Perhaps surprising, modern DePMs like Polymarket deviate materially from earlier designs like Truthcoin and Augur v1. We use our review to present a modular workflow comprising eight stages: underlying infrastructure, market topic, share structure and pricing, market initialization, trading, market resolution, settlement, and archiving. For each module, we enumerate the design variants, analyzing trade-offs around decentralization, expressiveness, and manipulation resistance. We also identify open problems for researchers interested in this ecosystem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15612v2</guid>
      <category>cs.CE</category>
      <category>cs.CR</category>
      <category>q-fin.TR</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nahid Rahman, Joseph Al-Chami, Jeremy Clark</dc:creator>
    </item>
    <item>
      <title>MMFCTUB: Multi-Modal Financial Credit Table Understanding Benchmark</title>
      <link>https://arxiv.org/abs/2601.04643</link>
      <description>arXiv:2601.04643v2 Announce Type: replace 
Abstract: The advent of multi-modal language models (MLLMs) has spurred research into their application across various table understanding tasks. However, their performance in credit table understanding (CTU) for financial credit review remains largely unexplored due to the following barriers: low data consistency, high annotation costs stemming from domain-specific knowledge and complex calculations, and evaluation paradigm gaps between benchmark and real-world scenarios. To address these challenges, we introduce MMFCTUB (Multi-Modal Financial Credit Table Understanding Benchmark), a practical benchmark, encompassing more than 7,600 high quality CTU samples across 5 table types. MMFCTUB employ a minimally supervised pipeline that adheres to inter-table constraints and maintains data distributions consistency. The benchmark leverages capacity-driven questions and mask-and-recovery strategy to evaluate models' cross-table structure perception, domain knowledge utilization, and numerical calculation capabilities. Utilizing MMFCTUB, we conduct comprehensive evaluations of both proprietary and open-source MLLMs, revealing their strengths and limitations in CTU tasks. MMFCTUB serves as a valuable resource for the research community, facilitating rigorous evaluation of MLLMs in the domain of CTU.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.04643v2</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cui Yakun, Yanting Zhang, Zhu Lei, Jian Xie, Zhizhuo Kou, Hang Du, Zhenghao Zhu, Sirui Han</dc:creator>
    </item>
    <item>
      <title>KARMA: Leveraging Multi-Agent LLMs for Automated Knowledge Graph Enrichment</title>
      <link>https://arxiv.org/abs/2502.06472</link>
      <description>arXiv:2502.06472v2 Announce Type: replace-cross 
Abstract: Maintaining comprehensive and up-to-date knowledge graphs (KGs) is critical for modern AI systems, but manual curation struggles to scale with the rapid growth of scientific literature. This paper presents KARMA, a novel framework employing multi-agent large language models (LLMs) to automate KG enrichment through structured analysis of unstructured text. Our approach employs nine collaborative agents, spanning entity discovery, relation extraction, schema alignment, and conflict resolution that iteratively parse documents, verify extracted knowledge, and integrate it into existing graph structures while adhering to domain-specific schema. Experiments on 1,200 PubMed articles from three different domains demonstrate the effectiveness of KARMA in knowledge graph enrichment, with the identification of up to 38,230 new entities while achieving 83.1\% LLM-verified correctness and reducing conflict edges by 18.6\% through multi-layer assessments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06472v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.DL</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Spotlight paper of NeurIPS 2025</arxiv:journal_reference>
      <dc:creator>Yuxing Lu, Wei Wu, Xukai Zhao, Rui Peng, Jinzhuo Wang</dc:creator>
    </item>
    <item>
      <title>Quantum Dynamics Simulation of the Advection-Diffusion Equation</title>
      <link>https://arxiv.org/abs/2503.13729</link>
      <description>arXiv:2503.13729v2 Announce Type: replace-cross 
Abstract: The advection-diffusion equation is simulated on a superconducting quantum computer via several quantum algorithms. Three formulations are considered: (1) Trotterization, (2) variational quantum time evolution (VarQTE), and (3) adaptive variational quantum dynamics simulation (AVQDS). These schemes were originally developed for the Hamiltonian simulation of many-body quantum systems. The finite-difference discretized operator of the transport equation is formulated as a Hamiltonian and solved without the need for ancillary qubits. Computations are conducted on a quantum simulator (IBM Qiskit Aer) and an actual quantum hardware (IBM Fez). The former emulates the latter without the noise. The predicted results are compared with direct numerical simulation (DNS) data with infidelities of the order $10^{-5}$. In the quantum simulator, Trotterization is observed to have the lowest infidelity and is suitable for fault-tolerant computation. The AVQDS algorithm requires the lowest gate count and the lowest circuit depth. The VarQTE algorithm is the next best in terms of gate counts, but the number of its optimization variables is directly proportional to the number of qubits. Due to current hardware limitations, Trotterization cannot be implemented, as it has an overwhelming large number of operations. Meanwhile, AVQDS and VarQTE can be executed, but suffer from large errors due to significant hardware noise. These algorithms present a new paradigm for computational transport phenomena on quantum computers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13729v2</guid>
      <category>quant-ph</category>
      <category>cs.CE</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1103/ndc3-bdwt</arxiv:DOI>
      <arxiv:journal_reference>Physical Review Research 7, 043318 (2025)</arxiv:journal_reference>
      <dc:creator>Hirad Alipanah, Feng Zhang, Yongxin Yao, Richard Thompson, Nam Nguyen, Junyu Liu, Peyman Givi, Brian J. McDermott, Juan Jos\'e Mendoza-Arenas</dc:creator>
    </item>
    <item>
      <title>Black-Scholes Model, comparison between Analytical Solution and Numerical Analysis</title>
      <link>https://arxiv.org/abs/2510.27277</link>
      <description>arXiv:2510.27277v2 Announce Type: replace-cross 
Abstract: The main purpose of this article is to give a general overview and understanding of the first widely used option-pricing model, the Black-Scholes model. The history and context are presented, with the usefulness and implications in the economics world. A brief review of fundamental calculus concepts is introduced to derive and solve the model. The equation is then resolved using both an analytical (variable separation) and a numerical method (finite differences). Conclusions are drawn in order to understand how Black-Scholes is employed nowadays. At the end a handy appendix (A) is written with some economics notions to ease the reader's comprehension of the paper; furthermore a second appendix (B) is given with some code scripts, to allow the reader to put in practice some concepts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27277v2</guid>
      <category>q-fin.PR</category>
      <category>cs.CE</category>
      <category>q-fin.CP</category>
      <category>q-fin.RM</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Francesco Romaggi</dc:creator>
    </item>
  </channel>
</rss>
