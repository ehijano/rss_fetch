<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 09 Oct 2025 01:43:40 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Geographically Weighted Regression for Air Quality Low-Cost Sensor Calibration</title>
      <link>https://arxiv.org/abs/2510.05646</link>
      <description>arXiv:2510.05646v1 Announce Type: new 
Abstract: This article focuses on the use of Geographically Weighted Regression (GWR) method to correct air quality low-cost sensors measurements. Those sensors are of major interest in the current era of high-resolution air quality monitoring at urban scale, but require calibration using reference analyzers. The results for NO2 are provided along with comments on the estimated GWR model and the spatial content of the estimated coefficients. The study has been carried out using the publicly available SensEURCity dataset in Antwerp, which is especially relevant since it includes 9 reference stations and 34 micro-sensors collocated and deployed within the city.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05646v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jean-Michel Poggi (LMO, UPCit\'e), Bruno Portier (LMI), Emma Thulliez (LMI)</dc:creator>
    </item>
    <item>
      <title>Copula-Based Clustering of Financial Time Series via Evidence Accumulation</title>
      <link>https://arxiv.org/abs/2510.05960</link>
      <description>arXiv:2510.05960v1 Announce Type: new 
Abstract: Understanding the dependence structure of asset returns is fundamental in risk assessment and is particularly relevant in a portfolio diversification strategy. We propose a clustering approach where evidence accumulated in a multiplicity of classifications is achieved using classical hierarchical procedures and multiple copula-based dissimilarity measures. Assets that are grouped in the same cluster are such that their stochastic behavior is similar during risky scenarios, and riskaverse investors could exploit this information to build a risk-diversified portfolio. An empirical demonstration of such a strategy is presented by using data from the EURO STOXX 50 index.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05960v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Andrea Mecchina, Roberta Pappad\`a, Nicola Torelli</dc:creator>
    </item>
    <item>
      <title>Measuring Data Quality for Project Lighthouse</title>
      <link>https://arxiv.org/abs/2510.06121</link>
      <description>arXiv:2510.06121v1 Announce Type: new 
Abstract: In this paper, we first situate the challenges for measuring data quality under Project Lighthouse in the broader academic context. We then discuss in detail the three core data quality metrics we use for measurement--two of which extend prior academic work. Using those data quality metrics as examples, we propose a framework, based on machine learning classification, for empirically justifying the choice of data quality metrics and their associated minimum thresholds. Finally we outline how these methods enable us to rigorously meet the principle of data minimization when analyzing potential experience gaps under Project Lighthouse, which we term quantitative data minimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06121v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adam Bloomston, Elizabeth Burke, Megan Cacace, Anne Diaz, Wren Dougherty, Matthew Gonzalez, Remington Gregg, Yeliz G\"ung\"or, Bryce Hayes, Eeway Hsu, Oron Israeli, Heesoo Kim, Sara Kwasnick, Joanne Lacsina, Demma Rosa Rodriguez, Adam Schiller, Whitney Schumacher, Jessica Simon, Maggie Tang, Skyler Wharton, Marilyn Wilcken</dc:creator>
    </item>
    <item>
      <title>Rapid calibration of atrial electrophysiology models using Gaussian process emulators in the ensemble Kalman filter</title>
      <link>https://arxiv.org/abs/2510.06191</link>
      <description>arXiv:2510.06191v1 Announce Type: new 
Abstract: Atrial fibrillation (AF) is a common cardiac arrhythmia characterised by disordered electrical activity in the atria. The standard treatment is catheter ablation, which is invasive and irreversible. Recent advances in computational electrophysiology offer the potential for patient-specific models, often referred to as digital twins, that can be used to guide clinical decisions. To be of practical value, we must be able to rapidly calibrate physics-based models using routine clinical measurements. We pose this calibration task as a static inverse problem, where the goal is to infer tissue-level electrophysiological parameters from the available observations. To make this tractable, we replace the expensive forward model with Gaussian process emulators (GPEs), and propose a novel adaptation of the ensemble Kalman filter (EnKF) for static non-linear inverse problems. The approach yields parameter samples that can be interpreted as coming from the best Gaussian approximation of the posterior distribution. We compare our results with those obtained using Markov chain Monte Carlo (MCMC) sampling and demonstrate the potential of the approach to enable near-real-time patient-specific calibration, a key step towards predicting outcomes of AF treatment within clinical timescales. The approach is readily applicable to a wide range of static inverse problems in science and engineering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06191v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mariya Mamajiwala, Cesare Corrado, Chris Lanyon, Steven A. Niederer, Richard D. Wilkinson, Richard H. Clayton</dc:creator>
    </item>
    <item>
      <title>Geographical inequalities in mortality by age and gender in Italy, 2002-2019: insights from a spatial extension of the Lee-Carter model</title>
      <link>https://arxiv.org/abs/2510.06210</link>
      <description>arXiv:2510.06210v1 Announce Type: new 
Abstract: Italy reports some of the lowest levels of mortality in the developed world. Recent evidence, however, suggests that even in low mortality countries improvements may be slowing and regional inequalities widening. This study contributes new empirical evidence to the debate by analysing mortality data by single year of age for males and females across 107 provinces in Italy from 2002 to 2019. We extend the widely used Lee Carter model to include spatially varying age specific effects, and further specify it to capture space age time interactions. The model is estimated in a Bayesian framework using the inlabru package, which builds on INLA (Integrated Nested Laplace Approximation) for non linear models and facilitates the use of smoothing priors. This approach borrows strength across provinces and years, mitigating random fluctuations in small area death counts. Results demonstrate the value of such a granular approach, highlighting the existence of an uneven geography of mortality despite overall national improvements. Mortality disadvantage is concentrated in parts of the Centre South and North West, while the Centre North and North East fare relatively better. These geographical differences have widened since 2010, with clear age and gender specific patterns, being more pronounced at younger adult ages for men and at older adult ages for women. Future work may involve refining the analysis to mortality by cause of death or socioeconomic status, informing more targeted public health policies to address mortality disparities across Italy's provinces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06210v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesca Fiori, Andrea Riebler, Sara Martino</dc:creator>
    </item>
    <item>
      <title>Aneurysm Growth Time Series Reconstruction Using Physics-informed Autoencoder</title>
      <link>https://arxiv.org/abs/2510.05183</link>
      <description>arXiv:2510.05183v1 Announce Type: cross 
Abstract: Arterial aneurysm (Fig.1) is a bulb-shape local expansion of human arteries, the rupture of which is a leading cause of morbidity and mortality in US. Therefore, the prediction of arterial aneurysm rupture is of great significance for aneurysm management and treatment selection. The prediction of aneurysm rupture depends on the analysis of the time series of aneurysm growth history. However, due to the long time scale of aneurysm growth, the time series of aneurysm growth is not always accessible. We here proposed a method to reconstruct the aneurysm growth time series directly from patient parameters. The prediction is based on data pairs of [patient parameters, patient aneurysm growth time history]. To obtain the mapping from patient parameters to patient aneurysm growth time history, we first apply autoencoder to obtain a compact representation of the time series for each patient. Then a mapping is learned from patient parameters to the corresponding compact representation of time series via a five-layer neural network. Moving average and convolutional output layer are implemented to explicitly taking account the time dependency of the time series.
  Apart from that, we also propose to use prior knowledge about the mechanism of aneurysm growth to improve the time series reconstruction results. The prior physics-based knowledge is incorporated as constraints for the optimization problem associated with autoencoder. The model can handle both algebraic and differential constraints. Our results show that including physical model information about the data will not significantly improve the time series reconstruction results if the training data is error-free. However, in the case of training data with noise and bias error, incorporating physical model constraints can significantly improve the predicted time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05183v1</guid>
      <category>q-bio.QM</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiacheng Wu</dc:creator>
    </item>
    <item>
      <title>Efficient Prediction of Pass@k Scaling in Large Language Models</title>
      <link>https://arxiv.org/abs/2510.05197</link>
      <description>arXiv:2510.05197v1 Announce Type: cross 
Abstract: Assessing the capabilities and risks of frontier AI systems is a critical area of research, and recent work has shown that repeated sampling from models can dramatically increase both. For instance, repeated sampling has been shown to increase their capabilities, such as solving difficult math and coding problems, but it has also been shown to increase their potential for harm, such as being jailbroken. Such results raise a crucial question for both capability and safety forecasting: how can one accurately predict a model's behavior when scaled to a massive number of attempts, given a vastly smaller sampling budget? This question is directly relevant to model providers, who serve hundreds of millions of users daily, and to governmental regulators, who seek to prevent harms. To answer this questions, we make three contributions. First, we find that standard methods for fitting these laws suffer from statistical shortcomings that hinder predictive accuracy, especially in data-limited scenarios. Second, we remedy these shortcomings by introducing a robust estimation framework, which uses a beta-binomial distribution to generate more accurate predictions from limited data. Third, we propose a dynamic sampling strategy that allocates a greater budget to harder problems. Combined, these innovations enable more reliable prediction of rare risks and capabilities at a fraction of the computational cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05197v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Kazdan, Rylan Schaeffer, Youssef Allouah, Colin Sullivan, Kyssen Yu, Noam Levi, Sanmi Koyejo</dc:creator>
    </item>
    <item>
      <title>A new composite Mann-Whitney test for two-sample survival comparisons with right-censored data</title>
      <link>https://arxiv.org/abs/2510.05353</link>
      <description>arXiv:2510.05353v2 Announce Type: cross 
Abstract: A fundamental challenge in comparing two survival distributions with right censored data is the selection of an appropriate nonparametric test, as the power of standard tests like the Log rank and Wilcoxon is highly dependent on the often unknown nature of the alternative hypothesis. This paper introduces a new, distribution free two sample test designed to overcome this limitation. The proposed method is based on a strategic decomposition of the data into uncensored and censored subsets, from which a composite test statistic is constructed as the sum of two independent Mann Whitney statistics. This design allows the test to automatically and inherently adapt to various patterns of difference including early, late, and crossing hazards without requiring pre specified parameters, pre testing, or complex weighting schemes. An extensive Monte Carlo simulation study demonstrates that the proposed test robustly maintains the nominal Type I error rate. Crucially, its power is highly competitive with the optimal traditional tests in standard scenarios and superior in complex settings with crossing survival curves, while also exhibiting remarkable robustness to high levels of censoring. The test power effectively approximates the maximum power achievable by either the Log rank or Wilcoxon tests across a wide range of alternatives, offering a powerful, versatile, and computationally simple tool for survival analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05353v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abid Hussain, Touqeer Ahmad</dc:creator>
    </item>
    <item>
      <title>Decade-long Emission Forecasting with an Ensemble Model in Taiwan</title>
      <link>https://arxiv.org/abs/2510.05548</link>
      <description>arXiv:2510.05548v1 Announce Type: cross 
Abstract: Taiwan's high population and heavy dependence on fossil fuels have led to severe air pollution, with the most prevalent greenhouse gas being carbon dioxide (CO2). There-fore, this study presents a reproducible and comprehensive case study comparing 21 of the most commonly employed time series models in forecasting emissions, analyzing both univariate and multivariate approaches. Among these, Feedforward Neural Network (FFNN), Support Vector Machine (SVM), and Random Forest Regressor (RFR) achieved the best performances. To further enhance robustness, the top performers were integrated with Linear Regression through a custom stacked generalization en-semble technique. Our proposed ensemble model achieved an SMAPE of 1.407 with no signs of overfitting. Finally, this research provides an accurate decade-long emission projection that will assist policymakers in making more data-driven decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05548v1</guid>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gordon Hung, Salinna Abdullah</dc:creator>
    </item>
    <item>
      <title>Domain-Shift-Aware Conformal Prediction for Large Language Models</title>
      <link>https://arxiv.org/abs/2510.05566</link>
      <description>arXiv:2510.05566v1 Announce Type: cross 
Abstract: Large language models have achieved impressive performance across diverse tasks. However, their tendency to produce overconfident and factually incorrect outputs, known as hallucinations, poses risks in real world applications. Conformal prediction provides finite-sample, distribution-free coverage guarantees, but standard conformal prediction breaks down under domain shift, often leading to under-coverage and unreliable prediction sets. We propose a new framework called Domain-Shift-Aware Conformal Prediction (DS-CP). Our framework adapts conformal prediction to large language models under domain shift, by systematically reweighting calibration samples based on their proximity to the test prompt, thereby preserving validity while enhancing adaptivity. Our theoretical analysis and experiments on the MMLU benchmark demonstrate that the proposed method delivers more reliable coverage than standard conformal prediction, especially under substantial distribution shifts, while maintaining efficiency. This provides a practical step toward trustworthy uncertainty quantification for large language models in real-world deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05566v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhexiao Lin, Yuanyuan Li, Neeraj Sarna, Yuanyuan Gao, Michael von Gablenz</dc:creator>
    </item>
    <item>
      <title>Automated Gating for Flow Cytometry Data Using a Kernel-Smoothed EM Algorithm</title>
      <link>https://arxiv.org/abs/2510.06051</link>
      <description>arXiv:2510.06051v1 Announce Type: cross 
Abstract: Phytoplankton are microscopic algae responsible for roughly half of the world's photosynthesis that play a critical role in global carbon cycles and oxygen production, and measuring the abundance of their subtypes across a wide range of spatiotemporal scales is of great relevance to oceanography. High-frequency flow cytometry is a powerful technique in which oceanographers at sea can rapidly record the optical properties of tens of thousands of individual phytoplankton cells every few minutes. Identifying distinct subpopulations within these vast datasets (a process known as "gating") remains a major challenge and has largely been performed manually so far. In this paper, we introduce a fast, automated gating method, which accurately identifies phytoplankton populations by fitting a time-evolving mixture of Gaussians model using an expectation-maximization-like algorithm with kernel smoothing. We use simulated data to demonstrate the validity and robustness of this approach, and use oceanographic cruise data to highlight the method's ability to not only replicate but surpass expert manual gating. Finally, we provide the flowkernel R package, written in literate programming, that implements the algorithm efficiently.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06051v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Farhad de Sousa, Fran\c{c}ois Ribalet, Jacob Bien</dc:creator>
    </item>
    <item>
      <title>Tensor time series change-point detection in cryptocurrency network data</title>
      <link>https://arxiv.org/abs/2510.06211</link>
      <description>arXiv:2510.06211v1 Announce Type: cross 
Abstract: Financial fraud has been growing exponentially in recent years. The rise of cryptocurrencies as an investment asset has simultaneously seen a parallel growth in cryptocurrency scams. To detect possible cryptocurrency fraud, and in particular market manipulation, previous research focused on the detection of changes in the network of trades; however, market manipulators are now trading across multiple cryptocurrency platforms, making their detection more difficult. Hence, it is important to consider the identification of changes across several trading networks or a `network of networks' over time. To this end, in this article, we propose a new change-point detection method in the network structure of tensor-variate data. This new method, labeled TenSeg, first employs a tensor decomposition, and second detects multiple change-points in the second-order (cross-covariance or network) structure of the decomposed data. It allows for change-point detection in the presence of frequent changes of possibly small magnitudes and is computationally fast. We apply our method to several simulated datasets and to a cryptocurrency dataset, which consists of network tensor-variate data from the Ethereum blockchain. We demonstrate that our approach substantially outperforms other state-of-the-art change-point techniques, and the detected change-points in the Ethereum data set coincide with changes across several trading networks or a `network of networks' over time. Finally, all the relevant \textsf{R} code implementing the method in the article are available on https://github.com/Anastasiou-Andreas/TenSeg.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06211v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Andreas Anastasiou, Ivor Cribben</dc:creator>
    </item>
    <item>
      <title>Estimating on-street parking occupancy using smart meter data</title>
      <link>https://arxiv.org/abs/2106.02270</link>
      <description>arXiv:2106.02270v2 Announce Type: replace 
Abstract: The excessive search for parking, known as cruising, generates pollution and congestion. Cities are looking for approaches that will reduce the negative impact associated with searching for parking. However, adequately measuring the number of vehicles in search of parking is difficult and requires sensing technologies. In this paper, we develop an approach that eliminates the need for sensing technology by using parking meter payment transactions to estimate parking occupancy and the number of cars searching for parking. The estimation scheme is based on Particle Markov Chain Monte Carlo. We validate the performance of the Particle Markov Chain Monte Carlo approach using data simulated from a GI/GI/s queue. We show that the approach generates asymptotically unbiased Bayesian estimates of the parking occupancy and underlying model parameters such as arrival rates, average parking time, and the payment compliance rate. Finally, we estimate parking occupancy and cruising using parking meter data from SFpark, a large scale parking experiment and subsequently, compare the Particle Markov Chain Monte Carlo parking occupancy estimates against the ground truth data from the parking sensors. Our approach is easily replicated and scalable given that it only requires using data that cities already possess, namely historical parking payment transactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2106.02270v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Daniel Jordon, Robert Hampshire, Tayo Fabusuyi</dc:creator>
    </item>
    <item>
      <title>Understanding the Impact of Seasonal Climate Change on Canada's Economy by Region and by Sector</title>
      <link>https://arxiv.org/abs/2311.03497</link>
      <description>arXiv:2311.03497v2 Announce Type: replace 
Abstract: To assess the impact of climate change on the Canadian economy, we investigate the relationship between seasonal climate variables and economic growth across provinces and economic sectors. We also provide projections of climate change impacts up to the year of 2050, taking into account the diverse climate change patterns and economic conditions across Canada. Our results indicate that rising Winter temperature anomalies have a notable adverse impact on Canadian economic growth. Province-wide, Quebec, Manitoba, and Ontario are anticipated to experience larger negative impacts, whereas British Columbia is less vulnerable. Industry-wide, Finance and Real Estate, Science and Technology, and Information, Culture and Recreation are consistently projected to see mild benefits, while adverse effects are predicted for Manufacturing, Agriculture, and Mining. The disparities of climate change effects between provinces and industries highlight the need for governments to tailor their policies accordingly, and offer targeted assistance to regions and industries that are particularly vulnerable in the face of climate change. Targeted approaches to climate change mitigation are likely to be more effective than one-size-fits-all policies for the whole economy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.03497v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shiyu He, Trang Bui, Yuying Huang, Wenling Zhang, Jie Jian, Samuel W. K. Wong, Tony S. Wirjanto</dc:creator>
    </item>
    <item>
      <title>Bot Identification in Social Media</title>
      <link>https://arxiv.org/abs/2503.23629</link>
      <description>arXiv:2503.23629v3 Announce Type: replace 
Abstract: Escalating proliferation of inorganic accounts, commonly known as bots, within the digital ecosystem represents an ongoing and multifaceted challenge to online security, trustworthiness, and user experience. These bots, often employed for the dissemination of malicious propaganda and manipulation of public opinion, wield significant influence in social media spheres with far-reaching implications for electoral processes, political campaigns and international conflicts. Swift and accurate identification of inorganic accounts is of paramount importance in mitigating their detrimental effects. This research paper focuses on the identification of such accounts and explores various effective methods for their detection through machine learning techniques. In response to the pervasive presence of bots in the contemporary digital landscape, this study extracts temporal and semantic features from tweet behaviors and proposes a bot detection algorithm utilizing fundamental machine learning approaches, including Support Vector Machines (SVM) and k-means clustering. Furthermore, the research ranks the importance of these extracted features for each detection technique and also provides uncertainty quantification using a distribution free method, called the conformal prediction, thereby contributing to the development of effective strategies for combating the prevalence of inorganic accounts in social media platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23629v3</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dhrubajyoti Ghosh, William Boettcher, Rob Johnston, Soumendra Lahiri</dc:creator>
    </item>
    <item>
      <title>Inequality Constrained Minimum Density Power Divergence Estimation in Panel Count Data</title>
      <link>https://arxiv.org/abs/2503.21534</link>
      <description>arXiv:2503.21534v4 Announce Type: replace-cross 
Abstract: The analysis of panel count data has garnered considerable attention in the literature, leading to the development of multiple statistical techniques. In inferential analysis, most works focus on leveraging estimating equation-based techniques or conventional maximum likelihood estimation. However, the robustness of these methods is largely questionable. In this paper, we present a robust density power divergence estimation method for panel count data arising from non-homogeneous Poisson processes correlated through a latent frailty variable. To cope with real-world incidents, it is often desirable to impose certain inequality constraints on the parameter space, leading to the constrained minimum density power divergence estimator. Being incorporated with inequality restrictions, coupled with the inherent complexity of our objective function, standard computational algorithms are inadequate for estimation purposes. To overcome this, we adopt sequential convex programming, which approximates the original problem through a series of subproblems. Further, we study the asymptotic properties of the resultant estimator, making a significant contribution to this work. The proposed method ensures high efficiency in the model estimation while providing reliable inference despite data contamination. Moreover, the density power divergence measure is governed by a tuning parameter $\gamma$, which controls the trade-off between robustness and efficiency. To effectively determine the optimal value of $\gamma$, this study employs a generalized score-matching technique, marking considerable progress in the data analysis. Simulation studies and real data examples are provided to illustrate the performance of the estimator and to substantiate the theory developed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21534v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Udita Goswami, Shuvashree Mondal</dc:creator>
    </item>
    <item>
      <title>Deep Learning Approaches with Explainable AI for Differentiating Alzheimer Disease and Mild Cognitive Impairment</title>
      <link>https://arxiv.org/abs/2510.00048</link>
      <description>arXiv:2510.00048v2 Announce Type: replace-cross 
Abstract: Early and accurate diagnosis of Alzheimer Disease is critical for effective clinical intervention, particularly in distinguishing it from Mild Cognitive Impairment, a prodromal stage marked by subtle structural changes. In this study, we propose a hybrid deep learning ensemble framework for Alzheimer Disease classification using structural magnetic resonance imaging. Gray and white matter slices are used as inputs to three pretrained convolutional neural networks such as ResNet50, NASNet, and MobileNet, each fine tuned through an end to end process. To further enhance performance, we incorporate a stacked ensemble learning strategy with a meta learner and weighted averaging to optimally combine the base models. Evaluated on the Alzheimer Disease Neuroimaging Initiative dataset, the proposed method achieves state of the art accuracy of 99.21% for Alzheimer Disease vs. Mild Cognitive Impairment and 91.0% for Mild Cognitive Impairment vs. Normal Controls, outperforming conventional transfer learning and baseline ensemble methods. To improve interpretability in image based diagnostics, we integrate Explainable AI techniques by Gradient weighted Class Activation, which generates heatmaps and attribution maps that highlight critical regions in gray and white matter slices, revealing structural biomarkers that influence model decisions. These results highlight the frameworks potential for robust and scalable clinical decision support in neurodegenerative disease diagnostics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00048v2</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fahad Mostafa, Kannon Hossain, Hafiz Khan</dc:creator>
    </item>
  </channel>
</rss>
