<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 03 Dec 2025 05:00:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Microbenchmarking NVIDIA's Blackwell Architecture: An in-depth Architectural Analysis</title>
      <link>https://arxiv.org/abs/2512.02189</link>
      <description>arXiv:2512.02189v1 Announce Type: new 
Abstract: As GPU architectures rapidly evolve to meet the overcoming demands of exascale computing and machine learning, the performance implications of architectural innovations remain poorly understood across diverse workloads. NVIDIA's Blackwell (B200) generation introduce significant architectural advances including the 5th generation tensor cores, tensor memory (TMEM), decompression engine (DE), and dual chips; however systematic methodologies for quantifying these improvements lag behind hardware development cycles. We contribute an open-source microbenchmark suite that offers practical insights into optimizing workloads to fully utilize the rich feature sets of the modern GPU architecture. This work aims to enable application developers make informed architectural decisions and guide future GPU design directions.
  Our work studies Blackwell GPUs, compares them to H200 generation with regards to the memory subsystem, tensor core pipeline and floating-point precisions (FP32, FP16, FP8, FP6, FP4). Our systematic evaluation of dense/sparse GEMM, transformer inference, and training workloads demonstrate that B200's tensor core enhancements achieves 1.56x higher mixed-precision throughput and 42% better energy efficiency than H200. Our memory analysis reveals 58% reduction in memory access latency in cache-misses, fundamentally changing optimal algorithm design strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02189v1</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aaron Jarmusch, Sunita Chandrasekaran</dc:creator>
    </item>
    <item>
      <title>Near-Memory Architecture for Threshold-Ordinal Surface-Based Corner Detection of Event Cameras</title>
      <link>https://arxiv.org/abs/2512.02346</link>
      <description>arXiv:2512.02346v1 Announce Type: new 
Abstract: Event-based Cameras (EBCs) are widely utilized in surveillance and autonomous driving applications due to their high speed and low power consumption. Corners are essential low-level features in event-driven computer vision, and novel algorithms utilizing event-based representations, such as Threshold-Ordinal Surface (TOS), have been developed for corner detection. However, the implementation of these algorithms on resource-constrained edge devices is hindered by significant latency, undermining the advantages of EBCs. To address this challenge, a near-memory architecture for efficient TOS updates (NM-TOS) is proposed. This architecture employs a read-write decoupled 8T SRAM cell and optimizes patch update speed through pipelining. Hardware-software co-optimized peripheral circuits and dynamic voltage and frequency scaling (DVFS) enable power and latency reductions. Compared to traditional digital implementations, our architecture reduces latency/energy by 24.7x/1.2x at Vdd = 1.2 V or 1.93x/6.6x at Vdd = 0.6 V based on 65nm CMOS process. Monte Carlo simulations confirm robust circuit operation, demonstrating zero bit error rate at operating voltages above 0.62 V, with only 0.2% at 0.61 V and 2.5% at 0.6 V. Corner detection evaluation using precision-recall area under curve (AUC) metrics reveals minor AUC reductions of 0.027 and 0.015 at 0.6 V for two popular EBC datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02346v1</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongyang Shang, An Guo, Shuai Dong, Junyi Yang, Ye Ke, Arindam Basu</dc:creator>
    </item>
    <item>
      <title>Monomorphism-based CGRA Mapping via Space and Time Decoupling</title>
      <link>https://arxiv.org/abs/2512.02859</link>
      <description>arXiv:2512.02859v1 Announce Type: new 
Abstract: Coarse-Grain Reconfigurable Arrays (CGRAs) provide flexibility and energy efficiency in accelerating compute-intensive loops. Existing compilation techniques often struggle with scalability, unable to map code onto large CGRAs. To address this, we propose a novel approach to the mapping problem where the time and space dimensions are decoupled and explored separately. We leverage an SMT formulation to traverse the time dimension first, and then perform a monomorphism-based search to find a valid spatial solution. Experimental results show that our approach achieves the same mapping quality of state-of-the-art techniques while significantly reducing compilation time, with this reduction being particularly tangible when compiling for large CGRAs. We achieve approximately $10^5\times$ average compilation speedup for the benchmarks evaluated on a $20\times 20$ CGRA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02859v1</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.23919/DATE64628.2025.10992940</arxiv:DOI>
      <dc:creator>Cristian Tirelli, Rodrigo Otoni, Laura Pozzi</dc:creator>
    </item>
    <item>
      <title>SAT-MapIt: A SAT-based Modulo Scheduling Mapper for Coarse Grain Reconfigurable Architectures</title>
      <link>https://arxiv.org/abs/2512.02875</link>
      <description>arXiv:2512.02875v1 Announce Type: new 
Abstract: Coarse-Grain Reconfigurable Arrays (CGRAs) are emerging low-power architectures aimed at accelerating compute-intensive application loops. The acceleration that a CGRA can ultimately provide, however, heavily depends on the quality of the mapping, i.e. on how effectively the loop is compiled onto the given platform. State of the Art compilation techniques achieve mapping through modulo scheduling, a strategy which attempts to minimize the II (Iteration Interval) needed to execute a loop, and they do so usually through well known graph algorithms, such as Max-Clique Enumeration.
  We address the mapping problem through a SAT formulation, instead, and thus explore the solution space more effectively than current SoA tools. To formulate the SAT problem, we introduce an ad-hoc schedule called the \textit{kernel mobility schedule} (KMS), which we use in conjunction with the data-flow graph and the architectural information of the CGRA in order to create a set of boolean statements that describe all constraints to be obeyed by the mapping for a given II. We then let the SAT solver efficiently navigate this complex space. As in other SoA techniques, the process is iterative: if a valid mapping does not exist for the given II, the II is increased and a new KMS and set of constraints is generated and solved.
  Our experimental results show that SAT-MapIt obtains better results compared to SoA alternatives in $47.72\%$ of the benchmarks explored: sometimes finding a lower II, and others even finding a valid mapping when none could previously be found.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02875v1</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.23919/DATE56975.2023.10137123</arxiv:DOI>
      <dc:creator>Cristian Tirelli, Lorenzo Ferretti, Laura Pozzi</dc:creator>
    </item>
    <item>
      <title>Mapping code on Coarse Grained Reconfigurable Arrays using a SAT solver</title>
      <link>https://arxiv.org/abs/2512.02884</link>
      <description>arXiv:2512.02884v1 Announce Type: new 
Abstract: Emerging low-powered architectures like Coarse-Grain Reconfigurable Arrays (CGRAs) are becoming more common. Often included as co-processors, they are used to accelerate compute-intensive workloads like loops. The speedup obtained is defined by the hardware design of the accelerator and by the quality of the compilation. State of the art (SoA) compilation techniques leverage modulo scheduling to minimize the Iteration Interval (II), exploit the architecture parallelism and, consequentially, reduce the execution time of the accelerated workload. In our work, we focus on improving the compilation process by finding the lowest II for any given topology, through a satisfiability (SAT) formulation of the mapping problem. We introduce a novel schedule, called Kernel Mobility Schedule, to encode all the possible mappings for a given Data Flow Graph (DFG) and for a given II. The schedule is used together with the CGRA architectural information to generate all the constraints necessary to find a valid mapping. Experimental results demonstrate that our method not only reduces compilation time on average but also achieves higher quality mappings compared to existing SoA techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02884v1</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-90203-1_40</arxiv:DOI>
      <dc:creator>Cristian Tirelli, Laura Pozzi</dc:creator>
    </item>
    <item>
      <title>ESACT: An End-to-End Sparse Accelerator for Compute-Intensive Transformers via Local Similarity</title>
      <link>https://arxiv.org/abs/2512.02403</link>
      <description>arXiv:2512.02403v1 Announce Type: cross 
Abstract: Transformers, composed of QKV generation, attention computation, and FFNs,
  have become the dominant model across various domains due to their outstanding performance.
  However, their high computational cost hinders efficient hardware deployment.
  Sparsity offers a promising solution,
  yet most existing accelerators exploit only intra-row sparsity in attention,
  while few consider inter-row sparsity.
  Approaches leveraging inter-row sparsity often rely on costly global similarity estimation,
  which diminishes the acceleration benefits of sparsity,
  and typically apply sparsity to only one or two transformer components.
  Through careful analysis of the attention distribution and computation flow,
  we observe that local similarity allows end-to-end sparse acceleration with lower computational overhead.
  Motivated by this observation, we propose ESACT,
  an end-to-end sparse accelerator for compute-intensive Transformers.
  ESACT centers on the Sparsity Prediction with Local Similarity (SPLS) mechanism,
  which leverages HLog quantization to accurately predict local attention sparsity prior to QK generation,
  achieving efficient sparsity across all transformer components.
  To support efficient hardware realization, we introduce three architectural innovations.
  Experimental results on 26 benchmarks demonstrate that
  SPLS reduces total computation by 52.03% with less than 1% accuracy loss.
  ESACT achieves an end-to-end energy efficiency of 3.29 TOPS/W,
  and improves attention-level energy efficiency by 2.95x and 2.26x over
  SOTA attention accelerators SpAtten and Sanger, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02403v1</guid>
      <category>cs.LG</category>
      <category>cs.AR</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongxiang Liu, Zhifang Deng, Tong Pu, Shengli Lu</dc:creator>
    </item>
    <item>
      <title>Evaluation of GPU Video Encoder for Low-Latency Real-Time 4K UHD Encoding</title>
      <link>https://arxiv.org/abs/2511.18688</link>
      <description>arXiv:2511.18688v2 Announce Type: replace 
Abstract: The demand for high-quality, real-time video streaming has grown exponentially, with 4K Ultra High Definition (UHD) becoming the new standard for many applications such as live broadcasting, TV services, and interactive cloud gaming. This trend has driven the integration of dedicated hardware encoders into modern Graphics Processing Units (GPUs). Nowadays, these encoders support advanced codecs like HEVC and AV1 and feature specialized Low-Latency and Ultra Low-Latency tuning, targeting end-to-end latencies of &lt; 2 seconds and &lt; 500 ms, respectively. As the demand for such capabilities grows toward the 6G era, a clear understanding of their performance implications is essential. In this work, we evaluate the low-latency encoding modes on GPUs from NVIDIA, Intel, and AMD from both Rate-Distortion (RD) performance and latency perspectives. The results are then compared against both the normal-latency tuning of hardware encoders and leading software encoders. Results show hardware encoders achieve significantly lower E2E latency than software solutions with slightly better RD performance. While standard Low-Latency tuning yields a poor quality-latency trade-off, the Ultra Low-Latency mode reduces E2E latency to 83 ms (5 frames) without additional RD impact. Furthermore, hardware encoder latency is largely insensitive to quality presets, enabling high-quality, low-latency streams without compromise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18688v2</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kasidis Arunruangsirilert, Jiro Katto</dc:creator>
    </item>
    <item>
      <title>Optimized Many-Hypercube Codes toward Lower Logical Error Rates and Earlier Realization</title>
      <link>https://arxiv.org/abs/2512.00561</link>
      <description>arXiv:2512.00561v2 Announce Type: replace-cross 
Abstract: Many-hypercube codes [H. Goto, Sci. Adv. 10, eadp6388 (2024)], concatenated ${[[n,n-2,2]]}$ quantum error-detecting codes ($n$ is even), have recently been proposed as high-rate quantum codes suitable for fault-tolerant quantum computing. While the original many-hypercube codes with ${n=6}$ can achieve remarkably high encoding rates (about 30% and 20% at concatenation levels 3 and 4, respectively), they have large code block sizes at high levels (216 and 1296 physical qubits per block at levels 3 and 4, respectively), making not only experimental realization difficult but also logical error rates per block high. Toward earlier experimental realization and lower logical error rates, here we comprehensively investigate smaller many-hypercube codes with $[[6,4,2]]$ and/or $[[4,2,2]]$ codes, where, e.g., $D_{6,4,4}$ denotes the many-hypercube code using $[[6,4,2]]$ at level 1 and $[[4,2,2]]$ at levels 2 and 3. As a result, we found a notable fact that $D_{6,4,4}$ ($D_{6,6,4,4}$) can achieve lower block error rates than $D_{4,4,4}$ ($D_{4,4,4,4}$), despite its higher encoding rate. Focusing on level 3, we also developed efficient fault-tolerant encoders realizing about 60% overhead reduction while maintaining or even improving the performance, compared to the original design. Using them, we numerically confirmed that $D_{6,4,4}$ also achieves the best performance for logical controlled-NOT gates in a circuit-level noise model. These results will be useful for early experimental realization of fault-tolerant quantum computing with high-rate quantum codes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00561v2</guid>
      <category>quant-ph</category>
      <category>cs.AR</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hayato Goto</dc:creator>
    </item>
  </channel>
</rss>
