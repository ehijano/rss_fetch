<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 22 Nov 2024 02:48:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Modelling Directed Networks with Reciprocity</title>
      <link>https://arxiv.org/abs/2411.12871</link>
      <description>arXiv:2411.12871v1 Announce Type: new 
Abstract: Asymmetric relational data is increasingly prevalent across diverse fields, underscoring the need for directed network models to address the complex challenges posed by their unique structures. Unlike undirected models, directed models can capture reciprocity, the tendency of nodes to form mutual links. In this work, we address a fundamental question: what is the effective sample size for modeling reciprocity? We examine this by analyzing the Bernoulli model with reciprocity, allowing for varying sparsity levels between non-reciprocal and reciprocal effects. We then extend this framework to a model that incorporates node-specific heterogeneity and link-specific reciprocity using covariates. Our findings reveal intriguing interplays between non-reciprocal and reciprocal effects in sparse networks. We propose a straightforward inference procedure based on maximum likelihood estimation that operates without prior knowledge of sparsity levels, whether covariates are included or not.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12871v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rui Feng, Chenlei Leng</dc:creator>
    </item>
    <item>
      <title>Goodness-of-fit tests for generalized Poisson distributions</title>
      <link>https://arxiv.org/abs/2411.12889</link>
      <description>arXiv:2411.12889v1 Announce Type: new 
Abstract: This paper presents and examines computationally convenient goodness-of-fit tests for the family of generalized Poisson distributions, which encompasses notable distributions such as the Compound Poisson and the Katz distributions. The tests are consistent against fixed alternatives and their null distribution can be consistently approximated by a parametric bootstrap. The goodness of the bootstrap estimator and the power for finite sample sizes are numerically assessed through an extensive simulation experiment, including comparisons with other tests. In many cases, the novel tests either outperform or match the performance of existing ones. Real data applications are considered for illustrative purposes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12889v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>A. Batsidis, B. Milo\v{s}evi\'c, M. D. Jim\'enez-Gamero</dc:creator>
    </item>
    <item>
      <title>From Estimands to Robust Inference of Treatment Effects in Platform Trials</title>
      <link>https://arxiv.org/abs/2411.12944</link>
      <description>arXiv:2411.12944v1 Announce Type: new 
Abstract: A platform trial is an innovative clinical trial design that uses a master protocol (i.e., one overarching protocol) to evaluate multiple treatments in an ongoing manner and can accelerate the evaluation of new treatments. However, the flexibility that marks the potential of platform trials also creates inferential challenges. Two key challenges are the precise definition of treatment effects and the robust and efficient inference on these effects. To address these challenges, we first define a clinically meaningful estimand that characterizes the treatment effect as a function of the expected outcomes under two given treatments among concurrently eligible patients. Then, we develop weighting and post-stratification methods for estimation of treatment effects with minimal assumptions. To fully leverage the efficiency potential of data from concurrently eligible patients, we also consider a model-assisted approach for baseline covariate adjustment to gain efficiency while maintaining robustness against model misspecification. We derive and compare asymptotic distributions of proposed estimators in theory and propose robust variance estimators. The proposed estimators are empirically evaluated in a simulation study and illustrated using the SIMPLIFY trial. Our methods are implemented in the R package RobinCID.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12944v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuhan Qian, Yifan Yi, Jun Shao, Yanyao Yi, Nicole Mayer-Hamblett, Patrick J. Heagerty, Ting Ye</dc:creator>
    </item>
    <item>
      <title>Bayesian Parameter Estimation of Normal Distribution from Sample Mean and Extreme Values</title>
      <link>https://arxiv.org/abs/2411.13131</link>
      <description>arXiv:2411.13131v1 Announce Type: new 
Abstract: This paper proposes a Bayesian method for estimating the parameters of a normal distribution when only limited summary statistics (sample mean, minimum, maximum, and sample size) are available. To estimate the parameters of a normal distribution, we introduce a data augmentation approach using the Gibbs sampler, where intermediate values are treated as missing values and samples from a truncated normal distribution conditional on the observed sample mean, minimum, and maximum values. Through simulation studies, we demonstrate that our method achieves estimation accuracy comparable to theoretical expectations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13131v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomoki Matsumoto</dc:creator>
    </item>
    <item>
      <title>Spatial error models with heteroskedastic normal perturbations and joint modeling of mean and variance</title>
      <link>https://arxiv.org/abs/2411.13432</link>
      <description>arXiv:2411.13432v1 Announce Type: new 
Abstract: This work presents the spatial error model with heteroskedasticity, which allows the joint modeling of the parameters associated with both the mean and the variance, within a traditional approach to spatial econometrics. The estimation algorithm is based on the log-likelihood function and incorporates the use of GAMLSS models in an iterative form. Two theoretical results show the advantages of the model to the usual models of spatial econometrics and allow obtaining the bias of weighted least squares estimators. The proposed methodology is tested through simulations, showing notable results in terms of the ability to recover all parameters and the consistency of its estimates. Finally, this model is applied to identify the factors associated with school desertion in Colombia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13432v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>J. D. Toloza, O. O. Melo, N. A. Cruz</dc:creator>
    </item>
    <item>
      <title>The R\'enyi Outlier Test</title>
      <link>https://arxiv.org/abs/2411.13542</link>
      <description>arXiv:2411.13542v1 Announce Type: new 
Abstract: Cox and Kartsonaki proposed a simple outlier test for a vector of p-values based on the R\'enyi transformation that is fast for large $p$ and numerically stable for very small p-values -- key properties for large data analysis. We propose and implement a generalization of this procedure we call the R\'enyi Outlier Test (ROT). This procedure maintains the key properties of the original but is much more robust to uncertainty in the number of outliers expected a priori among the p-values. The ROT can also account for two types of prior information that are common in modern data analysis. The first is the prior probability that a given p-value may be outlying. The second is an estimate of how far of an outlier a p-value might be, conditional on it being an outlier; in other words, an estimate of effect size. Using a series of pre-calculated spline functions, we provide a fast and numerically stable implementation of the ROT in our R package renyi.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13542v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan Christ, Ira Hall, David Steinsaltz</dc:creator>
    </item>
    <item>
      <title>Underlying Core Inflation with Multiple Regimes</title>
      <link>https://arxiv.org/abs/2411.12845</link>
      <description>arXiv:2411.12845v1 Announce Type: cross 
Abstract: This paper introduces a new approach for estimating core inflation indicators based on common factors across a broad range of price indices. Specifically, by utilizing procedures for detecting multiple regimes in high-dimensional factor models, we propose two types of core inflation indicators: one incorporating multiple structural breaks and another based on Markov switching. The structural breaks approach can eliminate revisions for past regimes, though it functions as an offline indicator, as real-time detection of breaks is not feasible with this method. On the other hand, the Markov switching approach can reduce revisions while being useful in real time, making it a simple and robust core inflation indicator suitable for real-time monitoring and as a short-term guide for monetary policy. Additionally, this approach allows us to estimate the probability of being in different inflationary regimes. To demonstrate the effectiveness of these indicators, we apply them to Canadian price data. To compare the real-time performance of the Markov switching approach to the benchmark model without regime-switching, we assess their abilities to forecast headline inflation and minimize revisions. We find that the Markov switching model delivers superior predictive accuracy and significantly reduces revisions during periods of substantial inflation changes. Hence, our findings suggest that accounting for time-varying factors and parameters enhances inflation signal accuracy and reduces data requirements, especially following sudden economic shifts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12845v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriel Rodriguez-Rondon</dc:creator>
    </item>
    <item>
      <title>On adaptivity and minimax optimality of two-sided nearest neighbors</title>
      <link>https://arxiv.org/abs/2411.12965</link>
      <description>arXiv:2411.12965v1 Announce Type: cross 
Abstract: Nearest neighbor (NN) algorithms have been extensively used for missing data problems in recommender systems and sequential decision-making systems. Prior theoretical analysis has established favorable guarantees for NN when the underlying data is sufficiently smooth and the missingness probabilities are lower bounded. Here we analyze NN with non-smooth non-linear functions with vast amounts of missingness. In particular, we consider matrix completion settings where the entries of the underlying matrix follow a latent non-linear factor model, with the non-linearity belonging to a \Holder function class that is less smooth than Lipschitz. Our results establish following favorable properties for a suitable two-sided NN: (1) The mean squared error (MSE) of NN adapts to the smoothness of the non-linearity, (2) under certain regularity conditions, the NN error rate matches the rate obtained by an oracle equipped with the knowledge of both the row and column latent factors, and finally (3) NN's MSE is non-trivial for a wide range of settings even when several matrix entries might be missing deterministically. We support our theoretical findings via extensive numerical simulations and a case study with data from a mobile health study, HeartSteps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12965v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tathagata Sadhukhan, Manit Paul, Raaz Dwivedi</dc:creator>
    </item>
    <item>
      <title>Distribution-free Measures of Association based on Optimal Transport</title>
      <link>https://arxiv.org/abs/2411.13080</link>
      <description>arXiv:2411.13080v1 Announce Type: cross 
Abstract: In this paper we propose and study a class of nonparametric, yet interpretable measures of association between two random vectors $X$ and $Y$ taking values in $\mathbb{R}^{d_1}$ and $\mathbb{R}^{d_2}$ respectively ($d_1, d_2\ge 1$). These nonparametric measures -- defined using the theory of reproducing kernel Hilbert spaces coupled with optimal transport -- capture the strength of dependence between $X$ and $Y$ and have the property that they are 0 if and only if the variables are independent and 1 if and only if one variable is a measurable function of the other. Further, these population measures can be consistently estimated using the general framework of geometric graphs which include $k$-nearest neighbor graphs and minimum spanning trees. Additionally, these measures can also be readily used to construct an exact finite sample distribution-free test of mutual independence between $X$ and $Y$. In fact, as far as we are aware, these are the only procedures that possess all the above mentioned desirable properties. The correlation coefficient proposed in Dette et al. (2013), Chatterjee (2021), Azadkia and Chatterjee (2021), at the population level, can be seen as a special case of this general class of measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13080v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nabarun Deb, Promit Ghosal, Bodhisattva Sen</dc:creator>
    </item>
    <item>
      <title>Jointly Modeling and Clustering Tensors in High Dimensions</title>
      <link>https://arxiv.org/abs/2104.07773</link>
      <description>arXiv:2104.07773v3 Announce Type: replace 
Abstract: We consider the problem of jointly modeling and clustering populations of tensors by introducing a high-dimensional tensor mixture model with heterogeneous covariances. To effectively tackle the high dimensionality of tensor objects, we employ plausible dimension reduction assumptions that exploit the intrinsic structures of tensors such as low-rankness in the mean and separability in the covariance. In estimation, we develop an efficient high-dimensional expectation-conditional-maximization (HECM) algorithm that breaks the intractable optimization in the M-step into a sequence of much simpler conditional optimization problems, each of which is convex, admits regularization and has closed-form updating formulas. Our theoretical analysis is challenged by both the non-convexity in the EM-type estimation and having access to only the solutions of conditional maximizations in the M-step, leading to the notion of dual non-convexity. We demonstrate that the proposed HECM algorithm, with an appropriate initialization, converges geometrically to a neighborhood that is within statistical precision of the true parameter. The efficacy of our proposed method is demonstrated through comparative numerical experiments and an application to a medical study, where our proposal achieves an improved clustering accuracy over existing benchmarking methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2104.07773v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Biao Cai, Jingfei Zhang, Will Wei Sun</dc:creator>
    </item>
    <item>
      <title>Causal and Counterfactual Views of Missing Data Models</title>
      <link>https://arxiv.org/abs/2210.05558</link>
      <description>arXiv:2210.05558v3 Announce Type: replace 
Abstract: It is often said that the fundamental problem of causal inference is a missing data problem -- the comparison of responses to two hypothetical treatment assignments is made difficult because for every experimental unit only one potential response is observed. In this paper, we consider the implications of the converse view: that missing data problems are a form of causal inference. We make explicit how the missing data problem of recovering the complete data law from the observed law can be viewed as identification of a joint distribution over counterfactual variables corresponding to values had we (possibly contrary to fact) been able to observe them. Drawing analogies with causal inference, we show how identification assumptions in missing data can be encoded in terms of graphical models defined over counterfactual and observed variables. We review recent results in missing data identification from this viewpoint. In doing so, we note interesting similarities and differences between missing data and causal identification theories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.05558v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Razieh Nabi, Rohit Bhattacharya, Ilya Shpitser, James M. Robins</dc:creator>
    </item>
    <item>
      <title>Clustering multivariate functional data using the epigraph and hypograph indices: a case study on Madrid air quality</title>
      <link>https://arxiv.org/abs/2307.16720</link>
      <description>arXiv:2307.16720v4 Announce Type: replace 
Abstract: With the rapid growth of data generation, advancements in functional data analysis (FDA) have become essential, especially for approaches that handle multiple variables at the same time. This paper introduces a novel formulation of the epigraph and hypograph indices, along with their generalized expressions, specifically designed for multivariate functional data (MFD). These new definitions account for interrelationships between variables, enabling effective clustering of MFD based on the original data curves and their first two derivatives. The methodology developed here has been tested on simulated datasets, demonstrating strong performance compared to state-of-the-art methods. Its practical utility is further illustrated with two environmental datasets: the Canadian weather dataset and a 2023 air quality study in Madrid. These applications highlight the potential of the method as a great tool for analyzing complex environmental data, offering valuable insights for researchers and policymakers in climate and environmental research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.16720v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bel\'en Pulido, Alba M. Franco-Pereira, Rosa E. Lillo</dc:creator>
    </item>
    <item>
      <title>Inconsistency and Acausality of Model Selection in Bayesian Inverse Problems</title>
      <link>https://arxiv.org/abs/2308.05858</link>
      <description>arXiv:2308.05858v3 Announce Type: replace 
Abstract: Bayesian inference paradigms are regarded as powerful tools for solution of inverse problems. However, when applied to inverse problems in physical sciences, Bayesian formulations suffer from a number of inconsistencies that are often overlooked. A well known, but mostly neglected, difficulty is connected to the notion of conditional probability densities. Borel, and later Kolmogorov's (1933/1956), found that the traditional definition of conditional densities is incomplete: In different parameterizations it leads to different results. We will show an example where two apparently correct procedures applied to the same problem lead to two widely different results. Another type of inconsistency involves violation of causality. This problem is found in model selection strategies in Bayesian inversion, such as Hierarchical Bayes and Trans-Dimensional Inversion where so-called hyperparameters are included as variables to control either the number (or type) of unknowns, or the prior uncertainties on data or model parameters. For Hierarchical Bayes we demonstrate that the calculated 'prior' distributions of data or model parameters are not prior-, but posterior information. In fact, the calculated 'standard deviations' of the data are a measure of the inability of the forward function to model the data, rather than uncertainties of the data. For trans-dimensional inverse problems we show that the so-called evidence is, in fact, not a measure of the success of fitting the data for the given choice (or number) of parameters, as often claimed. We also find that the notion of Natural Parsimony is ill-defined, because of its dependence on the parameter prior. Based on this study, we find that careful rethinking of Bayesian inversion practices is required, with special emphasis on ways of avoiding the Borel-Kolmogorov inconsistency, and on the way we interpret model selection results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.05858v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Klaus Mosegaard</dc:creator>
    </item>
    <item>
      <title>Calibrated Generalized Bayesian Inference</title>
      <link>https://arxiv.org/abs/2311.15485</link>
      <description>arXiv:2311.15485v2 Announce Type: replace 
Abstract: We provide a simple and general solution for accurate uncertainty quantification of Bayesian inference in misspecified or approximate models, and for generalized posteriors more generally. While existing solutions are based on explicit Gaussian posterior approximations, or post-processing procedures, we demonstrate that correct uncertainty quantification can be achieved by substituting the usual posterior with an intuitively appealing alternative posterior that conveys the same information. This solution applies to both likelihood-based and loss-based posteriors, and we formally demonstrate the reliable uncertainty quantification of this approach. The new approach is demonstrated through a range of examples, including linear models, and doubly intractable models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.15485v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David T. Frazier, Christopher Drovandi, Robert Kohn</dc:creator>
    </item>
    <item>
      <title>Robust Learning for Optimal Dynamic Treatment Regimes with Observational Data</title>
      <link>https://arxiv.org/abs/2404.00221</link>
      <description>arXiv:2404.00221v4 Announce Type: replace 
Abstract: Public policies and medical interventions often involve dynamics in their treatment assignments, where individuals receive a series of interventions over multiple stages. We study the statistical learning of optimal dynamic treatment regimes (DTRs) that guide the optimal treatment assignment for each individual at each stage based on the individual's evolving history. We propose a doubly robust, classification-based approach to learning the optimal DTR using observational data under the assumption of sequential ignorability. This approach learns the optimal DTR through backward induction. At each step, it constructs an augmented inverse probability weighting (AIPW) estimator of the policy value function and maximizes it to learn the optimal policy for the corresponding stage. We show that the resulting DTR can achieve an optimal convergence rate of $n^{-1/2}$ for welfare regret under mild convergence conditions on estimators of the nuisance components.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00221v4</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shosei Sakaguchi</dc:creator>
    </item>
    <item>
      <title>Design-based Causal Inference for Incomplete Block Designs</title>
      <link>https://arxiv.org/abs/2405.19312</link>
      <description>arXiv:2405.19312v3 Announce Type: replace 
Abstract: Researchers often turn to block randomization to increase the precision of their inference or due to practical considerations, such as in multi-site trials. However, if the number of treatments under consideration is large it might not be practical or even feasible to assign all treatments within each block. We develop novel inference results under the finite-population design-based framework for natural alternatives to the complete block design that do not require reducing the number of treatment arms, the incomplete block design (IBD) and the balanced incomplete block design (BIBD). This includes deriving the properties of two estimators and proposing conservative variance estimators. To assist practitioners in understanding the trade-offs of using these designs, precision comparisons are made to standard estimators for the complete block, cluster-randomized, and completely randomized designs. Simulations and a data illustration further demonstrate the trade-offs. This work highlights IBDs as practical and currently underutilized designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19312v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Taehyeon Koo, Nicole E. Pashley</dc:creator>
    </item>
    <item>
      <title>HAL-based Plugin Estimation of the Causal Dose-Response Curve</title>
      <link>https://arxiv.org/abs/2406.05607</link>
      <description>arXiv:2406.05607v2 Announce Type: replace 
Abstract: Estimating the marginally adjusted dose-response curve for continuous treatments is a longstanding statistical challenge critical across multiple fields. In the context of parametric models, mis-specification may result in substantial bias, hindering the accurate discernment of the true data generating distribution and the associated dose-response curve. In contrast, non-parametric models face difficulties as the dose-response curve isn't pathwise differentiable, and then there is no $\sqrt{n}$-consistent estimator. The emergence of the Highly Adaptive Lasso (HAL) MLE by van der Laan [2015] and van der Laan [2017] and the subsequent theoretical evidence by van der Laan [2023] regarding its pointwise asymptotic normality and uniform convergence rates, have highlighted the asymptotic efficacy of the HAL-based plug-in estimator for this intricate problem. This paper delves into the HAL-based plug-in estimators, including those with cross-validation and undersmoothing selectors, and introduces the undersmoothed smoothness-adaptive HAL-based plug-in estimator. We assess these estimators through extensive simulations, employing detailed evaluation metrics. Building upon the theoretical proofs in van der Laan [2023], our empirical findings underscore the asymptotic effectiveness of the undersmoothed smoothness-adaptive HAL-based plug-in estimator in estimating the marginally adjusted dose-response curve.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05607v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junming Shi, Wenxin Zhang, Alan E. Hubbard, Mar van der Laan</dc:creator>
    </item>
    <item>
      <title>Relative Cumulative Residual Information Measure</title>
      <link>https://arxiv.org/abs/2410.00125</link>
      <description>arXiv:2410.00125v2 Announce Type: replace 
Abstract: In this paper, we develop a relative cumulative residual information (RCRI) measure that intends to quantify the divergence between two survival functions. The dynamic relative cumulative residual information (DRCRI) measure is also introduced. We establish some characterization results under the proportional hazards model assumption. Additionally, we obtained the non-parametric estimators of RCRI and DRCRI measures based on the kernel density type estimator for the survival function. The effectiveness of the estimators are assessed through an extensive Monte Carlo simulation study. We consider the data from the third Gaia data release (Gaia DR3) for demonstrating the use of the proposed measure. For this study, we have collected epoch photometry data for the objects Gaia DR3 4111834567779557376 and Gaia DR3 5090605830056251776.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00125v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Mary Andrews, Smitha S, Sudheesh K. Kattumannil</dc:creator>
    </item>
    <item>
      <title>Robust Inference for High-dimensional Linear Models with Heavy-tailed Errors via Partial Gini Covariance</title>
      <link>https://arxiv.org/abs/2411.12578</link>
      <description>arXiv:2411.12578v2 Announce Type: replace 
Abstract: This paper introduces the partial Gini covariance, a novel dependence measure that addresses the challenges of high-dimensional inference with heavy-tailed errors, often encountered in fields like finance, insurance, climate, and biology. Conventional high-dimensional regression inference methods suffer from inaccurate type I errors and reduced power in heavy-tailed contexts, limiting their effectiveness. Our proposed approach leverages the partial Gini covariance to construct a robust statistical inference framework that requires minimal tuning and does not impose restrictive moment conditions on error distributions. Unlike traditional methods, it circumvents the need for estimating the density of random errors and enhances the computational feasibility and robustness. Extensive simulations demonstrate the proposed method's superior power and robustness over standard high-dimensional inference approaches, such as those based on the debiased Lasso. The asymptotic relative efficiency analysis provides additional theoretical insight on the improved efficiency of the new approach in the heavy-tailed setting. Additionally, the partial Gini covariance extends to the multivariate setting, enabling chi-square testing for a group of coefficients. We illustrate the method's practical application with a real-world data example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12578v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yilin Zhang, Songshan Yang, Yunan Wu, Lan Wang</dc:creator>
    </item>
    <item>
      <title>Estimate exponential memory decay in Hidden Markov Model and its applications</title>
      <link>https://arxiv.org/abs/1710.06078</link>
      <description>arXiv:1710.06078v2 Announce Type: replace-cross 
Abstract: Inference in hidden Markov model has been challenging in terms of scalability due to dependencies in the observation data. In this paper, we utilize the inherent memory decay in hidden Markov models, such that the forward and backward probabilities can be carried out with subsequences, enabling efficient inference over long sequences of observations. We formulate this forward filtering process in the setting of the random dynamical system and there exist Lyapunov exponents in the i.i.d random matrices production. And the rate of the memory decay is known as $\lambda_2-\lambda_1$, the gap of the top two Lyapunov exponents almost surely. An efficient and accurate algorithm is proposed to numerically estimate the gap after the soft-max parametrization. The length of subsequences $B$ given the controlled error $\epsilon$ is $B=\log(\epsilon)/(\lambda_2-\lambda_1)$. We theoretically prove the validity of the algorithm and demonstrate the effectiveness with numerical examples. The method developed here can be applied to widely used algorithms, such as mini-batch stochastic gradient method. Moreover, the continuity of Lyapunov spectrum ensures the estimated $B$ could be reused for the nearby parameter during the inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:1710.06078v2</guid>
      <category>stat.ML</category>
      <category>stat.ME</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Felix X. -F. Ye, Yi-an Ma, Hong Qian</dc:creator>
    </item>
    <item>
      <title>Empirical Bayes estimation: When does $g$-modeling beat $f$-modeling in theory (and in practice)?</title>
      <link>https://arxiv.org/abs/2211.12692</link>
      <description>arXiv:2211.12692v2 Announce Type: replace-cross 
Abstract: Empirical Bayes (EB) is a popular framework for large-scale inference that aims to find data-driven estimators to compete with the Bayesian oracle that knows the true prior. Two principled approaches to EB estimation have emerged over the years: $f$-modeling, which constructs an approximate Bayes rule by estimating the marginal distribution of the data, and $g$-modeling, which estimates the prior from data and then applies the learned Bayes rule. For the Poisson model, the prototypical examples are the celebrated Robbins estimator and the nonparametric MLE (NPMLE), respectively. It has long been recognized in practice that the Robbins estimator, while being conceptually appealing and computationally simple, lacks robustness and can be easily derailed by ``outliers'', unlike the NPMLE which provides more stable and interpretable fit thanks to its Bayes form. On the other hand, not only do the existing theories shed little light on this phenomenon, but they all point to the opposite, as both methods have recently been shown optimal in terms of regret (excess over the Bayes risk) for compactly supported and subexponential priors.
  In this paper we provide a theoretical justification for the superiority of $g$-modeling over $f$-modeling for heavy-tailed data by considering priors with bounded $p&gt;1$th moment. We show that with mild regularization, any $g$-modeling method that is Hellinger rate-optimal in density estimation achieves an optimal total regret $\tilde \Theta(n^{\frac{3}{2p+1}})$; in particular, the special case of NPMLE succeeds without regularization. In contrast, there exists an $f$-modeling estimator whose density estimation rate is optimal but whose EB regret is suboptimal by a polynomial factor. These results show that the proper Bayes form provides a ``general recipe of success'' for optimal EB estimation that applies to all $g$-modeling (but not $f$-modeling) methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.12692v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yandi Shen, Yihong Wu</dc:creator>
    </item>
    <item>
      <title>Adjustment with Many Regressors Under Covariate-Adaptive Randomizations</title>
      <link>https://arxiv.org/abs/2304.08184</link>
      <description>arXiv:2304.08184v4 Announce Type: replace-cross 
Abstract: Our paper discovers a new trade-off of using regression adjustments (RAs) in causal inference under covariate-adaptive randomizations (CARs). On one hand, RAs can improve the efficiency of causal estimators by incorporating information from covariates that are not used in the randomization. On the other hand, RAs can degrade estimation efficiency due to their estimation errors, which are not asymptotically negligible when the number of regressors is of the same order as the sample size. Ignoring the estimation errors of RAs may result in serious over-rejection of causal inference under the null hypothesis. To address the issue, we construct a new ATE estimator by optimally linearly combining the estimators with and without RAs. We then develop a unified inference theory for this estimator under CARs. It has two features: (1) the Wald test based on it achieves the exact asymptotic size under the null hypothesis, regardless of whether the number of covariates is fixed or diverges no faster than the sample size; and (2) it guarantees weak efficiency improvement over estimators both with and without RAs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.08184v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liang Jiang, Liyao Li, Ke Miao, Yichong Zhang</dc:creator>
    </item>
    <item>
      <title>Assumption Smuggling in Intermediate Outcome Tests of Causal Mechanisms</title>
      <link>https://arxiv.org/abs/2407.07072</link>
      <description>arXiv:2407.07072v2 Announce Type: replace-cross 
Abstract: Political scientists are increasingly interested in assessing causal mechanisms, or determining not just if a causal effect exists but also why it occurs. Even so, many researchers avoid formal causal mediation analyses due to their stringent assumptions, instead opting to explore causal mechanisms through what we call intermediate outcome tests. These tests estimate the effect of the treatment on one or more mediators and view such effects as suggestive evidence of a causal mechanism. In this paper, we use nonparametric bounding analysis to show that, without further assumptions, these tests can neither establish nor rule out the existence of a causal mechanism. To use intermediate outcome tests as a falsification test of causal mechanisms, researchers must make a very strong but rarely discussed monotonicity assumption. We develop a way to assess the plausibility of this monotonicity assumption and estimate our bounds for two recent experiments that use these tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07072v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Matthew Blackwell, Ruofan Ma, Aleksei Opacic</dc:creator>
    </item>
    <item>
      <title>Towards a framework on tabular synthetic data generation: a minimalist approach: theory, use cases, and limitations</title>
      <link>https://arxiv.org/abs/2411.10982</link>
      <description>arXiv:2411.10982v2 Announce Type: replace-cross 
Abstract: We propose and study a minimalist approach towards synthetic tabular data generation. The model consists of a minimalistic unsupervised SparsePCA encoder (with contingent clustering step or log transformation to handle nonlinearity) and XGboost decoder which is SOTA for structured data regression and classification tasks. We study and contrast the methodologies with (variational) autoencoders in several toy low dimensional scenarios to derive necessary intuitions. The framework is applied to high dimensional simulated credit scoring data which parallels real-life financial applications. We applied the method to robustness testing to demonstrate practical use cases. The case study result suggests that the method provides an alternative to raw and quantile perturbation for model robustness testing. We show that the method is simplistic, guarantees interpretability all the way through, does not require extra tuning and provide unique benefits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10982v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yueyang Shen, Agus Sudjianto, Arun Prakash R, Anwesha Bhattacharyya, Maorong Rao, Yaqun Wang, Joel Vaughan, Nengfeng Zhou</dc:creator>
    </item>
  </channel>
</rss>
