<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 04 Feb 2025 04:07:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Sharing GPUs and Programmable Switches in a Federated Testbed with SHARY</title>
      <link>https://arxiv.org/abs/2501.18840</link>
      <description>arXiv:2501.18840v1 Announce Type: new 
Abstract: Federated testbeds enable collaborative research by providing access to diverse resources, including computing power, storage, and specialized hardware like GPUs, programmable switches and smart Network Interface Cards (NICs). Efficiently sharing these resources across federated institutions is challenging, particularly when resources are scarce and costly. GPUs are crucial for AI and machine learning research, but their high demand and expense make efficient management essential. Similarly, advanced experimentation on programmable data plane requires very expensive programmable switches (e.g., based on P4) and smart NICs.
  This paper introduces SHARY (SHaring Any Resource made easY), a dynamic reservation system that simplifies resource booking and management in federated environments. We show that SHARY can be adopted for heterogenous resources, thanks to an adaptation layer tailored for the specific resource considered. Indeed, it can be integrated with FIGO (Federated Infrastructure for GPU Orchestration), which enhances GPU availability through a demand-driven sharing model. By enabling real-time resource sharing and a flexible booking system, FIGO improves access to GPUs, reduces costs, and accelerates research progress. SHARY can be also integrated with SUP4RNET platform to reserve the access of P4 switches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18840v1</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stefano Salsano, Andrea Mayer, Paolo Lungaroni, Pierpaolo Loreti, Lorenzo Bracciale, Andrea Detti, Marco Orazi, Paolo Giaccone, Fulvio Risso, Alessandro Cornacchia, Carla Fabiana Chiasserini</dc:creator>
    </item>
    <item>
      <title>Swift: Rethinking RDMA Control Plane for Elastic Computing</title>
      <link>https://arxiv.org/abs/2501.19051</link>
      <description>arXiv:2501.19051v1 Announce Type: new 
Abstract: Elastic computing enables dynamic scaling to meet workload demands, and Remote Direct Memory Access (RDMA) enhances this by providing high-throughput, low-latency network communication. However, integrating RDMA into elastic computing remains a challenge, particularly in control plane operations for RDMA connection setup.
  This paper revisits the assumptions of prior work on high-performance RDMA for elastic computing, and reveals that extreme microsecond-level control plane optimizations are often unnecessary. By challenging the conventional beliefs on the slowness of user-space RDMA control plane and the difficulty of user-space RDMA resource sharing, we uncover new design opportunities. Our key insight is that user-space RDMA connection setup can be significantly improved with caching, while RDMA resources can be efficiently shared among processes using fork. In light of this, we propose Swift, a simple yet effective solution that co-designs RDMA with a serverless framework to optimize performance for elastic computing. At its very core, Swift handles cold and warm serverless requests by swiftly initializing the RDMA control plane with cache-optimized libibverbs, and manages fork requests by leveraging the RDMA's fork capability. Implemented with OpenWhisk, Swift delivers 30.56-46.50% higher average throughput and 18.55-37.21% lower latency, at a cost of 6.5% control plane overhead, compared to prior solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19051v1</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junxue Zhang, Han Tian, Xinyang Huang, Wenxue Li, Kaiqiang Xu, Dian Shen, Yong Wang, Kai Chen</dc:creator>
    </item>
    <item>
      <title>Reliability Modeling for Beyond-5G Mission Critical Networks Using Effective Capacity</title>
      <link>https://arxiv.org/abs/2501.19109</link>
      <description>arXiv:2501.19109v1 Announce Type: new 
Abstract: Accurate reliability modeling for ultra-reliable low latency communication (URLLC) and hyper-reliable low latency communication (HRLLC) networks is challenging due to the complex interactions between network layers required to meet stringent requirements. In this paper, we propose such a model. We consider the acknowledged mode of the radio link control (RLC) layer, utilizing separate buffers for transmissions and retransmissions, along with the behavior of physical channels. Our approach leverages the effective capacity (EC) framework, which quantifies the maximum constant arrival rate a time-varying wireless channel can support while meeting statistical quality of service (QoS) constraints. We derive a reliability model that incorporates delay violations, various latency components, and multiple transmission attempts. Our method identifies optimal operating conditions that satisfy URLLC/HRLLC constraints while maintaining near-optimal EC, ensuring the system can handle peak traffic with a guaranteed QoS. Our model reveals critical trade-offs between EC and reliability across various use cases, providing guidance for URLLC/HRLLC network design for service providers and system designers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19109v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anudeep Karnam, Jobish John, Kishor C. Joshi, George Exarchakos, Sonia Heemstra de Groot, Ignas Niemegeers</dc:creator>
    </item>
    <item>
      <title>On Measuring Available Capacity in High-speed Cloud Networks</title>
      <link>https://arxiv.org/abs/2501.19167</link>
      <description>arXiv:2501.19167v1 Announce Type: new 
Abstract: Measurement of available path capacity with high accuracy over high-speed links deployed in cloud and transport networks is vital for performance assessment and traffic engineering. Methods for measuring the available path capacity rely on sending and receiving time stamped probe packets. A requirement for accurate estimates of the available path capacity is the ability to generate probe packets at a desired rate and also time stamping with high precision and accuracy. This is challenging especially for measurement systems deployed using general purpose hardware. To touch upon the challenge this paper describes and evaluates four approaches for sending and receiving probe packets in high-speed networks (10+ Gbps). The evaluation shows that the baseline approach, based on the native UDP socket, is suitable for available path capacity measurements over links with capacities up to 2.5 Gbps. For higher capacities we show that an implementation based on Data Plane Development Kit (DPDK) gives good results up to 10 Gbps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19167v1</guid>
      <category>cs.NI</category>
      <category>cs.OS</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ganapathy Raman Madanagopal, Christofer Flinta, Andreas Johnsson, Farnaz Moradi, Daniel Turull</dc:creator>
    </item>
    <item>
      <title>APEX: Automated Parameter Exploration for Low-Power Wireless Protocols</title>
      <link>https://arxiv.org/abs/2501.19194</link>
      <description>arXiv:2501.19194v1 Announce Type: new 
Abstract: Careful parametrization of networking protocols is crucial to maximize the performance of low-power wireless systems and ensure that stringent application requirements can be met. This is a non-trivial task involving thorough characterization on testbeds and requiring expert knowledge. Unfortunately, the community still lacks a tool to facilitate parameter exploration while minimizing the necessary experimentation time on testbeds. Such a tool would be invaluable, as exhaustive parameter searches can be time-prohibitive or unfeasible given the limited availability of testbeds, whereas non-exhaustive unguided searches rarely deliver satisfactory results. In this paper, we present APEX, a framework enabling an automated and informed parameter exploration for low-power wireless protocols and allowing to converge to an optimal parameter set within a limited number of testbed trials. We design APEX using Gaussian processes to effectively handle noisy experimental data and estimate the optimality of a certain parameter combination. After developing a prototype of APEX, we demonstrate its effectiveness by parametrizing two IEEE 802.15.4 protocols for a wide range of application requirements. Our results show that APEX can return the best parameter set with up to 10.6x, 4.5x and 3.25x less testbed trials than traditional solutions based on exhaustive search, greedy approaches, and reinforcement learning, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19194v1</guid>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohamed Hassaan M. Hydher, Markus Schuss, Olga Saukh, Kay R\"omer, Carlo Alberto Boano</dc:creator>
    </item>
    <item>
      <title>Characterizing User Behavior: The Interplay Between Mobility Patterns and Mobile Traffic</title>
      <link>https://arxiv.org/abs/2501.19348</link>
      <description>arXiv:2501.19348v1 Announce Type: new 
Abstract: Mobile devices have become essential for capturing human activity, and eXtended Data Records (XDRs) offer rich opportunities for detailed user behavior modeling, which is useful for designing personalized digital services. Previous studies have primarily focused on aggregated mobile traffic and mobility analyses, often neglecting individual-level insights. This paper introduces a novel approach that explores the dependency between traffic and mobility behaviors at the user level. By analyzing 13 individual features that encompass traffic patterns and various mobility aspects, we enhance the understanding of how these behaviors interact. Our advanced user modeling framework integrates traffic and mobility behaviors over time, allowing for fine-grained dependencies while maintaining population heterogeneity through user-specific signatures. Furthermore, we develop a Markov model that infers traffic behavior from mobility and vice versa, prioritizing significant dependencies while addressing privacy concerns. Using a week-long XDR dataset from 1,337,719 users across several provinces in Chile, we validate our approach, demonstrating its robustness and applicability in accurately inferring user behavior and matching mobility and traffic profiles across diverse urban contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19348v1</guid>
      <category>cs.NI</category>
      <category>cs.IR</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anne Josiane Kouam, Aline Carneiro Viana, Mariano G. Beir\'o, Leo Ferres, Luca Pappalardo</dc:creator>
    </item>
    <item>
      <title>Synthetic User Behavior Sequence Generation with Large Language Models for Smart Homes</title>
      <link>https://arxiv.org/abs/2501.19298</link>
      <description>arXiv:2501.19298v1 Announce Type: cross 
Abstract: In recent years, as smart home systems have become more widespread, security concerns within these environments have become a growing threat. Currently, most smart home security solutions, such as anomaly detection and behavior prediction models, are trained using fixed datasets that are precollected. However, the process of dataset collection is time-consuming and lacks the flexibility needed to adapt to the constantly evolving smart home environment. Additionally, the collection of personal data raises significant privacy concerns for users. Lately, large language models (LLMs) have emerged as a powerful tool for a wide range of tasks across diverse application domains, thanks to their strong capabilities in natural language processing, reasoning, and problem-solving. In this paper, we propose an LLM-based synthetic dataset generation IoTGen framework to enhance the generalization of downstream smart home intelligent models. By generating new synthetic datasets that reflect changes in the environment, smart home intelligent models can be retrained to overcome the limitations of fixed and outdated data, allowing them to better align with the dynamic nature of real-world home environments. Specifically, we first propose a Structure Pattern Perception Compression (SPPC) method tailored for IoT behavior data, which preserves the most informative content in the data while significantly reducing token consumption. Then, we propose a systematic approach to create prompts and implement data generation to automatically generate IoT synthetic data with normative and reasonable properties, assisting task models in adaptive training to improve generalization and real-world performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19298v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiyao Xu, Dan Zhao, Qingsong Zou, Jingyu Xiao, Yong Jiang, Zhenhui Yuan, Qing Li</dc:creator>
    </item>
    <item>
      <title>Distributed Asynchronous Service Deployment in the Edge-Cloud Multi-tier Network</title>
      <link>https://arxiv.org/abs/2312.11187</link>
      <description>arXiv:2312.11187v2 Announce Type: replace 
Abstract: In an edge-cloud multi-tier network, datacenters provide services to mobile users, with each service having specific latency constraints and computational requirements. Deploying such a variety of services while matching their requirements with the available computing resources is challenging. In addition, time-critical services may have to be migrated as the users move, to keep fulfilling their latency constraints. Unlike previous work relying on an orchestrator with an always-updated global view of the available resources and the users' locations, this work envisions a distributed solution to the above problems. In particular, we propose a distributed asynchronous framework for service deployment in the edge-cloud that increases the system resilience by avoiding a single point of failure, as in the case of a central orchestrator. Our solution ensures cost-efficient feasible placement of services, while using negligible bandwidth. Our results, obtained through trace-driven, large-scale simulations, show that the proposed solution provides performance very close to those obtained by state-of-the-art centralized solutions, and at the cost of a small communication overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.11187v2</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Itamar Cohen, Paolo Giaccone, Carla Fabiana Chiasserini</dc:creator>
    </item>
    <item>
      <title>AutoFlow: An Autoencoder-based Approach for IP Flow Record Compression with Minimal Impact on Traffic Classification</title>
      <link>https://arxiv.org/abs/2410.00030</link>
      <description>arXiv:2410.00030v2 Announce Type: replace 
Abstract: Network monitoring generates massive volumes of IP flow records, posing significant challenges for storage and analysis. This paper presents a novel deep learning-based approach to compressing these records using autoencoders, enabling direct analysis of compressed data without requiring decompression. Unlike traditional compression methods, our approach reduces data volume while retaining the utility of compressed data for downstream analysis tasks, including distinguishing modern application protocols and encrypted traffic from popular services. Through extensive experiments on a real-world network traffic dataset, we demonstrate that our autoencoder-based compression achieves a 1.313x reduction in data size while maintaining 99.27% accuracy in a multi-class traffic classification task, compared to 99.77% accuracy with uncompressed data. This marginal decrease in performance is offset by substantial gains in storage and processing efficiency. The implications of this work extend to more efficient network monitoring and scalable, real-time network management solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00030v2</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adrian Pekar</dc:creator>
    </item>
    <item>
      <title>On Efficient Topology Management in Service-Oriented 6G Networks: An Edge Video Distribution Case Study</title>
      <link>https://arxiv.org/abs/2410.10338</link>
      <description>arXiv:2410.10338v2 Announce Type: replace 
Abstract: An efficient topology management in future 6G networks is one of the fundamental challenges for a dynamic network creation based on location services, whereby each autonomous network entity, i.e., a sub-network, can be created for a specific application scenario. In this paper, we study the performance of a novel topology changes management system in a sample 6G network being dynamically organized in autonomous sub-networks. We propose and analyze an algorithm for intelligent prediction of topology changes and provide a comparative analysis with topology monitoring based approach. To this end, we present an industrially relevant case study on edge video distribution, as it is envisioned to be implemented in line with the 3GPP and ETSI MEC (Multi-access Edge Computing) standards. For changes prediction, we implement and analyze a novel topology change prediction algorithm, which can automatically optimize, train and, finally, select the best of different machine learning models available, based on the specific scenario under study. For link change scenario, the results show that three selected ML models exhibit high accuracy in detecting changes in link delay and bandwidth using measured throughput and RTT. ANN demonstrates the best performance in identifying cases with no changes, slightly outperforming random forest and XGBoost. For user mobility scenario, XGBoost is more efficient in learning patterns for topology change prediction while delivering much faster results compared to the more computationally demanding deep learning models, such as LSTM and CNN. In terms of cost efficiency, our ML-based approach represents a significantly cost-effective alternative to traditional monitoring approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10338v2</guid>
      <category>cs.NI</category>
      <category>cs.PF</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zied Ennaceur, Mounir Bensalem, Admela Jukan, Claus Keuker, Huanzhuo Wu, Rastin Pries</dc:creator>
    </item>
    <item>
      <title>UAV-Assisted MEC Architecture for Collaborative Task Offloading in Urban IoT Environment</title>
      <link>https://arxiv.org/abs/2501.15164</link>
      <description>arXiv:2501.15164v2 Announce Type: replace 
Abstract: Mobile edge computing (MEC) is a promising technology to meet the increasing demands and computing limitations of complex Internet of Things (IoT) devices. However, implementing MEC in urban environments can be challenging due to factors like high device density, complex infrastructure, and limited network coverage. Network congestion and connectivity issues can adversely affect user satisfaction. Hence, in this article, we use unmanned aerial vehicle (UAV)-assisted collaborative MEC architecture to facilitate task offloading of IoT devices in urban environments. We utilize the combined capabilities of UAVs and ground edge servers (ESs) to maximize user satisfaction and thereby also maximize the service provider's (SP) profit. We design IoT task-offloading as joint IoT-UAV-ES association and UAV-network topology optimization problem. Due to NP-hard nature, we break the problem into two subproblems: offload strategy optimization and UAV topology optimization. We develop a Three-sided Matching with Size and Cyclic preference (TMSC) based task offloading algorithm to find stable association between IoTs, UAVs, and ESs to achieve system objective. We also propose a K-means based iterative algorithm to decide the minimum number of UAVs and their positions to provide offloading services to maximum IoTs in the system. Finally, we demonstrate the efficacy of the proposed task offloading scheme over benchmark schemes through simulation-based evaluation. The proposed scheme outperforms by 19%, 12%, and 25% on average in terms of percentage of served IoTs, average user satisfaction, and SP profit, respectively, with 25% lesser UAVs, making it an effective solution to support IoT task requirements in urban environments using UAV-assisted MEC architecture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15164v2</guid>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <category>eess.SY</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>IEEE TNSM 2025</arxiv:journal_reference>
      <dc:creator>Subhrajit Barick, Chetna Singhal</dc:creator>
    </item>
  </channel>
</rss>
