<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 06 Nov 2025 05:00:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Faster Weak Expander Decompositions and Approximate Max Flow</title>
      <link>https://arxiv.org/abs/2511.02943</link>
      <description>arXiv:2511.02943v1 Announce Type: new 
Abstract: We give faster algorithms for weak expander decompositions and approximate max flow on undirected graphs. First, we show that it is possible to "warm start" the cut-matching game when computing weak expander decompositions, avoiding the cost of the recursion depth. Our algorithm is also flexible enough to support weaker flow subroutines than previous algorithms.
  Our second contribution is to streamline the recent non-recursive approximate max flow algorithm of Li, Rao, and Wang (SODA, 2025) and adapt their framework to use our new weak expander decomposition primitive. Consequently, we give an approximate max flow algorithm within a few logarithmic factors of the limit of expander decomposition-based approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02943v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Henry Fleischmann, George Z. Li, Jason Li</dc:creator>
    </item>
    <item>
      <title>Tight Better-Than-Worst-Case Bounds for Element Distinctness and Set Intersection</title>
      <link>https://arxiv.org/abs/2511.02954</link>
      <description>arXiv:2511.02954v1 Announce Type: new 
Abstract: The element distinctness problem takes as input a list $I$ of $n$ values from a totally ordered universe and the goal is to decide whether $I$ contains any duplicates. It is a well-studied problem with a classical worst-case $\Omega(n \log n)$ comparison-based lower bound by Fredman. At first glance, this lower bound appears to rule out any algorithm more efficient than the naive approach of sorting $I$ and comparing adjacent elements. However, upon closer inspection, the $\Omega(n \log n)$ bound does not apply if the input has many duplicates. We therefore ask: Are there comparison-based lower bounds for element distinctness that are sensitive to the amount of duplicates in the input?
  To address this question, we derive instance-specific lower bounds. For any input instance $I$, we represent the combinatorial structure of the duplicates in $I$ by an undirected graph $G(I)$ that connects identical elements. Each such graph $G$ is a union of cliques, and we study algorithms by their worst-case running time over all inputs $I'$ with $G(I') \cong G$. We establish an adversarial lower bound showing that, for any deterministic algorithm $\mathcal{A}$, there exists a graph $G$ and an algorithm $\mathcal{A}'$ that, for all inputs $I$ with $G(I) \cong G$, is a factor $O(\log \log n)$ faster than $\mathcal{A}$. Consequently, no deterministic algorithm can be $o(\log \log n)$-competitive for all graphs $G$. We complement this with an $O(\log \log n)$-competitive deterministic algorithm, thereby obtaining tight bounds for element distinctness that go beyond classical worst-case analysis.
  We subsequently study the related problem of set intersection. We show that no deterministic set intersection algorithm can be $o(\log n)$-competitive, and provide an $O(\log n)$-competitive deterministic algorithm. This shows a separation between element distinctness and the set intersection problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02954v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ivor van der Hoog, Eva Rotenberg, Daniel Rutschmann</dc:creator>
    </item>
    <item>
      <title>Implementation and Brief Experimental Analysis of the Duan et al. (2025) Algorithm for Single-Source Shortest Paths</title>
      <link>https://arxiv.org/abs/2511.03007</link>
      <description>arXiv:2511.03007v1 Announce Type: new 
Abstract: We present an implementation and a brief experimental analysis of the deterministic algorithm proposed by Duan et al. (2025) for the Single-Source Shortest Path (SSSP) problem, which achieves the best known asymptotic upper bound in the comparison-addition model, with running time $O(m \log^{2/3} n)$. We provide a faithful C++ implementation of this algorithm, following all structural details described in the original paper, and compare its empirical performance with the classical Dijkstra's algorithm using binary heaps. The experiments were conducted on both synthetic sparse random graphs and real-world road network instances from the DIMACS benchmark. Our results show that, despite its superior asymptotic complexity, the new algorithm presents significantly larger constant factors, making Dijkstra's algorithm faster for all tested sparse graph sizes, including instances with tens of millions of vertices. Our implementation achieves $O(m \log^{2/3} n)$ expected time, due to the use of hash tables, and some possibilities for making it worst-case are being considered. (This is a ongoing work.)</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03007v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucas Castro, Thailsson Clementino, Rosiane de Freitas</dc:creator>
    </item>
    <item>
      <title>A Branch-and-Bound Approach for Maximum Low-Diameter Dense Subgraph Problems</title>
      <link>https://arxiv.org/abs/2511.03157</link>
      <description>arXiv:2511.03157v1 Announce Type: new 
Abstract: A graph with $n$ vertices is an $f(\cdot)$-dense graph if it has at least $f(n)$ edges, $f(\cdot)$ being a well-defined function. The notion $f(\cdot)$-dense graph encompasses various clique models like $\gamma$-quasi cliques, $k$-defective cliques, and dense cliques, arising in cohesive subgraph extraction applications. However, the $f(\cdot)$-dense graph may be disconnected or weakly connected. To conquer this, we study the problem of finding the largest $f(\cdot)$-dense subgraph with a diameter of at most two in the paper. Specifically, we present a decomposition-based branch-and-bound algorithm to optimally solve this problem. The key feature of the algorithm is a decomposition framework that breaks the graph into $n$ smaller subgraphs, allowing independent searches in each subgraph. We also introduce decomposition strategies including degeneracy and two-hop degeneracy orderings, alongside a branch-and-bound algorithm with a novel sorting-based upper bound to solve each subproblem. Worst-case complexity for each component is provided. Empirical results on 139 real-world graphs under two $f(\cdot)$ functions show our algorithm outperforms the MIP solver and pure branch-and-bound, solving nearly twice as many instances optimally within one hour.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03157v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yi Zhoua, Chunyu Luoa, Zhengren Wangb, Zhang-Hua Fuc</dc:creator>
    </item>
    <item>
      <title>Optimal Stopping with a Predicted Prior</title>
      <link>https://arxiv.org/abs/2511.03289</link>
      <description>arXiv:2511.03289v1 Announce Type: new 
Abstract: There are two major models of value uncertainty in the optimal stopping literature: the secretary model, which assumes no prior knowledge, and the prophet inequality model, which assumes full information about value distributions. In practice, decision makers often rely on machine-learned priors that may be erroneous. Motivated by this gap, we formulate the model of optimal stopping with a predicted prior to design algorithms that are both consistent, exploiting the prediction when accurate, and robust, retaining worst-case guarantees when it is not.
  Existing secretary and prophet inequality algorithms are either pessimistic in consistency or not robust to misprediction. A randomized combination only interpolates their guarantees linearly. We show that a family of bi-criteria algorithms achieves improved consistency-robustness trade-offs, both for maximizing the expected accepted value and for maximizing the probability of accepting the maximum value. We further prove that for the latter objective, no algorithm can simultaneously match the best prophet inequality algorithm in consistency, and the best secretary algorithm in robustness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03289v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tian Bai, Zhiyi Huang, Chui Shan Lee, Dongchen Li</dc:creator>
    </item>
    <item>
      <title>Improved Online Load Balancing in the Two-Norm</title>
      <link>https://arxiv.org/abs/2511.03345</link>
      <description>arXiv:2511.03345v1 Announce Type: new 
Abstract: We study the online load balancing problem on unrelated machines, with the objective of minimizing the square of the $\ell_2$ norm of the loads on the machines. The greedy algorithm of Awerbuch et al. (STOC'95) is optimal for deterministic algorithms and achieves a competitive ratio of $3 + 2 \sqrt{2} \approx 5.828$, and an improved $5$-competitive randomized algorithm based on independent rounding has been shown by Caragiannis (SODA'08). In this work, we present the first algorithm breaking the barrier of $5$ on the competitive ratio, achieving a bound of $4.9843$. To obtain this result, we use a new primal-dual framework to analyze this problem based on a natural semidefinite programming relaxation, together with an online implementation of a correlated randomized rounding procedure of Im and Shadloo (SODA'20). This novel primal-dual framework also yields new, simple and unified proofs of the competitive ratio of the $(3 + 2 \sqrt{2})$-competitive greedy algorithm, the $5$-competitive randomized independent rounding algorithm, and that of a new $4$-competitive optimal fractional algorithm. We also provide lower bounds showing that the previous best randomized algorithm is optimal among independent rounding algorithms, that our new fractional algorithm is optimal, and that a simple greedy algorithm is optimal for the closely related online scheduling problem $R || \sum w_j C_j$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03345v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sander Borst, Danish Kashaev</dc:creator>
    </item>
    <item>
      <title>Hesse's Redemption: Efficient Convex Polynomial Programming</title>
      <link>https://arxiv.org/abs/2511.03440</link>
      <description>arXiv:2511.03440v1 Announce Type: new 
Abstract: Efficient algorithms for convex optimization, such as the ellipsoid method, require an a priori bound on the radius of a ball around the origin guaranteed to contain an optimal solution if one exists. For linear and convex quadratic programming, such solution bounds follow from classical characterizations of optimal solutions by systems of linear equations. For other programs, e.g., semidefinite ones, examples due to Khachiyan show that optimal solutions may require huge coefficients with an exponential number of bits, even if we allow approximations. Correspondingly, semidefinite programming is not even known to be in NP. The unconstrained minimization of convex polynomials of degree four and higher has remained a fundamental open problem between these two extremes: its optimal solutions do not admit a linear characterization and, at the same time, Khachiyan-type examples do not apply. We resolve this problem by developing new techniques to prove solution bounds when no linear characterizations are available. Even for programs minimizing a convex polynomial (of arbitrary degree) over a polyhedron, we prove that the existence of an optimal solution implies that an approximately optimal one with polynomial bit length also exists. These solution bounds, combined with the ellipsoid method, yield the first polynomial-time algorithm for convex polynomial programming, settling a question posed by Nesterov (Math. Program., 2019). Before, no polynomial-time algorithm was known even for unconstrained minimization of a convex polynomial of degree four.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03440v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>math.AG</category>
      <category>math.OC</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucas Slot, David Steurer, Manuel Wiedmer</dc:creator>
    </item>
    <item>
      <title>Dynamic Meta-Kernelization</title>
      <link>https://arxiv.org/abs/2511.03461</link>
      <description>arXiv:2511.03461v1 Announce Type: new 
Abstract: Kernelization studies polynomial-time preprocessing algorithms. Over the last 20 years, the most celebrated positive results of the field have been linear kernels for classical NP-hard graph problems on sparse graph classes. In this paper, we lift these results to the dynamic setting.
  As the canonical example, Alber, Fellows, and Niedermeier [J. ACM 2004] gave a linear kernel for dominating set on planar graphs. We provide the following dynamic version of their kernel: Our data structure is initialized with an $n$-vertex planar graph $G$ in $O(n \log n)$ amortized time, and, at initialization, outputs a planar graph $K$ with $\mathrm{OPT}(K) = \mathrm{OPT}(G)$ and $|K| = O(\mathrm{OPT}(G))$, where $\mathrm{OPT}(\cdot)$ denotes the size of a minimum dominating set. The graph $G$ can be updated by insertions and deletions of edges and isolated vertices in $O(\log n)$ amortized time per update, under the promise that it remains planar. After each update to $G$, the data structure outputs $O(1)$ updates to $K$, maintaining $\mathrm{OPT}(K) = \mathrm{OPT}(G)$, $|K| = O(\mathrm{OPT}(G))$, and planarity of $K$.
  Furthermore, we obtain similar dynamic kernelization algorithms for all problems satisfying certain conditions on (topological-)minor-free graph classes. Besides kernelization, this directly implies new dynamic constant-approximation algorithms and improvements to dynamic FPT algorithms for such problems.
  Our main technical contribution is a dynamic data structure for maintaining an approximately optimal protrusion decomposition of a dynamic topological-minor-free graph. Protrusion decompositions were introduced by Bodlaender, Fomin, Lokshtanov, Penninkx, Saurabh, and Thilikos [J. ACM 2016], and have since developed into a part of the core toolbox in kernelization and parameterized algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03461v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christian Bertram, Deborah Haun, Mads Vestergaard Jensen, Tuukka Korhonen</dc:creator>
    </item>
    <item>
      <title>Online Flow Time Minimization: Tight Bounds for Non-Preemptive Algorithms</title>
      <link>https://arxiv.org/abs/2511.03485</link>
      <description>arXiv:2511.03485v1 Announce Type: new 
Abstract: This paper studies the classical online scheduling problem of minimizing total flow time for $n$ jobs on $m$ identical machines. Prior work often cites the $\Omega(n)$ lower bound for non-preemptive algorithms to argue for the necessity of preemption or resource augmentation, which shows the trivial $O(n)$-competitive greedy algorithm is tight. However, this lower bound applies only to \emph{deterministic} algorithms in the \emph{single-machine} case, leaving several fundamental questions unanswered. Can randomness help in the non-preemptive setting, and what is the optimal online deterministic algorithm when $m \geq 2$? We resolve both questions. We present a polynomial-time randomized algorithm with competitive ratio $\Theta(\sqrt{n/m})$ and prove a matching randomized lower bound, settling the randomized non-preemptive setting for every $m$. This also improves the best-known offline approximation ratio from $O(\sqrt{n/m}\log(n/m))$ to $O(\sqrt{n/m})$. On the deterministic side, we present a non-preemptive algorithm with competitive ratio $O(n/m^{2}+\sqrt{n/m}\log m)$ and prove a nearly matching lower bound.
  Our framework also extends to the kill-and-restart model, where we reveal a sharp transition of deterministic algorithms: we design an asymptotically optimal algorithm with the competitive ratio $O(\sqrt{n/m})$ for $m\ge 2$, yet establish a strong $\Omega(n/\log n)$ lower bound for $m=1$. Moreover, we show that randomization provides no further advantage, as the lower bound coincides with that of the non-preemptive setting.
  While our main results assume prior knowledge of $n$, we also investigate the setting where $n$ is unknown. We show kill-and-restart is powerful enough to break the $O(n)$ barrier for $m \geq 2$ even without knowing $n$. Conversely, we prove randomization alone is insufficient, as no algorithm can achieve an $o(n)$ competitive ratio in this setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03485v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yutong Geng, Enze Sun, Zonghan Yang, Yuhao Zhang</dc:creator>
    </item>
    <item>
      <title>Randomized Rounding over Dynamic Programs</title>
      <link>https://arxiv.org/abs/2511.03490</link>
      <description>arXiv:2511.03490v1 Announce Type: new 
Abstract: We show that under mild assumptions for a problem whose solutions admit a dynamic programming-like recurrence relation, we can still find a solution under additional packing constraints, which need to be satisfied approximately. The number of additional constraints can be very large, for example, polynomial in the problem size. Technically, we reinterpret the dynamic programming subproblems and their solutions as a network design problem. Inspired by techniques from, for example, the Directed Steiner Tree problem, we construct a strong LP relaxation, on which we then apply randomized rounding. Our approximation guarantees on the packing constraints have roughly the form of a $(n^{\epsilon} \mathrm{polylog}\ n)$-approximation in time $n^{O(1/\epsilon)}$, for any $\epsilon &gt; 0$. By setting $\epsilon=\log \log n/\log n$, we obtain a polylogarithmic approximation in quasi-polynomial time, or by setting $\epsilon$ as a constant, an $n^\epsilon$-approximation in polynomial time.
  While there are necessary assumptions on the form of the DP, it is general enough to capture many textbook dynamic programs from Shortest Path to Longest Common Subsequence. Our algorithm then implies that we can impose additional constraints on the solutions to these problems. This allows us to model various problems from the literature in approximation algorithms, many of which were not thought to be connected to dynamic programming. In fact, our result can even be applied indirectly to some problems that involve covering instead of packing constraints, for example, the Directed Steiner Tree problem, or those that do not directly follow a recurrence relation, for example, variants of the Matching problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03490v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Etienne Bamas, Shi Li, Lars Rohwedder</dc:creator>
    </item>
    <item>
      <title>Engineering Algorithms for $\ell$-Isolated Maximal Clique Enumeration</title>
      <link>https://arxiv.org/abs/2511.03525</link>
      <description>arXiv:2511.03525v1 Announce Type: new 
Abstract: Maximal cliques play a fundamental role in numerous application domains, where their enumeration can prove extremely useful. Yet their sheer number, even in sparse real-world graphs, can make them impractical to be exploited effectively. To address this issue, one approach is to enumerate $\ell$-isolated maximal cliques, whose vertices have (on average) less than $\ell$ edges toward the rest of the graph. By tuning parameter $\ell$, the degree of isolation can be controlled, and cliques that are overly connected to the outside are filtered out. Building on Tomita et al.'s very practical recursive algorithm for maximal clique enumeration, we propose four pruning heuristics, applicable individually or in combination, that discard recursive search branches that are guaranteed not to yield $\ell$-isolated maximal cliques. Besides proving correctness, we characterize both the pruning power and the computational cost of these heuristics, and we conduct an extensive experimental study comparing our methods with Tomita's baseline and with a state-of-the-art approach. Results show that two of our heuristics offer substantial efficiency improvements, especially on real-world graphs with social network properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03525v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco D'Elia, Irene Finocchi, Maurizio Patrignani</dc:creator>
    </item>
    <item>
      <title>Improved Bounds with a Simple Algorithm for Edge Estimation for Graphs of Unknown Size</title>
      <link>https://arxiv.org/abs/2511.03650</link>
      <description>arXiv:2511.03650v1 Announce Type: new 
Abstract: We propose a randomized algorithm with query access that given a graph $G$ with arboricity $\alpha$, and average degree $d$, makes $\widetilde{O}\left(\frac{\alpha}{\varepsilon^2d}\right)$ \texttt{Degree} and $\widetilde{O}\left(\frac{1}{\varepsilon^2}\right)$ \texttt{Random Edge} queries to obtain an estimate $\widehat{d}$ satisfying $\widehat{d} \in (1\pm\varepsilon)d$. This improves the $\widetilde{O}_{\varepsilon,\log n}\left(\sqrt{\frac{n}{d}}\right)$ query algorithm of [Beretta et al., SODA 2026] that has access to \texttt{Degree}, \texttt{Neighbour}, and \texttt{Random Edge} queries. Our algorithm does not require any graph parameter as input, not even the size of the vertex set, and attains both simplicity and practicality through a new estimation technique. We complement our upper bounds with a lower bound that shows for all valid $n,d$, and $\alpha$, any algorithm that has access to \texttt{Degree}, \texttt{Neighbour}, and \texttt{Random Edge} queries, must make at least $\Omega\left(\min\left(d,\frac{\alpha}{d}\right)\right)$ queries to obtain a $(1\pm\varepsilon)$-multiplicative estimate of $d$, even with the knowledge of $n$ and $\alpha$. We also show that even with \texttt{Pair} and \texttt{FullNbr} queries, an algorithm must make $\Omega\left(\min\left(d,\frac{\alpha}{d}\right)\right)$ queries to obtain a $(1\pm\varepsilon)$-multiplicative estimate of $d$. Our work addresses both the questions raised by the work of [Beretta et al., SODA 2026].</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03650v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Debarshi Chanda</dc:creator>
    </item>
    <item>
      <title>An Improved Quality Hierarchical Congestion Approximator in Near-Linear Time</title>
      <link>https://arxiv.org/abs/2511.03716</link>
      <description>arXiv:2511.03716v1 Announce Type: new 
Abstract: A congestion approximator for a graph is a compact data structure that approximately predicts the edge congestion required to route any set of flow demands in a network. A congestion approximator is hierarchical if it consists of a laminar family of cuts in the graph. There is a tradeoff between the running time for computing a congestion approximator and its approximation quality. Currently, for an $n$-node graph there exists a polynomial time algorithm that achieves a $O(\log^{1.5}n \log \log n)$ approximation and a near-linear time algorithm that achieves w.h.p. a $O(\log^4 n)$ approximation. In this paper we give the first near-linear time algorithm, that achieves w.h.p. a $O(\log^2 n \log \log n)$ approximation, using an hierarchical congestion approximator with $O(n \log n)$ cuts. Based on a reduction from oblivious routing, we also present a lower bound of $\Omega(\log n)$ for the approximation quality of hierarchical congestion approximators.
  Our algorithm can also be implemented in the parallel setting achieving the same approximation quality, polylogarithmic span and near-linear work. This improves upon the best prior parallel algorithm, which has a $O(\log^9n)$ approximation.
  Crucial for achieving a near linear running time is a new partitioning routine that, unlike previous such routines, manages to avoid recursing on large subgraphs. To achieve the improved approximation quality, we introduce the new concept of border routability of a cut and give an improved sparsest cut oracle for general vertex weights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03716v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Monika Henzinger, Robin M\"unk, Harald R\"acke</dc:creator>
    </item>
    <item>
      <title>Supersimulators</title>
      <link>https://arxiv.org/abs/2509.17994</link>
      <description>arXiv:2509.17994v2 Announce Type: cross 
Abstract: We prove that every randomized Boolean function admits a supersimulator: a randomized polynomial-size circuit whose output on random inputs cannot be efficiently distinguished from reality with constant advantage, even by polynomially larger distinguishers. Our result builds on the landmark complexity-theoretic regularity lemma of Trevisan, Tulsiani and Vadhan (2009), which, in contrast, provides a simulator that fools smaller distinguishers. We circumvent lower bounds for the simulator size by letting the distinguisher size bound vary with the target function, while remaining below an absolute upper bound independent of the target function. This dependence on the target function arises naturally from our use of an iteration technique originating in the graph regularity literature.
  The simulators provided by the regularity lemma and recent refinements thereof, known as multiaccurate and multicalibrated predictors, respectively, as per Hebert-Johnson et al. (2018), have previously been shown to have myriad applications in complexity theory, cryptography, learning theory, and beyond. We first show that a recent multicalibration-based characterization of the computational indistinguishability of product distributions actually requires only (calibrated) multiaccuracy. We then show that supersimulators yield an even tighter result in this application domain, closing a complexity gap present in prior versions of the characterization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17994v2</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cynthia Dwork, Pranay Tankala</dc:creator>
    </item>
    <item>
      <title>The Contiguous Art Gallery Problem is in {\Theta}(n log n)</title>
      <link>https://arxiv.org/abs/2511.02960</link>
      <description>arXiv:2511.02960v1 Announce Type: cross 
Abstract: Recently, a natural variant of the Art Gallery problem, known as the \emph{Contiguous Art Gallery problem} was proposed. Given a simple polygon $P$, the goal is to partition its boundary $\partial P$ into the smallest number of contiguous segments such that each segment is completely visible from some point in $P$. Unlike the classical Art Gallery problem, which is NP-hard, this variant is polynomial-time solvable. At SoCG~2025, three independent works presented algorithms for this problem, each achieving a running time of $O(k n^5 \log n)$ (or $O(n^6\log n)$), where $k$ is the size of an optimal solution. Interestingly, these results were obtained using entirely different approaches, yet all led to roughly the same asymptotic complexity, suggesting that such a running time might be inherent to the problem.
  We show that this is not the case. In the real RAM-model, the prevalent model in computational geometry, we present an $O(n \log n)$-time algorithm, achieving an $O(k n^4)$ factor speed-up over the previous state-of-the-art. We also give a straightforward sorting-based lower bound by reducing from the set intersection problem. We thus show that the Contiguous Art Gallery problem is in $\Theta(n \log n)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02960v1</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarita de Berg, Jacobus Conradi, Ivor van der Hoog, Eva Rotenberg</dc:creator>
    </item>
    <item>
      <title>Min-Max Optimization Is Strictly Easier Than Variational Inequalities</title>
      <link>https://arxiv.org/abs/2511.03052</link>
      <description>arXiv:2511.03052v1 Announce Type: cross 
Abstract: Classically, a mainstream approach for solving a convex-concave min-max problem is to instead solve the variational inequality problem arising from its first-order optimality conditions. Is it possible to solve min-max problems faster by bypassing this reduction? This paper initiates this investigation. We show that the answer is yes in the textbook setting of unconstrained quadratic objectives: the optimal convergence rate for first-order algorithms is strictly better for min-max problems than for the corresponding variational inequalities. The key reason that min-max algorithms can be faster is that they can exploit the asymmetry of the min and max variables--a property that is lost in the reduction to variational inequalities. Central to our analyses are sharp characterizations of optimal convergence rates in terms of extremal polynomials which we compute using Green's functions and conformal mappings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03052v1</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Henry Shugart, Jason M. Altschuler</dc:creator>
    </item>
    <item>
      <title>Non-Monotonicity in Fair Division of Graphs</title>
      <link>https://arxiv.org/abs/2511.03629</link>
      <description>arXiv:2511.03629v1 Announce Type: cross 
Abstract: We consider the problem of fairly allocating the vertices of a graph among $n$ agents, where the value of a bundle is determined by its cut value -- the number of edges with exactly one endpoint in the bundle. This model naturally captures applications such as team formation and network partitioning, where valuations are inherently non-monotonic: the marginal values may be positive, negative, or zero depending on the composition of the bundle. We focus on the fairness notion of envy-freeness up to one item (EF1) and explore its compatibility with several efficiency concepts such as Transfer Stability (TS) that prohibits single-item transfers that benefit one agent without making the other worse-off. For general graphs, our results uncover a non-monotonic relationship between the number of agents $n$ and the existence of allocations satisfying EF1 and transfer stability (TS): such allocations always exist for $n=2$, may fail to exist for $n=3$, but exist again for all $n\geq 4$. We further show that existence can be guaranteed for any $n$ by slightly weakening the efficiency requirement or by restricting the graph to forests. All of our positive results are achieved via efficient algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03629v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hadi Hosseini, Shraddha Pathak, Yu Zhou</dc:creator>
    </item>
    <item>
      <title>Efficient Testing Implies Structured Symmetry</title>
      <link>https://arxiv.org/abs/2511.03653</link>
      <description>arXiv:2511.03653v1 Announce Type: cross 
Abstract: Given a small random sample of $n$-bit strings labeled by an unknown Boolean function, which properties of this function can be tested computationally efficiently? We show an equivalence between properties that are efficiently testable from few samples and properties with structured symmetry, which depend only on the function's average values on parts of a low-complexity partition of the domain. Without the efficiency constraint, a similar characterization in terms of unstructured symmetry was obtained by Blais and Yoshida (2019). Our main technical tool is supersimulation, which builds on methods from the algorithmic fairness literature to approximate arbitrarily complex functions by small-circuit simulators that fool significantly larger distinguishers.
  We extend the characterization along other axes as well. We show that allowing parts to overlap exponentially reduces their required number, broadening the scope of the construction from properties testable with $O(\log n)$ samples to properties testable with $O(n)$ samples. For larger sample sizes, we show that any efficient tester is essentially checking for indistinguishability from a bounded collection of small circuits, in the spirit of a characterization of testable graph properties. Finally, we show that our results for Boolean function testing generalize to high-entropy distribution testing on arbitrary domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03653v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cynthia Dwork, Pranay Tankala</dc:creator>
    </item>
    <item>
      <title>Time-Aware Projections: Truly Node-Private Graph Statistics under Continual Observation</title>
      <link>https://arxiv.org/abs/2403.04630</link>
      <description>arXiv:2403.04630v2 Announce Type: replace 
Abstract: We describe the first algorithms that satisfy the standard notion of node-differential privacy in the continual release setting (i.e., without an assumed promise on input streams). Previous work addresses node-private continual release by assuming an unenforced promise on the maximum degree in a graph, but leaves open whether such a bound can be verified or enforced privately. Our algorithms are accurate on sparse graphs, for several fundamental graph problems: counting edges, triangles, other subgraphs, and connected components; and releasing degree histograms. Our unconditionally private algorithms generally have optimal error, up to polylogarithmic factors and lower-order terms.
  We provide general transformations that take a base algorithm for the continual release setting, which need only be private for streams satisfying a promised degree bound, and produce an algorithm that is unconditionally private yet mimics the base algorithm when the stream meets the degree bound (and adds only linear overhead to the time and space complexity of the base algorithm). To do so, we design new projection algorithms for graph streams, based on the batch-model techniques of Day et al. 2016 and Blocki et al. 2013, which modify the stream to limit its degree. Our main technical innovation is to show that the projections are stable -- meaning that similar input graphs have similar projections -- when the input stream satisfies a privately testable safety condition. Our transformation then follows a novel online variant of the Propose-Test-Release framework (Dwork and Lei, 2009), privately testing the safety condition before releasing output at each step.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04630v2</guid>
      <category>cs.DS</category>
      <category>cs.CR</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/SP54263.2024.00196</arxiv:DOI>
      <dc:creator>Palak Jain, Adam Smith, Connor Wagaman</dc:creator>
    </item>
    <item>
      <title>PCF Learned Sort: a Learning Augmented Sort Algorithm with $O(n \log\log n)$ Expected Complexity</title>
      <link>https://arxiv.org/abs/2405.07122</link>
      <description>arXiv:2405.07122v2 Announce Type: replace 
Abstract: Sorting is one of the most fundamental algorithms in computer science. Recently, Learned Sorts, which use machine learning to improve sorting speed, have attracted attention. While existing studies show that Learned Sort is empirically faster than classical sorting algorithms, they do not provide theoretical guarantees about its computational complexity. We propose Piecewise Constant Function (PCF) Learned Sort, a theoretically guaranteed Learned Sort algorithm. We prove that the expected complexity of PCF Learned Sort is $\mathcal{O}(n \log \log n)$ under mild assumptions on the data distribution. We also confirm empirically that PCF Learned Sort has a computational complexity of $\mathcal{O}(n \log \log n)$ on both synthetic and real datasets. This is the first study to theoretically support the empirical success of Learned Sort, and provides evidence for why Learned Sort is fast. The code is available at https://github.com/atsukisato/PCF_Learned_Sort .</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07122v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Atsuki Sato, Yusuke Matsui</dc:creator>
    </item>
    <item>
      <title>LLM Query Scheduling with Prefix Reuse and Latency Constraints</title>
      <link>https://arxiv.org/abs/2502.04677</link>
      <description>arXiv:2502.04677v2 Announce Type: replace 
Abstract: The efficient deployment of large language models (LLMs) in online settings requires optimizing inference performance under stringent latency constraints, particularly the time-to-first-token (TTFT) and time-per-output-token (TPOT). This paper focuses on the query scheduling problem for LLM inference with prefix reuse, a technique that leverages shared prefixes across queries to reduce computational overhead. Our work reveals previously unknown limitations of the existing first-come-first-serve (FCFS) and longest-prefix-match (LPM) scheduling strategies with respect to satisfying latency constraints. We present a formal theoretical framework for LLM query scheduling under RadixAttention, a prefix reuse mechanism that stores and reuses intermediate representations in a radix tree structure. Our analysis establishes the NP-hardness of the scheduling problem with prefix reuse under TTFT constraints and proposes a novel scheduling algorithm, $k$-LPM, which generalizes existing methods by balancing prefix reuse and fairness in query processing. Theoretical guarantees demonstrate that $k$-LPM achieves improved TTFT performance under realistic traffic patterns captured by a data generative model. Empirical evaluations in a realistic serving setting validates our findings, showing significant reductions in P99 TTFT compared to baseline methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04677v2</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gregory Dexter, Shao Tang, Ata Fatahi Baarzi, Qingquan Song, Tejas Dharamsi, Aman Gupta</dc:creator>
    </item>
    <item>
      <title>Edge-weighted Online Stochastic Matching Under Jaillet-Lu LP</title>
      <link>https://arxiv.org/abs/2504.17392</link>
      <description>arXiv:2504.17392v2 Announce Type: replace 
Abstract: The online stochastic matching problem was introduced by [FMMM09], together with the $(1-\frac1e)$-competitive Suggested Matching algorithm. In the most general edge-weighted setting, this ratio has not been improved for more than one decade, until recently [Yan24] beat the $1-\frac1e$ bound and [QFZW23] further improved it to $0.650$. Both works measure the online competitiveness against the offline LP relaxation introduced by Jaillet and Lu [JL14]. The same LP has also played an important role in other settings as it is a natural choice for two-choice online algorithms.
  In this paper, we prove an upper bound of $0.663$ and a lower bound of $0.662$ for edge-weighted online stochastic matching under Jaillet-Lu LP. We propose a simple hard instance and identify the optimal online algorithm for this specific instance which has a competitive ratio of $&lt;0.663$. Despite the simplicity of the instance, we then show that a near-optimal algorithm for it, which has a competitive ratio of $&gt;0.662$, can be generalized to work on all instances without any loss.
  As our algorithm is generalized from a real near-optimal algorithm instead of manually combining trivial strategies, it has two natural advantages compared with previous works: (1) its matching strategy varies from time to time; (2) it utilizes global information about offline vertices. On the other hand, the upper bound suggests that more powerful LPs and multiple-choice strategies are needed if we want to further improve the ratio by $&gt;0.001$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17392v2</guid>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuyi Yan</dc:creator>
    </item>
    <item>
      <title>(Approximate) Matrix Multiplication via Convolutions</title>
      <link>https://arxiv.org/abs/2510.22193</link>
      <description>arXiv:2510.22193v2 Announce Type: replace 
Abstract: We study the capability of the Fast Fourier Transform (FFT) to accelerate exact and approximate matrix multiplication without using Strassen-like divide-and-conquer. We present a simple exact algorithm running in $O(n^{2.89})$ time, which only sums a few convolutions (FFTs) in $\mathbb{Z}_{m}^{k}$, building on the work of Cohn, Kleinberg, Szegedy and Umans (2005). As a corollary, combining this algorithm with linear sketching breaks the longstanding linear speed-accuracy tradeoff for "combinatorial" approximate matrix multiplication (AMM, Pagh'13, Sarlos'06, Clarkson-Woodruff'13), achieving error $\frac{1}{r^{1.1}}\left\lVert \mathbf{A} \right\rVert_{F}^{2}\left\lVert \mathbf{B}\right\rVert_{F}^{2}$ in $O(rn^{2})$ time, using nothing but FFTs.
  Motivated by the rich literature for approximating polynomials, our main contribution in this paper is extending the group-theoretic framework of Cohn and Umans (2003) to approximate matrix multiplication (AMM). Specifically, we introduce and study an approximate notion of the Triple Product Property, which in the abelian case is equivalent to finding a Sumset which minimizes (multi-)intersections with an arithmetic progression. We prove tight bounds on this quantity for abelian groups (yielding a simple and practical AMM algorithm via polynomial multiplication), and establish a weaker lower bound for non-abelian groups, extending a lemma of Gowers. Finally, we propose a concrete approach that uses low-degree approximation of multi-variate polynomials for AMM, which we believe will lead to practical, non-asymptotic AMM algorithms in real-world applications, most notably LLM inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22193v2</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yahel Uffenheimer, Omri Weinstein</dc:creator>
    </item>
    <item>
      <title>Nash Social Welfare with Submodular Valuations: Approximation Algorithms and Integrality Gaps</title>
      <link>https://arxiv.org/abs/2504.09669</link>
      <description>arXiv:2504.09669v3 Announce Type: replace-cross 
Abstract: We study the problem of allocating items to agents with submodular valuations with the goal of maximizing the weighted Nash social welfare (NSW). The best-known results for unweighted and weighted objectives are the $(4+\epsilon)$ approximation given by Garg, Husic, Li, V\'egh, and Vondr\'ak~[STOC 2023] and the $(233+\epsilon)$ approximation given by Feng, Hu, Li, and Zhang~[STOC 2025], respectively.
  In this work, we present a $(3.56+\epsilon)$-approximation algorithm for weighted NSW maximization with submodular valuations, simultaneously improving the previous approximation ratios of both the weighted and unweighted NSW problems. Our algorithm solves the configuration LP of Feng, Hu, Li, and Zhang~[STOC 2025] via a stronger separation oracle that loses an $e/(e-1)$ factor only on small items, and then rounds the solution via a new bipartite multigraph construction. Some key technical ingredients of our analysis include a greedy proxy function, additive within each configuration, that preserves the LP value while lower-bounding the rounded solution, together with refined concentration bounds and a series of mathematical programs analyzed partly by computer assistance.
  On the hardness side, we prove that the configuration LP for weighted NSW with submodular valuations has an integrality gap of at least $(2^{\ln 2}-\epsilon) \approx 1.617 - \epsilon$, which is larger than the current best-known $e/(e-1)-\epsilon \approx 1.582-\epsilon$ hardness~[SODA 2020]. For additive valuations, we show an integrality gap of $(e^{1/e}-\epsilon)$, which proves the tightness of the approximation ratio in~[ICALP 2024] for algorithms based on the configuration LP. For unweighted NSW with additive valuations, we show an integrality gap of $(2^{1/4}-\epsilon) \approx 1.189-\epsilon$, again larger than the current best-known $\sqrt{8/7} \approx 1.069$-hardness~[Math. Oper. Res. 2024].</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09669v3</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xiaohui Bei, Yuda Feng, Yang Hu, Shi Li, Ruilong Zhang</dc:creator>
    </item>
    <item>
      <title>New aspects of quantum topological data analysis: Betti number estimation, and testing and tracking of homology and cohomology classes</title>
      <link>https://arxiv.org/abs/2506.01432</link>
      <description>arXiv:2506.01432v3 Announce Type: replace-cross 
Abstract: We present new quantum algorithms for estimating homological invariants, specifically Betti and persistent Betti numbers, of a simplicial complex given through structured classical data. Our approach efficiently constructs block-encodings of (persistent) Laplacians, enabling estimation via stochastic rank methods with complexity polylogarithmic in the number of simplices across both sparse and dense regimes.
  Unlike prior spectral algorithms that suffer when Betti numbers are small, we introduce homology tracking and property testing techniques achieving exponential speedups under natural sparsity and structure assumptions. We also formulate homology triviality and equivalence testing as property testing problems, giving nearly linear-time quantum algorithms when the boundary rank is large. A cohomological formulation further yields rank-independent testing and polylog-time manipulation of $r$-cocycles via block-encoded projections. These results open a new direction in quantum topological data analysis and demonstrate provable quantum advantages in computing topological invariants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01432v3</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <category>math.AT</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junseo Lee, Nhat A. Nghiem</dc:creator>
    </item>
    <item>
      <title>Probabilistic Graph Cuts</title>
      <link>https://arxiv.org/abs/2511.02272</link>
      <description>arXiv:2511.02272v2 Announce Type: replace-cross 
Abstract: Probabilistic relaxations of graph cuts offer a differentiable alternative to spectral clustering, enabling end-to-end and online learning without eigendecompositions, yet prior work centered on RatioCut and lacked general guarantees and principled gradients. We present a unified probabilistic framework that covers a wide class of cuts, including Normalized Cut. Our framework provides tight analytic upper bounds on expected discrete cuts via integral representations and Gauss hypergeometric functions with closed-form forward and backward. Together, these results deliver a rigorous, numerically stable foundation for scalable, differentiable graph partitioning covering a wide range of clustering and contrastive learning objectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02272v2</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ayoub Ghriss</dc:creator>
    </item>
  </channel>
</rss>
