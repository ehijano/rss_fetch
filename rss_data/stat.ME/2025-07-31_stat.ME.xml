<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 01 Aug 2025 01:32:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Derivative Estimation from Coarse, Irregular, Noisy Samples: An MLE-Spline Approach</title>
      <link>https://arxiv.org/abs/2507.22176</link>
      <description>arXiv:2507.22176v1 Announce Type: new 
Abstract: We address numerical differentiation under coarse, non-uniform sampling and Gaussian noise. A maximum-likelihood estimator with $L_2$-norm constraint on a higher-order derivative is obtained, yielding spline-based solution. We introduce a non-standard parameterization of quadratic splines and develop recursive online algorithms. Two formulations -- quadratic and zero-order -- offer tradeoff between smoothness and computational speed. Simulations demonstrate superior performance over high-gain observers and super-twisting differentiators under coarse sampling and high noise, benefiting systems where higher sampling rates are impractical.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22176v1</guid>
      <category>stat.ME</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Konstantin E. Avrachenkov, Leonid B. Freidovich</dc:creator>
    </item>
    <item>
      <title>Dice, but don't slice: Optimizing the efficiency of ONEAudit</title>
      <link>https://arxiv.org/abs/2507.22179</link>
      <description>arXiv:2507.22179v1 Announce Type: new 
Abstract: ONEAudit provides more efficient risk-limiting audits than other extant methods when the voting system cannot report a cast-vote record linked to each cast card. It obviates the need for re-scanning; it is simpler and more efficient than 'hybrid' audits; and it is far more efficient than batch-level comparison audits. There may be room to improve the efficiency of ONEAudit further by tuning the statistical tests it uses and by using stratified sampling. We show that tuning the tests by optimizing for the reported batch-level tallies or integrating over a distribution reduces expected workloads by 70-85% compared to the current ONEAudit implementation across a range of simulated elections. The improved tests reduce the expected workload to audit the 2024 Mayoral race in San Francisco, California, by half -- from about 200 cards to about 100 cards. In contrast, stratified sampling does not help: it increases workloads by about 25% on average.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22179v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jacob V Spertus, Amanda K Glazer, Philip B Stark</dc:creator>
    </item>
    <item>
      <title>Risk-inclusive Contextual Bandits for Early Phase Clinical Trials</title>
      <link>https://arxiv.org/abs/2507.22344</link>
      <description>arXiv:2507.22344v1 Announce Type: new 
Abstract: Early-phase clinical trials face the challenge of selecting optimal drug doses that balance safety and efficacy due to uncertain dose-response relationships and varied participant characteristics. Traditional randomized dose allocation often exposes participants to sub-optimal doses by not considering individual covariates, necessitating larger sample sizes and prolonging drug development. This paper introduces a risk-inclusive contextual bandit algorithm that utilizes multi-arm bandit (MAB) strategies to optimize dosing through participant-specific data integration. By combining two separate Thompson samplers, one for efficacy and one for safety, the algorithm enhances the balance between efficacy and safety in dose allocation. The effect sizes are estimated with a generalized version of asymptotic confidence sequences (AsympCS, Waudby-Smith et al., 2024), offering a uniform coverage guarantee for sequential causal inference over time. The validity of AsympCS is also established in the MAB setup with a possibly mis-specified model. The empirical results demonstrate the strengths of this method in optimizing dose allocation compared to randomized allocations and traditional contextual bandits focused solely on efficacy. Moreover, an application on real data generated from a recent Phase IIb study aligns with actual findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22344v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rohit Kanrar, Chunlin Li, Zara Ghodsi, Margaret Gamalo</dc:creator>
    </item>
    <item>
      <title>Is External Information Useful for Data Fusion? An Evaluation before Acquisition</title>
      <link>https://arxiv.org/abs/2507.22351</link>
      <description>arXiv:2507.22351v1 Announce Type: new 
Abstract: We consider a general statistical estimation problem involving a finite-dimensional target parameter vector. Beyond an internal data set drawn from the population distribution, external information, such as additional individual data or summary statistics, can potentially improve the estimation when incorporated via appropriate data fusion techniques. However, since acquiring external information often incurs costs, it is desirable to assess its utility beforehand using only the internal data. To address this need, we introduce a utility measure based on estimation efficiency, defined as the ratio of semiparametric efficiency bounds for estimating the target parameters with versus without incorporating the external information. It quantifies the maximum potential efficiency improvement offered by the external information, independent of specific estimation methods. To enable inference on this measure before acquiring the external information, we propose a general approach for constructing its estimators using only the internal data, adopting the efficient influence function methodology. Several concrete examples, where the target parameters and external information take various forms, are explored, demonstrating the versatility of our general framework. For each example, we construct point and interval estimators for the proposed measure and establish their asymptotic properties. Simulation studies confirm the finite-sample performance of our approach, while a real data application highlights its practical value. In scientific research and business applications, our framework significantly empowers cost-effective decision making regarding acquisition of external information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22351v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guorong Dai, Lingxuan Shao, Jinbo Chen</dc:creator>
    </item>
    <item>
      <title>Inference in a generalized Bradley-Terry model for paired comparisons with covariates and a growing number of subjects</title>
      <link>https://arxiv.org/abs/2507.22472</link>
      <description>arXiv:2507.22472v1 Announce Type: new 
Abstract: Motivated by the home-field advantage in sports, we propose a generalized Bradley--Terry model that incorporates covariate information for paired comparisons. It has an $n$-dimensional merit parameter $\bs{\beta}$ and a fixed-dimensional regression coefficient $\bs{\gamma}$ for covariates. When the number of subjects $n$ approaches infinity and the number of comparisons between any two subjects is fixed, we show the uniform consistency of the maximum likelihood estimator (MLE) $(\widehat{\bs{\beta}}, \widehat{\bs{\gamma}})$ of $(\bs{\beta}, \bs{\gamma})$ Furthermore, we derive the asymptotic normal distribution of the MLE by characterizing its asymptotic representation. The asymptotic distribution of $\widehat{\bs{\gamma}}$ is biased, while that of $\widehat{\bs{\beta}}$ is not. This phenomenon can be attributed to the different convergence rates of $\widehat{\bs{\gamma}}$ and $\widehat{\bs{\beta}}$. To the best of our knowledge, this is the first study to explore the asymptotic theory in paired comparison models with covariates in a high-dimensional setting. The consistency result is further extended to an Erd\H{o}s--R\'{e}nyi comparison graph with a diverging number of covariates. Numerical studies and a real data analysis demonstrate our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22472v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ting Yan</dc:creator>
    </item>
    <item>
      <title>Bioequivalence Assessment for Locally Acting Drugs: A Framework for Feasible and Efficient Evaluation</title>
      <link>https://arxiv.org/abs/2507.22756</link>
      <description>arXiv:2507.22756v1 Announce Type: new 
Abstract: Equivalence testing plays a key role in several domains, such as the development of generic medical products, which are therapeutically equivalent to brand-name drugs but with reduced cost and increased accessibility. Promoting access to generics is a critical public health issue with substantial societal implications, but establishing equivalence is particularly challenging in multivariate settings. A notable example refers to locally acting drugs designed to exert their therapeutic effects at a localized area where they are administered rather than being absorbed into the bloodstream, where complex experimental protocols lead to reduced sample sizes and substantial experimental noise. Traditional approaches, such as the Two One-Sided Tests (TOST), cannot adequately tackle the complex multivariate nature of such data. In this work, we develop an adjustment for the TOST procedure by simultaneously correcting its significance level and equivalence margins to ensure control of the test size and increase its power. In large samples, this approach leads to an optimal adjustment for the univariate TOST procedure. In multivariate settings, where we show that an optimal adjustment does not exist, our proposal maintains equal marginal test sizes and overall size control while maximizing power in important cases. Through extensive simulation studies and a case study on multivariate bioequivalence assessment for two antifungal topical products, we demonstrate the superior performance of our method across various scenarios encountered in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22756v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luca Insolia, Yanyuan Ma, Younes Boulaguiem, St\'ephane Guerrier</dc:creator>
    </item>
    <item>
      <title>Futility Analysis under Scrutiny</title>
      <link>https://arxiv.org/abs/2507.22770</link>
      <description>arXiv:2507.22770v1 Announce Type: new 
Abstract: This paper investigates the robustness of futility analyses in clinical trials when interim analysis population deviates from the target population. We demonstrate how population shifts can distort early stopping decisions and propose post-stratification strategies to mitigate these effects. Simulation studies illustrate the impact of subgroup imbalances and the effectiveness of naive, model-based, and hybrid post-stratification methods. We also introduce a permutation-based screening test for identifying variables contributing to population heterogeneity. Our findings support the integration of post-stratification adjustments using all available baseline data at the interim analysis to enhance the validity and integrity of futility decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22770v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rui Jin, Cai Wu, Peter Mesenbrink</dc:creator>
    </item>
    <item>
      <title>Hawkes Processes with Variable Length Memory: Existence, Inference and Application to Neuronal Activity</title>
      <link>https://arxiv.org/abs/2507.22867</link>
      <description>arXiv:2507.22867v1 Announce Type: new 
Abstract: Motivated by applications in neuroscience, where the memory of a neuron may reset upon firing, we introduce a new class of nonlinear Hawkes processes with variable length memory. Multivariate Hawkes processes are past-dependant point processes originally introduced tomodel excitation effects, later extended to a nonlinear framework to account for the opposite effect, known as inhibition. Our model generalises classical Hawkes processes, with or without inhibition, focusing on the situation where the probability of an event occurring within a given subprocess depends solely on the history since its last event. Our main contributions are to prove existence of such processes, and to derive a workable likelihood maximisation method, capable of identifying both classical and variable memory dynamics. We demonstrate the effectiveness of our approach both on synthetic data, and on a neuronal activity dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22867v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sacha Quayle, Anna Bonnet, Maxime Sangnier</dc:creator>
    </item>
    <item>
      <title>Stacked SVD or SVD stacked? A Random Matrix Theory perspective on data integration</title>
      <link>https://arxiv.org/abs/2507.22170</link>
      <description>arXiv:2507.22170v1 Announce Type: cross 
Abstract: Modern data analysis increasingly requires identifying shared latent structure across multiple high-dimensional datasets. A commonly used model assumes that the data matrices are noisy observations of low-rank matrices with a shared singular subspace. In this case, two primary methods have emerged for estimating this shared structure, which vary in how they integrate information across datasets. The first approach, termed Stack-SVD, concatenates all the datasets, and then performs a singular value decomposition (SVD). The second approach, termed SVD-Stack, first performs an SVD separately for each dataset, then aggregates the top singular vectors across these datasets, and finally computes a consensus amongst them. While these methods are widely used, they have not been rigorously studied in the proportional asymptotic regime, which is of great practical relevance in today's world of increasing data size and dimensionality. This lack of theoretical understanding has led to uncertainty about which method to choose and limited the ability to fully exploit their potential. To address these challenges, we derive exact expressions for the asymptotic performance and phase transitions of these two methods and develop optimal weighting schemes to further improve both methods. Our analysis reveals that while neither method uniformly dominates the other in the unweighted case, optimally weighted Stack-SVD dominates optimally weighted SVD-Stack. We extend our analysis to accommodate multiple shared components, and provide practical algorithms for estimating optimal weights from data, offering theoretical guidance for method selection in practical data integration problems. Extensive numerical simulations and semi-synthetic experiments on genomic data corroborate our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22170v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tavor Z. Baharav, Phillip B. Nicol, Rafael A. Irizarry, Rong Ma</dc:creator>
    </item>
    <item>
      <title>A note on blinded continuous monitoring for continuous outcomes</title>
      <link>https://arxiv.org/abs/2507.22202</link>
      <description>arXiv:2507.22202v1 Announce Type: cross 
Abstract: Continuous monitoring is becoming more popular due to its significant benefits, including reducing sample sizes and reaching earlier conclusions. In general, it involves monitoring nuisance parameters (e.g., the variance of outcomes) until a specific condition is satisfied. The blinded method, which does not require revealing group assignments, was recommended because it maintains the integrity of the experiment and mitigates potential bias. Although Friede and Miller (2012) investigated the characteristics of blinded continuous monitoring through simulation studies, its theoretical properties are not fully explored. In this paper, we aim to fill this gap by presenting the asymptotic and finite-sample properties of the blinded continuous monitoring for continuous outcomes. Furthermore, we examine the impact of using blinded versus unblinded variance estimators in the context of continuous monitoring. Simulation results are also provided to evaluate finite-sample performance and to support the theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22202v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Long-Hao Xu, Tim Friede</dc:creator>
    </item>
    <item>
      <title>Set Invariance with Probability One for Controlled Diffusion: Score-based Approach</title>
      <link>https://arxiv.org/abs/2507.22385</link>
      <description>arXiv:2507.22385v1 Announce Type: cross 
Abstract: Given a controlled diffusion and a connected, bounded, Lipschitz set, when is it possible to guarantee controlled set invariance with probability one? In this work, we answer this question by deriving the necessary and sufficient conditions for the same in terms of gradients of certain log-likelihoods -- a.k.a. score vector fields -- for two cases: given finite time horizon and infinite time horizon. The deduced conditions comprise a score-based test that provably certifies or falsifies the existence of Markovian controllers for given controlled set invariance problem data. Our results are constructive in the sense when the problem data passes the proposed test, we characterize all controllers guaranteeing the desired set invariance. When the problem data fails the proposed test, there does not exist a controller that can accomplish the desired set invariance with probability one. The computation in the proposed tests involve solving certain Dirichlet boundary value problems, and in the finite horizon case, can also account for additional constraint of hitting a target subset at the terminal time. We illustrate the results using several semi-analytical and numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22385v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenqing Wang, Alexis M. H. Teter, Murat Arcak, Abhishek Halder</dc:creator>
    </item>
    <item>
      <title>Order Book Filtration and Directional Signal Extraction at High Frequency</title>
      <link>https://arxiv.org/abs/2507.22712</link>
      <description>arXiv:2507.22712v1 Announce Type: cross 
Abstract: With the advent of electronic capital markets and algorithmic trading agents, the number of events in tick-by-tick market data has exploded. A large fraction of these orders is transient. Their ephemeral character degrades the informativeness of directional alphas derived from the limit order book (LOB) state. We investigate whether directional signals such as order book imbalance (OBI) can be improved by structurally filtering high-frequency LOB data. Three real-time, observable filtration schemes: based on order lifetime, update count, and inter-update delay. These are used to recompute OBI on structurally filtered event streams. To assess the effect of filtration, we implement a three-layer diagnostic framework: contemporaneous correlation with returns, explanatory power under discretized regime counts, and causal coherence via Hawkes excitation norms. Empirical results show that structural filtration improves directional signal clarity in correlation and regime-based metrics, but leads to only limited gains in causal excitation strength. In contrast, OBI computed using trade events exhibits stronger causal alignment with future price movements. These findings highlight the importance of differentiating between associative and causal diagnostics when designing high-frequency directional signals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22712v1</guid>
      <category>q-fin.TR</category>
      <category>q-fin.CP</category>
      <category>q-fin.GN</category>
      <category>q-fin.ST</category>
      <category>stat.ME</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aditya Nittur Anantha, Shashi Jain, Prithwish Maiti</dc:creator>
    </item>
    <item>
      <title>Independent Approximates provide a maximum likelihood estimate for heavy-tailed distributions</title>
      <link>https://arxiv.org/abs/2407.06522</link>
      <description>arXiv:2407.06522v2 Announce Type: replace 
Abstract: Heavy-tailed distributions are infamously difficult to estimate because their moments tend to infinity as the shape of the tail decay increases. Nevertheless, this study shows the utilization of a modified group of moments for estimating a heavy-tailed distribution. These modified moments are determined from powers of the original distribution. The nth-power distribution is guaranteed to have finite moments up to n-1. Samples from the nth-power distribution are drawn from n-tuple Independent Approximates, which are the set of independent samples grouped into n-tuples and sub-selected to be approximately equal to each other. We show that Independent Approximates are a maximum likelihood estimator for the generalized Pareto and the Student's t distributions, which are members of the family of coupled exponential distributions. We use the first (original), second, and third power distributions to estimate their zeroth (geometric mean), first, and second power-moments respectively. In turn, these power-moments are used to estimate the scale and shape of the distributions. A least absolute deviation criteria is used to select the optimal set of Independent Approximates. Estimates using higher powers and moments are possible though the number of n-tuples that are approximately equal may be limited.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06522v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amenah AL-Najafi, Ugur Tirnakli, Kenric P. Nelson</dc:creator>
    </item>
    <item>
      <title>Assessing Spatial Disparities: A Bayesian Linear Regression Approach</title>
      <link>https://arxiv.org/abs/2407.19171</link>
      <description>arXiv:2407.19171v4 Announce Type: replace 
Abstract: Epidemiological investigations of regionally aggregated spatial data often involve detecting spatial health disparities among neighboring regions on a map of disease mortality or incidence rates. Analyzing such data introduces spatial dependence among health outcomes and seeks to report statistically significant spatial disparities by delineating boundaries that separate neighboring regions with disparate health outcomes. However, there are statistical challenges to appropriately define what constitutes a spatial disparity and to construct robust probabilistic inferences for spatial disparities. We enrich the familiar Bayesian linear regression framework to introduce spatial autoregression and offer model-based detection of spatial disparities. We derive exploitable analytical tractability that considerably accelerates computation. Simulation experiments conducted on a county map of the entire United States demonstrate the effectiveness of our method, and we apply our method to a data set from the Institute of Health Metrics and Evaluation (IHME) on age-standardized US county-level estimates of lung cancer mortality rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19171v4</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kyle Lin Wu, Sudipto Banerjee</dc:creator>
    </item>
    <item>
      <title>An integer programming-based approach to construct exact two-sample binomial tests with maximum power</title>
      <link>https://arxiv.org/abs/2503.13689</link>
      <description>arXiv:2503.13689v2 Announce Type: replace 
Abstract: Traditional hypothesis tests for differences between binomial proportions are at risk of being too liberal (Wald test) or overly conservative (Fisher's exact test). This problem is exacerbated in small samples. Regulators favour exact tests, which provide robust type I error control, even though they may have lower power than non-exact tests. To target an exact test with high power, we extend and evaluate an overlooked approach, proposed in 1969, which determines the rejection region through a binary decision for each outcome vector and uses integer programming to, in line with the Neyman-Pearson paradigm, find an optimal decision boundary that maximizes a power objective subject to type I error constraints. Despite only evaluating the type I error rate for a finite parameter set, our approach guarantees type I error control over the full parameter space. Our results show that the test maximizing average power exhibits remarkable robustness, often showing highest power among comparators while maintaining exact type I error control. The method can be further tailored to prior beliefs by using a weighted average. The findings highlight both the method's practical utility and how techniques from combinatorial optimization can improve statistical methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13689v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Stef Baas, Yaron Racah, Elad Berkman, Sofia S. Villar</dc:creator>
    </item>
    <item>
      <title>Group Sequential Design with Posterior and Posterior Predictive Probabilities</title>
      <link>https://arxiv.org/abs/2504.00856</link>
      <description>arXiv:2504.00856v2 Announce Type: replace 
Abstract: Group sequential designs drive innovation in clinical, industrial, and corporate settings. Early stopping for failure in sequential designs conserves experimental resources, whereas early stopping for success accelerates access to improved interventions. Bayesian decision procedures provide a formal and intuitive framework for early stopping using posterior and posterior predictive probabilities. Design parameters including decision thresholds and sample sizes are chosen to control the error rates associated with the sequential decision process. These choices are routinely made based on estimating the sampling distribution of posterior summaries via intensive Monte Carlo simulation for each sample size and design scenario considered. In this paper, we propose an efficient method to assess error rates and determine optimal sample sizes and decision thresholds for Bayesian group sequential designs. We prove theoretical results that enable posterior and posterior predictive probabilities to be modeled as a function of the sample size. Using these functions, we assess error rates at a range of sample sizes given simulations conducted at only two sample sizes. The effectiveness of our methodology is highlighted using two substantive examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.00856v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Luke Hagar, Shirin Golchi, Marina B. Klein</dc:creator>
    </item>
    <item>
      <title>Horseshoe Forests for High-Dimensional Causal Survival Analysis</title>
      <link>https://arxiv.org/abs/2507.22004</link>
      <description>arXiv:2507.22004v2 Announce Type: replace 
Abstract: We develop a Bayesian tree ensemble model to estimate heterogeneous treatment effects in censored survival data with high-dimensional covariates. Instead of imposing sparsity through the tree structure, we place a horseshoe prior directly on the step heights to achieve adaptive global-local shrinkage. This strategy allows flexible regularisation and reduces noise. We develop a reversible jump Gibbs sampler to accommodate the non-conjugate horseshoe prior within the tree ensemble framework. We show through extensive simulations that the method accurately estimates treatment effects in high-dimensional covariate spaces, at various sparsity levels, and under non-linear treatment effect functions. We further illustrate the practical utility of the proposed approach by a re-analysis of pancreatic ductal adenocarcinoma (PDAC) survival data from The Cancer Genome Atlas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22004v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tijn Jacobs, Wessel N. van Wieringen, St\'ephanie L. van der Pas</dc:creator>
    </item>
    <item>
      <title>Clustered Covariate Regression</title>
      <link>https://arxiv.org/abs/2302.09255</link>
      <description>arXiv:2302.09255v4 Announce Type: replace-cross 
Abstract: High covariate dimensionality is increasingly occurrent in model estimation, and existing techniques to address this issue typically require sparsity or discrete heterogeneity of the \emph{unobservable} parameter vector. However, neither restriction may be supported by economic theory in some empirical contexts, leading to severe bias and misleading inference. The clustering-based grouped parameter estimator (GPE) introduced in this paper drops both restrictions and maintains the natural one that the parameter support be bounded. GPE exhibits robust large sample properties under standard conditions and accommodates both sparse and non-sparse parameters whose support can be bounded away from zero. Extensive Monte Carlo simulations demonstrate the excellent performance of GPE in terms of bias reduction and size control compared to competing estimators. An empirical application of GPE to estimating price and income elasticities of demand for gasoline highlights its practical utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.09255v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.2139/ssrn.3394012</arxiv:DOI>
      <dc:creator>Abdul-Nasah Soale, Emmanuel Selorm Tsyawo</dc:creator>
    </item>
    <item>
      <title>Modeling Story Expectations to Understand Engagement: A Generative Framework Using LLMs</title>
      <link>https://arxiv.org/abs/2412.15239</link>
      <description>arXiv:2412.15239v3 Announce Type: replace-cross 
Abstract: Understanding when and why consumers engage with stories is crucial for content creators and platforms. While existing theories suggest that audience beliefs of what is going to happen should play an important role in engagement decisions, empirical work has mostly focused on developing techniques to directly extract features from actual content, rather than capturing forward-looking beliefs, due to the lack of a principled way to model such beliefs in unstructured narrative data. To complement existing feature extraction techniques, this paper introduces a novel framework that leverages large language models to model audience forward-looking beliefs about how stories might unfold. Our method generates multiple potential continuations for each story and extracts features related to expectations, uncertainty, and surprise using established content analysis techniques. Applying our method to over 30,000 book chapters, we demonstrate that our framework complements existing feature engineering techniques by amplifying their marginal explanatory power on average by 31%. The results reveal that different types of engagement-continuing to read, commenting, and voting-are driven by distinct combinations of current and anticipated content features. Our framework provides a novel way to study and explore how audience forward-looking beliefs shape their engagement with narrative media, with implications for marketing strategy in content-focused industries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15239v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.ME</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hortense Fong, George Gui</dc:creator>
    </item>
    <item>
      <title>Potential Outcome Modeling and Estimation in DiD Designs with Staggered Treatments</title>
      <link>https://arxiv.org/abs/2505.18391</link>
      <description>arXiv:2505.18391v2 Announce Type: replace-cross 
Abstract: We propose the first potential outcome modeling of Difference-in-Differences designs with multiple time periods and variation in treatment timing. Importantly, the modeling respects the two key identifying assumptions: parallel trends and noanticipation. We then introduce a straightforward Bayesian approach for estimation and inference of the time-varying group specific Average Treatment Effects on the Treated (ATT). To improve parsimony and guide prior elicitation, we reparametrize the model in a way that reduces the effective number of parameters. Prior information about the ATT's is incorporated through black-box training sample priors and, in small-sample settings, by thick-tailed t-priors that shrink ATT's of small magnitudes toward zero. We provide a computationally efficient Bayesian estimation procedure and establish a Bernstein-von Mises-type result that justifies posterior inference for the treatment effects. Simulation studies confirm that our method performs well in both large and small samples, offering credible uncertainty quantification even in settings that challenge standard estimators. We illustrate the practical value of the method through an empirical application that examines the effect of minimum wage increases on teen employment in the United States.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18391v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siddhartha Chib, Kenichi Shimizu</dc:creator>
    </item>
  </channel>
</rss>
