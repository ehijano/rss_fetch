<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 11 Jun 2025 04:01:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Diffusion Non-Additive Model for Multi-Fidelity Simulations with Tunable Precision</title>
      <link>https://arxiv.org/abs/2506.08328</link>
      <description>arXiv:2506.08328v1 Announce Type: new 
Abstract: Computer simulations are indispensable for analyzing complex systems, yet high-fidelity models often incur prohibitive computational costs. Multi-fidelity frameworks address this challenge by combining inexpensive low-fidelity simulations with costly high-fidelity simulations to improve both accuracy and efficiency. However, certain scientific problems demand even more accurate results than the highest-fidelity simulations available, particularly when a tuning parameter controlling simulation accuracy is available, but the exact solution corresponding to a zero-valued parameter remains out of reach. In this paper, we introduce the Diffusion Non-Additive (DNA) model, inspired by generative diffusion models, which captures nonlinear dependencies across fidelity levels using Gaussian process priors and extrapolates to the exact solution. The DNA model: (i) accommodates complex, non-additive relationships across fidelity levels; (ii) employs a nonseparable covariance kernel to model interactions between the tuning parameter and input variables, improving both predictive performance and physical interpretability; and (iii) provides closed-form expressions for the posterior predictive mean and variance, allowing efficient inference and uncertainty quantification. The methodology is validated on a suite of numerical studies and real-world case studies. An R package implementing the proposed methodology is available to support practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08328v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junoh Heo, Romain Boutelet, Chih-Li Sung</dc:creator>
    </item>
    <item>
      <title>midr: Learning from Black-Box Models by Maximum Interpretation Decomposition</title>
      <link>https://arxiv.org/abs/2506.08338</link>
      <description>arXiv:2506.08338v1 Announce Type: new 
Abstract: The use of appropriate methods of Interpretable Machine Learning (IML) and eXplainable Artificial Intelligence (XAI) is essential for adopting black-box predictive models in fields where model and prediction explainability is required. As a novel tool for interpreting black-box models, we introduce the R package midr, which implements Maximum Interpretation Decomposition (MID). MID is a functional decomposition approach that derives a low-order additive representation of a black-box model by minimizing the squared error between the model's prediction function and this additive representation. midr enables learning from black-box models by constructing a global surrogate model with advanced analytical capabilities. After reviewing related work and the theoretical foundation of MID, we demonstrate the package's usage and discuss some of its key features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08338v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryoichi Asashiba, Reiji Kozuma, Hirokazu Iwasawa</dc:creator>
    </item>
    <item>
      <title>Guidelines for LASSO and derivatives use under different dependence and scale structures</title>
      <link>https://arxiv.org/abs/2506.08582</link>
      <description>arXiv:2506.08582v1 Announce Type: new 
Abstract: In a multivariate linear regression model with $p&gt;1$ covariates, implementation of penalization techniques often implies a preliminary univariate standardization step. Although this prevents scale effects on the covariates selection procedure, possible dependence structures can be disrupted, leading to wrong results. This is particularly challenging in high-dimensional settings where $p \geq n$. In this paper, we analyze the standardization effect on the LASSO for different dependence-scales contexts by means of an extensive simulation study. Two distinct objectives are pursued: adequate covariate selection and proper predictive capability. Additionally, its behavior is compared with the one of some well-known or innovative competitors. This comparison is also extended to three real datasets facing different dependence-scales patterns. Eventually, we conclude with discussion and guidelines on the most suitable methodology for each case in terms of covariates selection or prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08582v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Laura Freijeiro-Gonz\'alez, Manuel Febrero-Bande, Wenceslao Gonz\'alez-Manteiga</dc:creator>
    </item>
    <item>
      <title>Gaussian copula correlation network analysis with application to multi-omics data</title>
      <link>https://arxiv.org/abs/2506.08586</link>
      <description>arXiv:2506.08586v1 Announce Type: new 
Abstract: Reconstructing gene regulatory networks from large-scale heterogeneous data is a key challenge in biology. In multi-omics data analysis, networks based on pairwise statistical association measures remain popular, as they are easy to build and understand. In the presence of mixed-type (discrete and continuous) data, however, the choice of good association measures remains an important issue. We propose here a novel approach based on the Gaussian copula, the parameters of which represent the links of the network. Novel properties of the model are obtained to guide the interpretation of the network. To estimate the copula parameters, we calculated a semiparametric pairwise likelihood for mixed data. In an extensive simulation study, we showed that the proposed estimation procedure was able to accurately estimate the copula correlation matrix. The proposed methodology was also applied to a real ICGC dataset on breast cancer, and is implemented in a freely available R package heterocop.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08586v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ekaterina Tomilina (MaIAGE, GABI), Florence Jaffr\'ezic (GABI), Gildas Mazo (MaIAGE)</dc:creator>
    </item>
    <item>
      <title>Akaike information criterion for segmented regression models</title>
      <link>https://arxiv.org/abs/2506.08760</link>
      <description>arXiv:2506.08760v1 Announce Type: new 
Abstract: In segmented regression, when the regression function is continuous at the change-points that are the boundaries of the segments, it is also called joinpoint regression, and the analysis package developed by \cite{KimFFM00} has become a standard tool for analyzing trends in longitudinal data in the field of epidemiology. In addition, it is sometimes natural to expect the regression function to be discontinuous at the change-points, and in the field of epidemiology, this model is used in \cite{JiaZS22}, which is considered important due to the analysis of COVID-19 data. On the other hand, model selection is also indispensable in segmented regression, including the estimation of the number of change-points; however, it can be said that only BIC-type information criteria have been developed. In this paper, we derive an information criterion based on the original definition of AIC, aiming to minimize the divergence between the true structure and the estimated structure. Then, using the statistical asymptotic theory specific to the segmented regression, we confirm that the penalty for the change-point parameter is 6 in the discontinuous case. On the other hand, in the continuous case, we show that the penalty for the change-point parameter remains 2 despite the rapid change in the derivative coefficients. Through numerical experiments, we observe that our AIC tends to reduce the divergence compared to BIC. In addition, through analyzing the same real data as in \cite{JiaZS22}, we find that the selection between continuous and discontinuous using our AIC yields new insights and that our AIC and BIC may yield different results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08760v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kazuki Nakajima, Yoshiyuki Ninomiya</dc:creator>
    </item>
    <item>
      <title>Safety-Driven Response Adaptive Randomisation: An Application in Non-inferiority Oncology Trials</title>
      <link>https://arxiv.org/abs/2506.08864</link>
      <description>arXiv:2506.08864v1 Announce Type: new 
Abstract: The majority of response-adaptive randomisation (RAR) designs in the literature use efficacy data to dynamically allocate patients. Their applicability in settings where the efficacy measure is observable with a random delay, such as overall survival, remains challenging. This paper introduces a RAR design referred to as SAFER (Safety-Aware Flexible Elastic Randomisation) design, which uses early-emerging safety data to inform treatment allocation decisions in oncology trials. However, the design is applicable to a range of settings where it may be desirable to favour the arm demonstrating a superior safety profile. This is particularly relevant in non-inferiority trials, which aim to demonstrate an experimental treatment is not inferior to the standard of care, while offering advantages in terms of safety and tolerability. Consequently, an unavoidable and well-established trade-off arises for such designs: to balance the goals of preserving inferential efficiency for the primary non-inferiority outcome while incorporating safety considerations into the randomisation process through RAR. Our method, defines a randomisation procedure which prioritises the assignment of patients to better-tolerated arms and adjusts the allocation proportion according to the observed association between safety and efficacy endpoints. We illustrate our procedure through a comprehensive simulation study, inspired by the CAPP-IT Phase III oncology trial. Our results demonstrate that SAFER preserves statistical power even when efficacy and safety endpoints are weakly associated and offers power gains when a strong positive association is present. Moreover, the approach enables a faster/slower adaptation when efficacy and safety endpoints are temporally aligned/misaligned, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08864v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maria Vittoria Chiaruttini, Lukas Pin, Sofia S. Villar</dc:creator>
    </item>
    <item>
      <title>On the Existence of Unbiased Hypothesis Tests: An Algebraic Approach</title>
      <link>https://arxiv.org/abs/2506.08259</link>
      <description>arXiv:2506.08259v1 Announce Type: cross 
Abstract: In hypothesis testing problems the property of strict unbiasedness describes whether a test is able to discriminate, in the sense of a difference in power, between any distribution in the null hypothesis space and any distribution in the alternative hypothesis space. In this work we examine conditions under which unbiased tests exist for discrete statistical models. It is shown that the existence of an unbiased test can be reduced to an algebraic criterion; an unbiased test exists if and only if there exists a polynomial that separates the null and alternative hypothesis sets. This places a strong, semialgebraic restriction on the classes of null hypotheses that have unbiased tests. The minimum degree of a separating polynomial coincides with the minimum sample size that is needed for an unbiased test to exist, termed the unbiasedness threshold. It is demonstrated that Gr\"obner basis techniques can be used to provide upper bounds for, and in many cases exactly find, the unbiasedness threshold. Existence questions for uniformly most powerful unbiased tests are also addressed, where it is shown that whether such a test exists can depend subtly on the specified level of the test and the sample size. Numerous examples, concerning tests in contingency tables, linear, log-linear, and mixture models are provided. All of the machinery developed in this work is constructive in the sense that when a test with a certain property is shown to exist it is possible to explicitly construct this test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08259v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew McCormack</dc:creator>
    </item>
    <item>
      <title>A New Lifetime Distribution: Exponentiated Exponential-Pareto-HalfNormal Mixture Model for Biomedical Applications</title>
      <link>https://arxiv.org/abs/2506.08313</link>
      <description>arXiv:2506.08313v1 Announce Type: cross 
Abstract: This study introduces the Exponentiated-Exponential-Pareto-Half Normal Mixture Distribution (EEPHND), a novel hybrid model developed to overcome the limitations of classical distributions in modeling complex real-world data. By compounding the Exponentiated-Exponential-Pareto (EEP) and Half-Normal distributions through a mixture mechanism, EEPHND effectively captures both early-time symmetry and long-tail behavior, features which are commonly observed in survival and reliability data. The model offers closed-form expressions for its probability density, cumulative distribution, survival and hazard functions, moments, and reliability metrics, ensuring analytical traceability and interpretability in the presence of censoring and heterogeneous risk dynamics. When applied to a real-world lung cancer dataset, EEPHND outperformed competing models in both goodness-of-fit and predictive accuracy, achieving a Concordance Index (CI) of 0.9997. These results highlight its potential as a flexible and powerful tool for survival analysis and biomedical engineering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08313v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oriyomi Ahmad Hassan, Aisha Tunrayo Maradesa, Abdulazeez Toyosi Alabi, Oyejide Surajudeen Salam, Ajani Busari, Akinwale Victor Famotire, Habeeb Abiodun Afolabi, Solomon Adeleke, Abayomi Ayodele Akomolafe</dc:creator>
    </item>
    <item>
      <title>Model-Free Kernel Conformal Depth Measures Algorithm for Uncertainty Quantification in Regression Models in Separable Hilbert Spaces</title>
      <link>https://arxiv.org/abs/2506.08325</link>
      <description>arXiv:2506.08325v1 Announce Type: cross 
Abstract: Depth measures are powerful tools for defining level sets in emerging, non--standard, and complex random objects such as high-dimensional multivariate data, functional data, and random graphs. Despite their favorable theoretical properties, the integration of depth measures into regression modeling to provide prediction regions remains a largely underexplored area of research. To address this gap, we propose a novel, model-free uncertainty quantification algorithm based on conditional depth measures--specifically, conditional kernel mean embeddings and an integrated depth measure. These new algorithms can be used to define prediction and tolerance regions when predictors and responses are defined in separable Hilbert spaces. The use of kernel mean embeddings ensures faster convergence rates in prediction region estimation. To enhance the practical utility of the algorithms with finite samples, we also introduce a conformal prediction variant that provides marginal, non-asymptotic guarantees for the derived prediction regions. Additionally, we establish both conditional and unconditional consistency results, as well as fast convergence rates in certain homoscedastic settings. We evaluate the finite--sample performance of our model in extensive simulation studies involving various types of functional data and traditional Euclidean scenarios. Finally, we demonstrate the practical relevance of our approach through a digital health application related to physical activity, aiming to provide personalized recommendations</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08325v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcos Matabuena, Rahul Ghosal, Pavlo Mozharovskyi, Oscar Hernan Madrid Padilla, Jukka-Pekka Onnela</dc:creator>
    </item>
    <item>
      <title>Wasserstein and Convex Gaussian Approximations for Non-stationary Time Series of Diverging Dimensionality</title>
      <link>https://arxiv.org/abs/2506.08723</link>
      <description>arXiv:2506.08723v1 Announce Type: cross 
Abstract: In high-dimensional time series analysis, Gaussian approximation (GA) schemes under various distance measures or on various collections of subsets of the Euclidean space play a fundamental role in a wide range of statistical inference problems. To date, most GA results for high-dimensional time series are established on hyper-rectangles and their equivalence. In this paper, by considering the 2-Wasserstein distance and the collection of all convex sets, we establish a general GA theory for a broad class of high-dimensional non-stationary (HDNS) time series, extending the scope of problems that can be addressed in HDNS time series analysis. For HDNS time series of sufficiently weak dependence and light tail, the GA rates established in this paper are either nearly optimal with respect to the dimensionality and time series length, or they are nearly identical to the corresponding best-known GA rates established for independent data. A multiplier bootstrap procedure is utilized and theoretically justified to implement our GA theory. We demonstrate by two previously undiscussed time series applications the use of the GA theory and the bootstrap procedure as unified tools for a wide range of statistical inference problems in HDNS time series analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08723v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miaoshiqi Liu, Jun Yang, Zhou Zhou</dc:creator>
    </item>
    <item>
      <title>Dirichlet kernel density estimation for strongly mixing sequences on the simplex</title>
      <link>https://arxiv.org/abs/2506.08816</link>
      <description>arXiv:2506.08816v1 Announce Type: cross 
Abstract: This paper investigates the theoretical properties of Dirichlet kernel density estimators for compositional data supported on simplices, for the first time addressing scenarios involving time-dependent observations characterized by strong mixing conditions. We establish rigorous results for the asymptotic normality and mean squared error of these estimators, extending previous findings from the independent and identically distributed (iid) context to the more general setting of strongly mixing processes. To demonstrate its practical utility, the estimator is applied to monthly market-share compositions of several Renault vehicle classes over a twelve-year period, with bandwidth selection performed via leave-one-out least squares cross-validation. Our findings underscore the reliability and strength of Dirichlet kernel techniques when applied to temporally dependent compositional data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08816v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hanen Daayeb, Salah Khardani, Fr\'ed\'eric Ouimet</dc:creator>
    </item>
    <item>
      <title>On the Bernstein-smoothed lower-tail Spearman's rho estimator</title>
      <link>https://arxiv.org/abs/2506.08857</link>
      <description>arXiv:2506.08857v1 Announce Type: cross 
Abstract: This note develops a Bernstein estimator for lower-tail Spearman's rho and establishes its strong consistency and asymptotic normality under mild regularity conditions. Smoothing the empirical copula yields a strictly smaller mean squared error (MSE) in tail regions by lowering sampling variance relative to the classical Spearman's rho estimator. A Monte Carlo simulation experiment with the Farlie--Gumbel--Morgenstern copula demonstrates variance reductions that translate into lower MSE estimates (up to $\sim 70\%$ lower) at deep-tail thresholds under weak to moderate dependence and small sample sizes. To facilitate reproducibility of the findings, the R code that generated all simulation results is readily accessible online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08857v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fr\'ed\'eric Ouimet, Selim Orhun Susam</dc:creator>
    </item>
    <item>
      <title>Local MDI+: Local Feature Importances for Tree-Based Models</title>
      <link>https://arxiv.org/abs/2506.08928</link>
      <description>arXiv:2506.08928v1 Announce Type: cross 
Abstract: Tree-based ensembles such as random forests remain the go-to for tabular data over deep learning models due to their prediction performance and computational efficiency. These advantages have led to their widespread deployment in high-stakes domains, where interpretability is essential for ensuring trustworthy predictions. This has motivated the development of popular local (i.e. sample-specific) feature importance (LFI) methods such as LIME and TreeSHAP. However, these approaches rely on approximations that ignore the model's internal structure and instead depend on potentially unstable perturbations. These issues are addressed in the global setting by MDI+, a feature importance method which exploits an equivalence between decision trees and linear models on a transformed node basis. However, the global MDI+ scores are not able to explain predictions when faced with heterogeneous individual characteristics. To address this gap, we propose Local MDI+ (LMDI+), a novel extension of the MDI+ framework to the sample specific setting. LMDI+ outperforms existing baselines LIME and TreeSHAP in identifying instance-specific signal features, averaging a 10% improvement in downstream task performance across twelve real-world benchmark datasets. It further demonstrates greater stability by consistently producing similar instance-level feature importance rankings across multiple random forest fits. Finally, LMDI+ enables local interpretability use cases, including the identification of closer counterfactuals and the discovery of homogeneous subgroups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08928v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhongyuan Liang, Zachary T. Rewolinski, Abhineet Agarwal, Tiffany M. Tang, Bin Yu</dc:creator>
    </item>
    <item>
      <title>NEST: Neural Estimation by Sequential Testing</title>
      <link>https://arxiv.org/abs/2405.04226</link>
      <description>arXiv:2405.04226v2 Announce Type: replace 
Abstract: Adaptive psychophysical procedures aim to increase the efficiency and reliability of measurements. With increasing stimulus and experiment complexity in the last decade, estimating multi-dimensional psychometric functions has become a challenging task for adaptive procedures. If the experimenter has limited information about the underlying psychometric function, it is not possible to use parametric techniques developed for the multi-dimensional stimulus space. Although there are non-parametric approaches that use Gaussian process methods and specific hand-crafted acquisition functions, their performance is sensitive to proper selection of the kernel function, which is not always straightforward. In this work, we use a neural network as the psychometric function estimator and introduce a novel acquisition function for stimulus selection. We thoroughly benchmark our technique both using simulations and by conducting psychovisual experiments under realistic conditions. We show that our method outperforms the state of the art without the need to select a kernel function and significantly reduces the experiment duration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04226v2</guid>
      <category>stat.ME</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sjoerd Bruin, Ji\v{r}\'i Kosinka, Cara Tursun</dc:creator>
    </item>
    <item>
      <title>Quantifying Treatment Effects: Estimating Risk Ratios in Causal Inference</title>
      <link>https://arxiv.org/abs/2410.12333</link>
      <description>arXiv:2410.12333v2 Announce Type: replace 
Abstract: Randomized Controlled Trials (RCT) are the current gold standards to empirically measure the effect of a new drug. However, they may be of limited size and resorting to complementary non-randomized data, referred to as observational, is promising, as additional sources of evidence. In both RCT and observational data, the Risk Difference (RD) is often used to characterize the effect of a drug. Additionally, medical guidelines recommend to also report the Risk Ratio (RR), which may provide a different comprehension of the effect of the same drug. While different methods have been proposed and studied to estimate the RD, few methods exist to estimate the RR. In this paper, we propose estimators of the RR both in RCT and observational data and provide both asymptotical and finite-sample analyses. We show that, even in an RCT, estimating treatment allocation probability or adjusting for covariates leads to lower asymptotic variance. In observational studies, we propose weighting and outcome modeling estimators and derive their asymptotic bias and variance for well-specified models. Using semi-parametric theory, we define two doubly robusts estimators with minimal variances among unbiased estimators. We support our theoretical analysis with empirical evaluations and illustrate our findings through experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12333v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ahmed Boughdiri (PREMEDICAL, IDESP), Julie Josse (PREMEDICAL, IDESP), Erwan Scornet</dc:creator>
    </item>
    <item>
      <title>Rational Expectations in Empirical Bayes</title>
      <link>https://arxiv.org/abs/2411.06129</link>
      <description>arXiv:2411.06129v2 Announce Type: replace 
Abstract: We propose a principled framework for nonparametric empirical Bayes (EB) estimation, based on the idea that the prior should be consistent with the observed posterior and that Bayesian updating should be stable. Focusing on discretized priors, we characterize EB estimators as fixed points of a posterior belief operator. We establish the uniqueness of such fixed points and illustrate how the approach improves transparency and interpretability in standard EB settings, including a recent model of discrimination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06129v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Valentino Dardanoni, Stefano Demichelis</dc:creator>
    </item>
    <item>
      <title>The Building Blocks of Classical Nonparametric Two-Sample Testing Procedures: Statistically Equivalent Blocks</title>
      <link>https://arxiv.org/abs/2501.10844</link>
      <description>arXiv:2501.10844v3 Announce Type: replace 
Abstract: Statistically equivalent blocks are not frequently considered in the context of nonparametric two-sample hypothesis testing. Despite the limited exposure, this paper shows that a number of classical nonparametric hypothesis tests can be derived on the basis of statistically equivalent blocks and their frequencies. Far from being a moot historical point, this allows for a more unified approach in considering the many two-sample nonparametric tests based on ranks, signs, placements, order statistics, and runs. Perhaps more importantly, this approach also allows for the easy extension of many univariate nonparametric tests into arbitrarily high dimensions that retain all null properties regardless of dimensionality and are invariant to the scaling of the observations. These generalizations do not require depth functions or the explicit use of spatial signs or ranks and may be of use in various areas such as life-testing and quality control. In the manuscript, an overview of statistically equivalent blocks and tests based on these blocks are provided. This is followed by reformulations of some popular univariate tests and generalizations to higher dimensions. A brief simulation study and comments comparing the proposed methods to existing testing procedures are offered along with some conclusions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10844v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chase Holcombe</dc:creator>
    </item>
    <item>
      <title>A primer on inference and prediction with epidemic renewal models and sequential Monte Carlo</title>
      <link>https://arxiv.org/abs/2503.18875</link>
      <description>arXiv:2503.18875v2 Announce Type: replace 
Abstract: Renewal models are widely used in statistical epidemiology as semi-mechanistic models of disease transmission. While primarily used for estimating the instantaneous reproduction number, they can also be used for generating projections, estimating elimination probabilities, modelling the effect of interventions, and more. We demonstrate how simple sequential Monte Carlo methods (also known as particle filters) can be used to perform inference on these models. Our goal is to acquaint a reader who has a working knowledge of statistical inference with these methods and models and to provide a practical guide to their implementation. We focus on these methods' flexibility and their ability to handle multiple statistical and other biases simultaneously. We leverage this flexibility to unify existing methods for estimating the instantaneous reproduction number and generating projections. A companion website SMC and epidemic renewal models provides additional worked examples, self-contained code to reproduce the examples presented here, and additional materials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18875v2</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Nicholas Steyn, Kris V. Parag, Robin N. Thompson, Christl A. Donnelly</dc:creator>
    </item>
    <item>
      <title>rd2d: Causal Inference in Boundary Discontinuity Designs</title>
      <link>https://arxiv.org/abs/2505.07989</link>
      <description>arXiv:2505.07989v2 Announce Type: replace 
Abstract: Boundary discontinuity designs -- also known as Multi-Score Regression Discontinuity (RD) designs, with Geographic RD designs as a prominent example -- are often used in empirical research to learn about causal treatment effects along a continuous assignment boundary defined by a bivariate score. This article introduces the R package rd2d, which implements and extends the methodological results developed in Cattaneo, Titiunik and Yu (2025) for boundary discontinuity designs. The package employs local polynomial estimation and inference using either the bivariate score or a univariate distance-to-boundary metric. It features novel data-driven bandwidth selection procedures, and offers both pointwise and uniform estimation and inference along the assignment boundary. The numerical performance of the package is demonstrated through a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07989v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.CO</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Rocio Titiunik, Ruiqi Rae Yu</dc:creator>
    </item>
    <item>
      <title>Online differentially private inference in stochastic gradient descent</title>
      <link>https://arxiv.org/abs/2505.08227</link>
      <description>arXiv:2505.08227v2 Announce Type: replace 
Abstract: We propose a general privacy-preserving optimization-based framework for real-time environments without requiring trusted data curators. In particular, we introduce a noisy stochastic gradient descent algorithm for online statistical inference with streaming data under local differential privacy constraints. Unlike existing methods that either disregard privacy protection or require full access to the entire dataset, our proposed algorithm provides rigorous local privacy guarantees for individual-level data. It operates as a one-pass algorithm without re-accessing the historical data, thereby significantly reducing both time and space complexity. We also introduce online private statistical inference by conducting two construction procedures of valid private confidence intervals. We formally establish the convergence rates for the proposed estimators and present a functional central limit theorem to show the averaged solution path of these estimators weakly converges to a rescaled Brownian motion, providing a theoretical foundation for our online inference tool. Numerical simulation experiments demonstrate the finite-sample performance of our proposed procedure, underscoring its efficacy and reliability. Furthermore, we illustrate our method with an analysis of two datasets: the ride-sharing data and the US insurance data, showcasing its practical utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08227v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinhan Xie, Enze Shi, Bei Jiang, Linglong Kong, Xuming He</dc:creator>
    </item>
    <item>
      <title>When Measurement Mediates the Effect of Interest</title>
      <link>https://arxiv.org/abs/2506.06267</link>
      <description>arXiv:2506.06267v2 Announce Type: replace 
Abstract: Many health promotion strategies aim to improve reach into the target population and outcomes among those reached. For example, an HIV prevention strategy could expand the reach of risk screening and the delivery of biomedical prevention to persons with HIV risk. This setting creates a complex missing data problem: the strategy improves health outcomes directly and indirectly through expanded reach, while outcomes are only measured among those reached. To formally define the total causal effect in such settings, we use Counterfactual Strata Effects: causal estimands where the outcome is only relevant for a group whose membership is subject to missingness and/or impacted by the exposure. To identify and estimate the corresponding statistical estimand, we propose a novel extension of Two-Stage targeted minimum loss-based estimation (TMLE). Simulations demonstrate the practical performance of our approach as well as the limitations of existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06267v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Joy Zora Nakato, Janice Litunya, Brian Beesiga, Jane Kabami, James Ayieko, Moses R. Kamya, Gabriel Chamie, Laura B. Balzer</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Independence Testing via Maximum and Average Distance Correlations</title>
      <link>https://arxiv.org/abs/2001.01095</link>
      <description>arXiv:2001.01095v4 Announce Type: replace-cross 
Abstract: This paper investigates the utilization of maximum and average distance correlations for multivariate independence testing. We characterize their consistency properties in high-dimensional settings with respect to the number of marginally dependent dimensions, compare the advantages of each test statistic, examine their respective null distributions, and present a fast chi-square-based testing procedure. The resulting tests are non-parametric and applicable to both Euclidean distance and the Gaussian kernel as the underlying metric. To better understand the practical use cases of the proposed tests, we evaluate the empirical performance of the maximum distance correlation, average distance correlation, and the original distance correlation across various multivariate dependence scenarios, as well as conduct a real data experiment to test the presence of various cancer types and peptide levels in human plasma.</description>
      <guid isPermaLink="false">oai:arXiv.org:2001.01095v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cencheng Shen, Yuexiao Dong</dc:creator>
    </item>
    <item>
      <title>A calibrated BISG for inferring race from surname and geolocation</title>
      <link>https://arxiv.org/abs/2304.09126</link>
      <description>arXiv:2304.09126v4 Announce Type: replace-cross 
Abstract: Bayesian Improved Surname Geocoding (BISG) is a ubiquitous tool for predicting race and ethnicity using an individual's geolocation and surname. Here we demonstrate that statistical dependence of surname and geolocation within racial/ethnic categories in the United States results in biases for minority subpopulations, and we introduce a raking-based improvement. Our method augments the data used by BISG--distributions of race by geolocation and race by surname--with the distribution of surname by geolocation obtained from state voter files. We validate our algorithm on state voter registration lists that contain self-identified race/ethnicity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.09126v4</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philip Greengard, Andrew Gelman</dc:creator>
    </item>
    <item>
      <title>Distinguishing Cause from Effect with Causal Velocity Models</title>
      <link>https://arxiv.org/abs/2502.05122</link>
      <description>arXiv:2502.05122v2 Announce Type: replace-cross 
Abstract: Bivariate structural causal models (SCM) are often used to infer causal direction by examining their goodness-of-fit under restricted model classes. In this paper, we describe a parametrization of bivariate SCMs in terms of a causal velocity by viewing the cause variable as time in a dynamical system. The velocity implicitly defines counterfactual curves via the solution of initial value problems where the observation specifies the initial condition. Using tools from measure transport, we obtain a unique correspondence between SCMs and the score function of the generated distribution via its causal velocity. Based on this, we derive an objective function that directly regresses the velocity against the score function, the latter of which can be estimated non-parametrically from observational data. We use this to develop a method for bivariate causal discovery that extends beyond known model classes such as additive or location scale noise, and that requires no assumptions on the noise distributions. When the score is estimated well, the objective is also useful for detecting model non-identifiability and misspecification. We present positive results in simulation and benchmark experiments where many existing methods fail, and perform ablation studies to examine the method's sensitivity to accurate score estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05122v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Johnny Xi, Hugh Dance, Peter Orbanz, Benjamin Bloem-Reddy</dc:creator>
    </item>
    <item>
      <title>Exact Upper and Lower Bounds for the Output Distribution of Neural Networks with Random Inputs</title>
      <link>https://arxiv.org/abs/2502.11672</link>
      <description>arXiv:2502.11672v2 Announce Type: replace-cross 
Abstract: We derive exact upper and lower bounds for the cumulative distribution function (cdf) of the output of a neural network (NN) over its entire support subject to noisy (stochastic) inputs. The upper and lower bounds converge to the true cdf over its domain as the resolution increases. Our method applies to any feedforward NN using continuous monotonic piecewise twice continuously differentiable activation functions (e.g., ReLU, tanh and softmax) and convolutional NNs, which were beyond the scope of competing approaches. The novelty and instrumental tool of our approach is to bound general NNs with ReLU NNs. The ReLU NN-based bounds are then used to derive the upper and lower bounds of the cdf of the NN output. Experiments demonstrate that our method delivers guaranteed bounds of the predictive output distribution over its support, thus providing exact error guarantees, in contrast to competing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11672v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrey Kofnov, Daniel Kapla, Ezio Bartocci, Efstathia Bura</dc:creator>
    </item>
    <item>
      <title>Continuous Temporal Learning of Probability Distributions via Neural ODEs with Applications in Continuous Glucose Monitoring Data</title>
      <link>https://arxiv.org/abs/2505.08698</link>
      <description>arXiv:2505.08698v2 Announce Type: replace-cross 
Abstract: Modeling the dynamics of probability distributions from time-dependent data samples is a fundamental problem in many fields, including digital health. The goal is to analyze how the distribution of a biomarker, such as glucose, changes over time and how these changes may reflect the progression of chronic diseases like diabetes. We introduce a probabilistic model based on a Gaussian mixture that captures the evolution of a continuous-time stochastic process. Our approach combines a non-parametric estimate of the distribution, obtained with Maximum Mean Discrepancy (MMD), and a Neural Ordinary Differential Equation (Neural ODE) that governs the temporal evolution of the mixture weights. The model is highly interpretable, detects subtle distribution shifts, and remains computationally efficient. Simulation studies show that our method matches or surpasses the estimation accuracy of state-of-the-art, less interpretable techniques such as normalizing flows and non-parametric kernel density estimators. We further demonstrate its utility using data from a digital clinical trial, revealing how interventions affect the time-dependent distribution of glucose levels. The proposed method enables rigorous comparisons between control and treatment groups from both mathematical and clinical perspectives, offering novel longitudinal characterizations that existing approaches cannot achieve.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08698v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.DS</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Antonio \'Alvarez-L\'opez, Marcos Matabuena</dc:creator>
    </item>
  </channel>
</rss>
