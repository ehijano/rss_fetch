<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 24 Oct 2024 04:00:53 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Gains-from-Trade in Bilateral Trade with a Broker</title>
      <link>https://arxiv.org/abs/2410.17444</link>
      <description>arXiv:2410.17444v1 Announce Type: new 
Abstract: We study bilateral trade with a broker, where a buyer and seller interact exclusively through the broker. The broker strategically maximizes her payoff through arbitrage by trading with the buyer and seller at different prices. We study whether the presence of the broker interferes with the mechanism's gains-from-trade (GFT) achieving a constant-factor approximation to the first-best gains-from-trade (FB).
  We first show that the GFT achieves a $1 / 36$-approximation to the FB even if the broker runs an optimal posted-pricing mechanism under symmetric agents with monotone-hazard-rate distributions. Beyond posted-pricing mechanisms, even if the broker uses an arbitrary incentive-compatible (IC) and individually-rational (IR) mechanism that maximizes her expected profit, we prove that it induces a $1 / 2$-approximation to the first-best GFT when the buyer and seller's distributions are uniform distributions with arbitrary support. This bound is shown to be tight.
  We complement such results by proving that if the broker uses an arbitrary profit-maximizing IC and IR mechanism, there exists a family of problem instances under which the approximation factor to the first-best GFT becomes arbitrarily bad. We show that this phenomenon persists even if we restrict one of the buyer's or seller's distributions to have a singleton support, or even in the symmetric setting where the buyer and seller have identical distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17444v1</guid>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilya Hajiaghayi, MohammadTaghi Hajiaghayi, Gary Peng, Suho Shin</dc:creator>
    </item>
    <item>
      <title>Computing Optimal Regularizers for Online Linear Optimization</title>
      <link>https://arxiv.org/abs/2410.17336</link>
      <description>arXiv:2410.17336v1 Announce Type: cross 
Abstract: Follow-the-Regularized-Leader (FTRL) algorithms are a popular class of learning algorithms for online linear optimization (OLO) that guarantee sub-linear regret, but the choice of regularizer can significantly impact dimension-dependent factors in the regret bound. We present an algorithm that takes as input convex and symmetric action sets and loss sets for a specific OLO instance, and outputs a regularizer such that running FTRL with this regularizer guarantees regret within a universal constant factor of the best possible regret bound. In particular, for any choice of (convex, symmetric) action set and loss set we prove that there exists an instantiation of FTRL which achieves regret within a constant factor of the best possible learning algorithm, strengthening the universality result of Srebro et al., 2011.
  Our algorithm requires preprocessing time and space exponential in the dimension $d$ of the OLO instance, but can be run efficiently online assuming a membership and linear optimization oracle for the action and loss sets, respectively (and is fully polynomial time for the case of constant dimension $d$). We complement this with a lower bound showing that even deciding whether a given regularizer is $\alpha$-strongly-convex with respect to a given norm is NP-hard.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17336v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Khashayar Gatmiry, Jon Schneider, Stefanie Jegelka</dc:creator>
    </item>
    <item>
      <title>Meta Stackelberg Game: Robust Federated Learning against Adaptive and Mixed Poisoning Attacks</title>
      <link>https://arxiv.org/abs/2410.17431</link>
      <description>arXiv:2410.17431v1 Announce Type: cross 
Abstract: Federated learning (FL) is susceptible to a range of security threats. Although various defense mechanisms have been proposed, they are typically non-adaptive and tailored to specific types of attacks, leaving them insufficient in the face of multiple uncertain, unknown, and adaptive attacks employing diverse strategies. This work formulates adversarial federated learning under a mixture of various attacks as a Bayesian Stackelberg Markov game, based on which we propose the meta-Stackelberg defense composed of pre-training and online adaptation. {The gist is to simulate strong attack behavior using reinforcement learning (RL-based attacks) in pre-training and then design meta-RL-based defense to combat diverse and adaptive attacks.} We develop an efficient meta-learning approach to solve the game, leading to a robust and adaptive FL defense. Theoretically, our meta-learning algorithm, meta-Stackelberg learning, provably converges to the first-order $\varepsilon$-meta-equilibrium point in $O(\varepsilon^{-2})$ gradient iterations with $O(\varepsilon^{-4})$ samples per iteration. Experiments show that our meta-Stackelberg framework performs superbly against strong model poisoning and backdoor attacks of uncertain and unknown types.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17431v1</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.GT</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tao Li, Henger Li, Yunian Pan, Tianyi Xu, Zizhan Zheng, Quanyan Zhu</dc:creator>
    </item>
    <item>
      <title>Evolution with Opponent-Learning Awareness</title>
      <link>https://arxiv.org/abs/2410.17466</link>
      <description>arXiv:2410.17466v1 Announce Type: cross 
Abstract: The universe involves many independent co-learning agents as an ever-evolving part of our observed environment. Yet, in practice, Multi-Agent Reinforcement Learning (MARL) applications are usually constrained to small, homogeneous populations and remain computationally intensive. In this paper, we study how large heterogeneous populations of learning agents evolve in normal-form games. We show how, under assumptions commonly made in the multi-armed bandit literature, Multi-Agent Policy Gradient closely resembles the Replicator Dynamic, and we further derive a fast, parallelizable implementation of Opponent-Learning Awareness tailored for evolutionary simulations. This enables us to simulate the evolution of very large populations made of heterogeneous co-learning agents, under both naive and advanced learning strategies. We demonstrate our approach in simulations of 200,000 agents, evolving in the classic games of Hawk-Dove, Stag-Hunt, and Rock-Paper-Scissors. Each game highlights distinct ways in which Opponent-Learning Awareness affects evolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17466v1</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <category>q-bio.PE</category>
      <category>q-fin.GN</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yann Bouteiller, Karthik Soma, Giovanni Beltrame</dc:creator>
    </item>
    <item>
      <title>Bridging Swarm Intelligence and Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2410.17517</link>
      <description>arXiv:2410.17517v1 Announce Type: cross 
Abstract: Swarm intelligence (SI) explores how large groups of simple individuals (e.g., insects, fish, birds) collaborate to produce complex behaviors, exemplifying that the whole is greater than the sum of its parts. A fundamental task in SI is Collective Decision-Making (CDM), where a group selects the best option among several alternatives, such as choosing an optimal foraging site. In this work, we demonstrate a theoretical and empirical equivalence between CDM and single-agent reinforcement learning (RL) in multi-armed bandit problems, utilizing concepts from opinion dynamics, evolutionary game theory, and RL. This equivalence bridges the gap between SI and RL and leads us to introduce a novel abstract RL update rule called Maynard-Cross Learning. Additionally, it provides a new population-based perspective on common RL practices like learning rate adjustment and batching. Our findings enable cross-disciplinary fertilization between RL and SI, allowing techniques from one field to enhance the understanding and methodologies of the other.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17517v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Karthik Soma, Yann Bouteiller, Heiko Hamann, Giovanni Beltrame</dc:creator>
    </item>
    <item>
      <title>Markov Potential Game with Final-time Reach-Avoid Objectives</title>
      <link>https://arxiv.org/abs/2410.17690</link>
      <description>arXiv:2410.17690v1 Announce Type: cross 
Abstract: We formulate a Markov potential game with final-time reach-avoid objectives by integrating potential game theory with stochastic reach-avoid control. Our focus is on multi-player trajectory planning where players maximize the same multi-player reach-avoid objective: the probability of all participants reaching their designated target states by a specified time, while avoiding collisions with one another. Existing approaches require centralized computation of actions via a global policy, which may have prohibitively expensive communication costs. Instead, we focus on approximations of the global policy via local state feedback policies. First, we adapt the recursive single player reach-avoid value iteration to the multi-player framework with local policies, and show that the same recursion holds on the joint state space. To find each player's optimal local policy, the multi-player reach-avoid value function is projected from the joint state to the local state using the other players' occupancy measures. Then, we propose an iterative best response scheme for the multi-player value iteration to converge to a pure Nash equilibrium. We demonstrate the utility of our approach in finding collision-free policies for multi-player motion planning in simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17690v1</guid>
      <category>eess.SY</category>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sarah H. Q. Li, Abraham P. Vinod</dc:creator>
    </item>
    <item>
      <title>The Double-Edged Sword of Behavioral Responses in Strategic Classification: Theory and User Studies</title>
      <link>https://arxiv.org/abs/2410.18066</link>
      <description>arXiv:2410.18066v1 Announce Type: cross 
Abstract: When humans are subject to an algorithmic decision system, they can strategically adjust their behavior accordingly (``game'' the system). While a growing line of literature on strategic classification has used game-theoretic modeling to understand and mitigate such gaming, these existing works consider standard models of fully rational agents. In this paper, we propose a strategic classification model that considers behavioral biases in human responses to algorithms. We show how misperceptions of a classifier (specifically, of its feature weights) can lead to different types of discrepancies between biased and rational agents' responses, and identify when behavioral agents over- or under-invest in different features. We also show that strategic agents with behavioral biases can benefit or (perhaps, unexpectedly) harm the firm compared to fully rational strategic agents. We complement our analytical results with user studies, which support our hypothesis of behavioral biases in human responses to the algorithm. Together, our findings highlight the need to account for human (cognitive) biases when designing AI systems, and providing explanations of them, to strategic human in the loop.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18066v1</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <category>cs.HC</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Raman Ebrahimi, Kristen Vaccaro, Parinaz Naghizadeh</dc:creator>
    </item>
    <item>
      <title>Optimal Bailouts and Strategic Debt Forgiveness in Financial Networks</title>
      <link>https://arxiv.org/abs/2202.10986</link>
      <description>arXiv:2202.10986v2 Announce Type: replace 
Abstract: A financial system is represented by a network, where nodes correspond to banks, and directed labeled edges correspond to debt contracts between banks. Once a payment schedule has been defined, where we assume that a bank cannot refuse a payment towards one of its lenders if it has sufficient funds, the liquidity of the system is defined as the sum of total payments made in the network. Maximizing systemic liquidity is a natural objective of any financial authority, so, we study the setting where the financial authority offers bailout money to some bank(s) or forgives the debts of others in order to maximize liquidity, and examine efficient ways to achieve this. We investigate the approximation ratio provided by the greedy bailout policy compared to the optimal one, and we study the computational hardness of finding the optimal debt-removal and budget-constrained optimal bailout policy, respectively.
  We also study financial systems from a game-theoretic standpoint. We observe that the removal of some incoming debt might be in the best interest of a bank, if that helps one of its borrowers remain solvent and avoid costs related to default. Assuming that a bank's well-being (i.e., utility) is aligned with the incoming payments they receive from the network, we define and analyze a game among banks who want to maximize their utility by strategically giving up some incoming payments. In addition, we extend the previous game by considering bailout payments. After formally defining the above games, we prove results about the existence and quality of pure Nash equilibria, as well as the computational complexity of finding such equilibria.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.10986v2</guid>
      <category>cs.GT</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Panagiotis Kanellopoulos, Maria Kyropoulou, Hao Zhou</dc:creator>
    </item>
    <item>
      <title>Mitigating Information Asymmetry in Two-Stage Contracts with Non-Myopic Agents</title>
      <link>https://arxiv.org/abs/2406.12648</link>
      <description>arXiv:2406.12648v2 Announce Type: replace 
Abstract: We consider a Stackelberg game in which a principal (she) establishes a two-stage contract with a non-myopic agent (he) whose type is unknown. The contract takes the form of an incentive function mapping the agent's first-stage action to his second-stage incentive. While the first-stage action reveals the agent's type under truthful play, a non-myopic agent could benefit from portraying a false type in the first stage to obtain a larger incentive in the second stage. The challenge is thus for the principal to design the incentive function so as to induce truthful play. We show that this is only possible with a constant, non-reactive incentive functions when the type space is continuous, whereas it can be achieved with reactive functions for discrete types. Additionally, we show that introducing an adjustment mechanism that penalizes inconsistent behavior across both stages allows the principal to design more flexible incentive functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12648v2</guid>
      <category>cs.GT</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Munther A. Dahleh, Thibaut Horel, M. Umar B. Niazi</dc:creator>
    </item>
    <item>
      <title>What's the Best Seat in the Game Left, Center, Right?</title>
      <link>https://arxiv.org/abs/2407.05069</link>
      <description>arXiv:2407.05069v2 Announce Type: replace-cross 
Abstract: Left, Center, Right is a popular dice game. We analyze the game using Markov chain and Monte Carlo methods. We compute the expected game length for two to eight players and determine the probability of winning for each player in the game. We discuss the surprising conclusions of which players have the highest and lowest chance of winning, and we propose a small rule change that makes the game a little more fair.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05069v2</guid>
      <category>math.HO</category>
      <category>cs.GT</category>
      <category>math.PR</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Richeson, David Richeson</dc:creator>
    </item>
  </channel>
</rss>
