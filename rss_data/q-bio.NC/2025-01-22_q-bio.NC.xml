<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 22 Jan 2025 05:01:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Active filtering: a predictive function of recurrent circuits of sensory cortex</title>
      <link>https://arxiv.org/abs/2501.10521</link>
      <description>arXiv:2501.10521v1 Announce Type: new 
Abstract: Our brains encode many features of the sensory world into memories: we can sing along with songs we have heard before, interpret spoken and written language composed of words we have learned, and recognize faces and objects. Where are these memories stored? Each area of the cerebral cortex has a huge number of local, recurrent, excitatory-excitatory synapses, as many as 500 million per cubic millimeter. Here I review evidence that cortical recurrent connectivity in sensory cortex is a substrate for sensory memories. Evidence suggests that the local recurrent network encodes the structure of natural sensory input, and that it does so via active filtering, transforming network inputs to boost or select those associated with natural sensation. This is a form of predictive processing, in which the cortical recurrent network selectively amplifies some input patterns and attenuates others, and a form of memory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10521v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Mark H. Histed</dc:creator>
    </item>
    <item>
      <title>Evolution of diverse (and advanced) cognitive abilities through adaptive fine-tuning of learning and chunking mechanisms</title>
      <link>https://arxiv.org/abs/2501.11201</link>
      <description>arXiv:2501.11201v1 Announce Type: new 
Abstract: The evolution of cognition is frequently discussed as the evolution of cognitive abilities or the evolution of some neuronal structures in the brain. However, since such traits or abilities are often highly complex, understanding their evolution requires explaining how they could have gradually evolved through selection acting on heritable variations in simpler cognitive mechanisms. With this in mind, making use of a previously proposed theory, here we show how the evolution of cognitive abilities can be captured by the fine-tuning of basic learning mechanisms and, in particular, chunking mechanisms. We use the term chunking broadly for all types of non-elemental learning, claiming that the process by which elements are combined into chunks and associated with other chunks, or elements, is critical for what the brain can do, and that it must be fine-tuned to ecological conditions. We discuss the relevance of this approach to studies in animal cognition, using examples from animal foraging and decision-making, problem solving, and cognitive flexibility. Finally, we explain how even the apparent human-animal gap in sequence learning ability can be explained in terms of different fine-tunings of a similar chunking process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11201v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arnon Lotem, Joseph Y. Halpern</dc:creator>
    </item>
    <item>
      <title>Artificial Neural Networks for Magnetoencephalography: A review of an emerging field</title>
      <link>https://arxiv.org/abs/2501.11566</link>
      <description>arXiv:2501.11566v1 Announce Type: new 
Abstract: Magnetoencephalography (MEG) is a cutting-edge neuroimaging technique that measures the intricate brain dynamics underlying cognitive processes with an unparalleled combination of high temporal and spatial precision. MEG data analytics has always relied on advanced signal processing and mathematical and statistical tools for various tasks ranging from data cleaning to probing the signals' rich dynamics and estimating the neural sources underlying the surface-level recordings. Like in most domains, the surge in Artificial Intelligence (AI) has led to the increased use of Machine Learning (ML) methods for MEG data classification. More recently, an emerging trend in this field is using Artificial Neural Networks (ANNs) to address many MEG-related tasks. This review provides a comprehensive overview of how ANNs are being used with MEG data from three vantage points: First, we review work that employs ANNs for MEG signal classification, i.e., for brain decoding. Second, we report on work that has used ANNs as putative models of information processing in the human brain. Finally, we examine studies that use ANNs as techniques to tackle methodological questions in MEG, including artifact correction and source estimation. Furthermore, we assess the current strengths and limitations of using ANNs with MEG and discuss future challenges and opportunities in this field. Finally, by establishing a detailed portrait of the field and providing practical recommendations for the future, this review seeks to provide a helpful reference for both seasoned MEG researchers and newcomers to the field who are interested in using ANNs to enhance the exploration of the complex dynamics of the human brain with MEG.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11566v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Arthur Dehgan, Hamza Abdelhedi, Vanessa Hadid, Irina Rish, Karim Jerbi</dc:creator>
    </item>
    <item>
      <title>Using Space-Filling Curves and Fractals to Reveal Spatial and Temporal Patterns in Neuroimaging Data</title>
      <link>https://arxiv.org/abs/2501.12111</link>
      <description>arXiv:2501.12111v1 Announce Type: new 
Abstract: We present a novel method, Fractal Space-Curve Analysis (FSCA), which combines Space-Filling Curve (SFC) mapping for dimensionality reduction with fractal Detrended Fluctuation Analysis (DFA). The method is suitable for multidimensional geometrically embedded data, especially for neuroimaging data which is highly correlated temporally and spatially. We conduct extensive feasibility studies on diverse, artificially generated data with known fractal characteristics: the fractional Brownian motion, Cantor sets, and Gaussian processes. We compare the suitability of dimensionality reduction via Hilbert SFC and a data-driven alternative. FSCA is then successfully applied to real-world magnetic resonance imaging (MRI) and functional MRI (fMRI) scans.
  The method utilizing Hilbert curves is optimized for computational efficiency, proven robust against boundary effects typical in experimental data analysis, and resistant to data sub-sampling. It is able to correctly quantify and discern correlations in both stationary and dynamic two-dimensional images. In MRI Alzheimer's dataset, patients reveal a progression of the disease associated with a systematic decrease of the Hurst exponent. In fMRI recording of breath-holding task, the change in the exponent allows distinguishing different experimental phases.
  This study introduces a robust method for fractal characterization of spatial and temporal correlations in many types of multidimensional neuroimaging data. Very few assumptions allow it to be generalized to more dimensions than typical for neuroimaging and utilized in other scientific fields. The method can be particularly useful in analyzing fMRI experiments to compute markers of pathological conditions resulting from neurodegeneration. We also showcase its potential for providing insights into brain dynamics in task-related experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12111v1</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <category>q-bio.QM</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1088/1741-2552/ada705</arxiv:DOI>
      <dc:creator>Jacek Grela, Zbigniew Drogosz, Jakub Janarek, Jeremi K. Ochab, Ignacio Cifre, Ewa Gudowska-Nowak, Maciej A. Nowak, Pawe{\l} O\'swi\k{e}cimka, Dante R. Chialvo</dc:creator>
    </item>
    <item>
      <title>Discontinuous phase transition of feature detection in lateral predictive coding</title>
      <link>https://arxiv.org/abs/2501.12139</link>
      <description>arXiv:2501.12139v1 Announce Type: new 
Abstract: The brain adopts the strategy of lateral predictive coding (LPC) to construct optimal internal representations for salient features in input sensory signals to reduce the energetic cost of information transmission. Here we consider the task of distinguishing a non-Gaussian signal by LPC from $(N-1)$ Gaussian background signals of the same magnitude, which is intractable by principal component decomposition. We study the emergence of feature detection function from the perspective of statistical mechanics, and formulate a thermodynamic free energy to implement the tradeoff between energetic cost $E$ and information robustness. We define $E$ as the mean $L_1$-norm of the internal state vectors, and quantify the level of information robustness by an entropy measure $S$. We demonstrate that energy--information tradeoff may induce a discontinuous phase transition of the optimal matrix, from a very weak one with $S \approx 0$ to a functional LPC system with moderate synaptic weights in which a single unit responds selectively to the input non-Gaussian feature with high signal-to-noise ratio.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12139v1</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhen-Ye Huang, Weikang Wang, Hai-Jun Zhou</dc:creator>
    </item>
    <item>
      <title>Unsupervised Learning in Echo State Networks for Input Reconstruction</title>
      <link>https://arxiv.org/abs/2501.11409</link>
      <description>arXiv:2501.11409v1 Announce Type: cross 
Abstract: Conventional echo state networks (ESNs) require supervised learning to train the readout layer, using the desired outputs as training data. In this study, we focus on input reconstruction (IR), which refers to training the readout layer to reproduce the input time series in its output. We reformulate the learning algorithm of the ESN readout layer to perform IR using unsupervised learning (UL). By conducting theoretical analysis and numerical experiments, we demonstrate that IR in ESNs can be effectively implemented under realistic conditions without explicitly using the desired outputs as training data; in this way, UL is enabled. Furthermore, we demonstrate that applications relying on IR, such as dynamical system replication and noise filtering, can be reformulated within the UL framework. Our findings establish a theoretically sound and universally applicable IR formulation, along with its related tasks in ESNs. This work paves the way for novel predictions and highlights unresolved theoretical challenges in ESNs, particularly in the context of time-series processing methods and computational models of the brain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11409v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>eess.SP</category>
      <category>nlin.CD</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Taiki Yamada, Yuichi Katori, Kantaro Fujiwara</dc:creator>
    </item>
    <item>
      <title>SPM 25: open source neuroimaging analysis software</title>
      <link>https://arxiv.org/abs/2501.12081</link>
      <description>arXiv:2501.12081v1 Announce Type: cross 
Abstract: Statistical Parametric Mapping (SPM) is an integrated set of methods for testing hypotheses about the brain's structure and function, using data from imaging devices. These methods are implemented in an open source software package, SPM, which has been in continuous development for more than 30 years by an international community of developers. This paper reports the release of SPM 25.01, a major new version of the software that incorporates novel analysis methods, optimisations of existing methods, as well as improved practices for open science and software development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12081v1</guid>
      <category>q-bio.QM</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tim M. Tierney, Nicholas A. Alexander, Nicole Labra Avila, Yael Balbastre, Gareth Barnes, Yulia Bezsudnova, Mikael Brudfors, Korbinian Eckstein, Guillaume Flandin, Karl Friston, Amirhossein Jafarian, Olivia S. Kowalczyk, Vladimir Litvak, Johan Medrano, Stephanie Mellor, George O'Neill, Thomas Parr, Adeel Razi, Ryan Timms, Peter Zeidman</dc:creator>
    </item>
    <item>
      <title>Shaping the distribution of neural responses with interneurons in a recurrent circuit model</title>
      <link>https://arxiv.org/abs/2405.17745</link>
      <description>arXiv:2405.17745v2 Announce Type: replace 
Abstract: Efficient coding theory posits that sensory circuits transform natural signals into neural representations that maximize information transmission subject to resource constraints. Local interneurons are thought to play an important role in these transformations, dynamically shaping patterns of local circuit activity to facilitate and direct information flow. However, the relationship between these coordinated, nonlinear, circuit-level transformations and the properties of interneurons (e.g., connectivity, activation functions, response dynamics) remains unknown. Here, we propose a normative computational model that establishes such a relationship. Our model is derived from an optimal transport objective that conceptualizes the circuit's input-response function as transforming the inputs to achieve an efficient target response distribution. The circuit, which is comprised of primary neurons that are recurrently connected to a set of local interneurons, continuously optimizes this objective by dynamically adjusting both the synaptic connections between neurons as well as the interneuron activation functions. In an example application motivated by redundancy reduction, we construct a circuit that learns a dynamical nonlinear transformation that maps natural image data to a spherical Gaussian, significantly reducing statistical dependencies in neural responses. Overall, our results provide a framework in which the distribution of circuit responses is systematically and nonlinearly controlled by adjustment of interneuron connectivity and activation functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17745v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Advances in Neural Information Processing (NeurIPS), December 2024</arxiv:journal_reference>
      <dc:creator>David Lipshutz, Eero P. Simoncelli</dc:creator>
    </item>
    <item>
      <title>Hyperbolic embedding of brain networks as a tool for epileptic seizures forecasting</title>
      <link>https://arxiv.org/abs/2406.10184</link>
      <description>arXiv:2406.10184v3 Announce Type: replace 
Abstract: The evidence indicates that intracranial EEG connectivity, as estimated from daily resting state recordings from epileptic patients, may be capable of identifying preictal states. In this study, we employed hyperbolic embedding of brain networks to capture non-trivial patterns that discriminate between connectivity networks from days with (preictal) and without (interictal) seizure. A statistical model was constructed by combining hyperbolic geometry and machine learning tools, which allowed for the estimation of the probability of an upcoming seizure. The results demonstrated that representing brain networks in a hyperbolic space enabled an accurate discrimination (85%) between interictal (no-seizure) and preictal (seizure within the next 24 hours) states. The proposed method also demonstrated excellent prediction performances, with an overall accuracy of 87% and an F1-score of 89% (mean Brier score and Brier skill score of 0.12 and 0.37, respectively). In conclusion, our findings indicate that representations of brain connectivity in a latent geometry space can reveal a daily and reliable signature of the upcoming seizure(s), thus providing a promising biomarker for seizure forecasting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10184v3</guid>
      <category>q-bio.NC</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martin Guillemaud, Louis Cousyn, Vincent Navarro, Mario Chavez</dc:creator>
    </item>
    <item>
      <title>Back to the Continuous Attractor</title>
      <link>https://arxiv.org/abs/2408.00109</link>
      <description>arXiv:2408.00109v3 Announce Type: replace 
Abstract: Continuous attractors offer a unique class of solutions for storing continuous-valued variables in recurrent system states for indefinitely long time intervals. Unfortunately, continuous attractors suffer from severe structural instability in general--they are destroyed by most infinitesimal changes of the dynamical law that defines them. This fragility limits their utility especially in biological systems as their recurrent dynamics are subject to constant perturbations. We observe that the bifurcations from continuous attractors in theoretical neuroscience models display various structurally stable forms. Although their asymptotic behaviors to maintain memory are categorically distinct, their finite-time behaviors are similar. We build on the persistent manifold theory to explain the commonalities between bifurcations from and approximations of continuous attractors. Fast-slow decomposition analysis uncovers the persistent manifold that survives the seemingly destructive bifurcation. Moreover, recurrent neural networks trained on analog memory tasks display approximate continuous attractors with predicted slow manifold structures. Therefore, continuous attractors are functionally robust and remain useful as a universal analogy for understanding analog memory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00109v3</guid>
      <category>q-bio.NC</category>
      <category>cs.NE</category>
      <category>nlin.AO</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>In Proceedings of the 37th Conference on Neural Information Processing Systems (NeurIPS 2024)</arxiv:journal_reference>
      <dc:creator>\'Abel S\'agodi, Guillermo Mart\'in-S\'anchez, Piotr Sok\'o\l, Il Memming Park</dc:creator>
    </item>
    <item>
      <title>Network reconstruction may not mean dynamics prediction</title>
      <link>https://arxiv.org/abs/2409.04240</link>
      <description>arXiv:2409.04240v2 Announce Type: replace 
Abstract: With an increasing amount of observations on the dynamics of many complex systems, it is required to reveal the underlying mechanisms behind these complex dynamics, which is fundamentally important in many scientific fields such as climate, financial, ecological, and neural systems. The underlying mechanisms are commonly encoded into network structures, e.g., capturing how constituents interact with each other to produce emergent behavior. Here, we address whether a good network reconstruction suggests a good dynamics prediction. The answer is quite dependent on the nature of the supplied (observed) dynamics sequences measured on the complex system. When the dynamics are not chaotic, network reconstruction implies dynamics prediction. In contrast, even if a network can be well reconstructed from the chaotic time series (chaos means that many unstable dynamics states coexist), the prediction of the future dynamics can become impossible as at some future point the prediction error will be amplified. This is explained by using dynamical mean-field theory on a toy model of random recurrent neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04240v2</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <category>nlin.CD</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhendong Yu, Haiping Huang</dc:creator>
    </item>
    <item>
      <title>Leveraging Quantum Superposition to Infer the Dynamic Behavior of a Spatial-Temporal Neural Network Signaling Model</title>
      <link>https://arxiv.org/abs/2403.18963</link>
      <description>arXiv:2403.18963v3 Announce Type: replace-cross 
Abstract: The exploration of new problem classes for quantum computation is an active area of research. In this paper, we introduce and solve a novel problem class related to dynamics on large-scale networks relevant to neurobiology and machine learning. Specifically, we ask if a network can sustain inherent dynamic activity beyond some arbitrary observation time or if the activity ceases through quiescence or saturation via an epileptic-like state. We show that this class of problems can be formulated and structured to take advantage of quantum superposition and solved efficiently using the Deutsch-Jozsa and Grover quantum algorithms. To do so, we extend their functionality to address the unique requirements of how input (sub)sets into the algorithms must be mathematically structured while simultaneously constructing the inputs so that measurement outputs can be interpreted as meaningful properties of the network dynamics. This, in turn, allows us to answer the question we pose.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18963v3</guid>
      <category>quant-ph</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriel A. Silva</dc:creator>
    </item>
    <item>
      <title>Automatic rating of incomplete hippocampal inversions evaluated across multiple cohorts</title>
      <link>https://arxiv.org/abs/2408.02496</link>
      <description>arXiv:2408.02496v2 Announce Type: replace-cross 
Abstract: Incomplete Hippocampal Inversion (IHI), sometimes called hippocampal malrotation, is an atypical anatomical pattern of the hippocampus found in about 20% of the general population. IHI can be visually assessed on coronal slices of T1 weighted MR images, using a composite score that combines four anatomical criteria. IHI has been associated with several brain disorders (epilepsy, schizophrenia). However, these studies were based on small samples. Furthermore, the factors (genetic or environmental) that contribute to the genesis of IHI are largely unknown. Large-scale studies are thus needed to further understand IHI and their potential relationships to neurological and psychiatric disorders. However, visual evaluation is long and tedious, justifying the need for an automatic method. In this paper, we propose, for the first time, to automatically rate IHI. We proceed by predicting four anatomical criteria, which are then summed up to form the IHI score, providing the advantage of an interpretable score. We provided an extensive experimental investigation of different machine learning methods and training strategies. We performed automatic rating using a variety of deep learning models (conv5-FC3, ResNet and SECNN) as well as a ridge regression. We studied the generalization of our models using different cohorts and performed multi-cohort learning. We relied on a large population of 2,008 participants from the IMAGEN study, 993 and 403 participants from the QTIM/QTAB studies as well as 985 subjects from the UKBiobank. We showed that deep learning models outperformed a ridge regression. We demonstrated that the performances of the conv5-FC3 network were at least as good as more complex networks while maintaining a low complexity and computation time. We showed that training on a single cohort may lack in variability while training on several cohorts improves generalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02496v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.59275/j.melba.2024-3d4e</arxiv:DOI>
      <arxiv:journal_reference>Machine.Learning.for.Biomedical.Imaging. 2 (2024)</arxiv:journal_reference>
      <dc:creator>Lisa Hemforth, Baptiste Couvy-Duchesne, Kevin De Matos, Camille Brianceau, Matthieu Joulot, Tobias Banaschewski, Arun L. W. Bokde, Sylvane Desrivi\`eres, Herta Flor, Antoine Grigis, Hugh Garavan, Penny Gowland, Andreas Heinz, R\"udiger Br\"uhl, Jean-Luc Martinot, Marie-Laure Paill\`ere Martinot, Eric Artiges, Dimitri Papadopoulos, Herve Lemaitre, Tomas Paus, Luise Poustka, Sarah Hohmann, Nathalie Holz, Juliane H. Fr\"ohner, Michael N. Smolka, Nilakshi Vaidya, Henrik Walter, Robert Whelan, Gunter Schumann, Christian B\"uchel, JB Poline, Bernd Itterman, Vincent Frouin, Alexandre Martin, IMAGEN study group, Claire Cury, Olivier Colliot</dc:creator>
    </item>
  </channel>
</rss>
