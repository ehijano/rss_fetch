<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 03 Feb 2026 05:01:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Semi-parametric Bernstein-von Mises Theorem in a Parabolic PDE Problem</title>
      <link>https://arxiv.org/abs/2602.00889</link>
      <description>arXiv:2602.00889v1 Announce Type: new 
Abstract: We consider the heat equation with absorption in a bounded domain of $\mathbb{R}^d$, where both the scalar diffusivity and the absorption function are unknown. We investigate a Bayesian approach for recovering the diffusivity from a noisy observation of the solution to the PDE over the domain. Given a Gaussian process prior on the absorption function, we derive a Bernstein-von Mises theorem for the marginal posterior distribution of the diffusivity under assumptions on the prior and on smoothness properties of the absorption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00889v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adel Magra, Frank van der Meulen, Aad van der Vaart</dc:creator>
    </item>
    <item>
      <title>Efficient Bayesian Inference in Strictly Semi-parametric Linear Inverse Problems</title>
      <link>https://arxiv.org/abs/2602.00901</link>
      <description>arXiv:2602.00901v1 Announce Type: new 
Abstract: We consider the efficient inference of finite dimensional parameters arising in the context of inverse problems. Our setup is the observation of a transformation of an unknown infinite dimensional signal $f$ corrupted by statistical noise, with the transformation $K_\theta$ being linear but unknown up to a scalar $\theta$. We adopt a Bayesian approach and put a prior on the pair $(\theta,f)$ and prove a Bernstein-von Mises theorem for the marginal posterior of $\theta$ under regularity conditions on the operators $K_\theta$ and on the prior. We apply our results to the recovery of location parameters in semi-blind deconvolution problems and to the recovery of attenuation constants in X-ray tomography.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00901v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adel Magra, Aad van der Vaart</dc:creator>
    </item>
    <item>
      <title>Improving Minimax Estimation Rates for Contaminated Mixture of Multinomial Logistic Experts via Expert Heterogeneity</title>
      <link>https://arxiv.org/abs/2602.00939</link>
      <description>arXiv:2602.00939v1 Announce Type: new 
Abstract: Contaminated mixture of experts (MoE) is motivated by transfer learning methods where a pre-trained model, acting as a frozen expert, is integrated with an adapter model, functioning as a trainable expert, in order to learn a new task. Despite recent efforts to analyze the convergence behavior of parameter estimation in this model, there are still two unresolved problems in the literature. First, the contaminated MoE model has been studied solely in regression settings, while its theoretical foundation in classification settings remains absent. Second, previous works on MoE models for classification capture pointwise convergence rates for parameter estimation without any guaranty of minimax optimality. In this work, we close these gaps by performing, for the first time, the convergence analysis of a contaminated mixture of multinomial logistic experts with homogeneous and heterogeneous structures, respectively. In each regime, we characterize uniform convergence rates for estimating parameters under challenging settings where ground-truth parameters vary with the sample size. Furthermore, we also establish corresponding minimax lower bounds to ensure that these rates are minimax optimal. Notably, our theories offer an important insight into the design of contaminated MoE, that is, expert heterogeneity yields faster parameter estimation rates and, therefore, is more sample-efficient than expert homogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00939v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fanqi Yan, Dung Le, Trang Pham, Huy Nguyen, Nhat Ho</dc:creator>
    </item>
    <item>
      <title>Asymptotic expansions for spectral convergence of compact self-adjoint operators on general spectral subsets, with application to kernel Gram matrices</title>
      <link>https://arxiv.org/abs/2602.00999</link>
      <description>arXiv:2602.00999v1 Announce Type: new 
Abstract: We study the spectral convergence of compact, self-adjoint operators on a separable Hilbert space under operator norm perturbations, and derive asymptotic expansions for their eigenvalues and eigenprojections. Our analysis focuses on eigenvalues indexed by a general subset, with minimal restrictions on their selection. The usefulness of the provided expansions is illustrated by an application to kernel Gram matrices, deriving concentration inequalities as well as weak convergence results, which, in contrast to existing literature, are primarily relying on assumptions on the kernel that are easy to check.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00999v1</guid>
      <category>math.ST</category>
      <category>math.SP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eunseong Bae, Wolfgang Polonik</dc:creator>
    </item>
    <item>
      <title>Estimating Conditional Distributions via Sklar's Theorem and Empirical Checkerboard Approximations, with Consequences to Nonparametric Regression</title>
      <link>https://arxiv.org/abs/2602.01144</link>
      <description>arXiv:2602.01144v1 Announce Type: new 
Abstract: We tackle the natural question of whether it is possible to estimate conditional distributions via Sklar's theorem by separately estimating the conditional distributions of the underlying copula and the marginals. Working with so-called empirical checkerboard/Bernstein approximations with suitably chosen resolution/degree, we first show that uniform weak convergence to the true underlying copula can be established under very mild regularity assumptions. Building upon these results and plugging in the univariate empirical marginal distribution functions we then provide an affirmative answer to the afore-mentioned question and prove strong consistency of the resulting estimators for the conditional distributions. Moreover, we show that aggregating our estimators allows to construct consistent nonparametric estimators for the mean, the quantile, and the expectile regression function, and beyond. Some simulations illustrating the performance of the estimators and a real data example complement the established theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01144v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kai Sch\"arer, Wolfgang Trutschnig</dc:creator>
    </item>
    <item>
      <title>Minimax optimal differentially private synthetic data for smooth queries</title>
      <link>https://arxiv.org/abs/2602.01607</link>
      <description>arXiv:2602.01607v1 Announce Type: new 
Abstract: Differentially private synthetic data enables the sharing and analysis of sensitive datasets while providing rigorous privacy guarantees for individual contributors. A central challenge is to achieve strong utility guarantees for meaningful downstream analysis. Many existing methods ensure uniform accuracy over broad query classes, such as all Lipschitz functions, but this level of generality often leads to suboptimal rates for statistics of practical interest. Since many common data analysis queries exhibit smoothness beyond what worst-case Lipschitz bounds capture, we ask whether exploiting this additional structure can yield improved utility.
  We study the problem of generating $(\varepsilon,\delta)$-differentially private synthetic data from a dataset of size $n$ supported on the hypercube $[-1,1]^d$, with utility guarantees uniformly for all smooth queries having bounded derivatives up to order $k$. We propose a polynomial-time algorithm that achieves a minimax error rate of $n^{-\min \{1, \frac{k}{d}\}}$, up to a $\log(n)$ factor. This characterization uncovers a phase transition at $k=d$. Our results generalize the Chebyshev moment matching framework of (Musco et al., 2025; Wang et al., 2016) and strictly improve the error rates for $k$-smooth queries established in (Wang et al., 2016). Moreover, we establish the first minimax lower bound for the utility of $(\varepsilon,\delta)$-differentially private synthetic data with respect to $k$-smooth queries, extending the Wasserstein lower bound for $\varepsilon$-differential privacy in (Boedihardjo et al., 2024).</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01607v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rundong Ding, Yiyun He, Yizhe Zhu</dc:creator>
    </item>
    <item>
      <title>Handling Covariate Mismatch in Federated Linear Prediction</title>
      <link>https://arxiv.org/abs/2602.02083</link>
      <description>arXiv:2602.02083v1 Announce Type: new 
Abstract: Federated learning enables institutions to train predictive models collaboratively without sharing raw data, addressing privacy and regulatory constraints. In the standard horizontal setting, clients hold disjoint cohorts of individuals and collaborate to learn a shared predictor. Most existing methods, however, assume that all clients measure the same features. We study the more realistic setting of covariate mismatch, where each client observes a different subset of features, which typically arises in multicenter collaborations with no prior agreement on data collection. We formalize learning a linear prediction under client-wise MCAR patterns and develop two modular approaches tailored to the dimensional regime and communication budget. In the low-dimensional setting, we propose a plug-in estimator that approximates the oracle linear predictor by aggregating sufficient statistics to estimate the covariance and cross-moment terms. In higher dimensions, we study an impute-then-regress strategy: (i) impute missing covariates using any exchangeability-preserving imputation procedure, and (ii) fit a ridge-regularized linear model on the completed data. We provide asymptotic and finite-sample learning rates for our predictors, explicitly characterizing their behaviour with the global dimension, the client-specific feature partition, and the distribution of samples across sites.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02083v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alexis Ayme, R\'emi Khellaf</dc:creator>
    </item>
    <item>
      <title>A Kullback-Leibler divergence test for multivariate extremes: theory and practice</title>
      <link>https://arxiv.org/abs/2602.02316</link>
      <description>arXiv:2602.02316v1 Announce Type: new 
Abstract: Testing whether two multivariate samples exhibit the same extremal behavior is an important problem in various fields including environmental and climate sciences. While several ad-hoc approaches exist in the literature, they often lack theoretical justification and statistical guarantees. On the other hand, extreme value theory provides the theoretical foundation for constructing asymptotically justified tests. We combine this theory with Kullback-Leibler divergence, a fundamental concept in information theory and statistics, to propose a test for equality of extremal dependence structures in practically relevant directions. Under suitable assumptions, we derive the limiting distributions of the proposed statistic under null and alternative hypotheses. Importantly, our test is fast to compute and easy to interpret by practitioners, making it attractive in applications. Simulations provide evidence of the power of our test. In a case study, we apply our method to show the strong impact of seasons on the strength of dependence between different aggregation periods (daily versus hourly) of heavy rainfall in France.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02316v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sebastian Engelke, Philippe Naveau, Chen Zhou</dc:creator>
    </item>
    <item>
      <title>New explanations and inference for least angle regression</title>
      <link>https://arxiv.org/abs/2602.02491</link>
      <description>arXiv:2602.02491v1 Announce Type: new 
Abstract: Efron et al. (2004) introduced least angle regression (LAR) as an algorithm for linear predictions, intended as an alternative to forward selection with connections to penalized regression. However, LAR has remained somewhat of a "black box," where some basic behavioral properties of LAR output are not well understood, including an appropriate termination point for the algorithm. We provide a novel framework for inference with LAR, which also allows LAR to be understood from new perspectives with several newly developed mathematical properties. The LAR algorithm at a data level can viewed as estimating a population counterpart "path" that organizes a response mean along regressor variables which are ordered according to a decreasing series of population "correlation" parameters; such parameters are shown to have meaningful interpretations for explaining variable contributions whereby zero correlations denote unimportant variables. In the output of LAR, estimates of all non-zero population correlations turn out to have independent normal distributions for use in inference, while estimates of zero-valued population correlations have a certain non-normal joint distribution. These properties help to provide a formal rule for stopping the LAR algorithm. While the standard bootstrap for regression can fail for LAR, a modified bootstrap provides a practical and formally justified tool for interpreting the entrance of variables and quantifying uncertainty in estimation. The LAR inference method is studied through simulation and illustrated with data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02491v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Karl B. Gregory, Daniel J. Nordman</dc:creator>
    </item>
    <item>
      <title>On the calibration of survival models with competing risks</title>
      <link>https://arxiv.org/abs/2602.00194</link>
      <description>arXiv:2602.00194v1 Announce Type: cross 
Abstract: Survival analysis deals with modeling the time until an event occurs, and accurate probability estimates are crucial for decision-making, particularly in the competing-risks setting where multiple events are possible. While recent work has addressed calibration in standard survival analysis, the competing-risks setting remains under-explored as it is harder (the calibration applies to both probabilities across classes and time horizon). We show that existing calibration measures are not suited to the competing-risk setting and that recent models do not give well-behaved probabilities. To address this, we introduce a dedicated framework with two novel calibration measures that are minimized for oracle estimators (i.e., both measures are proper). We also introduce some methods to estimate, test, and correct the calibration. Our recalibration methods yield good probabilities while preserving discrimination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00194v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>International Conference on Artificial Intelligence and Statistics (AISTATS) 2026, May 2026, Tanger, Morocco</arxiv:journal_reference>
      <dc:creator>Julie Alberge (DREES), Tristan Haugomat (DREES), Ga\"el Varoquaux (SODA, IP Paris), Judith Ab\'ecassis (SODA, IP Paris)</dc:creator>
    </item>
    <item>
      <title>Null-Validated Topological Signatures of Financial Market Dynamics</title>
      <link>https://arxiv.org/abs/2602.00383</link>
      <description>arXiv:2602.00383v1 Announce Type: cross 
Abstract: Financial markets exhibit temporal organization that is not fully captured by volatility measures or linear correlation structure. We study a null validated topological approach for quantifying market complexity and apply it to Bitcoin daily log returns. The analysis uses the $L^1$ norm of persistence landscapes computed from sliding-window delay embeddings. This quantity shows strong co-movement with stochastic volatility during periods of market stress, but remains intermittently elevated during low volatility regimes, indicating dynamical structure beyond fluctuation scale. Rolling correlation analysis reveals that the dependence between geometry and volatility is not stationary. Surrogate based null models provide statistical validation of these observations. Rejection of shuffle surrogates rules out explanations based on marginal distributions alone, while departures from phase randomized surrogates indicate sensitivity to nonlinear and phase dependent temporal organization beyond linear correlations. These results demonstrate that persistence landscape norms provide complementary information about market dynamics across market conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00383v1</guid>
      <category>q-fin.ST</category>
      <category>math.DS</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel W. Akingbade</dc:creator>
    </item>
    <item>
      <title>Non-standard analysis for coherent risk estimation: hyperfinite representations, discrete Kusuoka formulae, and plug-in asymptotics</title>
      <link>https://arxiv.org/abs/2602.00784</link>
      <description>arXiv:2602.00784v1 Announce Type: cross 
Abstract: We develop a non-standard analysis framework for coherent risk measures and their finite-sample analogues, coherent risk estimators, building on recent work of Aichele, Cialenco, Jelito, and Pitera. Coherent risk measures on $L^\infty$ are realised as standard parts of internal support functionals on Loeb probability spaces, and coherent risk estimators arise as finite-grid restrictions.
  Our main results are: (i) a hyperfinite robust representation theorem that yields, as finite shadows, the robust representation results for coherent risk estimators; (ii) a discrete Kusuoka representation for law-invariant coherent risk estimators as suprema of mixtures of discrete expected shortfalls on $\{k/n:k=1,\ldots,n\}$; (iii) uniform almost sure consistency (with an explicit rate) for canonical spectral plug-in estimators over Lipschitz spectral classes; (iv) a Kusuoka-type plug-in consistency theorem under tightness and uniform estimation assumptions; (v) bootstrap validity for spectral plug-in estimators via an NSA reformulation of the functional delta method (under standard smoothness assumptions on $F_X$); and (vi) asymptotic normality obtained through a hyperfinite central limit theorem.
  The hyperfinite viewpoint provides a transparent probability-to-statistics dictionary: applying a risk measure to a law corresponds to evaluating an internal functional on a hyperfinite empirical measure and taking the standard part. We include a standardd self-contained introduction to the required non-standard tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00784v1</guid>
      <category>q-fin.RM</category>
      <category>math.LO</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>q-fin.MF</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tomasz Kania</dc:creator>
    </item>
    <item>
      <title>High-accuracy sampling for diffusion models and log-concave distributions</title>
      <link>https://arxiv.org/abs/2602.01338</link>
      <description>arXiv:2602.01338v1 Announce Type: cross 
Abstract: We present algorithms for diffusion model sampling which obtain $\delta$-error in $\mathrm{polylog}(1/\delta)$ steps, given access to $\widetilde O(\delta)$-accurate score estimates in $L^2$. This is an exponential improvement over all previous results. Specifically, under minimal data assumptions, the complexity is $\widetilde O(d\,\mathrm{polylog}(1/\delta))$ where $d$ is the dimension of the data; under a non-uniform $L$-Lipschitz condition, the complexity is $\widetilde O(\sqrt{dL}\,\mathrm{polylog}(1/\delta))$; and if the data distribution has intrinsic dimension $d_\star$, then the complexity reduces to $\widetilde O(d_\star\,\mathrm{polylog}(1/\delta))$. Our approach also yields the first $\mathrm{polylog}(1/\delta)$ complexity sampler for general log-concave distributions using only gradient evaluations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01338v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fan Chen, Sinho Chewi, Constantinos Daskalakis, Alexander Rakhlin</dc:creator>
    </item>
    <item>
      <title>Phase Transitions for Feature Learning in Neural Networks</title>
      <link>https://arxiv.org/abs/2602.01434</link>
      <description>arXiv:2602.01434v1 Announce Type: cross 
Abstract: According to a popular viewpoint, neural networks learn from data by first identifying low-dimensional representations, and subsequently fitting the best model in this space. Recent works provide a formalization of this phenomenon when learning multi-index models. In this setting, we are given $n$ i.i.d. pairs $({\boldsymbol x}_i,y_i)$, where the covariate vectors ${\boldsymbol x}_i\in\mathbb{R}^d$ are isotropic, and responses $y_i$ only depend on ${\boldsymbol x}_i$ through a $k$-dimensional projection ${\boldsymbol \Theta}_*^{{\sf T}}{\boldsymbol x}_i$. Feature learning amounts to learning the latent space spanned by ${\boldsymbol \Theta}_*$.
  In this context, we study the gradient descent dynamics of two-layer neural networks under the proportional asymptotics $n,d\to\infty$, $n/d\to\delta$, while the dimension of the latent space $k$ and the number of hidden neurons $m$ are kept fixed. Earlier work establishes that feature learning via polynomial-time algorithms is possible if $\delta&gt; \delta_{\text{alg}}$, for $\delta_{\text{alg}}$ a threshold depending on the data distribution, and is impossible (within a certain class of algorithms) below $\delta_{\text{alg}}$. Here we derive an analogous threshold $\delta_{\text{NN}}$ for two-layer networks. Our characterization of $\delta_{\text{NN}}$ opens the way to study the dependence of learning dynamics on the network architecture and training algorithm.
  The threshold $\delta_{\text{NN}}$ is determined by the following scenario. Training first visits points for which the gradient of the empirical risk is large and learns the directions spanned by these gradients. Then the gradient becomes smaller and the dynamics becomes dominated by negative directions of the Hessian. The threshold $\delta_{\text{NN}}$ corresponds to a phase transition in the spectrum of the Hessian in this second phase.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01434v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrea Montanari, Zihao Wang</dc:creator>
    </item>
    <item>
      <title>Data-Driven Uniform Inference for General Continuous Treatment Models via Minimum-Variance Weighting</title>
      <link>https://arxiv.org/abs/2602.01595</link>
      <description>arXiv:2602.01595v1 Announce Type: cross 
Abstract: Ai et al. (2021) studied the estimation of a general dose-response function (GDRF) of a continuous treatment that includes the average dose-response function, the quantile dose-response function, and other expectiles of the dose-response distribution. They specified the GDRF as a parametric function of the treatment status only and proposed a weighted regression with the weighting function estimated using the maximum entropy approach. This paper specifies the GDRF as a nonparametric function of the treatment status, proposes a weighted local linear regression for estimating GDRF, and develops a bootstrap procedure for constructing the uniform confidence bands. We propose stable weights with minimum sample variance while eliminating the sample association between the treatment and the confounding variables. The proposed weights admit a closed-form expression, allowing them to be computed efficiently in the bootstrap sampling. Under certain conditions, we derive the uniform Bahadur representation for the proposed estimator of GDRF and establish the validity of the corresponding uniform confidence bands. A fully data-driven approach to choosing the undersmooth tuning parameters and a data-driven bias-control confidence band are included. A simulation study and an application demonstrate the usefulness of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01595v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chunrong Ai, Wei Huang, Zheng Zhang</dc:creator>
    </item>
    <item>
      <title>Stein-Rule Shrinkage for Stochastic Gradient Estimation in High Dimensions</title>
      <link>https://arxiv.org/abs/2602.01777</link>
      <description>arXiv:2602.01777v1 Announce Type: cross 
Abstract: Stochastic gradient methods are central to large-scale learning, yet their analysis typically treats mini-batch gradients as unbiased estimators of the population gradient. In high-dimensional settings, however, classical results from statistical decision theory show that unbiased estimators are generally inadmissible under quadratic loss, suggesting that standard stochastic gradients may be suboptimal from a risk perspective. In this work, we formulate stochastic gradient computation as a high-dimensional estimation problem and introduce a decision-theoretic framework based on Stein-rule shrinkage. We construct a shrinkage gradient estimator that adaptively contracts noisy mini-batch gradients toward a stable restricted estimator derived from historical momentum. The shrinkage intensity is determined in a data-driven manner using an online estimate of gradient noise variance, leveraging second-moment statistics commonly maintained by adaptive optimization methods. Under a Gaussian noise model and for dimension p&gt;=3, we show that the proposed estimator uniformly dominates the standard stochastic gradient under squared error loss and is minimax-optimal in the classical decision-theoretic sense. We further demonstrate how this estimator can be incorporated into the Adam optimizer, yielding a practical algorithm with negligible additional computational cost. Empirical evaluations on CIFAR10 and CIFAR100, across multiple levels of label noise, show consistent improvements over Adam in the large-batch regime. Ablation studies indicate that the gains arise primarily from selectively applying shrinkage to high-dimensional convolutional layers, while indiscriminate shrinkage across all parameters degrades performance. These results illustrate that classical shrinkage principles provide a principled and effective approach to improving stochastic gradient estimation in modern deep learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01777v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>M. Arashi, M. Amintoosi</dc:creator>
    </item>
    <item>
      <title>Exchangeable random permutations with an application to Bayesian graph matching</title>
      <link>https://arxiv.org/abs/2602.01993</link>
      <description>arXiv:2602.01993v1 Announce Type: cross 
Abstract: We introduce a general Bayesian framework for graph matching grounded in a new theory of exchangeable random permutations. Leveraging the cycle representation of permutations and the literature on exchangeable random partitions, we define, characterize, and study the structural and predictive properties of these probabilistic objects. A novel sequential metaphor, the position-aware generalized Chinese restaurant process, provides a constructive foundation for this theory and supports practical algorithmic design. Exchangeable random permutations offer flexible priors for a wide range of inferential problems centered on permutations. As an application, we develop a Bayesian model for graph matching that integrates a correlated stochastic block model with our novel class of priors. The cycle structure of the matching is linked to latent node partitions that explain connectivity patterns, an assumption consistent with the homogeneity requirement underlying the graph matching task itself. Posterior inference is performed through a node-wise blocked Gibbs sampler directly enabled by the proposed sequential construction. To summarize posterior uncertainty, we introduce perSALSO, an adaptation of SALSO to the permutation domain that provides principled point estimation and interpretable posterior summaries. Together, these contributions establish a unified probabilistic framework for modeling, inference, and uncertainty quantification over permutations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01993v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francesco Gaffi, Nathaniel Josephs, Lizhen Lin</dc:creator>
    </item>
    <item>
      <title>Posterior Uncertainty for Targeted Parameters in Bayesian Bootstrap Procedures</title>
      <link>https://arxiv.org/abs/2602.02216</link>
      <description>arXiv:2602.02216v1 Announce Type: cross 
Abstract: We propose a general method to carry out a valid Bayesian analysis of a finite-dimensional `targeted' parameter in the presence of a finite-dimensional nuisance parameter. We apply our methods to causal inference based on estimating equations. While much of the literature in Bayesian causal inference has relied on the conventional 'likelihood times prior' framework, a recently proposed method, the 'Linked Bayesian Bootstrap', deviated from this classical setting to obtain valid Bayesian inference using the Dirichlet process and the Bayesian bootstrap. These methods rely on an adjustment based on the propensity score and explain how to handle the uncertainty concerning it when studying the posterior distribution of a treatment effect. We examine theoretically the asymptotic properties of the posterior distribution obtained and show that our proposed method, a generalized version of the 'Linked Bayesian Bootstrap', enjoys desirable frequentist properties. In addition, we show that the credible intervals have asymptotically the correct coverage properties. We discuss the applications of our method to mis-specified and singly-robust models in causal inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02216v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Magid Sabbagh, David A. Stephens</dc:creator>
    </item>
    <item>
      <title>Refined Berry-Esseen bounds under local dependence</title>
      <link>https://arxiv.org/abs/2602.02217</link>
      <description>arXiv:2602.02217v1 Announce Type: cross 
Abstract: In this paper, we establish Berry--Esseen bounds for both self-normalized and non-self-normalized sums of locally dependent random variables. The proofs are based on Stein's method together with a concentration inequality approach. We develop a new class of concentration inequalities that extend classical results and achieve optimal convergence rates under more general dependence structures. As applications, we apply our main results to derive sharper Berry--Esseen bounds for graph dependency, distributed $U$-statistics, constrained $U$-statistics, and decorated injective homomorphism sums.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02217v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhi-Jun Cai, Qi-Man Shao, Zhuo-Song Zhang</dc:creator>
    </item>
    <item>
      <title>Statistical Learning Theory in Lean 4: Empirical Processes from Scratch</title>
      <link>https://arxiv.org/abs/2602.02285</link>
      <description>arXiv:2602.02285v1 Announce Type: cross 
Abstract: We present the first comprehensive Lean 4 formalization of statistical learning theory (SLT) grounded in empirical process theory. Our end-to-end formal infrastructure implement the missing contents in latest Lean 4 Mathlib library, including a complete development of Gaussian Lipschitz concentration, the first formalization of Dudley's entropy integral theorem for sub-Gaussian processes, and an application to least-squares (sparse) regression with a sharp rate. The project was carried out using a human-AI collaborative workflow, in which humans design proof strategies and AI agents execute tactical proof construction, leading to the human-verified Lean 4 toolbox for SLT. Beyond implementation, the formalization process exposes and resolves implicit assumptions and missing details in standard SLT textbooks, enforcing a granular, line-by-line understanding of the theory. This work establishes a reusable formal foundation and opens the door for future developments in machine learning theory. The code is available at https://github.com/YuanheZ/lean-stat-learning-theory</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02285v1</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuanhe Zhang, Jason D. Lee, Fanghui Liu</dc:creator>
    </item>
    <item>
      <title>Finite-Sample Wasserstein Error Bounds and Concentration Inequalities for Nonlinear Stochastic Approximation</title>
      <link>https://arxiv.org/abs/2602.02445</link>
      <description>arXiv:2602.02445v1 Announce Type: cross 
Abstract: This paper derives non-asymptotic error bounds for nonlinear stochastic approximation algorithms in the Wasserstein-$p$ distance. To obtain explicit finite-sample guarantees for the last iterate, we develop a coupling argument that compares the discrete-time process to a limiting Ornstein-Uhlenbeck process. Our analysis applies to algorithms driven by general noise conditions, including martingale differences and functions of ergodic Markov chains. Complementing this result, we handle the convergence rate of the Polyak-Ruppert average through a direct analysis that applies under the same general setting.
  Assuming the driving noise satisfies a non-asymptotic central limit theorem, we show that the normalized last iterates converge to a Gaussian distribution in the $p$-Wasserstein distance at a rate of order $\gamma_n^{1/6}$, where $\gamma_n$ is the step size. Similarly, the Polyak-Ruppert average is shown to converge in the Wasserstein distance at a rate of order $n^{-1/6}$. These distributional guarantees imply high-probability concentration inequalities that improve upon those derived from moment bounds and Markov's inequality. We demonstrate the utility of this approach by considering two applications: (1) linear stochastic approximation, where we explicitly quantify the transition from heavy-tailed to Gaussian behavior of the iterates, thereby bridging the gap between recent finite-sample analyses and asymptotic theory and (2) stochastic gradient descent, where we establish rate of convergence to the central limit theorem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02445v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seo Taek Kong, R. Srikant</dc:creator>
    </item>
    <item>
      <title>The envelope of a complex Gaussian random variable</title>
      <link>https://arxiv.org/abs/2305.03038</link>
      <description>arXiv:2305.03038v5 Announce Type: replace 
Abstract: The envelope of an elliptical Gaussian complex vector, or equivalently, the amplitude or norm of a bivariate normal random vector has application in many weather and signal processing contexts. We explicitly characterize its distribution in the general case through its probability density, cumulative distribution and moment generating function. Moments and limiting distributions are also derived. These derivations are exploited to also characterize the special cases where the bivariate Gaussian mean vector and covariance matrix have a simpler structure, providing new additional insights in many cases. Simulations illustrate the benefits of using our formulae over Monte Carlo methods. We also use our derivations to get a better initial characterization of the distribution of the observed values in structural Magnetic Resonance Imaging datasets, and of wind speed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.03038v5</guid>
      <category>math.ST</category>
      <category>eess.SP</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sattwik Ghosal, Ranjan Maitra</dc:creator>
    </item>
    <item>
      <title>High-order Accurate Inference on Manifolds</title>
      <link>https://arxiv.org/abs/2501.06652</link>
      <description>arXiv:2501.06652v3 Announce Type: replace 
Abstract: We present a new framework for statistical inference on Riemannian manifolds that achieves high-order accuracy, addressing the challenges posed by non-Euclidean parameter spaces frequently encountered in modern data science. Our approach leverages a novel and computationally efficient procedure to reach higher-order asymptotic precision. In particular, we develop a bootstrap algorithm on Riemannian manifolds that is both computationally efficient and accurate for hypothesis testing and confidence region construction. Although locational hypothesis testing can be reformulated as a standard Euclidean problem, constructing high-order accurate confidence regions necessitates careful treatment of manifold geometry. To this end, we establish high-order asymptotics under an appropriate coordinate representation induced by a second-order retraction, thereby enabling precise expansions that incorporate curvature effects. We demonstrate the versatility of this framework across various manifold settings, including spheres, the Stiefel manifold, fixed-rank matrix manifolds, and rank-one tensor manifolds; for Euclidean submanifolds, we also introduce a class of projection-like coordinate charts with strong consistency properties. Finally, numerical studies confirm the practical merits of the proposed procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06652v3</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Chengzhu Huang, Anru R. Zhang</dc:creator>
    </item>
    <item>
      <title>Conditional Feature Importance revisited: Double Robustness, Efficiency and Inference</title>
      <link>https://arxiv.org/abs/2501.17520</link>
      <description>arXiv:2501.17520v5 Announce Type: replace 
Abstract: Conditional Feature Importance (CFI) is a classical variable importance measure that accounts for the relationship between the studied feature and the others. However, CFI has not yet been studied from a theoretical perspective because the conditional sampling step has generally been considered a purely practical problem. In this article, we demonstrate that the recent Conditional Permutation Importance (CPI) is indeed a valid implementation of this concept. Under the conditional null hypothesis, we then establish a double robustness property that can be used for variable selection. With either a valid model or a valid conditional sampler, the method correctly identifies null coordinates.
  Under the alternative hypothesis, we study the theoretical target and link it to the popular Total Sobol Index (TSI). We introduce the Sobol-CPI, which generalizes CPI/CFI, prove that it is nonparametrically efficient, and provide a bias correction. Finally, we propose a consistent and valid type-I error test and present numerical experiments that illustrate our findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17520v5</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Angel Reyero-Lobo, Pierre Neuvial, Bertrand Thirion</dc:creator>
    </item>
    <item>
      <title>Multivariate Species Sampling Models</title>
      <link>https://arxiv.org/abs/2503.24004</link>
      <description>arXiv:2503.24004v3 Announce Type: replace 
Abstract: Species sampling processes have long served as the fundamental framework for modeling random discrete distributions and exchangeable sequences. However, data arising from distinct but related sources require a broader notion of probabilistic invariance, making partial exchangeability a natural choice. Countless models for partially exchangeable data, collectively known as dependent nonparametric priors, have been proposed. These include hierarchical, nested and additive processes, widely used in statistics and machine learning. Still, a unifying framework is lacking and key questions about their underlying learning mechanisms remain unanswered. We fill this gap by introducing multivariate species sampling models, a new general class of nonparametric priors that encompasses most existing finite- and infinite-dimensional dependent processes. They are characterized by the induced partially exchangeable partition probability function encoding their multivariate clustering structure. We establish their core distributional properties and analyze their dependence structure, demonstrating that borrowing of information across groups is entirely determined by shared ties. This provides new insights into the underlying learning mechanisms, offering, for instance, a principled rationale for the previously unexplained correlation structure observed in existing models. Beyond providing a cohesive theoretical foundation, our approach serves as a constructive tool for developing new models and opens novel research directions for capturing richer dependence structures beyond the framework of multivariate species sampling processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.24004v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Beatrice Franzolini, Antonio Lijoi, Igor Pr\"unster, Giovanni Rebaudo</dc:creator>
    </item>
    <item>
      <title>Modifications of the BIC for order selection in finite mixture models</title>
      <link>https://arxiv.org/abs/2506.20124</link>
      <description>arXiv:2506.20124v2 Announce Type: replace 
Abstract: Finite mixture models are ubiquitous in modern statistical modeling, and a recurring practical issue is choosing the model order. In \citet[Sankhy\=a Series A, \textbf62, pp. 49--66]{keribin2000consistent}, the Bayesian information criterion (BIC) was proved consistent in mixtures, but under strong regularity, including high moments and high-order derivatives of the component density. We introduce the $\nu$-BIC and $\epsilon$-BIC, which weight the BIC penalty by negligibly small logarithmic factors immaterial in practice. This minor modification yields consistency under substantially weaker conditions, without differentiability and with mild moment assumptions, and we also give a misspecification result: when the truth lies outside the candidate family, any vanishing-penalty IC eventually selects a Kullback--Leibler optimal order among candidates. Finally, we clarify two limitations of consistent IC-based selection in mixtures: there is no universally minimal BIC-scale penalty within our sufficient conditions, and order consistency can conflict with minimax optimality in Hellinger risk. We illustrate the theory for Gaussian mixtures, non-differentiable Laplace mixtures, heavy-tailed $t$-mixtures, and mixtures of regression models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20124v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hien Duy Nguyen, TrungTin Nguyen</dc:creator>
    </item>
    <item>
      <title>LSD of sample covariances of superposition of matrices with separable covariance structure</title>
      <link>https://arxiv.org/abs/2507.18505</link>
      <description>arXiv:2507.18505v2 Announce Type: replace 
Abstract: We study the asymptotic behavior of the spectra of matrices of the form $S_n = \frac{1}{n}XX^*$ where $X =\sum_{r=1}^K X_r$, where $X_r = A_r^\frac{1}{2}Z_rB_r^\frac{1}{2}$, $K \in \mathbb{N}$ and $A_r,B_r$ are sequences of positive semi-definite matrices of dimensions $p\times p$ and $n\times n$, respectively. We establish the existence of a limiting spectral distribution for $S_n$ by assuming that matrices $\{A_r\}_{r=1}^K$ are simultaneously diagonalizable and $\{B_r\}_{r=1}^K$ are simultaneously digaonalizable, and that the joint spectral distributions of $\{A_r\}_{r=1}^K$ and $\{B_r\}_{r=1}^K$ converge to $K$-dimensional distributions, as $p,n\to \infty$ such that $p/n \to c \in (0,\infty)$. The LSD of $S_n$ is characterized by system of equations with unique solutions within the class of Stieltjes transforms of measures on $\mathbb{R}_+$. These results generalize existing results on the LSD of sample covariances when the data matrices have a separable covariance structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18505v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Javed Hazarika, Debashis Paul</dc:creator>
    </item>
    <item>
      <title>Data denoising with self consistency, variance maximization, and the Kantorovich dominance</title>
      <link>https://arxiv.org/abs/2502.02925</link>
      <description>arXiv:2502.02925v2 Announce Type: replace-cross 
Abstract: We introduce a new framework for data denoising, partially inspired by martingale optimal transport. For a given noisy distribution (the data), our approach involves finding the closest distribution to it among all distributions which 1) have a particular prescribed structure (expressed by requiring they lie in a particular domain), and 2) are self-consistent with the data. We show that this amounts to maximizing the variance among measures in the domain which are dominated in convex order by the data. For particular choices of the domain, this problem and a relaxed version of it, in which the self-consistency condition is removed, are intimately related to various classical approaches to denoising. We prove that our general problem has certain desirable features: solutions exist under mild assumptions, have certain robustness properties, and, for very simple domains, coincide with solutions to the relaxed problem.
  We also introduce a novel relationship between distributions, termed Kantorovich dominance, which retains certain aspects of the convex order while being a weaker, more robust, and easier-to-verify condition. Building on this, we propose and analyze a new denoising problem by substituting the convex order in the previously described framework with Kantorovich dominance. We demonstrate that this revised problem shares some characteristics with the full convex order problem but offers enhanced stability, greater computational efficiency, and, in specific domains, more meaningful solutions. Finally, we present simple numerical examples illustrating solutions for both the full convex order problem and the Kantorovich dominance problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02925v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Zoen-Git Hiew, Tongseok Lim, Brendan Pass, Marcelo Cruz de Souza</dc:creator>
    </item>
    <item>
      <title>Dense associative memory for Gaussian distributions</title>
      <link>https://arxiv.org/abs/2509.23162</link>
      <description>arXiv:2509.23162v2 Announce Type: replace-cross 
Abstract: Dense associative memories (DAMs) store and retrieve patterns via energy-function based fixed points, but existing models are limited to vector representations. We extend DAMs to Gaussian densities equipped with the 2-Wasserstein distance. Our framework defines a log-sum-exp energy over stored distributions and a retrieval dynamics aggregating optimal transport maps in a Gibbs-weighted manner. Stationary points correspond to self-consistent Wasserstein barycenters, generalizing classical DAM fixed points. We prove exponential storage capacity and provide quantitative retrieval guarantees under Wasserstein perturbations. We validate the method on synthetic and real-world image (CelebA and CIFAR-10 datasets) and text (text8 and NLI corpus) datasets. By generalizing from vectors to distributions, our work bridges classical DAMs with modern generative modeling and paves way for distributional storage and retrieval in memory-augmented learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23162v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chandan Tankala, Krishnakumar Balasubramanian</dc:creator>
    </item>
    <item>
      <title>Counterfactual Forecasting for Panel Data</title>
      <link>https://arxiv.org/abs/2511.06189</link>
      <description>arXiv:2511.06189v2 Announce Type: replace-cross 
Abstract: We address the challenge of forecasting counterfactual outcomes in a panel data with missing entries and temporally dependent latent factors -- a common scenario in causal inference, where estimating unobserved potential outcomes ahead of time is essential. We propose Forecasting Counterfactuals under Stochastic Dynamics (FOCUS), a method that extends traditional matrix completion methods by leveraging time series dynamics of the factors, thereby enhancing the prediction accuracy of future counterfactuals. Building upon a consistent estimator of the factors, our method accommodates both stochastic and deterministic components within the factors, and provides a flexible framework for various applications. In case of stationary autoregressive factors and under standard conditions, we derive error bounds and establish asymptotic normality of our estimator. Empirical evaluations demonstrate that our method outperforms existing benchmarks when the latent factors have an autoregressive component. We illustrate FOCUS results on HeartSteps, a mobile health study, illustrating its effectiveness in forecasting step counts for users receiving activity prompts, thereby leveraging temporal patterns in user behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06189v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Navonil Deb, Raaz Dwivedi, Sumanta Basu</dc:creator>
    </item>
    <item>
      <title>On the Degrees of Freedom of some Lasso procedures</title>
      <link>https://arxiv.org/abs/2511.21595</link>
      <description>arXiv:2511.21595v2 Announce Type: replace-cross 
Abstract: The effective degrees of freedom of penalized regression models quantify the actual amount of information used to generate predictions, playing a pivotal role in model evaluation and selection. Although a closed-form estimator is available for the Lasso penalty, adaptive extensions of widely used penalized approaches, including the Adaptive Lasso and Adaptive Group Lasso, have remained without analogous theoretical characterization. This paper presents the first unbiased estimator of the effective degrees of freedom for these methods, along with their main theoretical properties, for both orthogonal and non-orthogonal designs, derived within Stein's unbiased risk estimation framework. The resulting expressions feature inflation terms influenced by the regularization parameter, coefficient signs, and least-squares estimates. These advances enable more accurate model selection criteria and unbiased prediction error estimates, illustrated through synthetic and real data. These contributions offer a rigorous theoretical foundation for understanding model complexity in adaptive regression, bridging a critical gap between theory and practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21595v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mauro Bernardi, Antonio Canale, Marco Stefanucci</dc:creator>
    </item>
  </channel>
</rss>
