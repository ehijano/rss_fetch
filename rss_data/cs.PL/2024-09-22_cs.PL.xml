<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PL</link>
    <description>cs.PL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 23 Sep 2024 04:00:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Higher-Order Asynchronous Effects</title>
      <link>https://arxiv.org/abs/2307.13795</link>
      <description>arXiv:2307.13795v4 Announce Type: replace 
Abstract: We explore asynchronous programming with algebraic effects. We complement their conventional synchronous treatment by showing how to naturally also accommodate asynchrony within them, namely, by decoupling the execution of operation calls into signalling that an operation's implementation needs to be executed, and interrupting a running computation with the operation's result, to which the computation can react by installing interrupt handlers. We formalise these ideas in a small core calculus and demonstrate its flexibility using examples ranging from a multi-party web application, to pre-emptive multi-threading, to (cancellable) remote function calls, to a parallel variant of runners of algebraic effects. In addition, the paper is accompanied by a formalisation of the calculus's type safety proofs in Agda, and a prototype implementation in OCaml.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.13795v4</guid>
      <category>cs.PL</category>
      <category>cs.LO</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Danel Ahman, Matija Pretnar</dc:creator>
    </item>
    <item>
      <title>MonoCoder: Domain-Specific Code Language Model for HPC Codes and Tasks</title>
      <link>https://arxiv.org/abs/2312.13322</link>
      <description>arXiv:2312.13322v3 Announce Type: replace 
Abstract: With easier access to powerful compute resources, there is a growing trend in AI for software development to develop large language models (LLMs) to address a variety of programming tasks. Even LLMs applied to tasks from the high-performance computing (HPC) domain are huge in size and demand expensive compute resources for training. This is partly because LLMs for HPC tasks are obtained by finetuning existing LLMs that support several natural and/or programming languages. We found this design choice confusing - why do we need LLMs trained on natural languages and programming languages unrelated to HPC for HPC-specific tasks? In this line of work, we aim to question choices made by existing LLMs by developing smaller language models (LMs) for specific domains - we call them domain-specific LMs. Specifically, we start with HPC as a domain and build an HPC-specific LM, named MonoCoder, which is orders of magnitude smaller than existing LMs but delivers better performance on non-HPC and HPC codes. Specifically, we pre-trained MonoCoder on an HPC-specific dataset (named HPCorpus) of C and C++ programs mined from GitHub. We evaluated the performance of MonoCoder against state-of-the-art multi-lingual LLMs. Results demonstrate that MonoCoder, although much smaller than existing LMs, outperforms other LLMs on normalized-perplexity tests (in relation to model size) while also delivering competing CodeBLEU scores for high-performance and parallel code generations. In other words, results suggest that MonoCoder understands HPC code better than state-of-the-art LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.13322v3</guid>
      <category>cs.PL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.SE</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tal Kadosh, Niranjan Hasabnis, Vy A. Vo, Nadav Schneider, Neva Krien, Mihai Capota, Abdul Wasay, Nesreen Ahmed, Ted Willke, Guy Tamir, Yuval Pinter, Timothy Mattson, Gal Oren</dc:creator>
    </item>
    <item>
      <title>Incremental Context-free Grammar Inference in Black Box Settings</title>
      <link>https://arxiv.org/abs/2408.16706</link>
      <description>arXiv:2408.16706v2 Announce Type: replace 
Abstract: Black-box context-free grammar inference presents a significant challenge in many practical settings due to limited access to example programs. The state-of-the-art methods, Arvada and Treevada, employ heuristic approaches to generalize grammar rules, initiating from flat parse trees and exploring diverse generalization sequences. We have observed that these approaches suffer from low quality and readability, primarily because they process entire example strings, adding to the complexity and substantially slowing down computations. To overcome these limitations, we propose a novel method that segments example strings into smaller units and incrementally infers the grammar. Our approach, named Kedavra, has demonstrated superior grammar quality (enhanced precision and recall), faster runtime, and improved readability through empirical comparison.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16706v2</guid>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Feifei Li, Xiao Chen, Xi Xiao, Xiaoyu Sun, Chuan Chen, Shaohua Wang, Jitao Han</dc:creator>
    </item>
    <item>
      <title>A Comparison of Quantum Compilers using a DAG-based or phase polynomial-based Intermediate Representation</title>
      <link>https://arxiv.org/abs/2304.08814</link>
      <description>arXiv:2304.08814v2 Announce Type: replace-cross 
Abstract: In the NISQ era, where quantum computing is dominated by hybrid quantum algorithms, it is important for quantum circuits to be well-optimized to reduce noise from unnecessary gates.
  We investigate different phase polynomial-based compilation strategies to determine the current best practices and compare them against the DAG-based Qiskit and TKET compilers. We find that phase polynomial-based compiling is very fast compared to DAG-based compiling. For long circuits, these compilers generate fewer CNOT gates than Qiskit or TKET, but for short circuits, they are quite inefficient. We also show that supplementary algorithms such as Reverse Traversal and simulated annealing might improve the generated CNOT count slightly, but the effect is negligable in most settings and generally not worth the additional compiler runtime. Instead, more sophisticated phase polynomial synthesis algorithms are needed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.08814v2</guid>
      <category>quant-ph</category>
      <category>cs.PL</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Arianne Meijer - van de Griend</dc:creator>
    </item>
  </channel>
</rss>
