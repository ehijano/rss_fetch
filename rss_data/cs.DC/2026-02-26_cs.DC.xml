<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 26 Feb 2026 05:00:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>General Convex Agreement with Near-Optimal Communication</title>
      <link>https://arxiv.org/abs/2602.21411</link>
      <description>arXiv:2602.21411v1 Announce Type: new 
Abstract: Convex Agreement (CA) strengthens Byzantine Agreement (BA) by requiring the output agreed upon to lie in the convex hull of the honest parties' inputs. This validity condition is motivated by practical aggregation tasks (e.g., robust learning or sensor fusion) where honest inputs need not coincide but should still constrain the decision. CA inherits BA lower bounds, and optimal synchronous round complexity is easy to obtain (e.g., via Byzantine Broadcast). The main challenge is \emph{communication}: standard approaches for CA have a communication complexity of $\Theta(Ln^2)$ for large $L$-bit inputs, leaving a gap in contrast to BA's lower bound of $\Omega(Ln)$ bits. While recent work achieves optimal communication complexity of $O(Ln)$ for sufficiently large $L$ [GLW,PODC'25], translating this result to general convexity spaces remained an open problem.
  We investigate this gap for abstract convexity spaces, and we present deterministic synchronous CA protocols with near-optimal communication complexity: when $L = \Omega(n \cdot \kappa)$, where $\kappa$ is a security parameter, we achieve $O(L\cdot n\log n)$ communication for finite convexity spaces and $O(L\cdot n^{1+o(1)})$ communication for Euclidean spaces $\mathbb{R}^d$. Our protocols have asymptotically optimal round complexity $O(n)$ and, when a bound on the inputs' lengths $L$ is fixed a priori, we achieve near-optimal resilience $t &lt; n/(\omega+\varepsilon)$ for any constant $\varepsilon&gt;0$, where $\omega$ is the Helly number of the convexity space. If $L$ is unknown, we still achieve resilience $t&lt;n/(\omega+\varepsilon+1)$ for any constant $\varepsilon &gt; 0$. We further note that our protocols can be leveraged to efficiently solve parallel BA.
  Our main technical contribution is the use of extractor graphs to obtain a deterministic assignment of parties to committees, which is resilient against adaptive adversaries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21411v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marc Dufay, Diana Ghinea, Anton Paramonov</dc:creator>
    </item>
    <item>
      <title>DualPath: Breaking the Storage Bandwidth Bottleneck in Agentic LLM Inference</title>
      <link>https://arxiv.org/abs/2602.21548</link>
      <description>arXiv:2602.21548v1 Announce Type: new 
Abstract: The performance of multi-turn, agentic LLM inference is increasingly dominated by KV-Cache storage I/O rather than computation. In prevalent disaggregated architectures, loading the massive KV-Cache from external storage creates a fundamental imbalance: storage NICs on prefill engines become bandwidth-saturated, while those on decoding engines remain idle. This asymmetry severely constrains overall system throughput.
  We present DualPath, an inference system that breaks this bottleneck by introducing dual-path KV-Cache loading. Beyond the traditional storage-to-prefill path, DualPath enables a novel storage-to-decode path, in which the KV-Cache is loaded into decoding engines and then efficiently transferred to prefill engines via RDMA over the compute network. DualPath combines this optimized data path -- which inherently avoids network congestion and avoids interference with latency-critical model execution communications -- with a global scheduler that dynamically balances load across prefill and decode engines.
  Our evaluation on three models with production agentic workloads demonstrates that DualPath improves offline inference throughput by up to 1.87$\times$ on our in-house inference system. It can also improve online serving throughput by an average factor of 1.96$\times$ without violating SLO.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21548v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yongtong Wu, Shaoyuan Chen, Yinmin Zhong, Rilin Huang, Yixuan Tan, Wentao Zhang, Liyue Zhang, Shangyan Zhou, Yuxuan Liu, Shunfeng Zhou, Mingxing Zhang, Xin Jin, Panpan Huang</dc:creator>
    </item>
    <item>
      <title>Multi-Layer Scheduling for MoE-Based LLM Reasoning</title>
      <link>https://arxiv.org/abs/2602.21626</link>
      <description>arXiv:2602.21626v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have achieved remarkable success across a wide range of tasks, but serving them efficiently at scale remains a critical challenge due to their substantial computational and latency demands. While most existing inference frameworks rely on simple scheduling strategies such as First-Come-First-Serve (FCFS) at the engine level and Round-Robin (RR) at the scheduler or coordinator level, they often fail to fully utilize system resources and may suffer from issues such as head-of-line blocking and load imbalance. Recent advances in Mixture-of-Experts (MoE) models have also introduced new challenges in scheduling arising from expert parallelism and routing complexity. This research proposes a multi-layer scheduling framework tailored for MoE-based LLM serving. It targets scheduling at three levels: request-level, enginelevel, and expert-level. At the request level, we explore algorithms such as Shortest-Job-First (SJF) and priority-aware aging to improve throughput and reduce latency. At the engine level, we design load-aware dispatching strategies that account for the current prefix token load, KV cache utilization, and user stickiness to achieve better resource matching. At the expert level, we focus on alleviating expert hotspots and strategically placing inter-layer expert dependencies to balance load and improve routing efficiency. Extensive experimental results from more than 100 experiments conducted under diverse workload distributions show that our approach consistently outperforms the state-of-theart inference framework vLLM, achieving up to 17.8% reduction in Time To First Token (TTFT) latency and 13.3% reduction in Time-Per-Output-Token (TPOT) latency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21626v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yifan Sun, Gholamreza Haffar, Minxian Xu, Rajkumar Buyya, Adel N. Toosi</dc:creator>
    </item>
    <item>
      <title>Lamport's Arrow of Time: The Category Mistake in Logical Clocks</title>
      <link>https://arxiv.org/abs/2602.21730</link>
      <description>arXiv:2602.21730v1 Announce Type: new 
Abstract: Lamport's 1978 paper introduced the happens-before relation and logical clocks, freeing distributed systems from dependence on synchronized physical clocks. This is widely understood as a move away from Newtonian absolute time. We argue that Lamport's formalism retains a deeper and largely unexamined assumption: that causality induces a globally well-defined directed acyclic graph (DAG) over events -- a forward-in-time-only (FITO) structure that functions as an arrow of time embedded at the semantic level. Following Ryle's analysis of category mistakes, we show that this assumption conflates an epistemic construct (the logical ordering of messages) with an ontic claim (that physical causality is globally acyclic and monotonic). We trace this conflation through Shannon's channel model, TLA+, Bell's theorem, and the impossibility results of Fischer-Lynch-Paterson and Brewer's CAP theorem. We then show that special and general relativity permit only local causal structure, and that recent work on indefinite causal order demonstrates that nature admits correlations with no well-defined causal ordering. We propose that mutual information conservation, rather than temporal precedence, provides a more fundamental primitive for distributed consistency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21730v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul Borrill</dc:creator>
    </item>
    <item>
      <title>DHP: Efficient Scaling of MLLM Training with Dynamic Hybrid Parallelism</title>
      <link>https://arxiv.org/abs/2602.21788</link>
      <description>arXiv:2602.21788v1 Announce Type: new 
Abstract: Scaling long-context capabilities is crucial for Multimodal Large Language Models (MLLMs). However, real-world multimodal datasets are extremely heterogeneous. Existing training frameworks predominantly rely on static parallelism strategies, which suffer from severe load imbalance, redundant communication, and suboptimal hardware utilization under data heterogeneity. In this work, we propose Dynamic Hybrid Parallelism (DHP), an efficient parallelism strategy that adaptively reconfigures communication groups and parallelism degrees during MLLM training. We generalize the non-power-of-two parallelism degrees and develop a polynomial-time algorithm to generate near-optimal parallelism strategies with only millisecond-level overhead per training batch. DHP is able to maintain high hardware efficiency even under extreme data variability. Experimental results demonstrate that DHP significantly outperforms Megatron-LM and DeepSpeed, achieving up to 1.36 $\times$ speedup in training throughput while maintaining near-linear scaling efficiency across large-scale NPU clusters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21788v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yifan Niu, Han Xiao, Dongyi Liu, Wei Zhou, Jia Li</dc:creator>
    </item>
    <item>
      <title>A task-based data-flow methodology for programming heterogeneous systems with multiple accelerator APIs</title>
      <link>https://arxiv.org/abs/2602.21897</link>
      <description>arXiv:2602.21897v1 Announce Type: new 
Abstract: Heterogeneous nodes that combine multi-core CPUs with diverse accelerators are rapidly becoming the norm in both high-performance computing (HPC) and AI infrastructures. Exploiting these platforms, however, requires orchestrating several low-level accelerator APIs such as CUDA, SYCL, and Triton. In some occasions they can be combined with optimized vendor math libraries: e.g., cuBLAS and oneAPI. Each API or library introduces its own abstractions, execution semantics, and synchronization mechanisms. Combining them within a single application is therefore error-prone and labor-intensive. We propose reusing a task-based data-flow methodology together with Task-Aware APIs (TA-libs) to overcome these limitations and facilitate the seamless integration of multiple accelerator programming models, while still leveraging the best-in-class kernels offered by each API.
  Applications are expressed as a directed acyclic graph (DAG) of host tasks and device kernels managed by an OpenMP/OmpSs-2 runtime. We introduce Task-Aware SYCL (TASYCL) and leverage Task-Aware CUDA (TACUDA), which elevate individual accelerator invocations to first-class tasks. When multiple native runtimes coexist on the same multi-core CPU, they contend for threads, leading to oversubscription and performance variability. To address this, we unify their thread management under the nOS-V tasking and threading library, to which we contribute a new port of the PoCL (Portable OpenCL) runtime.
  These results demonstrate that task-aware libraries, coupled with the nOS-V library, enable a single application to harness multiple accelerator programming models transparently and efficiently. The proposed methodology is immediately applicable to current heterogeneous nodes and is readily extensible to future systems that integrate even richer combinations of CPUs, GPUs, FPGAs, and AI accelerators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21897v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.future.2026.108383</arxiv:DOI>
      <arxiv:journal_reference>Future Generation Computer Systems, Volume 180, July 2026, 108383</arxiv:journal_reference>
      <dc:creator>Aleix Bon\'e, Alejandro Aguirre, David \'Alvarez, Pedro J. Martinez-Ferrer, Vicen\c{c} Beltran</dc:creator>
    </item>
    <item>
      <title>Energy Efficient Federated Learning with Hyperdimensional Computing over Wireless Communication Networks</title>
      <link>https://arxiv.org/abs/2602.21949</link>
      <description>arXiv:2602.21949v1 Announce Type: new 
Abstract: In this paper, we investigate a problem of minimizing total energy consumption for secure federated learning (FL) over wireless edge networks. To address the high computational cost and privacy challenges in conventional FL with neural networks (NN) for resource-constrained users, we propose a novel FL with hyperdimensional computing and differential privacy (FL-HDC-DP) framework. In the considered model, each edge user employs hyperdimensional computing (HDC) for local training, which replaces complex neural updates with simple hypervector operations, and applies differential privacy (DP) noise to protect transmitted model information. We optimize the total energy of computation and communication under both latency and privacy constraints. We formulate the problem as an optimization that minimizes the total energy of all users by jointly allocating HDC dimension, transmission time, system bandwidth, transmit power, and CPU frequency. To solve this problem, a sigmoid-variant function is proposed to characterize the relationship between the HDC dimension and the convergence rounds required to reach a target accuracy. Based on this model, we develop two alternating optimization algorithms, where closed-form expressions for time, frequency, bandwidth, and power allocations are derived at each iteration. Since the iterative algorithm requires a feasible initialization, we construct a feasibility problem and obtain feasible initial resource parameters by solving a per round transmission time minimization problem. Simulation results demonstrate that the proposed FL-HDC-DP framework achieves up to 83.3% total energy reduction compared with the baseline, while attaining about 90% accuracy in approximately 3.5X fewer communication rounds than the NN baseline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21949v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yahao Ding, Yinchao Yang, Jiaxiang Wang, Zhaohui Yang, Dusit Niyato, Zhu Han, Mohammad Shikh-Bahaei</dc:creator>
    </item>
    <item>
      <title>IOAgent: Democratizing Trustworthy HPC I/O Performance Diagnosis Capability via LLMs</title>
      <link>https://arxiv.org/abs/2602.22017</link>
      <description>arXiv:2602.22017v1 Announce Type: new 
Abstract: As the complexity of the HPC storage stack rapidly grows, domain scientists face increasing challenges in effectively utilizing HPC storage systems to achieve their desired I/O performance. To identify and address I/O issues, scientists largely rely on I/O experts to analyze their I/O traces and provide insights into potential problems. However, with a limited number of I/O experts and the growing demand for data-intensive applications, inaccessibility has become a major bottleneck, hindering scientists from maximizing their productivity. Rapid advances in LLMs make it possible to build an automated tool that brings trustworthy I/O performance diagnosis to domain scientists. However, key challenges remain, such as the inability to handle long context windows, a lack of accurate domain knowledge about HPC I/O, and the generation of hallucinations during complex interactions.In this work, we propose IOAgent as a systematic effort to address these challenges. IOAgent integrates a module-based pre-processor, a RAG-based domain knowledge integrator, and a tree-based merger to accurately diagnose I/O issues from a given Darshan trace file. Similar to an I/O expert, IOAgent provides detailed justifications and references for its diagnoses and offers an interactive interface for scientists to ask targeted follow-up questions. To evaluate IOAgent, we collected a diverse set of labeled job traces and released the first open diagnosis test suite, TraceBench. Using this test suite, we conducted extensive evaluations, demonstrating that IOAgent matches or outperforms state-of-the-art I/O diagnosis tools with accurate and useful diagnosis results. We also show that IOAgent is not tied to specific LLMs, performing similarly well with both proprietary and open-source LLMs. We believe IOAgent has the potential to become a powerful tool for scientists navigating complex HPC I/O subsystems in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22017v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/IPDPS64566.2025.00036</arxiv:DOI>
      <dc:creator>Chris Egersdoerfer (DK), Arnav Sareen (DK), Jean Luca Bez (DK), Suren Byna (DK),  Dongkuan (DK),  Xu, Dong Dai</dc:creator>
    </item>
    <item>
      <title>PASTA: A Modular Program Analysis Tool Framework for Accelerators</title>
      <link>https://arxiv.org/abs/2602.22103</link>
      <description>arXiv:2602.22103v1 Announce Type: new 
Abstract: The increasing complexity and diversity of hardware accelerators in modern computing systems demand flexible, low-overhead program analysis tools. We present PASTA, a low-overhead and modular Program AnalysiS Tool Framework for Accelerators. PASTA abstracts over low-level profiling APIs and diverse deep learning frameworks, offering users a unified interface to capture and analyze runtime events at multiple levels. Its extensible design enables researchers and practitioners to rapidly prototype custom tools with minimal overhead. We demonstrate the utility of PASTA by developing several analysis tools, including a deep learning workload characterization tool and a UVM optimization tool. Through extensive evaluation on mainstream deep learning workloads tested on NVIDIA and AMD GPUs under both single- and multi-GPU scenarios, we demonstrate PASTA's broad applicability. On NVIDIA GPUs, we further show that PASTA provides detailed performance insights with significantly lower overhead, up to 1.3*10^4 faster than conventional analysis tools, thanks to its GPU-accelerated backend. PASTA strikes a practical balance between usability, extensibility, and efficiency, making it well-suited for modern accelerator-based computing environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22103v1</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mao Lin, Hyeran Jeon, Keren Zhou</dc:creator>
    </item>
    <item>
      <title>LLMTailor: A Layer-wise Tailoring Tool for Efficient Checkpointing of Large Language Models</title>
      <link>https://arxiv.org/abs/2602.22158</link>
      <description>arXiv:2602.22158v1 Announce Type: new 
Abstract: Checkpointing is essential for fault tolerance in training large language models (LLMs). However, existing methods, regardless of their I/O strategies, periodically store the entire model and optimizer states, incurring substantial storage overhead and resource contention. Recent studies reveal that updates across LLM layers are highly non-uniform. Across training steps, some layers may undergo more significant changes, while others remain relatively stable or even unchanged. This suggests that selectively checkpointing only layers with significant updates could reduce overhead without harming training. Implementing such selective strategies requires fine-grained control over both weights and optimizer states, which no current tool provides. To address this gap, we propose \texttt{LLMTailor}, a checkpoint-merging framework that filters and assembles layers from different checkpoints to form a composite checkpoint. Our evaluation indicates that LLMTailor can work with different selective checkpointing strategies and effectively reduce checkpoint size (e.g., 4.3 times smaller for Llama3.1-8B) and checkpoint time (e.g., 2.8 times faster for Qwen2.5-7B) while maintaining model quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22158v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3731599.3767515</arxiv:DOI>
      <dc:creator>Minqiu Sun, Xin Huang, Luanzheng Guo, Nathan R. Tallent, Kento Sato, Dong Dai</dc:creator>
    </item>
    <item>
      <title>Make Every Draft Count: Hidden State based Speculative Decoding</title>
      <link>https://arxiv.org/abs/2602.21224</link>
      <description>arXiv:2602.21224v1 Announce Type: cross 
Abstract: Speculative decoding has emerged as a pivotal technique to accelerate LLM inference by employing a lightweight draft model to generate candidate tokens that are subsequently verified by the target model in parallel. However, while this paradigm successfully increases the arithmetic intensity of memory-bound inference, it causes significant compute inefficiency: the majority of draft tokens fail verification and are discarded, resulting in waste of computation. Motivated by the goal of recollecting this wasted computation, we propose a novel system that transforms discarded drafts into reusable tokens. Our key insight is to perform auto-regressive prediction at the hidden states level and postpone the integrating token information after the hidden states generation, so the draft hidden states are not contaminated by incorrect tokens, enabling hidden state reuse. To implement such a system, first we introduce a draft model architecture based on auto-regressive hidden states, which preserves richer semantics than token-based drafters to facilitate draft repurposing. Second, we design an efficient token information injection mechanism that leverages our specialized draft model to construct high-quality draft token trees and enables resampling tokens from verification failures. Third, we eliminate the overhead hidden in our design to further maximize hardware utilization. We conducted extensive evaluations against various baselines, demonstrating up to a 3.3x speedup against standard speculative decoding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21224v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuetao Chen, Xuliang Wang, Xinzhou Zheng, Ming Li, Peng Wang, Hong Xu</dc:creator>
    </item>
    <item>
      <title>PiPNN: Ultra-Scalable Graph-Based Nearest Neighbor Indexing</title>
      <link>https://arxiv.org/abs/2602.21247</link>
      <description>arXiv:2602.21247v1 Announce Type: cross 
Abstract: The fastest indexes for Approximate Nearest Neighbor Search today are also the slowest to build: graph-based methods like HNSW and Vamana achieve state-of-the-art query performance but have large construction times due to relying on random-access-heavy beam searches. We introduce PiPNN (Pick-in-Partitions Nearest Neighbors), an ultra-scalable graph construction algorithm that avoids this ``search bottleneck'' that existing graph-based methods suffer from.
  PiPNN's core innovation is HashPrune, a novel online pruning algorithm which dynamically maintains sparse collections of edges. HashPrune enables PiPNN to partition the dataset into overlapping sub-problems, efficiently perform bulk distance comparisons via dense matrix multiplication kernels, and stream a subset of the edges into HashPrune. HashPrune guarantees bounded memory during index construction which permits PiPNN to build higher quality indices without the use of extra intermediate memory.
  PiPNN builds state-of-the-art indexes up to 11.6x faster than Vamana (DiskANN) and up to 12.9x faster than HNSW. PiPNN is significantly more scalable than recent algorithms for fast graph construction. PiPNN builds indexes at least 19.1x faster than MIRAGE and 17.3x than FastKCNA while producing indexes that achieve higher query throughput. PiPNN enables us to build, for the first time, high-quality ANN indexes on billion-scale datasets in under 20 minutes using a single multicore machine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21247v1</guid>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <category>cs.IR</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tobias Rubel, Richard Wen, Laxman Dhulipala, Lars Gottesb\"uren, Rajesh Jayaram, Jakub {\L}\k{a}cki</dc:creator>
    </item>
    <item>
      <title>Epoch-based Optimistic Concurrency Control in Geo-replicated Databases</title>
      <link>https://arxiv.org/abs/2602.21566</link>
      <description>arXiv:2602.21566v1 Announce Type: cross 
Abstract: Geo-distribution is essential for modern online applications to ensure service reliability and high availability. However, supporting high-performance serializable transactions in geo-replicated databases remains a significant challenge. This difficulty stems from the extensive over-coordination inherent in distributed atomic commitment, concurrency control, and fault-tolerance replication protocols under high network latency.
  To address these challenges, we introduce Minerva, a unified distributed concurrency control designed for highly scalable multi-leader replication. Minerva employs a novel epoch-based asynchronous replication protocol that decouples data propagation from the commitment process, enabling continuous transaction replication. Optimistic concurrency control is used to allow any replicas to execute transactions concurrently and commit without coordination. In stead of aborting transactions when conflicts are detected, Minerva uses deterministic re-execution to resolve conflicts, ensuring serializability without sacrificing performance. To further enhance concurrency, we construct a conflict graph and use a maximum weight independent set algorithm to select the optimal subset of transactions for commitment, minimizing the number of re-executed transactions. Our evaluation demonstrates that Minerva significantly outperforms state-of-the-art replicated databases, achieving over $3\times$ higher throughput in scalability experiments and $2.8\times$ higher throughput during a high network latency simulation with the TPC-C benchmark.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21566v1</guid>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunhao Mao, Harunari Takata, Michail Bachras, Yuqiu Zhang, Shiquan Zhang, Gengrui Zhang, Hans-Arno Jacobsen</dc:creator>
    </item>
    <item>
      <title>Type-Based Enforcement of Non-Interference for Choreographic Programming</title>
      <link>https://arxiv.org/abs/2602.21630</link>
      <description>arXiv:2602.21630v1 Announce Type: cross 
Abstract: Choreographies describe distributed protocols from a global viewpoint, enabling correct-by-construction synthesis of local behaviours. We develop a policy-parametric type system that prevents information leaks from high-security data to low-security observers, handling both explicit and implicit flows through a program-counter discipline. The system supports recursive procedures via a procedure context that we reconstruct through constraint generation. We prove termination-insensitive non-interference with respect to a standard small-step semantics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21630v1</guid>
      <category>cs.PL</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marco Bertoni, Saverio Giallorenzo, Marco Peressotti</dc:creator>
    </item>
    <item>
      <title>JSAM: Privacy Straggler-Resilient Joint Client Selection and Incentive Mechanism Design in Differentially Private Federated Learning</title>
      <link>https://arxiv.org/abs/2602.21844</link>
      <description>arXiv:2602.21844v1 Announce Type: cross 
Abstract: Differentially private federated learning faces a fundamental tension: privacy protection mechanisms that safeguard client data simultaneously create quantifiable privacy costs that discourage participation, undermining the collaborative training process. Existing incentive mechanisms rely on unbiased client selection, forcing servers to compensate even the most privacy-sensitive clients ("privacy stragglers"), leading to systemic inefficiency and suboptimal resource allocation. We introduce JSAM (Joint client Selection and privacy compensAtion Mechanism), a Bayesian-optimal framework that simultaneously optimizes client selection probabilities and privacy compensation to maximize training effectiveness under budget constraints. Our approach transforms a complex 2N-dimensional optimization problem into an efficient three-dimensional formulation through novel theoretical characterization of optimal selection strategies. We prove that servers should preferentially select privacy-tolerant clients while excluding high-sensitivity participants, and uncover the counter-intuitive insight that clients with minimal privacy sensitivity may incur the highest cumulative costs due to frequent participation. Extensive evaluations on MNIST and CIFAR-10 demonstrate that JSAM achieves up to 15% improvement in test accuracy compared to existing unbiased selection mechanisms while maintaining cost efficiency across varying data heterogeneity levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21844v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.GT</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruichen Xu, Ying-Jun Angela Zhang, Jianwei Huang</dc:creator>
    </item>
    <item>
      <title>Hybrid Consensus with Quantum Sybil Resistance</title>
      <link>https://arxiv.org/abs/2602.22195</link>
      <description>arXiv:2602.22195v1 Announce Type: cross 
Abstract: Sybil resistance is a key requirement of decentralized consensus protocols. It is achieved by introducing a scarce resource (such as computational power, monetary stake, disk space, etc.), which prevents participants from costlessly creating multiple fake identities and hijacking the protocol. Quantum states are generically uncloneable, which suggests that they may serve naturally as an unconditionally scarce resource. In particular, uncloneability underlies quantum position based-cryptography, which is unachievable classically. We design a consensus protocol that combines classical hybrid consensus protocols with quantum position verification as the Sybil resistance mechanism, providing security in the standard model, and achieving improved energy efficiency compared to hybrid protocols based on Proof-of-Work. Our protocol inherits the benefits of other hybrid protocols, namely the faster confirmation times compared to pure Proof-of-Work protocols, and resilience against the compounding wealth issue that plagues protocols based on Proof-of-Stake Sybil resistance. We additionally propose a spam prevention mechanism for our protocol in the Random Oracle model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22195v1</guid>
      <category>quant-ph</category>
      <category>cs.DC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dar Gilboa, Siddhartha Jain, Or Sattath</dc:creator>
    </item>
    <item>
      <title>Strong and Hiding Distributed Certification of Bipartiteness</title>
      <link>https://arxiv.org/abs/2502.13854</link>
      <description>arXiv:2502.13854v3 Announce Type: replace 
Abstract: In this paper, we study the problem of certifying whether a graph is bipartite (i.e. $2$-colorable) with a locally checkable proof (LCP) that is able to hide a $2$-coloring from the verifier. More precisely, we say an LCP for $2$-coloring is hiding if, in a yes-instance, it is possible to assign certificates to nodes without revealing an explicit $2$-coloring. Motivated by the search for promise-free separations of extensions of the LOCAL model in the context of locally checkable labeling (LCL) problems, we also require the LCPs to satisfy what we refer to as the strong soundness property. This is a strengthening of soundness that requires that, in a no-instance (i.e., a non-$2$-colorable graph) and for every certificate assignment, the subset of accepting nodes must induce a $2$-colorable subgraph.
  We show that strong and hiding LCPs for $2$-coloring exist in specific graph classes and requiring only $O(\log n)$-sized certificates. Furthermore, when the input is promised to be a cycle or contains a node of degree $1$, we show the existence of strong and hiding LCPs even in an anonymous network and with constant-size certificates.
  Despite these upper bounds, we prove that there are no strong and hiding LCPs for $2$-coloring in general, unless the algorithm has access to node identifiers and uses certificates of size~$\omega(1)$. Furthermore, in anonymous networks, the lower bound holds regardless of the certificate size. The proof relies on a Ramsey-type result as well as an argument about the realizability of subgraphs of the neighborhood graph consisting of the accepting views of an LCP. Along the way, we also give a characterization of the hiding property for the general $k$-coloring problem that appears to be a key component for future investigations in this context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13854v3</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Benjamin Jauregui, Augusto Modanese, Pedro Montealegre, Mart\'in R\'ios-Wilson</dc:creator>
    </item>
    <item>
      <title>Parallelizing the Approximate Minimum Degree Ordering Algorithm: Strategies and Evaluation</title>
      <link>https://arxiv.org/abs/2504.17097</link>
      <description>arXiv:2504.17097v2 Announce Type: replace 
Abstract: The approximate minimum degree algorithm is widely used before numerical factorization to reduce fill-in for sparse matrices. While considerable attention has been given to the numerical factorization process, less focus has been placed on parallelizing the approximate minimum degree algorithm itself. In this paper, we explore different parallelization strategies, and introduce a novel parallel framework that leverages multiple elimination on distance-2 independent sets. Our evaluation shows that parallelism within individual elimination steps is limited due to low computational workload and significant memory contention. In contrast, our proposed framework overcomes these challenges by parallelizing the work across elimination steps. To the best of our knowledge, our implementation is the first scalable shared memory implementation of the approximate minimum degree algorithm. Experimental results show that we achieve up to a 7.29x speedup using 64 threads over the state-of-the-art sequential implementation in SuiteSparse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17097v2</guid>
      <category>cs.DC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1137/1.9781611979022.1</arxiv:DOI>
      <arxiv:journal_reference>Proc. 2026 SIAM Conf. on Parallel Processing for Scientific Computing (PP26), pp. 1-15 (2026)</arxiv:journal_reference>
      <dc:creator>Yen-Hsiang Chang, Ayd{\i}n Bulu\c{c}, James Demmel</dc:creator>
    </item>
    <item>
      <title>Optimizing Allreduce Operations for Modern Heterogeneous Architectures with Multiple Processes per GPU</title>
      <link>https://arxiv.org/abs/2508.13397</link>
      <description>arXiv:2508.13397v2 Announce Type: replace 
Abstract: Large inter-GPU all-reduce operations, prevalent throughout deep learning, are bottlenecked by communication costs. Emerging heterogeneous architectures are comprised of complex nodes, often containing $4$ GPUs and dozens to hundreds of CPU cores per node. Parallel applications are typically accelerated on the available GPUs, using only a single CPU core per GPU while the remaining cores sit idle. This paper presents novel optimizations to large GPU-aware all-reduce operations by extending the lane-aware algorithm to heterogeneous architectures and notably using multiple CPU cores per GPU to accelerate these operations. Using GPUDirect RDMA and host copy communications respectively, these multi-CPU-accelerated GPU-aware all-reduces yield speedups over system MPI of up to $3$x on LLNL's Tuolumne supercomputer and up to $2.45$x for large MPI all-reduces across the NVIDIA A100 GPUs of NCSA's Delta supercomputer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13397v2</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Adams, Amanda Bienz</dc:creator>
    </item>
    <item>
      <title>A Uniqueness Theorem for Distributed Computation under Physical Constraint</title>
      <link>https://arxiv.org/abs/2509.11754</link>
      <description>arXiv:2509.11754v2 Announce Type: replace 
Abstract: Foundational models of computation often abstract away physical hardware limitations. However, in extreme environments like In-Network Computing (INC), these limitations become inviolable laws, creating an acute trilemma among communication efficiency, bounded memory, and robust scalability. Prevailing distributed paradigms, while powerful in their intended domains, were not designed for this stringent regime and thus face fundamental challenges. This paper demonstrates that resolving this trilemma requires a shift in perspective - from seeking engineering trade-offs to deriving solutions from logical necessity. We establish a rigorous axiomatic system that formalizes these physical constraints and prove that for the broad class of computations admitting an idempotent merge operator, there exists a unique, optimal paradigm. Any system satisfying these axioms must converge to a single normal form: Self-Describing Parallel Flows (SDPF), a purely data-centric model where stateless executors process flows that carry their own control logic. We further prove this unique paradigm is convergent, Turing-complete, and minimal. In the same way that the CAP theorem established a boundary for what is impossible in distributed state management, our work provides a constructive dual: a uniqueness theorem that reveals what is \textit{inevitable} for distributed computation flows under physical law.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11754v2</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiyuan Ren, Mingxuan Lu, Wenchi Cheng</dc:creator>
    </item>
    <item>
      <title>BiScale: Energy-Efficient Disaggregated LLM Serving via Phase-Aware Placement and DVFS</title>
      <link>https://arxiv.org/abs/2602.18755</link>
      <description>arXiv:2602.18755v2 Announce Type: replace 
Abstract: Prefill/decode disaggregation is increasingly adopted in LLM serving to improve the latency-throughput tradeoff and meet strict TTFT and TPOT SLOs. However, LLM inference remains energy-hungry: autoscaling alone is too coarse-grained to track fast workload fluctuations, and applying fine-grained DVFS under disaggregation is complicated by phase-asymmetric dynamics and coupling between provisioning and frequency control.
  We present BiScale, a two-tier energy optimization framework for disaggregated LLM serving. BiScale jointly optimizes placement and DVFS across prefill and decode using predictive latency and power models. At coarse timescales, BiScale computes phase-aware placement and baseline frequencies that minimize energy while satisfying SLO constraints. At fine timescales, BiScale dynamically adapts GPU frequency per iteration using stage-specific control: model predictive control (MPC) for prefill to account for queue evolution and future TTFT impact, and lightweight slack-aware adaptation for decode to exploit its smoother, memory-bound dynamics. This hierarchical design enables coordinated control across timescales while preserving strict serving SLOs.
  Evaluation on a 16x H100 cluster serving Llama 3.3 70B with production-style traces shows that BiScale meets TTFT/TPOT SLOs while reducing energy by up to 39% in prefill and 48% in decode relative to DistServe.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18755v2</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Omar Basit, Yunzhao Liu, Z. Jonny Kong, Y. Charlie Hu</dc:creator>
    </item>
    <item>
      <title>A Granularity Characterization of Task Scheduling Effectiveness</title>
      <link>https://arxiv.org/abs/2602.20561</link>
      <description>arXiv:2602.20561v2 Announce Type: replace 
Abstract: Task-based runtime systems provide flexible load balancing and portability for parallel scientific applications, but their strong scaling is highly sensitive to task granularity. As parallelism increases, scheduling overhead may transition from negligible to dominant, leading to rapid drops in performance for some algorithms, while remaining negligible for others. Although such effects are widely observed empirically, there is a general lack of understanding how algorithmic structure impacts whether dynamic scheduling is always beneficial. In this work, we introduce a granularity characterization framework that directly links scheduling overhead growth to task-graph dependency topology. We show that dependency structure, rather than problem size alone, governs how overhead scales with parallelism. Based on this observation, we characterize execution behavior using a simple granularity measure that indicates when scheduling overhead can be amortized by parallel computation and when scheduling overhead dominates performance. Through experimental evaluation on representative parallel workloads with diverse dependency patterns, we demonstrate that the proposed characterization explains both gradual and abrupt strong-scaling breakdowns observed in practice. We further show that overhead models derived from dependency topology accurately predict strong-scaling limits and enable a practical runtime decision rule for selecting dynamic or static execution without requiring exhaustive strong-scaling studies or extensive offline tuning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20561v2</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sana Taghipour Anvari, David Kaeli</dc:creator>
    </item>
    <item>
      <title>A Morton-Type Space-Filling Curve for Pyramid Subdivision and Hybrid Adaptive Mesh Refinement</title>
      <link>https://arxiv.org/abs/2602.20887</link>
      <description>arXiv:2602.20887v2 Announce Type: replace 
Abstract: The forest-of-refinement-trees approach allows for dynamic adaptive mesh refinement (AMR) at negligible cost. While originally developed for quadrilateral and hexahedral elements, previous work established the theory and algorithms for unstructured meshes of simplicial and prismatic elements. To harness the full potential of tree-based AMR for three-dimensional mixed-element meshes, this paper introduces the pyramid as a new functional element type; its primary purpose is to connect tetrahedral and hexahedral elements without hanging edges. We present a well-defined space-filling curve (SFC) for the pyramid and detail how the unique challenges on the element and forest level associated with the pyramidal refinement are resolved. We propose the necessary functional design and generalize the fundamental global parallel algorithms for refinement, coarsening, partitioning, and face ghost exchange to fully support this new element. Our demonstrations confirm the efficiency and scalability of this complete, hybrid-element dynamic AMR framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20887v2</guid>
      <category>cs.DC</category>
      <category>cs.CG</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Knapp, Johannes Albrecht Holke, Thomas Spenke, Carsten Burstedde</dc:creator>
    </item>
    <item>
      <title>Is a LOCAL algorithm computable?</title>
      <link>https://arxiv.org/abs/2602.21022</link>
      <description>arXiv:2602.21022v2 Announce Type: replace 
Abstract: Common definitions of the "standard" LOCAL model tend to be sloppy and even self-contradictory on one point: do the nodes update their state using an arbitrary function or a computable function? So far, this distinction has been safe to neglect, since problems where it matters seem contrived and quite different from e.g. typical local graph problems studied in this context.
  We show that this question matters even for locally checkable labeling problems (LCLs), perhaps the most widely studied family of problems in the context of the LOCAL model. Furthermore, we show that assumptions about computability are directly connected to another aspect already recognized as highly relevant: whether we have any knowledge of $n$, the size of the graph. Concretely, we show that there is an LCL problem $\Pi$ with the following properties:
  1. $\Pi$ can be solved in $O(\log n)$ rounds if the LOCAL model is uncomputable.
  2. $\Pi$ can be solved in $O(\log n)$ rounds in the computable model if we know any upper bound on $n$.
  3. $\Pi$ requires $\Omega(\sqrt{n})$ rounds in the computable model if we do not know anything about $n$.
  We also show that the connection between computability and knowledge of $n$ holds in general: for any LCL problem $\Pi$, if you have any bound on $n$, then $\Pi$ has the same round complexity in the computable and uncomputable models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21022v2</guid>
      <category>cs.DC</category>
      <category>cs.CC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Antonio Cruciani, Avinandan Das, Massimo Equi, Henrik Lievonen, Diep Luong-Le, Augusto Modanese, Jukka Suomela</dc:creator>
    </item>
    <item>
      <title>Parallel Split Learning with Global Sampling</title>
      <link>https://arxiv.org/abs/2407.15738</link>
      <description>arXiv:2407.15738v5 Announce Type: replace-cross 
Abstract: Distributed deep learning in resource-constrained environments faces scalability and generalization challenges due to large effective batch sizes and non-identically distributed client data. We introduce a server-driven sampling strategy that maintains a fixed global batch size by dynamically adjusting client-side batch sizes. This decouples the effective batch size from the number of participating devices and ensures that global batches better reflect the overall data distribution. Using standard concentration bounds, we establish tighter deviation guarantees compared to existing approaches. Empirical results on a benchmark dataset confirm that the proposed method improves model accuracy, training efficiency, and convergence stability, offering a scalable solution for learning at the network edge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15738v5</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/FLLM67465.2025.11391108</arxiv:DOI>
      <dc:creator>Mohammad Kohankhaki, Ahmad Ayad, Mahdi Barhoush, Anke Schmeink</dc:creator>
    </item>
    <item>
      <title>MPPI-Generic: A CUDA Library for Stochastic Trajectory Optimization</title>
      <link>https://arxiv.org/abs/2409.07563</link>
      <description>arXiv:2409.07563v4 Announce Type: replace-cross 
Abstract: This paper introduces a new C++/CUDA library for GPU-accelerated stochastic optimization called MPPI-Generic. It provides implementations of Model Predictive Path Integral control, Tube-Model Predictive Path Integral Control, and Robust Model Predictive Path Integral Control, and allows for these algorithms to be used across many pre-existing dynamics models and cost functions. Furthermore, researchers can create their own dynamics models or cost functions following our API definitions without needing to change the actual Model Predictive Path Integral Control code. Finally, we compare computational performance to other popular implementations of Model Predictive Path Integral Control over a variety of GPUs to show the real-time capabilities our library can allow for. Library code can be found at: https://acdslab.github.io/mppi-generic-website/ .</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07563v4</guid>
      <category>cs.MS</category>
      <category>cs.DC</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bogdan Vlahov, Jason Gibson, Manan Gandhi, Evangelos A. Theodorou</dc:creator>
    </item>
    <item>
      <title>Accelerating Recommender Model ETL with a Streaming FPGA-GPU Dataflow</title>
      <link>https://arxiv.org/abs/2501.12032</link>
      <description>arXiv:2501.12032v3 Announce Type: replace-cross 
Abstract: The real-time performance of recommender models depends on the continuous integration of massive volumes of new user interaction data into training pipelines. While GPUs have scaled model training throughput, the data preprocessing stage - commonly expressed as Extract-Transform-Load (ETL) pipelines - has emerged as the dominant bottleneck. Production systems often dedicate clusters of CPU servers to support a single GPU node, leading to high operational cost. To address this issue, we present PipeRec, a hardware-accelerated ETL engine co-designed with online recommender model training. PipeRec introduces a training-aware ETL abstraction that exposes freshness, ordering, and batching semantics while compiling software-defined operators into reconfigurable FPGA dataflows and overlaps ETL with GPU training to maximize utilization under I/O constraints. To eliminate CPU bottlenecks, PipeRec implements a format-aware packer that streams training-ready batches directly into GPU memory via P2P DMA transfers, enabling zero-copy ingest and efficient GPU consumption. Our evaluation on three datasets shows that PipeRec accelerates ETL throughput by over 10x compared to CPU-based pipelines and up to 17x over state-of-the-art GPU ETL systems. When integrated with training, PipeRec maintains 64-91% GPU utilization and reduces end-to-end training time to 9.94% of the time taken by CPU-GPU pipelines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12032v3</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yu Zhu, Wenqi Jiang, Piyumi Jasin Pathiranage, Yongjun He, Gustavo Alonso</dc:creator>
    </item>
    <item>
      <title>On the Inference (In-)Security of Vertical Federated Learning: Efficient Auditing against Inference Tampering Attack</title>
      <link>https://arxiv.org/abs/2507.02376</link>
      <description>arXiv:2507.02376v2 Announce Type: replace-cross 
Abstract: Vertical Federated Learning (VFL) is an emerging distributed learning paradigm for cross-silo collaboration without accessing participants' data. However, existing VFL work lacks a mechanism to audit the inference correctness of the data party. The malicious data party can modify the local data and model to mislead the joint inference results. To exploit this vulnerability, we design a novel Vertical Federated Inference Tampering (VeFIT) attack, allowing the data party to covertly tamper with the local inference and mislead results on the task party's final prediction. VeFIT can decrease the task party's inference accuracy by an average of 34.49%. Existing defense mechanisms can not effectively detect this attack, and the detection performance is near random guessing. To mitigate the attack, we further design a Vertical Federated Inference Auditing (VeFIA) framework. VeFIA helps the task party to audit whether the data party's inferences are executed as expected during large-scale online inference. VeFIA does not leak the data party's privacy nor introduce additional latency. The core design is that the task party can use the inference results from a framework with Trusted Execution Environments (TEE) and the coordinator to validate the correctness of the data party's computation results. VeFIA guarantees that, as long as the proportion of inferences attacked by VeFIT exceeds 5.4%, the task party can detect the malicious behavior of the data party with a probability of 99.99%, without any additional online overhead. VeFIA's random sampling validation of VeFIA achieves 100% positive predictive value, negative predictive value, and true positive rate in detecting VeFIT. We further validate VeFIA's effectiveness in terms of privacy protection and scalability on real-world datasets. To the best of our knowledge, this is the first paper discussing the inference auditing problem towards VFL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02376v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chung-ju Huang, Ziqi Zhang, Yinggui Wang, Binghui Wang, Tao Wei, Leye Wang</dc:creator>
    </item>
  </channel>
</rss>
