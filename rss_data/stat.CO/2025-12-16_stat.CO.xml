<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 16 Dec 2025 05:01:47 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>StochTree: BART-based modeling in R and Python</title>
      <link>https://arxiv.org/abs/2512.12051</link>
      <description>arXiv:2512.12051v1 Announce Type: new 
Abstract: stochtree is a C++ library for Bayesian tree ensemble models such as BART and Bayesian Causal Forests (BCF), as well as user-specified variations. Unlike previous BART packages, stochtree provides bindings to both R and Python for full interoperability. stochtree boasts a more comprehensive range of models relative to previous packages, including heteroskedastic forests, random effects, and treed linear models. Additionally, stochtree offers flexible handling of model fits: the ability to save model fits, reinitialize models from existing fits (facilitating improved model initialization heuristics), and pass fits between R and Python. On both platforms, stochtree exposes lower-level functionality, allowing users to specify models incorporating Bayesian tree ensembles without needing to modify C++ code. We illustrate the use of stochtree in three settings: i) straightfoward applications of existing models such as BART and BCF, ii) models that include more sophisticated components like heteroskedasticity and leaf-wise regression models, and iii) as a component of custom MCMC routines to fit nonstandard tree ensemble models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12051v1</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Herren, P. Richard Hahn, Jared Murray, Carlos Carvalho</dc:creator>
    </item>
    <item>
      <title>Complexity of Markov Chain Monte Carlo for Generalized Linear Models</title>
      <link>https://arxiv.org/abs/2512.12748</link>
      <description>arXiv:2512.12748v1 Announce Type: new 
Abstract: Markov Chain Monte Carlo (MCMC), Laplace approximation (LA) and variational inference (VI) methods are popular approaches to Bayesian inference, each with trade-offs between computational cost and accuracy. However, a theoretical understanding of these differences is missing, particularly when both the sample size $n$ and the dimension $d$ are large. LA and Gaussian VI are justified by Bernstein-von Mises (BvM) theorems, and recent work has derived the characteristic condition $n\gg d^2$ for their validity, improving over the condition $n\gg d^3$. In this paper, we show for linear, logistic and Poisson regression that for $n\gtrsim d$, MCMC attains the same complexity scaling in $n$, $d$ as first-order optimization algorithms, up to sub-polynomial factors. Thus MCMC is competitive with LA and Gaussian VI in complexity, under a scaling between $n$ and $d$ more general than BvM regimes. Our complexities apply to appropriately scaled priors that are not necessarily Gaussian-tailed, including Student-$t$ and flat priors, with log-posteriors that are not necessarily globally concave or gradient-Lipschitz.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12748v1</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Chak, Giacomo Zanella</dc:creator>
    </item>
    <item>
      <title>Flow-matching Operators for Residual-Augmented Probabilistic Learning of Partial Differential Equations</title>
      <link>https://arxiv.org/abs/2512.12749</link>
      <description>arXiv:2512.12749v1 Announce Type: new 
Abstract: Learning probabilistic surrogates for PDEs remains challenging in data-scarce regimes: neural operators require large amounts of high-fidelity data, while generative approaches typically sacrifice resolution invariance. We formulate flow matching in an infinite-dimensional function space to learn a probabilistic transport that maps low-fidelity approximations to the manifold of high-fidelity PDE solutions via learned residual corrections. We develop a conditional neural operator architecture based on feature-wise linear modulation for flow-matching vector fields directly in function space, enabling inference at arbitrary spatial resolutions without retraining. To improve stability and representational control of the induced neural ODE, we parameterize the flow vector field as a sum of a linear operator and a nonlinear operator, combining lightweight linear components with a conditioned Fourier neural operator for expressive, input-dependent dynamics. We then formulate a residual-augmented learning strategy where the flow model learns probabilistic corrections from inexpensive low-fidelity surrogates to high-fidelity solutions, rather than learning the full solution mapping from scratch. Finally, we derive tractable training objectives that extend conditional flow matching to the operator setting with input-function-dependent couplings. To demonstrate the effectiveness of our approach, we present numerical experiments on a range of PDEs, including the 1D advection and Burgers' equation, and a 2D Darcy flow problem for flow through a porous medium.
  We show that the proposed method can accurately learn solution operators across different resolutions and fidelities and produces uncertainty estimates that appropriately reflect model confidence, even when trained on limited high-fidelity data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12749v1</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sahil Bhola, Karthik Duraisamy</dc:creator>
    </item>
    <item>
      <title>Scalable Spatial Stream Network (S3N) Models</title>
      <link>https://arxiv.org/abs/2512.12398</link>
      <description>arXiv:2512.12398v1 Announce Type: cross 
Abstract: Understanding how habitats shape species distributions and abundances across spatially complex, dendritic freshwater networks remains a longstanding and fundamental challenge in ecology, with direct implications for effective biodiversity management and conservation. Existing spatial stream network (SSN) models adapt spatial process models to river networks by creating covariance functions that account for stream distance, but preprocessing and estimation with these models is both computationally and time intensive, thus precluding the application of these models to regional or continental scales. This paper introduces a new class of Scalable Spatial Stream Network (S3N) models, which extend nearest-neighbor Gaussian processes to incorporate ecologically relevant spatial dependence while greatly improving computational efficiency. The S3N framework enables scalable modeling of spatial stream networks, demonstrated here for 285 fish species in the Ohio River Basin (&gt;4,000 river km). Validation analyses show that S3N accurately recovers spatial and covariance parameters, even with reduced bias and variance compared to standard SSN implementations. These results represent a key advancement toward large-scale mapping of freshwater fish distributions and quantifying the influence of environmental drivers across extensive river networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12398v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jessica P. Kunke, Julian D. Olden, Tyler H. McCormick</dc:creator>
    </item>
    <item>
      <title>Sleep pattern profiling using a finite mixture of contaminated multivariate skew-normal distributions on incomplete data</title>
      <link>https://arxiv.org/abs/2512.12464</link>
      <description>arXiv:2512.12464v1 Announce Type: cross 
Abstract: Medical data often exhibit characteristics that make cluster analysis particularly challenging, such as missing values, outliers, and cluster features like skewness. Typically, such data would need to be preprocessed -- by cleaning outliers and missing values -- before clustering could be performed. However, these preliminary steps rely on objective functions different from those used in the clustering stage. In this paper, we propose a unified model-based clustering approach that simultaneously handles atypical observations, missing values, and cluster-wise skewness within a single framework. Each cluster is modelled using a contaminated multivariate skew-normal distribution -- a convenient two-component mixture of multivariate skew-normal densities -- in which one component represents the main data (the "bulk") and the other captures potential outliers. From an inferential perspective, we implement and use a variant of the EM algorithm to obtain the maximum likelihood estimates of the model parameters. Simulation studies demonstrate that the proposed model outperforms existing approaches in both clustering accuracy and outlier detection, across low- and high-dimensional settings, even in the presence of substantial missingness. The method is further applied to the Cleveland Children's Sleep and Health Study (CCSHS), a dataset characterised by incomplete observations. Without any preprocessing, the proposed approach identifies five distinct groups of sleepers, revealing meaningful differences in sleeper typologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12464v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jason Pillay, Cristina Tortora, Antonio Punzo, Andriette Bekker</dc:creator>
    </item>
    <item>
      <title>A Bayesian approach to learning mixtures of nonparametric components</title>
      <link>https://arxiv.org/abs/2512.12988</link>
      <description>arXiv:2512.12988v1 Announce Type: cross 
Abstract: Mixture models are widely used in modeling heterogeneous data populations. A standard approach of mixture modeling is to assume that the mixture component takes a parametric kernel form, while the flexibility of the model can be obtained by using a large or possibly unbounded number of such parametric kernels. In many applications, making parametric assumptions on the latent subpopulation distributions may be unrealistic, which motivates the need for nonparametric modeling of the mixture components themselves. In this paper we study finite mixtures with nonparametric mixture components, using a Bayesian nonparametric modeling approach. In particular, it is assumed that the data population is generated according to a finite mixture of latent component distributions, where each component is endowed with a Bayesian nonparametric prior such as the Dirichlet process mixture. We present conditions under which the individual mixture component's distributions can be identified, and establish posterior contraction behavior for the data population's density, as well as densities of the latent mixture components. We develop an efficient MCMC algorithm for posterior inference and demonstrate via simulation studies and real-world data illustrations that it is possible to efficiently learn complex distributions for the latent subpopulations. In theory, the posterior contraction rate of the component densities is nearly polynomial, which is a significant improvement over the logarithm convergence rate of estimating mixing measures via deconvolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12988v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yilei Zhang, Yun Wei, Aritra Guha, XuanLong Nguyen</dc:creator>
    </item>
    <item>
      <title>Group-averaged Markov chains II: tuning of group action in finite state space</title>
      <link>https://arxiv.org/abs/2512.13067</link>
      <description>arXiv:2512.13067v1 Announce Type: cross 
Abstract: We study group-averaged Markov chains obtained by augmenting a $\pi$-stationary transition kernel $P$ with a group action on the state space via orbit kernels. Given a group $\mathcal{G}$ with orbits $(\mathcal{O}_i)_{i=1}^k$, we analyse three canonical orbit kernels: namely the Gibbs $(G)$, Metropolis-Hastings $(M)$, and Barker $(B)$ kernels, as well as their multiplicative sandwiches $QPQ$ and the additive mixtures $\frac{1}{2}(P+Q)$ where $Q\in\{G,M,B\}$. We show that $M^t, B^t \to G$ blockwise as $t \to \infty$ under suitable conditions, that the projection chains induced by $(\mathcal{O}_i)_{i=1}^k$ coincide for $GPG$ and $P$, and that orbit averaging never deteriorates the absolute spectral gap or asymptotic variance when $P$ is reversible. We give a direct and simple proof of Pythagorean identity under the Kullback-Leibler (KL) divergence, showing that $GPG$ arises naturally as an information projection of $P$ onto the set of $G$-invariant transition matrices. For a given $P$, we characterise the optimal choice of $G$ with a fixed number of orbits that minimises the one-step KL divergence to stationarity. Analogously, for a given $G$, we characterise the optimal choice of $P$ and give sufficient conditions under which $GPG = \Pi$. We further show that alternating projections over multiple group actions converge at a rate governed by the singular values of an overlap matrix, and that in structured cases, this yields exact sampling where the number of group actions grows logarithmically with the size of the state space. Based on the theory, we propose two heuristics to tune $G$ in practice. We also illustrate the results on discrete uniform and multimodal examples, including the Curie-Weiss model where $GPG$ achieves polynomial (in inverse temperature and dimension) mixing while Glauber dynamics remains exponentially slow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13067v1</guid>
      <category>math.PR</category>
      <category>cs.IT</category>
      <category>math.GR</category>
      <category>math.IT</category>
      <category>stat.CO</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael C. H. Choi, Ryan J. Y. Lim, Youjia Wang</dc:creator>
    </item>
    <item>
      <title>Temporal parallelisation of continuous-time maximum-a-posteriori trajectory estimation</title>
      <link>https://arxiv.org/abs/2512.13319</link>
      <description>arXiv:2512.13319v1 Announce Type: cross 
Abstract: This paper proposes a parallel-in-time method for computing continuous-time maximum-a-posteriori (MAP) trajectory estimates of the states of partially observed stochastic differential equations (SDEs), with the goal of improving computational speed on parallel architectures. The MAP estimation problem is reformulated as a continuous-time optimal control problem based on the Onsager-Machlup functional. This reformulation enables the use of a previously proposed parallel-in-time solution for optimal control problems, which we adapt to the current problem. The structure of the resulting optimal control problem admits a parallel solution based on parallel associative scan algorithms. In the linear Gaussian special case, it yields a parallel Kalman-Bucy filter and a parallel continuous-time Rauch-Tung-Striebel smoother. These linear computational methods are further extended to nonlinear continuous-time state-space models through Taylor expansions. We also present the corresponding parallel two-filter smoother. The graphics processing unit (GPU) experiments on linear and nonlinear models demonstrate that the proposed framework achieves a significant speedup in computations while maintaining the accuracy of sequential algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13319v1</guid>
      <category>cs.DC</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <category>eess.SY</category>
      <category>stat.CO</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hassan Razavi, \'Angel F. Garc\'ia-Fern\'andez, Simo S\"arkk\"a</dc:creator>
    </item>
    <item>
      <title>Fused $L_{1/2}$ prior for large scale linear inverse problem with Gibbs bouncy particle sampler</title>
      <link>https://arxiv.org/abs/2409.07874</link>
      <description>arXiv:2409.07874v2 Announce Type: replace 
Abstract: In this paper, we study Bayesian approach for solving large scale linear inverse problems arising in various scientific and engineering fields. We propose a fused $L_{1/2}$ prior with edge-preserving and sparsity-promoting properties and show that it can be formulated as a Gaussian mixture Markov random field. Since the density function of this family of prior is neither log-concave nor Lipschitz, gradient-based Markov chain Monte Carlo methods can not be applied to sample the posterior. Thus, we present a Gibbs sampler in which all the conditional posteriors involved have closed form expressions. The Gibbs sampler works well for small size problems but it is computationally intractable for large scale problems due to the need for sample high dimensional Gaussian distribution. To reduce the computation burden, we construct a Gibbs bouncy particle sampler (Gibbs-BPS) based on a piecewise deterministic Markov process. This new sampler combines elements of Gibbs sampler with bouncy particle sampler and its computation complexity is an order of magnitude smaller. We show that the new sampler converges to the target distribution. With computed tomography examples, we demonstrate that the proposed method shows competitive performance with existing popular Bayesian methods and is highly efficient in large scale problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07874v2</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiongwen Ke, Yanan Fan, Qingping Zhou</dc:creator>
    </item>
    <item>
      <title>An adaptive approximate Bayesian computation MCMC with Global-Local proposals</title>
      <link>https://arxiv.org/abs/2412.15644</link>
      <description>arXiv:2412.15644v2 Announce Type: replace 
Abstract: In this paper, we address the challenge of Markov Chain Monte Carlo (MCMC) algorithms within the approximate Bayesian Computation (ABC) framework, which often get trapped in local optima due to their inherent local exploration mechanism. We propose a novel Global-Local ABC-MCMC algorithm that combines the ``exploration" capabilities of global proposals with the ``exploitation" finesse of local proposals. We integrate iterated importance resampling into the likelihood-free framework to establish an effective global proposal distribution. For high-dimensional parameter spaces, we optimize the efficiency of the local sampler by leveraging Langevin dynamics and common random numbers. Furthermore, we introduce two adaptive schemes to enhance the algorithmic performance. The first scheme divides the update target of the importance proposal into a sequence of intermediate target distributions that progressively approximate the ABC posterior, thereby gradually updating the importance proposal distribution during the iterations. The second adaptive scheme automatically selects the optimal mixture of global and local moves through sequential optimization, based on a relative version of the expected squared jumping distance (ESJD). We theoretically and numerically demonstrate that our method is able to improve sampling efficiency and achieve more reliable convergence for complex posteriors. We develop a software package that is available at https://github.com/caofff/GL-ABC-MCMC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15644v2</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuefei Cao, Shijia Wang, Yongdao Zhou</dc:creator>
    </item>
    <item>
      <title>Multilevel Sampling in Algebraic Statistics</title>
      <link>https://arxiv.org/abs/2505.04062</link>
      <description>arXiv:2505.04062v2 Announce Type: replace 
Abstract: This paper proposes a multilevel sampling algorithm for fiber sampling problems in algebraic statistics, inspired by Henry Wynn's suggestion to adapt multilevel Monte Carlo (MLMC) ideas to discrete models. Focusing on log-linear models, we sample from high-dimensional lattice fibers defined by algebraic constraints. Building on Markov basis methods and results from Diaconis and Sturmfels, our algorithm uses variable step sizes to accelerate exploration and reduce the need for long burn-in. We introduce a novel Fiber Coverage Score (FCS) based on Voronoi partitioning to assess sample quality, and highlight the utility of the Maximum Mean Discrepancy (MMD) quality metric. Simulations on benchmark fibers show that multilevel sampling outperforms naive MCMC approaches. Our results demonstrate that multilevel methods, when properly applied, provide practical benefits for discrete sampling in algebraic statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04062v2</guid>
      <category>stat.CO</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nathan Kirk, Ivan Gvozdanovi\'c, Sonja Petrovi\'c</dc:creator>
    </item>
    <item>
      <title>On Nonparanormal Likelihoods</title>
      <link>https://arxiv.org/abs/2408.17346</link>
      <description>arXiv:2408.17346v2 Announce Type: replace-cross 
Abstract: Nonparanormal models describe the joint distribution of multivariate responses via latent Gaussian, and thus parametric, copulae while allowing flexible nonparametric marginals. Some aspects of such distributions, for example conditional independence, are formulated parametrically. Other features, such as marginal distributions, can be formulated non- or semiparametrically. Such models are attractive when multivariate normality is questionable.
  Most estimation procedures perform two steps, first estimating the nonparametric part. The copula parameters come second, treating the marginal estimates as known. This is sufficient for some applications. For other applications, e.g. when a semiparametric margin features parameters of interest or when standard errors are important, a simultaneous estimation of all parameters might be more advantageous.
  We present suitable parameterisations of nonparanormal models, possibly including semiparametric effects, and define four novel nonparanormal log-likelihood functions. In general, the corresponding one-step optimization problems are shown to be non-convex. In some cases, however, biconvex problems emerge. Several convex approximations are discussed.
  From a low-level computational point of view, the core contribution is the score function for multivariate normal log-probabilities computed via Genz' procedure. We present transformation discriminant analysis when some biomarkers are subject to limit-of-detection problems as an application and illustrate possible empirical gains in semiparametric efficient polychoric correlation analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17346v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Torsten Hothorn</dc:creator>
    </item>
    <item>
      <title>Fast Estimation of the Composite Link Model for Multidimensional Grouped Counts</title>
      <link>https://arxiv.org/abs/2412.04956</link>
      <description>arXiv:2412.04956v2 Announce Type: replace-cross 
Abstract: This paper presents a significant advancement in the estimation of the Composite Link Model within a penalized likelihood framework, specifically designed to address indirect observations of grouped count data. While the model is effective in these contexts, its application becomes computationally challenging in large, high-dimensional settings. To overcome this, we propose a reformulated iterative estimation procedure that leverages Generalized Linear Array Models, enabling the disaggregation and smooth estimation of latent distributions in multidimensional data. Through simulation studies and applications to high-dimensional mortality datasets, we demonstrate the model's capability to capture fine-grained patterns while comparing its computational performance to the conventional algorithm. The proposed methodology offers notable improvements in computational speed, storage efficiency, and practical applicability, making it suitable for a wide range of fields in which high-dimensional data are provided in grouped formats.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04956v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Carlo G. Camarda, Mar\'ia Durb\'an</dc:creator>
    </item>
    <item>
      <title>Compact Neural Network Algorithm for Electrocardiogram Classification</title>
      <link>https://arxiv.org/abs/2412.17852</link>
      <description>arXiv:2412.17852v2 Announce Type: replace-cross 
Abstract: In this paper, we present a powerful, compact electrocardiogram (ECG) classification algorithm for cardiac arrhythmia diagnosis that addresses the current reliance on deep learning and convolutional neural networks (CNNs) in ECG analysis. This work aims to reduce the demand for deep learning, which often requires extensive computational resources and large labeled datasets. Our approach introduces an artificial neural network (ANN) with a simple architecture combined with advanced feature engineering techniques. A key contribution of this work is the incorporation of 17 engineered features that enable the extraction of critical patterns from raw ECG signals. By integrating mathematical transformations, signal processing methods, and data extraction algorithms, our model captures the morphological and physiological characteristics of ECG signals with high efficiency, without requiring deep learning. Our method demonstrates a similar performance to other state-of-the-art models in classifying 4 types of arrhythmias, including atrial fibrillation, sinus tachycardia, sinus bradycardia, and ventricular flutter. Our algorithm achieved an accuracy of 97.36% on the MIT-BIH and St. Petersburg INCART arrhythmia databases. Our approach offers a practical and feasible solution for real-time diagnosis of cardiac disorders in medical applications, particularly in resource-limited environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17852v2</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mateo Frausto-Avila, Jos\'e Pablo Manriquez-Amavizca, Ana Karen Susana Rocha-Robledo, Mario A. Quiroz-Juarez, Alfred U'Ren</dc:creator>
    </item>
    <item>
      <title>Coherent Disaggregation and Uncertainty Quantification for Spatially Misaligned Data</title>
      <link>https://arxiv.org/abs/2502.10584</link>
      <description>arXiv:2502.10584v4 Announce Type: replace-cross 
Abstract: Spatial misalignment arises when datasets are aggregated or collected at different spatial scales, leading to information loss. We develop a Bayesian disaggregation framework that links misaligned data to a continuous-domain model through an iteratively linearised integration scheme implemented with the Integrated Nested Laplace Approximation (INLA). The framework accommodates different ways of handling observations depending on the application, resulting in four variants: (i) \textit{Raster at Full Resolution}, (ii) \textit{Raster Aggregation}, (iii) \textit{Polygon Aggregation} (PolyAgg), and (iv) \textit{Point Values} (PointVal). The first three represent increasing levels of spatial averaging, while the last two address situations with incomplete covariate information. For PolyAgg and PointVal, we reconstruct the covariate field using three strategies -- \textit{Value Plugin}, \textit{Joint Uncertainty}, and \textit{Uncertainty Plugin} -- with the latter two propagating uncertainty.
  We illustrate the framework with an example motivated by landslide modelling, focusing on methodology rather than interpreting landslide processes. Simulations show that uncertainty-propagating approaches outperform \textit{Value Plugin} method and remain robust under model misspecification. Point-pattern observations and full-resolution covariates are therefore preferable, and when covariate fields are incomplete, uncertainty-aware methods are most reliable. The framework is well suited to landslide susceptibility modelling and other spatial mapping tasks, and integrates seamlessly with INLA-based tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10584v4</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Man Ho Suen, Mark Naylor, Finn Lindgren</dc:creator>
    </item>
    <item>
      <title>CRPS-Based Targeted Sequential Design with Application in Chemical Space</title>
      <link>https://arxiv.org/abs/2503.11250</link>
      <description>arXiv:2503.11250v2 Announce Type: replace-cross 
Abstract: Sequential design of real and computer experiments via Gaussian Process (GP) models has proven useful for parsimonious, goal-oriented data acquisition purposes. In this work, we focus on acquisition strategies for a GP model that needs to be accurate within a predefined range of the response of interest. Such an approach is useful in various fields including synthetic chemistry, where finding molecules with particular properties is essential for developing useful materials and effective medications. GP modeling and sequential design of experiments have been successfully applied to a plethora of domains, including molecule research. Our main contribution here is to use the threshold-weighted Continuous Ranked Probability Score (CRPS) as a basic building block for acquisition functions employed within sequential design. We study pointwise and integral criteria relying on two different weighting measures and benchmark them against competitors, demonstrating improved performance with respect to considered goals. The resulting acquisition strategies are applicable to a wide range of fields and pave the way to further developing sequential design relying on scoring rules.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11250v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lea Friedli, Ath\'ena\"is Gautier, Anna Broccard, David Ginsbourger</dc:creator>
    </item>
  </channel>
</rss>
