<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NA</link>
    <description>cs.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 13 Sep 2024 04:01:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Transformed Physics-Informed Neural Networks for The Convection-Diffusion Equation</title>
      <link>https://arxiv.org/abs/2409.07671</link>
      <description>arXiv:2409.07671v1 Announce Type: new 
Abstract: Singularly perturbed problems are known to have solutions with steep boundary layers that are hard to resolve numerically. Traditional numerical methods, such as Finite Difference Methods (FDMs), require a refined mesh to obtain stable and accurate solutions. As Physics-Informed Neural Networks (PINNs) have been shown to successfully approximate solutions to differential equations from various fields, it is natural to examine their performance on singularly perturbed problems. The convection-diffusion equation is a representative example of such a class of problems, and we consider the use of PINNs to produce numerical solutions of this equation. We study two ways to use PINNS: as a method for correcting oscillatory discrete solutions obtained using FDMs, and as a method for modifying reduced solutions of unperturbed problems. For both methods, we also examine the use of input transformation to enhance accuracy, and we explain the behavior of input transformations analytically, with the help of neural tangent kernels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07671v1</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiajing Guan, Howard Elman</dc:creator>
    </item>
    <item>
      <title>Approximation of the Hilbert Transform on the unit circle</title>
      <link>https://arxiv.org/abs/2409.07810</link>
      <description>arXiv:2409.07810v1 Announce Type: new 
Abstract: The paper deals with the numerical approximation of the Hilbert transform on the unit circle using Szeg\"o and anti-Szeg\"o quadrature formulas. These schemes exhibit maximum precision with oppositely signed errors and allow for improved accuracy through their averaged results. Their computation involves a free parameter associated with the corresponding para-orthogonal polynomials. Here, it is suitably chosen to construct a Szeg\"o and anti-Szeg\"o formula whose nodes are strategically distanced from the singularity of the Hilbert kernel. Numerical experiments demonstrate the accuracy of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07810v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luisa Fermo, Valerio Loi</dc:creator>
    </item>
    <item>
      <title>Deep learning methods for stochastic Galerkin approximations of elliptic random PDEs</title>
      <link>https://arxiv.org/abs/2409.08063</link>
      <description>arXiv:2409.08063v1 Announce Type: new 
Abstract: This work considers stochastic Galerkin approximations of linear elliptic partial differential equations with stochastic forcing terms and stochastic diffusion coefficients, that cannot be bounded uniformly away from zero and infinity. A traditional numerical method for solving the resulting high-dimensional coupled system of partial differential equations (PDEs) is replaced by deep learning techniques. In order to achieve this, physics-informed neural networks (PINNs), which typically operate on the strong residual of the PDE and can therefore be applied in a wide range of settings, are considered. As a second approach, the Deep Ritz method, which is a neural network that minimizes the Ritz energy functional to find the weak solution, is employed. While the second approach only works in special cases, it overcomes the necessity of testing in variational problems while maintaining mathematical rigor and ensuring the existence of a unique solution. Furthermore, the residual is of a lower differentiation order, reducing the training cost considerably. The efficiency of the method is demonstrated on several model problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08063v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabio Musco, Andrea Barth</dc:creator>
    </item>
    <item>
      <title>Polynomial Methods for Ensuring Data Integrity in Financial Systems</title>
      <link>https://arxiv.org/abs/2409.07490</link>
      <description>arXiv:2409.07490v1 Announce Type: cross 
Abstract: Ensuring data integrity is a critical requirement in complex systems, especially in financial platforms where vast amounts of data must be consistently accurate and reliable. This paper presents a robust approach using polynomial interpolation methods to maintain data integrity across multiple indicators and dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07490v1</guid>
      <category>cs.CR</category>
      <category>cs.IT</category>
      <category>cs.NA</category>
      <category>math.IT</category>
      <category>math.NA</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ignacio Brasca</dc:creator>
    </item>
    <item>
      <title>Dual scale Residual-Network for turbulent flow sub grid scale resolving: A prior analysis</title>
      <link>https://arxiv.org/abs/2409.07605</link>
      <description>arXiv:2409.07605v1 Announce Type: cross 
Abstract: This paper introduces generative Residual Networks (ResNet) as a surrogate Machine Learning (ML) tool for Large Eddy Simulation (LES) Sub Grid Scale (SGS) resolving. The study investigates the impact of incorporating Dual Scale Residual Blocks (DS-RB) within the ResNet architecture. Two LES SGS resolving models are proposed and tested for prior analysis test cases: a super-resolution model (SR-ResNet) and a SGS stress tensor inference model (SGS-ResNet). The SR-ResNet model task is to upscale LES solutions from coarse to finer grids by inferring unresolved SGS velocity fluctuations, exhibiting success in preserving high-frequency velocity fluctuation information, and aligning with higher-resolution LES solutions' energy spectrum. Furthermore, employing DS-RB enhances prediction accuracy and precision of high-frequency velocity fields compared to Single Scale Residual Blocks (SS-RB), evident in both spatial and spectral domains. The SR-ResNet model is tested and trained on filtered/downsampled 2-D LES planar jet injection problems at two Reynolds numbers, two jet configurations, and two upscale ratios. In the case of SGS stress tensor inference, both SS-RB and DS-RB exhibit higher prediction accuracy over the Smagorinsky model with reference to the true DNS SGS stress tensor, with DS-RB-based SGS-ResNet showing stronger statistical alignment with DNS data. The SGS-ResNet model is tested on a filtered/downsampled 2-D DNS isotropic homogenous decay turbulence problem. The adoption of DS-RB incurs notable increases in network size, training time, and forward inference time, with the network size expanding by over tenfold, and training and forward inference times increasing by approximately 0.5 and 3 times, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07605v1</guid>
      <category>physics.flu-dyn</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Omar Sallam, Mirjam F\"urth</dc:creator>
    </item>
    <item>
      <title>Localized Schr\"odinger Bridge Sampler</title>
      <link>https://arxiv.org/abs/2409.07968</link>
      <description>arXiv:2409.07968v1 Announce Type: cross 
Abstract: We consider the generative problem of sampling from an unknown distribution for which only a sufficiently large number of training samples are available. In this paper, we build on previous work combining Schr\"odinger bridges and Langevin dynamics. A key bottleneck of this approach is the exponential dependence of the required training samples on the dimension, $d$, of the ambient state space. We propose a localization strategy which exploits conditional independence of conditional expectation values. Localization thus replaces a single high-dimensional Schr\"odinger bridge problem by $d$ low-dimensional Schr\"odinger bridge problems over the available training samples. As for the original approach, the localized sampler is stable and geometric ergodic. The sampler also naturally extends to conditional sampling and to Bayesian inference. We demonstrate the performance of our proposed scheme through experiments on a Gaussian problem with increasing dimensions and on a stochastic subgrid-scale parametrization conditional sampling problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07968v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.CO</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Georg A. Gottwald, Sebastian Reich</dc:creator>
    </item>
    <item>
      <title>Edge-Wise Graph-Instructed Neural Networks</title>
      <link>https://arxiv.org/abs/2409.08023</link>
      <description>arXiv:2409.08023v1 Announce Type: cross 
Abstract: The problem of multi-task regression over graph nodes has been recently approached through Graph-Instructed Neural Network (GINN), which is a promising architecture belonging to the subset of message-passing graph neural networks. In this work, we discuss the limitations of the Graph-Instructed (GI) layer, and we formalize a novel edge-wise GI (EWGI) layer. We discuss the advantages of the EWGI layer and we provide numerical evidence that EWGINNs perform better than GINNs over graph-structured input data with chaotic connectivity, like the ones inferred from the Erdos-R\'enyi graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08023v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francesco Della Santa, Antonio Mastropietro, Sandra Pieraccini, Francesco Vaccarino</dc:creator>
    </item>
    <item>
      <title>Learning incomplete factorization preconditioners for GMRES</title>
      <link>https://arxiv.org/abs/2409.08262</link>
      <description>arXiv:2409.08262v1 Announce Type: cross 
Abstract: In this paper, we develop a data-driven approach to generate incomplete LU factorizations of large-scale sparse matrices. The learned approximate factorization is utilized as a preconditioner for the corresponding linear equation system in the GMRES method. Incomplete factorization methods are one of the most commonly applied algebraic preconditioners for sparse linear equation systems and are able to speed up the convergence of Krylov subspace methods. However, they are sensitive to hyper-parameters and might suffer from numerical breakdown or lead to slow convergence when not properly applied. We replace the typically hand-engineered algorithms with a graph neural network based approach that is trained against data to predict an approximate factorization. This allows us to learn preconditioners tailored for a specific problem distribution. We analyze and empirically evaluate different loss functions to train the learned preconditioners and show their effectiveness to decrease the number of GMRES iterations and improve the spectral properties on our synthetic dataset. The code is available at https://github.com/paulhausner/neural-incomplete-factorization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08262v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul H\"ausner, Aleix Nieto Juscafresa, Jens Sj\"olund</dc:creator>
    </item>
    <item>
      <title>Multiscale mortar mixed finite element methods for the Biot system of poroelasticity</title>
      <link>https://arxiv.org/abs/2211.02949</link>
      <description>arXiv:2211.02949v2 Announce Type: replace 
Abstract: We develop a mixed finite element domain decomposition method on non-matching grids for the Biot system of poroelasticity. A displacement-pressure vector mortar function is introduced on the interfaces and utilized as a Lagrange multiplier to impose weakly continuity of normal stress and normal velocity. The mortar space can be on a coarse scale, resulting in a multiscale approximation. We establish existence, uniqueness, stability, and error estimates for the semidiscrete continuous-in-time formulation under a suitable condition on the richness of the mortar space. We further consider a fully-discrete method based on the backward Euler time discretization and show that the solution of the algebraic system at each time step can be reduced to solving a positive definite interface problem for the composite mortar variable. A multiscale stress-flux basis is constructed, which makes the number of subdomain solves independent of the number of iterations required for the interface problem, as well as the number of time steps. We present numerical experiments verifying the theoretical results and illustrating the multiscale capabilities of the method for a heterogeneous benchmark problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.02949v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manu Jayadharan, Ivan Yotov</dc:creator>
    </item>
    <item>
      <title>Fast randomized algorithms for computing the generalized tensor SVD based on the tubal product</title>
      <link>https://arxiv.org/abs/2305.05031</link>
      <description>arXiv:2305.05031v5 Announce Type: replace 
Abstract: This work deals with developing two fast randomized algorithms for computing the generalized tensor singular value decomposition (GTSVD) based on the tubal product (t-product). The random projection method is utilized to compute the important actions of the underlying data tensors and use them to get small sketches of the original data tensors, which are easier to be handled. Due to the small size of the sketch tensors, deterministic approaches are applied to them to compute their GTSVDs. Then, from the GTSVD of the small sketch tensors, the GTSVD of the original large-scale data tensors is recovered. Some experiments are conducted to show the effectiveness of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.05031v5</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Salman Ahmadi-Asl, Ugochukwu Ugwu</dc:creator>
    </item>
    <item>
      <title>Sharp inverse statements for kernel interpolation</title>
      <link>https://arxiv.org/abs/2306.14618</link>
      <description>arXiv:2306.14618v2 Announce Type: replace 
Abstract: While direct statements for kernel based interpolation on regions $\Omega \subset \mathbb{R}^d$ are well researched, far less is known about corresponding inverse statements. The available inverse statements for kernel based interpolation so far are not sharp. In this paper, we derive sharp inverse statements for interpolation using finitely smooth kernels, such as popular radial basis function (RBF) kernels like the class of Mat\'ern or Wendland kernels. In particular, the results show that there is a one-to-one correspondence between the smoothness of a function and its approximation rate via kernel interpolation: If a function can be approximated with a given rate, it has a corresponding smoothness and vice versa.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.14618v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tizian Wenzel</dc:creator>
    </item>
    <item>
      <title>An all-frequency stable integral system for Maxwell's equations in 3-D penetrable media: continuous and discrete model analysis</title>
      <link>https://arxiv.org/abs/2402.17713</link>
      <description>arXiv:2402.17713v2 Announce Type: replace 
Abstract: We introduce a new system of surface integral equations for Maxwell's transmission problem in three dimensions. This system has two remarkable features, both of which we prove. First, it is well-posed at all frequencies. Second, the underlying linear operator has a uniformly bounded inverse as the frequency approaches zero, ensuring that there is no low-frequency breakdown. The system is derived from a formulation we introduced in our previous work, which required additional integral constraints to ensure well -posedness across all frequencies. In this study, we eliminate those constraints and demonstrate that our new self adjoint, constraints-free linear system expressed in the desirable form of an identity plus a compact weakly-singular operator is stable for all frequencies. Furthermore, we propose and analyze a fully discrete numerical method for these systems and provide a proof of spectrally accurate convergence for the computational method. We also computationally demonstrate the high-order accuracy of the algorithm using benchmark scatterers with curved surfaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.17713v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mahadevan Ganesh, Stuart C. Hawkins, Darko Volkov</dc:creator>
    </item>
    <item>
      <title>Scientific machine learning for closure models in multiscale problems: a review</title>
      <link>https://arxiv.org/abs/2403.02913</link>
      <description>arXiv:2403.02913v2 Announce Type: replace 
Abstract: Closure problems are omnipresent when simulating multiscale systems, where some quantities and processes cannot be fully prescribed despite their effects on the simulation's accuracy. Recently, scientific machine learning approaches have been proposed as a way to tackle the closure problem, combining traditional (physics-based) modeling with data-driven (machine-learned) techniques, typically through enriching differential equations with neural networks. This paper reviews the different reduced model forms, distinguished by the degree to which they include known physics, and the different objectives of a priori and a posteriori learning. The importance of adhering to physical laws (such as symmetries and conservation laws) in choosing the reduced model form and choosing the learning method is discussed. The effect of spatial and temporal discretization and recent trends toward discretization-invariant models are reviewed. In addition, we make the connections between closure problems and several other research disciplines: inverse problems, Mori-Zwanzig theory, and multi-fidelity methods. In conclusion, much progress has been made with scientific machine learning approaches for solving closure problems, but many challenges remain. In particular, the generalizability and interpretability of learned models is a major issue that needs to be addressed further.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02913v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Benjamin Sanderse, Panos Stinis, Romit Maulik, Shady E. Ahmed</dc:creator>
    </item>
    <item>
      <title>Continuous iterative algorithms for anti-Cheeger cut</title>
      <link>https://arxiv.org/abs/2103.10705</link>
      <description>arXiv:2103.10705v2 Announce Type: replace-cross 
Abstract: As a judicious correspondence to the classical maxcut, the anti-Cheeger cut has more balanced structure, but few numerical results on it have been reported so far. In this paper, we propose a continuous iterative algorithm (CIA) for the anti-Cheeger cut problem through fully using an equivalent continuous formulation. It does not need rounding at all and has advantages that all subproblems have explicit analytic solutions, the objective function values are monotonically updated and the iteration points converge to a local optimum in finite steps via an appropriate subgradient selection. It can also be easily combined with the maxcut iterations for breaking out of local optima and improving the solution quality thanks to the similarity between the anti-Cheeger cut problem and the maxcut problem. The performance of CIAs is fully demonstrated through numerical experiments on G-set from two aspects: one is on the solution quality where we find that the approximate solutions obtained by CIAs are of comparable quality to those by the multiple search operator heuristic method; the other is on the computational cost where we show that CIAs always run faster than the often-used continuous iterative algorithm based on the rank-two relaxation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2103.10705v2</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.CO</category>
      <category>math.NA</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sihong Shao, Chuan Yang</dc:creator>
    </item>
    <item>
      <title>Generating synthetic data for neural operators</title>
      <link>https://arxiv.org/abs/2401.02398</link>
      <description>arXiv:2401.02398v2 Announce Type: replace-cross 
Abstract: Numerous developments in the recent literature show the promising potential of deep learning in obtaining numerical solutions to partial differential equations (PDEs) beyond the reach of current numerical solvers. However, data-driven neural operators all suffer from a similar problem: the data needed to train a network depends on classical numerical solvers such as finite difference or finite element, among others. In this paper, we propose a different approach to generating synthetic functional training data that does not require solving a PDE numerically. We draw a large number $N$ of independent and identically distributed 'random functions' $u_j$ from the underlying solution space (e.g., $H_0^1(\Omega)$) in which we know the solution lies according to classical theory. We then plug each such random candidate solution into the equation and get a corresponding right-hand side function $f_j$ for the equation, and consider $(f_j, u_j)_{j=1}^N$ as supervised training data for learning the underlying inverse problem $f \rightarrow u$. This `backwards' approach to generating training data only requires derivative computations, in contrast to standard `forward' approaches, which require a numerical PDE solver, enabling us to generate many data points quickly and efficiently. While the idea is simple, we hope this method will expand the potential for developing neural PDE solvers that do not depend on classical numerical solvers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.02398v2</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Erisa Hasani, Rachel A. Ward</dc:creator>
    </item>
    <item>
      <title>Quantum Realization of the Finite Element Method</title>
      <link>https://arxiv.org/abs/2403.19512</link>
      <description>arXiv:2403.19512v2 Announce Type: replace-cross 
Abstract: This paper presents a quantum algorithm for the solution of prototypical second-order linear elliptic partial differential equations discretized by $d$-linear finite elements on Cartesian grids of a bounded $d$-dimensional domain. An essential step in the construction is a BPX preconditioner, which transforms the linear system into a sufficiently well-conditioned one, making it amenable to quantum computation. We provide a constructive proof demonstrating that, for any fixed dimension, our quantum algorithm can compute suitable functionals of the solution to a given tolerance $\mathtt{tol}$ with an optimal complexity of order $\mathtt{tol}^{-1}$ up to logarithmic terms, significantly improving over existing approaches. Notably, this approach does not rely on regularity of the solution and achieves quantum advantage over classical solvers in two dimensions, whereas prior quantum methods required at least four dimensions for asymptotic benefits. We further detail the design and implementation of a quantum circuit capable of executing our algorithm, present simulator results, and report numerical experiments on current quantum hardware, confirming the feasibility of preconditioned finite element methods for near-term quantum computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19512v2</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthias Deiml, Daniel Peterseim</dc:creator>
    </item>
    <item>
      <title>The Pascal Matrix, Commuting Tridiagonal Operators and Fourier Algebras</title>
      <link>https://arxiv.org/abs/2407.21680</link>
      <description>arXiv:2407.21680v2 Announce Type: replace-cross 
Abstract: We consider the (symmetric) Pascal matrix, in its finite and infinite versions, and prove the existence of symmetric tridiagonal matrices commuting with it by giving explicit expressions for these commuting matrices. This is achieved by studying the associated Fourier algebra, which as a byproduct, allows us to show that all the linear relations of a certain general form for the entries of the Pascal matrix arise from only three basic relations. We also show that pairs of eigenvectors of the tridiagonal matrix define a natural eigenbasis for the binomial transform. Lastly, we show that the commuting tridiagonal matrices provide a numerically stable means of diagonalizing the Pascal matrix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21680v2</guid>
      <category>math.SP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>W. Riley Casper, Ignacio Zurrian</dc:creator>
    </item>
    <item>
      <title>A high-order procedure for computing globally optimal Wannier functions in one-dimensional crystalline insulators</title>
      <link>https://arxiv.org/abs/2409.04369</link>
      <description>arXiv:2409.04369v2 Announce Type: replace-cross 
Abstract: A standard task in solid state physics and quantum chemistry is the computation of localized molecular orbits known as Wannier functions. In this manuscript, we propose a new procedure for computing Wannier functions in one-dimensional crystalline materials. Our approach proceeds by first performing parallel transport of the Bloch functions using numerical integration. Then, using novel analysis, we show that a simple correction can be analytically computed that yields the optimally localized Wannier function. The resulting scheme is robust and capable of achieving high-order accuracy. We illustrate this in a number of numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04369v2</guid>
      <category>math-ph</category>
      <category>cs.NA</category>
      <category>math.MP</category>
      <category>math.NA</category>
      <category>physics.comp-ph</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abinand Gopal, Hanwen Zhang</dc:creator>
    </item>
  </channel>
</rss>
