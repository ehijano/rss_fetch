<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 09 Sep 2025 02:23:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Incentivising Personalised Colorectal Cancer Screening: an Adversarial Risk Analysis Approach</title>
      <link>https://arxiv.org/abs/2509.04592</link>
      <description>arXiv:2509.04592v1 Announce Type: new 
Abstract: This paper presents a framework for incentivising colorectal cancer (CRC) screening programs from the perspective of policymakers and under the assumption that the citizens participating in the program have misaligned objectives. To do so, it leverages tools from adversarial risk analysis to propose an optimal incentive scheme under uncertainty. The work relies on previous work on modeling CRC risk and optimal screening strategies and provides use cases regarding individual and group-based optimal incentives based on a simple financial scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04592v1</guid>
      <category>stat.AP</category>
      <category>cs.GT</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/CBMS65348.2025.00068</arxiv:DOI>
      <arxiv:journal_reference>D. Corrales and D. R. Insua, "Incentivising Personalised Colorectal Cancer Screening: an Adversarial Risk Analysis Approach," 2025 IEEE 38th International Symposium on Computer-Based Medical Systems (CBMS), Madrid, Spain, 2025, pp. 307-310</arxiv:journal_reference>
      <dc:creator>Daniel Corrales, David Rios Insua</dc:creator>
    </item>
    <item>
      <title>An Interactive Tool for Analyzing High-Dimensional Clusterings</title>
      <link>https://arxiv.org/abs/2509.04603</link>
      <description>arXiv:2509.04603v1 Announce Type: new 
Abstract: Technological advances have spurred an increase in data complexity and dimensionality. We are now in an era in which data sets containing thousands of features are commonplace. To digest and analyze such high-dimensional data, dimension reduction techniques have been developed and advanced along with computational power. Of these techniques, nonlinear methods are most commonly employed because of their ability to construct visually interpretable embeddings. Unlike linear methods, these methods non-uniformly stretch and shrink space to create a visual impression of the high-dimensional data. Since capturing high-dimensional structures in a significantly lower number of dimensions requires drastic manipulation of space, nonlinear dimension reduction methods are known to occasionally produce false structures, especially in noisy settings. In an effort to deal with this phenomenon, we developed an interactive tool that enables analysts to better understand and diagnose their dimension reduction results. It uses various analytical plots to provide a multi-faceted perspective on results to determine legitimacy. The tool is available via an R package named DRtool.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04603v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Justin Lin, Julia Fukuyama</dc:creator>
    </item>
    <item>
      <title>Precision Mental Health: Predicting Heterogeneous Treatment Effects for Depression through Data Integration</title>
      <link>https://arxiv.org/abs/2509.04604</link>
      <description>arXiv:2509.04604v1 Announce Type: new 
Abstract: When treating depression, clinicians are interested in determining the optimal treatment for a given patient, which is challenging given the amount of treatments available. To advance individualized treatment allocation, integrating data across multiple randomized controlled trials (RCTs) can enhance our understanding of treatment effect heterogeneity by increasing available information. However, extending these inferences to individuals outside of the original RCTs remains crucial for clinical decision-making. We introduce a two-stage meta-analytic method that predicts conditional average treatment effects (CATEs) in target patient populations by leveraging the distribution of CATEs across RCTs. Our approach generates 95\% prediction intervals for CATEs in target settings using first-stage models that can incorporate parametric regression or non-parametric methods such as causal forests or Bayesian additive regression trees (BART). We validate our method through simulation studies and operationalize it to integrate multiple RCTs comparing depression treatments, duloxetine and vortioxetine, to generate prediction intervals for target patient profiles. Our analysis reveals no strong evidence of effect heterogeneity across trials, with the exception of potential age-related variability. Importantly, we show that CATE prediction intervals capture broader uncertainty than study-specific confidence intervals when warranted, reflecting both within-study and between-study variability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04604v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carly L. Brantner, Trang Quynh Nguyen, Harsh Parikh, Congwen Zhao, Hwanhee Hong, Elizabeth A. Stuart</dc:creator>
    </item>
    <item>
      <title>Inferring Piece Value in Chess and Chess Variants</title>
      <link>https://arxiv.org/abs/2509.04691</link>
      <description>arXiv:2509.04691v1 Announce Type: new 
Abstract: We use logistic regression to estimate the value of the pieces in standard chess and several chess variants, namely Chess 960, Atomic chess, Antichess, and Horde chess. We perform our regressions on several years of data from Lichess, the free and open-source internet chess server. We use the published player ratings to control for the confounding effect of differential player skill. We adjust for the attenuation bias in regressions due to the noise in observed ratings. We find that major piece values, relative to the value of a pawn, are fairly consistent with historical valuation systems. However we find slightly higher value to bishops than knights. We find that piece values are smaller, in absolute value, in Atomic and Antichess than standard chess. We also present approximate values of the pieces to equalize odds when players of varying skill face off.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04691v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Steven Pav</dc:creator>
    </item>
    <item>
      <title>Precision Dose-Finding Design for Phase I Oncology Trials by Integrating Pharmacology Data</title>
      <link>https://arxiv.org/abs/2509.05120</link>
      <description>arXiv:2509.05120v1 Announce Type: new 
Abstract: Phase I oncology trials aim to identify a safe yet effective dose - often the maximum tolerated dose (MTD) - for subsequent studies. Conventional designs focus on population-level toxicity modeling, with recent attention on leveraging pharmacokinetic (PK) data to improve dose selection. We propose the Precision Dose-Finding (PDF) design, a novel Bayesian phase I framework that integrates individual patient PK profiles into the dose-finding process. By incorporating patient-specific PK parameters (such as volume of distribution and elimination rate), PDF models toxicity risk at the individual level, in contrast to traditional methods that ignore inter-patient variability. The trial is structured in two stages: an initial training stage to update model parameters using cohort-based dose escalation, and a subsequent test stage in which doses for new patients are chosen based on each patient's own PK-predicted toxicity probability. This two-stage approach enables truly personalized dose assignment while maintaining rigorous safety oversight. Extensive simulation studies demonstrate the feasibility of PDF and suggest that it provides improved safety and dosing precision relative to the continual reassessment method (CRM). The PDF design thus offers a refined dose-finding strategy that tailors the MTD to individual patients, aligning phase I trials with the ideals of precision medicine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05120v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kyong Ju Lee, Yuan Ji</dc:creator>
    </item>
    <item>
      <title>A Latent Class Bayesian Model for Multivariate Longitudinal Outcomes with Excess Zeros</title>
      <link>https://arxiv.org/abs/2509.04804</link>
      <description>arXiv:2509.04804v1 Announce Type: cross 
Abstract: Latent class models have been successfully used to handle complex datasets in different disciplines. For longitudinal outcomes, we often get a trajectory of the outcome for each individual, and on that basis, we cluster them for a powerful statistical inference. Latent class models have been used to handle multivariate longitudinal outcomes coming from biology, health sciences, and economics. In this paper, we propose a Bayesian latent class model for multivariate outcomes with excess zeros. We consider a Tobit model for zero-inflated continuous outcomes such as out-of-pocket medical expenses (OOPME), a two-part model for financial debt, and a ZIP model for counting outcomes with excess zeros. We develop a Bayesian mixture model and employ an adaptive Lasso-type shrinkage method for variable selection. We analyze data from the Health and Retirement Study conducted by the University of Michigan and consider modeling four important outcomes measuring the physical and financial health of the aged individuals. Our analysis detects several latent clusters for different outcomes. Practical usefulness of the proposed model is validated through a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04804v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chitradipa Chakraborty, Kiranmoy Das</dc:creator>
    </item>
    <item>
      <title>An information metric for comparing and assessing informative interim decisions in sequential clinical trials</title>
      <link>https://arxiv.org/abs/2509.04904</link>
      <description>arXiv:2509.04904v2 Announce Type: cross 
Abstract: Group sequential designs enable interim analyses and potential early stopping for efficacy or futility. While these adaptations improve trial efficiency and ethical considerations, they also introduce bias into the adapted analyses. We demonstrate how failing to account for informative interim decisions in the analysis can substantially affect posterior estimates of the treatment effect, often resulting in overly optimistic credible intervals aligned with the stopping decision. Drawing on information theory, we use the Kullback-Leibler divergence to quantify this distortion and highlight its use for post-hoc evaluation of informative interim decisions, with a focus on end-of-study inference. Unlike pointwise comparisons, this measure provides an integrated summary of this distortion on the whole parameter space. By comparing alternative decision boundaries and prior specifications, we illustrate how this measure can improve the understanding of trial results and inform the planning of future adaptive studies. We also introduce an expected version of this metric to support clinicians in choosing decision boundaries. This guidance complements traditional strategies based on type-I error rate control by offering insights into the distortion introduced to the treatment effect at each interim phase. The use of this pre-experimental measure is finally illustrated in a group sequential trial for evaluating a treatment for central nervous system disorders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04904v2</guid>
      <category>stat.ME</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>G. Caruso, W. F. Rosenberger, P. Mozgunov, N. Flournoy</dc:creator>
    </item>
    <item>
      <title>Dynamics of Liquidity Surfaces in Uniswap v3</title>
      <link>https://arxiv.org/abs/2509.05013</link>
      <description>arXiv:2509.05013v1 Announce Type: cross 
Abstract: This paper presents a comprehensive study on the empirical dynamics of Uniswap v3 liquidity, which we model as a time-tick surface, $L_t(x)$. Using a combination of functional principal component analysis (FPCA) and dynamic factor methods, we analyze three distinct pools over multiple sample periods. Our findings offer three main contributions: a statistical characterization of automated market maker liquidity, an interpretable and portable basis for dimension reduction, and a robust analysis of liquidity dynamics using rolling window metrics. For the 5 bps pools, the leading empirical eigenfunctions explain the majority of cross-tick variation and remain stable, aligning closely with a low-order Legendre polynomial basis. This alignment provides a parsimonious and interpretable structure, similar to the dynamic Nelson-Siegel method for yield curves. The factor coefficients exhibit a time series structure well-captured by AR(1) models with clear GARCH-type heteroskedasticity and heavy-tailed innovations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05013v1</guid>
      <category>q-fin.TR</category>
      <category>stat.AP</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jimmy Risk, Shen-Ning Tung, Tai-Ho Wang</dc:creator>
    </item>
    <item>
      <title>Beyond Linearity and Time-homogeneity: Relational Hyper Event Models with Time-Varying Non-Linear Effects</title>
      <link>https://arxiv.org/abs/2509.05289</link>
      <description>arXiv:2509.05289v2 Announce Type: cross 
Abstract: Recent technological advances have made it easier to collect large and complex networks of time-stamped relational events connecting two or more entities. Relational hyper-event models (RHEMs) aim to explain the dynamics of these events by modeling the event rate as a function of statistics based on past history and external information.
  However, despite the complexity of the data, most current RHEM approaches still rely on a linearity assumption to model this relationship. In this work, we address this limitation by introducing a more flexible model that allows the effects of statistics to vary non-linearly and over time. While time-varying and non-linear effects have been used in relational event modeling, we take this further by modeling joint time-varying and non-linear effects using tensor product smooths.
  We validate our methodology on both synthetic and empirical data. In particular, we use RHEMs to study how patterns of scientific collaboration and impact evolve over time. Our approach provides deeper insights into the dynamic factors driving relational hyper-events, allowing us to evaluate potential non-monotonic patterns that cannot be identified using linear models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05289v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martina Boschi, J\"urgen Lerner, Ernst C. Wit</dc:creator>
    </item>
    <item>
      <title>Modelling phylogeny in 16S rRNA gene sequencing datasets using string-based kernels</title>
      <link>https://arxiv.org/abs/2210.07696</link>
      <description>arXiv:2210.07696v3 Announce Type: replace 
Abstract: The bacterial microbiome is increasingly being recognised as a key factor in human health, driven in large part by datasets collected using 16S rRNA (ribosomal ribonucleic acid) gene sequencing, which enable cost-effective quantification of the composition of an individual's bacterial community. One of the defining characteristics of 16S rRNA datasets is the evolutionary relationships that exist between taxa (phylogeny). Here, we demonstrate the utility of modelling these phylogenetic relationships in two statistical tasks (the two sample test and host trait prediction) and propose a novel family of kernels for analysing microbiome datasets by leveraging string kernels from the natural language processing literature. We show via simulation studies that a kernel two-sample test using the proposed kernel is sensitive to the phylogenetic scale of the difference between the two populations. In a second set of simulations we also show how Gaussian process modelling with string kernels can infer the distribution of bacterial-host effects across the phylogenetic tree \new{and apply this approach to a real host-trait prediction task.} The results in the paper can be reproduced by running the code at https://github.com/jonathanishhorowicz/modelling_phylogeny_in_16srrna_using_string_kernels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.07696v3</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan Ish-Horowicz, Sarah Filippi</dc:creator>
    </item>
    <item>
      <title>Analyzing Classroom Interaction Data Using Prompt Engineering and Network Analysis</title>
      <link>https://arxiv.org/abs/2501.18912</link>
      <description>arXiv:2501.18912v2 Announce Type: replace 
Abstract: Classroom interactions play a vital role in developing critical thinking, collaborative problem-solving abilities, and enhanced learning outcomes. While analyzing these interactions is crucial for improving educational practices, the examination of classroom dialogues presents significant challenges due to the complexity and high-dimensionality of conversational data. This study presents an integrated framework that combines prompt engineering with network analysis to investigate classroom interactions comprehensively. Our approach automates utterance classification through prompt engineering, enabling efficient and scalable dialogue analysis without requiring pre-labeled datasets. The classified interactions are subsequently transformed into network representations, facilitating the analysis of classroom dynamics as structured social networks. To uncover complex interaction patterns and how underlying interaction structures relate to student learning, we utilize network mediation analysis. In this approach, latent interaction structures, derived from the additive and multiplicative effects network (AMEN) model that places students within a latent social space, act as mediators. In particular, we investigate how the gender gap in mathematics performance may be mediated by students' classroom interaction structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18912v2</guid>
      <category>stat.AP</category>
      <category>cs.SI</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gwanghee Kim, Ick Hoon Jin, Minjeong Jeon</dc:creator>
    </item>
    <item>
      <title>Survival Analysis with Adversarial Regularization</title>
      <link>https://arxiv.org/abs/2312.16019</link>
      <description>arXiv:2312.16019v5 Announce Type: replace-cross 
Abstract: Survival Analysis (SA) models the time until an event occurs, with applications in fields like medicine, defense, finance, and aerospace. Recent research indicates that Neural Networks (NNs) can effectively capture complex data patterns in SA, whereas simple generalized linear models often fall short in this regard. However, dataset uncertainties (e.g., noisy measurements, human error) can degrade NN model performance. To address this, we leverage advances in NN verification to develop training objectives for robust, fully-parametric SA models. Specifically, we propose an adversarially robust loss function based on a Min-Max optimization problem. We employ CROWN-Interval Bound Propagation (CROWN-IBP) to tackle the computational challenges inherent in solving this Min-Max problem. Evaluated over 10 SurvSet datasets, our method, Survival Analysis with Adversarial Regularization (SAWAR), consistently outperforms baseline adversarial training methods and state-of-the-art (SOTA) deep SA models across various covariate perturbations with respect to Negative Log Likelihood (NegLL), Integrated Brier Score (IBS), and Concordance Index (CI) metrics. Thus, we demonstrate that adversarial robustness enhances SA predictive performance and calibration, mitigating data uncertainty and improving generalization across diverse datasets by up to 150% compared to baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.16019v5</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Potter, Stefano Maxenti, Michael Everett</dc:creator>
    </item>
    <item>
      <title>Assumption-Lean Post-Integrated Inference with Surrogate Control Outcomes</title>
      <link>https://arxiv.org/abs/2410.04996</link>
      <description>arXiv:2410.04996v3 Announce Type: replace-cross 
Abstract: Data integration methods aim to extract low-dimensional embeddings from high-dimensional outcomes to remove unwanted variations, such as batch effects and unmeasured covariates, across heterogeneous datasets. However, multiple hypothesis testing after integration can be biased due to data-dependent processes. We introduce a robust post-integrated inference (PII) method that adjusts for latent heterogeneity using control outcomes. Leveraging causal interpretations, we derive nonparametric identifiability of the direct effects using negative control outcomes. By utilizing surrogate control outcomes as an extension of negative control outcomes, we develop semiparametric inference on projected direct effect estimands, accounting for hidden mediators, confounders, and moderators. These estimands remain statistically meaningful under model misspecifications and with error-prone embeddings. We provide bias quantifications and finite-sample linear expansions with uniform concentration bounds. The proposed doubly robust estimators are consistent and efficient under minimal assumptions and potential misspecification, facilitating data-adaptive estimation with machine learning algorithms. Our proposal is evaluated with random forests through simulations and analysis of single-cell CRISPR perturbed datasets with potential unmeasured confounders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04996v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>q-bio.GN</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin-Hong Du, Kathryn Roeder, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Nonparametric estimation of the Patient Weighted While-Alive Estimand</title>
      <link>https://arxiv.org/abs/2412.03246</link>
      <description>arXiv:2412.03246v2 Announce Type: replace-cross 
Abstract: In clinical trials with recurrent events, such as repeated hospitalizations terminating with death, it is important to consider the patient events overall history for a thorough assessment of treatment effects. The occurrence of fewer events due to early deaths can lead to misinterpretation, emphasizing the importance of a while-alive strategy as suggested in Schmidli et al. (2023). In this study, we focus on the patient weighted while-alive estimand, represented as the expected number of events divided by the time alive within a target window, and develop efficient estimation for this estimand. Specifically, we derive the corresponding efficient influence function and develop a one-step estimator initially applied to the simpler irreversible illness-death model. For the broader context of recurrent events, due to the increased complexity, this one-step estimator is practically intractable due to likely misspecification of the needed conditional transition intensities that depend on a patient's unique history. Therefore, we suggest an alternative estimator that is expected to have high efficiency, focusing on the randomized treatment setting. Additionally, we apply our proposed estimator to two real-world case studies, demonstrating the practical applicability of this second estimator and benefits of this while-alive approach over currently available alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03246v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Alessandra Ragni, Torben Martinussen, Thomas Scheike</dc:creator>
    </item>
    <item>
      <title>Adaptive Designs in Fast-Track Registration Processes</title>
      <link>https://arxiv.org/abs/2507.04092</link>
      <description>arXiv:2507.04092v2 Announce Type: replace-cross 
Abstract: Fast-track procedures play an important role in the context of conditional registration of health products, such as conditional approval processes and listing processes for digital health applications. Fast-track procedures offer the potential for earlier patient access to innovative products. They involve two registration steps. The applicants can apply first for conditional registration. A successful conditional registration provides a limited funding or approval period and time to prepare the application for permanent registration, which is the second step of the registration process. For conditional registration, typically, products have to fulfil only a part of the requirements necessary for permanent registration. There is interest in valid and efficient study designs for fast-track procedures. This will be addressed in this paper. A motivating example of the paper is the German fast-track registration process of digital health applications (DiGA) for reimbursement by statutory health insurances. The main focus of the paper is the systematic investigation of the utility of adaptive designs in the context of fast-track registration processes. The paper also covers a careful discussion of the different requirements found in the guidelines and their consequences. We demonstrate that the use of adaptive designs in the context of fast-track processes like the DiGA registration process is, in most cases, much more efficient than the current standard of two separate studies. The results presented in this paper are based on numerical calculations supported by mathematical arguments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04092v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 08 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Liane Kluge, Werner Brannath</dc:creator>
    </item>
  </channel>
</rss>
