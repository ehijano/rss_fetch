<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Apr 2025 04:02:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Exploring internal representation of self-supervised networks: few-shot learning abilities and comparison with human semantics and recognition of objects</title>
      <link>https://arxiv.org/abs/2504.20364</link>
      <description>arXiv:2504.20364v1 Announce Type: new 
Abstract: Recent advances in self-supervised learning have attracted significant attention from both machine learning and neuroscience. This is primarily because self-supervised methods do not require annotated supervisory information, making them applicable to training artificial networks without relying on large amounts of curated data, and potentially offering insights into how the brain adapts to its environment in an unsupervised manner. Although several previous studies have elucidated the correspondence between neural representations in deep convolutional neural networks (DCNNs) and biological systems, the extent to which unsupervised or self-supervised learning can explain the human-like acquisition of categorically structured information remains less explored. In this study, we investigate the correspondence between the internal representations of DCNNs trained using a self-supervised contrastive learning algorithm and human semantics and recognition. To this end, we employ a few-shot learning evaluation procedure, which measures the ability of DCNNs to recognize novel concepts from limited exposure, to examine the inter-categorical structure of the learned representations. Two comparative approaches are used to relate the few-shot learning outcomes to human semantics and recognition, with results suggesting that the representations acquired through contrastive learning are well aligned with human cognition. These findings underscore the potential of self-supervised contrastive learning frameworks to model learning mechanisms similar to those of the human brain, particularly in scenarios where explicit supervision is unavailable, such as in human infants prior to language acquisition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20364v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Asaki Kataoka, Yoshihiro Nagano, Masafumi Oizumi</dc:creator>
    </item>
    <item>
      <title>Small brains but big challenges: white matter tractography in early life samples</title>
      <link>https://arxiv.org/abs/2504.20554</link>
      <description>arXiv:2504.20554v1 Announce Type: new 
Abstract: In the human brain, white matter development is a complex and long-lasting process involving intermingling micro-and macrostructural mechanisms, such as fiber growth, pruning and myelination. Did you know that all these neurodevelopmental changes strongly affect MRI signals, with consequences on tractography performances and reliability? This communication aims to elaborate on these aspects, highlighting the importance of tracking and studying the developing connections with dedicated approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20554v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s00429-025-02922-8</arxiv:DOI>
      <arxiv:journal_reference>Brain Structure and Function, 2025, Neuroanatomy and Tractography, 230 (4), pp.58</arxiv:journal_reference>
      <dc:creator>Jessica Dubois (CEA, INSERM), Mareike Grotheer, Joseph Yuan-Mou Yang, Jacques-Donald Tournier, Christian Beaulieu, Catherine Lebel</dc:creator>
    </item>
    <item>
      <title>DB-GNN: Dual-Branch Graph Neural Network with Multi-Level Contrastive Learning for Jointly Identifying Within- and Cross-Frequency Coupled Brain Networks</title>
      <link>https://arxiv.org/abs/2504.20744</link>
      <description>arXiv:2504.20744v1 Announce Type: new 
Abstract: Within-frequency coupling (WFC) and cross-frequency coupling (CFC) in brain networks reflect neural synchronization within the same frequency band and cross-band oscillatory interactions, respectively. Their synergy provides a comprehensive understanding of neural mechanisms underlying cognitive states such as emotion. However, existing multi-channel EEG studies often analyze WFC or CFC separately, failing to fully leverage their complementary properties. This study proposes a dual-branch graph neural network (DB-GNN) to jointly identify within- and cross-frequency coupled brain networks. Firstly, DBGNN leverages its unique dual-branch learning architecture to efficiently mine global collaborative information and local cross-frequency and within-frequency coupling information. Secondly, to more fully perceive the global information of cross-frequency and within-frequency coupling, the global perception branch of DB-GNN adopts a Transformer architecture. To prevent overfitting of the Transformer architecture, this study integrates prior within- and cross-frequency coupling information into the Transformer inference process, thereby enhancing the generalization capability of DB-GNN. Finally, a multi-scale graph contrastive learning regularization term is introduced to constrain the global and local perception branches of DB-GNN at both graph-level and node-level, enhancing its joint perception ability and further improving its generalization performance. Experimental validation on the emotion recognition dataset shows that DB-GNN achieves a testing accuracy of 97.88% and an F1- score of 97.87%, reaching the state-of-the-art performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20744v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiang Wang, Hui Xu, Jing Cai, Ta Zhou, Xibei Yang, Wei Xue</dc:creator>
    </item>
    <item>
      <title>Bootstrap Prediction and Confidence Bands for Frequency Response Functions in Posturography</title>
      <link>https://arxiv.org/abs/2504.20588</link>
      <description>arXiv:2504.20588v1 Announce Type: cross 
Abstract: The frequency response function (FRF) is an established way to describe the outcome of experiments in posture control literature. The FRF is an empirical transfer function between an input stimulus and the induced body segment sway profile, represented as a vector of complex values associated with a vector of frequencies. For this reason, testing the components of the FRF independently with Bonferroni correction can result in a too-conservative approach. Performing statistics on scalar values defined on the FRF, e.g., comparing the averages, implies an arbitrary decision by the experimenter. This work proposes bootstrap prediction and confidence bands as general methods to evaluate the outcome of posture control experiments, overcoming the foretold limitations of previously used approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20588v1</guid>
      <category>stat.AP</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s40799-025-00808-2</arxiv:DOI>
      <arxiv:journal_reference>Lippi V. Bootstrap Prediction and Confidence Bands for Frequency Response Functions in Posturography (2025) Experimental Techniques. 10.1007/s40799-025-00808-2</arxiv:journal_reference>
      <dc:creator>Vittorio Lippi</dc:creator>
    </item>
    <item>
      <title>Observing hidden neuronal states in experiments</title>
      <link>https://arxiv.org/abs/2308.15477</link>
      <description>arXiv:2308.15477v2 Announce Type: replace 
Abstract: In this article, we revisit voltage-clamp electrophysiology, a feedback-control method that has successfully been applied to study neuronal (non)linear current-voltage relationships. Yet its full scope has not been explored to date. We reinterpret the voltage-clamp protocol under slow variation of the feedback reference signal using the language of multiple-timescale dynamical systems theory. Gauged under this new epistemic step, we unveil non-observable (hidden) states. More specifically, the experimental measurements from voltage clamp can be put into correspondence with predictions of both stable and unstable states from dynamical systems theory. We demonstrate a general protocol for constructing systematically experimental steady-state bifurcation diagrams for electrophysiologically active cells. We perform our experiments on entorhinal cortex neurons, both excitatory (pyramidal neurons) and inhibitiory (interneurons). Moreover, employing a current-clamp open-loop electrophysiology protocol with slow variation of an applied electrical current drives the neuron to dynamically transition between its observable rest states and its observable spiking states, hence revealing experimental dynamic bifurcations. Overlaying the two aforementioned protocols produces a map of all these neuronal states and effectively leads to an experimental validation of the classical slow-fast dissection method introduced by J. Rinzel in the 1980s and routinely applied ever since in order to analyse slow-fast neuronal models. Our approach opens doors to observing further complex hidden states with more advanced control strategies, allowing to control real cells beyond pharmacological manipulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.15477v2</guid>
      <category>q-bio.NC</category>
      <category>math.DS</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dmitry Amakhin, Anton Chizhov, Guillaume Girier, Mathieu Desroches, Jan Sieber, Serafim Rodrigues</dc:creator>
    </item>
    <item>
      <title>Implementing feature binding through dendritic networks of a single neuron</title>
      <link>https://arxiv.org/abs/2405.12645</link>
      <description>arXiv:2405.12645v2 Announce Type: replace 
Abstract: A single neuron receives an extensive array of synaptic inputs through its dendrites, raising the fundamental question of how these inputs undergo integration and summation, culminating in the initiation of spikes in the soma. Experimental and computational investigations have revealed various modes of integration operations that include linear, superlinear, and sublinear summation. Interestingly, distinct neuron types exhibit diverse patterns of dendritic integration contingent upon the spatial distribution of dendrites. The functional implications of these specific integration modalities remain largely unexplored. In this study, we employ the Purkinje cell as a model system to investigate these intricate questions. Our findings reveal that Purkinje cells (PCs) generally exhibit sublinear summation across their expansive dendrites. The degree of sublinearity is dynamically modulated by both spatial and temporal input. Strong sublinearity necessitates that the synaptic distribution in PCs be globally scattered sensitive, whereas weak sublinearity facilitates the generation of complex firing patterns in PCs. Leveraging dendritic branches characterized by strong sublinearity as computational units, we demonstrate that a neuron can adeptly address the feature-binding problem. Collectively, these results offer a systematic perspective on the functional role of dendritic sublinearity, providing inspiration for a broader understanding of dendritic integration across various neuronal types.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12645v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuanhong Tang, Shanshan Jia, Tiejun Huang, Zhaofei Yu, Jian K. Liu</dc:creator>
    </item>
    <item>
      <title>Artificial Neural Networks for Magnetoencephalography: A review of an emerging field</title>
      <link>https://arxiv.org/abs/2501.11566</link>
      <description>arXiv:2501.11566v3 Announce Type: replace 
Abstract: Magnetoencephalography (MEG) is a cutting-edge neuroimaging technique that measures the intricate brain dynamics underlying cognitive processes with an unparalleled combination of high temporal and spatial precision. MEG data analytics has always relied on advanced signal processing and mathematical and statistical tools for various tasks ranging from data cleaning to probing the signals' rich dynamics and estimating the neural sources underlying the surface-level recordings. Like in most domains, the surge in Artificial Intelligence (AI) has led to the increased use of Machine Learning (ML) methods for MEG data classification. More recently, an emerging trend in this field is using Artificial Neural Networks (ANNs) to address many MEG-related tasks. This review provides a comprehensive overview of how ANNs are being used with MEG data from three vantage points: First, we review work that employs ANNs for MEG signal classification, i.e., for brain decoding. Second, we report on work that has used ANNs as putative models of information processing in the human brain. Finally, we examine studies that use ANNs as techniques to tackle methodological questions in MEG, including artifact correction and source estimation. Furthermore, we assess the current strengths and limitations of using ANNs with MEG and discuss future challenges and opportunities in this field. Finally, by establishing a detailed portrait of the field and providing practical recommendations for the future, this review seeks to provide a helpful reference for both seasoned MEG researchers and newcomers to the field who are interested in using ANNs to enhance the exploration of the complex dynamics of the human brain with MEG.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11566v3</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Arthur Dehgan, Hamza Abdelhedi, Vanessa Hadid, Irina Rish, Karim Jerbi</dc:creator>
    </item>
  </channel>
</rss>
