<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 24 Jul 2024 01:40:27 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 23 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Attention-based SIC Ordering and Power Allocation for Non-orthogonal Multiple Access Networks</title>
      <link>https://arxiv.org/abs/2407.14740</link>
      <description>arXiv:2407.14740v1 Announce Type: new 
Abstract: Non-orthogonal multiple access (NOMA) emerges as a superior technology for enhancing spectral efficiency compared to orthogonal multiple access. In NOMA networks, successive interference cancellation (SIC) plays a crucial role in decoding user signals sequentially. The challenge lies in the joint optimization of SIC ordering and power allocation, due to the factorial nature of ordering combinations. This study introduces an innovative solution, the Attention-based SIC Ordering and Power Allocation (ASOPA) framework, targeting an uplink NOMA network with dynamic SIC ordering. ASOPA aims to maximize weighted proportional fairness by employing deep reinforcement learning, strategically decomposing the problem into two manageable subproblems: SIC ordering optimization and optimal power allocation. Our approach utilizes an attention-based neural network, which processes instantaneous channel gains and user weights to determine the SIC decoding sequence for each user. Once the SIC ordering is established, the power allocation subproblem transforms into a convex optimization problem, enabling efficient calculation. Extensive simulations validate ASOPA's efficacy, demonstrating a performance closely paralleling the exhaustive method, with over 97% confidence in normalized network utility. Notably, ASOPA maintains a low execution latency of approximately 50 milliseconds in a ten-user NOMA network, aligning with static SIC ordering algorithms. Furthermore, ASOPA demonstrates superior performance in various NOMA network configurations, including scenarios with imperfect channel state information, multiple base stations, and multiple-antenna setups. Such results underscore ASOPA's robustness and effectiveness, highlighting its ability to excel across various NOMA network environments. The complete source code for ASOPA is accessible at https://github.com/Jil-Menzerna/ASOPA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14740v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liang Huang, Bincheng Zhu, Runkai Nan, Kaikai Chi, Yuan Wu</dc:creator>
    </item>
    <item>
      <title>Efficient Intrusion Detection: Combining $\chi^2$ Feature Selection with CNN-BiLSTM on the UNSW-NB15 Dataset</title>
      <link>https://arxiv.org/abs/2407.14945</link>
      <description>arXiv:2407.14945v1 Announce Type: new 
Abstract: Intrusion Detection Systems (IDSs) have played a significant role in the detection and prevention of cyber-attacks in traditional computing systems. It is not surprising that this technology is now being applied to secure Internet of Things (IoT) networks against cyber threats. However, the limited computational resources available on IoT devices pose a challenge for deploying conventional computing-based IDSs. IDSs designed for IoT environments must demonstrate high classification performance, and utilize low-complexity models. Developing intrusion detection models in the field of IoT has seen significant advancements. However, achieving a balance between high classification performance and reduced complexity remains a challenging endeavor. In this research, we present an effective IDS model that addresses this issue by combining a lightweight Convolutional Neural Network (CNN) with bidirectional Long Short-Term Memory (BiLSTM). Additionally, we employ feature selection techniques to minimize the number of features inputted into the model, thereby reducing its complexity. This approach renders the proposed model highly suitable for resource-constrained IoT devices, ensuring it meets their computation capability requirements. Creating a model that meets the demands of IoT devices and attains enhanced precision is a challenging task. However, our suggested model outperforms previous works in the literature by attaining a remarkable accuracy rate of 97.90% within a prediction time of 1.1 seconds for binary classification. Furthermore, it achieves an accuracy rate of 97.09% within a prediction time of 2.10 seconds for multiclassification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14945v1</guid>
      <category>cs.NI</category>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammed Jouhari, Hafsa Benaddi, Khalil Ibrahimi</dc:creator>
    </item>
    <item>
      <title>Schedulability Analysis in Time-Sensitive Networking: A Systematic Literature Review</title>
      <link>https://arxiv.org/abs/2407.15031</link>
      <description>arXiv:2407.15031v1 Announce Type: new 
Abstract: Time-Sensitive Networking (TSN) is a set of standards that provide low-latency, high-reliability guarantees for the transmission of traffic in networks, and it is becoming an accepted solution for complex time-critical systems such as those in industrial automation and the automotive. In time-critical systems, it is essential to verify the timing predictability of the system, and the application of scheduling mechanisms in TSN can also bring about changes in system timing. Therefore, schedulability analysis techniques can be used to verify that the system is scheduled according to the scheduling mechanism and meets the timing requirements. In this paper, we provide a clear overview of the state-of-the-art works on the topic of schedulability analysis in TSN in an attempt to clarify the purpose of schedulability analysis, categorize the methods of schedulability analysis and compare their respective strengths and weaknesses, point out the scheduling mechanisms under analyzing and the corresponding traffic classes, clarify the network scenarios constructed during the evaluation and list the challenges and directions still needing to be worked on in schedulability analysis in TSN. To this end, we conducted a systematic literature review and finally identified 123 relevant research papers published in major conferences and journals in the past 15 years. Based on a comprehensive review of the relevant literature, we have identified several key findings and emphasized the future challenges in schedulability analysis for TSN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15031v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zitong Wang, Feng Luo, Yunpeng Li, Haotian Gan, Lei Zhu</dc:creator>
    </item>
    <item>
      <title>Secure Web Objects: Building Blocks for Metaverse Interoperability and Decentralization</title>
      <link>https://arxiv.org/abs/2407.15221</link>
      <description>arXiv:2407.15221v1 Announce Type: new 
Abstract: This position paper explores how to support the Web's evolution through an underlying data-centric approach that better matches the data-orientedness of modern and emerging applications. We revisit the original vision of the Web as a hypermedia system that supports document composability and application interoperability via name-based data access. We propose the use of secure web objects (SWO), a data-oriented communication approach that can reduce complexity, centrality, and inefficiency, particularly for collaborative and local-first applications, such as the Metaverse and other collaborative applications. SWO are named, signed, application-defined objects that are secured independently of their containers or communications channels, an approach that leverages the results from over a decade-long data-centric networking research. This approach does not require intermediation by aggregators of identity, storage, and other services that are common today. We present a brief design overview, illustrated through prototypes for two editors of shared hypermedia documents: one for 3D and one for LaTeX. We also discuss our findings and suggest a roadmap for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15221v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianyuan Yu, Xinyu Ma, Varun Patil, Yekta Kocaogullar, Yulong Zhang, Jeff Burke, Dirk Kutscher, Lixia Zhang</dc:creator>
    </item>
    <item>
      <title>Exploring the Design of Collaborative Applications via the Lens of NDN Workspace</title>
      <link>https://arxiv.org/abs/2407.15234</link>
      <description>arXiv:2407.15234v1 Announce Type: new 
Abstract: Metaverse applications desire to communicate with semantically identified objects among a diverse set of cyberspace entities, such as cameras for collecting images from, sensors for sensing environment, and users collaborating with each other, all could be nearby or far away, in a timely and secure way. However, supporting the above function faces networking challenges. Today's metaverse implementations are, by and large, use secure transport connections to communicate with cloud servers instead of letting participating entities communicate directly. In this paper, we use the design and implementation of NDN Workspace, a web-based, multi-user collaborative app to showcase a new way to networking that supports many-to-many secure data exchanges among communicating entities directly. NDN Workspace users establish trust relations among each other, exchange URI-identified objects directly, and can collaborate through intermittent connectivity, all in the absence of cloud servers. Its data-centric design offers an exciting new approach to metaverse app development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15234v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianyuan Yu, Xinyu Ma, Varun Patil, Yekta Kocaogullar, Lixia Zhang</dc:creator>
    </item>
    <item>
      <title>STrack: A Reliable Multipath Transport for AI/ML Clusters</title>
      <link>https://arxiv.org/abs/2407.15266</link>
      <description>arXiv:2407.15266v1 Announce Type: new 
Abstract: Emerging artificial intelligence (AI) and machine learning (ML) workloads present new challenges of managing the collective communication used in distributed training across hundreds or even thousands of GPUs. This paper presents STrack, a novel hardware-offloaded reliable transport protocol aimed at improving the performance of AI /ML workloads by rethinking key aspects of the transport layer. STrack optimizes congestion control and load balancing in tandem: it incorporates an adaptive load balancing algorithm leveraging ECN, while adopts RTT as multi-bit congestion indicators for precise congestion window adjustment. Additionally, STrack facilitates out-of-order delivery, selective retransmission, and swift loss recovery in hardware for multipath environment. The extensive simulation comparing STrack and RoCEv2 demonstrates that STrack outperforms RoCEv2 by up to 6X with synthetic workloads and by 27.4% with collective workloads, even with the optimized RoCEv2 system setup.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15266v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yanfang Le, Rong Pan, Peter Newman, Jeremias Blendin, Abdul Kabbani, Vipin Jain, Raghava Sivaramu, Francis Matus</dc:creator>
    </item>
    <item>
      <title>SLA Decomposition for Network Slicing: A Deep Neural Network Approach</title>
      <link>https://arxiv.org/abs/2407.15288</link>
      <description>arXiv:2407.15288v1 Announce Type: new 
Abstract: For a network slice that spans multiple technology and/or administrative domains, these domains must ensure that the slice's End-to-End (E2E) Service Level Agreement (SLA) is met. Thus, the E2E SLA should be decomposed to partial SLAs, assigned to each of these domains. Assuming a two level management architecture consisting of an E2E service orchestrator and local domain controllers, we consider that the former is only aware of historical data of the local controllers' responses to previous slice requests, and captures this knowledge in a risk model per domain. In this study, we propose the use of Neural Network (NN) based risk models, using such historical data, to decompose the E2E SLA. Specifically, we introduce models that incorporate monotonicity, applicable even in cases involving small datasets. An empirical study on a synthetic multidomain dataset demonstrates the efficiency of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15288v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/LNET.2023.3310359</arxiv:DOI>
      <dc:creator>Cyril Shih-Huan Hsu, Danny De Vleeschauwer, Chrysa Papagianni</dc:creator>
    </item>
    <item>
      <title>Preliminary approaches towards the integration of TSN communications into the NFV architectural framework</title>
      <link>https://arxiv.org/abs/2407.15442</link>
      <description>arXiv:2407.15442v1 Announce Type: new 
Abstract: This paper presents a preliminary architecture for the integration of Time-Sensitive Networking (TSN) communications into the Network Functions Virtualization (NFV) architectural framework. Synergies between functional blocks and constructs of NFV, and components of TSN networks, are investigated in order to arrive at an integrated architecture. Additionally, mechanisms and configuration procedures to enable TSN-compliant, real-time, and virtualized end stations under the NFV framework are explored.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15442v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jorge Sasiain, Asier Atutxa, David Franco, Jasone Astorga, Eduardo Jacob</dc:creator>
    </item>
    <item>
      <title>Enhancing Wireless Networks with Attention Mechanisms: Insights from Mobile Crowdsensing</title>
      <link>https://arxiv.org/abs/2407.15483</link>
      <description>arXiv:2407.15483v1 Announce Type: new 
Abstract: The increasing demand for sensing, collecting, transmitting, and processing vast amounts of data poses significant challenges for resource-constrained mobile users, thereby impacting the performance of wireless networks. In this regard, from a case of mobile crowdsensing (MCS), we aim at leveraging attention mechanisms in machine learning approaches to provide solutions for building an effective, timely, and secure MCS. Specifically, we first evaluate potential combinations of attention mechanisms and MCS by introducing their preliminaries. Then, we present several emerging scenarios about how to integrate attention into MCS, including task allocation, incentive design, terminal recruitment, privacy preservation, data collection, and data transmission. Subsequently, we propose an attention-based framework to solve network optimization problems with multiple performance indicators in large-scale MCS. The designed case study have evaluated the effectiveness of the proposed framework. Finally, we outline important research directions for advancing attention-enabled MCS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15483v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yaoqi Yang, Hongyang Du, Zehui Xiong, Dusit Niyato, Abbas Jamalipour, Zhu Han</dc:creator>
    </item>
    <item>
      <title>Future-Proofing Mobile Networks: A Digital Twin Approach to Multi-Signal Management</title>
      <link>https://arxiv.org/abs/2407.15520</link>
      <description>arXiv:2407.15520v1 Announce Type: new 
Abstract: Digital Twins (DTs) are set to become a key enabling technology in future wireless networks, with their use in network management increasing significantly. We developed a DT framework that leverages the heterogeneity of network access technologies as a resource for enhanced network performance and management, enabling smart data handling in the physical network. Tested in a \textit{Campus Area Network} environment, our framework integrates diverse data sources to provide real-time, holistic insights into network performance and environmental sensing. We also envision that traditional analytics will evolve to rely on emerging AI models, such as Generative AI (GenAI), while leveraging current analytics capabilities. This capacity can simplify analytics processes through advanced ML models, enabling descriptive, diagnostic, predictive, and prescriptive analytics in a unified fashion. Finally, we present specific research opportunities concerning interoperability aspects and envision aligning advancements in DT technology with evolved AI integration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15520v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roberto Morabito, Bivek Pandey, Paulius Daubaris, Yasith R Wanigarathna, Sasu Tarkoma</dc:creator>
    </item>
    <item>
      <title>Experimenting with Adaptive Bitrate Algorithms for Virtual Reality Streaming over Wi-Fi</title>
      <link>https://arxiv.org/abs/2407.15614</link>
      <description>arXiv:2407.15614v1 Announce Type: new 
Abstract: Interactive Virtual Reality (VR) streaming over Wi-Fi networks encounters significant challenges due to bandwidth fluctuations caused by channel contention and user mobility. Adaptive BitRate (ABR) algorithms dynamically adjust the video encoding bitrate based on the available network capacity, aiming to maximize image quality while mitigating congestion and preserving the user's Quality of Experience (QoE). In this paper, we experiment with ABR algorithms for VR streaming using Air Light VR (ALVR), an open-source VR streaming solution. We extend ALVR with a comprehensive set of metrics that provide a robust characterization of the network's state, enabling more informed bitrate adjustments. To demonstrate the utility of these performance indicators, we develop and test the Network-aware Step-wise ABR algorithm for VR streaming (NeSt-VR). Results validate the accuracy of the newly implemented network performance metrics and demonstrate NeSt-VR's video bitrate adaptation capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15614v1</guid>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ferran Maura, Miguel Casasnovas, Boris Bellalta</dc:creator>
    </item>
    <item>
      <title>Sustainable broadcasting in Blockchain Network with Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2407.15616</link>
      <description>arXiv:2407.15616v1 Announce Type: new 
Abstract: Recent estimates put the carbon footprint of Bitcoin and Ethereum at an average of 64 and 26 million tonnes of CO2 per year, respectively. To address this growing problem, several possible approaches have been proposed in the literature: creating alternative blockchain consensus mechanisms, applying redundancy reduction techniques, utilizing renewable energy sources, and employing energy-efficient devices, etc. In this paper, we follow the second avenue and propose an efficient approach based on reinforcement learning that improves the block broadcasting scheme in blockchain networks. The analysis and experimental results confirmed that the proposed improvement of the block propagation scheme could cleverly handle network dynamics and achieve better results than the default approach. Additionally, our technical integration of the simulator and developed RL environment can be used as a complete solution for further study of new schemes and protocols that use RL or other ML techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15616v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Danila Valko, Daniel Kudenko</dc:creator>
    </item>
    <item>
      <title>Asynchronous Telegate and Teledata Protocols for Distributed Quantum Computing</title>
      <link>https://arxiv.org/abs/2407.14987</link>
      <description>arXiv:2407.14987v1 Announce Type: cross 
Abstract: The cost of distributed quantum operations such as the telegate and teledata protocols is high due to latencies from distributing entangled photons and classical information. This paper proposes an extension to the telegate and teledata protocols to allow for asynchronous classical communication which hides the cost of distributed quantum operations. We then discuss the benefits and limitations of these asynchronous protocols and propose a potential way to improve these asynchronous protocols using nonunitary operators. Finally, a quantum network card is described as an example of how asynchronous quantum operations might be used.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14987v1</guid>
      <category>quant-ph</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jacob Peckham, Dwight Makaroff, Steven Rayan</dc:creator>
    </item>
    <item>
      <title>ViT LoS V2X: Vision Transformers for Environment-aware LoS Blockage Prediction for 6G Vehicular Networks</title>
      <link>https://arxiv.org/abs/2407.15023</link>
      <description>arXiv:2407.15023v1 Announce Type: cross 
Abstract: As wireless communication technology progresses towards the sixth generation (6G), high-frequency millimeter-wave (mmWave) communication has emerged as a promising candidate for enabling vehicular networks. It offers high data rates and low-latency communication. However, obstacles such as buildings, trees, and other vehicles can cause signal attenuation and blockage, leading to communication failures that can result in fatal accidents or traffic congestion. Predicting blockages is crucial for ensuring reliable and efficient communications. Furthermore, the advent of 6G technology is anticipated to integrate advanced sensing capabilities, utilizing a variety of sensor types. These sensors, ranging from traditional RF sensors to cameras and Lidar sensors, are expected to provide access to rich multimodal data, thereby enriching communication systems with a wealth of additional contextual information. Leveraging this multimodal data becomes essential for making precise network management decisions, including the crucial task of blockage detection. In this paper, we propose a Deep Learning (DL)-based approach that combines Convolutional Neural Networks (CNNs) and customized Vision Transformers (ViTs) to effectively extract essential information from multimodal data and predict blockages in vehicular networks. Our method capitalizes on the synergistic strengths of CNNs and ViTs to extract features from time-series multimodal data, which include images and beam vectors. To capture temporal dependencies between the extracted features and the blockage state at future time steps, we employ a Gated Recurrent Unit (GRU)-based architecture. Our results show that the proposed approach achieves high accuracy and outperforms state-of-the-art solutions, achieving more than $95\%$ accurate predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15023v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ghazi Gharsallah, Georges Kaddoum</dc:creator>
    </item>
    <item>
      <title>Edge Graph Intelligence: Reciprocally Empowering Edge Networks with Graph Intelligence</title>
      <link>https://arxiv.org/abs/2407.15320</link>
      <description>arXiv:2407.15320v1 Announce Type: cross 
Abstract: Recent years have witnessed a thriving growth of computing facilities connected at the network edge, cultivating edge computing networks as a fundamental infrastructure for supporting miscellaneous intelligent services. Meanwhile, Artificial Intelligence frontiers have extrapolated Machine Learning to the graph domain and promoted Graph Intelligence (GI), which unlocks unprecedented ability in learning from massive data in graph structures. Given the inherent relation between graphs and networks, the interdiscipline of graph representation learning and edge networks, i.e., Edge GI or EGI, has revealed a novel interplay between them -- GI models principally open a new door for modeling, understanding, and optimizing edge networks, and conversely, edge networks serve as physical support for training, deploying, and accelerating GI models. Driven by this delicate closed-loop, EGI can be widely recognized as a promising solution to fully unleash the potential of edge computing power and is garnering significant attention. Nevertheless, research on EGI yet remains nascent, and there is a soaring demand within both the communications and AI communities for a dedicated venue to share recent advancements. To this end, this paper promotes the concept of EGI, explores its scope and core principles, and conducts a comprehensive survey concerning recent research efforts on this emerging field and specifically, introduces and discusses: 1) fundamentals of edge computing and graph representation learning, 2) emerging techniques centering on the closed loop between graph intelligence and edge networks, and 3) open challenges and research opportunities of future EGI. By bridging the gap across communication, networking, and graph learning areas, we believe that this survey can garner increased attention, foster meaningful discussions, and inspire further research ideas in EGI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15320v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liekang Zeng (Sherman), Shengyuan Ye (Sherman), Xu Chen (Sherman), Xiaoxi Zhang (Sherman), Ju Ren (Sherman), Jian Tang (Sherman), Yang Yang (Sherman),  Xuemin (Sherman),  Shen</dc:creator>
    </item>
    <item>
      <title>RIFO: Pushing the Efficiency of Programmable Packet Schedulers</title>
      <link>https://arxiv.org/abs/2308.07442</link>
      <description>arXiv:2308.07442v2 Announce Type: replace 
Abstract: Packet scheduling is a fundamental networking task that recently received renewed attention in the context of programmable data planes. Programmable packet scheduling systems such as those based on Push-In First-Out (PIFO) abstraction enabled flexible scheduling policies, but are too resource-expensive for large-scale line rate operation. This prompted research into practical programmable schedulers (e.g., SP-PIFO, AIFO) approximating PIFO behavior on regular hardware. Yet, their scalability remains limited due to extensive number of memory operations. To address this, we design an effective yet resource-efficient packet scheduler, Range-In First-Out (RIFO), which uses only three mutable memory cells and one FIFO queue per PIFO queue. RIFO is based on multi-criteria decision-making principles and uses small guaranteed admission buffers. Our large-scale simulations in Netbench demonstrate that despite using fewer resources, RIFO generally achieves competitive flow completion times across all studied workloads, and is especially effective in workloads with a significant share of large flows, reducing flow completion time up to 4.91x in datamining workload compared to state-of-the-art solutions. Our prototype implementation using P4 on Tofino switches requires only 650 lines of code, is scalable, and runs at line rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.07442v2</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Habib Mostafaei, Maciej Pacut, Stefan Schmid</dc:creator>
    </item>
    <item>
      <title>Data On the Go: Seamless Data Routing for Intermittently-Powered Battery-Free Sensing</title>
      <link>https://arxiv.org/abs/2402.00872</link>
      <description>arXiv:2402.00872v2 Announce Type: replace 
Abstract: The rising demand for sustainable IoT has promoted the adoption of battery-free devices intermittently powered by ambient energy for sensing. However, the intermittency poses significant challenges in sensing data collection. Despite recent efforts to enable one-to-one communication, routing data across multiple intermittently-powered battery-free devices, a crucial requirement for a sensing system, remains a formidable challenge.
  This paper fills this gap by introducing Swift, which enables seamless data routing in intermittently-powered battery-free sensing systems. Swift overcomes the challenges posed by device intermittency and heterogeneous energy conditions through three major innovative designs. First, Swift incorporates a reliable node synchronization protocol backed by number theory, ensuring successful synchronization regardless of energy conditions. Second, Swift adopts a low-latency message forwarding protocol, allowing continuous message forwarding without repeated synchronization. Finally, Swift features a simple yet effective mechanism for routing path construction, enabling nodes to obtain the optimal path to the sink node with minimum hops. We implement Swift and perform large-scale experiments representing diverse realworld scenarios. The results demonstrate that Swift achieves an order of magnitude reduction in end-to-end message delivery time compared with the state-of-the-art approaches for intermittentlypowered battery-free sensing systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.00872v2</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gaosheng Liu, Lin Wang</dc:creator>
    </item>
    <item>
      <title>Characterizing Encrypted Application Traffic through Cellular Radio Interface Protocol</title>
      <link>https://arxiv.org/abs/2407.07361</link>
      <description>arXiv:2407.07361v2 Announce Type: replace 
Abstract: Modern applications are end-to-end encrypted to prevent data from being read or secretly modified. 5G tech nology provides ubiquitous access to these applications without compromising the application-specific performance and latency goals. In this paper, we empirically demonstrate that 5G radio communication becomes the side channel to precisely infer the user's applications in real-time. The key idea lies in observing the 5G physical and MAC layer interactions over time that reveal the application's behavior. The MAC layer receives the data from the application and requests the network to assign the radio resource blocks. The network assigns the radio resources as per application requirements, such as priority, Quality of Service (QoS) needs, amount of data to be transmitted, and buffer size. The adversary can passively observe the radio resources to fingerprint the applications. We empirically demonstrate this attack by considering four different categories of applications: online shopping, voice/video conferencing, video streaming, and Over-The-Top (OTT) media platforms. Finally, we have also demonstrated that an attacker can differentiate various types of applications in real-time within each category.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07361v2</guid>
      <category>cs.NI</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Ruman Islam (University of Nebraska Omaha), Raja Hasnain Anwar (University of Massachusetts Amherst), Spyridon Mastorakis (University of Notre Dame), Muhammad Taqi Raza (University of Massachusetts Amherst)</dc:creator>
    </item>
    <item>
      <title>MergeSFL: Split Federated Learning with Feature Merging and Batch Size Regulation</title>
      <link>https://arxiv.org/abs/2311.13348</link>
      <description>arXiv:2311.13348v2 Announce Type: replace-cross 
Abstract: Recently, federated learning (FL) has emerged as a popular technique for edge AI to mine valuable knowledge in edge computing (EC) systems. To mitigate the computing/communication burden on resource-constrained workers and protect model privacy, split federated learning (SFL) has been released by integrating both data and model parallelism. Despite resource limitations, SFL still faces two other critical challenges in EC, i.e., statistical heterogeneity and system heterogeneity. To address these challenges, we propose a novel SFL framework, termed MergeSFL, by incorporating feature merging and batch size regulation in SFL. Concretely, feature merging aims to merge the features from workers into a mixed feature sequence, which is approximately equivalent to the features derived from IID data and is employed to promote model accuracy. While batch size regulation aims to assign diverse and suitable batch sizes for heterogeneous workers to improve training efficiency. Moreover, MergeSFL explores to jointly optimize these two strategies upon their coupled relationship to better enhance the performance of SFL. Extensive experiments are conducted on a physical platform with 80 NVIDIA Jetson edge devices, and the experimental results show that MergeSFL can improve the final model accuracy by 5.82% to 26.22%, with a speedup by about 1.74x to 4.14x, compared to the baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.13348v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunming Liao, Yang Xu, Hongli Xu, Lun Wang, Zhiwei Yao, Chunming Qiao</dc:creator>
    </item>
  </channel>
</rss>
