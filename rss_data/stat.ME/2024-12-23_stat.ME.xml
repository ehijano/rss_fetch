<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 23 Dec 2024 05:00:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The Group R2D2 Shrinkage Prior for Sparse Linear Models with Grouped Covariates</title>
      <link>https://arxiv.org/abs/2412.15293</link>
      <description>arXiv:2412.15293v1 Announce Type: new 
Abstract: Shrinkage priors are a popular Bayesian paradigm to handle sparsity in high-dimensional regression. Still limited, however, is a flexible class of shrinkage priors to handle grouped sparsity, where covariates exhibit some natural grouping structure. This paper proposes a novel extension of the $R^2$-induced Dirichlet Decomposition (R2D2) prior to accommodate grouped variable selection in linear regression models. The proposed method, called Group R2D2 prior, employs a Dirichlet prior distribution on the coefficient of determination for each group, allowing for a flexible and adaptive shrinkage that operates at both group and individual variable levels. This approach improves the original R2D2 prior to handle grouped predictors, providing a balance between within-group dependence and group-level sparsity. To facilitate efficient computation, we develop an efficient Markov Chain Monte Carlo algorithm. Through simulation studies and real-data analysis, we demonstrate that our method outperforms traditional shrinkage priors in terms of both estimation accuracy, inference and prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15293v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric Yanchenko, Kaoru Irie, Shonosuke Sugasawa</dc:creator>
    </item>
    <item>
      <title>Quantile Mediation Analytics</title>
      <link>https://arxiv.org/abs/2412.15401</link>
      <description>arXiv:2412.15401v1 Announce Type: new 
Abstract: Mediation analytics help examine if and how an intermediate variable mediates the influence of an exposure variable on an outcome of interest. Quantiles, rather than the mean, of an outcome are scientifically relevant to the comparison among specific subgroups in practical studies. Albeit some empirical studies available in the literature, there lacks a thorough theoretical investigation of quantile-based mediation analysis, which hinders practitioners from using such methods to answer important scientific questions. To address this significant technical gap, in this paper, we develop a quantile mediation analysis methodology to facilitate the identification, estimation, and testing of quantile mediation effects under a hypothesized directed acyclic graph. We establish two key estimands, quantile natural direct effect (qNDE) and quantile natural indirect effect (qNIE), in the counterfactual framework, both of which have closed-form expressions. To overcome the issue that the null hypothesis of no mediation effect is composite, we establish a powerful adaptive bootstrap method that is shown theoretically and numerically to achieve a proper type I error control. We illustrate the proposed quantile mediation analysis methodology through both extensive simulation experiments and a real-world dataset in that we investigate the mediation effect of lipidomic biomarkers for the influence of exposure to phthalates on early childhood obesity clinically diagnosed by 95\% percentile of body mass index.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15401v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Canyi Chen, Yinqiu He, Huixia J. Wang, Gongjun Xu, Peter X. -K. Song</dc:creator>
    </item>
    <item>
      <title>Logistics Regression Model for Differentially-Private Matrix Masked Data</title>
      <link>https://arxiv.org/abs/2412.15520</link>
      <description>arXiv:2412.15520v1 Announce Type: new 
Abstract: A recently proposed scheme utilizing local noise addition and matrix masking enables data collection while protecting individual privacy from all parties, including the central data manager. Statistical analysis of such privacy-preserved data is particularly challenging for nonlinear models like logistic regression. By leveraging a relationship between logistic regression and linear regression estimators, we propose the first valid statistical analysis method for logistic regression under this setting. Theoretical analysis of the proposed estimators confirmed its validity under an asymptotic framework with increasing noise magnitude to account for strict privacy requirements. Simulations and real data analyses demonstrate the superiority of the proposed estimators over naive logistic regression methods on privacy-preserved data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15520v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Linh H Nghiem, Aidong A. Ding, Samuel Wu</dc:creator>
    </item>
    <item>
      <title>High-dimensional sliced inverse regression with endogeneity</title>
      <link>https://arxiv.org/abs/2412.15530</link>
      <description>arXiv:2412.15530v1 Announce Type: new 
Abstract: Sliced inverse regression (SIR) is a popular sufficient dimension reduction method that identifies a few linear transformations of the covariates without losing regression information with the response. In high-dimensional settings, SIR can be combined with sparsity penalties to achieve sufficient dimension reduction and variable selection simultaneously. Nevertheless, both classical and sparse estimators assume the covariates are exogenous. However, endogeneity can arise in a variety of situations, such as when variables are omitted or are measured with error. In this article, we show such endogeneity invalidates SIR estimators, leading to inconsistent estimation of the true central subspace. To address this challenge, we propose a two-stage Lasso SIR estimator, which first constructs a sparse high-dimensional instrumental variables model to obtain fitted values of the covariates spanned by the instruments, and then applies SIR augmented with a Lasso penalty on these fitted values. We establish theoretical bounds for the estimation and selection consistency of the true central subspace for the proposed estimators, allowing the number of covariates and instruments to grow exponentially with the sample size. Simulation studies and applications to two real-world datasets in nutrition and genetics illustrate the superior empirical performance of the two-stage Lasso SIR estimator compared with existing methods that disregard endogeneity and/or nonlinearity in the outcome model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15530v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Linh H. Nghiem, Francis. K. C. Hui, Samuel Muller, A. H. Welsh</dc:creator>
    </item>
    <item>
      <title>Lecture Notes on High Dimensional Linear Regression</title>
      <link>https://arxiv.org/abs/2412.15633</link>
      <description>arXiv:2412.15633v1 Announce Type: new 
Abstract: These lecture notes cover advanced topics in linear regression, with an in-depth exploration of the existence, uniqueness, relations, computation, and non-asymptotic properties of the most prominent estimators in this setting. The covered estimators include least squares, ridgeless, ridge, and lasso. The content follows a proposition-proof structure, making it suitable for students seeking a formal and rigorous understanding of the statistical theory underlying machine learning methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15633v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Alberto Quaini</dc:creator>
    </item>
    <item>
      <title>Prior-Posterior Derived-Predictive Consistency Checks for Post-Estimation Calculated Quantities of Interest (QOI-Check)</title>
      <link>https://arxiv.org/abs/2412.15809</link>
      <description>arXiv:2412.15809v1 Announce Type: new 
Abstract: With flexible modeling software - such as the probabilistic programming language Stan - growing in popularity, quantities of interest (QOIs) calculated post-estimation are increasingly desired and customly implemented, both by statistical software developers and applied scientists. Examples of QOI include the marginal expectation of a multilevel model with a non-linear link function, or an ANOVA decomposition of a bivariate regression spline. For this, the QOI-Check is introduced, a systematic approach to ensure proper calibration and correct interpretation of QOIs. It contributes to Bayesian Workflow, and aims to improve the interpretability and trust in post-estimation conclusions based on QOIs. The QOI-Check builds upon Simulation Based Calibration (SBC), and the Holdout Predictive Check (HPC). SBC verifies computational reliability of Bayesian inference algorithms by consistency check of posterior with prior when the posterior is estimated on prior-predicted data, while HPC ensures robust inference by assessing consistency of model predictions with holdout data. SBC and HPC are combined in QOI-Checking for validating post-estimation QOI calculation and interpretation in the context of a (hypothetical) population definition underlying the QOI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15809v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Holger Sennhenn-Reulen</dc:creator>
    </item>
    <item>
      <title>Simulation-based Bayesian predictive probability of success for interim monitoring of clinical trials with competing event data: two case studies</title>
      <link>https://arxiv.org/abs/2412.15899</link>
      <description>arXiv:2412.15899v1 Announce Type: new 
Abstract: Bayesian predictive probabilities of success (PPoS) use interim trial data to calculate the probability of trial success. These quantities can be used to optimize trial size or to stop for futility. In this paper, we describe a simulation-based approach to compute the PPoS for clinical trials with competing event data, for which no specific methodology is currently available. The proposed procedure hinges on modelling the joint distribution of time to event and event type by specifying Bayesian models for the cause-specific hazards of all event types. This allows the prediction of outcome data at the conclusion of the trial. The PPoS is obtained by numerically averaging the probability of success evaluated at fixed parameter values over the posterior distribution of the parameters. Our work is motivated by two randomised clinical trials: the I-SPY COVID phase II trial for the treatment of severe COVID-19 (NCT04488081) and the STHLM3 prostate cancer diagnostic trial (ISRCTN84445406), both of which are characterised by competing event data. We present different modelling alternatives for the joint distribution of time to event and event type and show how the choice of the prior distributions can be used to assess the PPoS under different scenarios. The role of the PPoS analyses in the decision making process for these two trials is also discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15899v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chiara Micoli, Alessio Crippa, Jason T. Connor, I-SPY COVID Consortium, Martin Eklund, Andrea Discacciati</dc:creator>
    </item>
    <item>
      <title>A Bayesian prevalence-incidence mixture model for screening outcomes with misclassification</title>
      <link>https://arxiv.org/abs/2412.16065</link>
      <description>arXiv:2412.16065v1 Announce Type: new 
Abstract: We propose BayesPIM, a Bayesian prevalence-incidence mixture model for estimating time- and covariate-dependent disease incidence from screening and surveillance data. The method is particularly suited to settings where some individuals may have the disease at baseline, baseline tests may be missing or incomplete, and the screening test has imperfect sensitivity. Building on the existing PIMixture framework, which assumes perfect sensitivity, BayesPIM accommodates uncertain test accuracy by incorporating informative priors. By including covariates, the model can quantify heterogeneity in disease risk, thereby informing personalized screening strategies. We motivate the model using data from high-risk familial colorectal cancer (CRC) surveillance through colonoscopy, where adenomas - precursors of CRC - may already be present at baseline and remain undetected due to imperfect test sensitivity. We show that conditioning incidence and prevalence estimates on covariates explains substantial heterogeneity in adenoma risk. Using a Metropolis-within-Gibbs sampler and data augmentation, BayesPIM robustly recovers incidence times while handling latent prevalence. Informative priors on the test sensitivity stabilize estimation and mitigate non-convergence issues. Model fit can be assessed using information criteria and validated against a non-parametric estimator. In this way, BayesPIM enhances estimation accuracy and supports the development of more effective, patient-centered screening policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16065v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Klausch, Birgit I. Lissenberg-Witte, Veerle M. Coup\'e</dc:creator>
    </item>
    <item>
      <title>Modeling Story Expectations to Understand Engagement: A Generative Framework Using LLMs</title>
      <link>https://arxiv.org/abs/2412.15239</link>
      <description>arXiv:2412.15239v1 Announce Type: cross 
Abstract: Understanding when and why consumers engage with stories is crucial for content creators and platforms. While existing theories suggest that audience beliefs of what is going to happen should play an important role in engagement decisions, empirical work has mostly focused on developing techniques to directly extract features from actual content, rather than capturing forward-looking beliefs, due to the lack of a principled way to model such beliefs in unstructured narrative data. To complement existing feature extraction techniques, this paper introduces a novel framework that leverages large language models to model audience forward-looking beliefs about how stories might unfold. Our method generates multiple potential continuations for each story and extracts features related to expectations, uncertainty, and surprise using established content analysis techniques. Applying our method to over 30,000 book chapters from Wattpad, we demonstrate that our framework complements existing feature engineering techniques by amplifying their marginal explanatory power on average by 31%. The results reveal that different types of engagement-continuing to read, commenting, and voting-are driven by distinct combinations of current and anticipated content features. Our framework provides a novel way to study and explore how audience forward-looking beliefs shape their engagement with narrative media, with implications for marketing strategy in content-focused industries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15239v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hortense Fong, George Gui</dc:creator>
    </item>
    <item>
      <title>A Comparative Study of DSPy Teleprompter Algorithms for Aligning Large Language Models Evaluation Metrics to Human Evaluation</title>
      <link>https://arxiv.org/abs/2412.15298</link>
      <description>arXiv:2412.15298v1 Announce Type: cross 
Abstract: We argue that the Declarative Self-improving Python (DSPy) optimizers are a way to align the large language model (LLM) prompts and their evaluations to the human annotations. We present a comparative analysis of five teleprompter algorithms, namely, Cooperative Prompt Optimization (COPRO), Multi-Stage Instruction Prompt Optimization (MIPRO), BootstrapFewShot, BootstrapFewShot with Optuna, and K-Nearest Neighbor Few Shot, within the DSPy framework with respect to their ability to align with human evaluations. As a concrete example, we focus on optimizing the prompt to align hallucination detection (using LLM as a judge) to human annotated ground truth labels for a publicly available benchmark dataset. Our experiments demonstrate that optimized prompts can outperform various benchmark methods to detect hallucination, and certain telemprompters outperform the others in at least these experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15298v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-fin.ST</category>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bhaskarjit Sarmah, Kriti Dutta, Anna Grigoryan, Sachin Tiwari, Stefano Pasquali, Dhagash Mehta</dc:creator>
    </item>
    <item>
      <title>Protocol for an Observational Study on the Effects of Paternal Alcohol Use Disorder on Children's Later Life Outcomes</title>
      <link>https://arxiv.org/abs/2412.15535</link>
      <description>arXiv:2412.15535v1 Announce Type: cross 
Abstract: The harmful effects of growing up with a parent with an alcohol use disorder have been closely examined in children and adolescents, and are reported to include mental and physical health problems, interpersonal difficulties, and a worsened risk of future substance use disorders. However, few studies have investigated how these impacts evolve into later life adulthood, leaving the ensuing long-term effects of interest. In this article, we provide the protocol for our observational study of the long-term consequences of growing up with a father who had an alcohol use disorder. We will use data from the Wisconsin Longitudinal Study to examine impacts on long-term economic success, interpersonal relationships, physical, and mental health. To reinforce our findings, we will conduct this investigation on two discrete subpopulations of individuals in our study, allowing us to analyze the replicability of our conclusions. We introduce a novel statistical design, called data turnover, to carry out this analysis. Data turnover allows a single group of statisticians and domain experts to work together to assess the strength of evidence gathered across multiple data splits, while incorporating both qualitative and quantitative findings from data exploration. We delineate our analysis plan using this new method and conclude with a brief discussion of some additional considerations for our study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15535v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William Bekerman, Marina Bogomolov, Ruth Heller, Matthew Spivey, Kevin G. Lynch, David W. Oslin, Dylan S. Small</dc:creator>
    </item>
    <item>
      <title>Deep learning joint extremes of metocean variables using the SPAR model</title>
      <link>https://arxiv.org/abs/2412.15808</link>
      <description>arXiv:2412.15808v1 Announce Type: cross 
Abstract: This paper presents a novel deep learning framework for estimating multivariate joint extremes of metocean variables, based on the Semi-Parametric Angular-Radial (SPAR) model. When considered in polar coordinates, the problem of modelling multivariate extremes is transformed to one of modelling an angular density, and the tail of a univariate radial variable conditioned on angle. In the SPAR approach, the tail of the radial variable is modelled using a generalised Pareto (GP) distribution, providing a natural extension of univariate extreme value theory to the multivariate setting. In this work, we show how the method can be applied in higher dimensions, using a case study for five metocean variables: wind speed, wind direction, wave height, wave period and wave direction. The angular variable is modelled empirically, while the parameters of the GP model are approximated using fully-connected deep neural networks. Our data-driven approach provides great flexibility in the dependence structures that can be represented, together with computationally efficient routines for training the model. Furthermore, the application of the method requires fewer assumptions about the underlying distribution(s) compared to existing approaches, and an asymptotically justified means for extrapolating outside the range of observations. Using various diagnostic plots, we show that the fitted models provide a good description of the joint extremes of the metocean variables considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15808v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ed Mackay, Callum Murphy-Barltrop, Jordan Richards, Philip Jonathan</dc:creator>
    </item>
    <item>
      <title>Typical Algorithms for Estimating Hurst Exponent of Time Sequence: A Data Analyst's Perspective</title>
      <link>https://arxiv.org/abs/2310.19051</link>
      <description>arXiv:2310.19051v4 Announce Type: replace 
Abstract: The Hurst exponent is a significant metric for characterizing time sequences with long-term memory property and it arises in many fields. The available methods for estimating the Hurst exponent can be categorized into time-domain and spectrum-domain methods. Although there are various estimation methods for the Hurst exponent, there are still some disadvantages that should be overcome: firstly, the estimation methods are mathematics-oriented instead of engineering-oriented; secondly, the accuracy and effectiveness of the estimation algorithms are inadequately assessed; thirdly, the framework of classification for the estimation methods are insufficient; and lastly there is a lack of clear guidance for selecting proper estimation in practical problems involved in data analysis. The contributions of this paper lie in four aspects: 1) the optimal sequence partition method is proposed for designing the estimation algorithms for Hurst exponent; 2) the algorithmic pseudo-codes are adopted to describe the estimation algorithms, which improves the understandability and usability of the estimation methods and also reduces the difficulty of implementation with computer programming languages; 3) the performance assessment is carried for the typical estimation algorithms via the ideal time sequence with given Hurst exponent and the practical time sequence captured in applications; 4) the guidance for selecting proper algorithms for estimating the Hurst exponent is presented and discussed. It is expected that the systematic survey of available estimation algorithms could help the users to understand the principles and the assessment of the various estimation methods could help the users to select, implement and apply the estimation algorithms of interest in practical situations in an easy way.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.19051v4</guid>
      <category>stat.ME</category>
      <category>cs.MS</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ACCESS.2024.3512542</arxiv:DOI>
      <arxiv:journal_reference>IEEE Access, 12(12): 185528--185556, 2024</arxiv:journal_reference>
      <dc:creator>Hong-Yan Zhang, Zhi-Qiang Feng, Si-Yu Feng, Yu Zhou</dc:creator>
    </item>
    <item>
      <title>A maximum penalised likelihood approach for semiparametric accelerated failure time models with time-varying covariates and partly interval censoring</title>
      <link>https://arxiv.org/abs/2403.12332</link>
      <description>arXiv:2403.12332v2 Announce Type: replace 
Abstract: Accelerated failure time (AFT) models are frequently used to model survival data, providing a direct quantification of the relationship between event times and covariates. These models allow for the acceleration or deceleration of failure times through a multiplicative factor that accounts for the effect of covariates. While existing literature provides numerous methods for fitting AFT models with time-fixed covariates, adapting these approaches to scenarios involving both time-varying covariates and partly interval-censored data remains challenging. Motivated by a randomised clinical trial dataset on advanced melanoma patients, we propose a maximum penalised likelihood approach for fitting a semiparametric AFT model to survival data with partly interval-censored failure times. This method also accommodates both time-fixed and time-varying covariates. We utilise Gaussian basis functions to construct a smooth approximation of the non-parametric baseline hazard and fit the model using a constrained optimisation approach. The effectiveness of our method is demonstrated through extensive simulations. Finally, we illustrate the relevance of our approach by applying it to a dataset from a randomised clinical trial involving patients with advanced melanoma.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12332v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aishwarya Bhaskaran, Ding Ma, Benoit Liquet, Angela Hong, Stephane Heritier, Serigne N Lo, Jun Ma</dc:creator>
    </item>
    <item>
      <title>Identifying Macro Conditional Independencies and Macro Total Effects in Summary Causal Graphs with Latent Confounding</title>
      <link>https://arxiv.org/abs/2407.07934</link>
      <description>arXiv:2407.07934v4 Announce Type: replace 
Abstract: Understanding causal relations in dynamic systems is essential in epidemiology. While causal inference methods have been extensively studied, they often rely on fully specified causal graphs, which may not always be available in complex dynamic systems. Partially specified causal graphs, and in particular summary causal graphs (SCGs), provide a simplified representation of causal relations between time series when working spacio-temporal data, omitting temporal information and focusing on causal structures between clusters of of temporal variables. Unlike fully specified causal graphs, SCGs can contain cycles, which complicate their analysis and interpretation. In addition, their cluster-based nature introduces new challenges concerning the types of queries of interest: macro queries, which involve relationships between clusters represented as vertices in the graph, and micro queries, which pertain to relationships between variables that are not directly visible through the vertices of the graph. In this paper, we first clearly distinguish between macro conditional independencies and micro conditional independencies and between macro total effects and micro total effects. Then, we demonstrate the soundness and completeness of the d-separation to identify macro conditional independencies in SCGs. Furthermore, we establish that the do-calculus is sound and complete for identifying macro total effects in SCGs. Finally, we give a graphical characterization for the non-identifiability of macro total effects in SCGs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07934v4</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Ferreira, Charles K. Assaad</dc:creator>
    </item>
    <item>
      <title>Fast QR updating methods for statistical applications</title>
      <link>https://arxiv.org/abs/2412.05905</link>
      <description>arXiv:2412.05905v2 Announce Type: replace 
Abstract: This paper introduces fast R updating algorithms designed for statistical applications, including regression, filtering, and model selection, where data structures change frequently. Although traditional QR decomposition is essential for matrix operations, it becomes computationally intensive when dynamically updating the design matrix in statistical models. The proposed algorithms efficiently update the R matrix without recalculating Q, significantly reducing computational costs. These algorithms provide a scalable solution for high-dimensional regression models, enhancing the feasibility of large-scale statistical analyses and model selection in data-intensive fields. Comprehensive simulation studies and real-world data applications reveal that the methods significantly reduce computational time while preserving accuracy. An extensive discussion highlights the versatility of fast R updating algorithms, illustrating their benefits across a wide range of models and applications in statistics and machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05905v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mauro Bernardi, Claudio Busatto, Manuela Cattelan</dc:creator>
    </item>
    <item>
      <title>Boosting Distributional Copula Regression for Bivariate Right-Censored Time-to-Event Data</title>
      <link>https://arxiv.org/abs/2412.15041</link>
      <description>arXiv:2412.15041v2 Announce Type: replace 
Abstract: We propose a highly flexible distributional copula regression model for bivariate time-to-event data in the presence of right-censoring. The joint survival function of the response is constructed using parametric copulas, allowing for a separate specification of the dependence structure between the time-to-event outcome variables and their respective marginal survival distributions. The latter are specified using well-known parametric distributions such as the log-Normal, log-Logistic (proportional odds model), or Weibull (proportional hazards model) distributions. Hence, the marginal univariate event times can be specified as parametric (also known as Accelerated Failure Time, AFT) models. Embedding our model into the class of generalized additive models for location, scale and shape, possibly all distribution parameters of the joint survival function can depend on covariates. We develop a component-wise gradient-based boosting algorithm for estimation. This way, our approach is able to conduct data-driven variable selection. To the best of our knowledge, this is the first implementation of multivariate AFT models via distributional copula regression with automatic variable selection via statistical boosting. A special merit of our approach is that it works for high-dimensional (p&gt;&gt;n) settings. We illustrate the practical potential of our method on a high-dimensional application related to semi-competing risks responses in ovarian cancer. All of our methods are implemented in the open source statistical software R as add-on functions of the package gamboostLSS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15041v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guillermo Briseno-Sanchez, Nadja Klein, Andreas Groll, Andreas Mayr</dc:creator>
    </item>
    <item>
      <title>Factor Augmented Tensor-on-Tensor Neural Networks</title>
      <link>https://arxiv.org/abs/2405.19610</link>
      <description>arXiv:2405.19610v2 Announce Type: replace-cross 
Abstract: This paper studies the prediction task of tensor-on-tensor regression in which both covariates and responses are multi-dimensional arrays (a.k.a., tensors) across time with arbitrary tensor order and data dimension. Existing methods either focused on linear models without accounting for possibly nonlinear relationships between covariates and responses, or directly employed black-box deep learning algorithms that failed to utilize the inherent tensor structure. In this work, we propose a Factor Augmented Tensor-on-Tensor Neural Network (FATTNN) that integrates tensor factor models into deep neural networks. We begin with summarizing and extracting useful predictive information (represented by the ``factor tensor'') from the complex structured tensor covariates, and then proceed with the prediction task using the estimated factor tensor as input of a temporal convolutional neural network. The proposed methods effectively handle nonlinearity between complex data structures, and improve over traditional statistical models and conventional deep learning approaches in both prediction accuracy and computational cost. By leveraging tensor factor models, our proposed methods exploit the underlying latent factor structure to enhance the prediction, and in the meantime, drastically reduce the data dimensionality that speeds up the computation. The empirical performances of our proposed methods are demonstrated via simulation studies and real-world applications to three public datasets. Numerical results show that our proposed algorithms achieve substantial increases in prediction accuracy and significant reductions in computational time compared to benchmark methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19610v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guanhao Zhou, Yuefeng Han, Xiufan Yu</dc:creator>
    </item>
    <item>
      <title>Sequential Conditional Transport on Probabilistic Graphs for Interpretable Counterfactual Fairness</title>
      <link>https://arxiv.org/abs/2408.03425</link>
      <description>arXiv:2408.03425v2 Announce Type: replace-cross 
Abstract: In this paper, we link two existing approaches to derive counterfactuals: adaptations based on a causal graph, and optimal transport. We extend "Knothe's rearrangement" and "triangular transport" to probabilistic graphical models, and use this counterfactual approach, referred to as sequential transport, to discuss fairness at the individual level. After establishing the theoretical foundations of the proposed method, we demonstrate its application through numerical experiments on both synthetic and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03425v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Agathe Fernandes Machado, Arthur Charpentier, Ewen Gallic</dc:creator>
    </item>
    <item>
      <title>Robust spectral clustering with rank statistics</title>
      <link>https://arxiv.org/abs/2408.10136</link>
      <description>arXiv:2408.10136v2 Announce Type: replace-cross 
Abstract: This paper analyzes the statistical performance of a robust spectral clustering method for latent structure recovery in noisy data matrices. We consider eigenvector-based clustering applied to a matrix of nonparametric rank statistics that is derived entrywise from the raw, original data matrix. This approach is robust in the sense that, unlike traditional spectral clustering procedures, it can provably recover population-level latent block structure even when the observed data matrix includes heavy-tailed entries and has a heterogeneous variance profile.
  Our main theoretical contributions are threefold and hold under flexible data generating conditions. First, we establish that robust spectral clustering with rank statistics can consistently recover latent block structure, viewed as communities of nodes in a graph, in the sense that unobserved community memberships for all but a vanishing fraction of nodes are correctly recovered with high probability when the data matrix is large. Second, we refine the former result and further establish that, under certain conditions, the community membership of any individual, specified node of interest can be asymptotically exactly recovered with probability tending to one in the large-data limit. Third, we establish asymptotic normality results associated with the truncated eigenstructure of matrices whose entries are rank statistics, made possible by synthesizing contemporary entrywise matrix perturbation analysis with the classical nonparametric theory of so-called simple linear rank statistics. Collectively, these results demonstrate the statistical utility of rank-based data transformations when paired with spectral techniques for dimensionality reduction. Additionally, for a dataset of human connectomes, our approach yields parsimonious dimensionality reduction and improved recovery of ground-truth neuroanatomical cluster structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10136v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joshua Cape, Xianshi Yu, Jonquil Z. Liao</dc:creator>
    </item>
    <item>
      <title>Scientific Realism vs. Anti-Realism: Toward a Common Ground</title>
      <link>https://arxiv.org/abs/2412.10643</link>
      <description>arXiv:2412.10643v2 Announce Type: replace-cross 
Abstract: The debate between scientific realism and anti-realism remains at a stalemate, making reconciliation seem hopeless. Yet, important work remains: exploring a common ground, even if only to uncover deeper points of disagreement and, ideally, to benefit both sides of the debate. I propose such a common ground. Specifically, many anti-realists, such as instrumentalists, have yet to seriously engage with Sober's call to justify their preferred version of Ockham's razor through a positive account. Meanwhile, realists face a similar challenge: providing a non-circular explanation of how their version of Ockham's razor connects to truth. The common ground I propose addresses these challenges for both sides; the key is to leverage the idea that everyone values some truths and to draw on insights from scientific fields that study scientific inference -- namely, statistics and machine learning. This common ground also isolates a distinctively epistemic root of the irreconcilability in the realism debate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10643v2</guid>
      <category>stat.OT</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hanti Lin</dc:creator>
    </item>
  </channel>
</rss>
