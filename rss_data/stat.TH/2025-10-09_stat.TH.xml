<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 10 Oct 2025 04:01:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Beyond independent component analysis: identifiability and algorithms</title>
      <link>https://arxiv.org/abs/2510.07525</link>
      <description>arXiv:2510.07525v1 Announce Type: new 
Abstract: Independent Component Analysis (ICA) is a classical method for recovering latent variables with useful identifiability properties. For independent variables, cumulant tensors are diagonal; relaxing independence yields tensors whose zero structure generalizes diagonality. These models have been the subject of recent work in non-independent component analysis. We show that pairwise mean independence answers the question of how much one can relax independence: it is identifiable, any weaker notion is non-identifiable, and it contains the models previously studied as special cases. Our results apply to distributions with the required zero pattern at any cumulant tensor. We propose an algebraic recovery algorithm based on least-squares optimization over the orthogonal group. Simulations highlight robustness: enforcing full independence can harm estimation, while pairwise mean independence enables more stable recovery. These findings extend the classical ICA framework and provide a rigorous basis for blind source separation beyond independence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07525v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alvaro Ribot, Anna Seigal, Piotr Zwiernik</dc:creator>
    </item>
    <item>
      <title>Adaptive Thresholds for Monitoring and Screening in Imbalanced Samples: Optimality and Boosting Sensitivity</title>
      <link>https://arxiv.org/abs/2510.08035</link>
      <description>arXiv:2510.08035v1 Announce Type: new 
Abstract: Suppose (standardized) measurements or statistics are monitored to raise an alarm when a threshold is exceeded. Often, the underlying population is heterogenous with respect to important discrete variables and thus samples may consist of imbalanced classes. We propose to use thresholds which depend on such covariates to boost the sensitivity for rare classes, which otherwise tend to be ignored. Under mild conditions, we identify optimal threshold functions and develop a feasible procedure for their computation. Further, for the proportional rule a nonparametric estimator of the threshold function is proposed and a central limit theorem is shown, including the case that conditional mean and variance used for standardization are estimated. For feasible uncertainty quantification a bootstrap scheme is proposed. The approach is illustrated and evaluated by a real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08035v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ansgar Steland</dc:creator>
    </item>
    <item>
      <title>Structured covariance estimation via tensor-train decomposition</title>
      <link>https://arxiv.org/abs/2510.08174</link>
      <description>arXiv:2510.08174v1 Announce Type: new 
Abstract: We consider a problem of covariance estimation from a sample of i.i.d. high-dimensional random vectors. To avoid the curse of dimensionality we impose an additional assumption on the structure of the covariance matrix $\Sigma$. To be more precise we study the case when $\Sigma$ can be approximated by a sum of double Kronecker products of smaller matrices in a tensor train (TT) format. Our setup naturally extends widely known Kronecker sum and CANDECOMP/PARAFAC models but admits richer interaction across modes. We suggest an iterative polynomial time algorithm based on TT-SVD and higher-order orthogonal iteration (HOOI) adapted to Tucker-2 hybrid structure. We derive non-asymptotic dimension-free bounds on the accuracy of covariance estimation taking into account hidden Kronecker product and tensor train structures. The efficiency of our approach is illustrated with numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08174v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Artsiom Patarusau, Nikita Puchkin, Maxim Rakhuba, Fedor Noskov</dc:creator>
    </item>
    <item>
      <title>Navigating Sparsities in High-Dimensional Linear Contextual Bandits</title>
      <link>https://arxiv.org/abs/2510.08435</link>
      <description>arXiv:2510.08435v1 Announce Type: new 
Abstract: High-dimensional linear contextual bandit problems remain a significant challenge due to the curse of dimensionality. Existing methods typically consider either the model parameters to be sparse or the eigenvalues of context covariance matrices to be (approximately) sparse, lacking general applicability due to the rigidity of conventional reward estimators. To overcome this limitation, a powerful pointwise estimator is introduced in this work that adaptively navigates both kinds of sparsity. Based on this pointwise estimator, a novel algorithm, termed HOPE, is proposed. Theoretical analyses demonstrate that HOPE not only achieves improved regret bounds in previously discussed homogeneous settings (i.e., considering only one type of sparsity) but also, for the first time, efficiently handles two new challenging heterogeneous settings (i.e., considering a mixture of two types of sparsity), highlighting its flexibility and generality. Experiments corroborate the superiority of HOPE over existing methods across various scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08435v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.TH</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rui Zhao, Zihan Chen, Zemin Zheng</dc:creator>
    </item>
    <item>
      <title>Computational and statistical lower bounds for low-rank estimation under general inhomogeneous noise</title>
      <link>https://arxiv.org/abs/2510.08541</link>
      <description>arXiv:2510.08541v1 Announce Type: new 
Abstract: Recent work has generalized several results concerning the well-understood spiked Wigner matrix model of a low-rank signal matrix corrupted by additive i.i.d. Gaussian noise to the inhomogeneous case, where the noise has a variance profile. In particular, for the special case where the variance profile has a block structure, a series of results identified an effective spectral algorithm for detecting and estimating the signal, identified the threshold signal strength required for that algorithm to succeed, and proved information-theoretic lower bounds that, for some special signal distributions, match the above threshold. We complement these results by studying the computational optimality of this spectral algorithm. Namely, we show that, for a much broader range of signal distributions, whenever the spectral algorithm cannot detect a low-rank signal, then neither can any low-degree polynomial algorithm. This gives the first evidence for a computational hardness conjecture of Guionnet, Ko, Krzakala, and Zdeborov\'a (2023). With similar techniques, we also prove sharp information-theoretic lower bounds for a class of signal distributions not treated by prior work. Unlike all of the above results on inhomogeneous models, our results do not assume that the variance profile has a block structure, and suggest that the same spectral algorithm might remain optimal for quite general profiles. We include a numerical study of this claim for an example of a smoothly-varying rather than piecewise-constant profile. Our proofs involve analyzing the graph sums of a matrix, which also appear in free and traffic probability, but we require new bounds on these quantities that are tighter than existing ones for non-negative matrices, which may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08541v1</guid>
      <category>math.ST</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Debsurya De, Dmitriy Kunisky</dc:creator>
    </item>
    <item>
      <title>A coupling-based approach to f-divergences diagnostics for Markov chain Monte Carlo</title>
      <link>https://arxiv.org/abs/2510.07559</link>
      <description>arXiv:2510.07559v1 Announce Type: cross 
Abstract: A long-standing gap exists between the theoretical analysis of Markov chain Monte Carlo convergence, which is often based on statistical divergences, and the diagnostics used in practice. We introduce the first general convergence diagnostics for Markov chain Monte Carlo based on any f-divergence, allowing users to directly monitor, among others, the Kullback--Leibler and the $\chi^2$ divergences as well as the Hellinger and the total variation distances. Our first key contribution is a coupling-based `weight harmonization' scheme that produces a direct, computable, and consistent weighting of interacting Markov chains with respect to their target distribution. The second key contribution is to show how such consistent weightings of empirical measures can be used to provide upper bounds to f-divergences in general. We prove that these bounds are guaranteed to tighten over time and converge to zero as the chains approach stationarity, providing a concrete diagnostic. Numerical experiments demonstrate that our method is a practical and competitive diagnostic tool.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07559v1</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adrien Corenflos, Hai-Dang Dau</dc:creator>
    </item>
    <item>
      <title>Statistical properties of Markov shifts (part I)</title>
      <link>https://arxiv.org/abs/2510.07757</link>
      <description>arXiv:2510.07757v1 Announce Type: cross 
Abstract: We prove central limit theorems, Berry-Esseen type theorems, almost sure invariance principles, large deviations and Livsic type regularity for partial sums of the form $S_n=\sum_{j=0}^{n-1}f_j(...,X_{j-1},X_j,X_{j+1},...)$, where $(X_j)$ is an inhomogeneous Markov chain satisfying some mixing assumptions and $f_j$ is a sequence of sufficiently regular functions. Even though the case of non-stationary chains and time dependent functions $f_j$ is more challenging, our results seem to be new already for stationary Markov chains. They also seem to be new for non-stationary Bernoulli shifts (that is when $(X_j)$ are independent but not identically distributed). This paper is the first one in a series of two papers. In \cite{Work} we will prove local limit theorems including developing the related reduction theory in the sense of \cite{DolgHaf LLT, DS}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07757v1</guid>
      <category>math.PR</category>
      <category>math.DS</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yeor Hafouta</dc:creator>
    </item>
    <item>
      <title>Detection of mean changes in partially observed functional data</title>
      <link>https://arxiv.org/abs/2510.07854</link>
      <description>arXiv:2510.07854v1 Announce Type: cross 
Abstract: We propose a test for a change in the mean for a sequence of functional observations that are only partially observed on subsets of the domain, with no information available on the complement. The framework accommodates important scenarios, including both abrupt and gradual changes. The significance of the test statistic is assessed via a permutation test. In addition to the classical permutation approach with a fixed number of permutation samples, we also discuss a variant with controlled resampling risk that relies on a random (data-driven) number of permutation samples. The small sample performance of the proposed methodology is illustrated in a Monte Carlo simulation study and an application to real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07854v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>\v{S}\'arka Hudecov\'a, Claudia Kirch</dc:creator>
    </item>
    <item>
      <title>On the Optimality of the Median-of-Means Estimator under Adversarial Contamination</title>
      <link>https://arxiv.org/abs/2510.07867</link>
      <description>arXiv:2510.07867v1 Announce Type: cross 
Abstract: The Median-of-Means (MoM) is a robust estimator widely used in machine learning that is known to be (minimax) optimal in scenarios where samples are i.i.d. In more grave scenarios, samples are contaminated by an adversary that can inspect and modify the data. Previous work has theoretically shown the suitability of the MoM estimator in certain contaminated settings. However, the (minimax) optimality of MoM and its limitations under adversarial contamination remain unknown beyond the Gaussian case. In this paper, we present upper and lower bounds for the error of MoM under adversarial contamination for multiple classes of distributions. In particular, we show that MoM is (minimax) optimal in the class of distributions with finite variance, as well as in the class of distributions with infinite variance and finite absolute $(1+r)$-th moment. We also provide lower bounds for MoM's error that match the order of the presented upper bounds, and show that MoM is sub-optimal for light-tailed distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07867v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xabier de Juan, Santiago Mazuelas</dc:creator>
    </item>
    <item>
      <title>Precise convergence rate of spectral radius of product of complex Ginibre</title>
      <link>https://arxiv.org/abs/2510.07942</link>
      <description>arXiv:2510.07942v1 Announce Type: cross 
Abstract: Let $Z_1, \cdots, Z_n$ denote the eigenvalues of the product $\prod_{j=1}^{k_n} \boldsymbol{A}_j$, where $\{\boldsymbol{A}_j\}_{1 \le j \le k_n}$ are independent $n\times n$ complex Ginibre matrices. Define $\alpha = \lim\limits_{n \to \infty} \frac{n}{k_n}$. We prove that $X_n,$ a suitably rescaled version of $\max_{1 \le j \le n} |Z_j|^2,$ converges weakly as follows: to a non-trivial distribution $\Phi_\alpha$ for $\alpha \in (0, +\infty)$, to the Gumbel distribution when $\alpha = +\infty$, and to the standard normal distribution when $\alpha = 0$. This result reveals a phase transition at the boundaries of $\alpha$. Furthermore, we establish the exact rates of convergence in each regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07942v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yutao Ma, Xujia Meng</dc:creator>
    </item>
    <item>
      <title>Fitting sparse high-dimensional varying-coefficient models with Bayesian regression tree ensembles</title>
      <link>https://arxiv.org/abs/2510.08204</link>
      <description>arXiv:2510.08204v1 Announce Type: cross 
Abstract: By allowing the effects of $p$ covariates in a linear regression model to vary as functions of $R$ additional effect modifiers, varying-coefficient models (VCMs) strike a compelling balance between interpretable-but-rigid parametric models popular in classical statistics and flexible-but-opaque methods popular in machine learning. But in high-dimensional settings where $p$ and/or $R$ exceed the number of observations, existing approaches to fitting VCMs fail to identify which covariates have a non-zero effect and which effect modifiers drive these effects. We propose sparseVCBART, a fully Bayesian model that approximates each coefficient function in a VCM with a regression tree ensemble and encourages sparsity with a global--local shrinkage prior on the regression tree leaf outputs and a hierarchical prior on the splitting probabilities of each tree. We show that the sparseVCBART posterior contracts at a near-minimax optimal rate, automatically adapting to the unknown sparsity structure and smoothness of the true coefficient functions. Compared to existing state-of-the-art methods, sparseVCBART achieved competitive predictive accuracy and substantially narrower and better-calibrated uncertainty intervals, especially for null covariate effects. We use sparseVCBART to investigate how the effects of interpersonal conversations on prejudice could vary according to the political and demographic characteristics of the respondents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08204v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soham Ghosh, Saloni Bhogale, Sameer K. Deshpande</dc:creator>
    </item>
    <item>
      <title>Doubly Robust Estimation with Stabilized Weights for Binary Proximal Outcomes in Micro-Randomized Trials</title>
      <link>https://arxiv.org/abs/2510.08359</link>
      <description>arXiv:2510.08359v1 Announce Type: cross 
Abstract: Micro-randomized trials (MRTs) are increasingly used to evaluate mobile health interventions with binary proximal outcomes. Standard inverse probability weighting (IPW) estimators are unbiased but unstable in small samples or under extreme randomization. Estimated mean excursion effect (EMEE) improves efficiency but lacks double robustness. We propose a doubly robust EMEE (DR-EMEE) with stabilized and truncated weights, combining per-decision IPW and outcome regression. We prove double robustness, asymptotic efficiency, and provide finite-sample variance corrections, with extensions to machine learning nuisance estimators. In simulations, DR-EMEE reduces root mean squared error, improves coverage, and achieves up to twofold efficiency gains over IPW and five to ten percent over EMEE. Applications to HeartSteps, PAMAP2, and mHealth datasets confirm stable and efficient inference across both randomized and observational settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08359v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinho Cha, Eunchan Cha</dc:creator>
    </item>
    <item>
      <title>Optimal Distillation of Qubit Clocks</title>
      <link>https://arxiv.org/abs/2510.08493</link>
      <description>arXiv:2510.08493v1 Announce Type: cross 
Abstract: We study coherence distillation under time-translation-invariant operations: given many copies of a quantum state containing coherence in the energy eigenbasis, the aim is to produce a purer coherent state while respecting the time-translation symmetry. This symmetry ensures that the output remains synchronized with the input and that the process can be realized by energy-conserving unitaries coupling the system to a reservoir initially in an energy eigenstate, thereby modeling thermal operations supplemented by a work reservoir or battery. For qubit systems, we determine the optimal asymptotic fidelity and show that it is governed by the purity of coherence, a measure of asymmetry derived from the right logarithmic derivative (RLD) Fisher information. In particular, we find that the lowest achievable infidelity (one minus fidelity) scales as $1/N$ times the reciprocal of the purity of coherence of each input qubit, where $N$ is the number of copies, giving this quantity a clear operational meaning. We additionally study many other interesting aspects of the coherence distillation problem for qubits, including computing higher-order corrections to the lowest achievable infidelity up to $O(1/N^3)$, and expressing the optimal channel as a boundary value problem that can be solved numerically.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08493v1</guid>
      <category>quant-ph</category>
      <category>cond-mat.stat-mech</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.ST</category>
      <category>physics.atom-ph</category>
      <category>stat.TH</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sujay Kazi, Iman Marvian</dc:creator>
    </item>
    <item>
      <title>A Necessary and Sufficient Condition for Size Controllability of Heteroskedasticity Robust Test Statistics</title>
      <link>https://arxiv.org/abs/2412.17470</link>
      <description>arXiv:2412.17470v2 Announce Type: replace 
Abstract: We revisit size controllability results in P\"otscher and Preinerstorfer (2025) concerning heteroskedasticity robust test statistics in regression models. For the special, but important, case of testing a single restriction (e.g., a zero restriction on a single coefficient), we povide a necessary and sufficient condition for size controllability, whereas the condition in P\"otscher and Preinerstorfer (2025) is, in general, only sufficient (even in the case of testing a single restriction).</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17470v2</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Benedikt M. P\"otscher, David Preinerstorfer</dc:creator>
    </item>
    <item>
      <title>Conditional distributions for the nested Dirichlet process via sequential imputation</title>
      <link>https://arxiv.org/abs/2505.00451</link>
      <description>arXiv:2505.00451v2 Announce Type: replace 
Abstract: We consider an array of random variables, taking values in a complete and separable metric space, that exhibits a kind of symmetry which we call row exchangeability. Given such an array, a natural model for Bayesian nonparametric inference is the nested Dirichlet process (NDP). Exactly determining posterior distributions for the NDP is infeasible, since the computations involved grow exponentially with the sample size. In this paper, we present a new approach to determining these posterior distributions that involves the use of sequential</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00451v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Evan Donald, Jason Swanson</dc:creator>
    </item>
    <item>
      <title>A Statistical Test for Comparing the Linkage and Admixture Model Based on Central Limit Theorems</title>
      <link>https://arxiv.org/abs/2509.12734</link>
      <description>arXiv:2509.12734v2 Announce Type: replace 
Abstract: In the Admixture Model, the probability that an individual carries a certain allele at a specific marker depends on the allele frequencies in $K$ ancestral populations and the proportion of the individual's genome originating from these populations. The markers are assumed to be independent. The Linkage Model is a Hidden Markov Model (HMM) that extends the Admixture Model by incorporating linkage between neighboring loci.
  We prove consistency and asymptotic normality of maximum likelihood estimators (MLEs) for the ancestry of individuals in the Linkage Model, complementing earlier results by \citep{pfaff2004information, pfaffelhuber2022central, HEINZEL2025} for the Admixture Model. These results are used to prove that a statistical test that allows for model selection between the Admixture Model and the Linkage Model is an asymptotic level-$\alpha$-test. Finally, we demonstrate the practical relevance of our results by applying the test to real-world data from the 1000 Genomes Project.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12734v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carola Sophia Heinzel</dc:creator>
    </item>
    <item>
      <title>Sample complexity for entropic optimal transport with radial cost</title>
      <link>https://arxiv.org/abs/2510.05685</link>
      <description>arXiv:2510.05685v2 Announce Type: replace 
Abstract: We prove a new sample complexity result for entropy regularized optimal transport. Our bound holds for probability measures on $\mathbb R^d$ with exponential tail decay and for radial cost functions that satisfy a local Lipschitz condition. It is sharp up to logarithmic factors, and captures the intrinsic dimension of the marginal distributions through a generalized covering number of their supports. Examples that fit into our framework include subexponential and subgaussian distributions and radial cost functions $c(x,y)=|x-y|^p$ for $p\ge 2.$</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05685v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruiyu Han, Johannes Wiesel</dc:creator>
    </item>
    <item>
      <title>One-Bit Distributed Mean Estimation with Unknown Variance</title>
      <link>https://arxiv.org/abs/2501.18502</link>
      <description>arXiv:2501.18502v3 Announce Type: replace-cross 
Abstract: In this work, we study the problem of distributed mean estimation with $1$-bit communication constraints when the variance is unknown. We focus on the specific case where each user has access to one i.i.d. sample drawn from a distribution that belongs to a scale-location family, and is limited to sending just a single bit of information to a central server whose goal is to estimate the mean. We propose simple non-adaptive and adaptive protocols that are shown to be asymptotically normal. We derive bounds on the asymptotic (in the number of users) Mean Squared Error (MSE) achieved by these protocols. For a class of symmetric log-concave distributions, we derive matching lower bounds for the MSE achieved by adaptive protocols, proving the optimality of our scheme. Furthermore, we develop a lower bound on the MSE for non-adaptive protocols that applies to any symmetric strictly log-concave distribution by means of a refined squared Hellinger distance analysis. Through this, we show that for many common distributions including a subclass of the generalized Gaussian family, the asymptotic minimax MSE achieved by the best non-adaptive protocol is higher than that achieved by our simple adaptive protocol. Our simulation results confirm a positive gap between the adaptive and non-adaptive settings, aligning with the theoretical bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18502v3</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ritesh Kumar, Shashank Vatedka</dc:creator>
    </item>
    <item>
      <title>Analyzing Uncertainty Quantification in Statistical and Deep Learning Models for Probabilistic Electricity Price Forecasting</title>
      <link>https://arxiv.org/abs/2509.19417</link>
      <description>arXiv:2509.19417v3 Announce Type: replace-cross 
Abstract: Precise probabilistic forecasts are fundamental for energy risk management, and there is a wide range of both statistical and machine learning models for this purpose. Inherent to these probabilistic models is some form of uncertainty quantification. However, most models do not capture the full extent of uncertainty, which arises not only from the data itself but also from model and distributional choices. In this study, we examine uncertainty quantification in state-of-the-art statistical and deep learning probabilistic forecasting models for electricity price forecasting in the German market. In particular, we consider deep distributional neural networks (DDNNs) and augment them with an ensemble approach, Monte Carlo (MC) dropout, and conformal prediction to account for model uncertainty. Additionally, we consider the LASSO-estimated autoregressive (LEAR) approach combined with quantile regression averaging (QRA), generalized autoregressive conditional heteroskedasticity (GARCH), and conformal prediction. Across a range of performance metrics, we find that the LEAR-based models perform well in terms of probabilistic forecasting, irrespective of the uncertainty quantification method. Furthermore, we find that DDNNs benefit from incorporating both data and model uncertainty, improving both point and probabilistic forecasting. Uncertainty itself appears to be best captured by the models using conformal prediction. Overall, our extensive study shows that all models under consideration perform competitively. However, their relative performance depends on the choice of metrics for point and probabilistic forecasting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19417v3</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andreas Lebedev, Abhinav Das, Sven Pappert, Stephan Schl\"uter</dc:creator>
    </item>
  </channel>
</rss>
