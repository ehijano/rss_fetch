<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 01 Jan 2026 05:00:50 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Fundamental limits for weighted empirical approximations of tilted distributions</title>
      <link>https://arxiv.org/abs/2512.23979</link>
      <description>arXiv:2512.23979v1 Announce Type: new 
Abstract: Consider the task of generating samples from a tilted distribution of a random vector whose underlying distribution is unknown, but samples from it are available. This finds applications in fields such as finance and climate science, and in rare event simulation. In this article, we discuss the asymptotic efficiency of a self-normalized importance sampler of the tilted distribution. We provide a sharp characterization of its accuracy, given the number of samples and the degree of tilt. Our findings reveal a surprising dichotomy: while the number of samples needed to accurately tilt a bounded random vector increases polynomially in the tilt amount, it increases at a super polynomial rate for unbounded distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23979v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sarvesh Ravichandran Iyer, Himadri Mandal, Dhruman Gupta, Rushil Gupta, Agniv Bandhyopadhyay, Achal Bassamboo, Varun Gupta, Sandeep Juneja</dc:creator>
    </item>
    <item>
      <title>Local Asymptotic Normality for Mixed Fractional Brownian Motion with $0&lt;H&lt;3/4$</title>
      <link>https://arxiv.org/abs/2512.24042</link>
      <description>arXiv:2512.24042v1 Announce Type: new 
Abstract: This paper establishes the Local Asymptotic Normality (LAN) property for the mixed fractional Brownian motion under high-frequency observations with Hurst index $H \in (0, 3/4)$. The simultaneous estimation of the volatility and the Hurst index encounters a degeneracy problem in the Fisher information matrix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24042v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chunhao Cai</dc:creator>
    </item>
    <item>
      <title>A goodness-of-fit test for the Zeta distribution with unknown parameter</title>
      <link>https://arxiv.org/abs/2512.24128</link>
      <description>arXiv:2512.24128v1 Announce Type: new 
Abstract: We introduce a new goodness-of-fit test for count data on $\mathbb{N}$ for the Zeta distribution with unknown parameter. The test is built on a Stein-type characterization that uses, as Stein operator, the infinitesimal generator of a birth-death process whose stationary distribution is Zeta. The resulting $L^2$-type statistic is shown to be omnibus consistent, and we establish the limit null behavior as well as the validity of the associated parametric bootstrap procedure. In a Monte Carlo simulation study, we compare the proposed test with the only existing Zeta-specific procedure of Meintanis (2009), as well as with more general competitors based on empirical distribution functions, kernel Stein discrepancies and other Stein-type characterizations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24128v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bruno Ebner, Daniel Hlubinka</dc:creator>
    </item>
    <item>
      <title>Score-based sampling without diffusions: Guidance from a simple and modular scheme</title>
      <link>https://arxiv.org/abs/2512.24152</link>
      <description>arXiv:2512.24152v1 Announce Type: new 
Abstract: Sampling based on score diffusions has led to striking empirical results, and has attracted considerable attention from various research communities. It depends on availability of (approximate) Stein score functions for various levels of additive noise. We describe and analyze a modular scheme that reduces score-based sampling to solving a short sequence of ``nice'' sampling problems, for which high-accuracy samplers are known. We show how to design forward trajectories such that both (a) the terminal distribution, and (b) each of the backward conditional distribution is defined by a strongly log concave (SLC) distribution. This modular reduction allows us to exploit \emph{any} SLC sampling algorithm in order to traverse the backwards path, and we establish novel guarantees with short proofs for both uni-modal and multi-modal densities. The use of high-accuracy routines yields $\varepsilon$-accurate answers, in either KL or Wasserstein distances, with polynomial dependence on $\log(1/\varepsilon)$ and $\sqrt{d}$ dependence on the dimension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24152v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M. J. Wainwright</dc:creator>
    </item>
    <item>
      <title>Implicit score matching meets denoising score matching: improved rates of convergence and log-density Hessian estimation</title>
      <link>https://arxiv.org/abs/2512.24378</link>
      <description>arXiv:2512.24378v1 Announce Type: new 
Abstract: We study the problem of estimating the score function using both implicit score matching and denoising score matching. Assuming that the data distribution exhibiting a low-dimensional structure, we prove that implicit score matching is able not only to adapt to the intrinsic dimension, but also to achieve the same rates of convergence as denoising score matching in terms of the sample size. Furthermore, we demonstrate that both methods allow us to estimate log-density Hessians without the curse of dimensionality by simple differentiation. This justifies convergence of ODE-based samplers for generative diffusion models. Our approach is based on Gagliardo-Nirenberg-type inequalities relating weighted $L^2$-norms of smooth functions and their derivatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24378v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Konstantin Yakovlev, Anna Markovich, Nikita Puchkin</dc:creator>
    </item>
    <item>
      <title>Dimension-free estimators of gradients of functions with(out) non-independent variables</title>
      <link>https://arxiv.org/abs/2512.24527</link>
      <description>arXiv:2512.24527v1 Announce Type: new 
Abstract: This study proposes a unified stochastic framework for approximating and computing the gradient of every smooth function evaluated at non-independent variables, using $\ell_p$-spherical distributions on $\R^d$ with $d, p\geq 1$. The upper-bounds of the bias of the gradient surrogates do not suffer from the curse of dimensionality for any $p\geq 1$. Also, the mean squared errors (MSEs) of the gradient estimators are bounded by $K_0 N^{-1} d$ for any $p \in [1, 2]$, and by $K_1 N^{-1} d^{2/p}$ when $2 \leq p \ll d$ with $N$ the sample size and $K_0, K_1$ some constants. Taking $\max\left\{2, \log(d) \right\} &lt; p \ll d$ allows for achieving dimension-free upper-bounds of MSEs. In the case where $d\ll p&lt; +\infty$, the upper-bound $K_2 N^{-1} d^{2-2/p}/ (d+2)^2$ is reached with $K_2$ a constant. Such results lead to dimension-free MSEs of the proposed estimators, which boil down to estimators of the traditional gradient when the variables are independent. Numerical comparisons show the efficiency of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24527v1</guid>
      <category>math.ST</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matieyendou Lamboni</dc:creator>
    </item>
    <item>
      <title>Nonparametric Bandits with Single-Index Rewards: Optimality and Adaptivity</title>
      <link>https://arxiv.org/abs/2512.24669</link>
      <description>arXiv:2512.24669v1 Announce Type: new 
Abstract: Contextual bandits are a central framework for sequential decision-making, with applications ranging from recommendation systems to clinical trials. While nonparametric methods can flexibly model complex reward structures, they suffer from the curse of dimensionality. We address this challenge using a single-index model, which projects high-dimensional covariates onto a one-dimensional subspace while preserving nonparametric flexibility.
  We first develop a nonasymptotic theory for offline single-index regression for each arm, combining maximum rank correlation for index estimation with local polynomial regression. Building on this foundation, we propose a single-index bandit algorithm and establish its convergence rate. We further derive a matching lower bound, showing that the algorithm achieves minimax-optimal regret independent of the ambient dimension $d$, thereby overcoming the curse of dimensionality.
  We also establish an impossibility result for adaptation: without additional assumptions, no policy can adapt to unknown smoothness levels. Under a standard self-similarity condition, however, we construct a policy that remains minimax-optimal while automatically adapting to the unknown smoothness. Finally, as the dimension $d$ increases, our algorithm continues to achieve minimax-optimal regret, revealing a phase transition that characterizes the fundamental limits of single-index bandit learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24669v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wanteng Ma, T. Tony Cai</dc:creator>
    </item>
    <item>
      <title>Reformulating Confidence as Extended Likelihood</title>
      <link>https://arxiv.org/abs/2512.24701</link>
      <description>arXiv:2512.24701v1 Announce Type: new 
Abstract: Fisher's fiducial probability has recently received renewed attention under the name confidence. In this paper, we reformulate it within an extended-likelihood framework, a representation that helps to resolve many long-standing controversies. The proposed formulation accommodates multi-dimensional parameters and shows how higher-order approximations can be used to refine standard asymptotic confidence statements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24701v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Youngjo Lee</dc:creator>
    </item>
    <item>
      <title>Approximate Computation via Le Cam Simulability</title>
      <link>https://arxiv.org/abs/2512.24860</link>
      <description>arXiv:2512.24860v1 Announce Type: new 
Abstract: We propose a decision-theoretic framework for computational complexity, complementary to classical theory: moving from syntactic exactness (Turing / Shannon) to semantic simulability (Le Cam). While classical theory classifies problems by the cost of exact solution, modern computation often seeks only decision-valid approximations. We introduce a framework where "computation" is viewed as the efficient simulation of a target statistical experiment within a bounded risk distortion (Le Cam deficiency).
  We formally define computational deficiency ($\delta_{\text{poly}}$) and use it to construct the complexity class LeCam-P (Decision-Robust Polynomial Time), characterizing problems that may be syntactically hard but semantically easy to approximate. We show that classical Karp reductions can be viewed as zero-deficiency simulations, and that approximate reductions correspond to bounded deficiency. Furthermore, we establish the No-Free-Transfer Inequality, showing that strictly invariant representations inevitably destroy decision-relevant information. This framework offers a statistical perspective on approximation theory, bridging the gap between algorithmic complexity and decision theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24860v1</guid>
      <category>math.ST</category>
      <category>cs.CC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Deniz Akdemir</dc:creator>
    </item>
    <item>
      <title>Basic Inequalities for First-Order Optimization with Applications to Statistical Risk Analysis</title>
      <link>https://arxiv.org/abs/2512.24999</link>
      <description>arXiv:2512.24999v1 Announce Type: new 
Abstract: We introduce \textit{basic inequalities} for first-order iterative optimization algorithms, forming a simple and versatile framework that connects implicit and explicit regularization. While related inequalities appear in the literature, we isolate and highlight a specific form and develop it as a well-rounded tool for statistical analysis. Let $f$ denote the objective function to be optimized. Given a first-order iterative algorithm initialized at $\theta_0$ with current iterate $\theta_T$, the basic inequality upper bounds $f(\theta_T)-f(z)$ for any reference point $z$ in terms of the accumulated step sizes and the distances between $\theta_0$, $\theta_T$, and $z$. The bound translates the number of iterations into an effective regularization coefficient in the loss function. We demonstrate this framework through analyses of training dynamics and prediction risk bounds. In addition to revisiting and refining known results on gradient descent, we provide new results for mirror descent with Bregman divergence projection, for generalized linear models trained by gradient descent and exponentiated gradient descent, and for randomized predictors. We illustrate and supplement these theoretical findings with experiments on generalized linear models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24999v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seunghoon Paik, Kangjie Zhou, Matus Telgarsky, Ryan J. Tibshirani</dc:creator>
    </item>
    <item>
      <title>Overflow-Avoiding Memory AMP</title>
      <link>https://arxiv.org/abs/2407.03898</link>
      <description>arXiv:2407.03898v1 Announce Type: cross 
Abstract: Approximate Message Passing (AMP) type algorithms are widely used for signal recovery in high-dimensional noisy linear systems. Recently, a principle called Memory AMP (MAMP) was proposed. Leveraging this principle, the gradient descent MAMP (GD-MAMP) algorithm was designed, inheriting the strengths of AMP and OAMP/VAMP. In this paper, we first provide an overflow-avoiding GD-MAMP (OA-GD-MAMP) to address the overflow problem that arises from some intermediate variables exceeding the range of floating point numbers. Second, we develop a complexity-reduced GD-MAMP (CR-GD-MAMP) to reduce the number of matrix-vector products per iteration by 1/3 (from 3 to 2) with little to no impact on the convergence speed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03898v1</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shunqi Huang, Lei Liu, Brian M. Kurkoski</dc:creator>
    </item>
    <item>
      <title>Marked point processes intensity estimation using sparse group Lasso method applied to locations of lucrative and cooperative banks in mainland France</title>
      <link>https://arxiv.org/abs/2512.23772</link>
      <description>arXiv:2512.23772v1 Announce Type: cross 
Abstract: In this paper, we model the locations of five major banks in mainland France, two lucrative and three cooperative institutions based on socio-economic considerations. Locations of banks are collected using web scrapping and constitute a bivariate spatial point process for which we estimate nonparametrically summary functions (intensity, Ripley and cross-Ripley's K functions). This shows that the pattern is highly inhomogenenous and exhibits a clustering effect especially at small scales, and thus a significant departure to the bivariate (inhomogeneous) Poisson point process is pointed out. We also collect socio-economic datasets (at the living area level) from INSEE and propose a parametric modelling of the intensity function using these covariates. We propose a group-penalized bivariate composite likelihood method to estimate the model parameters, and we establish its asymptotic properties. The application of the methodology to the banking dataset provides new insights into the specificity of the cooperative model within the sector, particularly in relation to the theories of institutional isomorphism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23772v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Am\'elie Artis (PACTE), Achmad Choiruddin (SVH), Jean-Fran\c{c}ois Coeurjolly (SVH), Fr\'ed\'erique Letu\'e (SVH)</dc:creator>
    </item>
    <item>
      <title>Completing and studentising Spearman's correlation in the presence of ties</title>
      <link>https://arxiv.org/abs/2512.23993</link>
      <description>arXiv:2512.23993v1 Announce Type: cross 
Abstract: Non-parametric correlation coefficients have been widely used for analysing arbitrary random variables upon common populations, when requiring an explicit error distribution to be known is an unacceptable assumption. We examine an \(\ell_{2}\) representation of a correlation coefficient (Emond and Mason, 2002) from the perspective of a statistical estimator upon random variables, and verify a number of interesting and highly desirable mathematical properties, mathematically similar to the Whitney embedding of a Hilbert space into the \(\ell_{2}\)-norm space. In particular, we show here that, in comparison to the traditional Spearman (1904) \(\rho\), the proposed Kemeny \(\rho_{\kappa}\) correlation coefficient satisfies Gauss-Markov conditions in the presence or absence of ties, thereby allowing both discrete and continuous marginal random variables. We also prove under standard regularity conditions a number of desirable scenarios, including the construction of a null hypothesis distribution which is Student-t distributed, parallel to standard practice with Pearson's r, but without requiring either continuous random variables nor particular Gaussian errors. Simulations in particular focus upon highly kurtotic data, with highly nominal empirical coverage consistent with theoretical expectation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23993v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Landon Hurley</dc:creator>
    </item>
    <item>
      <title>An exact unbiased semi-parametric maximum quasi-likelihood framework which is complete in the presence of ties</title>
      <link>https://arxiv.org/abs/2512.24009</link>
      <description>arXiv:2512.24009v1 Announce Type: cross 
Abstract: This paper introduces a novel quasi-likelihood extension of the generalised Kendall \(\tau_{a}\) estimator, together with an extension of the Kemeny metric and its associated covariance and correlation forms. The central contribution is to show that the U-statistic structure of the proposed coefficient \(\tau_{\kappa}\) naturally induces a quasi-maximum likelihood estimation (QMLE) framework, yielding consistent Wald and likelihood ratio test statistics. The development builds on the uncentred correlation inner-product (Hilbert space) formulation of Emond and Mason (2002) and resolves the associated sub-Gaussian likelihood optimisation problem under the \(\ell_{2}\)-norm via an Edgeworth expansion of higher-order moments. The Kemeny covariance coefficient \(\tau_{\kappa}\) is derived within a novel likelihood framework for pairwise comparison-continuous random variables, enabling direct inference on population-level correlation between ranked or weakly ordered datasets. Unlike existing approaches that focus on marginal or pairwise summaries, the proposed framework supports sample-observed weak orderings and accommodates ties without information loss. Drawing parallels with Thurstone's Case V latent ordering model, we derive a quasi-likelihood-based tie model with analytic standard errors, generalising classical U-statistics. The framework applies to general continuous and discrete random variables and establishes formal equivalence to Bradley-Terry and Thurstone models, yielding a uniquely identified linear representation with both analytic and likelihood-based estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24009v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Landon Hurley</dc:creator>
    </item>
    <item>
      <title>Random Multiplexing</title>
      <link>https://arxiv.org/abs/2512.24087</link>
      <description>arXiv:2512.24087v1 Announce Type: cross 
Abstract: As wireless communication applications evolve from traditional multipath environments to high-mobility scenarios like unmanned aerial vehicles, multiplexing techniques have advanced accordingly. Traditional single-carrier frequency-domain equalization (SC-FDE) and orthogonal frequency-division multiplexing (OFDM) have given way to emerging orthogonal time-frequency space (OTFS) and affine frequency-division multiplexing (AFDM). These approaches exploit specific channel structures to diagonalize or sparsify the effective channel, thereby enabling low-complexity detection. However, their reliance on these structures significantly limits their robustness in dynamic, real-world environments. To address these challenges, this paper studies a random multiplexing technique that is decoupled from the physical channels, enabling its application to arbitrary norm-bounded and spectrally convergent channel matrices. Random multiplexing achieves statistical fading-channel ergodicity for transmitted signals by constructing an equivalent input-isotropic channel matrix in the random transform domain. It guarantees the asymptotic replica MAP bit-error rate (BER) optimality of AMP-type detectors for linear systems with arbitrary norm-bounded, spectrally convergent channel matrices and signaling configurations, under the unique fixed point assumption. A low-complexity cross-domain memory AMP (CD-MAMP) detector is considered, leveraging the sparsity of the time-domain channel and the randomness of the equivalent channel. Optimal power allocations are derived to minimize the replica MAP BER and maximize the replica constrained capacity of random multiplexing systems. The optimal coding principle and replica constrained-capacity optimality of CD-MAMP detector are investigated for random multiplexing systems. Additionally, the versatility of random multiplexing in diverse wireless applications is explored.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24087v1</guid>
      <category>cs.IT</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lei Liu, Yuhao Chi, Shunqi Huang, Zhaoyang Zhang</dc:creator>
    </item>
    <item>
      <title>Efficient Inference for Inverse Reinforcement Learning and Dynamic Discrete Choice Models</title>
      <link>https://arxiv.org/abs/2512.24407</link>
      <description>arXiv:2512.24407v1 Announce Type: cross 
Abstract: Inverse reinforcement learning (IRL) and dynamic discrete choice (DDC) models explain sequential decision-making by recovering reward functions that rationalize observed behavior. Flexible IRL methods typically rely on machine learning but provide no guarantees for valid inference, while classical DDC approaches impose restrictive parametric specifications and often require repeated dynamic programming. We develop a semiparametric framework for debiased inverse reinforcement learning that yields statistically efficient inference for a broad class of reward-dependent functionals in maximum entropy IRL and Gumbel-shock DDC models. We show that the log-behavior policy acts as a pseudo-reward that point-identifies policy value differences and, under a simple normalization, the reward itself. We then formalize these targets, including policy values under known and counterfactual softmax policies and functionals of the normalized reward, as smooth functionals of the behavior policy and transition kernel, establish pathwise differentiability, and derive their efficient influence functions. Building on this characterization, we construct automatic debiased machine-learning estimators that allow flexible nonparametric estimation of nuisance components while achieving $\sqrt{n}$-consistency, asymptotic normality, and semiparametric efficiency. Our framework extends classical inference for DDC models to nonparametric rewards and modern machine-learning tools, providing a unified and computationally tractable approach to statistical inference in IRL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24407v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lars van der Laan, Aurelien Bibaut, Nathan Kallus</dc:creator>
    </item>
    <item>
      <title>Exact finite mixture representations for species sampling processes</title>
      <link>https://arxiv.org/abs/2512.24414</link>
      <description>arXiv:2512.24414v1 Announce Type: cross 
Abstract: Random probability measures, together with their constructions, representations, and associated algorithms, play a central role in modern Bayesian inference. A key class is that of proper species sampling processes, which offer a relatively simple yet versatile framework that extends naturally to non-exchangeable settings. We revisit this class from a computational perspective and show that they admit exact finite mixture representations. In particular, we prove that any proper species sampling process can be written, at the prior level, as a finite mixture with a latent truncation variable and reweighted atoms, while preserving its distributional features exactly. These finite formulations can be used as drop-in replacements in Bayesian mixture models, recasting posterior computation in terms of familiar finite-mixture machinery. This yields straightforward MCMC implementations and tractable expressions, while avoiding ad hoc truncations and model-specific constructions. The resulting representation preserves the full generality of the original infinite-dimensional priors while enabling practical gains in algorithm design and implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24414v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rams\'es H. Mena, Christos Merkatas, Theodoros Nicoleris, Carlos E. Rodr\'iguez</dc:creator>
    </item>
    <item>
      <title>The non-backtracking transition probability matrix and its usage for node clustering</title>
      <link>https://arxiv.org/abs/2512.24434</link>
      <description>arXiv:2512.24434v1 Announce Type: cross 
Abstract: Relation between the real eigenvalues of the non-backtracking matrix and those of the non-backtracking Laplacian is considered with respect to node clustering. For this purpose we use the real eigenvalues of the transition probability matrix (when the random walk goes through the oriented edges with the rule of ``not going back in the next step'') which have a linear relation to those of the non-backtracking Laplacian of Jost,Mulas. ``Inflation--deflation'' techniques are also developed for clustering the nodes of the non-backtracking graph. With further processing, it leads to the clustering of the nodes of the original graph, which usually comes from a sparse stochastic block model of Bordenave,Decelle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24434v1</guid>
      <category>math.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marianna Bolla</dc:creator>
    </item>
    <item>
      <title>Are First-Order Diffusion Samplers Really Slower? A Fast Forward-Value Approach</title>
      <link>https://arxiv.org/abs/2512.24927</link>
      <description>arXiv:2512.24927v1 Announce Type: cross 
Abstract: Higher-order ODE solvers have become a standard tool for accelerating diffusion probabilistic model (DPM) sampling, motivating the widespread view that first-order methods are inherently slower and that increasing discretization order is the primary path to faster generation. This paper challenges this belief and revisits acceleration from a complementary angle: beyond solver order, the placement of DPM evaluations along the reverse-time dynamics can substantially affect sampling accuracy in the low-neural function evaluation (NFE) regime.
  We propose a novel training-free, first-order sampler whose leading discretization error has the opposite sign to that of DDIM. Algorithmically, the method approximates the forward-value evaluation via a cheap one-step lookahead predictor. We provide theoretical guarantees showing that the resulting sampler provably approximates the ideal forward-value trajectory while retaining first-order convergence. Empirically, across standard image generation benchmarks (CIFAR-10, ImageNet, FFHQ, and LSUN), the proposed sampler consistently improves sample quality under the same NFE budget and can be competitive with, and sometimes outperform, state-of-the-art higher-order samplers. Overall, the results suggest that the placement of DPM evaluations provides an additional and largely independent design angle for accelerating diffusion sampling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24927v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuchen Jiao, Na Li, Changxiao Cai, Gen Li</dc:creator>
    </item>
    <item>
      <title>Modewise Additive Factor Model for Matrix Time Series</title>
      <link>https://arxiv.org/abs/2512.25025</link>
      <description>arXiv:2512.25025v1 Announce Type: cross 
Abstract: We introduce a Modewise Additive Factor Model (MAFM) for matrix-valued time series that captures row-specific and column-specific latent effects through an additive structure, offering greater flexibility than multiplicative frameworks such as Tucker and CP factor models. In MAFM, each observation decomposes into a row-factor component, a column-factor component, and noise, allowing distinct sources of variation along different modes to be modeled separately. We develop a computationally efficient two-stage estimation procedure: Modewise Inner-product Eigendecomposition (MINE) for initialization, followed by Complement-Projected Alternating Subspace Estimation (COMPAS) for iterative refinement. The key methodological innovation is that orthogonal complement projections completely eliminate cross-modal interference when estimating each loading space. We establish convergence rates for the estimated factor loading matrices under proper conditions. We further derive asymptotic distributions for the loading matrix estimators and develop consistent covariance estimators, yielding a data-driven inference framework that enables confidence interval construction and hypothesis testing. As a technical contribution of independent interest, we establish matrix Bernstein inequalities for quadratic forms of dependent matrix time series. Numerical experiments on synthetic and real data demonstrate the advantages of the proposed method over existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.25025v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Elynn Chen, Yuefeng Han, Jiayu Li, Ke Xu</dc:creator>
    </item>
    <item>
      <title>Testing Monotonicity in a Finite Population</title>
      <link>https://arxiv.org/abs/2512.25032</link>
      <description>arXiv:2512.25032v1 Announce Type: cross 
Abstract: We consider the extent to which we can learn from a completely randomized experiment whether everyone has treatment effects that are weakly of the same sign, a condition we call monotonicity. From a classical sampling perspective, it is well-known that monotonicity is untestable. By contrast, we show from the design-based perspective -- in which the units in the population are fixed and only treatment assignment is stochastic -- that the distribution of treatment effects in the finite population (and hence whether monotonicity holds) is formally identified. We argue, however, that the usual definition of identification is unnatural in the design-based setting because it imagines knowing the distribution of outcomes over different treatment assignments for the same units. We thus evaluate the informativeness of the data by the extent to which it enables frequentist testing and Bayesian updating. We show that frequentist tests can have nontrivial power against some alternatives, but power is generically limited. Likewise, we show that there exist (non-degenerate) Bayesian priors that never update about whether monotonicity holds. We conclude that, despite the formal identification result, the ability to learn about monotonicity from data in practice is severely limited.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.25032v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiafeng Chen, Jonathan Roth, Jann Spiess</dc:creator>
    </item>
    <item>
      <title>Compound Estimation for Binomials</title>
      <link>https://arxiv.org/abs/2512.25042</link>
      <description>arXiv:2512.25042v1 Announce Type: cross 
Abstract: Many applications involve estimating the mean of multiple binomial outcomes as a common problem -- assessing intergenerational mobility of census tracts, estimating prevalence of infectious diseases across countries, and measuring click-through rates for different demographic groups. The most standard approach is to report the plain average of each outcome. Despite simplicity, the estimates are noisy when the sample sizes or mean parameters are small. In contrast, the Empirical Bayes (EB) methods are able to boost the average accuracy by borrowing information across tasks. Nevertheless, the EB methods require a Bayesian model where the parameters are sampled from a prior distribution which, unlike the commonly-studied Gaussian case, is unidentified due to discreteness of binomial measurements. Even if the prior distribution is known, the computation is difficult when the sample sizes are heterogeneous as there is no simple joint conjugate prior for the sample size and mean parameter.
  In this paper, we consider the compound decision framework which treats the sample size and mean parameters as fixed quantities. We develop an approximate Stein's Unbiased Risk Estimator (SURE) for the average mean squared error given any class of estimators. For a class of machine learning-assisted linear shrinkage estimators, we establish asymptotic optimality, regret bounds, and valid inference. Unlike existing work, we work with the binomials directly without resorting to Gaussian approximations. This allows us to work with small sample sizes and/or mean parameters in both one-sample and two-sample settings. We demonstrate our approach using three datasets on firm discrimination, education outcomes, and innovation rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.25042v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yan Chen, Lihua Lei</dc:creator>
    </item>
    <item>
      <title>Functional Extreme-PLS</title>
      <link>https://arxiv.org/abs/2410.05517</link>
      <description>arXiv:2410.05517v2 Announce Type: replace 
Abstract: We propose an extreme dimension reduction method extending the Extreme-PLS approach to the case where the covariate lies in a possibly infinite-dimensional Hilbert space. The ideas are partly borrowed from both Partial Least-Squares and Sliced Inverse Regression techniques. As such, the method relies on the projection of the covariate onto a subspace and maximizes the covariance between its projection and the response conditionally to an extreme event driven by a random threshold to capture the tail-information. The covariate and the heavy-tailed response are supposed to be linked through a non-linear inverse single-index model and our goal is to infer the index in this regression framework. We propose a new family of estimators and show its asymptotic consistency with convergence rates under the model. Assuming mild conditions on the noise, most of the assumptions are stated in terms of regular variation unlike the standard literature on SIR and single-index regression. Finally, our results are illustrated on a finite-sample study with synthetic functional data as well as on real data from the financial realm, highlighting the effectiveness of the dimension reduction for estimating extreme risk measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05517v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>St\'ephane Girard, Cambyse Pakzad</dc:creator>
    </item>
    <item>
      <title>A Particle Algorithm for Mean-Field Variational Inference</title>
      <link>https://arxiv.org/abs/2412.20385</link>
      <description>arXiv:2412.20385v4 Announce Type: replace 
Abstract: Variational inference is a fast and scalable alternative to Markov chain Monte Carlo and has been widely applied to posterior inference tasks in statistics and machine learning. A traditional approach for implementing mean-field variational inference (MFVI) is coordinate ascent variational inference (CAVI), which relies crucially on parametric assumptions on complete conditionals. We introduce a novel particle-based algorithm for MFVI, named PArticle VI (PAVI), for nonparametric mean-field approximation. We obtain non-asymptotic error bounds for our algorithm. To our knowledge, this is the first end-to-end guarantee for particle-based MFVI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20385v4</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiang Du, Kaizheng Wang, Edith Zhang, Chenyang Zhong</dc:creator>
    </item>
    <item>
      <title>Sample complexity and weak limits of nonsmooth multimarginal Schr\"{o}dinger system with application to optimal transport barycenter</title>
      <link>https://arxiv.org/abs/2502.02726</link>
      <description>arXiv:2502.02726v2 Announce Type: replace 
Abstract: Multimarginal optimal transport (MOT) has emerged as a useful framework for many applied problems. However, compared to the well-studied classical two-marginal optimal transport theory, analysis of MOT is far more challenging and remains much less developed. In this paper, we study the statistical estimation and inference problems for the entropic MOT (EMOT), whose optimal solution is characterized by the multimarginal Schr\"{o}dinger system. Assuming only boundedness of the cost function, we derive sharp sample complexity for estimating several key quantities pertaining to EMOT (cost functional and Schr\"{o}dinger coupling) from point clouds that are randomly sampled from the input marginal distributions. Moreover, with substantially weaker smoothness assumption on the cost function than the existing literature, we derive distributional limits and bootstrap validity of various key EMOT objects. As an application, we propose the multimarginal Schr\"{o}dinger barycenter as a new and natural way to regularize the exact Wasserstein barycenter and demonstrate its statistical optimality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02726v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pengtao Li, Xiaohui Chen</dc:creator>
    </item>
    <item>
      <title>Consistent line clustering using geometric hypergraphs</title>
      <link>https://arxiv.org/abs/2505.24868</link>
      <description>arXiv:2505.24868v2 Announce Type: replace 
Abstract: Many datasets are naturally modeled as graphs, where vertices denote entities and edges encode pairwise interactions. However, some problems exhibit higher-order structure that lies beyond this framework. Among the simplest examples is line clustering, in which points in a Euclidean space are grouped into clusters well approximated by line segments. As any two points trivially determine a line, the relevant structure emerges only when considering higher-order tuples. To capture this, we construct a 3-uniform hypergraph by treating sets of three points as hyperedges whenever they are approximately collinear. This geometric hypergraph encodes information about the underlying line segments, which can be extracted using community recovery algorithms. We characterize the fundamental limits of line clustering and establish the near-optimality of hypergraph-based methods. In particular, we derive information-theoretic thresholds for exact and almost exact recovery for noisy observations from intersecting lines in the plane. Finally, we introduce a polynomial-time spectral algorithm that succeeds up to polylogarithmic factors of the information-theoretic bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24868v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kalle Alaluusua, Konstantin Avrachenkov, B. R. Vinay Kumar, Lasse Leskel\"a</dc:creator>
    </item>
    <item>
      <title>Robustified Gaussian quasi-likelihood inference for volatility</title>
      <link>https://arxiv.org/abs/2510.02666</link>
      <description>arXiv:2510.02666v3 Announce Type: replace 
Abstract: We consider statistical inference for a class of continuous semimartingale regression models based on high-frequency observations subject to contamination by finite-activity jumps and spike noise. By employing density-power weighting and H\"{o}lder-inequality-based normalization, we propose easy-to-implement, robustified versions of the conventional Gaussian quasi-maximum-likelihood estimator that require only a single tuning parameter. We prove their asymptotic mixed normality at the standard rate of $\sqrt{n}$. It is theoretically shown that these estimators are simultaneously robust against contamination in both the covariate and response processes. Additionally, under suitable conditions on the selection of the tuning parameter, the proposed estimators achieve the same asymptotic distribution as the conventional estimator in the contamination-free case. Illustrative simulation results highlight the estimators' insensitivity to the choice of the tuning parameter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02666v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shoichi Eguchi, Hiroki Masuda</dc:creator>
    </item>
    <item>
      <title>A Sieve M-Estimator for Entropic Optimal Transport</title>
      <link>https://arxiv.org/abs/2512.21981</link>
      <description>arXiv:2512.21981v2 Announce Type: replace 
Abstract: The entropically regularized optimal transport problem between probability measures on compact Euclidean subsets can be represented as an information projection with moment inequality constraints. This allows its Fenchel dual to be approximated by a sequence of convex, finite-dimensional problems using sieve methods, enabling tractable estimation of the primal value and dual optimizers from samples. Assuming only continuity of the cost function, I establish almost sure consistency of these estimators. I derive a finite-sample convergence rate for the primal value estimator, showing logarithmic dependence on sieve complexity, and quantify uncertainty for the dual optimal value estimator via matching stochastic bounds involving suprema of centered Gaussian processes. These results provide the first statistical guarantees for sieve-based estimators of entropic optimal transport, extending beyond the empirical Sinkhorn approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21981v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rami V. Tabri</dc:creator>
    </item>
    <item>
      <title>Studentising Kendall's Tau: U-Statistic Estimators and Bias Correction for a Generalised Rank Variance-Covariance framework</title>
      <link>https://arxiv.org/abs/2307.10973</link>
      <description>arXiv:2307.10973v2 Announce Type: replace-cross 
Abstract: Kemeny (1959) introduced a topologically complete metric space to study ordinal random variables, particularly in the context of Condorcet's paradox and the measurability of ties. Building on this, Emond &amp; Mason (2002) reformulated Kemeny's framework into a rank correlation coefficient by embedding the metric space into a Hilbert structure. This transformation enables the analysis of data under weak order-preserving transformations (monotonically non-decreasing) within a linear probabilistic framework. However, the statistical properties of this rank correlation estimator, such as bias, estimation variance, and Type I error rates, have not been thoroughly evaluated.
  In this paper, we derive and prove a complete U-statistic estimator in the presence of ties for Kemeny's \(\tau_{\kappa}\), addressing the positive bias introduced by tied ranks. We also introduce a consistent population standard error estimator. The null distribution of the test statistic is shown to follow a \(t_{(N-2)}\)-distribution. Simulation results demonstrate that the proposed method outperforms Kendall's \(\tau_{b}\), offering a more accurate and robust measure of ordinal association which is topologically complete upon standard linear models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.10973v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Landon Hurley</dc:creator>
    </item>
    <item>
      <title>The Population Resemblance Statistic: A Chi-Square Measure of Fit for Banking</title>
      <link>https://arxiv.org/abs/2307.11878</link>
      <description>arXiv:2307.11878v4 Announce Type: replace-cross 
Abstract: The Population Stability Index (PSI) is a widely used measure in credit risk modeling and monitoring within the banking industry. Its purpose is to monitor for changes in the population underlying a model, such as a scorecard, to ensure that the current population closely resembles the one used during model development. If substantial differences between populations are detected, model reconstruction may be necessary. Despite its widespread use, the origins and properties of the PSI are not well documented. Previous literature has suggested using arbitrary constants as a rule-of-thumb to assess resemblance (or "stability"), regardless of sample size. However, this approach too often calls for model reconstruction in small sample sizes while not detecting the need often enough in large sample sizes.
  This paper introduces an alternative discrepancy measure, the Population Resemblance statistic (PRS), based on the Pearson chi-square statistic. Properties of the PRS follow from the non-central chi-square distribution. Specifically, the PRS allows for critical values that are configured according to sample size and the number of risk categories. Implementation relies on the specification of a set of parameters, enabling practitioners to calibrate the procedure with their risk tolerance and sensitivity to population shifts. The PRS is demonstrated to be universally competent in a simulation study and with real-world examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.11878v4</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nelis Potgieter, Corli van Zyl, WD Schutte, Fred Lombard</dc:creator>
    </item>
    <item>
      <title>Are Ensembles Getting Better all the Time?</title>
      <link>https://arxiv.org/abs/2311.17885</link>
      <description>arXiv:2311.17885v3 Announce Type: replace-cross 
Abstract: Ensemble methods combine the predictions of several base models. We study whether or not including more models always improves their average performance. This question depends on the kind of ensemble considered, as well as the predictive metric chosen. We focus on situations where all members of the ensemble are a priori expected to perform equally well, which is the case of several popular methods such as random forests or deep ensembles. In this setting, we show that ensembles are getting better all the time if, and only if, the considered loss function is convex. More precisely, in that case, the loss of the ensemble is a decreasing function of the number of models. When the loss function is nonconvex, we show a series of results that can be summarised as: ensembles of good models keep getting better, and ensembles of bad models keep getting worse. To this end, we prove a new result on the monotonicity of tail probabilities that may be of independent interest. We illustrate our results on a medical problem (diagnosing melanomas using neural nets) and a "wisdom of crowds" experiment (guessing the ratings of upcoming movies).</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.17885v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Journal of Machine Learning Research, vol. 26 (201), 1-46, 2025</arxiv:journal_reference>
      <dc:creator>Pierre-Alexandre Mattei, Damien Garreau</dc:creator>
    </item>
    <item>
      <title>New affine invariant ensemble samplers and their dimensional scaling</title>
      <link>https://arxiv.org/abs/2505.02987</link>
      <description>arXiv:2505.02987v3 Announce Type: replace-cross 
Abstract: We introduce new affine invariant ensemble Markov chain Monte Carlo (MCMC) samplers that are easy to construct and improve upon existing methods, especially for high-dimensional problems. We first propose a simple derivative-free side move sampler that improves upon popular samplers in the \texttt{emcee} package by generating more effective proposal directions. We then develop a class of derivative-based affine invariant ensemble Hamiltonian Monte Carlo (HMC) samplers based on antisymmetric preconditioning using complementary ensembles, which outperform standard, non-affine-invariant HMC when sampling highly anisotropic distributions. We provide asymptotic scaling analysis for high-dimensional Gaussian targets to further elucidate the properties of these affine invariant ensemble samplers. In particular, with derivative information, the affine invariant ensemble HMC can scale much better with dimension compared to derivative-free ensemble samplers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02987v3</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifan Chen</dc:creator>
    </item>
  </channel>
</rss>
