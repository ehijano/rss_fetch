<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 21 Mar 2025 01:54:53 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The broken sample problem revisited: Proof of a conjecture by Bai-Hsing and high-dimensional extensions</title>
      <link>https://arxiv.org/abs/2503.14619</link>
      <description>arXiv:2503.14619v1 Announce Type: new 
Abstract: We revisit the classical broken sample problem: Two samples of i.i.d. data points $\mathbf{X}=\{X_1,\cdots, X_n\}$ and $\mathbf{Y}=\{Y_1,\cdots,Y_m\}$ are observed without correspondence with $m\leq n$. Under the null hypothesis, $\mathbf{X}$ and $\mathbf{Y}$ are independent. Under the alternative hypothesis, $\mathbf{Y}$ is correlated with a random subsample of $\mathbf{X}$, in the sense that $(X_{\pi(i)},Y_i)$'s are drawn independently from some bivariate distribution for some latent injection $\pi:[m] \to [n]$. Originally introduced by DeGroot, Feder, and Goel (1971) to model matching records in census data, this problem has recently gained renewed interest due to its applications in data de-anonymization, data integration, and target tracking. Despite extensive research over the past decades, determining the precise detection threshold has remained an open problem even for equal sample sizes ($m=n$). Assuming $m$ and $n$ grow proportionally, we show that the sharp threshold is given by a spectral and an $L_2$ condition of the likelihood ratio operator, resolving a conjecture of Bai and Hsing (2005) in the positive. These results are extended to high dimensions and settle the sharp detection thresholds for Gaussian and Bernoulli models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14619v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simiao Jiao, Yihong Wu, Jiaming Xu</dc:creator>
    </item>
    <item>
      <title>On the Precise Asymptotics of Universal Inference</title>
      <link>https://arxiv.org/abs/2503.14717</link>
      <description>arXiv:2503.14717v1 Announce Type: new 
Abstract: In statistical inference, confidence set procedures are typically evaluated based on their validity and width properties. Even when procedures achieve rate-optimal widths, confidence sets can still be excessively wide in practice due to elusive constants, leading to extreme conservativeness, where the empirical coverage probability of nominal $1-\alpha$ level confidence sets approaches one. This manuscript studies this gap between validity and conservativeness, using universal inference (Wasserman et al., 2020) with a regular parametric model under model misspecification as a running example. We identify the source of asymptotic conservativeness and propose a general remedy based on studentization and bias correction. The resulting method attains exact asymptotic coverage at the nominal $1-\alpha$ level, even under model misspecification, provided that the product of the estimation errors of two unknowns is negligible, exhibiting an intriguing resemblance to double robustness in semiparametric theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14717v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kenta Takatsu</dc:creator>
    </item>
    <item>
      <title>Hazard Rate for Associated Data in Deconvolution Problems: Asymptotic Normality</title>
      <link>https://arxiv.org/abs/2503.14759</link>
      <description>arXiv:2503.14759v1 Announce Type: new 
Abstract: In reliability theory and survival analysis, observed data are often weakly dependent and subject to additive measurement errors. Such contamination arises when the underlying data are neither independent nor strongly mixed but instead exhibit association. This paper focuses on estimating the hazard rate by deconvolving the density function and constructing an estimator of the distribution function. We assume that the data originate from a strictly stationary sequence satisfying association conditions. Under appropriate smoothness assumptions on the error distribution, we establish the quadratic-mean convergence and asymptotic normality of the proposed estimators. The finite-sample performance of both the hazard rate and distribution function estimators is evaluated through a simulation study. We conclude with a discussion of open problems and potential future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14759v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjrada Mohammed Essalih</dc:creator>
    </item>
    <item>
      <title>The Field Equations of Penalized non-Parametric Regression</title>
      <link>https://arxiv.org/abs/2503.14763</link>
      <description>arXiv:2503.14763v1 Announce Type: new 
Abstract: We view penalized risks through the lens of the calculus of variations. We consider risks comprised of a fitness-term (e.g. MSE) and a gradient-based penalty. After establishing the Euler-Lagrange field equations as a systematic approach to finding minimizers of risks involving only first derivatives, we proceed to exemplify this approach to the MSE penalized by the integral over the squared l2-norm of the gradient of the regression function. The minimizer of this risk is given as the solution to a second order inhomogeneous PDE, where the inhomogeneity is given as the conditional expectation of the target variable conditioned on the features. We discuss properties of the field equations and practical implications thereof, which also apply to the classical Ridge penalty for linear models, and embed our findings into the existing literature. In particular, we find that we can recover the Rudin-Osher-Fatemi model for image-denoising, if we consider the features as deterministic and evenly distributed. Last, we outline several directions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14763v1</guid>
      <category>math.ST</category>
      <category>math.AP</category>
      <category>math.FA</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sven Pappert</dc:creator>
    </item>
    <item>
      <title>Inferring diffusivity from killed diffusion</title>
      <link>https://arxiv.org/abs/2503.14978</link>
      <description>arXiv:2503.14978v1 Announce Type: new 
Abstract: We consider diffusion of independent molecules in an insulated Euclidean domain with unknown diffusivity parameter. At a random time and position, the molecules may bind and stop diffusing in dependence of a given `binding potential'. The binding process can be modeled by an additive random functional corresponding to the canonical construction of a `killed' diffusion Markov process. We study the problem of conducting inference on the infinite-dimensional diffusion parameter from a histogram plot of the `killing' positions of the process. We show first that these positions follow a Poisson point process whose intensity measure is determined by the solution of a certain Schr\"odinger equation. The inference problem can then be re-cast as a non-linear inverse problem for this PDE, which we show to be consistently solvable in a Bayesian way under natural conditions on the initial state of the diffusion, provided the binding potential is not too `aggressive'. In the course of our proofs we obtain novel posterior contraction rate results for high-dimensional Poisson count data that are of independent interest. A numerical illustration of the algorithm by standard MCMC methods is also provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14978v1</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Richard Nickl, Fanny Seizilles</dc:creator>
    </item>
    <item>
      <title>A Note on Local Linear Regression for Time Series in Banach Spaces</title>
      <link>https://arxiv.org/abs/2503.15039</link>
      <description>arXiv:2503.15039v1 Announce Type: new 
Abstract: This work extends local linear regression to Banach space-valued time series for estimating smoothly varying means and their derivatives in non-stationary data. The asymptotic properties of both the standard and bias-reduced Jackknife estimators are analyzed under mild moment conditions, establishing their convergence rates. Simulation studies assess the finite sample performance of these estimators and compare them with the Nadaraya-Watson estimator. Additionally, the proposed methods are applied to smooth EEG recordings for reconstructing eye movements and to video analysis for detecting pedestrians and abandoned objects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15039v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian Heinrichs</dc:creator>
    </item>
    <item>
      <title>Asymptotic Normality in LAD Polynomial Regression and Hilbert Matrices</title>
      <link>https://arxiv.org/abs/2503.15041</link>
      <description>arXiv:2503.15041v1 Announce Type: new 
Abstract: This paper investigates the asymptotic properties of least absolute deviation (LAD) regression for linear models with polynomial regressors, highlighting its robustness against heavy-tailed noise and outliers. Assuming independent and identically distributed (i.i.d.) errors, we establish the multiscale asymptotic normality of LAD estimators. A central result is the derivation of the asymptotic precision matrix, shown to be proportional to Hilbert matrices, with the proportionality coefficient depending on the asymptotic variance of the sample median of the noise distribution. We further explore the estimator's convergence properties, both in probability and almost surely, under varying model specifications. Through comprehensive simulations, we evaluate the speed of convergence of the LAD estimator and the empirical coverage probabilities of confidence intervals constructed under different scaling factors (T 1/2 and T $\alpha$ ). These experiments incorporate a range of noise distributions, including Laplace, Gaussian, and Cauchy, to demonstrate the estimator's robustness and efficiency. The findings underscore the versatility and practical relevance of LAD regression in handling non-standard data environments. By connecting the statistical properties of LAD estimators to classical mathematical structures, such as Hilbert matrices, this study offers both theoretical insights and practical tools for robust statistical modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15041v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sa\"id Maanan (LPP), Azzouz Dermoune (LPP), Ahmed El Ghini</dc:creator>
    </item>
    <item>
      <title>Finite sample expansions for semiparametric plug-in estimation and inference for BTL model</title>
      <link>https://arxiv.org/abs/2503.15045</link>
      <description>arXiv:2503.15045v1 Announce Type: new 
Abstract: The recent paper \cite{GSZ2023} on estimation and inference for top-ranking problem in Bradley-Terry-Lice (BTL) model presented a surprising result: componentwise estimation and inference can be done under much weaker conditions on the number of comparison then it is required for the full dimensional estimation. The present paper revisits this finding from completely different viewpoint. Namely, we show how a theoretical study of estimation in sup-norm can be reduced to the analysis of plug-in semiparametric estimation. For the latter, we adopt and extend the general approach from \cite{Sp2024} for high-dimensional estimation. The main tool of the analysis is a theory of perturbed marginal optimization when an objective function depends on a low-dimensional target parameter along with a high-dimensional nuisance parameter. A particular focus of the study is the critical dimension condition. Full-dimensional estimation requires in general the condition \( \mathbbmsl{N} \gg \mathbb{p} \) between the effective parameter dimension \( \mathbb{p} \) and the effective sample size \( \mathbbmsl{N} \) corresponding to the smallest eigenvalue of the Fisher information matrix \( \mathbbmsl{F} \). Inference on the estimated parameter is even more demanding: the condition \( \mathbbmsl{N} \gg \mathbb{p}^{2} \) cannot be generally avoided; see \cite{Sp2024}. However, for the sup-norm estimation, the critical dimension condition can be reduced to \( \mathbbmsl{N} \geq \CONST \log(\dimp) \). Compared to \cite{GSZ2023}, the proposed approach works for the classical MLE and does not require any resampling procedure, applies to more general structure of the comparison graph, and yields more accurate expansions for each component of the parameter vector.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15045v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vladimir Spokoiny</dc:creator>
    </item>
    <item>
      <title>Optimal Data Splitting for Holdout Cross-Validation in Large Covariance Matrix Estimation</title>
      <link>https://arxiv.org/abs/2503.15186</link>
      <description>arXiv:2503.15186v1 Announce Type: new 
Abstract: Cross-validation is a statistical tool that can be used to improve large covariance matrix estimation. Although its efficiency is observed in practical applications, the theoretical reasons behind it remain largely intuitive, with formal proofs currently lacking. To carry on analytical analysis, we focus on the holdout method, a single iteration of cross-validation, rather than the traditional $k$-fold approach. We derive a closed-form expression for the estimation error when the population matrix follows a white inverse Wishart distribution, and we observe the optimal train-test split scales as the square root of the matrix dimension. For general population matrices, we connected the error to the variance of eigenvalues distribution, but approximations are necessary. Interestingly, in the high-dimensional asymptotic regime, both the holdout and $k$-fold cross-validation methods converge to the optimal estimator when the train-test ratio scales with the square root of the matrix dimension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15186v1</guid>
      <category>math.ST</category>
      <category>q-fin.PM</category>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lamia Lamrani, Christian Bongiorno, Marc Potters</dc:creator>
    </item>
    <item>
      <title>Testing Conditional Stochastic Dominance at Target Points</title>
      <link>https://arxiv.org/abs/2503.14747</link>
      <description>arXiv:2503.14747v1 Announce Type: cross 
Abstract: This paper introduces a novel test for conditional stochastic dominance (CSD) at specific values of the conditioning covariates, referred to as target points. The test is relevant for analyzing income inequality, evaluating treatment effects, and studying discrimination. We propose a Kolmogorov-Smirnov-type test statistic that utilizes induced order statistics from independent samples. Notably, the test features a data-independent critical value, eliminating the need for resampling techniques such as the bootstrap. Our approach avoids kernel smoothing and parametric assumptions, instead relying on a tuning parameter to select relevant observations. We establish the asymptotic properties of our test, showing that the induced order statistics converge to independent draws from the true conditional distributions and that the test controls asymptotic size under weak regularity conditions. While our results apply to both continuous and discrete data, in the discrete case, the critical value only provides a valid upper bound. To address this, we propose a refined critical value that significantly enhances power, requiring only knowledge of the support size of the distributions. Additionally, we analyze the test's behavior in the limit experiment, demonstrating that it reduces to a problem analogous to testing unconditional stochastic dominance in finite samples. This framework allows us to prove the validity of permutation-based tests for stochastic dominance when the random variables are continuous. Monte Carlo simulations confirm the strong finite-sample performance of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14747v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Federico A. Bugni, Ivan A. Canay, Deborah Kim</dc:creator>
    </item>
    <item>
      <title>A Bivariate Poisson-Gamma Distribution: Statistical Properties and Practical Applications</title>
      <link>https://arxiv.org/abs/2503.15062</link>
      <description>arXiv:2503.15062v1 Announce Type: cross 
Abstract: Although the specification of bivariate probability models using a collection of assumed conditional distributions is not a novel concept, it has received considerable attention in the last decade. In this study, a bivariate distribution-the bivariate Poisson-Gamma conditional distribution-is introduced, combining both univariate continuous and discrete distributions. This work explores aspects of this model's structure and statistical inference that have not been studied before. This paper contributes to the field of statistical modeling and distribution theory through the use of maximum likelihood estimation, along with simulations and analyses of real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15062v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Indranil Ghosh, Mina Norouzirad, Filipe J. Marques</dc:creator>
    </item>
    <item>
      <title>Nonlinear Bayesian Update via Ensemble Kernel Regression with Clustering and Subsampling</title>
      <link>https://arxiv.org/abs/2503.15160</link>
      <description>arXiv:2503.15160v1 Announce Type: cross 
Abstract: Nonlinear Bayesian update for a prior ensemble is proposed to extend traditional ensemble Kalman filtering to settings characterized by non-Gaussian priors and nonlinear measurement operators. In this framework, the observed component is first denoised via a standard Kalman update, while the unobserved component is estimated using a nonlinear regression approach based on kernel density estimation. The method incorporates a subsampling strategy to ensure stability and, when necessary, employs unsupervised clustering to refine the conditional estimate. Numerical experiments on Lorenz systems and a PDE-constrained inverse problem illustrate that the proposed nonlinear update can reduce estimation errors compared to standard linear updates, especially in highly nonlinear scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15160v1</guid>
      <category>stat.ML</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yoonsang Lee</dc:creator>
    </item>
    <item>
      <title>Bernstein-type Inequalities and Nonparametric Estimation under Near-Epoch Dependence</title>
      <link>https://arxiv.org/abs/2208.11433</link>
      <description>arXiv:2208.11433v4 Announce Type: replace 
Abstract: The major contributions of this paper lie in two aspects. Firstly, we focus on deriving Bernstein-type inequalities for both geometric and algebraic irregularly-spaced NED random fields, which contain time series as special case. Furthermore, by introducing the idea of "effective dimension" to the index set of random field, our results reflect that the sharpness of inequalities are only associated with this "effective dimension". Up to the best of our knowledge, our paper may be the first one reflecting this phenomenon. Hence, the first contribution of this paper can be more or less regarded as an update of the pioneering work from \citeA{xu2018sieve}. Additionally, as a corollary of our first contribution, a Bernstein-type inequality for geometric irregularly-spaced $\alpha$-mixing random fields is also obtained. The second aspect of our contributions is that, based on the inequalities mentioned above, we show the $L_{\infty}$ convergence rate of the many interesting kernel-based nonparametric estimators. To do this, two deviation inequalities for the supreme of empirical process are derived under NED and $\alpha$-mixing conditions respectively. Then, for irregularly-spaced NED random fields, we prove the attainability of optimal rate for local linear estimator of nonparametric regression, which refreshes another pioneering work on this topic, \citeA{jenish2012nonparametric}. Subsequently, we analyze the uniform convergence rate of uni-modal regression under the same NED conditions as well. Furthermore, by following the guide of \citeA{rigollet2009optimal}, we also prove that the kernel-based plug-in density level set estimator could be optimal up to a logarithm factor. Meanwhile, when the data is collected from $\alpha$-mixing random fields, we also derive the uniform convergence rate of a simple local polynomial density estimator \cite{cattaneo2020simple}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.11433v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zihao Yuan, Martin Spindler</dc:creator>
    </item>
    <item>
      <title>Covariance Operator Estimation via Adaptive Thresholding</title>
      <link>https://arxiv.org/abs/2405.18562</link>
      <description>arXiv:2405.18562v2 Announce Type: replace 
Abstract: This paper studies sparse covariance operator estimation for nonstationary processes with sharply varying marginal variance and small correlation lengthscale. We introduce a covariance operator estimator that adaptively thresholds the sample covariance function using an estimate of the variance component. Building on recent results from empirical process theory, we derive an operator norm bound on the estimation error in terms of the sparsity level of the covariance and the expected supremum of a normalized process. Our theory and numerical simulations demonstrate the advantage of adaptive threshold estimators over universal threshold and sample covariance estimators in nonstationary settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18562v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Omar Al-Ghattas, Daniel Sanz-Alonso</dc:creator>
    </item>
    <item>
      <title>Uniform Convergence Rate of the Nonparametric Estimator for Integrated Diffusion Processes</title>
      <link>https://arxiv.org/abs/2410.05822</link>
      <description>arXiv:2410.05822v2 Announce Type: replace 
Abstract: The nonparametric estimation of integrated diffusion processes has been extensively studied, with most existing research focusing on pointwise convergence. This paper is the first to establish uniform convergence rates for the Nadaraya-Watson estimators of their coefficients. We derive these rates over unbounded support under the assumptions of a vanishing observation interval and a long time horizon. Our findings serve as essential tools for specification testing and semiparametric inference in various diffusion models and time series, facilitating applications in finance, geology, and physics through nonparametric estimation methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05822v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shaolin Ji, Linlin Zhu</dc:creator>
    </item>
    <item>
      <title>Rapid Bayesian Computation and Estimation for Neural Networks via Log-Concave Coupling</title>
      <link>https://arxiv.org/abs/2411.17667</link>
      <description>arXiv:2411.17667v3 Announce Type: replace 
Abstract: This paper studies a Bayesian estimation procedure for single-hidden-layer neural networks using $\ell_{1}$ controlled weights. We study the structure of the posterior density and provide a representation that makes it amenable to rapid sampling via Markov Chain Monte Carlo (MCMC), and to statistical risk guarantees. The neural network has $K$ neurons, internal weight dimension $d$, and fix the outer weights. Thus, $Kd$ parameters overall. With $N$ data observations, use a gain parameter of $\beta$ in the posterior density.
  The posterior is multimodal and not naturally suited to rapid mixing of direct MCMC algorithms. For a continuous uniform prior on the $\ell_{1}$ ball, we show that the posterior density can be written as a mixture density with suitably defined auxiliary random variables, where the mixture components are log-concave. Furthermore, when the number of model parameters $Kd$ is large enough that $Kd \geq C(\beta N)^{2}$, the mixing distribution of the auxiliary random variables is also log-concave. Thus, neuron parameters can be sampled from the posterior by only sampling log-concave densities. The authors refer to the mixture density as a log-concave coupling.
  For a discrete uniform prior restricted to a grid, we study the statistical risk (generalization error) of procedures based on the posterior. Using a gain of $\beta = C [(\log d)/N]^{1/4}$, we demonstrate squared error is on the order $O([(\log d)/N]^{1/4})$. Using independent Gaussian data with a variance $\sigma^{2} $ that matches the inverse gain, $\beta = 1/\sigma^{2}$, we show that the expected Kullback divergence has a cube root power $O([(\log d)/N]^{1/3})$.
  Future work aims to bridge the sampling ability of the continuous uniform prior with the risk control of the discrete uniform prior, resulting in a polynomial time Bayesian training algorithm for neural networks with statistical risk control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17667v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Curtis McDonald, Andrew R. Barron</dc:creator>
    </item>
    <item>
      <title>A Unified Framework for Semiparametrically Efficient Semi-Supervised Learning</title>
      <link>https://arxiv.org/abs/2502.17741</link>
      <description>arXiv:2502.17741v2 Announce Type: replace 
Abstract: We consider statistical inference under a semi-supervised setting where we have access to both a labeled dataset consisting of pairs $\{X_i, Y_i \}_{i=1}^n$ and an unlabeled dataset $\{ X_i \}_{i=n+1}^{n+N}$. We ask the question: under what circumstances, and by how much, can incorporating the unlabeled dataset improve upon inference using the labeled data? To answer this question, we investigate semi-supervised learning through the lens of semiparametric efficiency theory. We characterize the efficiency lower bound under the semi-supervised setting for an arbitrary inferential problem, and show that incorporating unlabeled data can potentially improve efficiency if the parameter is not well-specified. We then propose two types of semi-supervised estimators: a safe estimator that imposes minimal assumptions, is simple to compute, and is guaranteed to be at least as efficient as the initial supervised estimator; and an efficient estimator, which -- under stronger assumptions -- achieves the semiparametric efficiency bound. Our findings unify existing semiparametric efficiency results for particular special cases, and extend these results to a much more general class of problems. Moreover, we show that our estimators can flexibly incorporate predicted outcomes arising from ``black-box" machine learning models, and thereby achieve the same goal as prediction-powered inference (PPI), but with superior theoretical guarantees. We also provide a complete understanding of the theoretical basis for the existing set of PPI methods. Finally, we apply the theoretical framework developed to derive and analyze efficient semi-supervised estimators in a number of settings, including M-estimation, U-statistics, and average treatment effect estimation, and demonstrate the performance of the proposed estimators via simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17741v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zichun Xu, Daniela Witten, Ali Shojaie</dc:creator>
    </item>
    <item>
      <title>A Framework for Statistical Inference via Randomized Algorithms</title>
      <link>https://arxiv.org/abs/2307.11255</link>
      <description>arXiv:2307.11255v4 Announce Type: replace-cross 
Abstract: Randomized algorithms, such as randomized sketching or stochastic optimization, are a promising approach to ease the computational burden in analyzing large datasets. However, randomized algorithms also produce non-deterministic outputs, leading to the problem of evaluating their accuracy. In this paper, we develop a statistical inference framework for quantifying the uncertainty of the outputs of randomized algorithms.
  Our key conclusion is that one can perform statistical inference for the target of a sequence of randomized algorithms as long as in the limit, their outputs fluctuate around the target according to any (possibly unknown) probability distribution. In this setting, we develop appropriate statistical inference methods -- sub-randomization, multi-run plug-in and multi-run aggregation -- by estimating the unknown parameters of the limiting distribution either using multiple runs of the randomized algorithm, or by tailored estimates.
  As illustrations, we develop methods for statistical inference when using stochastic optimization (such as Polyak-Ruppert averaging in stochastic gradient descent and stochastic optimization with momentum). We also illustrate our methods in inference for least squares parameters via randomized sketching, by characterizing the limiting distributions of sketching estimates in a possibly growing dimensional case. We also characterize the computation and communication cost of our methods, showing that in certain cases, they add negligible overhead. The results are supported via a broad range of simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.11255v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhixiang Zhang, Sokbae Lee, Edgar Dobriban</dc:creator>
    </item>
    <item>
      <title>Sparse Hanson-Wright Inequalities with Applications</title>
      <link>https://arxiv.org/abs/2410.15652</link>
      <description>arXiv:2410.15652v3 Announce Type: replace-cross 
Abstract: We derive new Hanson-Wright-type inequalities tailored to the quadratic forms of random vectors with sparse independent components. Specifically, we consider cases where the components of the random vector are sparse $\alpha$-subexponential random variables with $\alpha&gt;0$. When $\alpha=\infty$, these inequalities can be seen as quadratic generalizations of the classical Bernstein and Bennett inequalities for sparse bounded random vectors. To establish this quadratic generalization, we also develop new Bersntein-type and Bennett-type inequalities for linear forms of sparse $\alpha$-subexponential random variables that go beyond the bounded case $(\alpha=\infty)$. Our proof relies on a novel combinatorial method for estimating the moments of both random linear forms and quadratic forms.
  We present two key applications of these new sparse Hanson-Wright inequalities: (1) A local law and complete eigenvector delocalization for sparse $\alpha$-subexponential Hermitian random matrices, generalizing the result of He et al. (2019) beyond sparse Bernoulli random matrices. To the best of our knowledge, this is the first local law and complete delocalization result for sparse $\alpha$-subexponeitial random matrices down to the near-optimal sparsity $p\geq \frac{\mathrm{polylog}(n)}{n}$ when $\alpha\in (0,2)$ as well as for unbounded sparse sub-gaussian random matrices down to the optimal sparsity $p\gtrsim \frac{\log n}{n}.$ (2) Concentration of the Euclidean norm for the linear transformation of a sparse $\alpha$-subexponential random vector, improving on the results of G\"otze et al. (2021) for sparse sub-exponential random vectors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15652v3</guid>
      <category>math.PR</category>
      <category>math.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiyun He, Ke Wang, Yizhe Zhu</dc:creator>
    </item>
    <item>
      <title>Information-theoretic limits and approximate message-passing for high-dimensional time series</title>
      <link>https://arxiv.org/abs/2501.13625</link>
      <description>arXiv:2501.13625v2 Announce Type: replace-cross 
Abstract: High-dimensional time series appear in many scientific setups, demanding a nuanced approach to model and analyze the underlying dependence structure. Theoretical advancements so far often rely on stringent assumptions regarding the sparsity of the underlying signal. In non-sparse regimes, analyses have primarily focused on linear regression models with the design matrix having independent rows. In this paper, we expand the scope by investigating a high-dimensional time series model wherein the number of features grows proportionally to the number of sampling points, without assuming sparsity in the signal. Specifically, we consider the stochastic regression model and derive a single-letter formula for the normalized mutual information between observations and the signal, as well as for minimum mean-square errors. We also empirically study the vector approximate message passing VAMP algorithm and show that, despite the lack of theoretical guarantees, its performance for inference in our time series model is robust and often statistically optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13625v2</guid>
      <category>cs.IT</category>
      <category>cond-mat.dis-nn</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daria Tieplova, Samriddha Lahiry, Jean Barbier</dc:creator>
    </item>
  </channel>
</rss>
