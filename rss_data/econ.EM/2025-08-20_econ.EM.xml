<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 21 Aug 2025 01:30:51 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Nonparametric Approach to Augmenting a Bayesian VAR with Nonlinear Factors</title>
      <link>https://arxiv.org/abs/2508.13972</link>
      <description>arXiv:2508.13972v1 Announce Type: new 
Abstract: This paper proposes a Vector Autoregression augmented with nonlinear factors that are modeled nonparametrically using regression trees. There are four main advantages of our model. First, modeling potential nonlinearities nonparametrically lessens the risk of mis-specification. Second, the use of factor methods ensures that departures from linearity are modeled parsimoniously. In particular, they exhibit functional pooling where a small number of nonlinear factors are used to model common nonlinearities across variables. Third, Bayesian computation using MCMC is straightforward even in very high dimensional models, allowing for efficient, equation by equation estimation, thus avoiding computational bottlenecks that arise in popular alternatives such as the time varying parameter VAR. Fourth, existing methods for identifying structural economic shocks in linear factor models can be adapted for the nonlinear case in a straightforward fashion using our model. Exercises involving artificial and macroeconomic data illustrate the properties of our model and its usefulness for forecasting and structural economic analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13972v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Todd Clark, Florian Huber, Gary Koop</dc:creator>
    </item>
    <item>
      <title>Partial Identification of Causal Effects for Endogenous Continuous Treatments</title>
      <link>https://arxiv.org/abs/2508.13946</link>
      <description>arXiv:2508.13946v1 Announce Type: cross 
Abstract: No unmeasured confounding is a common assumption when reasoning about counterfactual outcomes, but such an assumption may not be plausible in observational studies. Sensitivity analysis is often employed to assess the robustness of causal conclusions to unmeasured confounding, but existing methods are predominantly designed for binary treatments. In this paper, we provide natural extensions of two extensively used sensitivity frameworks -- the Rosenbaum and Marginal sensitivity models -- to the setting of continuous exposures. Our generalization replaces scalar sensitivity parameters with sensitivity functions that vary with exposure level, enabling richer modeling and sharper identification bounds. We develop a unified pseudo-outcome regression formulation for bounding the counterfactual dose-response curve under both models, and propose corresponding nonparametric estimators which have second order bias. These estimators accommodate modern machine learning methods for obtaining nuisance parameter estimators, which are shown to achieve $L^2$- consistency, minimax rates of convergence under suitable conditions. Our resulting estimators of bounds for the counterfactual dose-response curve are shown to be consistent and asymptotic normal allowing for a user-specified bound on the degree of uncontrolled exposure endogeneity. We also offer a geometric interpretation that relates the Rosenbaum and Marginal sensitivity model and guides their practical usage in global versus targeted sensitivity analysis. The methods are validated through simulations and a real-data application on the effect of second-hand smoke exposure on blood lead levels in children.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13946v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhinandan Dalal, Eric J. Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Reconstructing Subnational Labor Indicators in Colombia: An Integrated Machine and Deep Learning Approach</title>
      <link>https://arxiv.org/abs/2508.12514</link>
      <description>arXiv:2508.12514v2 Announce Type: replace 
Abstract: This study proposes a unified multi-stage framework to reconstruct consistent monthly and annual labor indicators for all 33 Colombian departments from 1993 to 2025. The approach integrates temporal disaggregation, time-series splicing and interpolation, statistical learning, and institutional covariates to estimate seven key variables: employment, unemployment, labor force participation (PEA), inactivity, working-age population (PET), total population, and informality rate, including in regions without direct survey coverage. The framework enforces labor accounting identities, scales results to demographic projections, and aligns all estimates with national benchmarks to ensure internal coherence. Validation against official departmental GEIH aggregates and city-level informality data for the 23 metropolitan areas yields in-sample Mean Absolute Percentage Errors (MAPEs) below 2.3% across indicators, confirming strong predictive performance. To our knowledge, this is the first dataset to provide spatially exhaustive and temporally consistent monthly labor measures for Colombia. By incorporating both quantitative and qualitative dimensions of employment, the panel enhances the empirical foundation for analysing long-term labor market dynamics, identifying regional disparities, and designing targeted policy interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12514v2</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jaime Vera-Jaramillo</dc:creator>
    </item>
    <item>
      <title>Robustly estimating heterogeneity in factorial data using Rashomon Partitions</title>
      <link>https://arxiv.org/abs/2404.02141</link>
      <description>arXiv:2404.02141v4 Announce Type: replace-cross 
Abstract: In both observational data and randomized control trials, researchers select statistical models to articulate how the outcome of interest varies with combinations of observable covariates. Choosing a model that is too simple can obfuscate important heterogeneity in outcomes between covariate groups, while too much complexity risks identifying spurious patterns. In this paper, we propose a novel Bayesian framework for model uncertainty called Rashomon Partition Sets (RPSs). The RPS consists of all models that have posterior density close to the maximum a posteriori (MAP) model. We construct the RPS by enumeration, rather than sampling, which ensures that we explore all models models with high evidence in the data, even if they offer dramatically different substantive explanations. We use a l0 prior, which allows the allows us to capture complex heterogeneity without imposing strong assumptions about the associations between effects, showing this prior is minimax optimal from an information-theoretic perspective. We characterize the approximation error of (functions of) parameters computed conditional on being in the RPS relative to the entire posterior. We propose an algorithm to enumerate the RPS from the class of models that are interpretable and unique, then provide bounds on the size of the RPS. We give simulation evidence along with three empirical examples: price effects on charitable giving, heterogeneity in chromosomal structure, and the introduction of microfinance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02141v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aparajithan Venkateswaran, Anirudh Sankar, Arun G. Chandrasekhar, Tyler H. McCormick</dc:creator>
    </item>
    <item>
      <title>On a Debiased and Semiparametric Efficient Changes-in-Changes Estimator</title>
      <link>https://arxiv.org/abs/2507.07228</link>
      <description>arXiv:2507.07228v2 Announce Type: replace-cross 
Abstract: We present a novel extension of the influential changes-in-changes (CiC) framework of Athey and Imbens (2006) for estimating the average treatment effect on the treated (ATT) and distributional causal effects in panel data with unmeasured confounding. While CiC relaxes the parallel trends assumption in difference-in-differences (DiD), existing methods typically assume a scalar unobserved confounder and monotonic outcome relationships, and lack inference tools that accommodate continuous covariates flexibly. Motivated by empirical settings with complex confounding and rich covariate information, we make two main contributions. First, we establish nonparametric identification under relaxed assumptions that allow high-dimensional, non-monotonic unmeasured confounding. Second, we derive semiparametrically efficient estimators that are Neyman orthogonal to infinite-dimensional nuisance parameters, enabling valid inference even with machine learning-based estimation of nuisance components. We illustrate the utility of our approach in an empirical analysis of mass shootings and U.S. electoral outcomes, where key confounders, such as political mobilization or local gun culture, are typically unobserved and challenging to quantify.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07228v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinghao Sun, Eric J. Tchetgen Tchetgen</dc:creator>
    </item>
  </channel>
</rss>
